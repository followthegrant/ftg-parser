<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30282938</article-id><article-id pub-id-type="pmc">6210887</article-id><article-id pub-id-type="doi">10.3390/s18103317</article-id><article-id pub-id-type="publisher-id">sensors-18-03317</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An Infrastructure-Free Indoor Localization Algorithm for Smartphones</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6551-6807</contrib-id><name><surname>Wang</surname><given-names>Qu</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03317">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6827-4225</contrib-id><name><surname>Luo</surname><given-names>Haiyong</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03317">2</xref><xref rid="c1-sensors-18-03317" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Men</surname><given-names>Aidong</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03317">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Fang</given-names></name><xref ref-type="aff" rid="af3-sensors-18-03317">3</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Yan</given-names></name><xref ref-type="aff" rid="af4-sensors-18-03317">4</xref></contrib></contrib-group><aff id="af1-sensors-18-03317"><label>1</label>School of Information and Communication Engineering, Beijing University of Posts and Telecommunication, Beijing 100876, China; <email>wangqu@ict.ac.cn</email> (Q.W.); <email>menad@bupt.edu.cn</email> (A.M.)</aff><aff id="af2-sensors-18-03317"><label>2</label>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology Chinese Academy of Sciences, Beijing 100190, China</aff><aff id="af3-sensors-18-03317"><label>3</label>School of Software Engineering, Beijing University of Posts and Telecommunication, Beijing 100876, China; <email>zfsse@bupt.edu.cn</email></aff><aff id="af4-sensors-18-03317"><label>4</label>State Key Laboratory of Advanced Optical Communication Systems and Networks, Peking University, Beijing 100871, China; <email>huangyan0910@pku.edu.cn</email></aff><author-notes><corresp id="c1-sensors-18-03317"><label>*</label>Correspondence: <email>yhluo@ict.ac.cn</email>; Tel.: +86-010-62600759</corresp></author-notes><pub-date pub-type="epub"><day>03</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>10</month><year>2018</year></pub-date><volume>18</volume><issue>10</issue><elocation-id>3317</elocation-id><history><date date-type="received"><day>01</day><month>9</month><year>2018</year></date><date date-type="accepted"><day>29</day><month>9</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 by the authors.</copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Accurate indoor positioning technology provides location-based service for a variety of applications. However, most existing indoor localization approaches (e.g., Wi-Fi and Bluetooth-based methods) rely heavily on positioning infrastructure, which prevents their large-scale deployment and limits the range at which they are applicable. Here, we proposed an infrastructure-free indoor positioning and tracking approach, termed LiMag, which used ubiquitous magnetic field and ambient lights (e.g., fluorescent, incandescent, and light-emitting diodes (LEDs)) without containing modulated information. We conducted an in-depth study on both the advantages and the challenges in leveraging magnetic field and ambient light intensity for indoor localization. Based on the insights from this study, we established a hybrid observation model that took full advantage of both the magnetic field and ambient light signals. To address the low discernibility of the hybrid observation model, LiMag first generated a single-step fingerprint model by vectorizing consecutive hybrid observations within each step. In order to accurately track users, a lightweight single-step tracking algorithm based on the single-step fingerprints and the particle filter framework was designed. LiMag leveraged the walking information of users and several single-step fingerprints to generate long trajectory fingerprints that exhibited much higher location differentiation ability than the single-step fingerprint. To accelerate particle convergence and eliminate the accumulative error of single-step tracking algorithm, a long trajectory calibration scheme based on long trajectory fingerprints was also introduced. An undirected weighted graph model was constructed to decrease the computational overhead resulting from this long trajectory matching. In addition to typical indoor scenarios including offices, shopping malls and parking lots, we also conducted experiments in more challenging scenarios, including large open-plan areas as well as environments characterized by strong sunlight. Our proposed algorithm achieved a 75th percentile localization accuracy of 1.8 m and 2.2 m, respectively, in the office and shopping mall tested. In conclusion, our LiMag algorithm provided location-based service of infrastructure-free with significantly improved localization accuracy and coverage, as well as satisfactory robustness inside complex indoor environments.</p></abstract><kwd-group><kwd>indoor positioning</kwd><kwd>visible light</kwd><kwd>magnetic field</kwd><kwd>fingerprints matching</kwd><kwd>smartphone</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-18-03317"><title>1. Introduction</title><p>Accurate and pervasive indoor positioning significantly facilitates of our daily life tasks [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>]. For example, localization and navigation services in shopping malls, office buildings or parking garages; security domain applications for anti-terrorism action, emergency rescue and exploration missions; consumer analytics through aggregated foot-traffic patterns and dwell time; product recommendation and coupon delivery in retail stores. A recent market report predicted that the global indoor location market size is expected to grow from USD 7.11 billion in 2017 to USD 40.99 billion by 2022, at a compound annual growth rate (CAGR) of 42.0% during the forecast period [<xref rid="B2-sensors-18-03317" ref-type="bibr">2</xref>].</p><p>In order to meet this explosive demand, various indoor positioning approaches have recently been developed, including Wi-Fi [<xref rid="B3-sensors-18-03317" ref-type="bibr">3</xref>,<xref rid="B4-sensors-18-03317" ref-type="bibr">4</xref>], UWB [<xref rid="B5-sensors-18-03317" ref-type="bibr">5</xref>], BLE [<xref rid="B6-sensors-18-03317" ref-type="bibr">6</xref>] and visual methods [<xref rid="B7-sensors-18-03317" ref-type="bibr">7</xref>]. However, these methods are costly and ineffective when the radio signal is weak or not available in some scenarios, such as underground parking lots. Moreover, positioning technology based on radio frequency (RF) is prohibited in electromagnetically sensitive environments such as hospitals, airplanes or mines, due to safety concerns [<xref rid="B8-sensors-18-03317" ref-type="bibr">8</xref>].</p><p>Various lighting infrastructures have been widely installed in indoor environments to provide ubiquitous illumination services. Visible light is susceptible to the propagation environment features, such as shadowing, scattering, and reflection from different surfaces, which cause obvious changes in light intensity at different locations. The propagation of light is however not affected by electromagnetic interference. In addition, the received power of optical signals is more stable than the received power of radio signals [<xref rid="B9-sensors-18-03317" ref-type="bibr">9</xref>]. Thus, using visible light instead of radio allows for higher indoor positioning accuracy. Moreover, visible light is used for illumination while allowing for indoor positioning. Therefore, visible light positioning (VLP) is becoming a promising positioning technology [<xref rid="B10-sensors-18-03317" ref-type="bibr">10</xref>,<xref rid="B11-sensors-18-03317" ref-type="bibr">11</xref>,<xref rid="B12-sensors-18-03317" ref-type="bibr">12</xref>,<xref rid="B13-sensors-18-03317" ref-type="bibr">13</xref>].</p><p>Most VLP studies were based on the propagation model of light [<xref rid="B8-sensors-18-03317" ref-type="bibr">8</xref>,<xref rid="B12-sensors-18-03317" ref-type="bibr">12</xref>,<xref rid="B14-sensors-18-03317" ref-type="bibr">14</xref>] and imaging geometry relations [<xref rid="B11-sensors-18-03317" ref-type="bibr">11</xref>] using modulated smart LEDs as beacons. However, high precision VLP schemes often require well-shaped LEDs [<xref rid="B8-sensors-18-03317" ref-type="bibr">8</xref>,<xref rid="B12-sensors-18-03317" ref-type="bibr">12</xref>,<xref rid="B14-sensors-18-03317" ref-type="bibr">14</xref>,<xref rid="B15-sensors-18-03317" ref-type="bibr">15</xref>] (for propagation modeling) or ultra-dense deployment [<xref rid="B12-sensors-18-03317" ref-type="bibr">12</xref>,<xref rid="B16-sensors-18-03317" ref-type="bibr">16</xref>] (multiple LEDs within the camera field of view), which significantly limit their application. Moreover, most existing VLP systems leverage specialized &#x0201c;smart beaconing LEDs&#x0201d; (working at specific flicker frequency by customized control circuits) as localization beacon for target estimation, are incompatible with the current lighting infrastructures. Unfortunately, fluorescent lights (FLs) currently occupy 85% of the commercial buildings [<xref rid="B17-sensors-18-03317" ref-type="bibr">17</xref>]. In comparison, essential LED lights account for only 12%, with a predicted market domination in approx. 10 to 15 years [<xref rid="B17-sensors-18-03317" ref-type="bibr">17</xref>], not to mention the scarcity of smart beaconing LEDs. Few studies based on existing illumination infrastructures and commercial smartphones achieve high accuracy in specific scenarios.</p><p>Jimenez et al. [<xref rid="B18-sensors-18-03317" ref-type="bibr">18</xref>] presented a light-matching-based indoor localization method using the position, orientation and shape information of lamps, and modeled illumination intensity using an inverse-square law to estimate user positions. Their system relies on the asymmetries/irregularities of luminary placement to distinguish different luminaries, which results in feeble anti-interference ability. Furthermore, the localization performance of light-matching was only evaluated via simulation, and its performance in real-world environments remained unknown. Zhang et al. introduced LiTell [<xref rid="B19-sensors-18-03317" ref-type="bibr">19</xref>], which utilizes unmodified fluorescent lights (FLs) and a commercial smartphone for indoor positioning. LiTell only works for FLs, and is unsuitable for LEDs and incandescent lamps. More importantly, the cameras of commercial smartphones are characterized by a low dynamic range, which limits the working range of LiTell to around 2.5 m, as a result of which many blind areas might be generated. The mechanism of multi-light matching results in high processing latency (&#x0003e;2 s) and high power consumption (&#x0003e;2.7 watts). The most problematic feature of LiTell is its use of the back-facing camera. Together, these factors severely limit its application in a real-world environment. Zhu et al. designed a different algorithm called iLAMP [<xref rid="B20-sensors-18-03317" ref-type="bibr">20</xref>], which uses the conventional LEDs and fluorescent lamps found inside today&#x02019;s buildings for VLP by extracting the spatial radiance pattern (SRP) of lamps. However, frequent SRP extraction inevitably results in high power consumption. While power consumption is significantly lower for iLAMP (0.93 watts) compared to LiTell (2.7 watts), it still remains too high to be suitable for smartphones. Zhao et al. proposed NaviLight [<xref rid="B21-sensors-18-03317" ref-type="bibr">21</xref>], which uses visible light fingerprints and KNN to achieve position estimation using existing lighting infrastructure without containing modulated information. NaviLight achieves sub-meter localization accuracy, under the premise of keeping smartphone attitude constant. However, it is challenging to maintain smartphone attitude constant and heading consistent with walking direction. Moreover, NaviLight fails to perform in an environment characterized by strong sunlight, such as a building with lots of windows or skylights, because sunlight severely affects fingerprints based on visible light. The most significant drawback of existing VLP systems is the fact that they are vulnerable to sunlight noise, as the luminance of a lamp is hundreds of times lower than that of sunlight [<xref rid="B22-sensors-18-03317" ref-type="bibr">22</xref>].</p><p>The Earth&#x02019;s magnetic field [<xref rid="B23-sensors-18-03317" ref-type="bibr">23</xref>] is omnipresent and susceptible to be affected by many factors, such as steel-reinforced concrete, metallic door frames, pillars, furniture, electronic equipment, appliances. These factors generate stable indoor magnetic anomalies that differ across locations. The magnetic field has the potential to localize and track users because of its location-related, infrastructure-free, and energy efficient features [<xref rid="B24-sensors-18-03317" ref-type="bibr">24</xref>]. The Earth&#x02019;s magnetic field has been considered for indoor positioning in many studies [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>,<xref rid="B24-sensors-18-03317" ref-type="bibr">24</xref>,<xref rid="B25-sensors-18-03317" ref-type="bibr">25</xref>,<xref rid="B26-sensors-18-03317" ref-type="bibr">26</xref>,<xref rid="B27-sensors-18-03317" ref-type="bibr">27</xref>]. However, several excellent reviews have commented that the magnitude of magnetism is ambiguous in large indoor spaces or non-steel structure buildings [<xref rid="B28-sensors-18-03317" ref-type="bibr">28</xref>,<xref rid="B29-sensors-18-03317" ref-type="bibr">29</xref>,<xref rid="B30-sensors-18-03317" ref-type="bibr">30</xref>,<xref rid="B31-sensors-18-03317" ref-type="bibr">31</xref>].</p><p>Indoor positioning technologies based on the magnetic field have attracted considerable interest because magnetic sensors have become an essential sensor in most mobile devices. SemanticSLAM [<xref rid="B32-sensors-18-03317" ref-type="bibr">32</xref>] extracts indoor magnetic field anomalies using an unsupervised-learning approach to identify locations. Another study designed a system that uses ambient magnetic field anomalies to build magnetic maps for indoor localization in [<xref rid="B33-sensors-18-03317" ref-type="bibr">33</xref>]. To estimate target position, MaLoc [<xref rid="B25-sensors-18-03317" ref-type="bibr">25</xref>] fuses the inertial sensor data and step counting, heading change and hybrid magnetic measurement between two contiguous steps within a novel reliability-augmented particle filter framework. LocateMe [<xref rid="B34-sensors-18-03317" ref-type="bibr">34</xref>] leverages sequences of magnetic readings to estimate a target&#x02019;s position using smartphones. However, it only achieves a room level localization accuracy. In addition, it is restricted in one-dimensional environments, such as a corridor. To estimate the target&#x02019;s positions, GROPING [<xref rid="B35-sensors-18-03317" ref-type="bibr">35</xref>] leverages crowdsourcing-based fingerprints map and the DTW algorithm based on a revised Monte Carlo model using the magnetic signal. Luo et al. [<xref rid="B36-sensors-18-03317" ref-type="bibr">36</xref>] also proposed an algorithm for the automatic construction of an indoor floor plan, together with a magnetic fingerprints map of unmapped buildings using crowdsourced smartphone data. Fusion of magnetic field and other location sources is a widely considered in indoor positioning. Magicol [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>] integrates magnetic field and Wi-Fi data to deal with the low discernibility of the magnetic field, which is designed with a particle filter-based inertial measurement unit (IMU) engine for localization and tracking. VMag [<xref rid="B37-sensors-18-03317" ref-type="bibr">37</xref>] utilizes a neural-network-based method to extract deep visual features of visual image and designs a context-aware particle filtering framework to fuse the magnetic field and deep visual features. These magnetic positioning methods have already achieved a high positioning accuracy, but it only applies to the areas with route constraints, e.g., long and narrow corridors.</p><p>As shown in <xref ref-type="fig" rid="sensors-18-03317-f001">Figure 1</xref>, we designed and evaluated LiMag&#x02014;a specialized infrastructure-free indoor positioning algorithm based on a smartphone using ambient <bold><italic>Li</italic></bold>ght sources (e.g., fluorescent, incandescent, and LEDs) and <bold><italic>Mag</italic></bold>netic field data with a trajectory matching approach. LiMag extracts the hybrid features of ambient light and magnetic field signals to establish a hybrid observation model, then several hybrid observations within one step or multi-steps constitute a hybrid fingerprints model (HFM) as a location-specific signature. The HFM not only alleviates sunlight interference but also enhances the discernibility of the open-plan area. LiMag performs a single-step tracking algorithm based on particle filter framework and long trajectory calibration scheme using HFM and an undirected weighted graph model (UWGM) [<xref rid="B38-sensors-18-03317" ref-type="bibr">38</xref>] to provide an energy efficient location-service.</p><p>To accomplish an infrastructure-free and accurate localization for complex indoor environments, we must address the following challenges: (1) It is difficult to distinguish different locations using only light or magnetic signals as we may find many locations with same magnetic field magnitude in the rather short trace. Mobile objects also have a considerable impact on the propagation of light. Therefore, a single observed value at a fixed position is insufficient to serve as a reliable and unique location signature. (2) The indoor environment is complex and various. Individual buildings may install sidewall windows or skylights through which sunlight will severely disturb the light sensor readings. Magnetic field signals fluctuate with feebleness in large indoor open-plan areas or non-steel structure building. (3) It remains challenging to quickly match fingerprints collected online with those pre-collected offline.</p><p>To address these challenges: (1) We collected the mixed signal of ambient light and magnetic, and leveraged user motion to vectorize several hybrid observations to form a higher discernibility signature, including single-step trajectory and long trajectory fingerprints. (2) We extracted the hybrid features of magnetic field signals and ambient light to establish a hybrid observation model that utilizes the complementary nature of the magnetic field and light intensity signals. (3) We added directional information to single-step fingerprints and trajectory fingerprint to reduce the matching complexity and improve matching accuracy. We designed LiMag to collect hybrid fingerprints and matches itself with the corresponding directed fingerprints in the pre-established hybrid map, a location database built in offline stage with mappings between light intensity and magnetic values and their locations, to estimate the user&#x02019;s position, leveraging a subsequent dynamic time warping (DTW) algorithm [<xref rid="B39-sensors-18-03317" ref-type="bibr">39</xref>] and UWGM. The key contributions of our study are as follows:<list list-type="bullet"><list-item><p>We performed an in-depth study of both the advantageous properties and the challenges in leveraging the magnetic field and ambient light intensity for indoor localization. Based on these studies, we extracted the hybrid features of indoor ambient light and magnetic field signals within one or several steps to construct a hybrid fingerprint model as a location-specific signature.</p></list-item><list-item><p>We designed a long trajectory calibration scheme based on an undirected weighted graph model. The undirected weighted graph model was constructed to reduce the computational overhead resulting from long trajectory matching (LTM. A result validation of trajectory matching was also designed to filter the DTW output and identify which LTM result is accurate. Lastly, the heading and step length were calibrated by the matched trajectory.</p></list-item><list-item><p>We proposed a single-step tracking algorithm based on the hybrid fingerprints model and the particle filter framework. The weight of particle filters was updated by the hybrid fingerprints model that not only alleviates sunlight interference but also enhances the location differentiation ability in large open-plan areas. The cumulative error of the particle filters was calibrated by the result of the long trajectory calibration scheme.</p></list-item><list-item><p>We implemented LiMag entirely on the Android platform and conducted extensive experiments in multiple scenarios, including a large open-plan area as well as an environment characterized by strong sunlight, and the results showed that LiMag achieved an accuracy of 1.8&#x0223c;3.5 m in typical indoor scenarios.</p></list-item></list></p><p>This paper is organized as follows: in <xref ref-type="sec" rid="sec2-sensors-18-03317">Section 2</xref>, we review the background and our empirical studies of ambient light intensity and magnetic field. In <xref ref-type="sec" rid="sec3-sensors-18-03317">Section 3</xref>, we detail the proposed the infrastructure-free hybrid indoor localization algorithm based on a smartphone. In <xref ref-type="sec" rid="sec4-sensors-18-03317">Section 4</xref>, we evaluate the proposed scheme, and in <xref ref-type="sec" rid="sec5-sensors-18-03317">Section 5</xref>, we provide conclusions that summarize the importance of our work for specialized infrastructure-free hybrid indoor localization.</p></sec><sec id="sec2-sensors-18-03317"><title>2. Insights on Visible Light and Magnetic Fields</title><p>In this section, we conducted extensive experiments in a real environment to investigate whether the hybrid signal of visible light and magnetic field is stable and discriminative enough to act as a suitable location signature.</p><sec id="sec2dot1-sensors-18-03317"><title>2.1. Favorable Properties and Challenges of Visible Light</title><sec id="sec2dot1dot1-sensors-18-03317"><title>2.1.1. Favorable Properties</title><p>To verify the temporal stability of light signals, we walked along the same path in an office building at different times and on different days that were three months apart to collect light signals using a commercial smartphone (Huawei Mate 9). As shown in <xref ref-type="fig" rid="sensors-18-03317-f002">Figure 2</xref>a, the variation trends of three curves were nearly equal. We also generated a light intensity map using a smartphone in a 20 m &#x000d7; 8 m indoor area over two months. As shown in <xref ref-type="fig" rid="sensors-18-03317-f002">Figure 2</xref>b, the light signals maps of different months were similar, although the light intensity differed across locations. These experiments indicate that light intensity at the same place is stable over time under the condition that the lamp layout and architecture topological structure remains unchanged.</p><p>In addition to temporal stability, visible light has stronger location differentiation ability at different locations due to the following factors: first, lamps are often (possibly) covered with lampshades which are made from different materials and irregular decorative panels, as well as inevitable manufacturing variations. Second, reflection, refraction, scattering, and diffraction are present in the process of light propagation, and different surfaces have different light reflection factors. Therefore, the light intensity produced even by a single source is a non-uniform distribution. Moreover, multiple types of lamps may exist simultaneously, thus leading to an even more complicated light intensity distribution. As shown in <xref ref-type="fig" rid="sensors-18-03317-f003">Figure 3</xref>, the light intensity sequences on different paths showed evident differences.</p></sec><sec id="sec2dot1dot2-sensors-18-03317"><title>2.1.2. Challenges</title><p>The propagation of light is assumed to follow a Lambertian radiation pattern [<xref rid="B40-sensors-18-03317" ref-type="bibr">40</xref>]. Therefore, the received light intensity is not only a function of the distance between receiver and beacons, but also depends on the irradiation angle and incidence angle. <xref ref-type="fig" rid="sensors-18-03317-f004">Figure 4</xref>a shows the sensor readings that were precisely collected by three individuals of different height (165, 174, 191 cm) along the same pa. In addition, we also collected sensor readings in different attitudes precisely along the same path using the same device and same people (see <xref ref-type="fig" rid="sensors-18-03317-f004">Figure 4</xref>b). From <xref ref-type="fig" rid="sensors-18-03317-f004">Figure 4</xref>a,b, we see that height has little effect on light intensity, but the attitude has a considerable impact.</p><p>In addition to user diversity, sunlight severely disturbs the light sensor readings. As shown in <xref ref-type="fig" rid="sensors-18-03317-f005">Figure 5</xref>, the intensity of sunlight is several times than that of the indoor lamps, thus resulting in that the light sensor readings are unable to reflect the property of indoor lamps in the area near the windows or skylights. Damage or turned off individual lamps as well as lamp layout changes also cause a significant signal fluctuation, leading to a large localization error.</p></sec></sec><sec id="sec2dot2-sensors-18-03317"><title>2.2. Favorable Properties and Challenges of Magnetic Field</title><sec id="sec2dot2dot1-sensors-18-03317"><title>2.2.1. Favorable Properties</title><p>Indoor magnetic fields exhibit certain anomalies due to the disturbances caused by building construction materials and electrical appliances [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>]. As long as the internal layout remains unchanged, the magnetic anomalies are stable over time [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>,<xref rid="B26-sensors-18-03317" ref-type="bibr">26</xref>,<xref rid="B30-sensors-18-03317" ref-type="bibr">30</xref>,<xref rid="B34-sensors-18-03317" ref-type="bibr">34</xref>,<xref rid="B41-sensors-18-03317" ref-type="bibr">41</xref>]. Therefore, the magnetic anomalies have the potential for accurate and pervasive indoor positioning without depending on any infrastructure [<xref rid="B26-sensors-18-03317" ref-type="bibr">26</xref>,<xref rid="B42-sensors-18-03317" ref-type="bibr">42</xref>].</p></sec><sec id="sec2dot2dot2-sensors-18-03317"><title>2.2.2. Challenges</title><p>The most problematic feature of the magnetic field is its low discernibility [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>,<xref rid="B26-sensors-18-03317" ref-type="bibr">26</xref>,<xref rid="B34-sensors-18-03317" ref-type="bibr">34</xref>]. Construction materials and electrical appliances have little influence on the magnetic field values even at a distance of a few meters [<xref rid="B1-sensors-18-03317" ref-type="bibr">1</xref>]. The discernibility of the magnetic field is limited, commonly within a few tens of &#x003bc;T. To demonstrate the discernibility of the magnetic field, we measured the magnitude of the magnetic field at 800 different locations in a corridor, and generated a histogram. We conducted the same experiments in a large open-plan area (600 <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). The distances from the observation point to the interference sources in the corridor is smaller than that in open-area. As shown in <xref ref-type="fig" rid="sensors-18-03317-f006">Figure 6</xref>, the deviation of the magnetic field observations in a large open-plan area is less than that in the corridor. In other words, the discernibility of the magnetic field in the large open-plan area is less than that in the corridor. Studies [<xref rid="B28-sensors-18-03317" ref-type="bibr">28</xref>,<xref rid="B29-sensors-18-03317" ref-type="bibr">29</xref>,<xref rid="B30-sensors-18-03317" ref-type="bibr">30</xref>] also demonstrates this conclusion.</p><p>A magnetometer outputs three-axis vectors (<italic>m<sub>x</sub></italic>, <italic>m<sub>y</sub></italic> and <italic>m<sub>z</sub></italic>), it is natural to think of using the three-dimensional values to improve the discernibility of the magnetic signal. However, in practice, it is hard to use all of the three-axis magnetic vectors because the carrier frame of the magnetometer may change frequently and is very difficult to align with the global frame and the carrier frame [<xref rid="B42-sensors-18-03317" ref-type="bibr">42</xref>]. To eliminate the influence of three-axis magnetic signal fluctuation with the smartphone attitude, reference [<xref rid="B27-sensors-18-03317" ref-type="bibr">27</xref>] estimated smartphone orientation and transformed the attitude-specific three-axis raw magnetic observation of the carrier frame (b frame, i.e., smartphone) to a uniform three-axis magnetic fingerprint in the navigation frame. However, such a transformation is error-prone and inaccurate because orientation estimation usually contains errors. It is challenging to use only the magnetic field for indoor localization in large open-plan areas or non-steel structure buildings.</p></sec></sec><sec id="sec2dot3-sensors-18-03317"><title>2.3. Fusion of Light Intensity and Magnetic Field</title><p>Visible light has stronger location differentiation ability at different locations, but light intensities are susceptible to environmental changes, while the magnetic field has weaker location differentiation ability in a large open-plan area and is robust against environmental changes. The complementary nature of light intensity and magnetic field signals is the fundamental reason that we combine light and magnetic signals to improve the uniqueness and robustness of location signatures.</p><p>To verify the location differentiation ability of the light intensity signal, magnetic field signal and the hybrid signal of the magnetic field and light intensity, we utilized a confusion matrix <inline-formula><mml:math id="mm2"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> to measure the location differentiation ability of <inline-formula><mml:math id="mm3"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> different locations, as shown in Equations (1) and (2):<disp-formula id="FD1-sensors-18-03317"><label>(1)</label><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022f0;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f0;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-sensors-18-03317"><label>(2)</label><mml:math id="mm5"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>L<sub>i</sub></italic> denotes the position of the obtained visible light and magnetic signals. The <italic>i<sup>th</sup></italic> row and <italic>j<sup>th</sup></italic> column of the matrix <italic>M</italic> represents the normalized Euclidean distance <italic>d<sub>i,j</sub></italic> between the two signal vectors of locations <italic>i</italic> and <italic>j</italic>. <italic>S</italic> represents the light intensity signal, the magnetic field signal, or the hybrid signal of light intensity and magnetic field, respectively.</p><p>We collected the light intensity and magnetic field signal at 10 positions apart 1 m from each other in a large open-plan area, generated normalized confusion matrixes (see <xref ref-type="fig" rid="sensors-18-03317-f007">Figure 7</xref>). <xref ref-type="fig" rid="sensors-18-03317-f007">Figure 7</xref> indicates that the location differentiation ability of the hybrid signal is better than that of the magnetic field or light intensity only.</p></sec></sec><sec id="sec3-sensors-18-03317"><title>3. Positioning Model</title><sec id="sec3dot1-sensors-18-03317"><title>3.1. System Architecture</title><p>As shown in <xref ref-type="fig" rid="sensors-18-03317-f008">Figure 8</xref>, LiMag employs the existing lighting infrastructures and magnetic field, cloud server and smartphone with light sensor, magnetometer, gyroscope and accelerometer for target localization. To reduce the energy- and resource-consumption of the smartphone, a single-step tracking algorithm based on particle filter and long trajectory calibration scheme is deployed on a cloud server. The single-step tracking algorithm based on particle filter framework continuously tracks a user through a pedestrian motion model and the HFM. The pedestrian motion model detects steps, as well as estimates the stride length and walking direction based on the observations from inertial sensor embedded in smartphone, and drives particle state update. The long trajectory calibration scheme based on UWGM provides opportunistic positioning result with high accuracy. The opportunistic positioning result is used to accelerate the particle convergence and calibrate the results of the single-step tracking algorithm.</p></sec><sec id="sec3dot2-sensors-18-03317"><title>3.2. Pedestrian Motion Model</title><sec id="sec3dot2dot1-sensors-18-03317"><title>3.2.1. Precise Pedestrian Heading Estimation Based on Historical Information</title><p>Precise heading estimation is significant for particle motion and LTM, however, it is challenging to precisely estimate walking directions by smartphone-embedded IMU sensors (gyroscope and accelerometer) and magnetometer, due to the complicated walking patterns and indoor electromagnetism interference [<xref rid="B43-sensors-18-03317" ref-type="bibr">43</xref>]. An improved heading estimation algorithm proposed by Wonho Kang is applied here [<xref rid="B44-sensors-18-03317" ref-type="bibr">44</xref>]. The fused heading is estimated as follows:
<disp-formula id="FD500-sensors-18-03317"><label>(3)</label><mml:math id="mm500"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>&#x003b8;<sub>k</sub></italic> and <italic>&#x003b8;<sub>k</sub></italic><sub>&#x02212;1</sub> represent the direction of current step and last step, respectively. <italic>&#x003b8;<sub>m,k</sub></italic> and <italic>&#x003b8;<sub>g,k</sub></italic> represent the direction of magnetic and gyroscope, respectively. <italic>&#x003b8;</italic><sub>&#x00394;,<italic>c</italic></sub> represents the difference between <italic>&#x003b8;<sub>m,k</sub></italic> and <italic>&#x003b8;<sub>g,k</sub></italic>. <italic>&#x003b8;</italic><sub>&#x00394;,<italic>m</italic></sub> represents the difference between <italic>&#x003b8;<sub>m,k</sub></italic> and <italic>&#x003b8;<sub>m,k</sub></italic><sub>&#x02212;1</sub>. <italic>&#x003b8;</italic><sub>&#x00394;,<italic>g</italic></sub> represents the difference between <italic>&#x003b8;<sub>g,k</sub></italic> and <italic>&#x003b8;<sub>g,k</sub></italic><sub>&#x02212;1</sub>. <italic>&#x003b1;<sub>i</sub></italic>, <italic>&#x003b2;<sub>I</sub></italic> and <italic>&#x003b3;<sub>i</sub></italic> represent the weights of <italic>&#x003b8;<sub>k</sub></italic><sub>&#x02212;1</sub>, <italic>&#x003b8;<sub>m,k</sub></italic> and <italic>&#x003b8;<sub>g,k</sub></italic>. <italic>&#x003b8;<sub>&#x003c4;,c</sub></italic>, <italic>&#x003b8;<sub>&#x003c4;,m</sub></italic> and <italic>&#x003b8;<sub>&#x003c4;,g</sub></italic> represent threshold parameters.</p></sec><sec id="sec3dot2dot2-sensors-18-03317"><title>3.2.2. Step Counting and Length Estimation</title><p>To improve the robustness of the step counting, we utilized the step counting algorithm proposed by Kourosh [<xref rid="B45-sensors-18-03317" ref-type="bibr">45</xref>], which solves the overcounting problem caused by false walking (e.g., when users use their phones for playing games in a still state). To provide more robust step length estimation, we employed a probabilistic context-aware (stationary, walking, walking sideways, climbing and descending stairs, and running) step length estimation algorithm proposed by Martinelli [<xref rid="B46-sensors-18-03317" ref-type="bibr">46</xref>]. The performance of a PDR algorithm using the proposed heading estimation method was evaluated as <xref ref-type="fig" rid="sensors-18-03317-f009">Figure 9</xref> shows.</p><p>We walked for 192 steps (including three turns), which corresponds to about 120 m in length. For each step, we calculated the Euclidean distance error between the estimated position and the real position using different heading estimation algorithms (compass, mahonyAHRS and our proposed method). Compared with the other two heading estimation methods, using our proposed heading estimation algorithm achieved higher positioning accuracy.</p></sec></sec><sec id="sec3dot3-sensors-18-03317"><title>3.3 Hybrid Fingerprints Model</title><p>When the vertical direction of smartphone was reliably estimated by the 3-axis gravity sensor built in smartphone, we extracted the vertical component <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and horizontal component <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the magnetic field vector <inline-formula><mml:math id="mm8"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> [<xref rid="B41-sensors-18-03317" ref-type="bibr">41</xref>]. Then, we constructed a three-dimensional magnetic observation (vertical component <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, horizontal component <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and instantaneous direction <inline-formula><mml:math id="mm11"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>). We established a hybrid observation model by combining the three-dimensional magnetic observation and light intensity information. The single hybrid observation is susceptible to sensor noise, thus resulting in an inaccurate localization result. To reduce the occasional fluctuation caused by sensor noise, we leveraged the walking information to vectorize several hybrid observations into hybrid fingerprints of high dimensional. Here, we constructed two types of hybrid fingerprints: single-step fingerprints and long trajectory fingerprints, which were used for user tracking and calibration, respectively.</p><sec id="sec3dot3dot1-sensors-18-03317"><title>3.3.1 Single-Step Fingerprints</title><p>Increasing the spatial coverage of observations is an efficient way to improve the discernibility of signals. Therefore, we define the vector of multiple hybrid observations between within consecutive steps as single-step fingerprints. Magnetic field and light intensity are collected between two consecutive steps, and automatically associates with the position of each step. Therefore, the single-step hybrid fingerprint <inline-formula><mml:math id="mm12"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> is described in Equation (4).
<disp-formula id="FD3-sensors-18-03317"><label>(4)</label><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the vertical and horizontal component of the magnetic field, instantaneous direction and light intensity observation sequence between two consecutive steps, respectively. <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents a global step identifier. <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the central position of one step. <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the step direction. To eliminate the occasional fluctuation caused by user swaying sideways and other noises, we regard the average value of instantaneous direction observation sequence between two consecutive steps as the step direction.</p></sec><sec id="sec3dot3dot2-sensors-18-03317"><title>3.3.2 Long Trajectory Fingerprints</title><p>We define the path between two turns as an atomic path. To further enhance the discernibility of hybrid signals, we combine all single-step fingerprints in atomic path into long trajectory fingerprints. Considering that pedestrians tend to walk in a straight line, we obtain a more precise trajectory direction by calculating the direction average of all single-step fingerprints in same atomic path.</p><p>Therefore, the long trajectory fingerprint <inline-formula><mml:math id="mm21"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is described as Equation (5).
<disp-formula id="FD4-sensors-18-03317"><label>(5)</label><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the single-step fingerprints sequence of an atomic path. <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the global long trajectory identifier. <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the trajectory direction and <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the step count of trajectory.</p></sec><sec id="sec3dot3dot3-sensors-18-03317"><title>3.3.3. Fingerprint Matching</title><p>As shown in <xref ref-type="fig" rid="sensors-18-03317-f010">Figure 10</xref>, different walking speeds result in the spatial sampling density variation issue. Fortunately, the DTW [<xref rid="B39-sensors-18-03317" ref-type="bibr">39</xref>] algorithm from the automatic speech recognition field has the potential to align and measure the similarity between two time-series with different spatial sampling densities, owing to its robustness against series compression, stretching, and phase shifts, as shown in <xref ref-type="fig" rid="sensors-18-03317-f011">Figure 11</xref>. To eliminate the influence of different walking speeds and different sampling ratios, we leveraged the DTW algorithm to measure the similarity between two hybrid fingerprints sequences.</p></sec></sec><sec id="sec3dot4-sensors-18-03317"><title>3.4. Single-Step Tracking Algorithm Based on Particle Filter Framework</title><p>The single-step tracking algorithm based on particle filter leverages a set of the particle to simulate all walking states of pedestrian, and the particle weight <italic>w</italic> represents the confidence of each walking state. A particle with higher weight means that it is closer to the actual position of pedestrian, and the weighted average of all particles&#x02019; positions is regarded as pedestrian&#x02019;s position estimation.</p><p>Here, we describe a particle state as Equation (6):<disp-formula id="FD5-sensors-18-03317"><label>(6)</label><mml:math id="mm27"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <italic>i<sup>th</sup></italic> particle state at time <italic>t</italic>, <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denote the position of the <italic>i<sup>th</sup></italic> particle at time <italic>t</italic>, <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the weight of the <italic>i<sup>th</sup></italic> particle at time <italic>t</italic>, <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represent the motion direction and distance of the <italic>i<sup>th</sup></italic> particle at time <italic>t</italic>, respectively.</p><p>The particle filter framework contains four essential steps: (1) particle initialization. (2) Particle state prediction using the pedestrian motion model and floor plan constraints. Those particles moving into unreachable areas would be killed. (3) The weights of all particles are updated according to the DTW similarity between the latest observation fingerprints and trained fingerprints. (4) Particle resampling based on the weights of all particles. After initializing all particles, the particle filter iterates the step (2) to (4) to consistently track the target.</p><sec id="sec3dot4dot1-sensors-18-03317"><title>3.4.1. Particle Initialization Based on Long Trajectory Matching</title><p>As shown in <xref ref-type="fig" rid="sensors-18-03317-f012">Figure 12</xref>, the initial position of user <italic>p</italic><sub>0</sub> is determined by the result of LTM, and the particles are uniformly distributed in a circle <italic>S</italic> with center <italic>p</italic><sub>0</sub> and radius <italic>R</italic>. The weight of the particles is defined as <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>N</italic> is the total number of particles.</p></sec><sec id="sec3dot4dot2-sensors-18-03317"><title>3.4.2. Motion Model</title><p>As shown in Equation (7) and <xref ref-type="fig" rid="sensors-18-03317-f013">Figure 13</xref>, we leveraged a pedometer to drive particle movement. Once the algorithm detects a step event, all particles move to new positions according to the heading estimation and step length of the latest step:<disp-formula id="FD6-sensors-18-03317"><label>(7)</label><mml:math id="mm34"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:mi>sin</mml:mi><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:mi>cos</mml:mi><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>t</mml:mi><mml:mrow/></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>&#x003b8;</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#x00394;<italic>&#x003b8;</italic> represents the heading change from the (<italic>k</italic>&#x02212;1)<italic><sup>th</sup></italic> step to the <italic>k<sup>th</sup></italic> step. &#x00394;<italic>&#x003b8;</italic> represents the heading obtained from the integral gyroscope. To enlarge the diversity of particles, we introduced Gaussian Noise <italic>G</italic>(0,<inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>&#x003b8;</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and <italic>G</italic>(0,<inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) in the heading and step length estimation, respectively. Every person&#x02019;s stride is different, and even the step length of the same person changes in different walking patterns. Inaccurate step length estimation leads to massive localization errors, and even to localization failure. Therefore, we employed a dynamic step length estimation to solve this challenge efficiently. We sorted the weight in descending order and chose the top 50% most weighted particles for dynamic step length estimation. <italic>Lt</italic> is the average step length of the former k-large particles weight at time <italic>t</italic>, which is defined as <inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo mathsize="small">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msubsup><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mi>g</mml:mi><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo mathsize="small">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. Compared with the step length estimation methods in [<xref rid="B46-sensors-18-03317" ref-type="bibr">46</xref>] that are based on a turn detection algorithm that updates step length only when turning, our algorithm dynamically updates step length based on particle distribution in real time and is independent from the accuracy of the turning detection.</p></sec><sec id="sec3dot4dot3-sensors-18-03317"><title>3.4.3. Particle Constraints Based on the Floor Plan</title><p>We constrained the particle motion range based on the floor plan that indicates the reachable areas (corridor, passage, hall) and the unreachable areas (walls, desks and other furniture). As shown in <xref ref-type="fig" rid="sensors-18-03317-f014">Figure 14</xref>, partial particles are across the wall. We called this kind of particles as irregular particles. We assumed that users would not go to the unreachable areas under normal circumstances. Therefore, all irregular particles&#x02019; weight will be set to 0 and we kill them.</p></sec><sec id="sec3dot4dot4-sensors-18-03317"><title>3.4.4. Weight Update Based on Single-Step Fingerprints</title><p>The weight of each particle is updated by the single-step fingerprints according to Equation (8):<disp-formula id="FD7-sensors-18-03317"><label>(8)</label><mml:math id="mm38"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the weight of the <italic>i<sup>th</sup></italic> particle at time <italic>t</italic>. <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represent <italic>the DTW</italic> distance of the magnetic field and light intensity between online and training single-step fingerprints, respectively. <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represent the parameters that reflect the overall disturbance of the magnetic field and light intensity signals, respectively. After the weight of all particles was updated, we performed a normalization operation for all particles to maintain their sum equal to one before the resampling.</p></sec><sec id="sec3dot4dot5-sensors-18-03317"><title>3.4.5. Particle Resampling</title><p>Resampling aims to eliminate particles with a small weight that contribute little to estimating the location of a pedestrian, and concentrate on particles with a large weight that are much closer to the actual state. In this work, we generated new particles from old ones according to the particle state of the last round according to Equation (9). After a normalization operation, we sort all particles in the descending order by particles&#x02019; weights. A particle is cloned (i.e., resampled) according to which weight range the generated random number <inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> belonging to:<disp-formula id="FD8-sensors-18-03317"><label>(9)</label><mml:math id="mm45"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:msup><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003c9;</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003c9;</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>&#x003c9;</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x022ef;</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>p<sup>k</sup></italic> denotes the <italic>k<sup>th</sup></italic> particle state of current round. <italic>p<sup>i</sup></italic> denotes the <italic>i<sup>th</sup></italic> particle state of the last round. <italic>y<sup>k</sup></italic> denotes the <italic>k<sup>th</sup></italic> random number generated by a uniform distribution between 0 and 1. <italic>&#x003c9;<sup>j</sup></italic> is the weight of the <italic>j<sup>th</sup></italic> particle of the last round. <italic>N</italic> is the total number of particles.</p></sec><sec id="sec3dot4dot6-sensors-18-03317"><title>3.4.6. Pedestrian Position Decision Strategy </title><p>The distribution of particles reflects the likelihood of the pedestrian&#x02019;s real position. The position <italic>p</italic> of pedestrian is estimated based on the weight <italic>&#x003c9;</italic> of particles by Equation (10):<disp-formula id="FD9-sensors-18-03317"><label>(10)</label><mml:math id="mm46"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>particle</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>divergency</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>particle</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>with</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>maxinum</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>weight</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>particle</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>converge</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If particles are diverging, we regard the position of the particle with maximum weight as the pedestrian position. We find pedestrian&#x02019;s position quickly, but it is unstable. We perform a weighted average for the positions of all particles to achieve a more stable position when the particles are converged.</p></sec></sec><sec id="sec3dot5-sensors-18-03317"><title>3.5. Long Trajectory Calibration Scheme Based on Undirected Weighted Graph Model</title><p>We collected users&#x02019; walking trajectories <italic>T</italic> = (<italic>t<sub>i</sub></italic>, <italic>i</italic> = 1,2,L,<italic>n</italic>). Each walking trajectory <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is determined by two consecutive turns. Long trajectory calibration scheme utilized several single-step fingerprints to enhance the discernibility of hybrid fingerprints. In addition, the undirected weighted graph model was used to enhance matching accuracy and decrease the computational overhead of LTM. We designed several empirical criteria to identify which matching result is accurate. Lastly, we leveraged the accurate matching result to calibrate the heading and step length.</p><sec id="sec3dot5dot1-sensors-18-03317"><title>3.5.1. Undirected Weighted Graph Model</title><p>As shown in <xref ref-type="fig" rid="sensors-18-03317-f015">Figure 15</xref>, the pathway information of the indoor floor-plan is modeled by the undirected trace-graph model. The vertexes represent the turning corners and the endings of pathways. The edges represent all pathways that users walk from one place to another. The edge is an atomic path including step number and Euclidean distance information between two vertexes. Here, the indoor floor-plan is denoted as <italic>Graph</italic>, which is described as:<disp-formula id="FD10-sensors-18-03317"><label>(11)</label><mml:math id="mm48"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>V</italic> is a coordinate set of all vertexes. If we use 2D coordinates to describe the indoor floor-plan, the set <italic>V</italic> is denoted as {(<italic>x<sub>i</sub></italic>,<italic>y<sub>i</sub></italic>), <italic>i</italic> =1,2,&#x02026;,<italic>N</italic>}. Here, <italic>N</italic> represents the total number of vertexes. <italic>E</italic> represents a reachable pathways set of all vertexes, denoted as {(<italic>w<sub>p,q</sub></italic>,<italic>d<sub>p,q</sub></italic>), <italic>p</italic> = 1,2,&#x02026;<italic>N</italic>, <italic>q</italic> = 1,2&#x02026;,<italic>N</italic>}. Here <italic>w<sub>p,q</sub></italic> denotes the number of steps between the <italic>p<sup>th</sup></italic> and <italic>q<sup>th</sup></italic> vertex. <italic>d<sub>p,q</sub></italic> denotes the Euclidean distance between the <italic>p<sup>th</sup></italic> and <italic>q<sup>th</sup></italic> vertex.The undirected weighted graph model not only improves the efficiency of collection and reduces storage space of fingerprints, but also significantly reduces the computational overhead of LTM.</p></sec><sec id="sec3dot5dot2-sensors-18-03317"><title>3.5.2. Subsequence Matching</title><p>In the process of real-time positioning, the user is usually in the middle of a trajectory and the input sequence is not complete. The online long trajectory sequence is usually much shorter than the training long trajectory sequences. We converted the DTW to subsequence DTW, which aligns and find the best matching long trajectory subsequence from all feasible training trajectories, as shown in <xref ref-type="fig" rid="sensors-18-03317-f016">Figure 16</xref>.</p><p>To reduce the computational overhead of LTM, we leveraged a sliding window mechanism to maintain an online sequence with fixed length. The sliding window matching mechanism triggers the matching operation immediately once the collected long trajectory fingerprints reach the predefined length, and then accumulates every step&#x02019;s fingerprints data and performs the matching operation continuously.</p></sec><sec id="sec3dot5dot3-sensors-18-03317"><title>3.5.3. Long Trajectory Calibration Scheme</title><p>Algorithm 1 describes the procedure of the long trajectory calibration scheme, where <italic>T</italic> is the pre-trained trajectory set stored in a database, and t is the online trajectory. Once a perfect matching result is obtained, the single-step tracking result will be calibrated to an accurate location.</p><array orientation="portrait"><tbody><tr><td colspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1"><bold>Algorithm 1.</bold> long trajectory calibration scheme</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">1:</td><td align="left" valign="middle" rowspan="1" colspan="1"><bold>Input:</bold> online trajectory t and training trajectory <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2:</td><td align="left" valign="middle" rowspan="1" colspan="1"><bold>Output:</bold> user position <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3:</td><td align="left" valign="middle" rowspan="1" colspan="1">Get the direction <inline-formula><mml:math id="mm51"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> and length <inline-formula><mml:math id="mm52"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> of online trajectory t</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4:</td><td align="left" valign="middle" rowspan="1" colspan="1"><bold>If</bold> the last step of t is turn <bold>then</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;Select similar trajectory according to <inline-formula><mml:math id="mm53"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm54"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;<bold>For</bold> each training trajectory T <bold>do</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;Compute trajectory similarity degree <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between t and <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> using DTW algorithm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">8:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;Result validation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">9:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;<bold>End for</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">10:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;<bold>If</bold> have perfect result <bold>then</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">11:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;Calibrate the result of single-step tracking algorithm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">12:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;<bold>End if</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">13:</td><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>End if</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">14:</td><td align="left" valign="middle" rowspan="1" colspan="1"><bold>If</bold> the last step of t is not turn, <bold>then</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">15:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x000a0;Select similar trajectory according to <inline-formula><mml:math id="mm57"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">16:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x000a0;<bold>For</bold> each training trajectory T <bold>do</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">17:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;Compute trajectory similarity <inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between t and <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> using subsequence DTW algorithm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">18:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;Result validation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">19:</td><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>End for</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">20:</td><td align="left" valign="middle" rowspan="1" colspan="1"><bold>If</bold> have perfect match result <bold>then</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">21:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;Calibrate the result of single-step tracking algorithm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">22:</td><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;<bold>End if</bold></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">23:</td><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>End if</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24:</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Return</bold>
<inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></array></sec><sec id="sec3dot5dot4-sensors-18-03317"><title>3.5.4. Result Validation of Long Trajectory Matching</title><p>The DTW algorithm has the ability to align and find the most similar trajectory from all training trajectories. However, the most similar trajectory is not invariably a perfect trajectory. Therefore, we designed several empirical criteria to filter the DTW matching result and identify the correct matching result.</p><list list-type="simple"><list-item><label>a)</label><p><italic>DTW distance</italic>. This quantity is calculated as the sum of the distances between the aligned samples normalized by the number of samples [<xref rid="B8-sensors-18-03317" ref-type="bibr">8</xref>]. The normalized DTW distance between trajectory pairs of matched must be less than 5.</p></list-item><list-item><label>b)</label><p><italic>Scale factor</italic>. For each trajectory pair of matched <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> samples. The scale factor is <inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac bevelled="true"><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, which represents how DTW stretches or compresses a trajectory to aligns and find the most similar trajectory. The scale factor must be less than 3. We assume that the speed of human walking differs little (commonly less than 3 times).</p></list-item><list-item><label>c)</label><p><italic>Spatial topology</italic>. The shape and space distance of matched trajectory in the PDR trace should also match. For each trajectory pair of matched <italic>t<sub>a</sub></italic>,<italic>t<sub>b</sub></italic>, we obtained two spatial coordinate vectors (<italic>X<sub>A</sub></italic>,<italic>Y<sub>A</sub></italic>) and (<italic>X<sub>B</sub></italic>,<italic>Y<sub>B</sub></italic>) <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. After normalizing the coordinate of PDR trace pair to the origin of coordinates, the similarity of PDR trace is calculated by <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm66"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> denotes the number of the matched point pair after DTW stretching or compressing. The similarity of PDR trace must be less than an empirical threshold. As shown in <xref ref-type="fig" rid="sensors-18-03317-f017">Figure 17</xref>, five paths (<italic>P</italic><sub>1</sub>,&#x02026;,<italic>P</italic><sub>5</sub>) correspond to five training trajectories (<italic>t</italic><sub>1</sub>,&#x02026;,<italic>t</italic><sub>5</sub>) (in red). A pedestrian walks along <italic>p</italic><sub>3</sub>, and obtains an online trajectory <italic>t</italic> (in blue). <italic>d<sub>i</sub></italic> denotes the DTW distance between t and <italic>t<sub>i</sub></italic>. The sensor noise may cause, <italic>d</italic><sub>1</sub>&#x0003c;<italic>d</italic><sub>3</sub>&#x0003c;min({<italic>d</italic><sub>2</sub>,<italic>d</italic><sub>4</sub>,<italic>d</italic><sub>5</sub>}) thus mistaking <italic>t</italic><sub>1</sub> as the most similar trajectory. However, the space distance between <italic>t</italic><sub>1</sub> and <italic>t</italic><sub>0</sub> is much larger than that between <italic>t</italic><sub>3</sub> and <italic>t</italic><sub>0</sub>. The spatial topology constraint can be used to filter out the incorrect mapping trajectory <italic>t</italic><sub>1</sub> and find correct trajectory <italic>t</italic><sub>3</sub> by the shape and space distance constraint of the PDR trace. Therefore, spatial topology filtering is an effective way to avoid mismatching.</p></list-item></list></sec><sec id="sec3dot5dot5-sensors-18-03317"><title>3.5.5. Adaptive Fusion of LTM Matching Results</title><p>We perform LTM algorithm for magnetic field fingerprints trajectory and light fingerprints trajectory, respectively. The confidence of LTM result is decided synthetically by the signal variance and the DTW distance. Therefore, the final pedestrian&#x02019;s position estimation is estimated as follows:<disp-formula id="FD11-sensors-18-03317"><label>(12)</label><mml:math id="mm67"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD12-sensors-18-03317"><label>(13)</label><mml:math id="mm68"><mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the LTM result of magnetic field and light intensity, respectively. <inline-formula><mml:math id="mm72"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> represents the DTW distance in signal space for online and training trajectories. The larger distance indicates the lower similarity. <inline-formula><mml:math id="mm73"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the variance of trajectory fingerprints. Larger variance indicates more signal features, and hence better matching results.</p></sec><sec id="sec3dot5dot6-sensors-18-03317"><title>3.5.6. Calibrating Heading and Step Length Based on the Undirected Weighted Graph Model</title><p>The subsequence DTW aligns and obtains the best-matched trajectory subsequence from the training trajectories. Once a perfect LTM result is obtained, the result of the single-step tracking algorithm is calibrated directly. In addition, the edge corresponding to the online trajectory is indexed from the UWGM. The Euclidean distance and direction of matched trajectory are then obtained according to the corresponding edge. The number of online steps from the last turn is counted by a pedometer. The length of online steps is calibrated by joining the step number of the online trajectory and the Euclidean distance of the matched trajectory. The pedestrian heading is also calibrated by the matched trajectory direction. Therefore, we obtained a more accurate pedestrian motion model by constantly updating both the step length and pedestrian heading. The long trajectory calibration scheme based on an undirected weighted graph model not only provides an initial position for the single-step tracking algorithm, but also improves the performance of the single-step tracking algorithm.</p></sec></sec><sec id="sec3dot6-sensors-18-03317"><title>3.6. Other Considerations: Floor Identification</title><p>Identifying different floors in multistory buildings is an essential task for precise indoor localization, since a positioning system needs to load the floor plan and training model of the corresponding floor number before performing the single-step tracking algorithm and long trajectory calibration scheme. In this paper, we use a barometric pressure information-based floor identification algorithm [<xref rid="B47-sensors-18-03317" ref-type="bibr">47</xref>] that identifies floors with more than 96.1% accuracy.</p></sec></sec><sec id="sec4-sensors-18-03317"><title>4. Experiments and Evaluation</title><p>To understand the effectiveness and limitations of LiMag, we implemented and evaluated LiMag in multiple complex indoor environments.</p><sec id="sec4dot1-sensors-18-03317"><title>4.1. Experimental Setup</title><p>To fully evaluate the performance of LiMag, we conducted a full-fledged implementation based on a client-server architecture. The client side was an Android application that automatically collected sensors readings and displayed positioning results. The server hosted the database of hybrid fingerprints, collected the user uploaded fingerprints data and performed the computationally intensive positioning tasks. For each localization request, the server first loaded the floor plan and training data of the corresponding floor number, then found the best matching location of the phone, and returned the location to the client.</p><p>During the experiment, the data were collected by five participants (including three males and two females) within the height group of 158&#x02013;193 cm using Android smartphones (one Huawei mate 9 with an 8 core 2.4 GHz processor and one Samsung S6 with a 4 Core 2.1 GHz and a 4 Core 1.5 GHz processor) in three typical scenarios (see <xref rid="sensors-18-03317-t001" ref-type="table">Table 1</xref>). The smartphones were equipped with a 3-axis magnetic field sensor, light sensor, a 3-axis accelerometer, and a gyroscope. The smartphones periodically collected the data generated from the sensors mentioned above with a 20 Hz sampling rate.</p><p>To reduce the sampling cost and storage space, we propose a data collection method based on UWGM: the surveyor walked at a constant speed along these edges with the phone facing the ceiling to cover the entire area. The system automatically collected data captured by light and magnetometer built in smartphone during the walking. We calculated the location of the intermediate steps according to the step count and the overall distance of each edge. The location of each fingerprint was then interpolated from the locations of two consecutive steps. Finally, the hybrid fingerprint map was constructed by extrapolating the hybrid fingerprints on the survey path towards both sides until filling up the entire reachable area according to a floor plan that indicates the indoor layout, reachable areas and unreachable areas. We simply assumed pedestrian walking at 1 m/s. Eventually, we obtained hybrid fingerprint in every 0.05 m &#x000d7; 0.05 m square. Taking a 520 square meter office for example, the time to sample the hybrid fingerprint was less than 15 min for one person. The total walking distance was 280 m (448 steps). The size of the hybrid fingerprint was about 1 M.</p></sec><sec id="sec4dot2-sensors-18-03317"><title>4.2. Localization Accuracy in Typical Scenarios</title><p>To verify the performance and practicality of the proposed method, we conducted extensive experiments in three typical scenarios: offices, shopping malls, and underground parking lots, see <xref ref-type="fig" rid="sensors-18-03317-f018">Figure 18</xref>. We walked randomly in each scenario and collected hybrid fingerprints of light intensity and magnetic field during walks. We then ran LiMag to estimate user positions and calculate the localization errors. As shown in <xref ref-type="fig" rid="sensors-18-03317-f019">Figure 19</xref>, the cumulative distribution function (CDF) of localization errors demonstrated that our proposed algorithm achieved 75th percentile localization accuracy of 1.8 m, 2.2 m and 3.3 m in offices, shopping malls and parking lots, respectively. <xref rid="sensors-18-03317-t002" ref-type="table">Table 2</xref> shows that the average localization error using hybrid fingerprints was 1.29 m, 1.58 m and 2.26 m in the offices, shopping malls and parking lots, respectively. Compared with offices and shopping malls, the standard deviation of the localization error in parking lots was larger. Among the three typical scenarios, the localization accuracy of the parking lots was relatively poor, since non-evident optical features were caused by the sparsely-deployed fluorescent lamps and unstable magnetic field distribution caused by movement of the vehicles.</p></sec><sec id="sec4dot3-sensors-18-03317"><title>4.3. Localization Accuracy in Sunlight Interference Scenario</title><p>To validate the anti-interference ability of LiMag, we conducted experiments in buildings with large windows and skylights. The localization performance in CDF form from our proposed algorithm using different fingerprint types is shown in <xref ref-type="fig" rid="sensors-18-03317-f020">Figure 20</xref>. The results demonstrated that our proposed algorithm achieved 75th percentile localization accuracy of 2.5 m, 3 m and 5.6 m using hybrid, magnetic field and light intensity fingerprint in an environment characterized by strong sunlight, respectively. The hybrid fingerprints of light and magnetism achieved 3 m localization accuracy with about 85% confidence, while the light fingerprints only achieved 3 m localization error only with 40% confidence. <xref rid="sensors-18-03317-t003" ref-type="table">Table 3</xref> shows that the average localization error was 1.83 m, 3.95 m and 2.05 m in an environment characterized by strong sunlight using hybrid fingerprints, light fingerprints only and magnetic fingerprints only, respectively. Compared with the magnetic fingerprints only and hybrid fingerprints, the standard deviation of the localization error using light fingerprints was larger. As shown in <xref ref-type="fig" rid="sensors-18-03317-f020">Figure 20</xref> and <xref rid="sensors-18-03317-t003" ref-type="table">Table 3</xref>, the positioning performance of light fingerprints only decreased dramatically when the user entered the sunlight interference area because the sunlight disturbs the light sensor readings severely in sunlight interference area. The result indicated that the hybrid fingerprints improved the localization accuracy evidently in an environment characterized by strong sunlight because the magnetic signal is not affected by the sunlight. We fully utilized the advantages of the magnetic field and light intensity to provide a more accurate location service.</p></sec><sec id="sec4dot4-sensors-18-03317"><title>4.4. Localization Accuracy in Open-Plan Areas</title><p>To evaluate the localization accuracy of the open-plan area, we conducted experiments in broad and open plan environments. <xref ref-type="fig" rid="sensors-18-03317-f021">Figure 21</xref> exhibits the localization error in CDF form from our proposed algorithm using light fingerprint, magnetic fingerprint, hybrid fingerprints of light and magnetic. The results demonstrated that our proposed algorithm achieved 75th percentile localization accuracy of 2.3 m 4.4 m and 2.9 m using hybrid, magnetic and light fingerprint in the open-plan area, respectively. The hybrid fingerprints of light and magnetism achieved 2 m localization error with about 72% confidence, while the magnetic fingerprints only achieved 2 m localization error only with 35% confidence. The hybrid fingerprints of light and magnetism achieved 3 m localization error with about 92% confidence, while the magnetic fingerprints only achieved 3 m localization error only with 64% confidence. <xref rid="sensors-18-03317-t004" ref-type="table">Table 4</xref> shows that the average localization error was 1.55 m, 1.88 m and 2.99 m in the open-plan area using hybrid fingerprints, the light fingerprints only and magnetic fingerprints only, respectively. Compared with the light fingerprints only and magnetic fingerprints only, the standard deviation of the localization error using and hybrid fingerprints of light magnetism was also substantially smaller. In other words, the feeble magnetic signal distortion in broad and open plan environments resulted in that localization accuracy solely based on magnetic fingerprints was inaccurate. The visible light improved the localization accuracy evidently in the open-plan area.</p></sec><sec id="sec4dot5-sensors-18-03317"><title>4.5. Localization Accuracy vs. Trajectory Length</title><p>Location differentiation ability of trajectory fingerprints depends on the length of trajectory fingerprints. To evaluate how the length of trajectory affects the localization performance of the long trajectory calibration scheme, and find the best trajectory length, we conducted extensive experiments on long trajectory calibration scheme with various length of trajectory. <xref ref-type="fig" rid="sensors-18-03317-f022">Figure 22</xref> shows the matching errors corresponding to different lengths of trajectory. From the figure, we saw that the matching errors decreased with an increase in trajectory length. However, increasing the trajectory length will greatly increase the computational complexity and reduce the number of trigger calibrations. To balance the contradiction between matching error and matching number, the trajectory length was set to 15 steps in this paper.</p></sec></sec><sec id="sec5-sensors-18-03317"><title>5. Conclusions</title><p>In this work, we have presented an infrastructure-free indoor positioning system based on smartphone user using ubiquitous magnetic field and arbitrary commercial off-the-shelf (COTS) lamps that already exist in today&#x02019;s buildings. LiMag constructed an HFM using magnetic field and light intensity information, and achieved real-time positioning and tracking based on HFM and particle filter. To accelerate the particle convergence and eliminate the accumulative error of our single-step tracking algorithm based on a particle filter framework, a long trajectory calibration scheme based on UWGM was designed. In addition to typical indoor scenarios, including offices, shopping malls and parking lots, we also conducted experiments in challenging scenarios including large open-plan areas and environments characterized by strong sunlight. The results demonstrated that LiMag provides location-based service with high accuracy, infrastructure-free, practically, as well as satisfactory robustness inside complex indoor environments.</p></sec></body><back><notes><title>Author Contributions</title><p>Q.W., H.L. and A.M. conceived the framework and designed the algorithm and experiments; Q.W. wrote the paper; Y.H. performed some experiments and analyzed the data; F.Z. guided the paper writing and reviewed the paper. All authors read and approved the final manuscript.</p></notes><notes><title>Funding</title><p>This work was supported in part by the National Key Research and Development Program (2018YFB0505200), the BUPT Excellent Ph.D. Students Foundation (CX2018102), the National Natural Science Foundation of China (61872046, 61671264 and 61671077).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-18-03317"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shu</surname><given-names>Y.</given-names></name><name><surname>Bo</surname><given-names>C.</given-names></name><name><surname>Shen</surname><given-names>G.</given-names></name><name><surname>Zhao</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name></person-group><article-title>Magicol: Indoor Localization Using Pervasive Magnetic Field and Opportunistic WiFi Sensing</article-title><source>IEEE J. Sel. Areas Commun.</source><year>2015</year><volume>33</volume><fpage>1443</fpage><lpage>1457</lpage><pub-id pub-id-type="doi">10.1109/JSAC.2015.2430274</pub-id></element-citation></ref><ref id="B2-sensors-18-03317"><label>2.</label><element-citation publication-type="web"><article-title>Indoor Location Market worth 40.99 Billion USD by 2022</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.marketsandmarkets.com/PressReleases/indoor-location.asp">https://www.marketsand markets.com/PressReleases/indoor-location.asp</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2018-10-01">(accessed on 1 October 2018)</date-in-citation></element-citation></ref><ref id="B3-sensors-18-03317"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bahl</surname><given-names>P.</given-names></name><name><surname>Padmanabhan</surname><given-names>V.N.</given-names></name></person-group><article-title>RADAR: An in-building RF-based user location and tracking system</article-title><source>Proceedings of the Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (IEEE INFOCOM 2000)</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>26&#x02013;30 March 2000</conf-date><volume>Volume 2</volume><fpage>775</fpage><lpage>784</lpage></element-citation></ref><ref id="B4-sensors-18-03317"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H.-H.</given-names></name><name><surname>Liu</surname><given-names>C.</given-names></name></person-group><article-title>Implementation of Wi-Fi Signal Sampling on an Android Smartphone for Indoor Positioning Systems</article-title><source>Sensors</source><year>2017</year><volume>18</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.3390/s18010003</pub-id><pub-id pub-id-type="pmid">29267234</pub-id></element-citation></ref><ref id="B5-sensors-18-03317"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tiemann</surname><given-names>J.</given-names></name><name><surname>Pillmann</surname><given-names>J.</given-names></name><name><surname>Wietfeld</surname><given-names>C.</given-names></name></person-group><article-title>Ultra-Wideband Antenna-Induced Error Prediction Using Deep Learning on Channel Response Data</article-title><source>Proceedings of the 2017 IEEE 85th Vehicular Technology Conference (VTC Spring)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>4&#x02013;7 June 2017</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/VTCSpring.2017.8108571</pub-id></element-citation></ref><ref id="B6-sensors-18-03317"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yohan</surname><given-names>A.</given-names></name><name><surname>Lo</surname><given-names>N.-W.</given-names></name><name><surname>Winata</surname><given-names>D.</given-names></name></person-group><article-title>An Indoor Positioning-Based Mobile Payment System Using Bluetooth Low Energy Technology</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>974</elocation-id><pub-id pub-id-type="doi">10.3390/s18040974</pub-id><pub-id pub-id-type="pmid">29587399</pub-id></element-citation></ref><ref id="B7-sensors-18-03317"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Fang</surname><given-names>Z.</given-names></name></person-group><article-title>A Visual-Based Approach for Indoor Radio Map Construction Using Smartphones</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1790</elocation-id><pub-id pub-id-type="doi">10.3390/s17081790</pub-id><pub-id pub-id-type="pmid">28777300</pub-id></element-citation></ref><ref id="B8-sensors-18-03317"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Feng</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>A.</given-names></name></person-group><article-title>Fusion Based on Visible Light Positioning and Inertial Navigation Using Extended Kalman Filters</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1093</elocation-id><pub-id pub-id-type="doi">10.3390/s17051093</pub-id></element-citation></ref><ref id="B9-sensors-18-03317"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso-Gonz&#x000e1;lez</surname><given-names>I.</given-names></name><name><surname>S&#x000e1;nchez-Rodr&#x000ed;guez</surname><given-names>D.</given-names></name><name><surname>Ley-Bosch</surname><given-names>C.</given-names></name><name><surname>Quintana-Su&#x000e1;rez</surname><given-names>M.</given-names></name></person-group><article-title>Discrete Indoor Three-Dimensional Localization System Based on Neural Networks Using Visible Light Communication</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>1040</elocation-id><pub-id pub-id-type="doi">10.3390/s18041040</pub-id><pub-id pub-id-type="pmid">29601525</pub-id></element-citation></ref><ref id="B10-sensors-18-03317"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>B.</given-names></name><name><surname>Gong</surname><given-names>S.</given-names></name><name><surname>Tan</surname><given-names>G.</given-names></name></person-group><article-title>LiPro: Light-based indoor positioning with rotating handheld devices</article-title><source>Wirel. Netw.</source><year>2018</year><volume>24</volume><fpage>49</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1007/s11276-016-1312-1</pub-id></element-citation></ref><ref id="B11-sensors-18-03317"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>Y.</given-names></name><name><surname>Pannuto</surname><given-names>P.</given-names></name><name><surname>Hsiao</surname><given-names>K.</given-names></name><name><surname>Dutta</surname><given-names>P.</given-names></name><name><surname>Arbor</surname><given-names>A.</given-names></name></person-group><article-title>Luxapose: Indoor Positioning with Mobile Phones and Visible Light</article-title><source>Proceedings of the 20th Annual International Conference on Mobile Computing and Networking (Mobicom &#x02019;14)</source><conf-loc>Maui, HI, USA</conf-loc><conf-date>7&#x02013;11 September 2014</conf-date><fpage>299</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1145/2639108.2639109</pub-id></element-citation></ref><ref id="B12-sensors-18-03317"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>P.</given-names></name><name><surname>Peng</surname><given-names>C.</given-names></name><name><surname>Shen</surname><given-names>G.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name></person-group><article-title>Epsilon: A Visible Light Based Positioning System</article-title><source>Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>2&#x02013;4 April 2014</conf-date></element-citation></ref><ref id="B13-sensors-18-03317"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Q.</given-names></name><name><surname>Zheng</surname><given-names>R.</given-names></name><name><surname>Hranilovic</surname><given-names>S.</given-names></name></person-group><article-title>IDyLL: Indoor Localization using Inertial and Light Sensors on Smartphones</article-title><source>Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp &#x02019;15)</source><conf-loc>Osaka, Japan</conf-loc><conf-date>7&#x02013;11 September 2015</conf-date><fpage>307</fpage><lpage>318</lpage></element-citation></ref><ref id="B14-sensors-18-03317"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Luo</surname><given-names>H.</given-names></name><name><surname>Men</surname><given-names>A.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name><name><surname>Gao</surname><given-names>X.</given-names></name><name><surname>Wei</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name></person-group><article-title>Light positioning: A high-accuracy visible light indoor positioning system based on attitude identification and propagation model</article-title><source>Int. J. Distrib. Sens. Netw.</source><year>2018</year><volume>14</volume><fpage>1550147718758263</fpage><pub-id pub-id-type="doi">10.1177/1550147718758263</pub-id></element-citation></ref><ref id="B15-sensors-18-03317"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>P.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Peng</surname><given-names>C.</given-names></name><name><surname>Shen</surname><given-names>G.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name></person-group><article-title>Pharos: Enable Physical Analytics Through Visible Light Based Indoor Localization</article-title><source>Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks (HotNets-XII)</source><conf-loc>College Park, MD, USA</conf-loc><conf-date>21&#x02013;22 November 2013</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B16-sensors-18-03317"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huynh</surname><given-names>P.</given-names></name><name><surname>Yoo</surname><given-names>M.</given-names></name></person-group><article-title>VLC-Based Positioning System for an Indoor Environment Using an Image Sensor and an Accelerometer Sensor</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>783</elocation-id><pub-id pub-id-type="doi">10.3390/s16060783</pub-id><pub-id pub-id-type="pmid">27240383</pub-id></element-citation></ref><ref id="B17-sensors-18-03317"><label>17.</label><element-citation publication-type="web"><article-title>Energy Savings Forecast of Solid-State Lighting in General Illumination Applications</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://nlb.org/energy-savings-forecast-of-solid-state-lighting-in-general-illumination-applications/">https://nlb.org/energy-savings-forecast-of-solid-state-lighting-in-general-illumination-applications/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2018-10-01">(accessed on 1 October 2018)</date-in-citation></element-citation></ref><ref id="B18-sensors-18-03317"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jim&#x000e9;nez</surname><given-names>A.R.</given-names></name><name><surname>Zampella</surname><given-names>F.</given-names></name><name><surname>Seco</surname><given-names>F.</given-names></name></person-group><article-title>Light-matching: A new signal of opportunity for pedestrian indoor navigation</article-title><source>Proceedings of the 4th International Conference on Indoor Positioning and Indoor Navigation (IPIN 2013)</source><conf-loc>Montbeliard-Belfort, France</conf-loc><conf-date>28&#x02013;31 October 2013</conf-date></element-citation></ref><ref id="B19-sensors-18-03317"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>LiTell</article-title><source>Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking (MobiCom &#x02019;16)</source><conf-loc>New York, NY, USA</conf-loc><conf-date>3&#x02013;7 October 2016</conf-date><publisher-name>ACM Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><fpage>230</fpage><lpage>242</lpage></element-citation></ref><ref id="B20-sensors-18-03317"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Enabling High-Precision Visible Light Localization in Today&#x02019;s Buildings</article-title><source>Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys &#x02019;17)</source><conf-loc>Niagara Falls, NY, USA</conf-loc><conf-date>19&#x02013;23 June 2017</conf-date><fpage>96</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1145/3081333.3081335</pub-id></element-citation></ref><ref id="B21-sensors-18-03317"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Peng</surname><given-names>C.</given-names></name><name><surname>Guo</surname><given-names>Q.</given-names></name><name><surname>Wu</surname><given-names>B.</given-names></name></person-group><article-title>NaviLight: Indoor localization and navigation under arbitrary lights</article-title><source>Proceedings of the 36th IEEE International Conference on Computer Communications (INFOCOM 2017)</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>1&#x02013;4 May 2017</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B22-sensors-18-03317"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D.-R.</given-names></name><name><surname>Yang</surname><given-names>S.-H.</given-names></name><name><surname>Kim</surname><given-names>H.-S.</given-names></name><name><surname>Son</surname><given-names>Y.-H.</given-names></name><name><surname>Han</surname><given-names>S.-K.</given-names></name></person-group><article-title>Outdoor visible light communication for inter-vehicle communication using controller area network</article-title><source>Proceedings of the 2012 4th International Conference on Communications and Electronics (ICCE 2012)</source><conf-loc>Hue, Vietnam</conf-loc><conf-date>1&#x02013;3 August 2012</conf-date></element-citation></ref><ref id="B23-sensors-18-03317"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ilyas</surname><given-names>M.</given-names></name><name><surname>Cho</surname><given-names>K.</given-names></name><name><surname>Baeg</surname><given-names>S.-H.</given-names></name><name><surname>Park</surname><given-names>S.</given-names></name></person-group><article-title>Drift Reduction in Pedestrian Navigation System by Exploiting Motion Constraints and Magnetic Field</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>1455</elocation-id><pub-id pub-id-type="doi">10.3390/s16091455</pub-id><pub-id pub-id-type="pmid">27618056</pub-id></element-citation></ref><ref id="B24-sensors-18-03317"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>W.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Luo</surname><given-names>H.</given-names></name><name><surname>Muhammad Zahid</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Li</surname><given-names>D.</given-names></name></person-group><article-title>Location Fingerprint Extraction for Magnetic Field Magnitude Based Indoor Positioning</article-title><source>J. Sens.</source><year>2016</year><volume>2016</volume><fpage>1945695</fpage><pub-id pub-id-type="doi">10.1155/2016/1945695</pub-id></element-citation></ref><ref id="B25-sensors-18-03317"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>H.</given-names></name><name><surname>Gu</surname><given-names>T.</given-names></name><name><surname>Tao</surname><given-names>X.</given-names></name><name><surname>Ye</surname><given-names>H.</given-names></name><name><surname>Lv</surname><given-names>J.</given-names></name></person-group><article-title>MaLoc: A Practical Magnetic Fingerprinting Approach to Indoor Localization using Smartphones Hongwei</article-title><source>Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp &#x02019;14)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;17 September 2014</conf-date><fpage>243</fpage><lpage>253</lpage></element-citation></ref><ref id="B26-sensors-18-03317"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>H.</given-names></name><name><surname>Gu</surname><given-names>T.</given-names></name><name><surname>Tao</surname><given-names>X.</given-names></name><name><surname>Ye</surname><given-names>H.</given-names></name><name><surname>Lu</surname><given-names>J.</given-names></name></person-group><article-title>A Reliability-Augmented Particle Filter for Magnetic Fingerprinting Based Indoor Localization on Smartphone</article-title><source>IEEE Trans. Mob. Comput.</source><year>2016</year><volume>15</volume><fpage>1877</fpage><lpage>1892</lpage><pub-id pub-id-type="doi">10.1109/TMC.2015.2480064</pub-id></element-citation></ref><ref id="B27-sensors-18-03317"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Luo</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name><name><surname>Shao</surname><given-names>W.</given-names></name></person-group><article-title>An indoor self-localization algorithm using the calibration of the online magnetic fingerprints and indoor landmarks</article-title><source>Proceedings of the 2016 International Conference on Indoor Positioning and Indoor Navigation (IPIN 2016)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>4&#x02013;7 October 2016</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B28-sensors-18-03317"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>J.</given-names></name><name><surname>Donahoe</surname><given-names>M.</given-names></name><name><surname>Schmandt</surname><given-names>C.</given-names></name><name><surname>Kim</surname><given-names>I.-J.</given-names></name><name><surname>Razavai</surname><given-names>P.</given-names></name><name><surname>Wiseman</surname><given-names>M.</given-names></name></person-group><article-title>Indoor location sensing using geo-magnetism</article-title><source>Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services (MobiSys &#x02019;11)</source><conf-loc>Bethesda, MD, USA</conf-loc><conf-date>28 June&#x02013;1 July 2011</conf-date><publisher-name>ACM Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2011</year><fpage>141</fpage></element-citation></ref><ref id="B29-sensors-18-03317"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Angermann</surname><given-names>M.</given-names></name><name><surname>Frassl</surname><given-names>M.</given-names></name><name><surname>Doniec</surname><given-names>M.</given-names></name><name><surname>Julian</surname><given-names>B.J.</given-names></name><name><surname>Robertson</surname><given-names>P.</given-names></name></person-group><article-title>Characterization of the indoor magnetic field for applications in Localization and Mapping</article-title><source>Proceedings of the 2012 International Conference on Indoor Positioning and Indoor Navigation (IPIN 2012)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>13&#x02013;15 November 2012</conf-date></element-citation></ref><ref id="B30-sensors-18-03317"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Storms</surname><given-names>W.</given-names></name><name><surname>Shockley</surname><given-names>J.</given-names></name><name><surname>Raquet</surname><given-names>J.</given-names></name></person-group><article-title>Magnetic field navigation in an indoor environment</article-title><source>Proceedings of the 2010 Ubiquitous Positioning Indoor Navigation and Location Based Service (UPINLBS 2010)</source><conf-loc>Helsinki, Finland</conf-loc><conf-date>14&#x02013;15 October 2010</conf-date></element-citation></ref><ref id="B31-sensors-18-03317"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasku</surname><given-names>V.</given-names></name><name><surname>De Angelis</surname><given-names>A.</given-names></name><name><surname>De Angelis</surname><given-names>G.</given-names></name><name><surname>Member</surname><given-names>S.</given-names></name><name><surname>Moschitta</surname><given-names>A.</given-names></name><name><surname>Carbone</surname><given-names>P.</given-names></name></person-group><article-title>Magnetic Field Analysis for 3-D Positioning Applications</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2017</year><volume>66</volume><fpage>935</fpage><lpage>943</lpage><pub-id pub-id-type="doi">10.1109/TIM.2017.2682738</pub-id></element-citation></ref><ref id="B32-sensors-18-03317"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdelnasser</surname><given-names>H.</given-names></name><name><surname>Mohamed</surname><given-names>R.</given-names></name><name><surname>Elgohary</surname><given-names>A.</given-names></name><name><surname>Alzantot</surname><given-names>M.F.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Sen</surname><given-names>S.</given-names></name><name><surname>Choudhury</surname><given-names>R.R.</given-names></name><name><surname>Youssef</surname><given-names>M.</given-names></name></person-group><article-title>SemanticSLAM: Using Environment Landmarks for Unsupervised Indoor Localization</article-title><source>IEEE Trans. Mob. Comput.</source><year>2016</year><volume>15</volume><fpage>1770</fpage><lpage>1782</lpage><pub-id pub-id-type="doi">10.1109/TMC.2015.2478451</pub-id></element-citation></ref><ref id="B33-sensors-18-03317"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gozick</surname><given-names>B.</given-names></name><name><surname>Subbu</surname><given-names>K.P.</given-names></name><name><surname>Dantu</surname><given-names>R.</given-names></name><name><surname>Maeshiro</surname><given-names>T.</given-names></name></person-group><article-title>Magnetic maps for indoor navigation</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2011</year><volume>60</volume><fpage>3883</fpage><lpage>3891</lpage><pub-id pub-id-type="doi">10.1109/TIM.2011.2147690</pub-id></element-citation></ref><ref id="B34-sensors-18-03317"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subbu</surname><given-names>K.P.</given-names></name><name><surname>Gozick</surname><given-names>B.</given-names></name><name><surname>Dantu</surname><given-names>R.</given-names></name></person-group><article-title>LocateMe: Magnetic-fields-based indoor localization using smartphones</article-title><source>ACM Trans. Intell. Syst. Technol.</source><year>2013</year><volume>4</volume><fpage>73</fpage><pub-id pub-id-type="doi">10.1145/2508037.2508054</pub-id></element-citation></ref><ref id="B35-sensors-18-03317"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Subbu</surname><given-names>K.P.</given-names></name><name><surname>Luo</surname><given-names>J.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name></person-group><article-title>GROPING: Geomagnetism and crowdsensing powered indoor navigation</article-title><source>IEEE Trans. Mob. Comput.</source><year>2015</year><volume>14</volume><fpage>387</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1109/TMC.2014.2319824</pub-id></element-citation></ref><ref id="B36-sensors-18-03317"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name><name><surname>Jiang</surname><given-names>M.</given-names></name><name><surname>Ma</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Constructing an Indoor Floor Plan Using Crowdsourcing Based on Magnetic Fingerprinting</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2678</elocation-id><pub-id pub-id-type="doi">10.3390/s17112678</pub-id><pub-id pub-id-type="pmid">29156639</pub-id></element-citation></ref><ref id="B37-sensors-18-03317"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Yin</surname><given-names>Y.</given-names></name><name><surname>Cheng</surname><given-names>L.</given-names></name><name><surname>Zimmermann</surname><given-names>R.</given-names></name></person-group><article-title>Fusion of Magnetic and Visual Sensors for Indoor Localization: Infrastructure-Free and More Effective</article-title><source>IEEE Trans. Multimed.</source><year>2017</year><volume>19</volume><fpage>874</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1109/TMM.2016.2636750</pub-id></element-citation></ref><ref id="B38-sensors-18-03317"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Z.</given-names></name><name><surname>Wen</surname><given-names>H.</given-names></name><name><surname>Markham</surname><given-names>A.</given-names></name><name><surname>Trigoni</surname><given-names>N.</given-names></name></person-group><article-title>Indoor tracking using undirected graphical models</article-title><source>IEEE Trans. Mob. Comput.</source><year>2015</year><volume>14</volume><fpage>2286</fpage><lpage>2301</lpage><pub-id pub-id-type="doi">10.1109/TMC.2015.2398431</pub-id></element-citation></ref><ref id="B39-sensors-18-03317"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rallapalli</surname><given-names>S.</given-names></name></person-group><source>Mobile Localization: Approach and Applications</source><publisher-name>The University of Texas at Austin</publisher-name><publisher-loc>Austin, TX, USA</publisher-loc><year>2014</year></element-citation></ref><ref id="B40-sensors-18-03317"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bejuri</surname><given-names>W.M.</given-names></name><name><surname>Wan</surname><given-names>Y.</given-names></name><name><surname>Mohamad</surname><given-names>M.M.</given-names></name><name><surname>Sapri</surname><given-names>M.</given-names></name></person-group><article-title>Ubiquitous Positioning: A Taxonomy for Location Determination on Mobile Navigation System</article-title><source>Proc. IEEE</source><year>2011</year><volume>85</volume><fpage>265</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.5121/sipij.2011.2103</pub-id></element-citation></ref><ref id="B41-sensors-18-03317"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Gallagher</surname><given-names>T.</given-names></name><name><surname>Dempster</surname><given-names>A.G.</given-names></name><name><surname>Rizos</surname><given-names>C.</given-names></name></person-group><article-title>How feasible is the use of magnetic field alone for indoor positioning?</article-title><source>Proceedings of the 2012 International Conference on Indoor Positioning and Indoor Navigation (IPIN 2012)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>13&#x02013;15 November 2012</conf-date><volume>Volume 15</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B42-sensors-18-03317"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shu</surname><given-names>Y.</given-names></name><name><surname>Shin</surname><given-names>K.G.</given-names></name><name><surname>He</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>Last-Mile Navigation Using Smartphones</article-title><source>Proceedings of the 21st Annual International Conference on Mobile Computing and Networking (MobiCom &#x02019;14)</source><conf-loc>Paris, France</conf-loc><conf-date>7&#x02013;11 September 2015</conf-date></element-citation></ref><ref id="B43-sensors-18-03317"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Shen</surname><given-names>G.</given-names></name></person-group><article-title>Use it free</article-title><source>Proceedings of the 20th Annual International Conference on Mobile Computing and Networking</source><conf-loc>Maui, HI, USA</conf-loc><conf-date>7&#x02013;11 September 2014</conf-date><publisher-name>ACM Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2014</year><fpage>605</fpage><lpage>616</lpage></element-citation></ref><ref id="B44-sensors-18-03317"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>W.</given-names></name><name><surname>Nam</surname><given-names>S.</given-names></name><name><surname>Han</surname><given-names>Y.</given-names></name><name><surname>Lee</surname><given-names>S.</given-names></name></person-group><article-title>Improved heading estimation for smartphone-based indoor positioning systems</article-title><source>Proceedings of the IEEE International Symposium on Personal, Indoor and Mobile Radio Communications</source><conf-loc>Sydney, Australia</conf-loc><conf-date>9&#x02013;12 September 2012</conf-date><fpage>2449</fpage><lpage>2453</lpage><pub-id pub-id-type="doi">10.1109/PIMRC.2012.6362768</pub-id></element-citation></ref><ref id="B45-sensors-18-03317"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>F.</given-names></name><name><surname>Khoshelham</surname><given-names>K.</given-names></name><name><surname>Shang</surname><given-names>J.</given-names></name><name><surname>Yu</surname><given-names>F.</given-names></name><name><surname>Wei</surname><given-names>Z.</given-names></name></person-group><article-title>Robust and Accurate Smartphone-Based Step Counting for Indoor Localization</article-title><source>IEEE Sens. J.</source><year>2017</year><volume>17</volume><fpage>3453</fpage><lpage>3460</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2017.2685999</pub-id></element-citation></ref><ref id="B46-sensors-18-03317"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinelli</surname><given-names>A.</given-names></name><name><surname>Gao</surname><given-names>H.</given-names></name><name><surname>Groves</surname><given-names>P.D.</given-names></name><name><surname>Morosi</surname><given-names>S.</given-names></name></person-group><article-title>Probabilistic Context-Aware Step Length Estimation for Pedestrian Dead Reckoning</article-title><source>IEEE Sens. J.</source><year>2018</year><volume>18</volume><fpage>1600</fpage><lpage>1611</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2017.2776100</pub-id></element-citation></ref><ref id="B47-sensors-18-03317"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>F.</given-names></name><name><surname>Luo</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Pang</surname><given-names>Z.</given-names></name><name><surname>Park</surname><given-names>H.</given-names></name></person-group><article-title>HYFI: Hybrid Floor Identification Based on Wireless Fingerprinting and Barometric Pressure</article-title><source>IEEE Trans. Ind. Inform.</source><year>2017</year><volume>13</volume><fpage>330</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1109/TII.2015.2491264</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-18-03317-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The working scenario of LiMag.</p></caption><graphic xlink:href="sensors-18-03317-g001"/></fig><fig id="sensors-18-03317-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The temporal stability of light intensity. (<bold>a</bold>) Light intensity readings captured at different dates along the same path; (<bold>b</bold>) Light field map captured by smartphone over two months.</p></caption><graphic xlink:href="sensors-18-03317-g002"/></fig><fig id="sensors-18-03317-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The location differentiation ability of light signals.</p></caption><graphic xlink:href="sensors-18-03317-g003"/></fig><fig id="sensors-18-03317-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>User Diversity. (<bold>a</bold>) Different heights; (<bold>b</bold>) Different attitudes.</p></caption><graphic xlink:href="sensors-18-03317-g004"/></fig><fig id="sensors-18-03317-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Sunlight disturbs light sensor readings obviously.</p></caption><graphic xlink:href="sensors-18-03317-g005"/></fig><fig id="sensors-18-03317-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Static magnetometer measurement distribution of different scenarios by the same smartphone. (<bold>a</bold>) corridor; (<bold>b</bold>) large open-plan area.</p></caption><graphic xlink:href="sensors-18-03317-g006"/></fig><fig id="sensors-18-03317-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Confusion matrixes of normalized Euclidean distance. (<bold>a</bold>) light signal; (<bold>b</bold>) magnetic signal; (<bold>c</bold>) light and magnetic signal.</p></caption><graphic xlink:href="sensors-18-03317-g007"/></fig><fig id="sensors-18-03317-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>The architecture of LiMag.</p></caption><graphic xlink:href="sensors-18-03317-g008"/></fig><fig id="sensors-18-03317-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Localization accuracy comparison of PDR using different heading estimation algorithms.</p></caption><graphic xlink:href="sensors-18-03317-g009"/></fig><fig id="sensors-18-03317-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Different walking speed and sampling rate along a same path.</p></caption><graphic xlink:href="sensors-18-03317-g010"/></fig><fig id="sensors-18-03317-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Atomic sequence matching using DTW.</p></caption><graphic xlink:href="sensors-18-03317-g011"/></fig><fig id="sensors-18-03317-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Particle initialization area based on long trajectory matching result.</p></caption><graphic xlink:href="sensors-18-03317-g012"/></fig><fig id="sensors-18-03317-f013" orientation="portrait" position="float"><label>Figure 13</label><caption><p>Motion model.</p></caption><graphic xlink:href="sensors-18-03317-g013"/></fig><fig id="sensors-18-03317-f014" orientation="portrait" position="float"><label>Figure 14</label><caption><p>Diagram of particle constraints with the floor plan.</p></caption><graphic xlink:href="sensors-18-03317-g014"/></fig><fig id="sensors-18-03317-f015" orientation="portrait" position="float"><label>Figure 15</label><caption><p>Undirected weighted graph model. (<bold>a</bold>) indoor floor-plan; (<bold>b</bold>) undirected weighted graph model.</p></caption><graphic xlink:href="sensors-18-03317-g015"/></fig><fig id="sensors-18-03317-f016" orientation="portrait" position="float"><label>Figure 16</label><caption><p>Long trajectory matching via Subsequence DTW.</p></caption><graphic xlink:href="sensors-18-03317-g016"/></fig><fig id="sensors-18-03317-f017" orientation="portrait" position="float"><label>Figure 17</label><caption><p>Spatial topology matching.</p></caption><graphic xlink:href="sensors-18-03317-g017"/></fig><fig id="sensors-18-03317-f018" orientation="portrait" position="float"><label>Figure 18</label><caption><p>Typical indoor scenarios. (<bold>a</bold>) Office; (<bold>b</bold>) Shopping mall; (<bold>c</bold>) Parking lot.</p></caption><graphic xlink:href="sensors-18-03317-g018"/></fig><fig id="sensors-18-03317-f019" orientation="portrait" position="float"><label>Figure 19</label><caption><p>Localization accuracy in typical scenarios.</p></caption><graphic xlink:href="sensors-18-03317-g019"/></fig><fig id="sensors-18-03317-f020" orientation="portrait" position="float"><label>Figure 20</label><caption><p>Localization accuracy of LiMag using different fingerprint types in sunlight interference areas.</p></caption><graphic xlink:href="sensors-18-03317-g020"/></fig><fig id="sensors-18-03317-f021" orientation="portrait" position="float"><label>Figure 21</label><caption><p>Localization accuracy of LiMag using different types of fingerprints in open-plan area.</p></caption><graphic xlink:href="sensors-18-03317-g021"/></fig><fig id="sensors-18-03317-f022" orientation="portrait" position="float"><label>Figure 22</label><caption><p>Matching accuracy vs. trajectory length.</p></caption><graphic xlink:href="sensors-18-03317-g022"/></fig><table-wrap id="sensors-18-03317-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03317-t001_Table 1</object-id><label>Table 1</label><caption><p>Three typical scenario description.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Office</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parking</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Shopping Mall</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Lamp</td><td align="center" valign="middle" rowspan="1" colspan="1">Deployment</td><td align="center" valign="middle" rowspan="1" colspan="1">Regular</td><td align="center" valign="middle" rowspan="1" colspan="1">Irregular</td><td align="center" valign="middle" rowspan="1" colspan="1">Irregular</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Type</td><td align="center" valign="middle" rowspan="1" colspan="1">CFL,LFL</td><td align="center" valign="middle" rowspan="1" colspan="1">ILB</td><td align="center" valign="middle" rowspan="1" colspan="1">CFL,LFL,LED</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number</td><td align="center" valign="middle" rowspan="1" colspan="1">362</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">660</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interval</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3&#x02013;2 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4&#x02013;8 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#x02013;3 m</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Building</td><td align="center" valign="middle" rowspan="1" colspan="1">Area</td><td align="center" valign="middle" rowspan="1" colspan="1">520 m<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">640 m<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">1800 m<sup>2</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Type</td><td align="center" valign="middle" rowspan="1" colspan="1">Concrete</td><td align="center" valign="middle" rowspan="1" colspan="1">Concrete</td><td align="center" valign="middle" rowspan="1" colspan="1">Concrete</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sunlight</td><td align="center" valign="middle" rowspan="1" colspan="1">Lots of windows</td><td align="center" valign="middle" rowspan="1" colspan="1">No windows</td><td align="center" valign="middle" rowspan="1" colspan="1">Lots of windows and skylights</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Electronic instrument</td><td align="center" valign="middle" rowspan="1" colspan="1">Lots of computers</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">Little computers</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Iron shelf</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Little shelves</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lots of shelves</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03317-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03317-t002_Table 2</object-id><label>Table 2</label><caption><p>The localization error in typical scenarios using hybrid fingerprints.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fingerprint Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean Localization Error</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation of Localization Error</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Office</td><td align="center" valign="middle" rowspan="1" colspan="1">1.29 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.69 m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Shopping mall</td><td align="center" valign="middle" rowspan="1" colspan="1">1.58 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 m</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Parking lot</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.26 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.32 m</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03317-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03317-t003_Table 3</object-id><label>Table 3</label><caption><p>The localization error in sunlight interference scenario using different fingerprints.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fingerprint Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean Localization Error</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation of Localization Error</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Light fingerprints</td><td align="center" valign="middle" rowspan="1" colspan="1">3.95 m</td><td align="center" valign="middle" rowspan="1" colspan="1">2.39 m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Magnetic field fingerprints</td><td align="center" valign="middle" rowspan="1" colspan="1">2.05 m</td><td align="center" valign="middle" rowspan="1" colspan="1">1.22 m</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hybrid fingerprints</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.83 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93 m</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03317-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03317-t004_Table 4</object-id><label>Table 4</label><caption><p>The localization error in the open-plan area using different fingerprints.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fingerprint Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean Localization Error</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation of Localization Error</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Light fingerprints</td><td align="center" valign="middle" rowspan="1" colspan="1">1.88 m</td><td align="center" valign="middle" rowspan="1" colspan="1">1.19 m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Magnetic field fingerprints</td><td align="center" valign="middle" rowspan="1" colspan="1">2.99 m</td><td align="center" valign="middle" rowspan="1" colspan="1">1.88 m</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hybrid fingerprints</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.55 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.94 m</td></tr></tbody></table></table-wrap></floats-group></article>