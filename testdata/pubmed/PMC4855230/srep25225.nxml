<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27142627</article-id><article-id pub-id-type="pmc">4855230</article-id><article-id pub-id-type="pii">srep25225</article-id><article-id pub-id-type="doi">10.1038/srep25225</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Unimodal and cross-modal prediction is enhanced in musicians</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Vassena</surname><given-names>Eliana</given-names></name><xref ref-type="corresp" rid="c1">a</xref><xref ref-type="aff" rid="a1">1</xref><xref ref-type="author-notes" rid="n1">*</xref></contrib><contrib contrib-type="author"><name><surname>Kochman</surname><given-names>Katty</given-names></name><xref ref-type="aff" rid="a2">2</xref><xref ref-type="author-notes" rid="n1">*</xref></contrib><contrib contrib-type="author"><name><surname>Latomme</surname><given-names>Julie</given-names></name><xref ref-type="aff" rid="a1">1</xref></contrib><contrib contrib-type="author"><name><surname>Verguts</surname><given-names>Tom</given-names></name><xref ref-type="aff" rid="a1">1</xref></contrib><aff id="a1"><label>1</label><institution>Department of Experimental Psychology, Ghent University</institution>, <country>Belgium</country></aff><aff id="a2"><label>2</label><institution>Institute for Psychoacoustics and Electronic Music</institution>, Ghent University, <country>Belgium</country></aff></contrib-group><author-notes><corresp id="c1"><label>a</label><email>eliana.vassena@ugent.be</email></corresp><fn id="n1"><label>*</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>04</day><month>05</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>6</volume><elocation-id>25225</elocation-id><history><date date-type="received"><day>14</day><month>12</month><year>2015</year></date><date date-type="accepted"><day>06</day><month>04</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016, Macmillan Publishers Limited</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Macmillan Publishers Limited</copyright-holder><license xmlns:xlink="http://www.w3.org/1999/xlink" license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><!--author-paid--><license-p>This work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder to reproduce the material. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions><abstract><p>Musical training involves exposure to complex auditory and visual stimuli, memorization of elaborate sequences, and extensive motor rehearsal. It has been hypothesized that such multifaceted training may be associated with differences in basic cognitive functions, such as prediction, potentially translating to a facilitation in expert musicians. Moreover, such differences might generalize to non-auditory stimuli. This study was designed to test both hypotheses. We implemented a cross-modal attentional cueing task with auditory and visual stimuli, where a target was preceded by compatible or incompatible cues in mainly compatible (80% compatible, predictable) or random blocks (50% compatible, unpredictable). This allowed for the testing of prediction skills in musicians and controls. Musicians showed increased sensitivity to the statistical structure of the block, expressed as advantage for compatible trials (disadvantage for incompatible trials), but only in the mainly compatible (predictable) blocks. Controls did not show this pattern. The effect held within modalities (auditory, visual), across modalities, and when controlling for short-term memory capacity. These results reveal a striking enhancement in cross-modal prediction in musicians in a very basic cognitive task.</p></abstract></article-meta></front><body><p>Music is a universal attribute to all human cultures and pervasive in daily life<xref ref-type="bibr" rid="b1">1</xref><xref ref-type="bibr" rid="b2">2</xref><xref ref-type="bibr" rid="b3">3</xref>. Advanced musical practice involves skills in processing various kinds of stimuli. First, musicians are highly trained in the memorization of auditory stimuli, from simple tones to complex rhythms and harmonic structures<xref ref-type="bibr" rid="b4">4</xref>. Second, musicians read symbolic visual stimuli associated with those tones. Third, musicians produce tones by performing automatized, refined actions involving haptic feedback in coordination with their environment<xref ref-type="bibr" rid="b5">5</xref><xref ref-type="bibr" rid="b6">6</xref>. These components are combined in a rapidly evolving processing stream, and yet organized in a meaningful sequence, which produces the pleasant stimulus the listener perceives as music<xref ref-type="bibr" rid="b7">7</xref><xref ref-type="bibr" rid="b8">8</xref>.</p><p>The complexity of musical training suggests that consistent exposure and expertise may be associated with measurable effects in several cognitive functions<xref ref-type="bibr" rid="b9">9</xref>, as well as on brain plasticity<xref ref-type="bibr" rid="b10">10</xref>. Musicians show enhanced auditory-perception skills, such as pitch discrimination, temporal order judgment<xref ref-type="bibr" rid="b11">11</xref> and discrimination of psychoacoustic features<xref ref-type="bibr" rid="b12">12</xref>. Moreover, structural and functional changes in brain regions dedicated to auditory processing have been consistently reported<xref ref-type="bibr" rid="b13">13</xref>. These auditory-perceptual advantages suggest that musical expertise is associated with improved temporal discrimination and attentional capacity<xref ref-type="bibr" rid="b14">14</xref>.</p><p>The generalization of these benefits to other cognitive functions remains debated<xref ref-type="bibr" rid="b15">15</xref>. A popular study reported that exposure to a 10-minute fragment by Mozart improved spatial reasoning<xref ref-type="bibr" rid="b16">16</xref>. This result had great media resonance and was named &#x0201c;Mozart effect&#x0201d; by the press, conveying the idea that classical music could in fact improve cognitive skills. Although not consistently replicated<xref ref-type="bibr" rid="b17">17</xref>, this result stimulated further research testing whether musical training provides benefits beyond auditory perception, yielding controversial results.</p><p>On the one hand, several studies have reported advantages for musicians in diverse cognitive domains. Musicians have shown better reproduction of time intervals for both auditory and visual intervals<xref ref-type="bibr" rid="b18">18</xref>, as well as better reproduction of multimodal sequences<xref ref-type="bibr" rid="b12">12</xref>. Musicians have also performed better in judging whether auditory and visual information was presented synchronously or asynchronously in musical videoclips<xref ref-type="bibr" rid="b19">19</xref>. Musicians also outperform controls in attention and visuo-spatial tasks such as detecting single elements in complex objects, detecting letters among digits<xref ref-type="bibr" rid="b20">20</xref>, and line bisection<xref ref-type="bibr" rid="b21">21</xref>. Finally, children exposed to 9-months of musical training have shown increased reading abilities as well as pitch discrimination in speech<xref ref-type="bibr" rid="b22">22</xref>. These findings suggest a cross-modal transfer of benefits for musicians beyond the musical domain and beyond the auditory modality.</p><p>On the other hand, several studies have found selective benefits to musical and auditory processing, with no generalization to other modalities. For example, advantages in attentional performance were reported only for auditory, but not for visual attention tasks<xref ref-type="bibr" rid="b12">12</xref><xref ref-type="bibr" rid="b23">23</xref>. Also, musicians proved selectively better at reproducing auditory sequences but not audio-visual sequences<xref ref-type="bibr" rid="b24">24</xref>. Additionally, no advantage was reported in learning sequence structure after passive listening. Lastly, musicians showed improved detection of audio-visual asynchrony but only with music and not with speech<xref ref-type="bibr" rid="b25">25</xref>. However, an important caveat is that most of these studies present correlational evidence, showing better performance in musicians compared to controls. Although informative, one cannot infer a causal influence of musical training on cognitive skills (be it selective to the auditory and musical domain or more general). Causal evidence remains sparse in the literature and should receive further attention in future research.</p><p>This overview suggests auditory processing benefits, possibly deriving from the extensive expertise that musicians acquire in fast-scale temporal processing. However, the evidence for advantages beyond the auditory domain remains varied.</p><p>A potential benefit on the core cognitive process of prediction has been hypothesized, but not directly tested. Prominent cognitive theories such as predictive coding and reinforcement learning suggest that cognitive processing proceeds by prediction<xref ref-type="bibr" rid="b26">26</xref><xref ref-type="bibr" rid="b27">27</xref><xref ref-type="bibr" rid="b28">28</xref><xref ref-type="bibr" rid="b29">29</xref><xref ref-type="bibr" rid="b30">30</xref>. In this framework, each stimulus or sequence leads to a prediction of the upcoming stimulus. Predicted and actual stimulus are compared, and this discrepancy is termed prediction error. Prediction errors drive both cognitive processing and learning<xref ref-type="bibr" rid="b31">31</xref>. Perception of musical rhythm and meter have been framed in the context of predictive theories<xref ref-type="bibr" rid="b32">32</xref>, as well as the relationship between perception and action in musical performance<xref ref-type="bibr" rid="b33">33</xref>. Error (and prediction error) minimization is the core concept shared by these accounts. Furthermore, it has been proposed that the surprise associated with prediction error carries an affective component, and an ideal amount of surprise (not too much but not too little) drives affective reaction to music, as well as guiding expert musicians in pleasing their audience<xref ref-type="bibr" rid="b34">34</xref>. From the empirical point of view, studies reported an advantage for musicians in detecting auditory prediction errors, both with simple sounds and complex harmonic structures<xref ref-type="bibr" rid="b35">35</xref><xref ref-type="bibr" rid="b36">36</xref>. The neural signature of deviance detection also reflects this facilitation. Rhythmic deviance also elicits error-related neural activity<xref ref-type="bibr" rid="b37">37</xref>. Moreover, expert musicians have shown neural correlates of error detection even before performing and incorrect action, supposedly arising from a continuous fast monitoring of predictions and outcomes<xref ref-type="bibr" rid="b38">38</xref>.</p><p>Taken together, these findings support a pivotal role of prediction in music perception and performance. An further intriguing possibility is that musical expertise might be associated with improved prediction skills. The goal of the current study was to test this hypothesis with a standard cognitive task involving basic stimulus-outcome prediction skills, outside the musical domain. Given that prediction applies to any stimulus sequence irrespective of its modalities, we hypothesized that this facilitation may extend to non-auditory (e.g. visual) and even cross-modal sequences (i.e., when an auditory stimulus is predictive of a visual one and vice-versa). The advantage should manifest as increased sensitivity to prediction errors, as a consequence of increased encoding of the statistical structure of the environment in both unimodal and cross-modal conditions. To test this, we implemented a cross-modal cueing paradigm with auditory and visual stimuli. We implemented different levels of predictability and thus prediction errors by different frequencies of compatible and incompatible cue-target pairs. Moreover, we administered two control tasks to measure verbal and visuo-spatial short-term memory. The goal was to determine whether the hypothesized difference would be specific to prediction, or simply accountable to differences in short-term memory capacity.</p><sec disp-level="1"><title>Results</title><p>In the cross-modal cueing task, overall accuracy was 78%&#x02009;&#x000b1;&#x02009;0.4. Accuracy rates were averaged for each subject and for each condition, and subjected to a rANOVA. No significant effect of group was found (<italic>F</italic><sub>(1,27)</sub>&#x02009;=&#x02009;0.524, <italic>p</italic>&#x02009;=&#x02009;0.48), showing that accuracy did not differ between musicians and controls. A significant main effect of compatibility was observed (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;4.57, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.14), with higher accuracy for compatible trials (M&#x02009;=&#x02009;0.96&#x02009;&#x000b1;&#x02009;0.02) relative to incompatible trials (M&#x02009;=&#x02009;0.95&#x02009;&#x000b1;&#x02009;0.02).</p><p>RTs were averaged for each subject and for each condition. Error trials (4.7%) were excluded from further analysis. To minimize the impact of outliers, trials with RTs higher or lower than 2.5 standard deviations of the individual mean were also excluded (2.9%). Trimming means by removing outlying observations is a common way of making the mean a more robust measure of central tendency<xref ref-type="bibr" rid="b39">39</xref>, and a 2.5 standard deviations cut-off is a commonly used convention in the field<xref ref-type="bibr" rid="b40">40</xref>. Subsequently, we tested the assumption of normality of the residuals with the Shapiro-Wilk test. All p-values were larger than 0.05, confirming that the residuals were normally distributed. For 2 out of the 16 conditions the p-values were still rather small (0.08 and 0.06). To test the robustness of our results, we log-transformed the data and run the main analysis again. All significant main effects and interactions reported in the main analysis were preserved when tested on the log-transformed data.</p><p>Crucially, the rANOVA revealed a significant interaction group&#x02009;&#x000d7;&#x02009;compatibility frequency&#x02009;&#x000d7;&#x02009;compatibility (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;6.24, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.18, see <xref ref-type="fig" rid="f1">Fig. 1</xref>), with musicians showing a stronger influence of compatibility frequency (enhanced compatibility effect in the 80/20 condition) as compared to controls.</p><p>Pairwise comparisons revealed a significant difference for musicians between compatible and incompatible trials in the 80/20 condition (<italic>t</italic><sub>(14)</sub>&#x02009;=&#x02009;&#x02212;20.17, <italic>p</italic>&#x02009;=&#x02009;0.001), but not in the 50/50 condition (<italic>t</italic><sub>(14)</sub>&#x02009;=&#x02009;&#x02212;4.9, <italic>p</italic>&#x02009;=&#x02009;0.21). Thus, musicians were relatively disadvantaged for incompatible targets, but only in the 80/20 condition. Conversely, controls showed no difference between compatible and incompatible trials in the 80/20 condition (<italic>t</italic><sub>(14)</sub>&#x02009;=&#x02009;&#x02212;1.68, <italic>p</italic>&#x02009;=&#x02009;0.12) but did show a small difference between compatible and incompatible trials in the 50/50 conditions (<italic>t</italic><sub>(14)</sub>&#x02009;=&#x02009;&#x02212;2.61, <italic>p</italic>&#x02009;=&#x02009;0.02). However, only the difference for the musicians between compatible and incompatible trials in the 80/20 condition remained significant after Bonferroni correction for multiple comparisons. This interaction shows increased sensitivity in musicians to compatibility frequency, suggesting a better representation of the statistical structure of the block. This translated in an increased compatibility effect, when incompatible trials were less frequent. Crucially, the cue modality (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;0.01, <italic>p</italic>&#x02009;=&#x02009;0.93) or target modality (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;0.73, <italic>p</italic>&#x02009;=&#x02009;0.4) did not interact with the 3-way group&#x02009;&#x000d7;&#x02009;compatibility frequency&#x02009;&#x000d7;&#x02009;compatibility interaction, indicating that the effect holds across cue and target modalities.</p><p>Furthermore, a main effect of group was observed (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;10.49, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.27), with musicians showing overall faster RTs than controls. Pairwise comparisons across compatibility frequency and compatibility frequency conditions revealed that musicians reacted faster than controls in all conditions: to compatible targets and incompatible targets in random blocks (C <italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;&#x02212;3.39, p&#x02009;=&#x02009;0.002, IC <italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;&#x02212;3.46, <italic>p</italic>&#x02009;=&#x02009;0.002), and to compatible and incompatible targets in mainly compatible blocks (C <italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;&#x02212;3.6, p&#x02009;=&#x02009;0.001, IC <italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;&#x02212;2.37, <italic>p</italic>&#x02009;=&#x02009;0.025). This last comparison however, was not significant after applying a Bonferroni correction for multiple comparisons.</p><p>Additionally, there was a main effect of target modality (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;189.51, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.87), with faster RTs to visual targets, and a main effect of compatibility (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;16.61, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.37), with faster RTs in compatible trial. A significant group&#x02009;&#x000d7;&#x02009;target modality interaction was also observed (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;5.6, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.17): Musicians responded faster to visual targets compared to auditory targets (<italic>t</italic><sub>(14)</sub>&#x02009;=&#x02009;&#x02212;7.81, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, mean difference &#x02212;87.6&#x02009;ms); controls also responded faster to visual targets (<italic>t</italic><sub>(14)</sub>&#x02009;=&#x02009;&#x02212;11.8, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, mean difference &#x02212;123.9&#x02009;ms); however, the difference for controls was larger, thus driving the interaction. This interaction might reflect a facilitation for musicians in responding to auditory stimuli. Furthermore, there was a significant cue modality&#x02009;&#x000d7;&#x02009;target modality interaction (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;32.93, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.54), with faster RTs to visual as compared to auditory targets (<italic>t</italic><sub>(29)</sub>&#x02009;=&#x02009;&#x02212;12.79, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001), but no significant difference between visual and auditory cues (<italic>t</italic><sub>(29)</sub>&#x02009;=&#x02009;0.21, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.84). A target modality&#x02009;&#x000d7;&#x02009;compatibility frequency interaction was also reported (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;5.29, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.16): RTs were faster for visual compared to auditory targets in both the 80/20 condition (<italic>t</italic><sub>(29)</sub>&#x02009;=&#x02009;&#x02212;12.38, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, mean difference&#x02009;=&#x02009;&#x02212;101.37&#x02009;ms) and 50/50 condition (<italic>t</italic><sub>(29)</sub>&#x02009;=&#x02009;&#x02212;12.56, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, mean difference&#x02009;=&#x02009;&#x02212;110.18&#x02009;ms), with a larger difference for the latter.</p><p>Subsequently, performance on the short-term memory tasks was analyzed. The overall verbal short-term memory capacity score was 74.03&#x02009;&#x000b1;&#x02009;6.01. No significant differences were observed between musicians (M&#x02009;=&#x02009;76.2&#x02009;&#x000b1;&#x02009;7.54) and controls (M&#x02009;=&#x02009;71.87&#x02009;&#x000b1;&#x02009;9.6, <italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;0.34, <italic>p</italic>&#x02009;=&#x02009;0.73). The overall visuo-spatial short-term memory capacity score was 74.4&#x02009;&#x000b1;&#x02009;4.36. No significant differences were reported between musicians (M&#x02009;=&#x02009;79.8&#x02009;&#x000b1;&#x02009;7.01) and controls (M&#x02009;=&#x02009;74.4&#x02009;&#x000b1;&#x02009;4.36, <italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;1.28, <italic>p</italic>&#x02009;=&#x02009;0.22).</p><p>Although there was no group effect in short-term memory, in order to further ensure that the RT effects in the cross-modal cueing task could not be explained by differences in short-term memory capacity, the main rANOVA on RTs was repeated, including both short-term memory scores as covariates. No significant interaction of any factor with short-term memory scores was observed. Moreover, the most relevant group&#x02009;&#x000d7;&#x02009;compatibility frequency&#x02009;&#x000d7;&#x02009;compatibility was preserved (<italic>F</italic><sub>(1,28)</sub>&#x02009;=&#x02009;7.07, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, <italic>&#x003b7;</italic><sup>2</sup>&#x02009;=&#x02009;0.21), showing that the core finding (<xref ref-type="fig" rid="f1">Fig. 1</xref>) is not driven by differences in short-term memory capacity.</p></sec><sec disp-level="1"><title>Discussion</title><p>This study investigated the basic cognitive skill of prediction in musicians and non-musicians. We hypothesized an advantage for musicians in encoding predictable event sequences. The results can be summarized as follows. First, musicians showed enhanced prediction relative to controls, expressed as increased sensitivity to statistical block structure (compatibility frequency) in a very basic cueing task. Second, modulation by musical expertise held across modalities, revealing a striking cross-modal generalization. This shows increased prediction skills in musicians as compared to controls irrespective of event modality. Third, enhanced prediction could not be explained by short-term memory differences.</p><p>Earlier work addressed the role of prediction in music and auditory processing. One conclusion was that regular sound sequences generate predictions and prediction errors at several hierarchical levels, which can determine the pleasurableness of the sequence<xref ref-type="bibr" rid="b41">41</xref>. Showing better prediction skills in musicians in a very basic cueing task indicates that the prediction machinery used in musical processing is rooted in basic cognitive prediction mechanisms.</p><p>Additionally, we reported an overall advantage for musicians, who responded faster in all conditions and irrespective of modality (although no differences in accuracy at any of the tasks were found). Moreover, the group &#x000d7; target modality interaction, suggested to some extent faster processing of auditory targets. On the one hand, this results is compatible with prominent accounts stating that musical training results in fine-tuning and increased efficiency and precision of the auditory system<xref ref-type="bibr" rid="b13">13</xref>, also generalizing to speech<xref ref-type="bibr" rid="b42">42</xref><xref ref-type="bibr" rid="b43">43</xref>. On the other hand, we report an advantage in prediction for musicians irrespective of cue and target modality, thus advocating for a non-selective effect.</p><p>In conclusion, this study provides several avenues for future work. First and foremost, we did not investigate causality. Our data is correlational in nature and does not allow for the drawing of inferences on the effect of musical training on cognition. In fact, a plausible alternative interpretation could be that people with better prediction skills become interested in music as a consequence of their natural abilities, and are presumably more suited to pursue music professionally in life. In order to disentangle the origin of such differences between musicians and non-musicians future studies should administer musical training to naive subjects and measure prediction before and after training. In addition to testing for causal relationship, this would allow investigating the amount of training required to induce measurable benefits. This might be particularly relevant in the context of longitudinal studies addressing the benefits of musical education.</p><p>Second, we did not distinguish between types of musical expertise or years of training, variables that were relevant in earlier research<xref ref-type="bibr" rid="b44">44</xref>. The extent of the advantage might depend on musical instrument, as well as vary as a function of years of training.</p><p>Third, our sample size was rather limited. Future studies should aim for a larger amount of participants in each group to increase power and generalizability of the results.</p><p>Finally, the role of prediction in cognitive processing has been widely studied in computational neuroscience (e.g., reinforcement learning<xref ref-type="bibr" rid="b45">45</xref>, predictive coding<xref ref-type="bibr" rid="b26">26</xref><xref ref-type="bibr" rid="b31">31</xref><xref ref-type="bibr" rid="b46">46</xref>). Here, prediction is central in perception, learning, memory, decision-making, and action selection. It is an exciting open question to what extent training prediction skills as implemented in musical practice, may improve such domain-general abilities.</p></sec><sec disp-level="1"><title>Methods</title><sec disp-level="2"><title>Participants</title><p>Thirty subjects participated to the study (age range 17&#x02013;33, M&#x02009;=&#x02009;19.5, SD&#x02009;=&#x02009;3.2), recruited among Ghent University students who earned credits for participation. The sample size was determined a priori, following earlier conventions in music research<xref ref-type="bibr" rid="b11">11</xref><xref ref-type="bibr" rid="b18">18</xref><xref ref-type="bibr" rid="b21">21</xref>. All participants provided written informed consent. Fifteen were selected for their musical expertise (nine males) with the following requirements: a minimum of five years of playing a musical instrument; having followed formal musical training in a music school; practicing the instrument on a daily basis; and currently playing the instrument at the time of the study. Fifteen control participants were selected (five males), where the listed requirements were exclusion criteria. Control participants and musicians did not differ in age (<italic>t</italic><sub>(28)</sub>&#x02009;=&#x02009;0.11, <italic>p</italic>&#x02009;=&#x02009;0.91) group. All participants gave written informed consent before participation. The experiment was conducted under the General Ethical Protocol for scientific research at the Department of Psychology and Educational Sciences of Ghent University, approved by the department&#x02019;s ethical committee. The procedure was in accordance with the guidelines provided within such protocol.</p></sec><sec disp-level="2"><title>Procedure</title><p>After completing the informed consent, participants performed the main task (cross-modal cueing task), followed by two control tasks measuring verbal short-term memory (verbal span task) and visuo-spatial short-term memory (Corsi block tapping task). All tasks were programmed in E-prime 2.0 (Psychology Software Tools, Pittsburgh, PA) and presented on a 15&#x02033; computer screen. Headphones were used to present auditory stimuli.</p></sec><sec disp-level="2"><title>Cross-modal cueing task</title><p>Each trial started with a fixation cross (see <xref ref-type="fig" rid="f2">Fig. 2</xref>). A first stimulus was presented as a cue (650&#x02009;ms). The cue was followed by a target (650&#x02009;ms), to which the participants had to respond as quickly and as accurately as possible, with a maximum response time limit of 1650&#x02009;ms. In all trials, pressing the response key terminated the trial. Cue and target could be compatible or incompatible. In auditory-auditory trials, cue and target were auditory stimuli. Each of them could be a low pitch tone (800&#x02009;Hz) or high pitch tone (1600&#x02009;Hz, 650&#x02009;ms). At the target, participants responded by indicating if the tone was low (left key press) or high (right key press). In compatible trials, the target matched the pitch of the cue, while in incompatible trials it did not. In visual-visual trials, two squares were presented, one to the left and one to the right of the fixation cross. The cue consisted of an arrow pointing left or right appearing in the center of the screen. The target stimulus consisted of an X, appearing either in the left square or in the right square. Participants had to indicate if the target stimulus X was on the left (left key press) or on the right (right key press). In compatible trials, the X appeared in the same direction as the arrow pointed, while in incompatible trials it appeared on the opposite side. In auditory-visual trials, the auditory cue was followed by the visual target. Trials were considered compatible when a low tone was followed by an X on the left, and a high tone by an X on the right. Trials were considered incompatible when a low tone was followed by an X on the right and when a high tone was followed by an X on the left. In visual-auditory trials the visual cue was followed by the auditory target. Trials were considered compatible when an arrow pointing left was followed by a low tone and an arrow pointing right by a high tone. Trials were considered incompatible when a left-pointing arrow was followed by a high tone and a right-pointing arrow by a low tone.</p><p>Cue modality and target modality were manipulated across blocks, yielding AA, AV, VA and VV blocks. Each of these block types occurred with 80% compatible and 20% incompatible trials (80/20 condition, 40 trials per block, 8 of which incompatible), or with 50% compatible and 50% incompatible trials (50/50 condition, 28 trials per block). This represents the crucial manipulation, as the 80/20 condition is characterized by a statistical structure that prompts prediction. This resulted in 8 blocks, repeated 4 times each, for a total of 32 blocks and 1088 trials. The order of presentation of these blocks was randomized, as well as the order of presentation of trials within a block.</p><p>To summarize, the design implemented the following factors: cue modality (auditory or visual), target modality (auditory or visual), compatibility frequency in the block (80% or 50%), and compatibility in the trial (compatible or incompatible). The task lasted about 45&#x02009;minutes.</p></sec><sec disp-level="2"><title>Verbal short-term memory task</title><p>To control for verbal short-term memory capacity, a letter span task was administered<xref ref-type="bibr" rid="b47">47</xref>. At the start of every trial, a blank screen was presented (1500&#x02009;ms). A sequence of 4 consonants followed, each remaining on the screen for 1200&#x02009;ms (inter-letter blank of 250&#x02009;ms). After a 1500&#x02009;ms retention period, participants were instructed to reproduce the sequence by pressing the corresponding keys on the keyboard in the correct order. The length of the string to be retained increased by one every time that the participant correctly reported three sequences of the current length. The maximum string length was 9. Failing to reproduce a sequence for three consecutive trials would terminate the task, constituting the span length for that participant. A first practice trial with feedback was presented in the beginning. The total task duration was 5&#x02009;minutes on average.</p></sec><sec disp-level="2"><title>Visuo-spatial short-term memory task</title><p>To control for visuo-spatial short-term memory capacity, a Corsi block tapping task was administered to all participants<xref ref-type="bibr" rid="b48">48</xref>. At the start of every trial, a grid of 9 grey-colored squares (35&#x02009;&#x000d7;&#x02009;35&#x02009;mm) was presented (1200&#x02009;ms). Three of the squares would sequentially turn black (each for 1000&#x02009;ms with 500&#x02009;ms in between). After a 1000&#x02009;ms blank screen, participants were asked to report the order of appearance of the squares, by clicking on the square in the order in which they were presented. Correctly reproducing the order in three trials would increase the number of squares by one. The maximum number of squares was 4. Failing to reproduce the order of presentation in three consecutive trials would terminate the task, providing the span length for that participant. A first practice trial with feedback was presented in the beginning of the task. The total task duration was on average 5&#x02009;minutes.</p></sec><sec disp-level="2"><title>Analysis</title><p>First, accuracy at the cross-modal cueing task was analyzed. A repeated measures analysis of variance (rANOVA) was performed on the accuracy data, with between-subjects factor group (musicians, controls), and within-subjects factors cue modality (auditory, visual), target modality (auditory, visual), compatibility frequency in the block (80/20, 50/50), and compatibility in the trial (compatible, incompatible). Second, the reaction times (RTs) of the cross-modal cueing task were analyzed. A rANOVA was conducted on this data, with between-subjects factor group (musicians, controls), and within-subjects factors cue modality (auditory, visual), target modality (auditory, visual), compatibility frequency in the block (80/20, 50/50), and compatibility in the trial (compatible, incompatible.) Third, the scores at the short-term memory tasks were computed. Following conventions from the literature, in both tasks accuracy was calculated as the longest sequence correctly reproduced, multiplied by the total number of correctly reproduced sequences. For example, in the letter span tasks, a participant who correctly reproduced a 6-letter span and overall reproduced 11 spans correctly received an accuracy score of 66. Two-sample t-tests were performed on the verbal short-term memory task scores and visuo-spatial short-term memory task to test for differences in short-term memory capacity between musicians and controls.</p></sec></sec><sec disp-level="1"><title>Additional Information</title><p><bold>How to cite this article</bold>: Vassena, E. <italic>et al</italic>. Unimodal and cross-modal prediction is enhanced in musicians. <italic>Sci. Rep</italic>. <bold>6</bold>, 25225; doi: 10.1038/srep25225 (2016).</p></sec></body><back><ack><p>We thank Esther De Loof for useful discussion, and Jean-Philippe Van Dijk for advice on the short-term memory tasks. The project was funded by Ghent University GOA grant BOF08/GOA011 and by the Ghent University Multidisciplinary Research Partnership &#x0201c;The integrative neuroscience of behavioral control&#x0201d;.</p></ack><ref-list><ref id="b1"><mixed-citation publication-type="journal"><name><surname>Cross</surname><given-names>I.</given-names></name>
<article-title>Music in the evolution of the mind</article-title>. <source>Trends Neurosci.</source>
<volume>24</volume>, <fpage>190</fpage> (<year>2001</year>).</mixed-citation></ref><ref id="b2"><mixed-citation publication-type="journal"><name><surname>Mithen</surname><given-names>S.</given-names></name>
<article-title>Singing in the brain</article-title>. <source>New Sci.</source>
<volume>197</volume>, <fpage>38</fpage>&#x02013;<lpage>39</lpage> (<year>2008</year>).</mixed-citation></ref><ref id="b3"><mixed-citation publication-type="journal"><name><surname>Savage</surname><given-names>P. E.</given-names></name>, <name><surname>Brown</surname><given-names>S.</given-names></name>, <name><surname>Sakai</surname><given-names>E.</given-names></name> &#x00026; <name><surname>Currie</surname><given-names>T. E.</given-names></name>
<article-title>Statistical universals reveal the structures and functions of human music</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>
<volume>112</volume>, <fpage>8987</fpage>&#x02013;<lpage>92</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26124105</pub-id></mixed-citation></ref><ref id="b4"><mixed-citation publication-type="journal"><name><surname>Vuust</surname><given-names>P.</given-names></name>, <name><surname>Gebauer</surname><given-names>L. K.</given-names></name> &#x00026; <name><surname>Witek</surname><given-names>M. A. G.</given-names></name>
<article-title>Neural underpinnings of music: the polyrhythmic brain</article-title>. <source>Adv. Exp. Med. Biol.</source>
<volume>829</volume>, <fpage>339</fpage>&#x02013;<lpage>356</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">25358719</pub-id></mixed-citation></ref><ref id="b5"><mixed-citation publication-type="journal"><name><surname>Maes</surname><given-names>P.-J.</given-names></name>, <name><surname>Leman</surname><given-names>M.</given-names></name>, <name><surname>Palmer</surname><given-names>C.</given-names></name> &#x00026; <name><surname>Wanderley</surname><given-names>M. M.</given-names></name>
<article-title>Action-based effects on music perception</article-title>. <source>Front. Psychol.</source>
<volume>4</volume>, <fpage>1008</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24454299</pub-id></mixed-citation></ref><ref id="b6"><mixed-citation publication-type="journal"><name><surname>Zatorre</surname><given-names>R. J.</given-names></name>, <name><surname>Chen</surname><given-names>J. L.</given-names></name> &#x00026; <name><surname>Penhune</surname><given-names>V. B.</given-names></name>
<article-title>When the brain plays music: auditory-motor interactions in music perception and production</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>8</volume>, <fpage>547</fpage>&#x02013;<lpage>558</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17585307</pub-id></mixed-citation></ref><ref id="b7"><mixed-citation publication-type="journal"><name><surname>Zatorre</surname><given-names>R.</given-names></name>
<article-title>Music, the food of neuroscience?</article-title>
<source>Nature</source>
<volume>434</volume>, <fpage>312</fpage>&#x02013;<lpage>315</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15772648</pub-id></mixed-citation></ref><ref id="b8"><mixed-citation publication-type="journal"><name><surname>Zatorre</surname><given-names>R. J.</given-names></name> &#x00026; <name><surname>Salimpoor</surname><given-names>V. N.</given-names></name>
<article-title>From perception to pleasure: music and its neural substrates</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>110</volume>, <fpage>10430</fpage>&#x02013;<lpage>10437</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23754373</pub-id></mixed-citation></ref><ref id="b9"><mixed-citation publication-type="journal"><name><surname>Zuk</surname><given-names>J.</given-names></name>, <name><surname>Benjamin</surname><given-names>C.</given-names></name>, <name><surname>Kenyon</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Gaab</surname><given-names>N.</given-names></name>
<article-title>Behavioral and neural correlates of executive functioning in musicians and non-musicians</article-title>. <source>PloS One</source>
<volume>9</volume>, <fpage>e99868</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24937544</pub-id></mixed-citation></ref><ref id="b10"><mixed-citation publication-type="journal"><name><surname>Herholz</surname><given-names>S. C.</given-names></name> &#x00026; <name><surname>Zatorre</surname><given-names>R. J.</given-names></name>
<article-title>Musical Training as a Framework for Brain Plasticity: Behavior, Function, and Structure</article-title>. <source>Neuron</source>
<volume>76</volume>, <fpage>486</fpage>&#x02013;<lpage>502</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23141061</pub-id></mixed-citation></ref><ref id="b11"><mixed-citation publication-type="journal"><name><surname>Hodges</surname><given-names>D. A.</given-names></name>, <name><surname>Hairston</surname><given-names>W. D.</given-names></name> &#x00026; <name><surname>Burdette</surname><given-names>J. H.</given-names></name>
<article-title>Aspects of multisensory perception: the integration of visual and auditory information in musical experiences</article-title>. <source>Ann. N. Y. Acad. Sci.</source>
<volume>1060</volume>, <fpage>175</fpage>&#x02013;<lpage>185</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">16597762</pub-id></mixed-citation></ref><ref id="b12"><mixed-citation publication-type="journal"><name><surname>Carey</surname><given-names>D.</given-names></name>
<etal/>. <article-title>Generality and specificity in the effects of musical expertise on perception and cognition</article-title>. <source>Cognition</source>
<volume>137</volume>, <fpage>81</fpage>&#x02013;<lpage>105</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25618010</pub-id></mixed-citation></ref><ref id="b13"><mixed-citation publication-type="journal"><name><surname>Kraus</surname><given-names>N.</given-names></name> &#x00026; <name><surname>Chandrasekaran</surname><given-names>B.</given-names></name>
<article-title>Music training for the development of auditory skills</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>11</volume>, <fpage>599</fpage>&#x02013;<lpage>605</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20648064</pub-id></mixed-citation></ref><ref id="b14"><mixed-citation publication-type="other"><name><surname>Lim</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Sinnett</surname><given-names>S.</given-names></name> Exploring Visual Attention in Musicians: Temporal, Spatial and Capacity Considerations. In Carlson, L., H&#x000f6;lscher, C., &#x00026; Shipley, T. <italic>Proceedings of the 33rd Annual Conference of the Cognitive Science Society</italic> (pp. 580&#x02013;585). Austin, TX: Cognitive Science Society (2011).</mixed-citation></ref><ref id="b15"><mixed-citation publication-type="journal"><name><surname>Moreno</surname><given-names>S.</given-names></name> &#x00026; <name><surname>Bidelman</surname><given-names>G. M.</given-names></name>
<article-title>Examining neural plasticity and cognitive benefit through the unique lens of musical training</article-title>. <source>Hear. Res.</source>
<volume>308</volume>, <fpage>84</fpage>&#x02013;<lpage>97</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24079993</pub-id></mixed-citation></ref><ref id="b16"><mixed-citation publication-type="journal"><name><surname>Rauscher</surname><given-names>F. H.</given-names></name>, <name><surname>Shaw</surname><given-names>G. L.</given-names></name> &#x00026; <name><surname>Ky</surname><given-names>K. N.</given-names></name>
<article-title>Music and spatial task performance</article-title>. <source>Nature</source>
<volume>365</volume>(6447), <fpage>611</fpage> (<year>1993</year>).</mixed-citation></ref><ref id="b17"><mixed-citation publication-type="journal"><name><surname>Schellenberg</surname><given-names>E. G.</given-names></name>
<article-title>Cognitive Performance after listening to music: A review of the Mozart Effect</article-title>. In <name><surname>MacDonald</surname><given-names>R.</given-names></name>, <name><surname>Kreuz</surname><given-names>G.</given-names></name> &#x00026; <name><surname>Mitchell</surname><given-names>L.</given-names></name><source>Music, health, and wellbeing</source>
<fpage>324</fpage>&#x02013;<lpage>338</lpage>. Oxford: Oxford University Press (<year>2012</year>).</mixed-citation></ref><ref id="b18"><mixed-citation publication-type="journal"><name><surname>Aagten-Murphy</surname><given-names>D.</given-names></name>, <name><surname>Cappagli</surname><given-names>G.</given-names></name> &#x00026; <name><surname>Burr</surname><given-names>D.</given-names></name>
<article-title>Musical training generalises across modalities and reveals efficient and adaptive mechanisms for reproducing temporal intervals</article-title>. <source>Acta Psychol. (Amst.)</source>
<volume>147</volume>, <fpage>25</fpage>&#x02013;<lpage>33</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24184174</pub-id></mixed-citation></ref><ref id="b19"><mixed-citation publication-type="journal"><name><surname>Bishop</surname><given-names>L.</given-names></name> &#x00026; <name><surname>Goebl</surname><given-names>W.</given-names></name>
<article-title>Context-specific effects of musical expertise on audiovisual integration</article-title>. <source>Front. Psychol.</source>
<volume>5</volume>, <fpage>1123</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">25324819</pub-id></mixed-citation></ref><ref id="b20"><mixed-citation publication-type="journal"><name><surname>Helmbold</surname><given-names>N.</given-names></name>, <name><surname>Rammsayer</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Altenm&#x000fc;ller</surname><given-names>E.</given-names></name>
<article-title>Differences in primary mental abilities between musicians and nonmusicians</article-title>. <source>J. Individ. Differ.</source>
<volume>26</volume>, <fpage>74</fpage>&#x02013;<lpage>85</lpage> (<year>2005</year>).</mixed-citation></ref><ref id="b21"><mixed-citation publication-type="journal"><name><surname>Patston</surname><given-names>L. L.</given-names></name>, <name><surname>Hogg</surname><given-names>S. L.</given-names></name> &#x00026; <name><surname>Tippett</surname><given-names>L. J.</given-names></name>
<article-title>Attention in musicians is more bilateral than in non-musicians</article-title>. <source>Laterality</source>
<volume>12</volume>, <fpage>262</fpage>&#x02013;<lpage>272</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17454575</pub-id></mixed-citation></ref><ref id="b22"><mixed-citation publication-type="journal"><name><surname>Moreno</surname><given-names>S.</given-names></name>
<etal/>. <article-title>Musical training influences linguistic abilities in 8-year-old children: more evidence for brain plasticity</article-title>. <source>Cereb. Cortex</source>
<volume>19</volume>, <fpage>712</fpage>&#x02013;<lpage>723</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">18832336</pub-id></mixed-citation></ref><ref id="b23"><mixed-citation publication-type="journal"><name><surname>Strait</surname><given-names>D. L.</given-names></name>, <name><surname>Kraus</surname><given-names>N.</given-names></name>, <name><surname>Parbery-Clark</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Ashley</surname><given-names>R.</given-names></name>
<article-title>Musical experience shapes top-down auditory mechanisms: evidence from masking and auditory attention performance</article-title>. <source>Hear. Res.</source>
<volume>261</volume>, <fpage>22</fpage>&#x02013;<lpage>29</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20018234</pub-id></mixed-citation></ref><ref id="b24"><mixed-citation publication-type="journal"><name><surname>Tierney</surname><given-names>A. T.</given-names></name>, <name><surname>Bergeson-Dana</surname><given-names>T. R.</given-names></name> &#x00026; <name><surname>Pisoni</surname><given-names>D. B.</given-names></name>
<article-title>Effects of early musical experience on auditory sequence memory</article-title>. <source>Empir. Musicol. Rev. EMR</source>
<volume>3</volume>, <fpage>178</fpage> (<year>2008</year>).<pub-id pub-id-type="pmid">21394231</pub-id></mixed-citation></ref><ref id="b25"><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>H.</given-names></name> &#x00026; <name><surname>Noppeney</surname><given-names>U.</given-names></name>
<article-title>Long-term music training tunes how the brain temporally binds signals from multiple senses</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>108</volume>, <fpage>E1441</fpage>&#x02013;<lpage>E1450</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">22114191</pub-id></mixed-citation></ref><ref id="b26"><mixed-citation publication-type="journal"><name><surname>Friston</surname><given-names>K.</given-names></name> &#x00026; <name><surname>Kiebel</surname><given-names>S.</given-names></name>
<article-title>Cortical circuits for perceptual inference</article-title>. <source>Neural Netw.</source>
<volume>22</volume>, <fpage>1093</fpage>&#x02013;<lpage>1104</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19635656</pub-id></mixed-citation></ref><ref id="b27"><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>H.</given-names></name> &#x00026; <name><surname>Noppeney</surname><given-names>U.</given-names></name>
<article-title>Temporal prediction errors in visual and auditory cortices</article-title>. <source>Curr. Biol.</source>
<volume>24</volume>, <fpage>R309</fpage>&#x02013;<lpage>R310</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24735850</pub-id></mixed-citation></ref><ref id="b28"><mixed-citation publication-type="journal"><name><surname>DenOuden</surname><given-names>H. E. M. D.</given-names></name>, <name><surname>Friston</surname><given-names>K. J.</given-names></name>, <name><surname>Daw</surname><given-names>N. D.</given-names></name>, <name><surname>McIntosh</surname><given-names>A. R.</given-names></name> &#x00026; <name><surname>Stephan</surname><given-names>K. E.</given-names></name>
<article-title>A dual role for prediction error in associative learning</article-title>. <source>Cereb. Cortex</source>
<volume>19</volume>, <fpage>1175</fpage>&#x02013;<lpage>1185</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">18820290</pub-id></mixed-citation></ref><ref id="b29"><mixed-citation publication-type="journal"><name><surname>Vassena</surname><given-names>E.</given-names></name>, <name><surname>Krebs</surname><given-names>R. M.</given-names></name>, <name><surname>Silvetti</surname><given-names>M.</given-names></name>, <name><surname>Fias</surname><given-names>W.</given-names></name> &#x00026; <name><surname>Verguts</surname><given-names>T.</given-names></name>
<article-title>Dissociating contributions of ACC and vmPFC in reward prediction, outcome, and choice</article-title>. <source>Neuropsychologia</source>
<volume>59</volume>, <fpage>112</fpage>&#x02013;<lpage>123</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24813149</pub-id></mixed-citation></ref><ref id="b30"><mixed-citation publication-type="journal"><name><surname>Clark</surname><given-names>A.</given-names></name>
<article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title>. <source>Behav. Brain Sci.</source>
<volume>36</volume>, <fpage>181</fpage>&#x02013;<lpage>204</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23663408</pub-id></mixed-citation></ref><ref id="b31"><mixed-citation publication-type="journal"><name><surname>Summerfield</surname><given-names>C.</given-names></name> &#x00026; <name><surname>Egner</surname><given-names>T.</given-names></name>
<article-title>Expectation (and attention) in visual cognition</article-title>. <source>Trends Cogn. Sci.</source>
<volume>13</volume>, <fpage>403</fpage>&#x02013;<lpage>409</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19716752</pub-id></mixed-citation></ref><ref id="b32"><mixed-citation publication-type="journal"><name><surname>Vuust</surname><given-names>P.</given-names></name> &#x00026; <name><surname>Witek</surname><given-names>M. A. G.</given-names></name>
<article-title>Rhythmic complexity and predictive coding: a novel approach to modeling rhythm and meter perception in music</article-title>. <source>Audit. Cogn. Neurosci.</source>
<volume>5</volume>, <fpage>1111</fpage> (<year>2014</year>).</mixed-citation></ref><ref id="b33"><mixed-citation publication-type="journal"><name><surname>Maes</surname><given-names>P.-J.</given-names></name>
<article-title>Sensorimotor Grounding of Musical Embodiment and the Role of Prediction: A Review</article-title>. <source>Front. Psychol.</source>
<volume>308</volume> doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2016.00308</pub-id> (<year>2016</year>).</mixed-citation></ref><ref id="b34"><mixed-citation publication-type="journal"><name><surname>Schaefer</surname><given-names>R. S.</given-names></name>, <name><surname>Overy</surname><given-names>K.</given-names></name> &#x00026; <name><surname>Nelson</surname><given-names>P.</given-names></name>
<article-title>Affect and non-uniform characteristics of predictive processing in musical behaviour</article-title>. <source>Behav. Brain Sci.</source>
<volume>36</volume>, <fpage>226</fpage>&#x02013;<lpage>227</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23663552</pub-id></mixed-citation></ref><ref id="b35"><mixed-citation publication-type="journal"><name><surname>Kuchenbuch</surname><given-names>A.</given-names></name>, <name><surname>Paraskevopoulos</surname><given-names>E.</given-names></name>, <name><surname>Herholz</surname><given-names>S. C.</given-names></name> &#x00026; <name><surname>Pantev</surname><given-names>C.</given-names></name>
<article-title>Effects of musical training and event probabilities on encoding of complex tone patterns</article-title>. <source>BMC Neurosci.</source>
<volume>14</volume>, <fpage>51</fpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23617597</pub-id></mixed-citation></ref><ref id="b36"><mixed-citation publication-type="journal"><name><surname>Oechslin</surname><given-names>M. S.</given-names></name>, <name><surname>Van De Ville</surname><given-names>D.</given-names></name>, <name><surname>Lazeyras</surname><given-names>F.</given-names></name>, <name><surname>Hauert</surname><given-names>C.-A.</given-names></name> &#x00026; <name><surname>James</surname><given-names>C. E.</given-names></name>
<article-title>Degree of Musical Expertise Modulates Higher Order Brain Functioning</article-title>. <source>Cereb. Cortex</source>
<volume>23</volume>, <fpage>2213</fpage>&#x02013;<lpage>2224</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">22832388</pub-id></mixed-citation></ref><ref id="b37"><mixed-citation publication-type="journal"><name><surname>Vuust</surname><given-names>P.</given-names></name>, <name><surname>Ostergaard</surname><given-names>L.</given-names></name>, <name><surname>Pallesen</surname><given-names>K. J.</given-names></name>, <name><surname>Bailey</surname><given-names>C.</given-names></name> &#x00026; <name><surname>Roepstorff</surname><given-names>A.</given-names></name>
<article-title>Predictive coding of music &#x02013; Brain responses to rhythmic incongruity</article-title>. <source>Cortex</source>
<volume>45</volume>, <fpage>80</fpage>&#x02013;<lpage>92</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19054506</pub-id></mixed-citation></ref><ref id="b38"><mixed-citation publication-type="journal"><name><surname>Maidhof</surname><given-names>C.</given-names></name>, <name><surname>Rieger</surname><given-names>M.</given-names></name>, <name><surname>Prinz</surname><given-names>W.</given-names></name> &#x00026; <name><surname>Koelsch</surname><given-names>S.</given-names></name>
<article-title>Nobody Is Perfect: ERP Effects Prior to Performance Errors in Musicians Indicate Fast Monitoring Processes</article-title>. <source>PLOS ONE</source>
<volume>4</volume>, <fpage>e5032</fpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19337379</pub-id></mixed-citation></ref><ref id="b39"><mixed-citation publication-type="journal"><name><surname>Wilcox</surname><given-names>R. R.</given-names></name> &#x00026; <name><surname>Keselman</surname><given-names>H. J.</given-names></name>
<article-title>Modern robust data analysis methods: measures of central tendency</article-title>. <source>Psychol. Methods</source>
<volume>8</volume>, <fpage>254</fpage>&#x02013;<lpage>274</lpage> (<year>2003</year>).<pub-id pub-id-type="pmid">14596490</pub-id></mixed-citation></ref><ref id="b40"><mixed-citation publication-type="journal"><name><surname>Fischer</surname><given-names>R.</given-names></name>, <name><surname>Dreisbach</surname><given-names>G.</given-names></name> &#x00026; <name><surname>Goschke</surname><given-names>T.</given-names></name>
<article-title>Context-sensitive adjustments of cognitive control: conflict-adaptation effects are modulated by processing demands of the ongoing task</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
<volume>34</volume>, <fpage>712</fpage>&#x02013;<lpage>718</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18444768</pub-id></mixed-citation></ref><ref id="b41"><mixed-citation publication-type="journal"><name><surname>Schmidhuber</surname><given-names>J.</given-names></name> In <source>Anticipatory Behavior in Adaptive Learning Systems</source> (eds. <name><surname>Pezzulo</surname><given-names>G.</given-names></name>, <name><surname>Butz</surname><given-names>M. V.</given-names></name>, <name><surname>Sigaud</surname><given-names>O.</given-names></name> &#x00026; <name><surname>Baldassarre</surname><given-names>G.</given-names></name>) <fpage>48</fpage>&#x02013;<lpage>76</lpage> (Springer Berlin Heidelberg, <year>2009</year>).</mixed-citation></ref><ref id="b42"><mixed-citation publication-type="journal"><name><surname>Patel</surname><given-names>A. D.</given-names></name>
<article-title>The OPERA hypothesis: assumptions and clarifications</article-title>. <source>Ann. N. Y. Acad. Sci.</source>
<volume>1252</volume>, <fpage>124</fpage>&#x02013;<lpage>128</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">22524349</pub-id></mixed-citation></ref><ref id="b43"><mixed-citation publication-type="journal"><name><surname>Patel</surname><given-names>A. D.</given-names></name>
<article-title>Why would Musical Training Benefit the Neural Encoding of Speech? The OPERA Hypothesis</article-title>. <source>Front. Psychol.</source>
<volume>2</volume>, <fpage>142</fpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21747773</pub-id></mixed-citation></ref><ref id="b44"><mixed-citation publication-type="journal"><name><surname>Cameron</surname><given-names>D. J.</given-names></name> &#x00026; <name><surname>Grahn</surname><given-names>J. A.</given-names></name>
<article-title>Neuroscientific investigations of musical rhytm</article-title>. <source>Acoust. Aust.</source>
<volume>42</volume>, <fpage>111</fpage> (<year>2014</year>).</mixed-citation></ref><ref id="b45"><mixed-citation publication-type="journal"><name><surname>O&#x02019;Doherty</surname><given-names>J. P.</given-names></name>, <name><surname>Dayan</surname><given-names>P.</given-names></name>, <name><surname>Friston</surname><given-names>K.</given-names></name>, <name><surname>Critchley</surname><given-names>H.</given-names></name> &#x00026; <name><surname>Dolan</surname><given-names>R. J.</given-names></name>
<article-title>Temporal difference models and reward-related learning in the human brain</article-title>. <source>Neuron</source>
<volume>38</volume>, <fpage>329</fpage>&#x02013;<lpage>337</lpage> (<year>2003</year>).<pub-id pub-id-type="pmid">12718865</pub-id></mixed-citation></ref><ref id="b46"><mixed-citation publication-type="journal"><name><surname>Wacongne</surname><given-names>C.</given-names></name>, <name><surname>Changeux</surname><given-names>J.-P.</given-names></name> &#x00026; <name><surname>Dehaene</surname><given-names>S.</given-names></name>
<article-title>A neuronal model of predictive coding accounting for the mismatch negativity</article-title>. <source>J. Neurosci.</source>
<volume>32</volume>, <fpage>3665</fpage>&#x02013;<lpage>3678</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">22423089</pub-id></mixed-citation></ref><ref id="b47"><mixed-citation publication-type="journal"><name><surname>Daneman</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Carpenter</surname><given-names>P. A.</given-names></name>
<article-title>Individual differences in working memory and reading</article-title>. <source>J. Verbal Learn. Verbal Behav.</source>
<volume>19</volume>, <fpage>450</fpage>&#x02013;<lpage>466</lpage> (<year>1980</year>).</mixed-citation></ref><ref id="b48"><mixed-citation publication-type="journal"><name><surname>Corsi</surname><given-names>P.</given-names></name>
<source>Human memory and the medial temporal region of the brain.</source> (McGill University, <year>1972</year>).</mixed-citation></ref></ref-list><fn-group><fn fn-type="COI-statement"><p>The authors declare no competing financial interests.</p></fn><fn><p><bold>Author Contributions</bold> All authors developed the study concept and contributed to the experimental design. E.V., K.K. and T.V. drafted the manuscript. J.L. conducted data collection. E.V. and J.L. analyzed the data. All authors approved the final version of the manuscript for submission.</p></fn></fn-group></back><floats-group><fig id="f1"><label>Figure 1</label><caption><title>Group &#x000d7; compatibility frequency &#x000d7; compatibility interaction.</title><p>(<bold>a</bold>) Average RTs for musicians in compatible (C) and incompatible (IC) trials, as a function of compatibility frequency in the block (50/50, 80/20). The error bars represent one standard error of the mean. (<bold>b</bold>) Average RTs for controls in compatible (C) and incompatible (IC) trials, as a function of compatibility frequency in the block (50/50, 80/20). The error bars represent one standard error of the mean.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep25225-f1"/></fig><fig id="f2"><label>Figure 2</label><caption><title>Task structure.</title><p>(<bold>a</bold>) Task timing with an example of the two unimodal trial types: auditory-auditory (AA) with auditory cue and auditory target (tones); visual-visual (VV) with a visual cue (arrow) and visual target (X). (<bold>b</bold>) Example of the two cross-modal trial types: auditory-visual (AV) with auditory cue (tone) and visual target (X); visual-auditory (VA) with visual cue (arrow) and auditory target (tone). (<bold>c</bold>) Cue-target combinations and compatibility for AA and AV trials. From left to right: Auditory cues (low tone 800&#x02009;hz, high tone 1600&#x02009;hz); compatible (C) and incompatible (IC) auditory targets (AA trial); compatible (C) and incompatible (C) visual targets (AV trial). (<bold>d</bold>) Cue-target combinations and compatibility for VV and VA trials. From left to right: Visual cues (left or right pointing arrow); compatible (C) and incompatible (C) visual targets (VV trial); compatible (C) and incompatible (C) auditory targets (VA trial).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep25225-f2"/></fig></floats-group></article>