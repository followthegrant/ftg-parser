<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Int J Comput Assist Radiol Surg</journal-id><journal-id journal-id-type="iso-abbrev">Int J Comput Assist Radiol Surg</journal-id><journal-title-group><journal-title>International Journal of Computer Assisted Radiology and Surgery</journal-title></journal-title-group><issn pub-type="ppub">1861-6410</issn><issn pub-type="epub">1861-6429</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30159833</article-id><article-id pub-id-type="pmc">6223755</article-id><article-id pub-id-type="publisher-id">1835</article-id><article-id pub-id-type="doi">10.1007/s11548-018-1835-2</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Classification of lung adenocarcinoma transcriptome subtypes from pathological images using deep convolutional networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Antonio</surname><given-names>Victor Andrew A.</given-names></name><address><email>victor.antonio.uv9@is.naist.jp</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7722-055X</contrib-id><name><surname>Ono</surname><given-names>Naoaki</given-names></name><address><email>nono@is.naist.jp</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Saito</surname><given-names>Akira</given-names></name><address><email>asaitou-tky@umin.ac.jp</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Sato</surname><given-names>Tetsuo</given-names></name><address><email>tsato@is.naist.jp</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Altaf-Ul-Amin</surname><given-names>Md.</given-names></name><address><email>amin-m@is.naist.jp</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kanaya</surname><given-names>Shigehiko</given-names></name><address><email>skanaya@gtc.naist.jp</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9227 2257</institution-id><institution-id institution-id-type="GRID">grid.260493.a</institution-id><institution>Graduate School of Science and Technology, </institution><institution>Nara Institute of Science and Technology, </institution></institution-wrap>Ikoma, Japan </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9227 2257</institution-id><institution-id institution-id-type="GRID">grid.260493.a</institution-id><institution>Data Science Center, </institution><institution>Nara Institute of Science and Technology, </institution></institution-wrap>Ikoma, Japan </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2151 536X</institution-id><institution-id institution-id-type="GRID">grid.26999.3d</institution-id><institution>Division for Health Service Promotion, </institution><institution>University of Tokyo, </institution></institution-wrap>Tokyo, Japan </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9227 2257</institution-id><institution-id institution-id-type="GRID">grid.260493.a</institution-id><institution>Graduate School of information Science, </institution><institution>Nara Institute of Science and Technology, </institution></institution-wrap>Ikoma, Japan </aff></contrib-group><pub-date pub-type="epub"><day>29</day><month>8</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>8</month><year>2018</year></pub-date><pub-date pub-type="ppub"><year>2018</year></pub-date><volume>13</volume><issue>12</issue><fpage>1905</fpage><lpage>1913</lpage><history><date date-type="received"><day>10</day><month>3</month><year>2018</year></date><date date-type="accepted"><day>23</day><month>7</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><sec><title>Purpose</title><p id="Par1">Convolutional neural networks have become rapidly popular for image recognition and image analysis because of its powerful potential. In this paper, we developed a method for classifying subtypes of lung adenocarcinoma from pathological images using neural network whose that can evaluate phenotypic features from wider area to consider cellular distributions.</p></sec><sec><title>Methods</title><p id="Par2">In order to recognize the types of tumors, we need not only to detail features of cells, but also to incorporate statistical distribution of the different types of cells. Variants of autoencoders as building blocks of pre-trained convolutional layers of neural networks are implemented. A sparse deep autoencoder which minimizes local information entropy on the encoding layer is then proposed and applied to images of size <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048\times 2048$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq1.gif"/></alternatives></inline-formula>. We applied this model for feature extraction from pathological images of lung adenocarcinoma, which is comprised of three transcriptome subtypes previously defined by the Cancer Genome Atlas network. Since the tumor tissue is composed of heterogeneous cell populations, recognition of tumor transcriptome subtypes requires more information than local pattern of cells. The parameters extracted using this approach will then be used in multiple reduction stages to perform classification on larger images.</p></sec><sec><title>Results</title><p id="Par3">We were able to demonstrate that these networks successfully recognize morphological features of lung adenocarcinoma. We also performed classification and reconstruction experiments to compare the outputs of the variants. The results showed that the larger input image that covers a certain area of the tissue is required to recognize transcriptome subtypes. The sparse autoencoder network with <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048 \times 2048$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq2.gif"/></alternatives></inline-formula> input provides a 98.9% classification accuracy.</p></sec><sec><title>Conclusion</title><p id="Par4">This study shows the potential of autoencoders as a feature extraction paradigm and paves the way for a whole slide image analysis tool to predict molecular subtypes of tumors from pathological features.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Deep learning</kwd><kwd>Lung cancer</kwd><kwd>Computer-aided diagnosis</kwd><kwd>Autoencoder</kwd><kwd>Independent subspace analysis</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>15H01123</award-id><principal-award-recipient><name><surname>Ono</surname><given-names>Naoaki</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; CARS 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par6">Recent rapid development of machine learning algorithms brings us a wide range of applications for image recognition and classification. In particular, a significant advancement of visual recognition using deep learning architectures has been shown by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)&#x000a0;[<xref ref-type="bibr" rid="CR16">16</xref>], which has served as a testbed for a few generations of large-scale image classification systems. A convolutional neural network (CNN) provides a promising architecture that can extract features from given images automatically, optimize the manifold of image space, and show great success in image classification and medical image analysis&#x000a0;[<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. In this study, we propose an application of CNNs for feature extraction and classification of lung adenocarcinoma pathological images and use the learned features for classification of large image data.</p><p id="Par7">These approaches use small images as input, usually less than 300&#x000a0;px by 300&#x000a0;px. However, whole slide images gathered by the Cancer Genome Atlas (TCGA) network are of a much larger magnitude. This paper presents an approach that transfers information learned from small input images to larger input data. By applying unsupervised learning through autoencoders, we will be able to extract features that are not heavily reliant on classification information&#x000a0;[<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR7">7</xref>].</p><p id="Par8">Specifically, this project has two goals. First, it aims to compare the performance of different network architectures in reconstruction and visualization tasks. To do this, we conduct experiments using several types of networks on lung cancer images gathered from the Cancer Genome Atlas database (<ext-link ext-link-type="uri" xlink:href="http://cancergenome.nih.gov">http://cancergenome.nih.gov</ext-link>) as an example training database. Second, we developed the extended network to process much larger input of pathological images, in order to evaluate not only local phenotypic features but also their distribution in the tissue, in order to apply the deep convolutional autoencoders, for the classification of lung adenocarcinoma images into their transcriptome subtypes, with the understanding that modifying existing machine learning methods to target specific image sets can optimize the precision and accuracy of the analysis.</p></sec><sec id="Sec2"><title>Review of previous work</title><sec id="Sec3"><title>Classification of lung adenocarcinoma transcriptome subtypes</title><p id="Par9">Lung cancer is the leading cause of cancer-related mortality, and adenocarcinoma is its most common histological subtype&#x000a0;[<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. The overall prognosis for lung cancer remains poor, despite recent advances in molecular targeted therapies. Several cancer genome projects have analyzed cohorts of lung cancer patients and revealed genome and transcriptome alterations. Most recently, the Cancer Genome Atlas (TCGA) has described the comprehensive genomic landscape of lung adenocarcinoma in a large cohort&#x000a0;[<xref ref-type="bibr" rid="CR18">18</xref>]. These studies not only elucidated oncogenic mechanisms but also shed light on previously unappreciated heterogeneity of gene expression profiles. As a consequence of genomic alterations and gene mutations in cancer cells, aberrant patterns of gene expression profiles occur, which eventually determine cancer cell behaviors. The pathological images from resection are paired with transcriptome data from DNA microarray for each patient. In line with this, it is worth noting that the aforementioned TCGA study has identified three transcriptome subtypes of lung adenocarcinoma: the terminal respiratory unit (TRU, formerly bronchioid), the proximal-proliferative (PP, formerly magnoid), and the proximal-inflammatory (PI, formerly squamoid) transcriptional subtypes&#x000a0;[<xref ref-type="bibr" rid="CR19">19</xref>]. It has been further demonstrated that this classification is associated with clinical features and gene mutation profiles. In terms of morphological features, lung adenocarcinomas display high inter-individual and intra-tumoral heterogeneity. However, it remains undetermined whether the transcriptome subtypes are associated with distinctive patterns of pathological findings. If it is the case, image analyses on biopsy of resected tissue samples will be helpful to infer transcriptional changes in tumor tissues, which can assist precise diagnosis and clinical decision making. In this study, we propose a model to classify three lung adenocarcinoma transcriptome subtypes from their pathological images using a deep learning approach. Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> shows a sample of each of the three subtypes alongside four different samples of normal images.<fig id="Fig1"><label>Fig. 1</label><caption><p>Pathological images of three lung adenocarcinoma subtypes</p></caption><graphic xlink:href="11548_2018_1835_Fig1_HTML" id="MO16"/></fig>
</p></sec><sec id="Sec4"><title>Image processing via CNNs</title><p id="Par10">Machine learning continues to be a vital innovation used in several fields. From a biology standpoint, it can be used for gene expression interpolation and classification of several datasets. Moreover, it has been an important instrument for image classification and inference in recent years. On the other hand, image classification and analysis has been an important achievement of computational systems in recent times, and in fact, it is still a growing, revolutionary field. Specifically, being able to perform analysis on pathological images proves to be vital for medicine and bioinformatics. Image processing methods using deep neural networks are currently developing very rapidly. However, those approaches mainly target general image analysis such as photo-classification and face recognition. On the other hand, an analysis of biomedical images requires a more specific viewpoint focusing extensively on their biological features&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>].</p><p id="Par11">This study relies on an investigation conducted by Masci et al.&#x000a0;[<xref ref-type="bibr" rid="CR13">13</xref>] to determine the ability of standard CNNs to extract features using an unsupervised learning method. They used the MNIST dataset, which has been the standard for this type of testing. We see in this work the ability of pre-training a network for reconstruction to still be capable of performing classification. They were able to extract features from unlabeled data, which when combined with backpropagation algorithms can still become efficient classifiers.</p><p id="Par12">We also constructed a deep learning model of the sparse autoencoder (SAE) for the differentiation of the distinct types of lung adenocarcinoma from pathological images&#x000a0;[<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR21">21</xref>].</p><p id="Par13">Finally, we see from&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>] a specific type of CNN using the notion of a reconstruction independent subspace analysis (RISA). This is an unsupervised learning method for the reconstruction of images emphasizing the invariance between the extracted features, which means that neighboring filters are designed to share the same property. They were able to show that these invariant features are vital in the classification of images if we attach a supervised layer to the pre-trained RISA network.</p><p id="Par14">Since part of our goal is an understanding of the internal architectures of several CNN variations, we also look at&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>]. Their work provides a number of visualization experiments for this purpose, and we can follow a similar approach for our data.</p><p id="Par15">Finally, machine learning and computer vision has provided us powerful methods to improve accuracy and efficiency of image classification. These methods rely upon manually curated image features to characterize specific features of tumors. However, recent development of approaches like deep neural networks allows us to extract image features from given data automatically, without using handmade features. Using pre-trained neural networks, we can extract features of tumors and distinguish them according to their shapes. However, when we address the classification of adenocarcinoma subtypes, local features of cell shapes are not enough to describe the variation and distribution of various cells in the tissue&#x000a0;[<xref ref-type="bibr" rid="CR10">10</xref>]. In this paper, we propose variations of CNNs that uses multiple reduction layers in order to evaluate a large area of pathological images and classify lung adenocarcinoma subtypes.</p></sec></sec><sec id="Sec5"><title>Model</title><sec id="Sec6"><title>Autoencoders</title><p id="Par16">An autoencoder is an unsupervised machine learning architecture that extracts characteristic features from given inputs by learning a network which reproduces input data from those features. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> shows the basic design of the autoencoder used in our model. The input data are scanned by a convolutional filter and down-sampled by a max-pooling layer then passed on to an encoding layer. The output here can then be used to generate the input data using the reversed network. The total network is optimized to minimize the difference between input and output data.<fig id="Fig2"><label>Fig. 2</label><caption><p>Autoencoder model based on a convolutional neural network</p></caption><graphic xlink:href="11548_2018_1835_Fig2_HTML" id="MO17"/></fig>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Pipelines for classifier variants. Conv.: convolution layer with <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5 \times 5$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq3.gif"/></alternatives></inline-formula> filters. Pool.: pooling layers with <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2 \times 2$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq4.gif"/></alternatives></inline-formula> max pooling. Dense: fully connected layers. Unpool: unpooling by copying to <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2 \times 2$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq5.gif"/></alternatives></inline-formula> pixels. Deconv.: deconvolution with the same size of filters</p></caption><graphic xlink:href="11548_2018_1835_Fig3_HTML" id="MO18"/></fig>
</p><p id="Par17">To enhance the efficiency of feature extraction and information compression in the autoencoder, we introduced a sparsity penalty. We compute information entropy of the output of the encoding layer and add the penalty for the optimization function (<italic>L</italic>) to minimize the effect of overfitting. The optimization function is defined as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L=R+\lambda _{s}S, \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>S</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11548_2018_1835_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R=\sum _{i}^{N}\left( x_{i}^{\text {output}}-x_{i}^{\text {input}}\right) ^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mtext>output</mml:mtext></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mtext>input</mml:mtext></mml:msubsup></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11548_2018_1835_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>and<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S=\sum {k=1}^n\sum _{j=1}^{M}\left( -r_{j}^{\text {encode}}\log r_{j}^{\text {encode}}\right) . \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02211;</mml:mo><mml:msup><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mfenced close=")" open="(" separators=""><mml:mo>-</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mtext>encode</mml:mtext></mml:msubsup><mml:mo>log</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mtext>encode</mml:mtext></mml:msubsup></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11548_2018_1835_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>Here, <inline-formula id="IEq6"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_{j}^{\text {encode}}$$\end{document}</tex-math><mml:math id="M18"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mtext>encode</mml:mtext></mml:msubsup></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq6.gif"/></alternatives></inline-formula> is the output intensity of filter <italic>j</italic> in the encoding layer relative to their total summation. <italic>N</italic> and <italic>M</italic> are the numbers of nodes in the input and encoding layers, respectively, and <inline-formula id="IEq7"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{s}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq7.gif"/></alternatives></inline-formula> is a weight constant.</p><p id="Par18">Stacked autoencoders allow us to extract more complex image features with higher-order structures, while some detail information will be lost in down-sampling. It is worth noting that stacked autoencoders can be trained independently. That is, the network of the first autoencoder can be fixed after training and left aside when we train the network for the second optimizer. This reduces the number of trainable parameters and required computation.</p></sec><sec id="Sec7"><title>Classifier variants</title><p id="Par19">In the first part of this study, we implemented three types of classifiers and compared their corresponding results. These networks can be distinguished based on how the convolutional filters will be learned and extracted.</p><p id="Par20">We call the first network a direct classifier, and it is described by a convolution network attached to a softmax classification layer. The features will be extracted according to optimal classification. Softmax cross-entropy will be used as a loss function. The subsequent networks are pre-trained autoencoder CNNs. The final layer of these networks will be attached to a softmax classification layer, and its features will be extracted similar to the direct classifier.</p><p id="Par21">Particularly, the second network is a pre-trained autoencoder whose features are extracted following the reconstruction paradigm <italic>R</italic> from Eq.&#x000a0;(<xref rid="Equ2" ref-type="">2</xref>).</p><p id="Par22">On the other hand, the third network is a pre-trained reconstruction independent subspace analysis (RISA) network. It is a two-layer autoencoder variant composed of convolution and pooling layers. The main distinction of a RISA network is that it emphasizes minimal translational invariance&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>]. If we denote the learned matrix from the convolutional layer as <italic>C</italic>, and the fixed matrix from the pooling layer as <italic>H</italic>, then for an input vector <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {x}$$\end{document}</tex-math><mml:math id="M22"><mml:mi mathvariant="bold">x</mml:mi></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq8.gif"/></alternatives></inline-formula>, the second layer output is<disp-formula id="Equ4"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} p_{i}\left( \mathbf {x};C,H\right) =\sqrt{\sum _{m=1}^{k}H_{im}\left( \sum _{j=1}^{n}C_{mj}\mathbf {x}_{j}\right) ^{2}}. \end{aligned}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x0037e;</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi mathvariant="italic">im</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">mj</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11548_2018_1835_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>The features extracted from a RISA network will be learned through the following heuristic:<disp-formula id="Equ5"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \arg \min _{C}\sum _{t=1}^{N}\left( \frac{1}{N}\left\| CC^{\mathrm {T}}{\mathbf {x}}^{\left( t\right) }-{\mathbf {x}}^{\left( t\right) }\right\| ^{2}+\lambda \sum _{i=1}^{k}p_{i}\left( {\mathbf {x}}^{\left( t\right) };C,H\right) \right) , \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>arg</mml:mo><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mi>C</mml:mi></mml:munder><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mfenced close="&#x02225;" open="&#x02225;" separators=""><mml:mi>C</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:msup></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:msup><mml:mo>&#x0037e;</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11548_2018_1835_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ \mathbf {x}^{\left( t\right) }\right\} _{t=1}^{N}$$\end{document}</tex-math><mml:math id="M28"><mml:msubsup><mml:mfenced close="}" open="{" separators=""><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:msup></mml:mfenced><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq9.gif"/></alternatives></inline-formula> is the input dataset and <inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M30"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq10.gif"/></alternatives></inline-formula> a weight constant.</p><p id="Par23">This rule extracts features less expensively than manually designed feature extraction methods.</p><p id="Par24">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> shows a summary for the different pipelines for the three variants. Here, the softmax classifier takes logistic outputs.</p></sec><sec id="Sec8"><title>Toward classification of larger images</title><p id="Par25">In the second part of this study, we constructed a model based on three autoencoders and one classification reducer that takes logistic outputs. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> shows the structure of the network. <inline-formula id="IEq11"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048\,\hbox {px} \times 2048\,\hbox {px}$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mn>2048</mml:mn><mml:mspace width="0.166667em"/><mml:mtext>px</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mn>2048</mml:mn><mml:mspace width="0.166667em"/><mml:mtext>px</mml:mtext></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq11.gif"/></alternatives></inline-formula> slices from the pathological images were used as input for the first autoencoder. For an initial feature extraction, we first pre-train three stages of convolutional autoencoder. The output from the encoding layer of the third autoencoder is passed to the reduction classifier. Since the size of the third encoding layer is still large, we divided it into <inline-formula id="IEq12"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$16 \times 16$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mn>16</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq12.gif"/></alternatives></inline-formula> subpanes, and in each subpane, the input from the encoding layer is reduced to 24 output nodes through fully connected networks. Note that all the subpanes share the same reduction network; in other words, it is also a convolution without overlap between windows. Finally, the output of the reduction layer is reduced again into three nodes which represent the three classes of lung adenocarcinoma subtypes. Using multiple reduction layers, we can evaluate larger pathological images in order to recognize the features based from cell distribution in the cancer tumor and classify the transcriptome subtypes. The network in this model is composed of 11 layers and 97,227 nodes in total. We implemented these networks based on python using TensorFlow&#x000a0;[<xref ref-type="bibr" rid="CR1">1</xref>] libraries, which provides various basic functions for neural networks and machine learning algorithms. We constructed and trained our network from scratch instead of applying transfer learning since the features of pathological images are not consistent with general image recognition. This time we incorporate the sparsity penalty as described in Eq.&#x000a0;(<xref rid="Equ3" ref-type="">3</xref>) to extract features and Adam algorithm for optimization.<fig id="Fig4"><label>Fig. 4</label><caption><p>Structure of the whole network</p></caption><graphic xlink:href="11548_2018_1835_Fig4_HTML" id="MO19"/></fig>
</p><p id="Par26">The actual dataset is composed of pathological images of lung adenocarcinoma from the Cancer Genome Atlas (TCGA)&#x000a0;[<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. There are 409 whole slides from 230 cancer patients which are classified into three transcriptome subtypes according to their gene expression patterns. The original pathological slide images have quite high resolution of over 20,000&#x02013;40,000 pixels, whose actual sizes are approximately 1&#x02013;2&#x000a0;<inline-formula id="IEq13"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {cm}^{2}$$\end{document}</tex-math><mml:math id="M36"><mml:msup><mml:mtext>cm</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq13.gif"/></alternatives></inline-formula>. We randomly clipped the original images into slices of <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048\times 2048$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq14.gif"/></alternatives></inline-formula> image size and obtained 106,505 slices (TRU:43122, PP:27047, PI:36336) as the input data for our models.</p></sec></sec><sec id="Sec9"><title>Results</title><sec id="Sec10"><title>Visualization of filters</title><p id="Par27">First we look at the results of the reconstruction algorithm. While the actual slides are paired with their respective transcriptome subtypes, we use the unlabeled tiles for the autoencoder and apply the labeling for the classifier. Now, we trained three stages of autoencoder as pre-training. Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows an example of the output of the first stage of the autoencoder. The original images here come from the general collection of images.<fig id="Fig5"><label>Fig. 5</label><caption><p>Example of the output of the autoencoder</p></caption><graphic xlink:href="11548_2018_1835_Fig5_HTML" id="MO20"/></fig>
</p><p id="Par28">We now look into some of the activations of the autoencoder. Though some color hue changed after reconstruction, the structural detail of the original input was recovered from compressed information of encoded layers whose resolution is one fourth of the original image, as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>Left: input image, a sample of TRU subtype. Right: output of some encoding layers in the second autoencoder. The gradient from red to blue represents increase in signal intensity</p></caption><graphic xlink:href="11548_2018_1835_Fig6_HTML" id="MO21"/></fig>
</p><p id="Par29">In order to understand how the network extracts features after training, we randomly clipped the original images to generate 10,000 sample input of size <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$32 \times 32$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq15.gif"/></alternatives></inline-formula> pixels. Then, we computed the output of the encoding layer and sorted them according to the value of one node in the encoding layer of the third stage. The goal of Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> is to emphasize a specific feature extracted by the autoencoder. We take the average of the pixel intensities of the top 100 encoded images based on the sorted feature activation. A sample image is then obtained representing the activation in one of the encoding layers. This represents a feature of the training image patterns. It seems that they represent different local structures of cell boundaries such as stripe- or target-like patterns.<fig id="Fig7"><label>Fig. 7</label><caption><p>Examples of optimized local image for encoded outputs</p></caption><graphic xlink:href="11548_2018_1835_Fig7_HTML" id="MO22"/></fig>
</p></sec><sec id="Sec11"><title>Internetwork comparison</title><p id="Par30">We want to determine whether the convolutional filter size has an effect on the reconstruction outputs and the classification accuracy of the networks.</p><p id="Par31">For the following experiments, we used <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$64\times 64$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mn>64</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq16.gif"/></alternatives></inline-formula> images as input, and the networks follow the pipeline described in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. First, we take a look at the reconstruction. Here we vary the convolution filter size on a standard convolutional autoencoder and a RISA network.<fig id="Fig8"><label>Fig. 8</label><caption><p>Comparison of AE and RISA training</p></caption><graphic xlink:href="11548_2018_1835_Fig8_HTML" id="MO23"/></fig>
</p><p id="Par32">The results are shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>. We take the natural logarithm of the reconstruction error over the number of training steps. Here, <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3\times 3, 4\times 4$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq17.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5\times 5$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq18.gif"/></alternatives></inline-formula> are the convolutional window sizes. It can be observed that performance does not vary significantly as we change the filter size. However, it can be seen, especially in the RISA network experiment, that a slight increase in reconstruction performance is brought about by a decrease in the filter size. This implies that a smaller receptive field works better for this type of task.</p><p id="Par33">Next, we performed a comparison between the activated filters of each of the networks that we are working on. We take the activations of the first layer of the direct classifier, the first stage of the autoencoder, and the lone convolutional layer of the RISA network. The goal here is to determine and hopefully interpret the features extracted from each of the networks.<fig id="Fig9"><label>Fig. 9</label><caption><p>Comparison between filters and output of networks. The upper image is an original sample from PP subtype. The middle row shows outputs of some feature filters. The lower row shows the reconstructed images</p></caption><graphic xlink:href="11548_2018_1835_Fig9_HTML" id="MO24"/></fig>
</p><p id="Par34">In Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>, we can observe several differences in the types of filters extracted. We observe that the output for the direct classifier shows some edge detection scheme through the contours in some of the filters. On the other hand, the standard autoencoder seems to emphasize shape and hue. The RISA network shows features similar to the standard autoencoder, but we also observe that some of them have paired up as part of the underlying architecture of RISA. (Note that the RISA filters were scaled to match the filters of the other networks.)</p><p id="Par35">In the interest of finding the most accurate implementation of the convolutional classifier, we continue the experiment of varying the size of the input along with the convolutional filter size of the network variants. Specifically, we incorporate <inline-formula id="IEq19"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$32 \times 32, 64 \times 64$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>64</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq19.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq20"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$128 \times 128$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq20.gif"/></alternatives></inline-formula> experiments. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> summarizes the accuracy of the different convolution models. In this table, we can see that, in general, there is some slight improvement in performance when we increase the filter window size.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Subtype classification accuracy tables for varying networks and filter sizes <inline-formula id="IEq21"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( \#\right) $$\end{document}</tex-math><mml:math id="M52"><mml:mfenced close=")" open="("><mml:mo>#</mml:mo></mml:mfenced></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq21.gif"/></alternatives></inline-formula>&#x02014;umber of test images</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Window size</th><th align="left">Direct class.</th><th align="left">AE&#x000a0;<inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+$$\end{document}</tex-math><mml:math id="M54"><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq22.gif"/></alternatives></inline-formula>&#x000a0;class.</th><th align="left">RISA&#x000a0;<inline-formula id="IEq23"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+$$\end{document}</tex-math><mml:math id="M56"><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq23.gif"/></alternatives></inline-formula>&#x000a0;class.</th></tr></thead><tbody><tr><td align="left" colspan="4"><inline-formula id="IEq24"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$32\times 32$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq24.gif"/></alternatives></inline-formula> (12,000)</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq25"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${3\times 3}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq25.gif"/></alternatives></inline-formula></td><td align="left">73.6</td><td align="left">76.4</td><td align="left">52.5</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq26"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${4\times 4}$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq26.gif"/></alternatives></inline-formula></td><td align="left">74.1</td><td align="left">65.9</td><td align="left">50.9</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq27"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${5\times 5}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq27.gif"/></alternatives></inline-formula></td><td align="left">80.5</td><td align="left">68.8</td><td align="left">71.3</td></tr><tr><td align="left" colspan="4"><inline-formula id="IEq28"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$64\times 64$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mn>64</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq28.gif"/></alternatives></inline-formula> (3000)</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq29"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${3\times 3}$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq29.gif"/></alternatives></inline-formula></td><td align="left">82.9</td><td align="left">74.7</td><td align="left">89.2</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq30"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${4\times 4}$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq30.gif"/></alternatives></inline-formula></td><td align="left">87.8</td><td align="left">79.5</td><td align="left">62.5</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq31"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${5\times 5}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq31.gif"/></alternatives></inline-formula></td><td align="left">89.0</td><td align="left">82.2</td><td align="left">71.2</td></tr><tr><td align="left" colspan="4"><inline-formula id="IEq32"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$128\times 128$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq32.gif"/></alternatives></inline-formula> (750)</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq33"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${3\times 3}$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq33.gif"/></alternatives></inline-formula></td><td align="left">68.4</td><td align="left">73.3</td><td align="left">56.7</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq34"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${4\times 4}$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq34.gif"/></alternatives></inline-formula></td><td align="left">86.4</td><td align="left">74.3</td><td align="left">35.9</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;<inline-formula id="IEq35"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${5\times 5}$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq35.gif"/></alternatives></inline-formula></td><td align="left">89.1</td><td align="left">54.9</td><td align="left">72.1</td></tr></tbody></table></table-wrap>
</p><p id="Par36">However, if we look at the accuracies of the RISA network, we see a different result. This can be attributed to the fact that as we increase the filter size, we have a relatively significant drop in reconstruction performance. It must be said that the effect does not seem to be drastic for the standard AE.</p></sec><sec id="Sec12"><title>Deeper networks</title><p id="Par37">Using a pre-trained three-stage sparse autoencoder network, we trained to classify the transcriptome subtypes. First we confirmed the effect of block reduction. We evaluated the accuracy of the network by changing the input image size. This time, we used <inline-formula id="IEq36"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$128 \times {} 128, 512 \times {} 512$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>512</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>512</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq36.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq37"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048 \times {} 2048$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>2048</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq37.gif"/></alternatives></inline-formula> images as input. From the results described in the previous section, we see that there is some advantage to altering the filter size of the autoencoder. As such, we use <inline-formula id="IEq38"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$7\times {}7, 5\times {}5$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq38.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq39"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3\times {}3$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq39.gif"/></alternatives></inline-formula> for the filter size of the three stages of the autoencoder and <inline-formula id="IEq40"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$16\times {}16$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mn>16</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq40.gif"/></alternatives></inline-formula> for the classifier.</p><p id="Par38">To actually perform the classification on the <inline-formula id="IEq41"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048\times {}2048$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>2048</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq41.gif"/></alternatives></inline-formula> images, we first divide them into smaller tiles on which to apply the pre-trained convolutional autoencoder. We then concatenate the output of the final stage of the autoencoder and use it as input for the convolutional classifier.</p><p id="Par39">Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> shows that when the input size was small, the network could not learn the difference between transcriptome subtypes very well. But as we increase the input size, more information is being read by the network, and hence, more complex features are extracted. Accordingly, the accuracy increases. It is worth noting that the number of nodes was not changed for the three experiments.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Confusion matrices and accuracy for 128&#x000a0;px, 512&#x000a0;px, and 2048&#x000a0;px experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Subtype</th><th align="left" colspan="5">Prediction</th></tr><tr><th align="left">TRU</th><th align="left">PP</th><th align="left">PI</th><th align="left">Total</th><th align="left">Accuracy (%)</th></tr></thead><tbody><tr><td align="left" colspan="6">
<italic>Diagnosis</italic>
</td></tr><tr><td align="left" colspan="6">128&#x000a0;px</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;TRU</td><td align="left">47</td><td align="left">32</td><td align="left">1</td><td align="left">80</td><td align="left">58.8</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;PP</td><td align="left">20</td><td align="left">64</td><td align="left">11</td><td align="left">95</td><td align="left">67.4</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;PI</td><td align="left">16</td><td align="left">21</td><td align="left">44</td><td align="left">81</td><td align="left">54.3</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Total</td><td align="left">83</td><td align="left">117</td><td align="left">56</td><td align="left">256</td><td align="left">60.5</td></tr><tr><td align="left" colspan="6">512&#x000a0;px</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;TRU</td><td align="left">54</td><td align="left">32</td><td align="left">0</td><td align="left">86</td><td align="left">62.8</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;PP</td><td align="left">15</td><td align="left">48</td><td align="left">15</td><td align="left">78</td><td align="left">61.5</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;PI</td><td align="left">17</td><td align="left">16</td><td align="left">59</td><td align="left">92</td><td align="left">64.1</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Total</td><td align="left">86</td><td align="left">96</td><td align="left">74</td><td align="left">256</td><td align="left">62.9</td></tr><tr><td align="left" colspan="6">2048&#x000a0;px</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;TRU</td><td align="left">60</td><td align="left">0</td><td align="left">0</td><td align="left">60</td><td align="left">100.0</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;PP</td><td align="left">0</td><td align="left">49</td><td align="left">1</td><td align="left">50</td><td align="left">98.0</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;PI</td><td align="left">1</td><td align="left">0</td><td align="left">65</td><td align="left">66</td><td align="left">98.5</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Total</td><td align="left">61</td><td align="left">49</td><td align="left">66</td><td align="left">256</td><td align="left">98.9</td></tr></tbody></table></table-wrap>
</p></sec></sec><sec id="Sec13"><title>Discussion and conclusions</title><p id="Par40">We aimed to implement models involving CNNs for the reconstruction and classification of lung adenocarcinoma transcriptome subtypes. The experiments using different input sizes indicate that the network requires a certain numbers of cells in the input images to recognize difference between transcriptome subtypes.</p><p id="Par41">Looking at the differences of the convolutional filter output of each of the networks, we can see the features emphasized by the three variants. The convolutional network classifier outperforms the other two networks, and it can be seen that the important features have something to do with some combination of edge and hue detection. On the other hand, the autoencoder network emphasizes hue above all else. A deeper analysis of these filters is worth pursuing. Moreover, the pre-training implemented on the autoencoder-classifier networks provides several advantages like lower computational cost without a drastic effect on accuracy.</p><p id="Par42">Using the pre-trained autoencoder as a feature extraction mechanism for a convolutional classifier and tiling the <inline-formula id="IEq42"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2048\times {}2048$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow/><mml:mn>2048</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11548_2018_1835_Article_IEq42.gif"/></alternatives></inline-formula> images into individual and independent tiles paved the way for a classification algorithm involving large image input, having a 98.89% test accuracy. Even though they belong to different clusters in gene expression profiles, it was difficult to distinguish them from their morphological phenotypes since their local cell structures were not so different. In order to distinguish statistical distribution of cellular features in larger tissue images, we introduced multiple reduction layers and succeeded to classify transcriptome subtypes correctly.</p><p id="Par43">This new approach will be helpful for differentiation of various tissue types, not clearly different in cell morphology, but different in cellular distribution in the tissue. This result will help the diagnosis of lung cancer for appropriate treatment, and further applications will provide us useful tools for diagnosis of various tumor types.</p></sec></body><back><ack><title>Acknowledgements</title><p>The results published here are wholly based upon data generated by the TCGA Research Network.</p></ack><notes notes-type="funding-information"><title>Funding</title><p>This work was supported by JSPS Grant-in-Aid for Scientific Research on Innovative Areas (Multidisciplinary Computational Anatomy) JSPS KAKENHI Grant No. 15H01123 and No. 17H05297.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Conflict of interest</title><p>The authors declare that they have no conflict of interest.</p></sec><sec id="FPar2"><title>Ethical approval</title><p>This article does not contain any studies with human participants performed by any of the authors.</p></sec><sec id="FPar3"><title>Informed consent</title><p>This articles does not contain patient data, except that is already published by the TCGA Research Network.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado G S, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Mane D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Viegas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zhe X (2016) Tensorflow: large-scale machine learning on heterogeneous distributed systems. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.04467">arXiv:1603.04467</ext-link></mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arevalo</surname><given-names>J</given-names></name><name><surname>Cruz-Roa</surname><given-names>A</given-names></name><name><surname>Arias</surname><given-names>V</given-names></name><name><surname>Romero</surname><given-names>E</given-names></name><name><surname>Gonz&#x000e1;lez</surname><given-names>FA</given-names></name></person-group><article-title>An unsupervised feature learning framework for basal cell carcinoma image analysis</article-title><source>Artif Intell Med</source><year>2013</year><volume>64</volume><issue>2</issue><fpage>131</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.artmed.2015.04.004</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>Hang</given-names></name><name><surname>Nayak</surname><given-names>Nandita</given-names></name><name><surname>Spellman</surname><given-names>Paul T.</given-names></name><name><surname>Parvin</surname><given-names>Bahram</given-names></name></person-group><article-title>Characterization of Tissue Histopathology via Predictive Sparse Decomposition and Spatial Pyramid Matching</article-title><source>Advanced Information Systems Engineering</source><year>2013</year><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer Berlin Heidelberg</publisher-name><fpage>91</fpage><lpage>98</lpage></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>JTH</given-names></name><name><surname>Lee</surname><given-names>YM</given-names></name><name><surname>Huang</surname><given-names>RS</given-names></name></person-group><article-title>The impact of the Cancer Genome Atlas on lung cancer</article-title><source>Transl Res</source><year>2015</year><volume>166</volume><issue>6</issue><fpage>568</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1016/j.trsl.2015.08.001</pub-id><pub-id pub-id-type="pmid">26318634</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name></person-group><article-title>Deep similarity learning for multimodal medical images</article-title><source>Comput Methods Biomech Biomed Eng Imaging Vis</source><year>2018</year><volume>6</volume><issue>3</issue><fpage>248</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1080/21681163.2015.1135299</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>K</given-names></name><name><surname>Vendt</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Freymann</surname><given-names>J</given-names></name><name><surname>Kirby</surname><given-names>J</given-names></name><name><surname>Koppel</surname><given-names>P</given-names></name><name><surname>Moore</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>S</given-names></name><name><surname>Maffitt</surname><given-names>D</given-names></name><name><surname>Pringle</surname><given-names>M</given-names></name><name><surname>Tarbox</surname><given-names>L</given-names></name><name><surname>Prior</surname><given-names>F</given-names></name></person-group><article-title>The Cncer Imaging Archive (TCIA): maintaining and operating a public information repository</article-title><source>J Digit Imaging</source><year>2013</year><volume>26</volume><issue>60</issue><fpage>1045</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.1007/s10278-013-9622-7</pub-id><pub-id pub-id-type="pmid">23884657</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furusho</surname><given-names>Y</given-names></name><name><surname>Kubo</surname><given-names>T</given-names></name><name><surname>Ikeda</surname><given-names>K</given-names></name></person-group><article-title>Roles of pre-training in deep neural networks from information theoretical perspective</article-title><source>Neurocomputing</source><year>2017</year><volume>248</volume><fpage>76</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2016.12.083</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Roberts</surname><given-names>P</given-names></name><name><surname>Bell</surname><given-names>MB</given-names></name><name><surname>Thorne</surname><given-names>L</given-names></name><name><surname>Schallheim</surname><given-names>J</given-names></name><name><surname>Bernard</surname><given-names>P</given-names></name><name><surname>Funkhouser</surname><given-names>B</given-names></name></person-group><article-title>Subtypes of lung adenocarcinoma derived from gene expression patterns are recapitulated using a tissue microarray system and immunohistochemistry</article-title><source>Cancer Res</source><year>2007</year><volume>67</volume><issue>9</issue><fpage>188</fpage><lpage>188</lpage></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>DN</given-names></name><name><surname>Monti</surname><given-names>S</given-names></name><name><surname>Parmigiani</surname><given-names>G</given-names></name><name><surname>Gilks</surname><given-names>CB</given-names></name><name><surname>Naoki</surname><given-names>K</given-names></name><name><surname>Bhattacharjee</surname><given-names>A</given-names></name><name><surname>Socinski</surname><given-names>MA</given-names></name><name><surname>Perou</surname><given-names>C</given-names></name><name><surname>Meyerson</surname><given-names>M</given-names></name></person-group><article-title>Gene expression profiling reveals reproducible human lung adenocarcinoma subtypes in multiple independent patient cohorts</article-title><source>J Clin Oncol</source><year>2006</year><volume>24</volume><issue>31</issue><fpage>5079</fpage><lpage>5090</lpage><pub-id pub-id-type="doi">10.1200/JCO.2005.05.1748</pub-id><pub-id pub-id-type="pmid">17075127</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kothari</surname><given-names>S</given-names></name><name><surname>Phan</surname><given-names>JH</given-names></name><name><surname>Stokes</surname><given-names>TH</given-names></name><name><surname>Wang</surname><given-names>MD</given-names></name></person-group><article-title>Pathology imaging informatics for quantitative analysis of whole-slide images</article-title><source>J Am Med Inform Assoc</source><year>2013</year><volume>20</volume><issue>6</issue><fpage>1099</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1136/amiajnl-2012-001540</pub-id><pub-id pub-id-type="pmid">23959844</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Le Q, Han J, Gray J, Spellman P, Borowsky A, Parvin B (2012) Learning invariant features of tumor signatures. In: 2012 9th IEEE international symposium on biomedical imaging (ISBI), pp 302&#x02013;305</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malon</surname><given-names>C</given-names></name><name><surname>Cosatto</surname><given-names>E</given-names></name></person-group><article-title>Classification of mitotic figures with convolutional neural networks and seeded blob features</article-title><source>J Pathol Inform</source><year>2013</year><volume>4</volume><fpage>9</fpage><pub-id pub-id-type="doi">10.4103/2153-3539.112694</pub-id><pub-id pub-id-type="pmid">23858384</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Masci</surname><given-names>Jonathan</given-names></name><name><surname>Meier</surname><given-names>Ueli</given-names></name><name><surname>Cire&#x0015f;an</surname><given-names>Dan</given-names></name><name><surname>Schmidhuber</surname><given-names>J&#x000fc;rgen</given-names></name></person-group><article-title>Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction</article-title><source>Lecture Notes in Computer Science</source><year>2011</year><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer Berlin Heidelberg</publisher-name><fpage>52</fpage><lpage>59</lpage></element-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Nayak N, Chang H, Borowsky A, Spellman P, Parvin B (2013) Classification of tumor histopathology via sparse feature learning. In: 2013 IEEE 10th international symposium on biomedical imaging, pp 410&#x02013;413</mixed-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>HR</given-names></name><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Seff</surname><given-names>A</given-names></name><name><surname>Cherry</surname><given-names>K</given-names></name><name><surname>Kim</surname><given-names>L</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name></person-group><article-title>Improving computer-aided detection using convolutional neural networks and random view aggregation</article-title><source>IEEE Trans Med Imaging</source><year>2016</year><volume>35</volume><issue>5</issue><fpage>1170</fpage><lpage>1181</lpage><pub-id pub-id-type="doi">10.1109/TMI.2015.2482920</pub-id><pub-id pub-id-type="pmid">26441412</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><article-title>ImageNet large scale visual recognition challenge</article-title><source>Comput Sci</source><year>2015</year><volume>115</volume><issue>3</issue><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>HC</given-names></name><name><surname>Roth</surname><given-names>HR</given-names></name><name><surname>Gao</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Nogues</surname><given-names>I</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Mollura</surname><given-names>D</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name></person-group><article-title>Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</article-title><source>IEEE Trans Med Imaging</source><year>2016</year><volume>35</volume><issue>5</issue><fpage>1285</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1109/TMI.2016.2528162</pub-id><pub-id pub-id-type="pmid">26886976</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>The Cancer Genome Atlas Research Network</collab></person-group><article-title>Comprehensive molecular profiling of lung adenocarcinoma</article-title><source>Nature</source><year>2014</year><volume>511</volume><fpage>543</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.1038/nature13385</pub-id><pub-id pub-id-type="pmid">25079552</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkerson</surname><given-names>MD</given-names></name><name><surname>Yin</surname><given-names>X</given-names></name><name><surname>Walter</surname><given-names>V</given-names></name><name><surname>Zhao</surname><given-names>N</given-names></name><name><surname>Cabanski</surname><given-names>CR</given-names></name><name><surname>Hayward</surname><given-names>MC</given-names></name><name><surname>Miller</surname><given-names>CR</given-names></name><name><surname>Socinski</surname><given-names>MA</given-names></name><name><surname>Parsons</surname><given-names>AM</given-names></name><name><surname>Thorne</surname><given-names>LB</given-names></name><name><surname>Haithcock</surname><given-names>BE</given-names></name><name><surname>Veeramachaneni</surname><given-names>NK</given-names></name><name><surname>Funkhouser</surname><given-names>WK</given-names></name><name><surname>Randell</surname><given-names>SH</given-names></name><name><surname>Bernard</surname><given-names>PS</given-names></name><name><surname>Perou</surname><given-names>CM</given-names></name><name><surname>Hayes</surname><given-names>DN</given-names></name></person-group><article-title>Differential pathogenesis of lung adenocarcinoma subtypes involving sequence mutations, copy number, chromosomal instability, and methylation</article-title><source>PLoS ONE</source><year>2012</year><volume>7</volume><issue>5</issue><fpage>e36530</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0036530</pub-id><pub-id pub-id-type="pmid">22590557</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Xu Y, Mo T, Feng Q, Zhong P, Lai M, Chang EIC (2014) Deep learning of feature representation with multiple instance learning for medical image analysis. In: 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp 1626&#x02013;1630</mixed-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Xiang</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Gilmore</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>J</given-names></name><name><surname>Madabhushi</surname><given-names>A</given-names></name></person-group><article-title>Stacked sparse autoencoder (SSAE) for nuclei detection on breast cancer histopathology images</article-title><source>IEEE Trans Med Imaging</source><year>2016</year><volume>35</volume><issue>1</issue><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1109/TMI.2015.2458702</pub-id><pub-id pub-id-type="pmid">26208307</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Zeiler MD, Fergus R (2014) Visualizing and understanding convolutional networks. In: Fleet D, Pajdla T, Schiele B, Tuytelaars T (eds) Computer vision (ECCV 2014), Lecture notes in computer science. Springer, vol 8689, pp 818&#x02013;833</mixed-citation></ref></ref-list></back></article>