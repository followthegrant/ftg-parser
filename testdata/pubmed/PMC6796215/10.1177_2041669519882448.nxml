<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Iperception</journal-id><journal-id journal-id-type="iso-abbrev">Iperception</journal-id><journal-id journal-id-type="publisher-id">IPE</journal-id><journal-id journal-id-type="hwp">spipe</journal-id><journal-title-group><journal-title>i-Perception</journal-title></journal-title-group><issn pub-type="epub">2041-6695</issn><publisher><publisher-name>SAGE Publications</publisher-name><publisher-loc>Sage UK: London, England</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31662838</article-id><article-id pub-id-type="pmc">6796215</article-id><article-id pub-id-type="doi">10.1177/2041669519882448</article-id><article-id pub-id-type="publisher-id">10.1177_2041669519882448</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Virtual Walking Sensation by Prerecorded Oscillating Optic Flow and
Synchronous Foot Vibration</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0966-4842</contrib-id><name><surname>Kitazaki</surname><given-names>Michiteru</given-names></name><xref ref-type="corresp" rid="corresp1-2041669519882448"/><aff id="aff1-2041669519882448">Department of Computer Science and Engineering,
Toyohashi University of Technology, Japan</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Hamada</surname><given-names>Takeo</given-names></name><aff id="aff2-2041669519882448">Interfaculty Initiative in Information Studies,
The
University of Tokyo, Japan</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Yoshiho</surname><given-names>Katsuya</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kondo</surname><given-names>Ryota</given-names></name><aff id="aff3-2041669519882448">Department of Computer Science and Engineering,
Toyohashi University of Technology, Japan</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7079-9167</contrib-id><name><surname>Amemiya</surname><given-names>Tomohiro</given-names></name><aff id="aff4-2041669519882448">The Graduate School of Information Science and
Technology,
The
University of Tokyo, Japan</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Hirota</surname><given-names>Koichi</given-names></name><aff id="aff5-2041669519882448">The
University of
Electro-Communications, Tokyo,
Japan</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Ikei</surname><given-names>Yasushi</given-names></name></contrib><aff id="aff6-2041669519882448">Tokyo
Metropolitan University, Japan</aff></contrib-group><author-notes><corresp id="corresp1-2041669519882448">Michiteru Kitazaki, Department of Computer
Science and Engineering, Toyohashi University of Technology, 1-1 Hibarigaoka,
Tempaku-cho, Toyohashi, Aichi 441-8580, Japan. Email:
<email>mich@cs.tut.ac.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>15</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><season>Sep-Oct</season><year>2019</year></pub-date><volume>10</volume><issue>5</issue><elocation-id>2041669519882448</elocation-id><history><date date-type="received"><day>19</day><month>8</month><year>2019</year></date><date date-type="accepted"><day>23</day><month>9</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019</copyright-statement><copyright-year>2019</copyright-year><copyright-holder content-type="society">SAGE Publications Ltd. Manuscript content
on this site is licensed under Creative Commons Licenses</copyright-holder><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>Creative Commons CC BY: This article is distributed under the terms
of the Creative Commons Attribution 4.0 License (<ext-link ext-link-type="uri" xlink:href="http://www.creativecommons.org/licenses/by/4.0/">http://www.creativecommons.org/licenses/by/4.0/</ext-link>) which
permits any use, reproduction and distribution of the work without further
permission provided the original work is attributed as specified on the SAGE
and Open Access pages (<ext-link ext-link-type="uri" xlink:href="https://us.sagepub.com/en-us/nam/open-access-at-sage">https://us.sagepub.com/en-us/nam/open-access-at-sage</ext-link>).</license-p></license></permissions><abstract abstract-type="short"><p>This article reports the first psychological evidence that the combination of
oscillating optic flow and synchronous foot vibration evokes a walking
sensation. In this study, we first captured a walker&#x02019;s first-person-view scenes
with footstep timings. Participants observed the naturally oscillating scenes on
a head-mounted display with vibrations on their feet and rated walking-related
sensations using a Visual Analogue Scale. They perceived stronger sensations of
self-motion, walking, leg action, and telepresence from the oscillating visual
flow with foot vibrations than with randomized-timing vibrations or without
vibrations. The artificial delay of foot vibrations with respect to the scenes
diminished the walking-related sensations. These results suggest that the
oscillating visual scenes and synchronous foot vibrations are effective for
creating virtual walking sensations.</p></abstract><kwd-group><kwd>optic flow</kwd><kwd>self-motion</kwd><kwd>vection</kwd><kwd>walking</kwd><kwd>jitter</kwd><kwd>tactile vibration</kwd></kwd-group><funding-group><award-group id="award1-2041669519882448"><funding-source id="funding1-2041669519882448"><institution-wrap><institution>Japan Society for the Promotion of Science</institution><institution-id institution-id-type="FundRef">http://doi.org/10.13039/501100001691</institution-id></institution-wrap></funding-source><award-id rid="funding1-2041669519882448">Grant-in-Aid for Challenging
Exploratory Research </award-id><award-id rid="funding1-2041669519882448">Grant-in-Aid for Scientific Research
(A) (18H04118</award-id></award-group><award-group id="award2-2041669519882448"><funding-source id="funding2-2041669519882448"><institution-wrap><institution>MIC Japan</institution><institution-id institution-id-type="FundRef"/></institution-wrap></funding-source><award-id rid="funding2-2041669519882448">SCOPE program (141203019)</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>cover-date</meta-name><meta-value>September-October 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-2041669519882448"><title>Introduction</title><p>Walking is a natural and frequent action performed by healthy adults in everyday
life. It involves various sensations as well as motor commands and actions. During
walking, a person moves their legs and arms and strikes the ground with their feet.
At the same time, they perceive vestibular sensations and proprioception, observe
visual motion flow, hear changing sounds, feel airflows on the skin, experience
smell, and receive tactile sensations on the feet. We are motivated to develop a
virtual reality (VR) system that can present experiences of walking to persons who
are at a distance or have a disability that prevents them from walking. The virtual
walking system would enable people to walk on strange places such as the moon or the
ocean floor and improve the quality of life of people who have walking disabilities
in future. As the first step for it, we aim to create a virtual sensation of walking
using limited modalities such as vision and tactile sensations.</p><p>Visual motion flow or optic flow is one of the most extensively studied stimuli for
investigating self-motion. Optic flow contains information of self-motion as well as
object and environment motions and structures (<xref rid="bibr5-2041669519882448" ref-type="bibr">Banton, Stefanucci, Durgin, Fass, &#x00026; Proffitt,
2005</xref>; <xref rid="bibr14-2041669519882448" ref-type="bibr">Gibson, 1966, 1968</xref>;
<xref rid="bibr25-2041669519882448" ref-type="bibr">Kitazaki &#x00026; Shimojo,
1998</xref>; <xref rid="bibr29-2041669519882448" ref-type="bibr">Nakayama,
1985</xref>). Vection can be defined as a visually induced illusory self-motion
perception. It is an important component of the walking sensation. The definition of
vection has been comprehensively discussed and updated by <xref rid="bibr37-2041669519882448" ref-type="bibr">Palmisano, Allison, Schira, and Barry
(2015)</xref>. Definitions of vection are categorized into four groups: (a)
visual illusion of self-motion in a stationary observer, (b) modality-independent
illusion of self-motion, (c) visually mediated perception of self-motion in either
reality or illusion, and (d) real or illusory conscious subjective experience of
self-motion. The first one is the narrowest category, while the last one is the
broadest. The feeling of self-motion during real walking is included in the fourth
definition. In this study, we strived to utilize vection in terms of the first
definition (visually induced self-motion illusion) and the second definition
(self-motion illusion from vision and tactile sensation on feet) to make a virtual
walking system for stationary observers.</p><p>Vection is dominated by background motion (<xref rid="bibr8-2041669519882448" ref-type="bibr">Brandt, Wist, &#x00026; Dichgans, 1975</xref>; <xref rid="bibr30-2041669519882448" ref-type="bibr">Ohmi, Howard, &#x00026; Landolt,
1987</xref>) and nonattended motions (<xref rid="bibr24-2041669519882448" ref-type="bibr">Kitazaki &#x00026; Sato, 2003</xref>), and it is
enhanced by enlarging the field of view (<xref rid="bibr12-2041669519882448" ref-type="bibr">Dichgans &#x00026; Brandt, 1978</xref>), binocular
stereopsis (<xref rid="bibr1-2041669519882448" ref-type="bibr">Allison, Ash, &#x00026;
Palmisano, 2014</xref>; <xref rid="bibr33-2041669519882448" ref-type="bibr">Palmisano, 1996,
2002</xref>), and adding perspective jitter on the radial optic flow (<xref rid="bibr35-2041669519882448" ref-type="bibr">Palmisano, Allison, Ash, Nakamura,
&#x00026; Apthorp, 2014</xref>; <xref rid="bibr36-2041669519882448" ref-type="bibr">Palmisano, Allison, Kim, &#x00026; Bonato, 2011</xref>; <xref rid="bibr38-2041669519882448" ref-type="bibr">Palmisano, Burke, &#x00026; Allison, 2003</xref>;
<xref rid="bibr39-2041669519882448" ref-type="bibr">Palmisano, Gillam, &#x00026;
Blackburn, 2000</xref>). This perspective jitter is similar to the oscillation
of the visual scene during actual walking. The sensations of walking and vection are
improved by adding oscillating patterns of optic flow based on motions of the
walker&#x02019;s head and eye motions (<xref rid="bibr9-2041669519882448" ref-type="bibr">Bubka &#x00026; Bonato, 2010</xref>; <xref rid="bibr27-2041669519882448" ref-type="bibr">L&#x000e9;cuyer, Burkhardt, Henaff, &#x00026; Donikian,
2006</xref>). However, the added jitter is not required to be realistic for
enhancing vection (<xref rid="bibr35-2041669519882448" ref-type="bibr">Palmisano
et&#x000a0;al., 2014</xref>). Vection is inhibited during walking on a treadmill (<xref rid="bibr3-2041669519882448" ref-type="bibr">Ash, Palmisano, Apthorp, &#x00026;
Allison, 2013</xref>; <xref rid="bibr31-2041669519882448" ref-type="bibr">Onimaru, Sato, &#x00026; Kitazaki, 2010</xref>). Contrary to these studies, a study
reported that forward vection was enhanced by forward walking (<xref rid="bibr44-2041669519882448" ref-type="bibr">Seno, Ito, &#x00026; Sunaga, 2011</xref>). In the
study, the speed of optic flow (57.6 km/hour) and the speed of treadmill (2 km/hour)
were very different, although the speed of optic flow was matched or similar to the
speed of treadmill in the other studies. A simulated viewpoint jitter enhances
vection even during walking (<xref rid="bibr3-2041669519882448" ref-type="bibr">Ash
et&#x000a0;al., 2013</xref>). Thus, we predicted that the oscillation of a visual scene
simulating the eye and head motion would contribute to the sensation of walking.</p><p>In VR research, various systems have been developed for presenting the sensation of
walking. Omnidirectional treadmills enable users to walk in any direction in one
place (<xref rid="bibr20-2041669519882448" ref-type="bibr">Iwata, 1999</xref>),
while leg-support actuator systems enable users to walk and navigate up or down
stairs (<xref rid="bibr21-2041669519882448" ref-type="bibr">Iwata, Yano, &#x00026;
Nakaizumi, 2001</xref>). These VR systems focus on leg movements and the motor
commands required to walk in the real world. By combining the systems with a
display, such as a head-mounted display (HMD) or a large projection screen, walking
experiences have been created in VR studies. Based on the first vection study using
a new-generation HMD that has a large visual field (&#x0003e;90&#x000b0;), very fast sampling of
head motion (1 kHz), and immediately synchronized visual updating, it was reported
that visual compensation with head motion improved the vection sensation (<xref rid="bibr22-2041669519882448" ref-type="bibr">Kim, Chung, Nakamura, Palmisano,
&#x00026; Khuu, 2015</xref>).</p><p>Rhythmic stimulation to the feet may induce spinal central pattern generators to
produce an active walking sensation, which is expected to contribute to walking
rehabilitation (<xref rid="bibr11-2041669519882448" ref-type="bibr">Ch&#x000e9;ron et&#x000a0;al.,
2012</xref>; <xref rid="bibr16-2041669519882448" ref-type="bibr">Gravano et&#x000a0;al.,
2011</xref>). A VR system was developed by utilizing rhythmic stimulations on
the feet and small movements of the feet, legs, and trunk enforced by actuators with
multisensory presentations of airflow, smell, changing sounds, and three-dimensional
video images (<xref rid="bibr18-2041669519882448" ref-type="bibr">Ikei, Abe, Hirota,
&#x00026; Amemiya, 2012</xref>; <xref rid="bibr19-2041669519882448" ref-type="bibr">Ikei et&#x000a0;al., 2015</xref>). However, there is no psychological evidence on the
strength of sensory perception of walking by users and the critical factors
affecting the walking sensations.</p><p>In this study, we strived to identify the critical parameters for enabling stationary
observers to experience virtual walking without leg action. We focused on tactile
sensations on the feet and oscillating or jittering optic flow. We developed a VR
system with a large-field-of-view HMD and the ability to produce four-channel
vibrations on the forefeet and heels of both feet. In the experiments, we captured
actual walking scenes with footstep timings and measured the psychological responses
to walking-related sensations. The visual oscillation caused by the walker&#x02019;s actual
head motion was included in the stimuli, and the image had a binocular disparity.
However, the visual compensation of the observer&#x02019;s head motion was not implemented
so that they could not gaze around the scene.</p></sec><sec id="sec2-2041669519882448"><title>Experiment 1</title><sec id="sec3-2041669519882448"><title>Methods</title><sec id="sec4-2041669519882448"><title>Participants</title><p>Fifteen undergraduate and graduate students (all males, mean age of 21.35
years, &#x000b1;0.88 standard deviation) participated in Experiment 1. All
participants provided written informed consent and had normal or
corrected-to-normal vision. The methods of the experiment and all
experimental protocols were approved by the Ethical Committee for
Human-Subject Research at the Toyohashi University of Technology. The
experiments were strictly conducted in accordance with the approved
guidelines of the committee and the Code of Ethics of the World Medical
Association (Declaration of Helsinki).</p></sec><sec id="sec5-2041669519882448"><title>Stimuli and apparatus</title><p>Two cameras (GoPro HERO 4 Session, 2,560 [height]&#x02009;&#x000d7;&#x02009;1,440 [width] pixels,
122.6&#x000b0;&#x02009;&#x000d7;&#x02009;94.4&#x000b0;, 30 fps, 65-mm intercamera distance (<xref rid="bibr13-2041669519882448" ref-type="bibr">Dodgson, 2004</xref>); <xref ref-type="fig" rid="fig1-2041669519882448">Figure 1</xref>, top-left)
were mounted on the forehead to capture binocular-stereo first-person-view
optic flow. Four small condenser microphones (SP Limited, XCM6035) were
embedded in the soles of a pair of shoes to obtain footstep timings (left
and right heels and forefeet; <xref ref-type="fig" rid="fig3-2041669519882448">Figure 3</xref>, top-right). Two walkers
wore these cameras (camera viewpoint height: 169.1 and 172.0 cm) and shoes
and walked at three different locations (a corridor in a school building, a
lobby in a school building, and an outdoor paved road in the university
campus; <xref ref-type="fig" rid="fig3-2041669519882448">Figure 3</xref>,
bottom). These locations were familiar to the participants. To exclude the
possibility of artifacts caused by a specific scene or situation, we used
three different scenes. To exclude the possibility of artifacts caused by a
specific walker or walking movement, we used two different walkers. Walkers
stomped at one place for four steps while observing their feet at the
beginning, after which they gazed forward and walked straight.</p><fig id="fig1-2041669519882448" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>Stereo camera device for capturing stereo motion images (top-left). A
pair of shoes with microphones for capturing timings of footsteps
(top-right). Three locations where the walking scenes were captured
(bottom).</p></caption><graphic xlink:href="10.1177_2041669519882448-fig1"/></fig><p>The timings of heels and forefeet strikes on the ground were extracted from
the sounds by applying a high-pass filter at 2.1 kHz and visual and hearing
inspections. Walkers were asked to walk at 2 steps/second after training.
Foot vibrations were produced by applying a low-pass filter at 240 Hz to the
sounds of real footsteps on a paved road surface. Vibrations were 200-ms
long and different for the forefoot and heel (<xref ref-type="fig" rid="fig2-2041669519882448">Figure 2</xref>, left). Stereo motion images
were presented on an HMD (Oculus Rift DK2, 960 [width]&#x02009;&#x000d7;&#x02009;1,080 [height]
pixels, 90&#x000b0;&#x02009;&#x000d7;&#x02009;110&#x000b0;, refresh rate of 60 Hz). Captured images were
appropriately trimmed and formatted for the HMD to exclude visual
discrepancies. Vibrations (200-ms duration) were presented on the heels and
forefeet of the observer at the actual timings of foot strikes (Vibrotactile
device Acouve Lab Vp408; <xref ref-type="fig" rid="fig2-2041669519882448">Figure 2</xref>, right). A computer (Intel Core i7-4790 CPU @ 3.60 GHz,
NVidia GeForce GTX 745) controlled the visual stimuli on the HMD and the
tactile stimuli on the vibrotactile devices. Vibrations were presented on
the vibrotactile devices by inputting sound signals from a power amplifier
(Behringer EPQ450, 4 0W (8&#x003a9;)&#x02009;&#x000d7;&#x02009;4 ch) through a USB multichannel preamplifier
(Behringer FCA1616, input 16 ch, output 16 ch) controlled by the computer.
The vibrations to the heel and the forefoot were presented at the timing
extracted from the actual walking.</p><fig id="fig2-2041669519882448" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Profiles of presented vibrations to heel and forefoot (left).
Experimental apparatus for tactile stimuli (right).</p></caption><graphic xlink:href="10.1177_2041669519882448-fig2"/></fig></sec><sec id="sec6-2041669519882448"><title>Design</title><p>Experiment 1 contained three repetitions of all combinations of three
vibration conditions (synchronous, random, and no vibration), two walkers,
and three locations (54 trials in total). The frequency of random vibrations
was identical to that of synchronous vibrations; however, its presentation
timings were randomized.</p></sec><sec id="sec7-2041669519882448"><title>Procedure</title><p>Participants observed each stimulus for 20 seconds, after which they were
asked to rate the sensation strengths of (a) self-motion (vection), (b)
walking, (c) leg action (footstep), and (d) telepresence by using Visual
Analogue Scale (VAS). We explained these sensations to participants as
follows. If the participants feel as if they were passively moving, it is a
self-motion sensation. If the participants feel as if they were walking, it
is a walking sensation. If the participants feel as if they were stamping or
stepping on the ground, it is a leg-action sensation. If the participants
feel as if they were physically present in the visual scenes, it is
telepresence. Although presence has different definitions (<xref rid="bibr45-2041669519882448" ref-type="bibr">Skarbez, Brooks, &#x00026;
Whitton, 2017</xref>), in this study, we use the term
<italic>telepresence</italic> in the sense of spatial presence at the
place in video images. They were seated during all the experiments and asked
not to move their body or head for all the trials. In all the experiments,
before the actual trials, they experienced several trials as a practice
session in which a different scene was used.</p><p>Four sentences regarding these sensations were presented on the screen after
each stimulus presentation: Question 1: I felt that my whole body was moving
forward; Question 2: I felt like I was walking forward; Question 3: I felt
like my feet were striking the ground; and Question 4: I felt like I was
actually there in the scene. The order of questions was constant though all
trials in the experiment. Lines and cursors for the VAS ratings were placed
on the right side of each question (<xref ref-type="fig" rid="fig3-2041669519882448">Figure 3</xref>). The data were converted
into a numerical scale ranging from 0 to 100. Participants were informed
that the left end (0) meant <italic>no sensation</italic> and the right end
(100) meant the <italic>same sensation</italic> as in actual walking. They
had adequate time to judge all questions without time limitations. During
experiments, noise-canceling headphones (Bose Quiet Comfort 2) were used to
present white noise (70 dBA) to prevent participants from hearing the
vibrotactile devices. The participants&#x02019; heads were not strictly fixed. The
participants were instructed not to move their heads but to observe the
stimuli from a relaxed state.</p><fig id="fig3-2041669519882448" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>An example of a screen for VAS ratings.</p></caption><graphic xlink:href="10.1177_2041669519882448-fig3"/></fig></sec></sec><sec id="sec8-2041669519882448"><title>Results</title><p>After observing dynamic scenes while receiving vibrations on the feet,
participants rated the strength of perceived self-motion, walking sensation,
leg-action sensation, and telepresence using a VAS. Self-motion refers to the
sense of passive motion similar to vection. If the participants felt as if they
were walking, they experienced a walking sensation. If the participants felt as
if they were stamping or stepping on the ground, they experienced a leg-action
sensation. If the participants felt as if they were physically present in the
visual scenes, they experienced telepresence.</p><p>Self-motion, walking sensation, leg-action sensation, and telepresence were all
rated significantly higher by observing a walker&#x02019;s first-person-view scenes that
included the actual oscillation or jittering of the walker&#x02019;s head position with
synchronized vibrations on the feet (heels and forefeet) than with
randomized-timing vibrations or without vibrations (<xref ref-type="fig" rid="fig4-2041669519882448">Figure 4</xref>). We conducted three-way
repeated-measure analyses of variance with vibration conditions (synchronous,
random, and no vibrations), scenes (three different locations), and walkers (two
persons with 169.1 cm and 172.0 cm heights) as the factors using digitized VAS
data (0&#x02013;100; 0&#x02009;=&#x02009;<italic>no sense</italic>, 100&#x02009;=&#x02009;<italic>identical to actual
walking in the real world</italic>) for self-motion, walking sensation,
leg-action sensation, and telepresence.</p><fig id="fig4-2041669519882448" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Results of Experiment 1. Averaged VAS ratings among participants are
plotted, and vertical error bars indicate the standard error of the
mean. VAS&#x02009;=&#x02009;Visual Analogue Scale.</p></caption><graphic xlink:href="10.1177_2041669519882448-fig4"/></fig><p>All the main effects of the vibration conditions showed statistical significance,
self-motion: <italic>F</italic>(2, 28)&#x02009;=&#x02009;16.701, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001,
<inline-formula id="ilm1-2041669519882448"><mml:math id="mml-math1-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.544; walking sensation: <italic>F</italic>(2, 28)&#x02009;=&#x02009;51.771,
<italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <inline-formula id="ilm2-2041669519882448"><mml:math id="mml-math2-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.787; leg-action sensation: <italic>F</italic>(2,
28)&#x02009;=&#x02009;80.906, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <inline-formula id="ilm3-2041669519882448"><mml:math id="mml-math3-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.852; and telepresence: <italic>F</italic>(2, 28)&#x02009;=&#x02009;20.523,
<italic>p</italic> &#x0003c;.001, <inline-formula id="ilm4-2041669519882448"><mml:math id="mml-math4-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.594. Multiple comparisons showed that the synchronized foot
vibrations elicited stronger sensations of self-motion, walking, leg action, and
telepresence than the randomized-vibration or no-vibration conditions (Shaffer&#x02019;s
<italic>F</italic>-modified sequentially rejective Bonferroni procedure
<italic>p</italic>s&#x02009;&#x0003c;&#x02009;.05). There was no difference between the
randomized-vibration and no-vibration conditions. Thus, the combination of
oscillating optic flow with synchronized vibrations or vibrations at the actual
timings on the feet was necessary to enhance the virtual walking experience.</p><p>The main effect of the walker conditions was significant for the leg-action
sensation, <italic>F</italic>(1, 14)&#x02009;=&#x02009;6.762, <italic>p</italic>&#x02009;=&#x02009;.021,
<inline-formula id="ilm5-2041669519882448"><mml:math id="mml-math5-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.326, and the rating was higher with the shorter walker&#x02019;s
stimuli than the taller walker&#x02019;s stimuli. We obtained no other main effects or
interactions.</p><p>We found that the oscillating optic flow with synchronized vibrations on the feet
was critical to enhance the virtual walking experience in comparison with the
random vibrations or without vibrations. However, it is not clear how much we
are sensitive to synchronization of visual oscillation and vibrations for the
virtual walking. Thus, in the next experiment, we investigated the effect of
phase delay of the vibrations.</p></sec></sec><sec id="sec9-2041669519882448"><title>Experiment 2</title><sec id="sec10-2041669519882448"><title>Methods</title><sec id="sec11-2041669519882448"><title>Participants</title><p>Fifteen undergraduate and graduate students (1 female and 14 males, mean age
of 20.7 years, &#x000b1;1.3 standard deviation) participated in Experiment 2. None
of them participated in Experiment 1. All participants provided written
informed consent and had normal or corrected-to-normal vision. The methods
of the experiment and all experimental protocols were approved by the
Ethical Committee for Human-Subject Research at the Toyohashi University of
Technology. The experiments were strictly conducted in accordance with the
approved guidelines of the committee.</p></sec><sec id="sec12-2041669519882448"><title>Stimuli and apparatus</title><p>The stimuli and apparatus were identical to Experiment1.</p></sec><sec id="sec13-2041669519882448"><title>Design</title><p>Experiment 2 contained three repetitions of all combinations of three
vibration-delay conditions (0, 0.25, and 0.5 phase-delayed vibrations), two
walkers, and three locations (54 trials in total). The conditions of 0.25
and 0.5 phase delay approximately correspond to 250- and 500-millisecond
delay sounds, respectively. The 0.5 phase delay implied that the left and
right feet vibrations were almost reversed with respect to the scene
oscillation because the walkers in the scene walked at 2 steps/second.</p></sec><sec id="sec14-2041669519882448"><title>Procedure</title><p>Participants performed the same task as in Experiment 1; they observed each
stimulus for 20 seconds, after which they were asked to rate the sensation
strengths of (a) self-motion (vection), (b) walking, (c) leg action
(footstep), and (d) telepresence by using VAS.</p></sec></sec><sec id="sec15-2041669519882448"><title>Results</title><p>Self-motion, walking sensation, and leg-action sensation significantly decreased
with 0.25 and 0.5 phase-delayed (250 and 500 milliseconds) vibrations on the
feet in a comparison with the synchronized (no delay) condition when observing
the walker&#x02019;s oscillating first-person-view scenes (<xref ref-type="fig" rid="fig5-2041669519882448">Figure 5</xref>). We conducted three-way
repeated-measure analyses of variance with the vibration-delay conditions
(synchronous, 0.25 phase delay, and 0.5 phase delay), scenes (three different
locations), and walkers (two persons with 169.1 cm and 172.0 cm heights) as the
factors using digitized VAS data for all sensations.</p><fig id="fig5-2041669519882448" orientation="portrait" position="float"><label>Figure 5.</label><caption><p>Results of Experiment 2. VAS&#x02009;=&#x02009;Visual Analogue Scale.</p></caption><graphic xlink:href="10.1177_2041669519882448-fig5"/></fig><p>Statistical significance of the main effect of the vibration-delay condition was
obtained for the sensation of self-motion, <italic>F</italic>(2, 28)&#x02009;=&#x02009;4.679,
<italic>p</italic>&#x02009;=&#x02009;.018, <inline-formula id="ilm6-2041669519882448"><mml:math id="mml-math6-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.251, walking, <italic>F</italic>(2, 28)&#x02009;=&#x02009;5.402,
<italic>p</italic>&#x02009;=&#x02009;.010, <inline-formula id="ilm7-2041669519882448"><mml:math id="mml-math7-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.278, and leg action, <italic>F</italic>(2, 28)&#x02009;=&#x02009;4.656,
<italic>p</italic>&#x02009;=&#x02009;.018, <inline-formula id="ilm8-2041669519882448"><mml:math id="mml-math8-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.250; however, no statistical significance was obtained for
telepresence, <italic>F</italic>(2, 28)&#x02009;=&#x02009;3.098, <italic>p</italic>&#x02009;=&#x02009;.061,
<inline-formula id="ilm9-2041669519882448"><mml:math id="mml-math9-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.181. Multiple comparisons showed that the synchronized foot
vibrations elicited stronger sensations of self-motion, walking, and leg action
than the 0.25 or 0.5 phase-delay conditions (<italic>p</italic>s&#x02009;&#x0003c;&#x02009;.05).
There was no difference between the 0.25 and 0.5 phase-delay conditions. Thus,
the consistency of timings of foot vibrations and scene oscillations affected
the strength of walking-related sensations. However, the effect of phase delay
was not very significant.</p><p>The main effect of the scene conditions was significant for the leg-action
sensation, <italic>F</italic>(1, 14)&#x02009;=&#x02009;4.330, <italic>p</italic>&#x02009;=&#x02009;.023,
<inline-formula id="ilm10-2041669519882448"><mml:math id="mml-math10-2041669519882448"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>&#x003b7;</mml:mtext></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>2</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>&#x02009;=&#x02009;.236, and the rating was higher with the outdoor road scene
than the indoor corridor and lobby scenes. However, multiple comparisons showed
no significant differences between the scenes (<italic>p</italic>s&#x02009;&#x0003e;&#x02009;.081).
We obtained no other main effects or interactions.</p><p>We found that the 0.25 or 0.5 phase delay of foot vibration deteriorated the
strength of self-motion, walking and leg-action sensation, but not the
telepresence. Thus, not only the rhythmical vibration but also its
synchronization to the visual oscillation are necessary for the virtual
walking.</p></sec></sec><sec sec-type="discussion" id="sec16-2041669519882448"><title>Discussion</title><sec id="sec17-2041669519882448"><title>Summary of Results</title><p>The captured first-person-view scenes with image oscillations caused by the
walker&#x02019;s head motion and the foot vibrations at synchronized timings induced
sensations of self-motion, walking, leg action, and telepresence. The
synchronous presentation of visual oscillations and foot vibrations was critical
for enhancing the virtual walking experience.</p><p>The effect of the foot vibration was notable in all experiments. The foot
vibration had to match the actual walking, while the randomized vibrations had
no effect. These results suggest that the tactile stimulation on the feet for
footsteps is effective for enhancing virtual walking sensations.</p></sec><sec id="sec18-2041669519882448"><title>Sensitivity to Phase Delay of Vibrations</title><p>The phase delay of foot vibrations to the captured timings of footsteps
significantly decreased the sensations of self-motion, walking, and leg action
in Experiment 2. The 0.25 and 0.5 phase delays were delays of 250 and 500
milliseconds, respectively. The high sensitivity to such a small discrepancy
between visual oscillation and foot tactile sensations may have been related to
the reciprocal inhibitory interaction between the visual and vestibular system
and the tactile and vestibular system (<xref rid="bibr6-2041669519882448" ref-type="bibr">Berthoz, Pavard, &#x00026; Young, 1975</xref>;
<xref rid="bibr7-2041669519882448" ref-type="bibr">Brandt, Bartenstein,
Janek, &#x00026; Dietrich, 1998</xref>; <xref rid="bibr17-2041669519882448" ref-type="bibr">Hwang, Agada, Kiemel, &#x00026; Jeka, 2014</xref>;
<xref rid="bibr26-2041669519882448" ref-type="bibr">Kleinschimdt et&#x000a0;al.,
2002</xref>; <xref rid="bibr40-2041669519882448" ref-type="bibr">Peterka,
2002</xref>; <xref rid="bibr41-2041669519882448" ref-type="bibr">Peterka
&#x00026; Benolken, 1995</xref>; <xref rid="bibr43-2041669519882448" ref-type="bibr">Redfern &#x00026; Furman, 1994</xref>; <xref rid="bibr49-2041669519882448" ref-type="bibr">Wong &#x00026; Frost,
1981</xref>). Visual information is dominant in the absence of vestibular
information (<xref rid="bibr6-2041669519882448" ref-type="bibr">Berthoz et&#x000a0;al.,
1975</xref>; <xref rid="bibr7-2041669519882448" ref-type="bibr">Brandt
et&#x000a0;al., 1998</xref>; <xref rid="bibr26-2041669519882448" ref-type="bibr">Kleinschimdt et&#x000a0;al., 2002</xref>; <xref rid="bibr49-2041669519882448" ref-type="bibr">Wong &#x00026; Frost, 1981</xref>), and the visual
and somatosensory information is utilized more for postural control in the
absence of vestibular information (<xref rid="bibr12-2041669519882448" ref-type="bibr">Dichgans &#x00026; Brandt, 1978</xref>; <xref rid="bibr17-2041669519882448" ref-type="bibr">Hwang et&#x000a0;al., 2014</xref>;
<xref rid="bibr40-2041669519882448" ref-type="bibr">Peterka, 2002</xref>;
<xref rid="bibr41-2041669519882448" ref-type="bibr">Peterka &#x00026; Benolken,
1995</xref>; <xref rid="bibr43-2041669519882448" ref-type="bibr">Redfern
&#x00026; Furman, 1994</xref>). Thus, the sensitivity to vision and touch might
be enhanced in the absence of vestibular information in our virtual walking
system. Moreover, it is reported that active observers are sensitive to small
discrepancies between visual oscillation and their head motion (<xref rid="bibr4-2041669519882448" ref-type="bibr">Ash, Palmisano, Govan, &#x00026;
Kim, 2011</xref>). Display lag for active observers who are physically
oscillating their head impairs vection if the lag is 50 or 100 milliseconds;
however, it does not impair vection if the lag is 200 milliseconds. This finding
is consistent with our result. Thus, it is suggested that the strict
synchronization of vision and touch contributes to the enhancement of
walking-related sensations for stationary observers in the virtual walking
system.</p></sec><sec id="sec19-2041669519882448"><title>Perceptual Compensation of Visual Oscillation</title><p>We seem rarely aware of visual oscillation or image jittering during actual
walking because perceptual compensation stabilizes the visual stimuli (<xref rid="bibr46-2041669519882448" ref-type="bibr">Wallach, 1987</xref>) and
quickly adapts to the environment (<xref rid="bibr23-2041669519882448" ref-type="bibr">Kitazaki, 2013</xref>; <xref rid="bibr47-2041669519882448" ref-type="bibr">Wallach &#x00026; Canal, 1976</xref>). Thus, it is
suggested that the amplitude of visual oscillation for virtual walking should be
as weak as the oscillations perceived during actual walking to obtain the
optimal effect.</p></sec><sec id="sec20-2041669519882448"><title>Active Walking and Passive Vection</title><p>In this study passively seated observers were simulated to be actively walking
based on the oscillating optic flow and the foot vibration, and we found that
the oscillating optic flow with the foot vibration enhanced the sensation of
walking as well as vection. By contrast, in the previous studies (<xref rid="bibr3-2041669519882448" ref-type="bibr">Ash et&#x000a0;al., 2013</xref>; <xref rid="bibr31-2041669519882448" ref-type="bibr">Onimaru et&#x000a0;al., 2010</xref>),
participants actively walked and passively stood on treadmills, while viewing
oscillating and smooth patterns of optic flow, and they have shown that active
walking on a treadmill decreases vection (<xref rid="bibr3-2041669519882448" ref-type="bibr">Ash et&#x000a0;al., 2013</xref>; <xref rid="bibr31-2041669519882448" ref-type="bibr">Onimaru et&#x000a0;al., 2010</xref>). Thus, one may
predict that the foot vibrations enhance illusory perceptions of active walking
but interfere with illusory perceptions of self-motion/vection. However, we did
not obtain such inhibitory results. Thus, we speculated that the passive
sensation of self-motion and the active sensation of walking can be concurrently
elicited and can interact with each other in some situations or levels. For
example, both occur when we walk on a moving walkway. Vection occurs during
walking on a treadmill even though it is weakened by actual walking (<xref rid="bibr3-2041669519882448" ref-type="bibr">Ash et&#x000a0;al., 2013</xref>; <xref rid="bibr31-2041669519882448" ref-type="bibr">Onimaru et&#x000a0;al., 2010</xref>).
Our virtual walking system probably does not reach the level at which the
passive sensation of self-motion is weakened. If the sensation of active walking
is significantly increased compared with that in the present system, the
sensation of passive self-motion might be decreased. This issue should be
further investigated in a future study.</p></sec><sec id="sec21-2041669519882448"><title>Dependency on Stimulus Walkers and Scenes</title><p>We used two walkers with different heights, and the perception of a
three-dimensional scene depends on eye height (<xref rid="bibr32-2041669519882448" ref-type="bibr">Ooi, Wu, &#x00026; He, 2001</xref>; <xref rid="bibr42-2041669519882448" ref-type="bibr">Proffitt, 2006</xref>).
Actually, we obtained a significant effect of walker height. In Experiment 1,
the leg-action sensation was better with the shorter walker&#x02019;s stimuli than the
taller walker&#x02019;s stimuli. It might have been caused by the difference of heights
or the different movements of individuals. Although we did not collect the data
of participants&#x02019; exact heights, the range was 160 to 180 cm, and the average was
approximately 170 cm. In a preliminary experiment using a prototype system (a
narrower-FoV HMD with one vibrator each for the left and right heel), we found
no correlation of ratings between the walker height and participant height.
However, it can be expected that the matching of eye height of the walker and
the participant or the normalization of eye height improves the walking
sensation. This aspect should be investigated in a future study.</p><p>The difference of scenes or locations had some effects on the walking-related
sensations, although the effects of visual oscillation and foot vibration were
found in all the scenes. In Experiment 2, the leg-action sensation was higher
with the outdoor paved road scene than the indoor corridor and lobby scenes (not
significant in multiple comparisons). These results suggest that the sensations
relating to walking depended on scenes and situations. In this study, we used
identical foot-vibration stimuli for all scenes. As the tactile sensation of the
feet depends on the type of floor/ground, shoes, walker weight, and other
parameters, we should investigate their different effects and optimize the
stimuli in a future study. If we could achieve this objective, we would be able
to present a variety of experiences of virtual walking to persons who are at a
distance or have a disability that prevents them from walking.</p></sec><sec id="sec22-2041669519882448"><title>Limitation of Subjective Measurements</title><p>One may argue that it is difficult for participants to rate the sensation of
walking and leg action using VAS because walking and the leg action are actions
and rather implicit in perception. However, as the obtained data were
quantitative and consistent among participants, the results of this study seem
reliable. In a future, we need to explore behavioral or physiological evidence
for virtual walking sensations, rather than subjective evidence using VAS. We
have already attempted the proprioceptive self-localization task (<xref rid="bibr28-2041669519882448" ref-type="bibr">Lenggenhager, Tadi, Metzinger,
&#x00026; Blanke, 2007</xref>) and jogging after-effect (<xref rid="bibr2-2041669519882448" ref-type="bibr">Anstis, 1995</xref>) test after observing the
virtual walking stimuli for 60 seconds or 90 seconds. We had expected that the
proprioceptive self-location or blind walking at one place would shift in the
direction of virtual walking after experiencing the virtual walking with
synchronous foot vibrations rather than with randomized vibrations. However,
thus far, we found no effect of virtual walking on the self-localization or
position shift of blind walking at one place. We plan to measure the
cognitive-map performance during virtual walking with and without foot
vibrations. We predict that virtual walking may improve the cognitive-map or
spatial-memory performance because our spatial representation is effectively
updated when we actually move around (<xref rid="bibr10-2041669519882448" ref-type="bibr">Burgess, 2006</xref>; <xref rid="bibr48-2041669519882448" ref-type="bibr">Wang &#x00026; Simons, 1999</xref>). </p><p>Furthermore, one may be concerned about the possibility of cross-contamination of
four VAS ratings in each trial. We presented all questions on the screen, and we
asked participants to rate each one without a time limitation to prevent the
cross-contamination of VAS ratings. They were conscious of differences among
four ratings because of contrasting questions at a given time. As the resulting
ratings were different across conditions, we believe that participants
understood the questions appropriately and provided the ratings. However, we
could not check all possibilities of cross-contamination in the participants&#x02019;
subjective judgments. This aspect may have been a limitation of our study. To
address this issue, we should try other behavioral measurements to prove the
sensation of walking.</p></sec></sec><sec sec-type="data-availability" id="sec23-2041669519882448"><title>Data Accessibility Statement</title><p>The data sets generated and analyzed during this study are available from the
corresponding author on a reasonable request.</p></sec></body><back><ack><title>Acknowledgements</title><p>The authors thank Takaaki Hayashizaki and Keisuke Goto for data collection and
analysis.</p></ack><sec id="sec24-2041669519882448"><title>Authors&#x02019; Contributions</title><p>M. K., T. H., K. Y., R. K., T. A., K. H., and Y. I. conceived and designed the
experiments. K.Y., T. H., and R. K. collected and analyzed the data. M. K.
contributed to the preparation of the manuscript. All authors reviewed the
manuscript.</p></sec><sec id="sec25-2041669519882448"><title>Declaration of Conflicting Interests</title><p>The author(s) declared no potential conflicts of interest with respect to the
research, authorship, and/or publication of this article.</p></sec><sec id="sec26-2041669519882448"><title>Funding</title><p>The author(s) disclosed receipt of the following financial support for the research,
authorship, and/or publication of this article: This research was supported by the
SCOPE program (141203019), Grant-in-Aid for Scientific Research (A) (18H04118), and
Grant-in-Aid for Challenging Exploratory Research (16K12477) by MEXT Japan.</p></sec><ref-list content-type="nameDate"><title>References</title><ref id="bibr1-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>R. S.</given-names></name><name><surname>Ash</surname><given-names>A.</given-names></name><name><surname>Palmisano</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>). <article-title>Binocular contributions to
linear vertical vection.</article-title>
<source>Journal of Vision</source>, <volume>14</volume>, 5&#x02013;5.
doi:10.1167/14.12.5.</mixed-citation></ref><ref id="bibr2-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anstis</surname><given-names>S.</given-names></name></person-group> (<year>1995</year>). <article-title>Aftereffects from
jogging.</article-title>
<source>Experimental Brain Research</source>, <volume>103</volume>,
<fpage>476</fpage>&#x02013;<lpage>478</lpage>.<pub-id pub-id-type="pmid">7789454</pub-id></mixed-citation></ref><ref id="bibr3-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ash</surname><given-names>A.</given-names></name><name><surname>Palmisano</surname><given-names>S.</given-names></name><name><surname>Apthorp</surname><given-names>D., &#x00026;</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name></person-group> (<year>2013</year>). <article-title>Vection in depth during
treadmill walking.</article-title>
<source>Perception</source>, <volume>42</volume>,
<fpage>562</fpage>&#x02013;<lpage>576</lpage>.<pub-id pub-id-type="pmid">23964381</pub-id></mixed-citation></ref><ref id="bibr4-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ash</surname><given-names>A.</given-names></name><name><surname>Palmisano</surname><given-names>S.</given-names></name><name><surname>Govan</surname><given-names>G.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>Display lag and gain effects
on vection experienced by active observers.</article-title>
<source>Aviation, Space, and Environmental Medicine</source>,
<volume>82</volume>, <fpage>763</fpage>&#x02013;<lpage>769</lpage>.</mixed-citation></ref><ref id="bibr5-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banton</surname><given-names>T.</given-names></name><name><surname>Stefanucci</surname><given-names>J.</given-names></name><name><surname>Durgin</surname><given-names>F.</given-names></name><name><surname>Fass</surname><given-names>A.</given-names></name><name><surname>Proffitt</surname><given-names>D.</given-names></name></person-group> (<year>2005</year>). <article-title>The perception of walking
speed in a virtual environment.</article-title>
<source>Presence: Teleoperators and Virtual Environments</source>,
<volume>14</volume>, <fpage>394</fpage>&#x02013;<lpage>406</lpage>.</mixed-citation></ref><ref id="bibr6-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berthoz</surname><given-names>A.</given-names></name><name><surname>Pavard</surname><given-names>B.</given-names></name><name><surname>Young</surname><given-names>L. R.</given-names></name></person-group> (<year>1975</year>). <article-title>Perception of linear
horizontal self-motion induced by peripheral vision (linear
vection).</article-title>
<source>Experimental Brain Research</source>, <volume>23</volume>,
<fpage>471</fpage>&#x02013;<lpage>489</lpage>.<pub-id pub-id-type="pmid">1081949</pub-id></mixed-citation></ref><ref id="bibr7-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandt</surname><given-names>T.</given-names></name><name><surname>Bartenstein</surname><given-names>P.</given-names></name><name><surname>Janek</surname><given-names>A.</given-names></name><name><surname>Dietrich</surname><given-names>M.</given-names></name></person-group> (<year>1998</year>). <article-title>Reciprocal inhibitory
visual-vestibular interaction: Visual motion stimulation deactivates the
parieto-insular vestibular cortex.</article-title>
<source>Brain</source>, <volume>121</volume>,
<fpage>1749</fpage>&#x02013;<lpage>1758</lpage>.<pub-id pub-id-type="pmid">9762962</pub-id></mixed-citation></ref><ref id="bibr8-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandt</surname><given-names>T.</given-names></name><name><surname>Wist</surname><given-names>E. R.</given-names></name><name><surname>Dichgans</surname><given-names>J. D.</given-names></name></person-group> (<year>1975</year>). <article-title>Foreground and background in
dynamic spatial orientation.</article-title>
<source>Perception &#x00026; Psychophysics</source>, <volume>17</volume>,
<fpage>497</fpage>&#x02013;<lpage>503</lpage>.</mixed-citation></ref><ref id="bibr9-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bubka</surname><given-names>A.</given-names></name><name><surname>Bonato</surname><given-names>F.</given-names></name></person-group> (<year>2010</year>). <article-title>Natural visual-field
features enhance vection.</article-title>
<source>Perception</source>, <volume>39</volume>,
<fpage>627</fpage>&#x02013;<lpage>635</lpage>.<pub-id pub-id-type="pmid">20677700</pub-id></mixed-citation></ref><ref id="bibr10-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>N.</given-names></name></person-group> (<year>2006</year>). <article-title>Spatial memory: how
egocentric and allocentric combine.</article-title>
<source>Trends in Cognitive Sciences</source>, <volume>10</volume>,
<fpage>551</fpage>&#x02013;<lpage>557</lpage>.<pub-id pub-id-type="pmid">17071127</pub-id></mixed-citation></ref><ref id="bibr11-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ch&#x000e9;ron</surname><given-names>G.</given-names></name><name><surname>Duvinage</surname><given-names>M.</given-names></name><name><surname>De Saedeleer</surname><given-names>C.</given-names></name><name><surname>Castermans</surname><given-names>T.</given-names></name><name><surname>Bengoetxea</surname><given-names>A.</given-names></name><name><surname>Petieau</surname><given-names>M.</given-names></name><name><surname>Ivanenko</surname><given-names>Y.</given-names></name></person-group> (<year>2012</year>). <article-title>From spinal central pattern
generators to cortical network: Integrated BCI for walking
rehabilitation.</article-title>
<source>Neural Plasticity</source>, <volume>2012</volume>,
<fpage>375148</fpage>.<pub-id pub-id-type="pmid">22272380</pub-id></mixed-citation></ref><ref id="bibr12-2041669519882448"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dichgans</surname><given-names>J.</given-names></name><name><surname>Brandt</surname><given-names>T.</given-names></name></person-group> (<year>1978</year>). <chapter-title>Visual-vestibular
interactions: effects on self-motion perception and postural
control</chapter-title> In <person-group person-group-type="editor"><name><surname>Held</surname><given-names>R.</given-names></name><name><surname>Leibowitz</surname><given-names>H. W.</given-names></name><name><surname>Teuber</surname><given-names>H. L.</given-names></name></person-group> (Eds.), <source>Handbook of sensory physiology</source> (
<volume>Vol. 8</volume>, pp. <fpage>755</fpage>&#x02013;<lpage>804</lpage>).
<publisher-loc>Berlin, Germany</publisher-loc>:
<publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="bibr13-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Dodgson</surname><given-names>N. A.</given-names></name></person-group> (<year>2004</year>, May). Variation and extrema of human
interpupillary distance. In Woods A. J, Merritt J. O, Benton S. A, &#x00026; Bolas
M. T (Eds.), <italic>Proceedings Volume 5291, Stereoscopic Displays and Virtual
Reality Systems XI</italic>, San Jose, California, USA, 19-22 January 2004
(pp. 36&#x02013;46). doi:10.1117/12.529999</mixed-citation></ref><ref id="bibr14-2041669519882448"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>J. J.</given-names></name></person-group> (<year>1966</year>). <source>The senses considered as perceptual
systems</source>. <publisher-loc>Boston, MA</publisher-loc>:
<publisher-name>Houghton Mifflin</publisher-name>.</mixed-citation></ref><ref id="bibr15-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>J. J.</given-names></name></person-group> (<year>1968</year>). <article-title>What gives rise to the
perception of motion?</article-title>
<source>Psychological Review</source>, <volume>75</volume>,
<fpage>335</fpage>&#x02013;<lpage>346</lpage>.<pub-id pub-id-type="pmid">4875887</pub-id></mixed-citation></ref><ref id="bibr16-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gravano</surname><given-names>S.</given-names></name><name><surname>Ivanenko</surname><given-names>Y. P.</given-names></name><name><surname>Maccioni</surname><given-names>G.</given-names></name><name><surname>Macellari</surname><given-names>V.</given-names></name><name><surname>Poppele</surname><given-names>R. E.</given-names></name><name><surname>Lacquaniti</surname><given-names>F.</given-names></name></person-group> (<year>2011</year>). <article-title>A novel approach to
mechanical foot stimulation during human locomotion under body weight
support.</article-title>
<source>Human Movement Science</source>, <volume>30</volume>,
<fpage>352</fpage>&#x02013;<lpage>367</lpage>.<pub-id pub-id-type="pmid">20417979</pub-id></mixed-citation></ref><ref id="bibr17-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>S.</given-names></name><name><surname>Agada</surname><given-names>P.</given-names></name><name><surname>Kiemel</surname><given-names>T.</given-names></name><name><surname>Jeka</surname><given-names>J. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Dynamic reweighting of three
modalities for sensor fusion.</article-title>
<source>PLoS One</source>, <volume>9</volume>,
<fpage>e88132</fpage>.<pub-id pub-id-type="pmid">24498252</pub-id></mixed-citation></ref><ref id="bibr18-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Ikei</surname><given-names>Y.</given-names></name><name><surname>Abe</surname><given-names>K.</given-names></name><name><surname>Hirota</surname><given-names>K.</given-names></name><name><surname>Amemiya</surname><given-names>T.</given-names></name></person-group> (<year>2012</year>). A multisensory VR system exploring the
ultra-reality. In Guidi, G. &#x00026; Addison, A. C (Eds.), <italic>Proceedings of
18th International Conference on Virtual Systems and Multimedia</italic>
(VSMM 2012), Milan, Italy, 2&#x02013;5 September 2012, (pp. 71&#x02013;78).
IEEE.</mixed-citation></ref><ref id="bibr19-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Ikei</surname><given-names>Y.</given-names></name><name><surname>Shimabukuro</surname><given-names>S.</given-names></name><name><surname>Kato</surname><given-names>S.</given-names></name><name><surname>Komase</surname><given-names>K.</given-names></name><name><surname>Okuya</surname><given-names>Y.</given-names></name><name><surname>Hirota</surname><given-names>K.</given-names></name><name><surname>Amemiya</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). Five senses theatre project: Sharing
experiences through bodily ultra-reality. In <italic>Proceedings of IEEE Virtual
Reality</italic>, Arles, France, 23&#x02013;27 March 2015, (pp.
195&#x02013;196).</mixed-citation></ref><ref id="bibr20-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Iwata</surname><given-names>H.</given-names></name></person-group> (<year>1999</year>). Walking about virtual environments on an
infinite floor. In <italic>Proceedings of IEEE Virtual Reality</italic>,
Houston, TX, USA, 13&#x02013;17 March, (pp. 286&#x02013;293).</mixed-citation></ref><ref id="bibr21-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Iwata</surname><given-names>H.</given-names></name><name><surname>Yano</surname><given-names>H.</given-names></name><name><surname>Nakaizumi</surname><given-names>F.</given-names></name></person-group> (<year>2001</year>). Gait Master: A versatile locomotion
interface for uneven virtual terrain. In <italic>Proceedings of IEEE Virtual
Reality</italic>, Yokohama, Japan, 13&#x02013;17 March 2001, (pp.
131&#x02013;137).</mixed-citation></ref><ref id="bibr22-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Chung</surname><given-names>E.</given-names></name><name><surname>Nakamura</surname><given-names>S.</given-names></name><name><surname>Palmisano</surname><given-names>S.</given-names></name><name><surname>Khuu</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>The Oculus Rift: A
cost-effective tool for studying visual-vestibular interactions in
self-motion perception.</article-title>
<source>Frontiers in Psychology</source>, <volume>6</volume>,
<fpage>248</fpage>. doi:10.3389/fpsyg.2015.00248.<pub-id pub-id-type="pmid">25821438</pub-id></mixed-citation></ref><ref id="bibr23-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitazaki</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>Effects of retinal position
on visuo-motor adaptation of visual stability in a virtual
environment.</article-title>
<source>i-Perception</source>, <volume>4</volume>,
<fpage>242</fpage>&#x02013;<lpage>252</lpage>.<pub-id pub-id-type="pmid">24349685</pub-id></mixed-citation></ref><ref id="bibr24-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitazaki</surname><given-names>M.</given-names></name><name><surname>Sato</surname><given-names>T.</given-names></name></person-group> (<year>2003</year>). <article-title>Attentional modulation of
self-motion perception.</article-title>
<source>Perception</source>, <volume>32</volume>,
<fpage>475</fpage>&#x02013;<lpage>484</lpage>.<pub-id pub-id-type="pmid">12785485</pub-id></mixed-citation></ref><ref id="bibr25-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitazaki</surname><given-names>M.</given-names></name><name><surname>Shimojo</surname><given-names>S.</given-names></name></person-group> (<year>1998</year>). <article-title>Surface discontinuity is
critical in a moving observer&#x02019;s perception of objects&#x02019; depth order and
relative motion from retinal image motion.</article-title>
<source>Perception</source>, <volume>27</volume>,
<fpage>1153</fpage>&#x02013;<lpage>1176</lpage>.<pub-id pub-id-type="pmid">10505195</pub-id></mixed-citation></ref><ref id="bibr26-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinschimdt</surname><given-names>A.</given-names></name><name><surname>Thilo</surname><given-names>K. V.</given-names></name><name><surname>Buchel</surname><given-names>C.</given-names></name><name><surname>Gresty</surname><given-names>M. A.</given-names></name><name><surname>Bronstein</surname><given-names>A.</given-names></name><name><surname>Frackowiak</surname><given-names>R. S. J.</given-names></name></person-group> (<year>2002</year>). <article-title>Neural correlates of
visual-motion perception as object- or self-motion.</article-title>
<source>Neuroimage</source>, <volume>16</volume>,
<fpage>873</fpage>&#x02013;<lpage>882</lpage>.<pub-id pub-id-type="pmid">12202076</pub-id></mixed-citation></ref><ref id="bibr27-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>L&#x000e9;cuyer</surname><given-names>A.</given-names></name><name><surname>Burkhardt</surname><given-names>J. M.</given-names></name><name><surname>Henaff</surname><given-names>J. M.</given-names></name><name><surname>Donikian</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). Camera motions improve the sensation of
walking in virtual environments. In <italic>Proceedings of IEEE Virtual
Reality</italic>, Alexandria, VA, USA, 25&#x02013;29 March 2006, (pp.
11&#x02013;18).</mixed-citation></ref><ref id="bibr28-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenggenhager</surname><given-names>B.</given-names></name><name><surname>Tadi</surname><given-names>T.</given-names></name><name><surname>Metzinger</surname><given-names>T.</given-names></name><name><surname>Blanke</surname><given-names>O.</given-names></name></person-group> (<year>2007</year>). <article-title>Video ergo sum: Manipulating
bodily self-consciousness.</article-title>
<source>Science</source>, <volume>317</volume>,
<fpage>1096</fpage>&#x02013;<lpage>1099</lpage>.<pub-id pub-id-type="pmid">17717189</pub-id></mixed-citation></ref><ref id="bibr29-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakayama</surname><given-names>K.</given-names></name></person-group> (<year>1985</year>). <article-title>Biological image motion
processing: A review.</article-title>
<source>Vision Research</source>, <volume>25</volume>,
<fpage>625</fpage>&#x02013;<lpage>660</lpage>.<pub-id pub-id-type="pmid">3895725</pub-id></mixed-citation></ref><ref id="bibr30-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohmi</surname><given-names>M.</given-names></name><name><surname>Howard</surname><given-names>I. P.</given-names></name><name><surname>Landolt</surname><given-names>J. P.</given-names></name></person-group> (<year>1987</year>). <article-title>Circular vection as a
function of foreground-background relationships.</article-title>
<source>Perception</source>, <volume>16</volume>,
<fpage>17</fpage>&#x02013;<lpage>22</lpage>.<pub-id pub-id-type="pmid">3671036</pub-id></mixed-citation></ref><ref id="bibr31-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Onimaru</surname><given-names>S.</given-names></name><name><surname>Sato</surname><given-names>T.</given-names></name><name><surname>Kitazaki</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Veridical walking inhibits
vection perception.</article-title>
<source>Journal of Vision</source>, <volume>10</volume>,
<fpage>860</fpage>.</mixed-citation></ref><ref id="bibr32-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ooi</surname><given-names>T. L.</given-names></name><name><surname>Wu</surname><given-names>B.</given-names></name><name><surname>He</surname><given-names>Z. J.</given-names></name></person-group> (<year>2001</year>). <article-title>Distance determined by the
angular declination below the horizon.</article-title>
<source>Nature</source>, <volume>414</volume>,
<fpage>197</fpage>.<pub-id pub-id-type="pmid">11700556</pub-id></mixed-citation></ref><ref id="bibr33-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S. A.</given-names></name></person-group> (<year>1996</year>). <article-title>Perceiving self-motion in
depth: The role of stereoscopic motion and changing-size
cues.</article-title>
<source>Perception &#x00026; Psychophysics</source>, <volume>58</volume>,
<fpage>1168</fpage>&#x02013;<lpage>1176</lpage>.<pub-id pub-id-type="pmid">8961828</pub-id></mixed-citation></ref><ref id="bibr34-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Consistent stereoscopic
information increases the perceived speed of vection in
depth.</article-title>
<source>Perception</source>, <volume>31</volume>,
<fpage>463</fpage>&#x02013;<lpage>480</lpage>.<pub-id pub-id-type="pmid">12018791</pub-id></mixed-citation></ref><ref id="bibr35-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S. A.</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name><name><surname>Ash</surname><given-names>A.</given-names></name><name><surname>Nakamura</surname><given-names>S.</given-names></name><name><surname>Apthorp</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>Evidence against an
ecological explanation of the jitter advantage for vection.</article-title>
<source>Frontiers in Psychology</source>, <volume>5</volume>.
doi:10.3389/fpsyg.2014.01297.</mixed-citation></ref><ref id="bibr36-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S. A.</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Bonato</surname><given-names>F.</given-names></name></person-group> (<year>2011</year>). <article-title>Simulated viewpoint jitter
shakes sensory conflict accounts of self-motion perception.</article-title>
<source>Seeing and Perceiving</source>, <volume>24</volume>,
<fpage>173</fpage>&#x02013;<lpage>200</lpage>.<pub-id pub-id-type="pmid">21864457</pub-id></mixed-citation></ref><ref id="bibr37-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S.</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name><name><surname>Schira</surname><given-names>M. M.</given-names></name><name><surname>Barry</surname><given-names>R. J.</given-names></name></person-group> (<year>2015</year>). <article-title>Future challenges for
vection research: Definitions, functional significance, measures and neural
bases.</article-title>
<source>Frontiers in Psychology</source>, <volume>6</volume>.
doi:10.3389/fpsyg.2015.00193.</mixed-citation></ref><ref id="bibr38-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S. A.</given-names></name><name><surname>Burke</surname><given-names>D.</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name></person-group> (<year>2003</year>). <article-title>Coherent perspective jitter
induces visual illusions of self-motion.</article-title>
<source>Perception</source>, <volume>32</volume>,
<fpage>97</fpage>&#x02013;<lpage>110</lpage>.<pub-id pub-id-type="pmid">12613789</pub-id></mixed-citation></ref><ref id="bibr39-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmisano</surname><given-names>S. A.</given-names></name><name><surname>Gillam</surname><given-names>B.</given-names></name><name><surname>Blackburn</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>Global-perspective jitter
improves vection in central vision.</article-title>
<source>Perception</source>, <volume>29</volume>,
<fpage>57</fpage>&#x02013;<lpage>67</lpage>.<pub-id pub-id-type="pmid">10820591</pub-id></mixed-citation></ref><ref id="bibr40-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterka</surname><given-names>R. J.</given-names></name></person-group> (<year>2002</year>). <article-title>Sensorimotor integration in
human postural control.</article-title>
<source>Journal of Neurophysiology</source>, <volume>88</volume>,
<fpage>1097</fpage>&#x02013;<lpage>1118</lpage>.<pub-id pub-id-type="pmid">12205132</pub-id></mixed-citation></ref><ref id="bibr41-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterka</surname><given-names>R. J.</given-names></name><name><surname>Benolken</surname><given-names>M. S.</given-names></name></person-group> (<year>1995</year>). <article-title>Role of somatosensory and
vestibular cues in attenuating visually induced human postural
sway.</article-title>
<source>Experimental Brain Research</source>, <volume>105</volume>,
<fpage>101</fpage>&#x02013;<lpage>110</lpage>.<pub-id pub-id-type="pmid">7589307</pub-id></mixed-citation></ref><ref id="bibr42-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proffitt</surname><given-names>D. R.</given-names></name></person-group> (<year>2006</year>). <article-title>Distance
perception.</article-title>
<source>Current Directions in Psychological Science</source>,
<volume>15</volume>, <fpage>131</fpage>&#x02013;<lpage>135</lpage>.</mixed-citation></ref><ref id="bibr43-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redfern</surname><given-names>M. S.</given-names></name><name><surname>Furman</surname><given-names>J. M.</given-names></name></person-group> (<year>1994</year>). <article-title>Postural sway of patients
with vestibular disorders during optic flow.</article-title>
<source>Journal of Vestibular Research</source>, <volume>4</volume>,
<fpage>221</fpage>&#x02013;<lpage>230</lpage>.<pub-id pub-id-type="pmid">7921340</pub-id></mixed-citation></ref><ref id="bibr44-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seno</surname><given-names>T.</given-names></name><name><surname>Ito</surname><given-names>H.</given-names></name><name><surname>Sunaga</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <article-title>Inconsistent locomotion
inhibits vection.</article-title>
<source>Perception</source>, <volume>40</volume>,
<fpage>747</fpage>&#x02013;<lpage>750</lpage>.<pub-id pub-id-type="pmid">21936303</pub-id></mixed-citation></ref><ref id="bibr45-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skarbez</surname><given-names>R.</given-names></name><name><surname>Brooks</surname><given-names>F. J.</given-names></name><name><surname>Whitton</surname><given-names>M. C.</given-names></name></person-group> (<year>2017</year>). <article-title>A survey of presence and
related concepts.</article-title>
<source>ACM Computing Surveys</source>, <volume>50</volume>, <fpage>96</fpage>.
doi:10.1145/3134301</mixed-citation></ref><ref id="bibr46-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>H.</given-names></name></person-group> (<year>1987</year>). <article-title>Perceiving a stable
environment when one moves.</article-title>
<source>Annual Review of Psychology</source>, <volume>38</volume>,
<fpage>1</fpage>&#x02013;<lpage>27</lpage>.</mixed-citation></ref><ref id="bibr47-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>H.</given-names></name><name><surname>Canal</surname><given-names>T.</given-names></name></person-group> (<year>1976</year>). <article-title>Two kinds of adaptation in
the constancy of visual direction.</article-title>
<source>Perception &#x00026; Psychophysics</source>, <volume>19</volume>,
<fpage>445</fpage>&#x02013;<lpage>449</lpage>.</mixed-citation></ref><ref id="bibr48-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R. F.</given-names></name><name><surname>Simons</surname><given-names>D. J.</given-names></name></person-group> (<year>1999</year>). <article-title>Active and passive scene
recognition across views.</article-title>
<source>Cognition</source>, <volume>70</volume>,
<fpage>191</fpage>&#x02013;<lpage>210</lpage>.<pub-id pub-id-type="pmid">10349763</pub-id></mixed-citation></ref><ref id="bibr49-2041669519882448"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>S. C. P.</given-names></name><name><surname>Frost</surname><given-names>B. J.</given-names></name></person-group> (<year>1981</year>). <article-title>The effect of
visual-vestibular conflict on the latency of steady-state visually induced
subjective rotation.</article-title>
<source>Perception &#x00026; Psychophysics</source>, <volume>30</volume>,
<fpage>228</fpage>&#x02013;<lpage>236</lpage>.<pub-id pub-id-type="pmid">6976558</pub-id></mixed-citation></ref></ref-list><ref-list content-type="nameDate"><title>How to cite this article</title><ref id="bibr50-2041669519882448"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kitazaki</surname><given-names>M.</given-names></name><name><surname>Hamada</surname><given-names>T.</given-names></name><name><surname>Yoshiho</surname><given-names>K.</given-names></name><name><surname>Kondo</surname><given-names>R.</given-names></name><name><surname>Amemiya</surname><given-names>T.</given-names></name><name><surname>Hirota</surname><given-names>K.</given-names></name><name><surname>Ikei</surname><given-names>Y.</given-names></name></person-group> (<year>2019</year>). Virtual walking sensation by prerecorded
oscillating optic flow and synchronous foot vibration. <italic>i-Perception,
10</italic>(5), 1&#x02013;14. doi:10.1177/2041669519882448</mixed-citation></ref></ref-list></back></article>