<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27654941</article-id><article-id pub-id-type="pmc">5031457</article-id><article-id pub-id-type="publisher-id">PONE-D-16-06715</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0163041</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Signaling and Communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Signaling and Communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Model Organisms</subject><subj-group><subject>Animal Models</subject><subj-group><subject>Marmosets</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject><subj-group><subject>New World monkeys</subject><subj-group><subject>Marmosets</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Technology Development</subject><subj-group><subject>Prototypes</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Signal Filtering</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Computational Biology</subject><subj-group><subject>Computational Neuroscience</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational Neuroscience</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Support Vector Machines</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Support Vector Machines</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Machine Learning Algorithms for Automatic Classification of Marmoset Vocalizations</article-title><alt-title alt-title-type="running-head">Automatic Classification of Marmoset Vocalizations</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8971-5349</contrib-id><name><surname>Turesson</surname><given-names>Hjalmar K.</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Ribeiro</surname><given-names>Sidarta</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Pereira</surname><given-names>Danillo R.</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Papa</surname><given-names>Jo&#x000e3;o P.</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>de Albuquerque</surname><given-names>Victor Hugo C.</given-names></name><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>Instituto do C&#x000e9;rebro, Universidade Federal do Rio Grande do Norte, Natal, Brazil</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>Departamento de Computa&#x000e7;&#x000e3;o, Universidade Estadual Paulista &#x0201c;J&#x000fa;lio de Mesquita Filho&#x0201d;, Bauru, S&#x000e3;o Paulo, Brazil</addr-line>
</aff><aff id="aff003">
<label>3</label>
<addr-line>Programa de P&#x000f3;s-Gradua&#x000e7;&#x000e3;o em Inform&#x000e1;tica Aplicada, Laborat&#x000f3;rio de Bioinform&#x000e1;tica, Universidade de Fortaleza, Fortaleza, CE, Brazil</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Smotherman</surname><given-names>Michael</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Texas A&#x00026;M University College Station, UNITED STATES</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p><list list-type="simple"><list-item><p><bold>Conceptualization:</bold> VHCA HKT.</p></list-item><list-item><p><bold>Data curation:</bold> HKT.</p></list-item><list-item><p><bold>Formal analysis:</bold> HKT DRP.</p></list-item><list-item><p><bold>Funding acquisition:</bold> HKT VHCA SR.</p></list-item><list-item><p><bold>Investigation:</bold> HKT.</p></list-item><list-item><p><bold>Methodology:</bold> VHCA JPP HKT DRP.</p></list-item><list-item><p><bold>Project administration:</bold> VHCA SR.</p></list-item><list-item><p><bold>Resources:</bold> HKT SR.</p></list-item><list-item><p><bold>Software:</bold> JPP HKT DRP.</p></list-item><list-item><p><bold>Supervision:</bold> VHCA SR.</p></list-item><list-item><p><bold>Validation:</bold> HKT.</p></list-item><list-item><p><bold>Visualization:</bold> HKT.</p></list-item><list-item><p><bold>Writing &#x02013; original draft:</bold> HKT.</p></list-item><list-item><p><bold>Writing &#x02013; review &#x00026; editing:</bold> HKT SR JPP.</p></list-item></list></p></fn><corresp id="cor001">* E-mail: <email>turesson@neuro.ufrn.br</email></corresp></author-notes><pub-date pub-type="collection"><year>2016</year></pub-date><pub-date pub-type="epub"><day>21</day><month>9</month><year>2016</year></pub-date><volume>11</volume><issue>9</issue><elocation-id>e0163041</elocation-id><history><date date-type="received"><day>3</day><month>3</month><year>2016</year></date><date date-type="accepted"><day>1</day><month>9</month><year>2016</year></date></history><permissions><copyright-statement>&#x000a9; 2016 Turesson et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Turesson et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0163041.pdf"/><abstract><p>Automatic classification of vocalization type could potentially become a useful tool for acoustic the monitoring of captive colonies of highly vocal primates. However, for classification to be useful in practice, a reliable algorithm that can be successfully trained on small datasets is necessary. In this work, we consider seven different classification algorithms with the goal of finding a robust classifier that can be successfully trained on small datasets. We found good classification performance (accuracy &#x0003e; 0.83 and F<sub>1</sub>-score &#x0003e; 0.84) using the Optimum Path Forest classifier. Dataset and algorithms are made publicly available.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>CNPq</institution></funding-source><award-id>402422/2012-0</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8971-5349</contrib-id><name><surname>Turesson</surname><given-names>Hjalmar K.</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>CNPq</institution></funding-source><award-id>308775/2015-5</award-id><principal-award-recipient><name><surname>Ribeiro</surname><given-names>Sidarta</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>CNPq</institution></funding-source><award-id>470501/2013-8</award-id><principal-award-recipient><name><surname>de Albuquerque</surname><given-names>Victor Hugo C.</given-names></name></principal-award-recipient></award-group><award-group id="award004"><funding-source><institution>CNPq</institution></funding-source><award-id>301928/2014-2</award-id><principal-award-recipient><name><surname>de Albuquerque</surname><given-names>Victor Hugo C.</given-names></name></principal-award-recipient></award-group><award-group id="award005"><funding-source><institution>FAPESP</institution></funding-source><award-id>2013/07699-0</award-id><principal-award-recipient><name><surname>Ribeiro</surname><given-names>Sidarta</given-names></name></principal-award-recipient></award-group><award-group id="award006"><funding-source><institution>FAPESP</institution></funding-source><award-id>2014/16250-9</award-id><principal-award-recipient><name><surname>Papa</surname><given-names>Jo&#x000e3;o P.</given-names></name></principal-award-recipient></award-group><award-group id="award007"><funding-source><institution>FAPESP</institution></funding-source><award-id>2015/50319-9</award-id><principal-award-recipient><name><surname>Papa</surname><given-names>Jo&#x000e3;o P.</given-names></name></principal-award-recipient></award-group><award-group id="award008"><funding-source><institution>CNPq</institution></funding-source><award-id>470571/2013-6</award-id><principal-award-recipient><name><surname>Papa</surname><given-names>Jo&#x000e3;o P.</given-names></name></principal-award-recipient></award-group><award-group id="award009"><funding-source><institution>CNPq</institution></funding-source><award-id>306166/2014-3</award-id><principal-award-recipient><name><surname>Papa</surname><given-names>Jo&#x000e3;o P.</given-names></name></principal-award-recipient></award-group><funding-statement>This work was supported by National Council for Scientific and Technological Development 402422/2012-0 (<ext-link ext-link-type="uri" xlink:href="http://cnpq.br/">http://cnpq.br/</ext-link>) HKT and SR; National Council for Scientific and Technological Development 470501/2013-8 and 301928/2014-2 (<ext-link ext-link-type="uri" xlink:href="http://cnpq.br/">http://cnpq.br/</ext-link>) VHCA; S&#x000e3;o Paulo Research Foundation 2013/07699-0 (<ext-link ext-link-type="uri" xlink:href="http://www.fapesp.br">http://www.fapesp.br</ext-link>) SR; S&#x000e3;o Paulo Research Foundation 2014/16250-9 and 2015/50319-9 (<ext-link ext-link-type="uri" xlink:href="http://www.fapesp.br">http://www.fapesp.br</ext-link>) JPP; and National Council for Scientific and Technological Development 470571/2013-6 and 306166/2014-3 (<ext-link ext-link-type="uri" xlink:href="http://cnpq.br/">http://cnpq.br/</ext-link>) JPP. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="2"/><table-count count="5"/><page-count count="14"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>The dataset is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/yqpvk/">https://osf.io/yqpvk/</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://neuro.ufrn.br/data/marmosetvocalizations">http://neuro.ufrn.br/data/marmosetvocalizations</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>The dataset is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/yqpvk/">https://osf.io/yqpvk/</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://neuro.ufrn.br/data/marmosetvocalizations">http://neuro.ufrn.br/data/marmosetvocalizations</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>The common marmoset (<italic>Callithrix jacchus</italic>) is a species of arboreal New World monkeys native to the northeast region of Brazil. This species is becoming an increasingly important primate model of a number of human diseases, as well as for basic research in neuroscience [<xref rid="pone.0163041.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0163041.ref002" ref-type="bibr">2</xref>] and genetics [<xref rid="pone.0163041.ref003" ref-type="bibr">3</xref>]. Examples of the species&#x02019; use as a disease model come from multiple sclerosis [<xref rid="pone.0163041.ref004" ref-type="bibr">4</xref>], herpes virus [<xref rid="pone.0163041.ref005" ref-type="bibr">5</xref>] and tuberculosis [<xref rid="pone.0163041.ref006" ref-type="bibr">6</xref>] research. Some of the reasons for this popularity are that marmosets have a similar disease susceptibility profile to humans, are relatively easy to handle, have a high reproductive rate, and that important genetic and neuroscience research tools already exist [<xref rid="pone.0163041.ref007" ref-type="bibr">7</xref>]. In particular, marmosets are an excellent model for the neurophysiological study of vocal communication [<xref rid="pone.0163041.ref008" ref-type="bibr">8</xref>]. A pubmed search on &#x0201c;Callithrix jacchus&#x0201d; shows between 113 and 164 publications per year during the last 10 years, with most of the publications in biomedicine. Given the widespread use of marmosets in laboratories, methods to reliably monitor health and behavior in captive colonies are of a high priority.</p><p>As typical for Neotropical arboreal primates, marmosets rely heavily on acoustic communication. This together with cooperative breeding and the complex social organization that follows, means that marmosets have a large vocal repertoire, with 9&#x02013;13 different types of vocalizations reported [<xref rid="pone.0163041.ref009" ref-type="bibr">9</xref>&#x02013;<xref rid="pone.0163041.ref013" ref-type="bibr">13</xref>]. The vocalizations produced by a group of marmosets provide a rich source of information about their activities and well-being [<xref rid="pone.0163041.ref014" ref-type="bibr">14</xref>]. However, although informative, it is practically untenable to manually track the vocalizations of a colony over any longer period of time. Manually classifying calls require expert knowledge and daily hours of work, making it too time-consuming and prone to inconsistencies among researchers. Thus, a reliable automatic method for call identification is necessary.</p><p>Much work has been done on the automatic classification of animal vocalizations, especially bird song [<xref rid="pone.0163041.ref015" ref-type="bibr">15</xref>&#x02013;<xref rid="pone.0163041.ref019" ref-type="bibr">19</xref>], but also mammalian [<xref rid="pone.0163041.ref020" ref-type="bibr">20</xref>&#x02013;<xref rid="pone.0163041.ref022" ref-type="bibr">22</xref>] and amphibian calls [<xref rid="pone.0163041.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pone.0163041.ref025" ref-type="bibr">25</xref>]. However, only a few studies have addressed non-human primates (hereafter primates). Pertinent to the current study, there are only three other studies in which different primate vocalizations were analyzed. Mielke and Zuberb&#x000fc;hler (2013) used artificial neural networks (ANNs) with Mel-Frequency Cepstral Coefficients features to classify blue monkey call types (<italic>Cercopithecus mitis stuhlmanni</italic>). Furthermore, they predicted caller identity from the alarm call, and identified the blue monkey alarm call among the alarm calls of other sympatric species [<xref rid="pone.0163041.ref026" ref-type="bibr">26</xref>].</p><p>Pozzi, Gamba and Giacoma (2010) also used ANNs, but with hand-designed features derived from fundamental frequency and formants to classify call type among seven distinct types made by the black lemur (<italic>Eulemur macaco</italic>) [<xref rid="pone.0163041.ref027" ref-type="bibr">27</xref>]. In a later study, the same group used the long grunt (a vocalization included in the repertoire of all lemurs) and similar analytical methods to classify species among the five species in the <italic>Eulemur</italic> genus [<xref rid="pone.0163041.ref028" ref-type="bibr">28</xref>]. Therefore, to our knowledge, there is no previous work on classifying vocalization types from Neotropical primates.</p><p>In contrast, several studies have explored the related question of how specific to the caller are the acoustic properties of individual vocalizations, that is, how well caller identity can be predicted from a given call. These studies have all relied on manually designed features and linear discriminant analysis (LDA). Particularly relevant are Jones et al. (1993) and Miller et al. (2010), who both studied the common marmoset&#x02019;s Phee call to classify caller individual [<xref rid="pone.0163041.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0163041.ref030" ref-type="bibr">30</xref>]. Both studies found that the call is highly caller-specific. Similar studies have been done on Japanese macaque (<italic>Macaca fuscata</italic>) [<xref rid="pone.0163041.ref031" ref-type="bibr">31</xref>], blue monkey [<xref rid="pone.0163041.ref032" ref-type="bibr">32</xref>], ring-tailed lemur (<italic>Lemur catta</italic>) [<xref rid="pone.0163041.ref033" ref-type="bibr">33</xref>], and cotton-top tamarin (<italic>Saguinus oedipus</italic>) [<xref rid="pone.0163041.ref034" ref-type="bibr">34</xref>].</p><p>Algorithms for the automatic classification of vocalizations learn the mapping from input (call features) to label (call type). Therefore, a dataset with labeled calls is necessary to train the algorithms. In general, classification performance increases with the amount of training data. However, in ethology, large sets of labeled data are often hard to obtain. The amount of data may be limited because data collection is labor-intensive, or the data of interest are inherently scarce because they are produced by animals passing through transitory learning or developmental stages. In the latter case, the amount of possible data is strictly limited. Thus, a method that achieves high accuracy with a relatively small number of labeled call exemplars is highly desirable.</p><p>To address the need for automatic call classification given the aforementioned constraints, we compared seven different types of classification algorithms with the goal of finding a reliable method.</p><p>To extract acoustic features, we used Linear Predictive Coding (LPC), a method commonly used for speech processing [<xref rid="pone.0163041.ref035" ref-type="bibr">35</xref>]. For classification purposes, we used seven different algorithms: (i) Optimum Path Forest (OPF), (ii) Bayesian Classifier, (ii) Multilayer Artificial Neural Network (MLP), (iv) Support Vector Machines (SVM), (v) k-Nearest Neighbors (k-NN), (vi) Logistic regression, and (vii) AdaBoost.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and Methods</title><sec id="sec003"><title>Dataset description</title><p>The subjects were five captive-born adult common marmosets (two females and three males), housed at the Instituto do C&#x000e9;rebro, Universidade Federal do Rio Grande do Norte. The marmosets where housed socially in two wire mesh enclosures (1.20 x 1.50 x 2.45 m), enriched with tree branches, ropes, plants, hammocks and nesting tubes. The animals were fed twice daily with fresh and dried fruit, nuts, egg and chicken, and had <italic>ad libitum</italic> access to water. The colony was maintained outdoors protected by a roof allowing daily sunbaths in natural light. The animals were housed in compliance with SISBIO permit 18394, and the experiment was approved by the ethics committee of Universidade Federal do Rio Grande do Norte with CEUA permit 11/2016. No animal was sacrificed at the end of the experiment.</p><p>A directional microphone (ECM-CG50 Pro Shotgun Microphone, Sony, Tokyo, Japan) was placed at a distance of 10 cm above the home cage and connected to a computer. The microphone signal was streamed to a computer where a custom-written Python script segmented the incoming signal. The signal was bandpass filtered between 4 and 10 kHz, and when the amplitude exceeded a threshold a segment beginning 0.5 s before first threshold crossing and ending 0.5 s after the last threshold crossing was saved. Sound was sampled at 44.1 kHz.</p><p>From the raw recordings, we selected and manually labeled 27&#x02013;30 exemplars per class of marmoset vocalization. We attempted to cover the marmoset&#x02019;s vocal repertoire of approximately nine [<xref rid="pone.0163041.ref036" ref-type="bibr">36</xref>] to 13 [<xref rid="pone.0163041.ref037" ref-type="bibr">37</xref>] distinct vocalizations. The number of exemplars of each of the 11 types investigated in this work is listed in <xref rid="pone.0163041.t001" ref-type="table">Table 1</xref>, and spectrograms of representative exemplars of each type are shown in <xref ref-type="fig" rid="pone.0163041.g001">Fig 1</xref>. All vocalizations were produced spontaneously, that is, without any intervention from the experimenter. The recordings were done over a period of two months. The dataset is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/yqpvk/">https://osf.io/yqpvk/</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://neuro.ufrn.br/data/marmosetvocalizations">http://neuro.ufrn.br/data/marmosetvocalizations</ext-link>.</p><fig id="pone.0163041.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.g001</object-id><label>Fig 1</label><caption><title>Vocalization exemplars.</title><p>Amplitude and time-frequency spectrograms are shown for representative exemplars of the marmoset call types considered in this study. A: Alarm, B: Chirp, C: Loud shrill, D: Phee-2, E: Phee-3, F: Phee-4, G: Seep, H: Trill, I: Tsik, J: Tsik-Ek, K: Twitter.</p></caption><graphic xlink:href="pone.0163041.g001"/></fig><table-wrap id="pone.0163041.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.t001</object-id><label>Table 1</label><caption><title>The number of calls considering each class.</title></caption><alternatives><graphic id="pone.0163041.t001g" xlink:href="pone.0163041.t001"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Vocalization type</th><th align="left" rowspan="1" colspan="1">Exemplars per class</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Alarm</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Chirp</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Loud-shrill</td><td align="left" rowspan="1" colspan="1">27</td></tr><tr><td align="left" rowspan="1" colspan="1">Phee-2</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Phee-3</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Phee-4</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Seep</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Trill</td><td align="left" rowspan="1" colspan="1">27</td></tr><tr><td align="left" rowspan="1" colspan="1">Tsik</td><td align="left" rowspan="1" colspan="1">27</td></tr><tr><td align="left" rowspan="1" colspan="1">Tsik-ek</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Twitter</td><td align="left" rowspan="1" colspan="1">30</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec004"><title>Feature extraction</title><p>The success of a signal classification system depends on the choice of features used to characterize the raw signals. In this work, we used LPC, a method commonly used for analysis and compression of speech and animal vocalizations [<xref rid="pone.0163041.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0163041.ref038" ref-type="bibr">38</xref>]. The linear prediction filter coefficients were used as input features to the classification algorithms. Those are the coefficients of an n<sup>th</sup>-order linear finite impulse response filter that predicts the current value of the vocalization from past samples [<xref rid="pone.0163041.ref039" ref-type="bibr">39</xref>]. The number of features extracted from each call was set to 20 after experimenting with filter orders from 10 to 25, in steps of 5.</p></sec><sec id="sec005"><title>Classification algorithms</title><sec id="sec006"><title>Optimum-Path Forest</title><p>The OPF classifier models the problem of pattern recognition as a graph partition task, in which a predefined set of samples from each class (i.e. <italic>prototypes</italic>) compete for minimal path cost to the rest of the samples. This results in a collection of optimum-path trees rooted at the prototype nodes, building an optimum-path forest considering from all training samples. Test samples are classified through incrementally evaluating the optimum paths from the prototypes, as though they were part of the forest, and assigning the labels of the most strongly connected roots. The notion of optimum-path connectivity comes from the minimization of a path-cost function [<xref rid="pone.0163041.ref040" ref-type="bibr">40</xref>]. An OPF classifier can be designed as long as we use a smooth path-cost function. Although there are two different versions of the supervised OPF classifier [<xref rid="pone.0163041.ref041" ref-type="bibr">41</xref>&#x02013;<xref rid="pone.0163041.ref043" ref-type="bibr">43</xref>], in this paper we make use of the former and most widely used approach, as described below.</p><p>The OPF with complete graph was first proposed by Papa et al. [<xref rid="pone.0163041.ref041" ref-type="bibr">41</xref>]. Later on, Papa et al. [<xref rid="pone.0163041.ref042" ref-type="bibr">42</xref>] presented an improved version with a more efficient classification step. In this section, we present the OPF algorithm as described by those authors, using the same formalism.</p><p>Let <inline-formula id="pone.0163041.e001"><alternatives><graphic xlink:href="pone.0163041.e001.jpg" id="pone.0163041.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> be a dataset partitioned into a training (<inline-formula id="pone.0163041.e002"><alternatives><graphic xlink:href="pone.0163041.e002.jpg" id="pone.0163041.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>) and a test (<inline-formula id="pone.0163041.e003"><alternatives><graphic xlink:href="pone.0163041.e003.jpg" id="pone.0163041.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>) set. In addition, let <inline-formula id="pone.0163041.e004"><alternatives><graphic xlink:href="pone.0163041.e004.jpg" id="pone.0163041.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> be the graph originated from <inline-formula id="pone.0163041.e005"><alternatives><graphic xlink:href="pone.0163041.e005.jpg" id="pone.0163041.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>, such that <inline-formula id="pone.0163041.e006"><alternatives><graphic xlink:href="pone.0163041.e006.jpg" id="pone.0163041.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0163041.e007"><alternatives><graphic xlink:href="pone.0163041.e007.jpg" id="pone.0163041.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mi mathvariant="script">E</mml:mi></mml:math></alternatives></inline-formula> stands for an adjacency relation that defines a full connectedness graph, that is, a graph where each pair of nodes is connected to each other. The arcs are weighted by the distance between their corresponding nodes. Each graph node <inline-formula id="pone.0163041.e008"><alternatives><graphic xlink:href="pone.0163041.e008.jpg" id="pone.0163041.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is modeled as an <italic>n</italic>-dimensional feature vector, and <italic>w</italic>(<bold>s</bold>,<bold>t</bold>) is a weight (distance) between two graph nodes <bold>x</bold><sub><italic>i</italic></sub> and <bold>x</bold><sub><italic>j</italic></sub> used to weight the arc <inline-formula id="pone.0163041.e009"><alternatives><graphic xlink:href="pone.0163041.e009.jpg" id="pone.0163041.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">t</mml:mi><mml:mo>&#x0232a;</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Mathematically, <italic>w</italic> is a function that takes two graph nodes and returns the distance between them, that is, <inline-formula id="pone.0163041.e010"><alternatives><graphic xlink:href="pone.0163041.e010.jpg" id="pone.0163041.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mo>&#x0211c;</mml:mo><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. In this work, a node is the set of <italic>n</italic> features extracted from a particular marmoset vocalization.</p><p>As aforementioned, the training step of the OPF classifier aims at building an optimum-path forest rooted in a set of prototype samples. Let <inline-formula id="pone.0163041.e011"><alternatives><graphic xlink:href="pone.0163041.e011.jpg" id="pone.0163041.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> be that set, such that <inline-formula id="pone.0163041.e012"><alternatives><graphic xlink:href="pone.0163041.e012.jpg" id="pone.0163041.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>&#x02286;</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. A very common and easy way to obtain <inline-formula id="pone.0163041.e013"><alternatives><graphic xlink:href="pone.0163041.e013.jpg" id="pone.0163041.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> would be though a random sampling over the training samples. However, two requirements need to be met: (i) each class should be represented by, at least, one prototype node, and (ii) the prototype&#x02019;s distribution should cover different regions of the feature space. These requirements make the na&#x000ef;ve approach too time-consuming. In order to circumvent this problem, Papa et al. [<xref rid="pone.0163041.ref041" ref-type="bibr">41</xref>] proposed to place prototypes in the regions more prone to errors, that is, nearby the frontier of the classes. The idea is to compute a Minimum Spanning Tree (MST) over the training set, and then mark the connected samples from different classes as prototypes. We say that <inline-formula id="pone.0163041.e014"><alternatives><graphic xlink:href="pone.0163041.e014.jpg" id="pone.0163041.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula> is an optimum set of prototypes when the training step minimizes the classification errors in <inline-formula id="pone.0163041.e015"><alternatives><graphic xlink:href="pone.0163041.e015.jpg" id="pone.0163041.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> [<xref rid="pone.0163041.ref044" ref-type="bibr">44</xref>]. The optimum prototypes are the closest samples of the MST with different labels in <inline-formula id="pone.0163041.e016"><alternatives><graphic xlink:href="pone.0163041.e016.jpg" id="pone.0163041.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.</p><p>Given the prototypes, the next step is to find the smallest path-cost from the prototypes to the remaining training samples in <inline-formula id="pone.0163041.e017"><alternatives><graphic xlink:href="pone.0163041.e017.jpg" id="pone.0163041.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula> to the remaining nodes in <inline-formula id="pone.0163041.e018"><alternatives><graphic xlink:href="pone.0163041.e018.jpg" id="pone.0163041.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>\</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. In this work, we adopted the same path-cost function as Papa et al. [<xref rid="pone.0163041.ref041" ref-type="bibr">41</xref>, <xref rid="pone.0163041.ref042" ref-type="bibr">42</xref>], which computes the maximum arc-weight along a path, as follows:
<disp-formula id="pone.0163041.e019"><alternatives><graphic xlink:href="pone.0163041.e019.jpg" id="pone.0163041.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x027e8;</mml:mo><mml:mi>s</mml:mi><mml:mo>&#x027e9;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mfenced close="" open="{" separators=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x0221e;</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo>&#x027e8;</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x027e9;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>&#x003c0;</italic> &#x022c5; &#x02329;<italic>s</italic>, <italic>t</italic>&#x0232a; stands for the concatenation of path <italic>&#x003c0;</italic> and the arc &#x02329;<italic>s</italic>, <italic>t</italic>&#x0232a;. A path is defined as a sequence of distinct and adjacent samples.</p><p>After computing the optimum-path forest, unseen samples from <inline-formula id="pone.0163041.e020"><alternatives><graphic xlink:href="pone.0163041.e020.jpg" id="pone.0163041.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> can be classified. For each test sample, the classification step consists of connecting the sample to all training nodes, evaluating which training sample offers the optimum-path cost <italic>C</italic> according to <italic>f</italic><sub><italic>max</italic></sub> defined in <xref ref-type="disp-formula" rid="pone.0163041.e019">Eq 1</xref>, that is:
<disp-formula id="pone.0163041.e021"><alternatives><graphic xlink:href="pone.0163041.e021.jpg" id="pone.0163041.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mo>&#x02200;</mml:mo><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
Let <italic>s</italic>* be the training sample that satisfies the above equation. Then, test sample <italic>t</italic> will be assigned the same label as sample <italic>s</italic>*. For the current experiments we used the LibOPF library available at <ext-link ext-link-type="uri" xlink:href="https://github.com/LibOPF/LibOPF">https://github.com/LibOPF/LibOPF</ext-link>.</p></sec><sec id="sec007"><title>Bayesian Classifier</title><p>A Bayesian Classifier estimates the probability that a given vocalization belongs to a certain class. This probability can be derived from Bayes&#x02019; Theorem [<xref rid="pone.0163041.ref045" ref-type="bibr">45</xref>]:
<disp-formula id="pone.0163041.e022"><alternatives><graphic xlink:href="pone.0163041.e022.jpg" id="pone.0163041.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula>
where <italic>p</italic>(<bold>x</bold>|<italic>&#x003c9;</italic><sub><italic>i</italic></sub>) denotes the probability of observing feature vector <bold>x</bold> given the class <italic>&#x003c9;</italic><sub><italic>i</italic></sub>, <italic>P</italic>(<italic>&#x003c9;</italic><sub><italic>i</italic></sub>) is the prior probability of class <italic>&#x003c9;</italic><sub><italic>i</italic></sub>, and <italic>p</italic>(<bold>x</bold>) is the probability of <bold>x</bold>. In order to estimate <italic>p</italic>(<bold>x</bold>|<italic>&#x003c9;</italic><sub><italic>i</italic></sub>) we assumed that the likelihood function is Gaussian, and could thus estimate its parameters from the dataset [<xref rid="pone.0163041.ref046" ref-type="bibr">46</xref>].</p></sec><sec id="sec008"><title>Multilayer Artificial Neural Network</title><p>An MLP classifier is a feedforward neural network composed of several neuron layers aiming to solve multiclass problems [<xref rid="pone.0163041.ref047" ref-type="bibr">47</xref>]. The input to each layer is a weighted sum of the output from the previous layer. The number of neurons in the first layer equals the number of features of the input, while the number of neurons in the last layer is equal to the number of classes. The neural network assigns a feature vector extracted from a vocalization <bold>x</bold> to the class <italic>&#x003c9;</italic><sub><italic>q</italic></sub> if the <italic>q</italic>-th output neuron has the highest activation. We used the MLP implementation from scikit-learn [<xref rid="pone.0163041.ref048" ref-type="bibr">48</xref>], with two hidden layers of eight and 16 neurons. The network was trained using backpropagation and the Limited-memory BFGS optimization algorithm [<xref rid="pone.0163041.ref049" ref-type="bibr">49</xref>] to update the weights. The learning rate was set to 0.001.</p></sec><sec id="sec009"><title>Support Vector Machines</title><p>While the learning of MLP is based on the principle of empirical risk minimization, the SVM induction process is rooted in the principle of structural risk minimization [<xref rid="pone.0163041.ref050" ref-type="bibr">50</xref>&#x02013;<xref rid="pone.0163041.ref052" ref-type="bibr">52</xref>], aiming at establishing an optimal discriminative function between two classes of patterns while accomplishing the trade-off between generalization and overfitting. The SVM training algorithm constructs the optimal hyperplane separating the two classes [<xref rid="pone.0163041.ref050" ref-type="bibr">50</xref>]. In order to extend from linear to nonlinear classification the <italic>kernel trick</italic> is used [<xref rid="pone.0163041.ref051" ref-type="bibr">51</xref>], where kernel functions nonlinearly map input data into high-dimensional feature spaces in a computationally-efficient manner.</p><p>For classification problems with multiple classes, two approaches are commonly used for binary SVMs, <italic>one-against-one</italic> and <italic>one-against-all</italic> [<xref rid="pone.0163041.ref053" ref-type="bibr">53</xref>]. Both strategies tend to lead to similar results in terms of classification accuracy, but the former, which was the one adopted here usually requires shorter training time, although incurring a higher number of binary decompositions.</p><p>For the current experiments, we used the LibSVM library [<xref rid="pone.0163041.ref054" ref-type="bibr">54</xref>] (available at <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</ext-link>). The hyperparameters C and <italic>&#x003c3;</italic> of the SVM classifier were determined via a 5-fold cross-validation grid-search in the ranges [2<sup>&#x02212;5</sup>, 2<sup>15</sup>] and [2<sup>&#x02212;15</sup>, 2<sup>3</sup>], changing the exponent in steps of two.</p></sec><sec id="sec010"><title>k-Nearest Neighbors</title><p>k-NN is a very simple algorithm that works well in many different applications. In contrast to the OPF, the k-NN uses all training samples as prototypes.</p><p>The k-NN requires an input parameter <italic>k</italic> setting the number of neighbors that contribute to the classification of a sample [<xref rid="pone.0163041.ref055" ref-type="bibr">55</xref>, <xref rid="pone.0163041.ref056" ref-type="bibr">56</xref>]. In order to classify a test sample <italic>t</italic>, the majority label in a region (e.g. sphere or hypercube) containing <italic>k</italic> training samples and centered at <italic>t</italic>, determines <italic>t</italic>&#x02019;s label. Note that for <italic>k</italic> = 1, the testing sample <italic>t</italic> is classified as the class of the closest training sample. For the current experiments, we defined the value of <italic>k</italic> as the best value of a grid-search in the range <inline-formula id="pone.0163041.e023"><alternatives><graphic xlink:href="pone.0163041.e023.jpg" id="pone.0163041.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x0230a;</mml:mo><mml:mfrac><mml:mi>m</mml:mi><mml:mn>5</mml:mn></mml:mfrac><mml:mo>&#x0230b;</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in steps of two; where <italic>m</italic> is the number of training samples.</p></sec><sec id="sec011"><title>Logistic regression</title><p>The logistic regression classifier is essentially a one-layer artificial neural network where the weighted input features are feed to the logistic function. Here, we trained the classifier under a one-against-all scheme, regularized by the L2 norm of the classifier weights. We used the LIBLINEAR [<xref rid="pone.0163041.ref057" ref-type="bibr">57</xref>] implementation of logistic regression.</p></sec><sec id="sec012"><title>AdaBoost</title><p>AdaBoost, short for Adaptive Boosting, is a meta-classifier that trains an ensemble of weak classifiers. It iteratively trains classifiers on the same dataset while adjusting the weights of incorrectly classified samples such that subsequent classifiers focus more on those difficult samples. The resulting ensemble classifier tends to be less susceptible to over-fitting than other classifiers, but it is also known to be sensitive to noisy data and outliers [<xref rid="pone.0163041.ref058" ref-type="bibr">58</xref>]. Here, we used the AdaBoost-SAMME.R algorithm [<xref rid="pone.0163041.ref059" ref-type="bibr">59</xref>] from the from scikit-learn package [<xref rid="pone.0163041.ref048" ref-type="bibr">48</xref>].</p></sec></sec><sec id="sec013"><title>Statistical evaluation metrics</title><p>In regard to the recognition rate, we used an accuracy measure proposed by Papa et al. [<xref rid="pone.0163041.ref041" ref-type="bibr">41</xref>], which is similar to the Kappa index [<xref rid="pone.0163041.ref060" ref-type="bibr">60</xref>], but more restrictive. If, for example, there are two classes of vocalizations with very different sizes and a classifier always assigns the label of the largest class, the average number of correct assignments will be deceivingly high. A better accuracy measure should take into account the high error rate of the smallest class. The accuracy used here is measured by taking into account that classes may have different sizes in <inline-formula id="pone.0163041.e024"><alternatives><graphic xlink:href="pone.0163041.e024.jpg" id="pone.0163041.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. Let us define:
<disp-formula id="pone.0163041.e025"><alternatives><graphic xlink:href="pone.0163041.e025.jpg" id="pone.0163041.e025g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M25"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mfenced close="|" open="|" separators=""><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced><mml:mo>-</mml:mo><mml:mfenced close="|" open="|" separators=""><mml:msubsup><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives><label>(4)</label></disp-formula>
and
<disp-formula id="pone.0163041.e026"><alternatives><graphic xlink:href="pone.0163041.e026.jpg" id="pone.0163041.e026g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M26"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close="|" open="|" separators=""><mml:msubsup><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:mfenced></mml:mfrac><mml:mtext>,</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula>
where <italic>K</italic> stands for the number of classes, <inline-formula id="pone.0163041.e027"><alternatives><graphic xlink:href="pone.0163041.e027.jpg" id="pone.0163041.e027g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M27"><mml:mo>|</mml:mo><mml:msubsup><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>|</mml:mo></mml:math></alternatives></inline-formula> concerns the number of samples in <inline-formula id="pone.0163041.e028"><alternatives><graphic xlink:href="pone.0163041.e028.jpg" id="pone.0163041.e028g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M28"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> that come from vocalization class <italic>i</italic>, and <italic>FP</italic><sub><italic>i</italic></sub> and <italic>FN</italic><sub><italic>i</italic></sub> stand for the numbers of false positives and false negatives for class <italic>i</italic>, respectively. That is, <italic>FP</italic><sub><italic>i</italic></sub> is the number of samples from other vocalization classes that were classified as being from the class <italic>i</italic> in <inline-formula id="pone.0163041.e029"><alternatives><graphic xlink:href="pone.0163041.e029.jpg" id="pone.0163041.e029g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M29"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>, and <italic>FN</italic><sub><italic>i</italic></sub> is the number of samples from the class <italic>i</italic> that were incorrectly classified as being from other classes in <inline-formula id="pone.0163041.e030"><alternatives><graphic xlink:href="pone.0163041.e030.jpg" id="pone.0163041.e030g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M30"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. The error terms <italic>e</italic><sub><italic>i</italic>,1</sub> and <italic>e</italic><sub><italic>i</italic>,2</sub> are then used to define the total error from class <italic>i</italic>:
<disp-formula id="pone.0163041.e031"><alternatives><graphic xlink:href="pone.0163041.e031.jpg" id="pone.0163041.e031g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M31"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(6)</label></disp-formula>
Finally, the accuracy <italic>Acc</italic> is then defined as follows:
<disp-formula id="pone.0163041.e032"><alternatives><graphic xlink:href="pone.0163041.e032.jpg" id="pone.0163041.e032g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M32"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(7)</label></disp-formula></p><p>Sensitivity (<italic>Se</italic>), often called recall, is the ratio of the number of correctly classified vocalizations from a given class and the total number of vocalizations in that class (including misclassified vocalizations):
<disp-formula id="pone.0163041.e033"><alternatives><graphic xlink:href="pone.0163041.e033.jpg" id="pone.0163041.e033g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M33"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>+</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(8)</label></disp-formula>
where <italic>TP</italic> and <italic>FN</italic> are the number of vocalizations from a given class that were correctly or incorrectly classified, respectively.</p><p>Positive predictive value (<italic>PPV</italic>), often called precision, is the ratio between the correctly classified vocalizations from a given class and the total number of vocalizations classified as pertaining to that class:
<disp-formula id="pone.0163041.e034"><alternatives><graphic xlink:href="pone.0163041.e034.jpg" id="pone.0163041.e034g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M34"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>+</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(9)</label></disp-formula>
where FP denotes the number of vocalizations incorrectly classified as belonging to the considered class.</p><p>Finally, as a more global performance metric, we calculated the averaged F<sub>1</sub>-score for all eight classes. The F<sub>1</sub>-score for a given class is calculated as the harmonic mean of the <italic>Se</italic> and <italic>PPV</italic> values for that class:
<disp-formula id="pone.0163041.e035"><alternatives><graphic xlink:href="pone.0163041.e035.jpg" id="pone.0163041.e035g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M35"><mml:mrow><mml:msub><mml:mi mathvariant="italic">F</mml:mi><mml:mn mathvariant="italic">1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(10)</label></disp-formula>
These four metrics allow us to reliably evaluate the performance of the classification algorithms considered in this work. The performance metrics are reported as the averages over 100 repetitions of classifier training and testing. All training and testing of the classification algorithms was done on a computer with an Intel i7 5500U processor with 8GB of RAM using Linux as operational system. A Python script to reproduce the results is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kalleknast/call_class">https://github.com/kalleknast/call_class</ext-link>.</p></sec></sec><sec sec-type="results" id="sec014"><title>Results</title><p>In order to find a robust method for the automatic classification of marmoset vocalizations, we compared the classification performance of seven different algorithms. In addition, we further explored different configurations of both OPF and SVM. OPF was tested using the following distance metrics: Euclidean, Manhattan, Canberra, Chi-Square and Bray-Curtis, and SVM was tested with linear, radial basis function and polynomial kernels. The goal was to find an algorithm that could be successfully trained on small sets of primate vocalization data. For this reason, we split our original dataset into training sets of increasing sizes, ranging from 10% to 90% of the original dataset. We found good performance of all algorithms except AdaBoost, Naive Bayes, SVM when using linear and polynomial kernels, and OPF when using Chi-Square, Bray-Curtis and Canberra distance metrics (see <xref rid="pone.0163041.t002" ref-type="table">Table 2</xref> and <xref ref-type="fig" rid="pone.0163041.g002">Fig 2</xref>). These poorly performing algorithms were excluded from further consideration. SVM, k-NN and OPF using Euclidean and Manhattan distances performed similarly, and well above chance (accuracy &#x02248; 0.5 and F<sub>1</sub>-score &#x02248; 0.5) using as little as 10% of the original dataset, and reaching an accuracy around 0.8 when trained on 90% of the data. However, in spite of the good performance, <xref ref-type="fig" rid="pone.0163041.g002">Fig 2</xref> shows that classification accuracy keeps improving with training set size, suggesting that adding even more training data would further improve performance.</p><fig id="pone.0163041.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.g002</object-id><label>Fig 2</label><caption><title>The effect of training set size on classification performance.</title><p>For the sake of visual clarity, the results of OPF using the distance metrics Bray-Curtis and Chi-Square, and SVM using linear and polynomial kernels are excluded.</p></caption><graphic xlink:href="pone.0163041.g002"/></fig><table-wrap id="pone.0163041.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.t002</object-id><label>Table 2</label><caption><title>Classification of all eight classes of vocalizations using different algorithms.</title><p>90% of the samples were used for the training set. Time refers to the time required to classify one sample in milliseconds.</p></caption><alternatives><graphic id="pone.0163041.t002g" xlink:href="pone.0163041.t002"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" colspan="2" rowspan="1">F<sub>1</sub>-score</th><th align="left" colspan="2" rowspan="1">Accuracy</th><th align="left" rowspan="1" colspan="1">Time (ms)</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Mean</th><th align="left" rowspan="1" colspan="1">SEM</th><th align="left" rowspan="1" colspan="1">Mean</th><th align="left" rowspan="1" colspan="1">SEM</th><th align="left" rowspan="1" colspan="1">Mean</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">MLP</td><td align="char" char="." rowspan="1" colspan="1">0.757</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.744</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">145.6</td></tr><tr><td align="left" rowspan="1" colspan="1">OPF&#x02013;Chi-Square</td><td align="char" char="." rowspan="1" colspan="1">0.257</td><td align="char" char="." rowspan="1" colspan="1">0.009</td><td align="char" char="." rowspan="1" colspan="1">0.197</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">24.4</td></tr><tr><td align="left" rowspan="1" colspan="1">OPF&#x02013;Bray Curtis</td><td align="char" char="." rowspan="1" colspan="1">0.466</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.421</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">6.9</td></tr><tr><td align="left" rowspan="1" colspan="1">OPF&#x02013;Canberra</td><td align="char" char="." rowspan="1" colspan="1">0.622</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.595</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">24.5</td></tr><tr><td align="left" rowspan="1" colspan="1">OPF&#x02013;Euclidean</td><td align="char" char="." rowspan="1" colspan="1">0.840</td><td align="char" char="." rowspan="1" colspan="1">0.007</td><td align="char" char="." rowspan="1" colspan="1">0.832</td><td align="char" char="." rowspan="1" colspan="1">0.008</td><td align="char" char="." rowspan="1" colspan="1">7.7</td></tr><tr><td align="left" rowspan="1" colspan="1">OPF&#x02013;Manhattan</td><td align="char" char="." rowspan="1" colspan="1">0.818</td><td align="char" char="." rowspan="1" colspan="1">0.008</td><td align="char" char="." rowspan="1" colspan="1">0.805</td><td align="char" char="." rowspan="1" colspan="1">0.009</td><td align="char" char="." rowspan="1" colspan="1">10.7</td></tr><tr><td align="left" rowspan="1" colspan="1">k-NN</td><td align="char" char="." rowspan="1" colspan="1">0.842</td><td align="char" char="." rowspan="1" colspan="1">0.007</td><td align="char" char="." rowspan="1" colspan="1">0.833</td><td align="char" char="." rowspan="1" colspan="1">0.008</td><td align="char" char="." rowspan="1" colspan="1">465.1</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM</td><td align="char" char="." rowspan="1" colspan="1">0.852</td><td align="char" char="." rowspan="1" colspan="1">0.008</td><td align="char" char="." rowspan="1" colspan="1">0.843</td><td align="char" char="." rowspan="1" colspan="1">0.009</td><td align="char" char="." rowspan="1" colspan="1">115.2</td></tr><tr><td align="left" rowspan="1" colspan="1">Naive Bayes</td><td align="char" char="." rowspan="1" colspan="1">0.507</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.475</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">252.0</td></tr><tr><td align="left" rowspan="1" colspan="1">AdaBoost</td><td align="char" char="." rowspan="1" colspan="1">0.603</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">0.571</td><td align="char" char="." rowspan="1" colspan="1">0.013</td><td align="char" char="." rowspan="1" colspan="1">10173.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Logistic regression</td><td align="char" char="." rowspan="1" colspan="1">0.744</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.741</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">89.6</td></tr></tbody></table></alternatives></table-wrap><p>The OPF algorithm configured with the Euclidean distance metric was selected for further analysis since it yielded better or comparable classification performance on both the smallest and the largest datasets (top accuracy &#x02248; 0.83 and F<sub>1</sub>-score &#x02248; 0.84). It is parameter free and thus, easy to train, and computation time is an order of magnitude less than for SVM and k-NN (see <xref rid="pone.0163041.t002" ref-type="table">Table 2</xref>). <xref rid="pone.0163041.t003" ref-type="table">Table 3</xref> presents the results as a confusion matrix.</p><table-wrap id="pone.0163041.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.t003</object-id><label>Table 3</label><caption><title>Confusion matrix considering the classification of all eight classes of vocalizations using OPF with Manhattan distance and 90% of the samples for training set.</title></caption><alternatives><graphic id="pone.0163041.t003g" xlink:href="pone.0163041.t003"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="center" colspan="8" rowspan="1"><bold>True class[%]</bold></td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"><bold>Alarm</bold></td><td align="left" rowspan="1" colspan="1"><bold>Chirp</bold></td><td align="left" rowspan="1" colspan="1"><bold>Loud-shrill</bold></td><td align="left" rowspan="1" colspan="1"><bold>Phee</bold></td><td align="left" rowspan="1" colspan="1"><bold>Seep</bold></td><td align="left" rowspan="1" colspan="1"><bold>Trill</bold></td><td align="left" rowspan="1" colspan="1"><bold>Tsik</bold></td><td align="left" rowspan="1" colspan="1"><bold>Twitter</bold></td></tr><tr><td align="left" rowspan="8" colspan="1"><bold>Classified as[%]</bold></td><td align="left" rowspan="1" colspan="1"><bold>Alarm</bold></td><td align="left" rowspan="1" colspan="1">78.4</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">4.7</td><td align="left" rowspan="1" colspan="1">4.1</td><td align="left" rowspan="1" colspan="1">4.3</td><td align="left" rowspan="1" colspan="1">0.7</td><td align="left" rowspan="1" colspan="1">2.5</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Chirp</bold></td><td align="left" rowspan="1" colspan="1">3.9</td><td align="left" rowspan="1" colspan="1">89.5</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">1.6</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">4.4</td><td align="left" rowspan="1" colspan="1">7.0</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Loud-shrill</bold></td><td align="left" rowspan="1" colspan="1">5.9</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">75.3</td><td align="left" rowspan="1" colspan="1">5.9</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">19.2</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.4</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Phee</bold></td><td align="left" rowspan="1" colspan="1">4.3</td><td align="left" rowspan="1" colspan="1">0.1</td><td align="left" rowspan="1" colspan="1">4.3</td><td align="left" rowspan="1" colspan="1">83.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.9</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.5</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Seep</bold></td><td align="left" rowspan="1" colspan="1">5.7</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">90.6</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">8.3</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Trill</bold></td><td align="left" rowspan="1" colspan="1">1.6</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">15.7</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">74.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Tsik</bold></td><td align="left" rowspan="1" colspan="1">0.2</td><td align="left" rowspan="1" colspan="1">1.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">5.1</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">77.5</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Twitter</bold></td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">9.4</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">5.4</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">5.2</td><td align="left" rowspan="1" colspan="1">7.3</td><td align="left" rowspan="1" colspan="1">92.1</td></tr></tbody></table></alternatives></table-wrap><p>We opted for a hierarchical strategy to investigate how well all 11 types of vocalization in the dataset could be classified. Such approach was performed through sorting the Phee and Tsik classes into three and two sub-classes, respectively. The Phee class was sorted into Phee-2, Phee-3 and Phee-4 depending on how many separate whistles the call contained (<xref ref-type="fig" rid="pone.0163041.g001">Fig 1(d), 1(e) and 1(f)</xref>, respectively). The Tsik call was sorted into the sub-classes Tsik and Tsik-ek depending on the presence of the harmonic &#x0201c;ek&#x0201d; component following the &#x0201c;tsik&#x0201d; in the Tsik-ek calls [<xref rid="pone.0163041.ref037" ref-type="bibr">37</xref>] (<xref ref-type="fig" rid="pone.0163041.g001">Fig 1(i) and 1(j)</xref>, respectively).</p><p>
<xref rid="pone.0163041.t004" ref-type="table">Table 4</xref> presents a confusion matrix of the results from the Phee calls using OPF with Euclidean distance metric. Overall, the classification accuracy (56%) was not quite as good as when classifying the eight principal classes. Phee-2 was correctly classified in 70% of the cases, and Phee-3 was classified at 46%, where the remaining 64% were misclassified as Phee-2 15% or Phee-3 39%. Finally, Phee-4 was accurately classified in 52% of the samples, where the remaining 48% were misclassified as Phee-3 34% or Phee-2 15%. Classification of the Phee sub-class from the eight principal classes resulted in a compounded accuracy of 58% for Phee-2, 38% for Phee-3, and 42% for Phee-4, since accuracy for the principal Phee class was 83% (see Tables <xref rid="pone.0163041.t003" ref-type="table">3</xref> and <xref rid="pone.0163041.t004" ref-type="table">4</xref>).</p><table-wrap id="pone.0163041.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.t004</object-id><label>Table 4</label><caption><title>Confusion matrix considering the classification of the principal Phee class into sub-classes using OPF with Euclidean distance metric and 90% of the samples for training set.</title></caption><alternatives><graphic id="pone.0163041.t004g" xlink:href="pone.0163041.t004"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="center" colspan="3" rowspan="1"><bold>True class[%]</bold></td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"><bold>Phee-2</bold></td><td align="left" rowspan="1" colspan="1"><bold>Phee-3</bold></td><td align="left" rowspan="1" colspan="1"><bold>Phee-4</bold></td></tr><tr><td align="left" rowspan="3" colspan="1"><bold>Classified as[%]</bold></td><td align="left" rowspan="1" colspan="1"><bold>Phee-2</bold></td><td align="left" rowspan="1" colspan="1">70.4</td><td align="left" rowspan="1" colspan="1">19.8</td><td align="left" rowspan="1" colspan="1">9.7</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Phee-3</bold></td><td align="left" rowspan="1" colspan="1">14.8</td><td align="left" rowspan="1" colspan="1">46.2</td><td align="left" rowspan="1" colspan="1">39.1</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Phee-4</bold></td><td align="left" rowspan="1" colspan="1">14.8</td><td align="left" rowspan="1" colspan="1">34.0</td><td align="left" rowspan="1" colspan="1">51.2</td></tr></tbody></table></alternatives></table-wrap><p>
<xref rid="pone.0163041.t005" ref-type="table">Table 5</xref> presents a confusion matrix of the results using OPF with Euclidean distance metric for Tsik vocalizations. The overall accuracy was 83%, where the accuracy for sub-class Tsik was 91%, and that of Tsik-ek was 76%. The compounded accuracies for the sub-classes Tsik and Tsik-ek were 70% and 58%, respectively (see Tables <xref rid="pone.0163041.t003" ref-type="table">3</xref> and <xref rid="pone.0163041.t005" ref-type="table">5</xref>).</p><table-wrap id="pone.0163041.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0163041.t005</object-id><label>Table 5</label><caption><title>Confusion matrix for the classification of the principal Tsik class into sub-classes using OPF with Manhattan distance metric and 90% of the samples for training set.</title></caption><alternatives><graphic id="pone.0163041.t005g" xlink:href="pone.0163041.t005"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" colspan="2" rowspan="1"><bold>True class[%]</bold></td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"><bold>Tsik</bold></td><td align="left" rowspan="1" colspan="1"><bold>Tsik-ek</bold></td></tr><tr><td align="left" rowspan="2" colspan="1"><bold>Classified as[%]</bold></td><td align="left" rowspan="1" colspan="1"><bold>Tsik</bold></td><td align="left" rowspan="1" colspan="1">90.5</td><td align="left" rowspan="1" colspan="1">24.5</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Tsik-ek</bold></td><td align="left" rowspan="1" colspan="1">9.5</td><td align="left" rowspan="1" colspan="1">75.5</td></tr></tbody></table></alternatives></table-wrap></sec><sec sec-type="conclusions" id="sec015"><title>Discussion</title><p>In this work, we presented methods for the automatic classification of commonly occurring vocalizations of the common marmoset. The provided dataset can be used for acoustic analysis, further algorithm development and playback experiments. The method presented should enable the online monitoring of vocal activity in colonies of captive marmosets, so as to provide valuable information about the colony&#x02019;s health and well-being. Further, the method allows for interactive experimental designs, in which different actions can be triggered depending on the vocal behavior of the subjects.</p><p>Since recording and manually labeling data is both labor-intensive and time-consuming, the most important factor when selecting a classification algorithm is how well it performs on a small amount of data. <xref ref-type="fig" rid="pone.0163041.g002">Fig 2</xref> demonstrates clear advantages for the k-NN, SVM and OPF (using Euclidean and Manhattan distances) algorithms. These algorithms perform best on both the least and the greatest amount of data.</p><p>Another factor to consider is how easy an algorithm is to use. Most algorithms have hyperparameters that require careful optimization for good performance. The standard way of doing this is to repeatedly re-train the algorithm on a manually specified range of hyperparameter values, each time evaluating classification performance on data that were not included among the training data. Both the k-NN and SVM algorithms require such hyperparameter optimization, whereas the OPF algorithm is parameterless, making it easier to use.</p><p>Under some circumstances, the time required for classification can become important. Algorithms that do not require much computational resources are valuable when real-time classification is necessary, and especially when the computations are performed on small single board computers or embedded devices with limited capacity. Examples of this are on-site portable audio acquisition and analysis [<xref rid="pone.0163041.ref061" ref-type="bibr">61</xref>], or home-cage vocal conditioning systems [<xref rid="pone.0163041.ref062" ref-type="bibr">62</xref>]. The OPF algorithm requires an order of magnitude less time than comparably performing k-NN and SVM algorithms and is thus suitable for these goals.</p><p>The proposed methods are mainly suited for laboratory conditions where audio can be recorded under consistent conditions, and with high signal-to-noise ratio. Such conditions are unlikely to be available under field conditions. Robust classification within field conditions would probable require much improved pre-processing before the classification step.</p></sec></body><back><ack><p>We thank Daniel Y. Takahashi for comments on the manuscript, and Thamiris Botelho, William Wood, Josy Pontes, Janaina Figueiredo, Bruno Soares, Maria Bernardete de Souza and the Primatology Center of UFRN for help with various aspects of housing &#x00026; specialized marmoset care. Research equipment and HKT&#x02019;s stipend were funded by CNPq/BJT grant # 402422/2012-0 to SR and HKT. The last author thanks CNPq for providing financial support through a grants # 470501/2013-8 and # 301928/2014-2. The second author thanks the FAPESP Research, Innovation and Dissemination Center for Neuromathematics (grant # 2013/07699-0, S. Paulo Research Foundation). The fourth author thanks FAPESP grants # 2014/16250-9 and # 2015/50319-9, as well as CNPq grants # 470571/2013-6 and # 306166/2014-3.</p></ack><ref-list><title>References</title><ref id="pone.0163041.ref001"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Solomon</surname><given-names>SG</given-names></name>, <name><surname>Rosa</surname><given-names>MG</given-names></name>. <article-title>A simpler primate brain: the visual system of the marmoset monkey</article-title>, <source>Front Neural Circuits</source>. <year>2014</year>; <volume>8</volume>: <fpage>1</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.3389/fncir.2014.00096</pub-id><pub-id pub-id-type="pmid">24478635</pub-id></mixed-citation></ref><ref id="pone.0163041.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Okano</surname><given-names>H</given-names></name>, <name><surname>Miyawaki</surname><given-names>A</given-names></name>, <name><surname>Kasai</surname><given-names>K</given-names></name>. <article-title>Brain/MINDS: Brain-mapping project in Japan</article-title>, <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2015</year>; <volume>370</volume>: <fpage>1</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2014.0310</pub-id></mixed-citation></ref><ref id="pone.0163041.ref003"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Kishi</surname><given-names>N</given-names></name>, <name><surname>Sato</surname><given-names>K</given-names></name>, <name><surname>Sasaki</surname><given-names>E</given-names></name>, <name><surname>Okano</surname><given-names>H</given-names></name>. <article-title>Common marmoset as a new model animal for neuroscience research and genome editing technology</article-title>. <source>Dev Growth Differ</source>. <year>2014</year>; <volume>56</volume>: <fpage>53</fpage>&#x02013;<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1111/dgd.12109</pub-id>
<pub-id pub-id-type="pmid">24387631</pub-id></mixed-citation></ref><ref id="pone.0163041.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Hart</surname><given-names>BA</given-names></name>, <name><surname>van Kooyk</surname><given-names>Y</given-names></name>, <name><surname>Geurts</surname><given-names>JJ</given-names></name>, <name><surname>Gran</surname><given-names>B</given-names></name>. <article-title>The primate autoimmune encephalomyelitis model; a bridge between mouse and man</article-title>. <source>Ann Clin Transl Neurol</source>. <year>2015</year>; <volume>2</volume>: <fpage>581</fpage>&#x02013;<lpage>593</lpage>. <pub-id pub-id-type="doi">10.1002/acn3.194</pub-id>
<pub-id pub-id-type="pmid">26000330</pub-id></mixed-citation></ref><ref id="pone.0163041.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Horvat</surname><given-names>B</given-names></name>, <name><surname>Berges</surname><given-names>BK</given-names></name>, <name><surname>Lusso</surname><given-names>P</given-names></name>. <article-title>Recent developments in animal models for human herpesvirus 6A and 6B</article-title>. <source>Curr Opin Virol</source>. <year>2014</year>; <volume>9</volume>: <fpage>97</fpage>&#x02013;<lpage>103</lpage>. <pub-id pub-id-type="doi">10.1016/j.coviro.2014.09.012</pub-id>
<pub-id pub-id-type="pmid">25462440</pub-id></mixed-citation></ref><ref id="pone.0163041.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Scanga</surname><given-names>CA</given-names></name>, <name><surname>Flynn</surname><given-names>JL</given-names></name>. <article-title>Modeling tuberculosis in nonhuman primates</article-title>. <source>Cold Spring Harb Perspect Med</source>. <year>2014</year>; <volume>4</volume>: <fpage>a018564</fpage>
<pub-id pub-id-type="doi">10.1101/cshperspect.a018564</pub-id>
<pub-id pub-id-type="pmid">25213189</pub-id></mixed-citation></ref><ref id="pone.0163041.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Okano</surname><given-names>H</given-names></name>, <name><surname>Hikishima</surname><given-names>K</given-names></name>, <name><surname>Iriki</surname><given-names>A</given-names></name>, <name><surname>Sasaki</surname><given-names>E</given-names></name>. <article-title>The common marmoset as a novel animal model system for biomedical and neuroscience research applications</article-title>. <source>Semin Fetal Neonatal Med</source>. <year>2012</year>; <volume>17</volume>: <fpage>336</fpage>&#x02013;<lpage>340</lpage>. <pub-id pub-id-type="doi">10.1016/j.siny.2012.07.002</pub-id>
<pub-id pub-id-type="pmid">22871417</pub-id></mixed-citation></ref><ref id="pone.0163041.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Wang</surname><given-names>X</given-names></name>. <article-title>On cortical coding of vocal communication sounds in primates</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2000</year>; <volume>97</volume>: <fpage>11843</fpage>&#x02013;<lpage>11849</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.97.22.11843</pub-id>
<pub-id pub-id-type="pmid">11050218</pub-id></mixed-citation></ref><ref id="pone.0163041.ref009"><label>9</label><mixed-citation publication-type="book">
<name><surname>Snowdon</surname><given-names>CT</given-names></name>. <chapter-title>Social processes in the evolution of complex cognition and communication</chapter-title>, In: <name><surname>Oller</surname><given-names>D</given-names></name>, <name><surname>Griebel</surname><given-names>U</given-names></name>, editors. <source>Evolution of Communication Systems: A Comparative Approach, The Vienna series in theoretical biology</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2004</year> pp. <fpage>131</fpage>&#x02013;<lpage>150</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref010"><label>10</label><mixed-citation publication-type="book">
<name><surname>Jones</surname><given-names>CB</given-names></name>. <chapter-title>Quantitative analysis of marmoset vocal communication</chapter-title> In: <name><surname>Pryce</surname><given-names>C.</given-names></name>, <name><surname>Scott</surname><given-names>L</given-names></name>, <name><surname>Schnell</surname><given-names>C</given-names></name>, editors. <source>Marmosets and tamarins in biological and biomedical research: proceedings of a workshop</source>. <publisher-loc>Salisbury</publisher-loc>: <publisher-name>DSSD Imagery</publisher-name>; <year>1997</year> pp. <fpage>145</fpage>&#x02013;<lpage>151</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref011"><label>11</label><mixed-citation publication-type="other">Bezerra BM. Vocaliza&#x000e7;&#x000e3;o do sag&#x000fc;i comum: influ&#x000ea;ncias sociais e ontog&#x000ea;nicas em ambiente natural. Ph.D. Thesis, Universidade Federal de Pernambuco. 2006. Available: <ext-link ext-link-type="uri" xlink:href="http://repositorio.ufpe.br/handle/123456789/666">http://repositorio.ufpe.br/handle/123456789/666</ext-link></mixed-citation></ref><ref id="pone.0163041.ref012"><label>12</label><mixed-citation publication-type="book">
<name><surname>Stevenson</surname><given-names>MF</given-names></name>, <name><surname>Rylands</surname><given-names>AB</given-names></name>. <chapter-title>The marmosets, genus <italic>callithrix</italic></chapter-title> In: <name><surname>Mittermeier</surname><given-names>RA</given-names></name>, <name><surname>Rylands</surname><given-names>AB</given-names></name>, <name><surname>Coimbra-Filho</surname><given-names>AF</given-names></name>, <name><surname>da Fonseca</surname><given-names>GAB</given-names></name>, editors. <source>Ecology and Behavior of Neotropical Primates</source>, <volume>Vol. 2</volume>
<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>World Wildlife Fund</publisher-name>; <year>1988</year> pp. <fpage>131</fpage>&#x02013;<lpage>222</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref013"><label>13</label><mixed-citation publication-type="book">
<name><surname>Winter</surname><given-names>M</given-names></name>. <chapter-title>Some aspects of the ontogeny of vocalizations of hand-reared common marmosets</chapter-title> In: <name><surname>Rothe</surname><given-names>H</given-names></name>, <name><surname>Wolters</surname><given-names>HJ</given-names></name>, <name><surname>Hearn</surname><given-names>JP</given-names></name>, editors. <source>Hearn Biology and behaviour of marmosets: Proceedings of the Marmoset Workshop</source>. <publisher-loc>Gottingen</publisher-loc>: <publisher-name>H. Rothe, Eigenverlag Hartmut</publisher-name>; <year>1978</year> pp. <fpage>127</fpage>&#x02013;<lpage>139</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Bezerra</surname><given-names>BM</given-names></name>, <name><surname>Souto</surname><given-names>AS</given-names></name>, <name><surname>Oliveira</surname><given-names>MAB</given-names></name>, <name><surname>Halsey</surname><given-names>LG</given-names></name>. <article-title>Vocalisations of wild common marmosets are influenced by diurnal and ontogenetic factors</article-title>. <source>Primates</source>. <year>2009</year>; <volume>50</volume>: <fpage>231</fpage>&#x02013;<lpage>237</lpage>. <pub-id pub-id-type="doi">10.1007/s10329-009-0132-7</pub-id>
<pub-id pub-id-type="pmid">19224328</pub-id></mixed-citation></ref><ref id="pone.0163041.ref015"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Stowell</surname><given-names>D</given-names></name>, <name><surname>Plumbley</surname><given-names>MD</given-names></name>. <article-title>Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</article-title>. <source>PeerJ</source>. <year>2014</year>; <volume>2</volume>: <fpage>e488</fpage>
<pub-id pub-id-type="doi">10.7717/peerj.488</pub-id>
<pub-id pub-id-type="pmid">25083350</pub-id></mixed-citation></ref><ref id="pone.0163041.ref016"><label>16</label><mixed-citation publication-type="journal">
<name><surname>McIlraith</surname><given-names>A</given-names></name>, <name><surname>Card</surname><given-names>H</given-names></name>. <article-title>Birdsong recognition using backpropagation and multivariate statistics</article-title>. <source>IEEE Trans Signal Process</source>. <year>1997</year>; <volume>45</volume>: <fpage>2740</fpage>&#x02013;<lpage>2748</lpage>. <pub-id pub-id-type="doi">10.1109/78.650100</pub-id></mixed-citation></ref><ref id="pone.0163041.ref017"><label>17</label><mixed-citation publication-type="other">Lakshminarayanan B, Raich R, Fern X. A syllable-level probabilistic framework for bird species identification. In: Machine Learning and Applications (ICMLA), International Conference on, IEEE; 2009. pp. 53&#x02013;59.</mixed-citation></ref><ref id="pone.0163041.ref018"><label>18</label><mixed-citation publication-type="other">Damoulas T, Henry S, Farnsworth A, Lanzone M, Gomes C. Bayesian classification of flight calls with a novel dynamic time warping kernel. In: Machine Learning and Applications (ICMLA), Ninth International Conference on, IEEE. 2010; pp. 424&#x02013;429.</mixed-citation></ref><ref id="pone.0163041.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Briggs</surname><given-names>F</given-names></name>, <name><surname>Lakshminarayanan</surname><given-names>B</given-names></name>, <name><surname>Neal</surname><given-names>L</given-names></name>, <name><surname>Fern</surname><given-names>XZ</given-names></name>, <name><surname>Raich</surname><given-names>R</given-names></name>, <name><surname>Hadley</surname><given-names>SJ</given-names></name>, <etal>et al</etal>
<article-title>Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach</article-title>. <source>J Acoust Soc Am</source>. <year>2012</year>; <volume>131</volume>: <fpage>4640</fpage>&#x02013;<lpage>4650</lpage>. <pub-id pub-id-type="doi">10.1121/1.4707424</pub-id>
<pub-id pub-id-type="pmid">22712937</pub-id></mixed-citation></ref><ref id="pone.0163041.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Moln&#x000e1;r</surname><given-names>C</given-names></name>, <name><surname>Kaplan</surname><given-names>F</given-names></name>, <name><surname>Roy</surname><given-names>P</given-names></name>, <name><surname>Pachet</surname><given-names>F</given-names></name>, <name><surname>Pongr&#x000e1;cz</surname><given-names>P</given-names></name>, <name><surname>D&#x000f3;ka</surname><given-names>A</given-names></name>, <etal>et al</etal>
<article-title>Classification of dog barks: A machine learning approach</article-title>. <source>Anim Cogn</source>. <year>2008</year>; <volume>11</volume>: <fpage>389</fpage>&#x02013;<lpage>400</lpage>. <pub-id pub-id-type="doi">10.1007/s10071-007-0129-9</pub-id>
<pub-id pub-id-type="pmid">18197442</pub-id></mixed-citation></ref><ref id="pone.0163041.ref021"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Jarvis</surname><given-names>S</given-names></name>, <name><surname>DiMarzio</surname><given-names>N</given-names></name>, <name><surname>Morrissey</surname><given-names>R</given-names></name>, <name><surname>Moretti</surname><given-names>D</given-names></name>. <article-title>A novel multi-class support vector machine classifier for automated classification of beaked whales and other small odontocetes</article-title>. <source>Can Acous</source>. <year>2008</year>; <volume>36</volume>: <fpage>34</fpage>&#x02013;<lpage>40</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref022"><label>22</label><mixed-citation publication-type="other">Weninger F, Schuller B. Audio recognition in the wild: Static and dynamic classification on a real-world database of animal vocalizations. In: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2011. pp. 337&#x02013;340.</mixed-citation></ref><ref id="pone.0163041.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Acevedo</surname><given-names>MA</given-names></name>, <name><surname>Corrada-Bravo</surname><given-names>CJ</given-names></name>, <name><surname>Corrada-Bravo</surname><given-names>H</given-names></name>, <name><surname>Villanueva-Rivera</surname><given-names>LJ</given-names></name>, <name><surname>Aide</surname><given-names>TM</given-names></name>. <article-title>Automated classification of bird and amphibian calls using machine learning: A comparison of methods</article-title>. <source>Ecol Inform</source>. <year>2009</year>; <volume>4</volume>: <fpage>206</fpage>&#x02013;<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1016/j.ecoinf.2009.06.005</pub-id></mixed-citation></ref><ref id="pone.0163041.ref024"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Huang</surname><given-names>CJ</given-names></name>, <name><surname>Yang</surname><given-names>YJ</given-names></name>, <name><surname>Yang</surname><given-names>DX</given-names></name>, <name><surname>Chen</surname><given-names>YJ</given-names></name>. <article-title>Frog classification using machine learning techniques</article-title>. <source>Expert Syst Appl</source>. <year>2009</year>; <volume>36</volume>: <fpage>3737</fpage>&#x02013;<lpage>3743</lpage>. <pub-id pub-id-type="doi">10.1016/j.eswa.2008.02.059</pub-id></mixed-citation></ref><ref id="pone.0163041.ref025"><label>25</label><mixed-citation publication-type="other">Dayou J, Han NC, Mun HC, Ahmad AH, Muniandy SV, Dalimin MN. Classification and identification of frog sound based on entropy approach. In: 2011 International Conference on Life Science and Technology, Vol 3; 2011. pp. 184&#x02013;187.</mixed-citation></ref><ref id="pone.0163041.ref026"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Mielke</surname><given-names>A</given-names></name>, <name><surname>Zuberb&#x000fc;hler</surname><given-names>K</given-names></name>. <article-title>A method for automated individual, species and call type recognition in free-ranging animals</article-title>. <source>Anim Behav</source>. <year>2013</year>; <volume>86</volume>: <fpage>475</fpage>&#x02013;<lpage>482</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2013.04.017</pub-id></mixed-citation></ref><ref id="pone.0163041.ref027"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Pozzi</surname><given-names>L</given-names></name>, <name><surname>Gamba</surname><given-names>M</given-names></name>, <name><surname>Giacoma</surname><given-names>C</given-names></name>. <article-title>The use of artificial neural networks to classify primate vocalizations: A pilot study on black lemurs</article-title>. <source>Am J Primatol</source>. <year>2010</year>; <volume>72</volume>: <fpage>337</fpage>&#x02013;<lpage>348</lpage>. <pub-id pub-id-type="pmid">20034021</pub-id></mixed-citation></ref><ref id="pone.0163041.ref028"><label>28</label><mixed-citation publication-type="book">
<name><surname>Pozzi</surname><given-names>L</given-names></name>, <name><surname>Gamba</surname><given-names>M</given-names></name>, <name><surname>Giacoma</surname><given-names>C</given-names></name>. <chapter-title>Artificial neural networks: A new tool for studying lemur vocal communication</chapter-title> In: <source>Leaping Ahead</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2013</year> pp. <fpage>305</fpage>&#x02013;<lpage>313</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref029"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Jones</surname><given-names>BS</given-names></name>, <name><surname>Harris</surname><given-names>DHR</given-names></name>, <name><surname>Catchpole</surname><given-names>CK</given-names></name>. <article-title>The stability of the vocal signature in phee calls of the common marmoset, <italic>callithrix jacchus</italic></article-title>. <source>Am J Primatol</source>. <year>1993</year>; <volume>31</volume>: <fpage>67</fpage>&#x02013;<lpage>75</lpage>. <pub-id pub-id-type="doi">10.1002/ajp.1350310107</pub-id></mixed-citation></ref><ref id="pone.0163041.ref030"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Miller</surname><given-names>CT</given-names></name>, <name><surname>Mandel</surname><given-names>K</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>. <article-title>The communicative content of the common marmoset phee call during antiphonal calling</article-title>. <source>Am J Primatol</source>. <year>2010</year>; <volume>72</volume>: <fpage>974</fpage>&#x02013;<lpage>980</lpage>. <pub-id pub-id-type="doi">10.1002/ajp.20854</pub-id>
<pub-id pub-id-type="pmid">20549761</pub-id></mixed-citation></ref><ref id="pone.0163041.ref031"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Ceugniet</surname><given-names>M</given-names></name>, <name><surname>Izumi</surname><given-names>A</given-names></name>. <article-title>Individual vocal differences of the coo call in japanese monkeys</article-title>. <source>C R Biol</source>. <year>2004</year>; <volume>327</volume>: <fpage>149</fpage>&#x02013;<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1016/j.crvi.2003.11.008</pub-id>
<pub-id pub-id-type="pmid">15060986</pub-id></mixed-citation></ref><ref id="pone.0163041.ref032"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Butynski</surname><given-names>TM</given-names></name>, <name><surname>Chapman</surname><given-names>CA</given-names></name>, <name><surname>Chapman</surname><given-names>LJ</given-names></name>, <name><surname>Weary</surname><given-names>DM</given-names></name>. <article-title>Use of male blue monkey &#x0201c;pyow&#x0201d; calls for long-term individual identification</article-title>. <source>Am J Primatol</source>. <year>1992</year>; <volume>28</volume>: <fpage>183</fpage>&#x02013;<lpage>189</lpage>. <pub-id pub-id-type="doi">10.1002/ajp.1350280303</pub-id></mixed-citation></ref><ref id="pone.0163041.ref033"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Macedonia</surname><given-names>JM</given-names></name>. <article-title>Individuality in a contact call of the ringtailed lemur (<italic>lemur catta</italic>)</article-title>. <source>Am J Primatol</source>. <year>1986</year>; <volume>11</volume>: <fpage>163</fpage>&#x02013;<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1002/ajp.1350110208</pub-id></mixed-citation></ref><ref id="pone.0163041.ref034"><label>34</label><mixed-citation publication-type="journal">
<name><surname>Snowdon</surname><given-names>CT</given-names></name>, <name><surname>Cleveland</surname><given-names>J</given-names></name>, <name><surname>French</surname><given-names>JA</given-names></name>. <article-title>Responses to context- and individual-specific cues in cotton-top tamarin long calls</article-title>. <source>Anim Behav</source>. <year>1983</year>; <volume>31</volume>: <fpage>92</fpage>&#x02013;<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1016/S0003-3472(83)80177-8</pub-id></mixed-citation></ref><ref id="pone.0163041.ref035"><label>35</label><mixed-citation publication-type="other">Bradbury J. Linear predictive coding. 2000. Available: <ext-link ext-link-type="uri" xlink:href="http://my.fit.edu/~vkepuska/ece5525/lpc_paper.pdf">http://my.fit.edu/~vkepuska/ece5525/lpc_paper.pdf</ext-link></mixed-citation></ref><ref id="pone.0163041.ref036"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Epple</surname><given-names>G</given-names></name>. <article-title>Comparative studies on vocalization in marmoset monkeys (<italic>hapalidae</italic>)</article-title>. <source>Folia Primatol</source>. <year>1968</year>; <volume>8</volume>: <fpage>1</fpage>&#x02013;<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1159/000155129</pub-id>
<pub-id pub-id-type="pmid">4966050</pub-id></mixed-citation></ref><ref id="pone.0163041.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Bezerra</surname><given-names>BM</given-names></name>, <name><surname>Souto</surname><given-names>A</given-names></name>. <article-title>Structure and usage of the vocal repertoire of <italic>callithrix jacchus</italic></article-title>. <source>Int J Primatol</source>. <year>2008</year>; <volume>29</volume>: <fpage>671</fpage>&#x02013;<lpage>701</lpage>. <pub-id pub-id-type="doi">10.1007/s10764-008-9250-0</pub-id></mixed-citation></ref><ref id="pone.0163041.ref038"><label>38</label><mixed-citation publication-type="book">
<name><surname>Deng</surname><given-names>L</given-names></name>, <name><surname>O&#x02019;Shaughnessy</surname><given-names>D</given-names></name>. <source>Speech processing: A dynamic and optimization-oriented approach</source>. <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>CRC Press</publisher-name>; <year>2003</year>.</mixed-citation></ref><ref id="pone.0163041.ref039"><label>39</label><mixed-citation publication-type="book">
<name><surname>Jackson</surname><given-names>LB</given-names></name>. <source>Digital filters and signal processing</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>1996</year>.</mixed-citation></ref><ref id="pone.0163041.ref040"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Falc&#x000e3;o</surname><given-names>AX</given-names></name>, <name><surname>Stolfi</surname><given-names>J</given-names></name>, <name><surname>Lotufo</surname><given-names>RA</given-names></name>. <article-title>The image foresting transform: Theory, algorithms, and applications</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2004</year>; <volume>26</volume>: <fpage>19</fpage>&#x02013;<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2004.1261076</pub-id>
<pub-id pub-id-type="pmid">15382683</pub-id></mixed-citation></ref><ref id="pone.0163041.ref041"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Papa</surname><given-names>JP</given-names></name>, <name><surname>Falc&#x000e3;o</surname><given-names>AX</given-names></name>, <name><surname>Suzuki</surname><given-names>CTN</given-names></name>. <article-title>Supervised pattern classification based on Optimum-Path Forest</article-title>. <source>Int J Imaging Syst Technol</source>. <year>2009</year>; <volume>19</volume>: <fpage>120</fpage>&#x02013;<lpage>131</lpage>. <pub-id pub-id-type="doi">10.1002/ima.20188</pub-id></mixed-citation></ref><ref id="pone.0163041.ref042"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Papa</surname><given-names>JP</given-names></name>, <name><surname>Albuquerque</surname><given-names>VHC</given-names></name>, <name><surname>Falc&#x000e3;o</surname><given-names>AX</given-names></name>, <name><surname>Tavares</surname><given-names>JMRS</given-names></name>. <article-title>Efficient supervised Optimum-Path Forest classification for large datasets</article-title>. <source>Pattern Recognit</source>. <year>2012</year>; <volume>45</volume>: <fpage>512</fpage>&#x02013;<lpage>520</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2011.07.013</pub-id></mixed-citation></ref><ref id="pone.0163041.ref043"><label>43</label><mixed-citation publication-type="book">
<name><surname>Papa</surname><given-names>JP</given-names></name>, <name><surname>Falc&#x000e3;o</surname><given-names>AX</given-names></name>. <chapter-title>A new variant of the optimum-path forest classifier</chapter-title> In: <name><surname>Bebis</surname><given-names>G</given-names></name>, <name><surname>Boyle</surname><given-names>R</given-names></name>, <name><surname>Parvin</surname><given-names>B</given-names></name>, <name><surname>Remagnino</surname><given-names>P</given-names></name>, <name><surname>Porikli</surname><given-names>F</given-names></name>, <name><surname>Peters</surname><given-names>J</given-names></name>, <etal>et al</etal>, editors. <source>Advances in visual computing</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2008</year>
<fpage>935</fpage>&#x02013;<lpage>944</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref044"><label>44</label><mixed-citation publication-type="journal">
<name><surname>All&#x000e8;ne</surname><given-names>C</given-names></name>, <name><surname>Audibert</surname><given-names>JY</given-names></name>, <name><surname>Couprie</surname><given-names>M</given-names></name>, <name><surname>Keriven</surname><given-names>R</given-names></name>. <article-title>Some links between extremum spanning forests, watersheds and min-cuts</article-title>. <source>Image Vis Comput</source>. <year>2010</year>; <volume>28</volume>: <fpage>1460</fpage>&#x02013;<lpage>1471</lpage>. <pub-id pub-id-type="doi">10.1016/j.imavis.2009.06.017</pub-id></mixed-citation></ref><ref id="pone.0163041.ref045"><label>45</label><mixed-citation publication-type="book">
<name><surname>Jaynes</surname><given-names>ET</given-names></name>. <source>Probability theory: The logic of science</source>. <publisher-loc>Camebridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>
<year>2003</year>.</mixed-citation></ref><ref id="pone.0163041.ref046"><label>46</label><mixed-citation publication-type="book">
<name><surname>Duda</surname><given-names>RO</given-names></name>, <name><surname>Hart</surname><given-names>PE</given-names></name>, <name><surname>Stork</surname><given-names>DG</given-names></name>. <source>Pattern classification</source>. <edition>2nd ed</edition>
<publisher-loc>New York</publisher-loc>: <publisher-name>Wiley-Interscience Publication</publisher-name>
<year>2000</year>.</mixed-citation></ref><ref id="pone.0163041.ref047"><label>47</label><mixed-citation publication-type="book">
<name><surname>Haykin</surname><given-names>S</given-names></name>. <source>Neural networks: A comprehensive foundation</source>. <edition>2nd ed</edition>
<publisher-name>Prentice Hall</publisher-name>
<year>1999</year>.</mixed-citation></ref><ref id="pone.0163041.ref048"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>V</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Grisel</surname><given-names>O</given-names></name>, <etal>et al</etal>
<article-title>Scikit-learn: Machine learning in Python</article-title>. <source>J Mach Learn Res</source>. <year>2011</year>; <volume>12</volume>: <fpage>2825</fpage>&#x02013;<lpage>2830</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref049"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Byrd</surname><given-names>RH</given-names></name>, <name><surname>Lu</surname><given-names>P</given-names></name>, <name><surname>Nocedal</surname><given-names>J</given-names></name>. <article-title>A limited memory algorithm for bound constrained optimization</article-title>. <source>SIAM J Sci Comput</source>. <year>1995</year>; <volume>16</volume>: <fpage>1190</fpage>&#x02013;<lpage>1208</lpage>. <pub-id pub-id-type="doi">10.1137/0916069</pub-id></mixed-citation></ref><ref id="pone.0163041.ref050"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Vapnik</surname><given-names>VN</given-names></name>. <article-title>An overview of statistical learning theory</article-title>. <source>IEEE Trans Neural Netw</source>. <year>1999</year>; <volume>10</volume>: <fpage>988</fpage>&#x02013;<lpage>999</lpage>. <pub-id pub-id-type="doi">10.1109/72.788640</pub-id>
<pub-id pub-id-type="pmid">18252602</pub-id></mixed-citation></ref><ref id="pone.0163041.ref051"><label>51</label><mixed-citation publication-type="book">
<name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name>, <name><surname>Smola</surname><given-names>AJ</given-names></name>. <source>Learning with kernels</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>
<year>2002</year>.</mixed-citation></ref><ref id="pone.0163041.ref052"><label>52</label><mixed-citation publication-type="journal">
<name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Vapnik</surname><given-names>V</given-names></name>. <article-title>Support vector networks</article-title>. <source>Mach Learn</source>. <year>1995</year>; <volume>20</volume>: <fpage>273</fpage>&#x02013;<lpage>297</lpage>. <pub-id pub-id-type="doi">10.1023/A:1022627411411</pub-id></mixed-citation></ref><ref id="pone.0163041.ref053"><label>53</label><mixed-citation publication-type="book">
<name><surname>Witten</surname><given-names>IH</given-names></name>, <name><surname>Frank</surname><given-names>E</given-names></name>. <source>Data mining: Practical machine learning tools and techniques</source>, <edition>2nd ed</edition>
<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Morgan Kaufmann Publishers</publisher-name>
<year>2005</year>.</mixed-citation></ref><ref id="pone.0163041.ref054"><label>54</label><mixed-citation publication-type="journal">
<name><surname>Chang</surname><given-names>CC</given-names></name>, <name><surname>Lin</surname><given-names>CJ</given-names></name>. <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Trans Intell Syst Technol</source>. <year>2011</year>; <volume>2</volume>: <fpage>1</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></mixed-citation></ref><ref id="pone.0163041.ref055"><label>55</label><mixed-citation publication-type="journal">
<name><surname>Coomans</surname><given-names>D</given-names></name>, <name><surname>Massart</surname><given-names>D</given-names></name>. <article-title>Alternative k-nearest neighbour rules in supervised pattern recognition</article-title>. <source>Anal Chim Acta</source>. <year>1982</year>; <volume>136</volume>: <fpage>15</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1016/S0003-2670(01)95359-0</pub-id></mixed-citation></ref><ref id="pone.0163041.ref056"><label>56</label><mixed-citation publication-type="journal">
<name><surname>Hall</surname><given-names>P</given-names></name>, <name><surname>Park</surname><given-names>BU</given-names></name>, <name><surname>Samworth</surname><given-names>RJ</given-names></name>. <article-title>Choice of neighbor order in nearest-neighbor classification</article-title>. <source>Ann Statist</source>. <year>2008</year>; <volume>36</volume>: <fpage>2135</fpage>&#x02013;<lpage>2152</lpage>. <pub-id pub-id-type="doi">10.1214/07-AOS537</pub-id></mixed-citation></ref><ref id="pone.0163041.ref057"><label>57</label><mixed-citation publication-type="journal">
<name><surname>Fan</surname><given-names>RE</given-names></name>, <name><surname>Chang</surname><given-names>KW</given-names></name>, <name><surname>Hsieh</surname><given-names>CJ</given-names></name>, <name><surname>Wang</surname><given-names>XR</given-names></name>, <name><surname>Lin</surname><given-names>CJ</given-names></name>. <article-title>LIBLINEAR: A library for large linear classification</article-title>. <source>J Mach Learn Res</source>. <year>2008</year>; <volume>9</volume>: <fpage>1871</fpage>&#x02013;<lpage>1874</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref058"><label>58</label><mixed-citation publication-type="book">
<name><surname>Kanamori</surname><given-names>T</given-names></name>, <name><surname>Takenouchi</surname><given-names>T</given-names></name>, <name><surname>Eguchi</surname><given-names>S</given-names></name>, <name><surname>Murata</surname><given-names>N</given-names></name>. <chapter-title>The most robust loss function for boosting</chapter-title>, in <source>Neural Information Processing</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>
<year>2004</year>; <fpage>496</fpage>&#x02013;<lpage>501</lpage>.</mixed-citation></ref><ref id="pone.0163041.ref059"><label>59</label><mixed-citation publication-type="journal">
<name><surname>Zhu</surname><given-names>J</given-names></name>, <name><surname>Zou</surname><given-names>H</given-names></name>, <name><surname>Rosset</surname><given-names>S</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>. <article-title>Multi-class AdaBoost</article-title>. <source>Stat Interface</source>. <year>2009</year>; <volume>2</volume>: <fpage>349</fpage>&#x02013;<lpage>360</lpage>. <pub-id pub-id-type="doi">10.4310/SII.2009.v2.n3.a8</pub-id></mixed-citation></ref><ref id="pone.0163041.ref060"><label>60</label><mixed-citation publication-type="journal">
<name><surname>Cohen</surname><given-names>J</given-names></name>. <article-title>A coefficient of agreement for nominal scales</article-title>. <source>Educ Psychol Meas</source>. <year>1960</year>; <volume>20</volume>: <fpage>37</fpage>&#x02013;<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id></mixed-citation></ref><ref id="pone.0163041.ref061"><label>61</label><mixed-citation publication-type="journal">
<name><surname>Aide</surname><given-names>TM</given-names></name>, <name><surname>Corrada-Bravo</surname><given-names>C</given-names></name>, <name><surname>Campos-Cerqueira</surname><given-names>M</given-names></name>, <name><surname>Milan</surname><given-names>C</given-names></name>, <name><surname>Vega</surname><given-names>G</given-names></name>, <name><surname>Alvarez</surname><given-names>R</given-names></name>. <article-title>Real-time bioacoustics monitoring and automated species identification</article-title>. <source>PeerJ</source>. <year>2013</year>; <volume>1</volume>: <fpage>e103</fpage>
<pub-id pub-id-type="doi">10.7717/peerj.103</pub-id>
<pub-id pub-id-type="pmid">23882441</pub-id></mixed-citation></ref><ref id="pone.0163041.ref062"><label>62</label><mixed-citation publication-type="journal">
<name><surname>Turesson</surname><given-names>HK</given-names></name>, <name><surname>Ribeiro</surname><given-names>S</given-names></name>. <article-title>Can vocal conditioning trigger a semiotic ratchet in marmosets?</article-title>
<source>Front Psychol</source>. <year>2015</year>; <volume>6</volume>: <fpage>1</fpage>&#x02013;<lpage>11</lpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.01519</pub-id><pub-id pub-id-type="pmid">25688217</pub-id></mixed-citation></ref></ref-list></back></article>