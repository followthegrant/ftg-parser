<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30986963</article-id><article-id pub-id-type="pmc">6480562</article-id><article-id pub-id-type="doi">10.3390/s19071593</article-id><article-id pub-id-type="publisher-id">sensors-19-01593</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Efficient Traffic Video Dehazing Using Adaptive Dark Channel Prior and Spatial&#x02013;Temporal Correlations</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dong</surname><given-names>Tianyang</given-names></name></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Guoqing</given-names></name></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Jiamin</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ye</surname><given-names>Yang</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Ying</given-names></name><xref rid="c1-sensors-19-01593" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-19-01593">College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou 310023, China; <email>dty@zjut.edu.cn</email> (T.D.); <email>m15695719625@163.com</email> (G.Z.); <email>hzwujiamin@163.com</email> (J.W.); <email>yeyang80@zjut.edu.cn</email> (Y.Y.)</aff><author-notes><corresp id="c1-sensors-19-01593"><label>*</label>Correspondence: <email>shenying@zjut.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>4</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>4</month><year>2019</year></pub-date><volume>19</volume><issue>7</issue><elocation-id>1593</elocation-id><history><date date-type="received"><day>06</day><month>3</month><year>2019</year></date><date date-type="accepted"><day>26</day><month>3</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>In order to restore traffic videos with different degrees of haziness in a real-time and adaptive manner, this paper presents an efficient traffic video dehazing method using adaptive dark channel prior and spatial-temporal correlations. This method uses a haziness flag to measure the degree of haziness in images based on dark channel prior. Then, it gets the adaptive initial transmission value by establishing the relationship between the image contrast and haziness flag. In addition, this method takes advantage of the spatial and temporal correlations among traffic videos to speed up the dehazing process and optimize the block structure of restored videos. Extensive experimental results show that the proposed method has superior haze removing and color balancing capabilities for the images with different degrees of haze, and it can restore the degraded videos in real time. Our method can restore the video with a resolution of 720 &#x000d7; 592 at about 57 frames per second, nearly four times faster than dark-channel-prior-based method and one time faster than image-contrast-enhanced method.</p></abstract><kwd-group><kwd>image dehazing</kwd><kwd>traffic video dehazing</kwd><kwd>dark channel prior</kwd><kwd>spatial-temporal correlation</kwd><kwd>contrast enhancement</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-19-01593"><title>1. Introduction</title><p>Today, traffic video analysis plays a very important role in intelligent transportation systems. It has become a common way to help people track a vehicle, as well as locate and judge an accident. Because the images captured by outdoor cameras are often affected by different weather conditions, they suffer from poor visibility and lack of contrast. In the literature, there are many enhancements and dehazing algorithms that improve different images, such as traffic videos, underwater images, and satellite imagery [<xref rid="B1-sensors-19-01593" ref-type="bibr">1</xref>,<xref rid="B2-sensors-19-01593" ref-type="bibr">2</xref>,<xref rid="B3-sensors-19-01593" ref-type="bibr">3</xref>]. The hazy weather that happens frequently all over the world is becoming a video analysis killer. The haze captured in the video degrades the contrast and color information and reduces the visibility. Therefore, the problem of how to efficiently and effectively remove the haze in traffic videos has attracted broad attention from both academia and industry. In general, when dealing with haze removal in traffic videos, the existing dehazing algorithms often exhibit poor real-time performance, overstretched contrast, and even fail to remove dense haze. The key issue of these problems is how to deal with images in different scenes with different degrees of haze, thus an adaptive algorithm that can remove haze based on the image characteristics is needed. Moreover, the existing video-dehazing methods are almost universal for all videos and do not consider the characteristics of videos in particular scenarios. For traffic videos, the time continuity, lane space structure, and camera spatial locations can be effectively used to decrease computational cost.</p><p>In order to restore traffic videos with different degrees of haziness in a real-time and adaptive manner, this paper presents an efficient traffic video dehazing method using adaptive dark channel prior and spatial-temporal correlations. This method can avoid overstretched contrast after haze removal and obtain satisfactory restored results for dense hazy videos by using a novel approach involving adaptive transmission estimation. This method also takes full advantage of the temporal and spatial correlations in traffic videos to meet the requirements of real-time dehazing, such as using time continuity to set the time slice, refining transmission by characteristics of block structure, decreasing restored area according to the lane space, and simplifying the calculation of parameters by using multi-camera distribution.</p></sec><sec id="sec2-sensors-19-01593"><title>2. Related Works</title><p>Essentially, videos are composed of frames, thus the haze removal method for images can be used for videos. The image dehazing method is the most common way to restore hazy images. This method considers the inverse process of image degradation and describes the image degradation process in detail through an established physical model. The most critical step of this method is to obtain the parameters of the degradation model. Oakley et al. [<xref rid="B4-sensors-19-01593" ref-type="bibr">4</xref>] improved the image quality by using the physical model and estimated the degradation model parameters based on a statistical model. This method is not widely used because it is only useful for gray-scale images, and the acquiring parameters require calibrated radar to get depth information. Narasimhan et al. [<xref rid="B5-sensors-19-01593" ref-type="bibr">5</xref>] proposed a method to estimate the depth information by comparing two images of the same scene in different weather conditions. Chen et al. [<xref rid="B6-sensors-19-01593" ref-type="bibr">6</xref>] used a sunny image and a foggy image for reference images to calculate parameters. Both of these methods need to receive eligible images in advance, which increases the difficulty of image acquisition.</p><p>To obtain the parameters of the degradation model effectively, some dehazing methods based on prior knowledge or assumptions were proposed, and they do not need to get reference images in advance or use an additional hardware device. Therefore, these methods have better adaptability than previous methods. Based on the assumption that a haze-free image has a higher contrast than a hazy image, Tan [<xref rid="B7-sensors-19-01593" ref-type="bibr">7</xref>] proposed a haze removal approach by maximizing the contrast of recovered scene radiance. This approach can produce a satisfactory result for haze removal in single images, but it tends to overcompensate for the reduced contrast and leads to halo effects. Fattal [<xref rid="B8-sensors-19-01593" ref-type="bibr">8</xref>] decomposed scene radiance of an image into the albedo and shading and then estimated the scene radiance based on independent component analysis, assuming that transmission shading and surface shading are locally uncorrelated. However, this method cannot generate impressive results when the captured image is heavily obscured by fog. He et al. [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>] presented a single image haze removal method by using dark channel prior, which can estimate the transmission map directly. However, when a large white area without shading exists in the images, or the images have uneven illumination, this method takes a long time to restore the hazy images. In addition, the use of the soft matting algorithm makes this a complex computation. Then, Lai et al. [<xref rid="B10-sensors-19-01593" ref-type="bibr">10</xref>] presented a haze removal method based on the difference-structure-preservation prior. In this method, the difference-structure-preservation dictionary is learned such that the local consistency features of the transmission map can be well preserved after coefficient shrinkage. Zhu et al. [<xref rid="B11-sensors-19-01593" ref-type="bibr">11</xref>] presented a simple but effective Color Attenuation Prior (CAP)algorithm similar to Dark Channel Prior (DCP)using the difference in brightness and saturation to estimate the haze concentration to build a depth model for dehazing. Up until now, other researchers have improved their dehazing algorithms based on the dark channel prior. Yeh et al. [<xref rid="B12-sensors-19-01593" ref-type="bibr">12</xref>] introduced a haze removal algorithm based on region decomposition and feature fusion, which is especially suitable for hazy images with large sky regions. Li et al. [<xref rid="B13-sensors-19-01593" ref-type="bibr">13</xref>] proposed a novel haze removal method based on sky segmentation and dark channel prior to restore images. In this method, the average image intensity of the sky region is chosen as the atmospheric light value. Wang et al. [<xref rid="B14-sensors-19-01593" ref-type="bibr">14</xref>] designed a new method of selecting atmospheric light values to weaken the area where the dark channel priority does not work effectively. A visibility restoration method was introduced by Huang et al. [<xref rid="B15-sensors-19-01593" ref-type="bibr">15</xref>], which consists of three modules: (i) depth estimation module based on dark channel priority, (ii) color analysis module that repairs depth estimation distortion, and (iii) visibility restoration module that generates repair results. Riaz et al. [<xref rid="B16-sensors-19-01593" ref-type="bibr">16</xref>] proposed a new and efficient method for transmission estimation with bright-object handling capability, which uses a local average haziness value to compute the transmission of such surfaces based on the observation that the transmission of a surface is loosely connected to its neighbors.</p><p>Usually, traffic video dehazing algorithms are proposed based on single-image dehazing algorithms. However, the computational complexity makes it difficult to apply single-image dehazing algorithms directly to video dehazing. Most existing research on video dehazing is to speed up the process of dehazing. Sun et al. [<xref rid="B17-sensors-19-01593" ref-type="bibr">17</xref>] proposed a real-time haze removal method based on bilateral filtering to reduce the processing time of 320 &#x000d7; 240 images to a speed of 20 frames per second. However, this method cannot satisfy the requirements of high-definition videos. Wang et al. [<xref rid="B18-sensors-19-01593" ref-type="bibr">18</xref>] proposed a method based on Retinex theory that enhances image contrast in YUV color space and can process an image of 704 &#x000d7; 576 in 0.055 s. Kumari et al. [<xref rid="B19-sensors-19-01593" ref-type="bibr">19</xref>] proposed an approach for dehazing images and videos based on a filtering method. The use of a gray-scale morphological operation made the approach faster, and it took only 80% of the execution time compared to a fast bilateral filter. Berman et al. [<xref rid="B20-sensors-19-01593" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-01593" ref-type="bibr">21</xref>] proposed a new method via calculating the air-light to dehaze fogs, which was based on a non-local prior. Their algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors that form tight clusters in RGB space. It performs well on a wide variety of images. However, these methods take every frame in videos as a single image, and they are completely based on image dehazing methods.</p><p>The characteristics of videos can be applied in specific video dehazing algorithms. Tarel et al. [<xref rid="B22-sensors-19-01593" ref-type="bibr">22</xref>] proposed a video dehazing method for onboard video systems. This method can separate moving objects and driveway regions in videos and only update the depth information of moving objects. Zhang et al. [<xref rid="B23-sensors-19-01593" ref-type="bibr">23</xref>] proposed a method based on spatial and temporal correlation that uses spatial and temporal similarity between frames to optimize the estimation of a scene depth map. Shin et al. [<xref rid="B24-sensors-19-01593" ref-type="bibr">24</xref>] proposed an effective video dehazing technique to reduce flicker artifacts by using adaptive temporal average. However, these methods cannot remove the haze from videos in real time. Therefore, Kim et al. [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>] proposed an image-dehazing method based on the image degradation model and kept a balance between image contrast enhancement and image information loss. To improve the speed of video dehazing, they adopted a video dehazing method by using temporal correlation, which can reach a speed of 30 frames per second for videos with a resolution of 640 &#x000d7; 480. However, this method adopts a fixed initial transmission value that cannot be adapted to images with different degrees of haze, and it cannot efficiently remove dense haze in videos. Our method uses an adaptive initial transmission value based on image characteristics to handle different degrees of hazes; meanwhile, it can reduce the processing time through lane space separation.</p></sec><sec id="sec3-sensors-19-01593"><title>3. Single-Image Dehazing Using Adaptive Dark Channel Prior</title><sec id="sec3dot1-sensors-19-01593"><title>3.1. Framework of Single-Image Dehazing Method</title><p>The most common dehazing model is based on atmospheric optics [<xref rid="B26-sensors-19-01593" ref-type="bibr">26</xref>], which can describe the degradation process of a hazy image. In [<xref rid="B27-sensors-19-01593" ref-type="bibr">27</xref>], the modeling function is simplified, and it is represented by Equation (1).
<disp-formula id="FD1-sensors-19-01593"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> is a pixel in the image, <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the observed and haze-free image, respectively, <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the global atmospheric light, and <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the transmission map for each pixel that describes the proportion of the light arriving at a digital camera without scattering.</p><p>The process of haze removal for every frame of a traffic video can be divided into three steps: calculating atmospheric light, estimating the transmission map, and restoring the image. In this paper, we present a novel adaptive method for transmission map estimation, thus the dehazing algorithm can be applied to images with different degrees of haze. The framework of the single-image dehazing algorithm is shown in <xref ref-type="fig" rid="sensors-19-01593-f001">Figure 1</xref>.</p><p>We use a hierarchical searching method based on quad-tree subdivision [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>] to find the areas least affected by haze and to get the brightest pixel in this area. The detailed steps are as follows:<list list-type="simple"><list-item><label>Step 1:</label><p>Divide an input image into four rectangular regions.</p></list-item><list-item><label>Step 2:</label><p>Define the score of each region as the average pixel value subtracted from the standard deviation of the pixel values within the region.</p></list-item><list-item><label>Step 3:</label><p>Select the region with the highest score and divide it further into four smaller regions.</p></list-item><list-item><label>Step 4:</label><p>Repeat Steps 1 through Step 3 until the size of the selected region is smaller than a prespecified threshold. The prespecified threshold in this paper is 200, which is that the height * width of the selected region is smaller than 200.</p></list-item></list></p><p>At last, we choose the color vector, which minimizes the distance <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>255</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>255</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>255</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the value of pixel <inline-formula><mml:math id="mm9"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> in the selected region as the atmospheric light.</p></sec><sec id="sec3dot2-sensors-19-01593"><title>3.2. Transmission Estimation for Enhancing the Contrast of Blocks</title><p>In general, a hazy block yields low contrast, and the contrast of a restored block increases as the value of the estimated transmission decreases. We adopt the image-contrast-enhanced method [<xref rid="B18-sensors-19-01593" ref-type="bibr">18</xref>] to maximize the contrast of the restored blocks and get the best estimated transmission value.</p><p>Mean squared error contrast (CMSE) [<xref rid="B28-sensors-19-01593" ref-type="bibr">28</xref>] can define the contrast of a restored block, which is represented by Equation (2):<disp-formula id="FD2-sensors-19-01593"><label>(2)</label><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the RGB color channel of pixel p in a block of input image, <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the average value of <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm15"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is the number of pixels in a block.</p><p>According to the assumption that the scene depths are locally similar [<xref rid="B8-sensors-19-01593" ref-type="bibr">8</xref>,<xref rid="B12-sensors-19-01593" ref-type="bibr">12</xref>,<xref rid="B16-sensors-19-01593" ref-type="bibr">16</xref>], the dehazing algorithm in this paper determines a single transmission value for each block of size 32 &#x000d7; 32, and then gets the fixed optimal transmission value <inline-formula><mml:math id="mm16"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> for each block. For a pixel <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in a block, <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in Equation (1) can be replaced with the fixed estimated transmission <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> of its block. Hence, <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is represented by Equation (3).
<disp-formula id="FD3-sensors-19-01593"><label>(3)</label><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If Equation (3) is put into Equation (2), <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be represented by Equation (4):<disp-formula id="FD4-sensors-19-01593"><label>(4)</label><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the average value of <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in the input block. According to Equation (4), we can find that the mean squared error contrast is a decreasing function of <inline-formula><mml:math id="mm26"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. Thus, we can select a small value of <inline-formula><mml:math id="mm27"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> to increase the contrast of a restored block. However, the value of <inline-formula><mml:math id="mm28"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> influences the pixel&#x02019;s restored image value according to Equation (3).</p><p>However, when a block contains dense haze, it has a relatively narrow value range for input pixels. Thus, even though it is assigned a small <inline-formula><mml:math id="mm29"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> value, most of the input values are not truncated, and the block can be correctly restored. On the contrary, a block without haze usually has a broad range of values for input pixels and should be assigned a larger <italic>t</italic> value to reduce the information loss due to the truncation. Thus, we should not only enhance the contrast but also reduce the information loss.</p><p>Therefore, we need to set quantitative evaluations for contrast and information integrity. The contrast cost <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the information loss cost <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were proposed by Kim [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>] to evaluate the contrast and information integrity, respectively.
<disp-formula id="FD5-sensors-19-01593"><label>(5)</label><mml:math id="mm32"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the average values of <inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in block <inline-formula><mml:math id="mm37"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, and <inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of pixels in <inline-formula><mml:math id="mm39"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>. Thus, we can maximize the mean squared error contrast by minimizing the value of <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.
<disp-formula id="FD6-sensors-19-01593"><label>(6)</label><mml:math id="mm41"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">min</mml:mi><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">max</mml:mi><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>255</mml:mn><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">min</mml:mi><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>255</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote the truncated values for output pixels due to the underflow and overflow, respectively.</p><p>If we want to get a better restored image, the image contrast should be smoother, and the color information should be maintained as much as possible. Thus, these two factors should be taken into consideration synthetically, and the overall cost function is described as Equation (7).
<disp-formula id="FD7-sensors-19-01593"><label>(7)</label><mml:math id="mm44"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a weight coefficient that controls the relative importance of the contrast cost and the information loss cost [<xref rid="B18-sensors-19-01593" ref-type="bibr">18</xref>]. The minimum value of <inline-formula><mml:math id="mm46"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> represents the most suitable contrast for restored images, and the color loss is as small as possible. Finally, for each block in a hazy image, we can get an optimal transmission <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> by minimizing the value of <inline-formula><mml:math id="mm48"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula>. The value of <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the transmission we use while dehazing.</p></sec><sec id="sec3dot3-sensors-19-01593"><title>3.3. Adaptive Estimation of Initial Transmission</title><sec id="sec3dot3dot1-sensors-19-01593"><title>3.3.1. Calculating Image Haziness Flag</title><p>We present a haziness flag <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to measure the degree of haze in an image. The dark channel prior [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>] can estimate the transmission of a block, which represents the luminosity of objects. The transmission has a close relationship with the degree of haze. Therefore, we can adopt the average value of transmission as the haziness flag <italic>T</italic> of an image. The haziness flag <italic>T</italic> is concerned with the effects of the degree of haze in images.</p><p>The dark channel prior is based on the observation that most local blocks in haze-free outdoor images contain some pixels that have very low intensities in at least one color channel. In other words, the dark channel value of a haze-free image is close to zero [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>]. For any input image <inline-formula><mml:math id="mm51"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula>, dark channel <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as Equation (8).
<disp-formula id="FD8-sensors-19-01593"><label>(8)</label><mml:math id="mm53"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represent a local block centered at <inline-formula><mml:math id="mm56"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm57"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> is a pixel in the local block <inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. A dark channel is the outcome of two minimum operators: <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:munder><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is performed on each pixel, and <inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:math></inline-formula> is a minimum filter [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>].</p><p>Assuming that the atmospheric light <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is given, we can normalize the haze imaging Equation (1) by <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>]:<disp-formula id="FD9-sensors-19-01593"><label>(9)</label><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the transmission <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a constant <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in local block, and the value of <inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is given, the dark channel operation can be given by the following equations [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>].
<disp-formula id="FD10-sensors-19-01593"><label>(10)</label><mml:math id="mm67"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Using the concept of a dark channel [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>], if <inline-formula><mml:math id="mm68"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is an outdoor haze-free image except for the sky region, the intensity of dark channel is low and tends to be zero, which leads to:<disp-formula id="FD11-sensors-19-01593"><label>(11)</label><mml:math id="mm69"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Putting Equation (11) into Equation (9), we can eliminate the multiplicative term and estimate the transmission <inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> simply by
<disp-formula id="FD12-sensors-19-01593"><label>(12)</label><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted value of transmission of a block [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>]. We need to calculate the average transmission for all blocks to obtain the average transmission T for the whole image, which is the value of image haziness flag.</p></sec><sec id="sec3dot3dot2-sensors-19-01593"><title>3.3.2. Correction of Initial Transmission</title><p>According to our experimental results, in a hazy image, the range of <inline-formula><mml:math id="mm73"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is generally between 0.4 and 0.6. Although the image haziness flag <inline-formula><mml:math id="mm74"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> can characterize the nature of the image, taking <inline-formula><mml:math id="mm75"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> as the initial transmission value to get the optimal transmission leads to an excessive value of <inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, we set a correction value <inline-formula><mml:math id="mm77"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>, and set <inline-formula><mml:math id="mm78"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as the initial transmission value to decrease this initial value.</p><p>The structural similarity (SSIM) index is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. To guarantee that the restored images are closer to ground truths, we adopted the SSIM index [<xref rid="B29-sensors-19-01593" ref-type="bibr">29</xref>] to measure the similarity between the ground truths and restored images. Because the traffic video is captured by a fixed camera, we can get a haze-free image of the same scene as a reference image in advance and compare the restored image with the reference image. The initial value of <inline-formula><mml:math id="mm79"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> can be obtained directly because it is relevant to the nature of images, whereas the unknown value <inline-formula><mml:math id="mm80"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> is calculated by the SSIM. In our experiments, we set <inline-formula><mml:math id="mm81"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> as a series of values between 0.3 and 1.2, and the interval is 0.02. Then, we take every <inline-formula><mml:math id="mm82"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> in this range multiplied by <italic>T</italic>, that is, <inline-formula><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, as the initial value of transmission and get the corresponding restored image. At last, we find a restored image that is closest to the haze-free image based on the maximum value of the SSIM index. Thus, the value of transmission is the optimal initial value, and the corresponding correction value <inline-formula><mml:math id="mm84"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> is the optimal correction value of initial transmission.</p><p>However, this method needs a haze-free image to get the optimal correction value <inline-formula><mml:math id="mm85"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>. This method is limited in practical applications, thus it is necessary to get the correction value according to the image characteristics. After analyzing the image contrast and the haze in images, we find the relationship between the correction value of initial transmission and the image characteristics. Therefore, a relatively reasonable initial transmission correction value can be obtained directly from hazy images.</p><p>If the relatively reasonable correction value of initial transmission is <inline-formula><mml:math id="mm86"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, we take <inline-formula><mml:math id="mm87"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> as the initial transmission value. Because the dehazing algorithm is based on the concept of enhancing the image contrast to the greatest degree, the contrast is the important indicator. The value of image haziness flag <italic>T</italic> represents the degree of haze that degrades the image contrast. Thus, the image contrast and haziness flag value should be considered simultaneously. We set <inline-formula><mml:math id="mm88"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> as the image contrast and set <inline-formula><mml:math id="mm89"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as a quantitative value representing the image characteristics. The constant value <inline-formula><mml:math id="mm90"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> depends on the range of value <inline-formula><mml:math id="mm91"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><xref rid="sensors-19-01593-t001" ref-type="table">Table 1</xref> shows the values of <inline-formula><mml:math id="mm92"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for different ranges of <inline-formula><mml:math id="mm93"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In <xref rid="sensors-19-01593-t001" ref-type="table">Table 1</xref>, <inline-formula><mml:math id="mm94"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> is the optimal correction value obtained by the method with reference images, and <inline-formula><mml:math id="mm95"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the relatively reasonable correction value obtained by the ranges of <inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In the dehazing algorithm, the initial transmission value is the key factor that affects the dehazing result. <xref rid="sensors-19-01593-t001" ref-type="table">Table 1</xref> shows the values of <inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm98"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which are the initial transmission value derived by optimal correction of initial <inline-formula><mml:math id="mm99"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and relatively reasonable correction value <inline-formula><mml:math id="mm100"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, respectively. <xref ref-type="fig" rid="sensors-19-01593-f002">Figure 2</xref> shows the histogram of <inline-formula><mml:math id="mm101"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm102"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where the values of <inline-formula><mml:math id="mm103"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm104"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> in the same group have similar values, and the difference of the values in the same group does not affect the dehazing results significantly. Therefore, our method can determine the optimal initial transmission value using only the nature of images and then obtain a more adaptive transmission value.</p></sec></sec></sec><sec id="sec4-sensors-19-01593"><title>4. Adaptive Traffic Video Dehazing Method Using Spatial&#x02013;Temporal Correlations</title><p>Compared with static traffic images, traffic videos have some unique characteristics. First, a traffic video is a collection of images with time continuity. Second, the cameras are fixed on the road and capture videos of the same scene over a long time, thus the videos are consistent in space. Therefore, we can use the correlations of spatial-temporal information to speed up traffic video dehazing.</p><sec id="sec4dot1-sensors-19-01593"><title>4.1. Time Continuity of Traffic Videos</title><p>Because the cameras are fixed, the scenes in traffic videos barely change over a long period of time, and the influence of haze is stable. In our experiments, we use the traffic videos from ZhongHe elevated freeways in Hangzhou City, set a cycle of five minutes, and regard the frames in one cycle as a collection of images with the same characteristics. <xref ref-type="fig" rid="sensors-19-01593-f003">Figure 3</xref> shows images whose interval is 1 min in a 5 min cycle, and the difference of <inline-formula><mml:math id="mm105"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is very small, usually less than 0.04. <xref ref-type="fig" rid="sensors-19-01593-f004">Figure 4</xref> presents the difference in restored images by using different <inline-formula><mml:math id="mm106"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> values where the results have no obvious influence on visibility with the difference of <inline-formula><mml:math id="mm107"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> less than 0.04. Therefore, if the videos are captured at the same scene, the values of <inline-formula><mml:math id="mm108"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for these video images in a 5 min cycle are at the same level, and the cycle of 5 min is reasonable in practical application.</p><p>After setting the 5 min cycle, we can take the first frame of a video segment as a reference frame. We can determine the image haziness flag value <inline-formula><mml:math id="mm109"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and the relatively reasonable initial transmission correction value <inline-formula><mml:math id="mm110"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> from the reference frame and then calculate the optimal transmission <inline-formula><mml:math id="mm111"><mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. In this way, we can speed up the dehazing processing for the traffic video. This method can avoid incorrect transmission estimation, which is caused by the changes in atmospheric light, and eliminate the discontinuity of videos after dehazing.</p></sec><sec id="sec4dot2-sensors-19-01593"><title>4.2. Transmission Refinement Based on Spatial Structure</title><p>We estimate the optimal transmission based on the assumption that all pixels in a block have the same transmission. However, scene depths may vary spatially within a block, and the block-based transmission map usually has a blocking-artifact problem. Therefore, an edge-preserving filter is adopted to refine the block-based transmission map.</p><p>The single-image dehazing method using dark channel prior [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>] employs the soft matting technique [<xref rid="B30-sensors-19-01593" ref-type="bibr">30</xref>] to refine the large block size in the transmission map, which causes an enormous computational burden. In this paper, the guided filter method [<xref rid="B31-sensors-19-01593" ref-type="bibr">31</xref>] is adopted to refine the transmission map, which has less computational cost. The filtered transmission <inline-formula><mml:math id="mm112"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is an affine combination of the guidance image <inline-formula><mml:math id="mm113"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, as show in Equation (13):<disp-formula id="FD13-sensors-19-01593"><label>(13)</label><mml:math id="mm114"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm115"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a scaling vector, and <inline-formula><mml:math id="mm116"><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:math></inline-formula> is an offset determined by the size of block. For a block in one image, the optimal parameters of <inline-formula><mml:math id="mm117"><mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm118"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003c8;</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained by minimizing the difference between the transmission <inline-formula><mml:math id="mm119"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the filtered transmission <inline-formula><mml:math id="mm120"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> using the least squares method as Equation (14):<disp-formula id="FD14-sensors-19-01593"><label>(14)</label><mml:math id="mm121"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c8;</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003a9;</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If the transmission is too small, the noise will be enhanced in the restored image [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>]. Thus, the lower limit of the transmission is set to 0.1. If a window slides pixel by pixel over the entire image, there will be multiple windows that overlap at each pixel position. Therefore, we adopt the centered window scheme, which sets the final transmission values as the average of all associated refined transmission values at each pixel position. However, the average transmission value in this scheme will cause blurring in the final transmission map, especially around object boundaries, where the depths change abruptly. To overcome this problem, the shiftable window scheme [<xref rid="B32-sensors-19-01593" ref-type="bibr">32</xref>] is employed instead of the centered window scheme. The centered window scheme overlays a window on each pixel so that the window contains multiple objects with different depths, which leads to unreliable depth estimation. In the shiftable window scheme, the window is shifted within a block of 40 &#x000d7; 40. The optimal shift position is selected depending on the smallest change of pixel values within the window. Even though a shiftable window is selected for a specific pixel, the number of overlapping windows usually varies at different positions. The windows in smooth regions are selected more frequently than those in rough boundary regions. Thus, the shiftable window scheme can reduce the effects of unreliable transmission values derived from rough boundary regions, thereby alleviating the blurring artifacts.</p></sec><sec id="sec4dot3-sensors-19-01593"><title>4.3. Lane Separation for Traffic Videos</title><p>After analyzing the spatial characteristics of traffic video, we found that the traffic lane is an obvious structure. In a traffic video detection system, the detected objects are mostly concentrated in the driveway regions. The areas outside lanes are not the regions of interest in traffic video processing. Therefore, we can process haze removal only in the driveway region of traffic video to reduce computing time.</p><p>However, the estimations of atmospheric light and transmission are based on the whole image. If these values are achieved only through the driveway regions, it may cause some deviations, especially when the sky occupies a large area of the image, such as the cases shown in <xref rid="sensors-19-01593-t002" ref-type="table">Table 2</xref>. The larger the sky region is, the greater the deviation for the value of <inline-formula><mml:math id="mm122"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is. Therefore, the separated lane can be used in the last step to restore the pixels only for the driveway regions.</p><p>We adopt a straight-line extraction algorithm based on the Hough transform to detect the lanes and separate the driveway region from the global image. The process of haze removal combined with the driveway region separation is described as follows:<list list-type="order"><list-item><p>Calculate the global atmospheric light <inline-formula><mml:math id="mm123"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula>, the value of haziness flag <inline-formula><mml:math id="mm124"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, and the image contrast <inline-formula><mml:math id="mm125"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>, then estimate the optimal transmission map for each block in an image.</p></list-item><list-item><p>Get the driveway region, as shown in <xref ref-type="fig" rid="sensors-19-01593-f005">Figure 5</xref>.<list list-type="simple"><list-item><p>Step 1: Obtain the edge information in the video through edge detection.</p></list-item><list-item><p>Step 2: Remove obviously wrong-angle lines by Hough linear fitting, and obtain lane candidates, as shown in <xref ref-type="fig" rid="sensors-19-01593-f005">Figure 5</xref>b.</p></list-item><list-item><p>Step 3: Find the far left lane and the far right lane, and set them as the driveway boundaries, then find the intersection of these two lines, as shown in <xref ref-type="fig" rid="sensors-19-01593-f005">Figure 5</xref>c.</p></list-item><list-item><p>Step 4: Identify a rectangular area as the driveway region, which is composed of the boundary of the image and a horizontal line across the intersection, as shown in <xref ref-type="fig" rid="sensors-19-01593-f005">Figure 5</xref>c. If the intersection is outside the image, take the whole image area as the driveway region.</p></list-item></list></p></list-item><list-item><p>Use the original pixel values and the optimal transmission of driveway region in the dehazing model to restore the image in the driveway region.</p></list-item></list></p><p>In a traffic video detection system, each camera is located at a fixed position and captures the same traffic scenes for a long time. Based on the time continuity, the result of lane space separation for the initial frame of a traffic video can be used over a long time period. Lane space separation can decrease the area of haze removal and improve the efficiency of the dehazing algorithm. <xref ref-type="fig" rid="sensors-19-01593-f006">Figure 6</xref> shows the haze removal results with and without lane separation. In this scene, the dehazing of 2000 frames needs 35.301 s without lane separation and 32.74 s with lane separation (lane space separation takes 0.182 s). Although lane separation requires some time, the operation just occurs in the first frame. Thus, the time for lane separation can be shared by all frames of a traffic video. With an increasing number of frames, the efficiency of the dehazing algorithm with lane separation will be improved more significantly. Hence, if the driveway region is a larger portion of a whole image, the processing time can be decreased obviously. When real-time processing is required, a little reduction in processing time has been of practical significance.</p></sec><sec id="sec4dot4-sensors-19-01593"><title>4.4. Optimization Based on Spatial Distribution of Cameras</title><p>With an increasingly complex layout of transportation networks, the number of traffic monitoring cameras also increases gradually, and sometimes there are multiple cameras in the same section of road. These cameras located in close physical proximity usually have the same hardware indicators. In a traffic video detection system, multiple cameras are connected to one system. These cameras have similar characteristics according to their spatial distribution. The weather is also an index with spatial characteristics, that is, the degrees of haze are similar in nearby regions. Thus, we can use the spatial distribution information of cameras to speed up dehazing and optimize the performance of the traffic video detection system.</p><p><xref ref-type="fig" rid="sensors-19-01593-f007">Figure 7</xref> shows the images captured by four surveillance videos of DE-elevated freeways in Hangzhou City at the same time. The locations of these cameras are shown in <xref ref-type="fig" rid="sensors-19-01593-f008">Figure 8</xref>, where the distance between the cameras is about 500 to 600 m. <xref rid="sensors-19-01593-t003" ref-type="table">Table 3</xref> shows the initial transmission values of these four videos. The haziness flag values <italic>T</italic> calculated from each video are shown in the first column of <xref rid="sensors-19-01593-t003" ref-type="table">Table 3</xref>. We obtain relatively proper initial transmission correction value <inline-formula><mml:math id="mm126"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> by using the method proposed in <xref ref-type="sec" rid="sec3-sensors-19-01593">Section 3</xref>, and then determine the initial transmission value <inline-formula><mml:math id="mm127"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. According to the results, these initial transmission values are very numerically similar, thus there may be no obvious influence on the restored images.</p><p>In traffic video dehazing, the cameras are divided into different regions according to their locations, and one camera in a region is set as the calibration camera. The images from the calibration camera are used to calculate the initial transmission value, which is also applied to other cameras in the same region. Therefore, we can avoid repeatedly calculating the values of <inline-formula><mml:math id="mm128"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm129"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm130"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for other cameras, thus improving the efficiency of haze removal. The results of haze removal with the initial transmission value obtained by calibration cameras is shown in <xref ref-type="fig" rid="sensors-19-01593-f009">Figure 9</xref>b, and the result directly using the initial transmission value obtained by the image itself is shown in <xref ref-type="fig" rid="sensors-19-01593-f009">Figure 9</xref>c. It is obvious that the results are very similar in these two ways. It takes 0.033 s to calculate the initial transmission value, which can be saved by using that of the calibration camera.</p></sec></sec><sec sec-type="results" id="sec5-sensors-19-01593"><title>5. Results</title><p>In the efficient traffic video dehazing method using adaptive dark channel prior and spatial-temporal correlations, a video sequence is converted into <inline-formula><mml:math id="mm131"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> color space where <inline-formula><mml:math id="mm132"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> represents the luminance and <inline-formula><mml:math id="mm133"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>/</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the chromaticity. Human eyes are more sensitive to high-frequency signals than low-frequency signals and more sensitive to changes in visibility than changes in color. The <inline-formula><mml:math id="mm134"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm135"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> components are less affected by haze than the <inline-formula><mml:math id="mm136"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> component. Thus, we can only adopt the luminance (<inline-formula><mml:math id="mm137"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>) component to reduce computational complexity. In our experiments, we implemented each method with Opencv and C/C++ language. The source codes were compiled with Microsoft Visual Studio 2010 and run on an Intel Core I5-2400 processor and 4 GB of main memory running a Windows 7 system.</p><sec id="sec5dot1-sensors-19-01593"><title>5.1. Results for Single Image Dehazing</title><p>Our adaptive method can determine the initial transmission according to the image characteristics, thus it can produce a more satisfactory dehazing result than the method with fixed initial transmission. <xref ref-type="fig" rid="sensors-19-01593-f010">Figure 10</xref> shows the restored images using our adaptive method, and there are four different initial transmission values, 0.1, 0.2, 0.3, and 0.4. It is obvious from the experimental results that the smaller initial transmission values may lead to some blocks in the images with overstretched contrast, therefore the optimal initial transmission for the first image is between 0.2 and 0.3, the value for the second image is between 0.3 and 0.4, and the value for the third and fourth images is above 0.4. The <inline-formula><mml:math id="mm138"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> values for the images obtained by our method are all located in the range of the optimal initial transmission. Therefore, our method is adaptable for images with different degrees of haze.</p><p><xref ref-type="fig" rid="sensors-19-01593-f011">Figure 11</xref> shows four images from Foggy Road Image Database (FRIDA) [<xref rid="B33-sensors-19-01593" ref-type="bibr">33</xref>] and restored these images using the dark-channel-prior-based method [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>,<xref rid="B31-sensors-19-01593" ref-type="bibr">31</xref>], the visibility enhancement algorithm [<xref rid="B34-sensors-19-01593" ref-type="bibr">34</xref>], the image-contrast-enhanced method [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>], the non-local image dehazing method [<xref rid="B20-sensors-19-01593" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-01593" ref-type="bibr">21</xref>], and our method. The SSIM values in <xref ref-type="fig" rid="sensors-19-01593-f007">Figure 7</xref> are the average values of three channels of RGB. In FRIDA [<xref rid="B33-sensors-19-01593" ref-type="bibr">33</xref>], each image without fog is associated with some hazy images, and different kinds of fog are added in each image&#x02014;uniform fog, heterogeneous fog, cloudy fog, and cloudy heterogeneous fog. According to the experimental results, the dark-channel-prior-based method does not have satisfactory results for haze removal in heterogeneous fog and cloudy heterogeneous fog, while the image-contrast-enhanced method and our method achieves more satisfactory results for these two cases. In addition, our method obtains the highest <inline-formula><mml:math id="mm139"><mml:mrow><mml:mrow><mml:mi>SSIM</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for the restored images compared to the first three methods, thus the restored images using our method are more similar to ground truth. As to the results of non-local image dehazing method [<xref rid="B20-sensors-19-01593" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-01593" ref-type="bibr">21</xref>], the <inline-formula><mml:math id="mm140"><mml:mrow><mml:mrow><mml:mi>SSIM</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for some restored images may be higher than those of our method. However, the non-local image dehazing method takes longer processing time, as shown in <xref rid="sensors-19-01593-t004" ref-type="table">Table 4</xref>. <xref rid="sensors-19-01593-t004" ref-type="table">Table 4</xref> provides the overall processing times of these methods. Our method is faster than the dark-channel-prior-based method [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>,<xref rid="B31-sensors-19-01593" ref-type="bibr">31</xref>] and visibility enhancement algorithm [<xref rid="B34-sensors-19-01593" ref-type="bibr">34</xref>]. However, our method takes more time than the image-contrast-enhanced method [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>] because it spends some time in calculating the image haziness flag value and the initial transmission correction value. However, the results for haze removal using the proposed method are better than the results of the image-contrast-enhanced method. Although the non-local image dehazing method can get more satisfactory restored images, it is too slow to be used in real-time scenarios. In addition, it usually needs to manually set the parameters to different scenes, which is not suitable for real-time traffic video processing. Further still, we can spread this part of the computation time over all frames in video dehazing and reach a faster dehazing speed through the fusion of spatial and temporal information.</p></sec><sec id="sec5dot2-sensors-19-01593"><title>5.2. Results for Traffic Video Dehazing</title><p>To get better restored images, we restore the whole image for the first frame of a time slice and use the area outside the lane space of the restored frame to replace those areas of the following frames. Moreover, we adopt the parallel programming tools SIMD [<xref rid="B35-sensors-19-01593" ref-type="bibr">35</xref>] and OpenMP [<xref rid="B36-sensors-19-01593" ref-type="bibr">36</xref>] for rapid calculation. <xref ref-type="fig" rid="sensors-19-01593-f012">Figure 12</xref> presents a comparison of three approaches for traffic video dehazing, where <xref ref-type="fig" rid="sensors-19-01593-f012">Figure 12</xref>a shows the original videos; <xref ref-type="fig" rid="sensors-19-01593-f012">Figure 12</xref>b shows the results for the dark-channel-prior-based method with guided filtering [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>,<xref rid="B31-sensors-19-01593" ref-type="bibr">31</xref>], which uses the transmission map obtained from the first frame to filter the following frames; <xref ref-type="fig" rid="sensors-19-01593-f012">Figure 12</xref>c shows the results for the image-contrast-enhanced method [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>], whose initial transmission is a constant value 0.3; <xref ref-type="fig" rid="sensors-19-01593-f012">Figure 12</xref>d shows the results produced by our method. Experimental results demonstrate that the image-contrast-enhanced method leads to some blocks with overstretched contrast, such as the images in groups (1), (3), and (4). For some urban scenes, the color is not obviously different between the driveway and background, such as the examples in group (1) with medium haze and group (2) with dense haze. Our method can restore these videos in a manner more similar to the haze-free scenes, and the driveway and the vehicles can been seen more clearly. However, the dark-channel-prior-based method cannot deal with these videos. For the suburban scenes where the trees and road surface are obviously different in color, such as images in group (3) that were captured in daytime and images in group (4) that were captured in dense haze with vehicle headlights on, our method achieves better restored results than the other two methods. For the restored images using our method in group (3), the driveway color is more uniform. For the restored images using our method in group (4), there are no blocks with overstretched contrast, and the color of trees with hierarchical structure is more realistic. Therefore, our method can maintain the image details and restore images that are more similar to the real scene with proper contrast.</p><p>As we can see from the experiment results, our method produces better haze removal results by determining parameters according to image characteristics. It is also applicable to dense fog or a variety of fog densities. Moreover, it makes the restored images more similar to the real scene and avoids the problem that the restored images exhibit overstretched contrast. Therefore, it can solve the general problems in the existing dehazing algorithms&#x02014;contrast distortion after video dehazing and failure to remove dense haze.</p><p>In addition, our method adopts the spatial correlation, time continuity, lane separation, and spatial distribution of cameras to improve computational efficiency. Besides the processing time, the performance parameters of frames per second (fps) and SSIM of different methods for the video dehazing in <xref ref-type="fig" rid="sensors-19-01593-f012">Figure 12</xref> are shown in <xref rid="sensors-19-01593-t005" ref-type="table">Table 5</xref>. In order to meet the actual traffic scenarios, we process the video frame by frame, and the data show the total processing time for 1000 frames. Our method uses the initial frame in a time slice to calculate the transmission map and atmospheric light and adopts the lane separation to decrease the dehazing areas. Compared with other methods, the time of dehazing in our method decreases when the time slice increases. According to the experiment results, our method can obviously speed up video dehazing, especially if the video has high resolution or the driveway is only a small part of the whole image. Our method can restore the video with a resolution of 720 &#x000d7; 592 at about 57 fps, nearly four times faster than dark-channel-prior-based method and one time faster than image-contrast-enhanced method. Furthermore, our method obtains the highest SSIM for the restored videos compared with other existing methods, thus the restored videos using our method are more similar to ground truth. Therefore, the proposed method not only has superior haze removing and color balancing capabilities but also restores and enhances the degraded videos in real time.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-19-01593"><title>6. Conclusions</title><p>Traditional haze removal methods fail to restore the images with different degrees of haziness in a real-time and adaptive manner under most circumstances. To solve this problem, we propose an efficient traffic video dehazing method using adaptive dark channel prior and spatial-temporal correlations. The dark channel prior is based on the statistics of outdoor haze-free images, but it cannot adaptively estimate the initial transmission value based on the degree of haze and contrast of images. Therefore, we adopt the image-contrast-enhanced method to obtain the best estimated transmission value as the initial transmission value of dark channel prior. The image dehazing method using adaptive dark channel prior can overcome the shortcomings of existing dehazing algorithms that overstretch contrast after haze removal and deal with images with dense haze to a satisfactory level. Additionally, we introduce the temporal-spatial correlation of traffic videos to speed up the traffic video dehazing using the time continuity to set a time slice, the characteristics of block structure to refine transmission, lane space structure to decrease the restored area, and multi-camera distribution to simplify the calculation of parameters. The experiment results show that our method can restore satisfactory image appearance, which can remove dense haze effectively and does not produce results with overstretched contrast. The temporal and spatial characteristics can reduce the computation time, especially for dehazing multiple videos.</p><p>However, the dark channel prior is a kind of statistic, and it may not work for some particular traffic videos. When there are rapidly changing hazes in the videos, the dark channel of the scene radiance has a great difference at different times. In addition, if the scene objects are inherently similar to the atmospheric light and no shadow is cast on them, the adaptive dark channel prior is invalid. The dark channel of the scene radiance has bright values near such objects. As a result, our method may underestimate the transmission of these objects and overestimate the haze layer.</p></sec></body><back><notes><title>Author Contributions</title><p>Formal analysis, G.Z. and J.W.; methodology, T.D., Y.Y. and Y.S.; project administration, T.D.; validation, Y.Y.; literature search, J.W. and G.Z.; writing-original draft, T.D. and J.W.; writing-review and editing, G.Z. and Y.S. </p></notes><notes><title>Funding</title><p>This work is supported by National Natural Science Foundation of China (No. 61672414, 61572437).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-19-01593"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyka</surname><given-names>K.</given-names></name></person-group><article-title>Wavelet-Based Local Contrast Enhancement for Satellite, Aerial and Close Range Images</article-title><source>Remote Sens.</source><year>2017</year><volume>9</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.3390/rs9010025</pub-id></element-citation></ref><ref id="B2-sensors-19-01593"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>R.</given-names></name><name><surname>Pan</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Tang</surname><given-names>J.</given-names></name></person-group><article-title>Single Image Dehazing via Conditional Generative Adversarial Network</article-title><source>Proceedings of the CVPR Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;22 July 2018</conf-date><fpage>8202</fpage><lpage>8211</lpage></element-citation></ref><ref id="B3-sensors-19-01593"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangeruga</surname><given-names>M.</given-names></name><name><surname>Bruno</surname><given-names>F.</given-names></name><name><surname>Cozza</surname><given-names>M.</given-names></name><name><surname>Agrafiotis</surname><given-names>P.</given-names></name><name><surname>Skarlatos</surname><given-names>D.</given-names></name></person-group><article-title>Guidelines for Underwater Image Enhancement Based on Benchmarking of Different Methods</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>1652</elocation-id><pub-id pub-id-type="doi">10.3390/rs10101652</pub-id></element-citation></ref><ref id="B4-sensors-19-01593"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oakley</surname><given-names>J.P.</given-names></name><name><surname>Satherley</surname><given-names>B.L.</given-names></name></person-group><article-title>Improving image quality in poor visibility conditions using a physical model for contrast degradation</article-title><source>IEEE Trans. Image Process.</source><year>1998</year><volume>7</volume><fpage>167</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1109/83.660994</pub-id><pub-id pub-id-type="pmid">18267391</pub-id></element-citation></ref><ref id="B5-sensors-19-01593"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Narasimhan</surname><given-names>S.G.</given-names></name><name><surname>Nayar</surname><given-names>S.K.</given-names></name></person-group><article-title>Removing weather effects from monochrome images</article-title><source>Proceedings of the CVPR Computer Vision and Pattern Recognition</source><conf-loc>Kauai, HI, USA</conf-loc><conf-date>8&#x02013;14 December 2001</conf-date><fpage>186</fpage><lpage>193</lpage></element-citation></ref><ref id="B6-sensors-19-01593"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name></person-group><article-title>A Novel Physics-based Method for Restoration of Foggy Day Images</article-title><source>J. Image Graph.</source><year>2008</year><volume>13</volume><fpage>888</fpage><lpage>893</lpage></element-citation></ref><ref id="B7-sensors-19-01593"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>R.T.</given-names></name></person-group><article-title>Visibility in bad weather from a single image</article-title><source>Proceedings of the CVPR Computer Vision and Pattern Recognition</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>23&#x02013;28 June 2008</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B8-sensors-19-01593"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fattal</surname><given-names>R.</given-names></name></person-group><article-title>Single image dehazing</article-title><source>Proceedings of the ACM Siggraph</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>11&#x02013;15 August 2008</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B9-sensors-19-01593"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Single image haze removal using dark channel prior</article-title><source>Proceedings of the CVPR Computer Vision and Pattern Recognition</source><conf-loc>Miami, FL, USA</conf-loc><conf-date>20&#x02013;25 June 2009</conf-date><fpage>1956</fpage><lpage>1963</lpage></element-citation></ref><ref id="B10-sensors-19-01593"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Chiou</surname><given-names>C.</given-names></name><name><surname>Hsu</surname><given-names>C.</given-names></name></person-group><article-title>Single-Image Dehazing via Optimal Transmission Map Under Scene Priors</article-title><source>Circuits Syst. Video Technol.</source><year>2015</year><volume>25</volume><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="B11-sensors-19-01593"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Q.</given-names></name><name><surname>Mai</surname><given-names>J.</given-names></name><name><surname>Shao</surname><given-names>L.</given-names></name></person-group><article-title>A Fast Single Image Haze Removal Algorithm Using Color Attenuation Prior</article-title><source>IEEE Trans. Image Process.</source><year>2015</year><volume>24</volume><fpage>3522</fpage><lpage>3533</lpage><pub-id pub-id-type="pmid">26099141</pub-id></element-citation></ref><ref id="B12-sensors-19-01593"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>C.</given-names></name><name><surname>Kang</surname><given-names>L.</given-names></name><name><surname>Lee</surname><given-names>M.</given-names></name><name><surname>Lin</surname><given-names>C.</given-names></name></person-group><article-title>Haze effect removal from image via haze density estimation in optical model</article-title><source>Opt. Express</source><year>2013</year><volume>21</volume><fpage>27127</fpage><lpage>27141</lpage><pub-id pub-id-type="doi">10.1364/OE.21.027127</pub-id><pub-id pub-id-type="pmid">24216937</pub-id></element-citation></ref><ref id="B13-sensors-19-01593"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Zheng</surname><given-names>J.</given-names></name><name><surname>Zheng</surname><given-names>L.</given-names></name></person-group><article-title>Single image haze removal using content-adaptive dark channel and post enhancement</article-title><source>IET Comput. Vis.</source><year>2014</year><volume>8</volume><fpage>131</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1049/iet-cvi.2013.0011</pub-id></element-citation></ref><ref id="B14-sensors-19-01593"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>He</surname><given-names>N.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Lu</surname><given-names>K.</given-names></name></person-group><article-title>Single image dehazing with a physical model and dark channel prior</article-title><source>Neurocomputing</source><year>2015</year><volume>149</volume><fpage>718</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2014.08.005</pub-id></element-citation></ref><ref id="B15-sensors-19-01593"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name></person-group><article-title>Visibility Restoration of Single Hazy Images Captured in Real-World Weather Conditions</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2014</year><volume>24</volume><fpage>1814</fpage><lpage>1824</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2014.2317854</pub-id></element-citation></ref><ref id="B16-sensors-19-01593"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riaz</surname><given-names>I.</given-names></name><name><surname>Fan</surname><given-names>X.</given-names></name><name><surname>Shin</surname><given-names>H.</given-names></name></person-group><article-title>Single image dehazing with bright object handling</article-title><source>IET Comput. Vis.</source><year>2016</year><volume>10</volume><fpage>817</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1049/iet-cvi.2015.0451</pub-id></element-citation></ref><ref id="B17-sensors-19-01593"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>K.</given-names></name><name><surname>Wang</surname><given-names>B.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name></person-group><article-title>Real time image haze removal using bilateral filter</article-title><source>Trans. Beijing Inst. Technol.</source><year>2011</year><volume>31</volume><fpage>810</fpage><lpage>814</lpage></element-citation></ref><ref id="B18-sensors-19-01593"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Fan</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>A foggy video images enhancement algorithm of monitoring system</article-title><source>J. Xian Univ. Posts Telecommun.</source><year>2012</year><volume>5</volume><fpage>TP391.41</fpage></element-citation></ref><ref id="B19-sensors-19-01593"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kumari</surname><given-names>A.</given-names></name><name><surname>Sahdev</surname><given-names>S.</given-names></name><name><surname>Sahoo</surname><given-names>S.K.</given-names></name></person-group><article-title>Improved single image and video dehazing using morphological operation</article-title><source>Proceedings of the IEEE International Conference on VLSI Systems, Architecture, Technology and Applications</source><conf-loc>Bangalore, India</conf-loc><conf-date>8&#x02013;10 January 2015</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B20-sensors-19-01593"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>D.</given-names></name><name><surname>Treibitz</surname><given-names>T.</given-names></name><name><surname>Avidan</surname><given-names>S.</given-names></name></person-group><article-title>Non-Local Image Dehazing</article-title><source>Proceedings of the CVPR Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>1674</fpage><lpage>1682</lpage></element-citation></ref><ref id="B21-sensors-19-01593"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>D.</given-names></name><name><surname>Treibitz</surname><given-names>T.</given-names></name><name><surname>Avidan</surname><given-names>S.</given-names></name></person-group><article-title>Air-light Estimation using Haze-Lines</article-title><source>Proceedings of the IEEE 13th International Conference on Intelligent Computer Communication and Processing</source><conf-loc>Stanford, CA, USA</conf-loc><conf-date>12&#x02013;14 May 2017</conf-date><fpage>5178</fpage><lpage>5191</lpage></element-citation></ref><ref id="B22-sensors-19-01593"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tarel</surname><given-names>J.</given-names></name><name><surname>Hauti&#x000e8;re</surname><given-names>N.</given-names></name><name><surname>Cord</surname><given-names>A.</given-names></name><name><surname>Gruyer</surname><given-names>D.</given-names></name><name><surname>Halmaoui</surname><given-names>H.</given-names></name></person-group><article-title>Improved visibility of road scene images under heterogeneous fog</article-title><source>Proceedings of the IEEE Intelligent Vehicles Symposium</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>21&#x02013;24 June 2010</conf-date><fpage>478</fpage><lpage>485</lpage></element-citation></ref><ref id="B23-sensors-19-01593"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>G.</given-names></name><name><surname>Cao</surname><given-names>X.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Video dehazing with spatial and temporal coherence</article-title><source>Vis. Comput.</source><year>2011</year><volume>27</volume><fpage>749</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1007/s00371-011-0569-8</pub-id></element-citation></ref><ref id="B24-sensors-19-01593"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>D.K.</given-names></name><name><surname>Kim</surname><given-names>Y.M.</given-names></name><name><surname>Park</surname><given-names>K.T.</given-names></name><name><surname>Lee</surname><given-names>D.</given-names></name><name><surname>Choi</surname><given-names>W.</given-names></name><name><surname>Moon</surname><given-names>Y.S.</given-names></name></person-group><article-title>Video dehazing without flicker artifacts using adaptive temporal average</article-title><source>Proceedings of the IEEE International Symposium on Consumer Electronics</source><conf-loc>JeJu Island, Korea</conf-loc><conf-date>22&#x02013;25 June 2014</conf-date><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="B25-sensors-19-01593"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Jang</surname><given-names>W.</given-names></name><name><surname>Sim</surname><given-names>J.Y.</given-names></name><name><surname>Kim</surname><given-names>C.S.</given-names></name></person-group><article-title>Optimized contrast enhancement for real-time image and video dehazing</article-title><source>J. Vis. Commun. Image Represent.</source><year>2013</year><volume>24</volume><fpage>410</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/j.jvcir.2013.02.004</pub-id></element-citation></ref><ref id="B26-sensors-19-01593"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narasimhan</surname><given-names>S.G.</given-names></name><name><surname>Nayar</surname><given-names>S.K.</given-names></name></person-group><article-title>Vision and the Atmosphere</article-title><source>Int. J. Comput. Vis.</source><year>2002</year><volume>48</volume><fpage>233</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1023/A:1016328200723</pub-id></element-citation></ref><ref id="B27-sensors-19-01593"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Xie</surname><given-names>F.</given-names></name><name><surname>Jiang</surname><given-names>Z.</given-names></name><name><surname>Yin</surname><given-names>J.</given-names></name></person-group><article-title>Haze Removal for a Single Remote Sensing Image Based on Deformed Haze Imaging Model</article-title><source>IEEE Signal Process. Lett.</source><year>2015</year><volume>22</volume><fpage>1806</fpage><lpage>1810</lpage><pub-id pub-id-type="doi">10.1109/LSP.2015.2432466</pub-id></element-citation></ref><ref id="B28-sensors-19-01593"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peli</surname><given-names>E.</given-names></name></person-group><article-title>Contrast in complex images</article-title><source>J. Opt. Soc. Am. A</source><year>1990</year><volume>7</volume><fpage>2032</fpage><lpage>2040</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.7.002032</pub-id><pub-id pub-id-type="pmid">2231113</pub-id></element-citation></ref><ref id="B29-sensors-19-01593"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Bovik</surname><given-names>A.C.</given-names></name><name><surname>Sheikh</surname><given-names>H.R.</given-names></name><name><surname>Simoncelli</surname><given-names>E.P.</given-names></name></person-group><article-title>Image quality assessment: From error visibility to structural similarity</article-title><source>IEEE Trans. Image Process.</source><year>2004</year><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="B30-sensors-19-01593"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname><given-names>A.</given-names></name><name><surname>Lischinski</surname><given-names>D.</given-names></name><name><surname>Weiss</surname><given-names>Y.</given-names></name></person-group><article-title>A closed-form solution to natural image matting</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2007</year><volume>30</volume><fpage>228</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.1177</pub-id><pub-id pub-id-type="pmid">18084055</pub-id></element-citation></ref><ref id="B31-sensors-19-01593"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Guided image filtering</article-title><source>Proceedings of the Springer ECCV European Conference on Computer Vision</source><conf-loc>Heraklion, Greece</conf-loc><conf-date>5&#x02013;11 September 2010</conf-date><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="B32-sensors-19-01593"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szeliski</surname><given-names>R.</given-names></name></person-group><source>Computer Vision: Algorithms and Applications</source><publisher-name>Springer</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2010</year></element-citation></ref><ref id="B33-sensors-19-01593"><label>33.</label><element-citation publication-type="web"><article-title>Foggy Road Image DAtabase FRIDA</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.lcpc.fr/english/products/image-databases/article/frida-foggy-road-image-database">http://www.lcpc.fr/english/products/image-databases/article/frida-foggy-road-image-database</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2012-06-08">(accessed on 8 June 2012)</date-in-citation></element-citation></ref><ref id="B34-sensors-19-01593"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Cheng</surname><given-names>Y.</given-names></name></person-group><article-title>An Efficient Visibility Enhancement Algorithm for Road Scenes Captured by Intelligent Transportation Systems</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2014</year><volume>15</volume><fpage>2321</fpage><lpage>2332</lpage><pub-id pub-id-type="doi">10.1109/TITS.2014.2314696</pub-id></element-citation></ref><ref id="B35-sensors-19-01593"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>D.A.</given-names></name><name><surname>Hennessy</surname><given-names>J.L.</given-names></name></person-group><source>Computer Organization and Design: The Hardware/Software Interface</source><publisher-name>Morgan Kaufmann Publishers</publisher-name><publisher-loc>Burlington, MA, USA</publisher-loc><year>1998</year></element-citation></ref><ref id="B36-sensors-19-01593"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chapman</surname><given-names>B.</given-names></name><name><surname>Jost</surname><given-names>G.</given-names></name><name><surname>van der Pas</surname><given-names>R.</given-names></name></person-group><source>Using OpenMP: Portable Shared Memory Parallel Programming (Scientific and Engineering Computation)</source><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2008</year></element-citation></ref></ref-list></back><floats-group><fig id="sensors-19-01593-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Framework of single-image dehazing method.</p></caption><graphic xlink:href="sensors-19-01593-g001"/></fig><fig id="sensors-19-01593-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The histogram of <italic>T</italic> * <italic>X</italic> and <italic>T</italic> * <italic>X</italic>&#x02032;.</p></caption><graphic xlink:href="sensors-19-01593-g002"/></fig><fig id="sensors-19-01593-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The difference of <italic>T</italic> for the images in a 5 min cycle. The images come from different scenes (<bold>a</bold>,<bold>b</bold>).</p></caption><graphic xlink:href="sensors-19-01593-g003"/></fig><fig id="sensors-19-01593-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The images with different <italic>T</italic> values.</p></caption><graphic xlink:href="sensors-19-01593-g004"/></fig><fig id="sensors-19-01593-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Lane space separation: (<bold>a</bold>) original Image; (<bold>b</bold>) lane candidates; (<bold>c</bold>) driveway boundary; (<bold>d</bold>) result for lane separation.</p></caption><graphic xlink:href="sensors-19-01593-g005"/></fig><fig id="sensors-19-01593-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Results for video dehazing with lane separation: (<bold>a</bold>) before haze removal; (<bold>b</bold>) haze removal without lane separation; (<bold>c</bold>) haze removal with lane separation.</p></caption><graphic xlink:href="sensors-19-01593-g006"/></fig><fig id="sensors-19-01593-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Example images of the nearby regions.</p></caption><graphic xlink:href="sensors-19-01593-g007"/></fig><fig id="sensors-19-01593-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>The locations of cameras.</p></caption><graphic xlink:href="sensors-19-01593-g008"/></fig><fig id="sensors-19-01593-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Results of haze removal with and without calibration camera: (<bold>a</bold>) original image; (<bold>b</bold>) initial transmission value for calibration camera is 0.596; (<bold>c</bold>) initial transmission value for image itself is 0.578.</p></caption><graphic xlink:href="sensors-19-01593-g009"/></fig><fig id="sensors-19-01593-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Results for different initial transmission using our adaptive method.</p></caption><graphic xlink:href="sensors-19-01593-g010"/></fig><fig id="sensors-19-01593-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Comparison of the restored images using different methods; * SSIM = structural similarity.</p></caption><graphic xlink:href="sensors-19-01593-g011"/></fig><fig id="sensors-19-01593-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Comparison of restored videos. (<bold>a</bold>) Original Videos; (<bold>b</bold>) Dark-channel-prior-based method; (<bold>c</bold>) Image-contrast-enhanced method; (<bold>d</bold>) Non-local Image Dehazing; (<bold>e</bold>) Our method.</p></caption><graphic xlink:href="sensors-19-01593-g012a"/><graphic xlink:href="sensors-19-01593-g012b"/></fig><table-wrap id="sensors-19-01593-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-01593-t001_Table 1</object-id><label>Table 1</label><caption><p>The value of <italic>x&#x02019;</italic> for different ranges of <italic>T * C.</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Image No.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic>T</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic>C</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic>T * C</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic>X</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">The Range of <italic>T * C</italic></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic>X</italic>&#x02032;
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic>T</italic> * <italic>X</italic>&#x02032;
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic>T</italic> * <italic>X</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4032</td><td align="center" valign="middle" rowspan="1" colspan="1">3.8224</td><td align="center" valign="middle" rowspan="1" colspan="1">1.5414</td><td align="center" valign="middle" rowspan="1" colspan="1">0.50</td><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1"><italic>T * C</italic> &#x0003c; 10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2016</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2016</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4006</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3436</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5410</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2003</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2083</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4177</td><td align="center" valign="middle" rowspan="1" colspan="1">8.4845</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5437</td><td align="center" valign="middle" rowspan="1" colspan="1">0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2088</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2088</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4113</td><td align="center" valign="middle" rowspan="1" colspan="1">13.4080</td><td align="center" valign="middle" rowspan="1" colspan="1">5.5151</td><td align="center" valign="middle" rowspan="1" colspan="1">0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2056</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2057</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4329</td><td align="center" valign="middle" rowspan="1" colspan="1">13.2774</td><td align="center" valign="middle" rowspan="1" colspan="1">5.7476</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2164</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1991</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4444</td><td align="center" valign="middle" rowspan="1" colspan="1">17.6432</td><td align="center" valign="middle" rowspan="1" colspan="1">7.84004</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2222</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2044</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4211</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.7160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.3039</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2190</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4584</td><td align="center" valign="middle" rowspan="1" colspan="1">22.1363</td><td align="center" valign="middle" rowspan="1" colspan="1">10.1480</td><td align="center" valign="middle" rowspan="1" colspan="1">0.54</td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">10 &#x02264; <italic>T * C</italic> &#x0003c; 15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2750</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2476</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4275</td><td align="center" valign="middle" rowspan="1" colspan="1">25.5289</td><td align="center" valign="middle" rowspan="1" colspan="1">10.9141</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2565</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2736</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4732</td><td align="center" valign="middle" rowspan="1" colspan="1">26.9131</td><td align="center" valign="middle" rowspan="1" colspan="1">12.7346</td><td align="center" valign="middle" rowspan="1" colspan="1">0.62</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2839</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2934</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4370</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.9037</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.9419</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2622</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3321</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4862</td><td align="center" valign="middle" rowspan="1" colspan="1">31.3389</td><td align="center" valign="middle" rowspan="1" colspan="1">15.2359</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">15 &#x02264; <italic>T * C</italic> &#x0003c; 20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3403</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3209</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4469</td><td align="center" valign="middle" rowspan="1" colspan="1">38.3871</td><td align="center" valign="middle" rowspan="1" colspan="1">17.1555</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3128</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3754</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4987</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.6754</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.7904</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3491</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3092</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4555</td><td align="center" valign="middle" rowspan="1" colspan="1">44.9152</td><td align="center" valign="middle" rowspan="1" colspan="1">20.4609</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">20 &#x02264; <italic>T * C</italic> &#x0003c; 25</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3644</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3644</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4625</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.9075</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.5422</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3700</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3977</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4724</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.3643</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.1012</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25 &#x02264; <italic>T * C</italic> &#x0003c; 30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4252</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4441</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4812</td><td align="center" valign="middle" rowspan="1" colspan="1">63.6731</td><td align="center" valign="middle" rowspan="1" colspan="1">30.6395</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1"><italic>T * C</italic> &#x02265; 30</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4812</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4812</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4909</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.3751</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.5454</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4909</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5203</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-01593-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-01593-t002_Table 2</object-id><label>Table 2</label><caption><p>Global image and driveway.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Regions</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Parameters</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Case 1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Case 2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Case 3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Case 4</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-19-01593-i001.jpg"/>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-19-01593-i002.jpg"/>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-19-01593-i003.jpg"/>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-19-01593-i004.jpg"/>
</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Driveway Region</td><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>T</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.590856</td><td align="center" valign="middle" rowspan="1" colspan="1">0.704105</td><td align="center" valign="middle" rowspan="1" colspan="1">0.839763</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83898</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">contrast</td><td align="center" valign="middle" rowspan="1" colspan="1">47.7547</td><td align="center" valign="middle" rowspan="1" colspan="1">49.0273</td><td align="center" valign="middle" rowspan="1" colspan="1">54.0312</td><td align="center" valign="middle" rowspan="1" colspan="1">62.208</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><italic>X</italic>&#x02032;
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90000</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0000</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0000</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><italic>T</italic> * <italic>X</italic>&#x02019;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.53200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.70400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8390</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Global Image</td><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>T</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.563265</td><td align="center" valign="middle" rowspan="1" colspan="1">0.632323</td><td align="center" valign="middle" rowspan="1" colspan="1">0.773405</td><td align="center" valign="middle" rowspan="1" colspan="1">0.563549</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">contrast</td><td align="center" valign="middle" rowspan="1" colspan="1">48.8811</td><td align="center" valign="middle" rowspan="1" colspan="1">49.2619</td><td align="center" valign="middle" rowspan="1" colspan="1">57.5056</td><td align="center" valign="middle" rowspan="1" colspan="1">127.7800</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><italic>X</italic>&#x02019;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9000</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0000</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0000</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><italic>T</italic> * <italic>X</italic>&#x02019;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5070</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6320</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7730</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5660</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-01593-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-01593-t003_Table 3</object-id><label>Table 3</label><caption><p>Initial transmission values for videos in nearby regions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Cases</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Haze Flag Value <italic>T</italic></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Initial Transmission Correction Value <italic>X</italic>&#x02032;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Initial Transmission Value <italic>T</italic> * <italic>X</italic>&#x02032;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">a</td><td align="center" valign="middle" rowspan="1" colspan="1">0.524188</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.524</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">c</td><td align="center" valign="middle" rowspan="1" colspan="1">0.580732</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.581</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">b</td><td align="center" valign="middle" rowspan="1" colspan="1">0.569918</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.570</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">d</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.517431</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.517</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-01593-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-01593-t004_Table 4</object-id><label>Table 4</label><caption><p>Processing times for single-image dehazing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Image Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dark-Channel-Prior Method [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>,<xref rid="B31-sensors-19-01593" ref-type="bibr">31</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Visibility Enhancement Algorithm [<xref rid="B34-sensors-19-01593" ref-type="bibr">34</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Image-Contrast-Enhanced Method [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dehazing Only Using Adaptive Dark Channel Prior</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Non-Local Image Dehazing [<xref rid="B20-sensors-19-01593" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-01593" ref-type="bibr">21</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Our Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">640 &#x000d7; 480</td><td align="center" valign="top" rowspan="1" colspan="1">0.897 s</td><td align="center" valign="top" rowspan="1" colspan="1">1.014 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.396 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.506 s</td><td align="center" valign="top" rowspan="1" colspan="1">2.546 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.433 s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">480 &#x000d7; 400</td><td align="center" valign="top" rowspan="1" colspan="1">0.516 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.895 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.165 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.301 s</td><td align="center" valign="top" rowspan="1" colspan="1">2.387 s</td><td align="center" valign="top" rowspan="1" colspan="1">0.252 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">320 &#x000d7; 240</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.173 s</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.348 s</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.057 s</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.262 s</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">2.024 s</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.211 s</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-01593-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-01593-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparing the performance parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Case</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Image Resolution</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">He et al. [<xref rid="B9-sensors-19-01593" ref-type="bibr">9</xref>,<xref rid="B31-sensors-19-01593" ref-type="bibr">31</xref>]</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Kim et al. [<xref rid="B25-sensors-19-01593" ref-type="bibr">25</xref>]</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Our Method</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">fps</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">fps</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">fps</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSIM</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">(1)</td><td align="center" valign="middle" rowspan="1" colspan="1">640 &#x000d7; 480</td><td align="center" valign="middle" rowspan="1" colspan="1">66.787s</td><td align="center" valign="middle" rowspan="1" colspan="1">15.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6870</td><td align="center" valign="middle" rowspan="1" colspan="1">35.359 s</td><td align="center" valign="middle" rowspan="1" colspan="1">28.3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6990</td><td align="center" valign="middle" rowspan="1" colspan="1">17.507 s</td><td align="center" valign="middle" rowspan="1" colspan="1">57.1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7012</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(2)</td><td align="center" valign="middle" rowspan="1" colspan="1">640 &#x000d7; 480</td><td align="center" valign="middle" rowspan="1" colspan="1">64.576 s</td><td align="center" valign="middle" rowspan="1" colspan="1">15.4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7002</td><td align="center" valign="middle" rowspan="1" colspan="1">34.471 s</td><td align="center" valign="middle" rowspan="1" colspan="1">29.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7079</td><td align="center" valign="middle" rowspan="1" colspan="1">18.005 s</td><td align="center" valign="middle" rowspan="1" colspan="1">55.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7232</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(3)</td><td align="center" valign="middle" rowspan="1" colspan="1">720 &#x000d7; 592</td><td align="center" valign="middle" rowspan="1" colspan="1">95.638 s</td><td align="center" valign="middle" rowspan="1" colspan="1">10.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6155</td><td align="center" valign="middle" rowspan="1" colspan="1">37.858 s</td><td align="center" valign="middle" rowspan="1" colspan="1">26.4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6322</td><td align="center" valign="middle" rowspan="1" colspan="1">17.604 s</td><td align="center" valign="middle" rowspan="1" colspan="1">56.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6488</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(4)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">720 &#x000d7; 592</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.911 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5932</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.855 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6011</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.925 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6155</td></tr></tbody></table></table-wrap></floats-group></article>