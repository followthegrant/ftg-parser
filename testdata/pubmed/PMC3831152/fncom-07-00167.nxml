<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Comput Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Comput Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Comput. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Computational Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5188</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">24302910</article-id><article-id pub-id-type="pmc">3831152</article-id><article-id pub-id-type="doi">10.3389/fncom.2013.00167</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research Article</subject></subj-group></subj-group></article-categories><title-group><article-title>Visual motion integration is mediated by directional ambiguities in local motion signals</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rocchi</surname><given-names>Francesca</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ledgeway</surname><given-names>Tim</given-names></name><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref></contrib><contrib contrib-type="author"><name><surname>Webb</surname><given-names>Ben S.</given-names></name></contrib></contrib-group><aff><institution>School of Psychology, University of Nottingham</institution><country>Nottingham, UK</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Terrence J. Sejnowski, The Salk Institute for Biological Studies, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Yoram Burak, Hebrew University, Israel; Tim Gollisch, University Medical Center G&#x000f6;ttingen, Germany</p></fn><corresp id="fn001">*Correspondence: Tim Ledgeway, Visual Neuroscience Group, School of Psychology, University of Nottingham, Room C64, University Park, NG7 2RD Nottingham, UK e-mail: <email xlink:type="simple">timothy.legeway@nottingham.ac.uk</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to the journal Frontiers in Computational Neuroscience.</p></fn></author-notes><pub-date pub-type="epub"><day>18</day><month>11</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>7</volume><elocation-id>167</elocation-id><history><date date-type="received"><day>09</day><month>6</month><year>2013</year></date><date date-type="accepted"><day>29</day><month>10</month><year>2013</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2013 Rocchi, Ledgeway and Webb.</copyright-statement><copyright-year>2013</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>The output of primary visual cortex (V1) is a piecemeal representation of the visual scene and the response of any one cell cannot unambiguously guide sensorimotor behavior. It remains unsolved how subsequent stages of cortical processing combine (&#x0201c;pool&#x0201d;) these early visual signals into a coherent representation. We (Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>, <xref ref-type="bibr" rid="B31">2011</xref>) have shown that responses of human observers on a pooling task employing broadband, random dot motion can be accurately predicted by decoding the maximum likelihood direction from a population of motion-sensitive neurons. Whereas Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>) found that the vector average velocity of arrays of narrowband, two-dimensional (2-d) plaids predicts perceived global motion. To reconcile these different results, we designed two experiments in which we used 2-d noise textures moving behind spatially distributed apertures and measured the point of subjective equality between pairs of global noise textures. Textures in the <italic>standard</italic> stimulus moved rigidly in the same direction, whereas their directions in the <italic>comparison</italic> stimulus were sampled from a set of probability distributions. Human observers judged which noise texture had a more clockwise (CW) global direction. In agreement with Amano and colleagues, observers' perceived global motion coincided with the vector average stimulus direction. To test if directional ambiguities in local motion signals governed perceived global direction, we manipulated the fidelity of the texture motion within each aperture. A proportion of the apertures contained texture that underwent rigid translation and the remainder contained dynamic (temporally uncorrelated) noise to create locally ambiguous motion. Perceived global motion matched the vector average when the majority of apertures contained rigid motion, but with increasing levels of dynamic noise shifted toward the maximum likelihood direction. A class of population decoders utilizing power-law non-linearities can accommodate this flexible pooling.</p></abstract><kwd-group><kwd>vision</kwd><kwd>global motion</kwd><kwd>local motion</kwd><kwd>spatial pooling</kwd><kwd>vector average</kwd><kwd>maximum likelihood</kwd><kwd>winner-take-all</kwd></kwd-group><counts><fig-count count="6"/><table-count count="0"/><equation-count count="6"/><ref-count count="35"/><page-count count="9"/><word-count count="6780"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>One of the fundamental computational challenges faced by the visual system is to extract information about movement in the world from the patterns of light that form the retinal image. It is well-established that in the primate, motion sensitivity first emerges in primary visual cortex (area V1) where many visual neurons exhibit direction-selectivity to local edges and contours that move within their receptive fields. However, these neurons have relatively small receptive fields (e.g., Hubel and Wiesel, <xref ref-type="bibr" rid="B10">1968</xref>) and consequently suffer from the &#x0201c;aperture problem,&#x0201d; in that the response of any one cell cannot convey the global (overall) direction of a spatially extensive object translating across the visual field. That is, neural responses based upon a purely local analysis of motion are inherently ambiguous and are potentially consistent with many possible real-world situations (Stumpf, <xref ref-type="bibr" rid="B27">1911</xref>; Wallach, <xref ref-type="bibr" rid="B28">1935</xref>). In principle, this ambiguity can be overcome by pooling the outputs of local motion detectors across space and over time. Although the site of this spatiotemporal pooling is consistent with the known properties of extrastriate cortical areas such as V5/MT, where motion-sensitive cells have receptive fields much larger (~100 times) than those in V1 (Gattass and Gross, <xref ref-type="bibr" rid="B9">1981</xref>; Albright and Desimone, <xref ref-type="bibr" rid="B3">1987</xref>), the precise nature of the global motion computation is still the subject of much uncertainty.</p><p>A variety of solutions have been proposed to account for the pooling of local motion signals in the human visual system. Early psychophysical studies tended to employ moving plaid patterns (composed of two, superimposed drifting gratings of different orientations) to investigate the computation underlying global motion perception. For example, Adelson and Movshon (<xref ref-type="bibr" rid="B1">1982</xref>) suggested that the visual system might integrate spatially one-dimensional (1-d) component motion signals into a global percept by utilizing the <italic>intersection of constraints</italic> (IOC) rule across different orientations. In this rule each 1-d component is represented as a vector orthogonal to its orientation and velocity constraint lines (indicating the range of possible velocities for each component), drawn perpendicular to each vector. These constraint lines intersect at a single point in velocity space that reveals the plaid's true motion. The IOC is a mathematically elegant solution to the problem of determining the global direction of a rigidly moving object from its spatially 1-d features, and computational models based on this principle have been developed (Simoncelli and Heeger, <xref ref-type="bibr" rid="B24a">1998</xref>). However, its complexity and inability to generalize to non-rigid movement (i.e., when the retinal image of an object deforms as it moves) cast doubt on its utility as a general model of human global motion processing. An alternative to the IOC, is the <italic>vector average</italic> (VA) or <italic>vector sum</italic><xref ref-type="fn" rid="fn0001"><sup>1</sup></xref> rule (Ferrera and Wilson, <xref ref-type="bibr" rid="B8">1990</xref>; Wilson et al., <xref ref-type="bibr" rid="B34">1992</xref>; Kim and Wilson, <xref ref-type="bibr" rid="B14">1993</xref>), in which each spatially 1-d motion component is again represented as a vector orthogonal to its orientation and the two vectors are combined additively to determine the resultant direction of image motion. For example Yo and Wilson (<xref ref-type="bibr" rid="B35">1992</xref>) found that when two drifting gratings with identical spatial frequencies and contrasts but different speeds are superimposed to form a type II plaid pattern (which has different IOC and VA predictions), its perceived global direction at brief stimulus exposures (&#x0003c;60 ms), was readily predicted by the VA of the local directions. Similarly Wilson and colleagues have shown that the perceived direction of plaids constructed from two, second-order, contrast-modulated gratings is consistent with a VA computation of the directions of the individual components. Thus, for relatively simple narrowband stimuli such as plaids, the IOC and VA have been proposed as candidate solutions for the problem of encoding the global direction of the image. However, they are predominantly stimulus-based theories of global motion processing, albeit with different assumptions, based on rules rather than the known computations of the underlying neural mechanisms.</p><p>More recent psychophysical investigations of the computational principles of global motion processing in human vision have tended to employ broadband random-dot-kinematograms (RDKs) as stimuli. In RDKs a dense spatial array of randomly-positioned dots are displaced over space and time, according to a set of rules, to create the impression of net motion in a given direction. Williams and Sekuler (<xref ref-type="bibr" rid="B33">1984</xref>), for example, employed RDKs in which the individual dot directions were selected from a uniform probability distribution, such that each dot was assigned an independent, random walk in direction over time. They reported that provided that the range of local dot directions was constrained, the stimulus appeared to drift <italic>en masse</italic> in a global direction close to the VA of the dot directions. Several other studies (e.g., Watamaniuk and Duchon, <xref ref-type="bibr" rid="B29">1992</xref>; Zohary et al., <xref ref-type="bibr" rid="B36">1996</xref>) have also identified different stimulus-based summary statistics (e.g., mean or mode of the dot motions) which best characterize the perceived global direction or speed of RDKs. We (Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>) have recently shown, however, that when local dot directions are drawn from asymmetric probability distributions, image summary statistics are consistently poor predictors of perceived global motion direction. Instead mechanism-based, read-out algorithms, such as maximum-likelihood (ML) or winner-take-all (WTA) decoders, operating on the outputs of a simulated population of biologically-plausible, direction-selective neurons, offer a much more accurate and robust guide to human global motion perception (Webb et al., <xref ref-type="bibr" rid="B31">2011</xref>). Given that the visual system has no direct access to any motion stimulus, only the responses of neural mechanisms on which to base decisions, this approach highlights the importance of considering mechanism-based explanations of global motion phenomena.</p><p>The presentation of moving stimuli within multiple apertures provides a useful tool for studying the factors that determine how local motion estimates are integrated across space, in both artificial and naturalistic stimuli (e.g., Mingolla et al., <xref ref-type="bibr" rid="B18">1992</xref>; Alais et al., <xref ref-type="bibr" rid="B2">1998</xref>; Kane et al., <xref ref-type="bibr" rid="B12">2009</xref>, <xref ref-type="bibr" rid="B13">2011</xref>). For example to probe the nature of the pooling process underlying global motion perception, Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>) developed a novel multi-aperture hybrid stimulus that contains aspects of both narrowband stimuli (e.g., plaids) used in conventional studies of global motion and also the multi-element nature of RDKs. In a series of psychophysical experiments they showed that when a moving surface consists of arrays of either Gabors (1-d) or plaids (2-d) moving behind fixed, spatially restricted apertures, the visual system employs different computations for pooling 1-d and 2-d local motion signals. According to Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>), the spatial integration of 1-d motion information conforms to the IOC rule, whereas the spatial pooling of local 2-d motion signals follows the VA of the physical velocities present in the stimulus. The authors concluded that spatial motion integration exhibits great flexibility and the strategy employed by the human visual system to encode global motion is dependent on the stimulus characteristics. Most importantly, for 2-d stimuli in which there is no local directional ambiguity (i.e., plaid micropatterns for which the aperture problem can be solved at each location in space) perceived direction appeared to be broadly consistent with a VA strategy of those 2-d directions across space. This result appears to be in stark contrast to that found by Webb et al. (<xref ref-type="bibr" rid="B30">2007</xref>), which suggested that for spatially 2-d moving stimuli such a random dots a neural-based explanation, based on say a ML or WTA computation, might be the key to understanding the nature of the pooling process under a wide-range of conditions. Indeed Webb et al. (<xref ref-type="bibr" rid="B31">2011</xref>) have subsequently shown that caution should be exercised before concluding that changes in behavior on a task, driven by changes in the stimulus configuration, is evidence of a flexible motion pooling system that uses different computations under different circumstances. They found that a single, ML computation could accommodate the apparently flexible nature of spatiotemporal motion pooling using RDKs in humans.</p><p>Thus, considerable uncertainty still remains concerning the computations that govern perceived global direction in human vision. The present study aimed to reconcile the disparate results reported by Webb et al. (<xref ref-type="bibr" rid="B30">2007</xref>) and Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>), and clarify the nature of the spatial pooling of local 2-d motion signals, by conducting two experiments that incorporated key elements of both of these studies. In Experiment 1 we used a novel multi-element stimulus consisting of spatially 2-d (isotropic), broadband noise textures that moved behind fixed, spatial apertures. This allowed us to determine if differences in perceived global motion reported in previous studies depend critically on the choice of stimulus-based characteristics (i.e., constraining local motions to a set of sparse spatial apertures). To test whether or not the local ambiguity of the direction information contained within each region of space is the principal factor that governs the computation underlying perceived global direction, in Experiment 2 we systematically varied the fidelity of the texture motion within each spatial aperture. In addition for each experiment we compared the psychophysical performance of the human observers to the performance of a neural population-decoding model utilizing either a VA, ML, or WTA algorithm.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and methods</title><sec><title>Observers</title><p>Five observers took part in the experiments. Observer FR was one of the authors and the remaining observers (AA, LX, KH, and ZH) were all na&#x000ef;ve to the purpose of the study. All had normal or corrected-to-normal acuity and no history of ocular ill health. Informed consent was obtained from all subjects and experimental procedures met the ethical guidelines of the School of Psychology at the University of Nottingham.</p></sec><sec><title>Apparatus and stimuli</title><p>Global motion stimuli were computer generated and displayed on a <italic>LaCie Electron 22blue</italic> CRT monitor with a spatial resolution of 1024 &#x000d7; 768 pixels and at a frame rate of 75 Hz. The monitor was carefully gamma-corrected with a spot photometer (<italic>Konica Minolta LS-110</italic>) and internal look up tables. Psychophysical procedures were also used to confirm that any residual luminance non-linearities were minimized (Ledgeway and Smith, <xref ref-type="bibr" rid="B16">1994</xref>). The mean luminance of the display was 25 cd/m<sup>2</sup>. Viewing was binocular and from a distance of 76.3 cm, such that one screen pixel subtended 1.35 arc min of visual angle.</p><p>The motion stimuli used were similar in principle to those employed in previous studies of &#x0201c;motion-defined&#x0201d; contours (e.g., Ledgeway and Hess, <xref ref-type="bibr" rid="B15">2002</xref>) and were presented within the confines of a circular display window (diameter 17.28&#x000b0;). Each stimulus consisted of a random spatial array of multiple patches of isotropic, uniformly distributed (spatially 2-d) noise, presented on a uniform background of mean luminance (see Figure <xref ref-type="fig" rid="F1">1</xref>). Each patch was composed of binary random noise elements (0.05&#x000b0; square) presented within a smooth stationary Gaussian spatial envelope (<italic>SD</italic> 0.23&#x000b0;, truncated at &#x000b1; 0.56&#x000b0;). The Michelson contrast of the noise texture within each aperture was 0.99. The noise texture within each aperture could be either displaced (Experiment 1), or replaced with another stochastic sample (Experiment 2), at an update rate of 37.5 Hz. On each update the noise could be made to move in any desired direction, independently of both its previous displacements and those of neighboring apertures in the image, at a drift speed of 4.64&#x000b0;/s. The minimum center-to-center spacing between apertures was 1.13&#x000b0; and all noise textures were presented for 530 ms.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Illustration of the multi-element stimulus used to investigate the computations that govern performance on tasks that require human subjects to pool local visual motion signals across space.</bold> The stimulus consisted of spatially 2-d (isotropic), broadband noise textures that moved behind fixed, spatially distributed apertures. Observers judged which of two sequentially-presented stimuli (<italic>standard</italic> or <italic>comparison</italic>) moved in a more clockwise global direction of motion. The <italic>standard</italic> was composed of noise textures all moving rigidly in the same direction. For the <italic>comparison</italic> stimulus the individual texture directions on each displacement were sampled (with replacement) from probability distributions designed to distinguish between different global motion encoding strategies.</p></caption><graphic xlink:href="fncom-07-00167-g0001"/></fig></sec><sec><title>Procedure</title><p>A two-alternative forced-choice (2AFC) task was employed. On each trial observers were presented with a central fixation cross, followed by the sequential presentation of two motion stimuli (each for 530 ms), separated by an inter-stimulus interval of 500 ms. One of the stimuli contained noise textures that all moved in the same direction (termed the <italic>standard</italic>) and the other (termed the <italic>comparison</italic>) contained noise textures whose directions were sampled from an underlying probability distribution (see Direction distributions below). The order of presentation of the <italic>standard</italic> and the <italic>comparison</italic> stimuli was randomized on each trial. The observers were required to judge the angular difference between the global directions of the two global noise texture stimuli (<italic>standard</italic> and <italic>comparison</italic>) and identify the one that moved in the more clockwise (CW) global motion direction. For example if the stimulus presented first on a given trial was perceived as translating (say) vertically upwards, equivalent to 12 o'clock, and the second stimulus toward 1 o'clock, then the observer would respond that the stimulus in the second interval had the most CW direction. For both experiments the global direction of the <italic>comparison</italic> stimulus on each trial could be either the same, slightly more CW or slightly more counter-clockwise (CCW) than the <italic>standard</italic>. This was done using the method of constant stimuli, in which the magnitude of the angular direction difference between the two stimuli on each trial was randomly chosen from a set of nine predetermined values (selected on the basis of pilot studies to bracket the Point of Subjective Equality&#x02014;PSE). This procedure effectively avoids systematic order effects whilst allowing the experimenter full control of the stimulus set presented. Following the observer's response the fixation cross was presented for 500 ms before the next trial commenced.</p><p>Each observer completed a minimum of four runs of 90 trials for each condition tested and the resulting data were used to derive a psychometric function, expressing the percentage of trials on which the <italic>comparison</italic> was judged to be more CW than the <italic>standard</italic> as a function of the angular difference between them, as illustrated in Figure <xref ref-type="fig" rid="F3">3</xref>. Each psychometric function was fitted with the following logistic function in order to estimate the PSE (the angular difference at which the observer judged the two motion stimuli to have the same perceived direction):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>P</italic><sub><italic>cw</italic></sub> is the percentage of <italic>comparison</italic> CW responses, <italic>x</italic> is the direction difference between the global noise texture stimuli, <italic>a</italic> is the PSE and <italic>b</italic> is an estimate of the slope of the function at the inflection point.</p></sec><sec><title>Direction distributions</title><p>In Experiment 1 the <italic>standard</italic> stimulus was composed of all local noise textures moving rigidly in the same direction (randomly chosen, on each trial, from the 360&#x000b0; range). Whereas, on each displacement, the individual texture directions of the <italic>comparison</italic> stimulus were sampled discretely, with replacement, from one of a set of 12 skewed probability distributions (with either a Gaussian or a rectangular profile) with a total range of 180 deg (see Figure <xref ref-type="fig" rid="F2">2</xref>). Although the texture within each spatial aperture could therefore change its motion direction frequently during a single trial, the stimulus nonetheless appeared to drift <italic>en masse</italic> in a global direction as it does under analogous circumstances using RDKs (Williams and Sekuler, <xref ref-type="bibr" rid="B33">1984</xref>). The global direction of each of these 12 probability distributions could be set to any desired value so that it was either the same or different to that of the <italic>standard</italic> stimulus on each trial, in order to generate psychometric functions like that illustrated in Figure <xref ref-type="fig" rid="F3">3</xref>. These probability distributions, collectively, were designed to distinguish between different global motion encoding schemes (i.e., stimulus-based statistics vs. neural-based population decoders). We independently varied the half-widths and sampling densities of the CCW and CW halves of the distributions (relative to the modal direction for Gaussian distributions and the median direction for uniform distributions). We have used these particular distributions previously in analogous studies using RDKs and their construction and characteristic properties are extensively documented elsewhere (for details see Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>, <xref ref-type="bibr" rid="B31">2011</xref>).</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Distributions of texture directions used for the different <italic>comparison</italic> stimuli.</bold> The directions of the global noise textures were drawn from asymmetric Gaussian <bold>(A,B)</bold> or uniform <bold>(C)</bold> probability distributions. The half-widths and sampling densities of the counter-clockwise (CCW) and clockwise (CW) halves of the distributions (relative to the modal direction for Gaussian distributions and the median direction for uniform distributions) were varied to distinguish between different global motion encoding schemes. These probability distributions have been used previously in studies of global motion employing RDKs and are fully documented elsewhere (see Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>, <xref ref-type="bibr" rid="B31">2011</xref>).</p></caption><graphic xlink:href="fncom-07-00167-g0002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Example psychometric function for observer FR showing the percentage of trials on which the <italic>comparison</italic> was judged as more clockwise than the <italic>standard</italic> as a function of the difference between their modal directions.</bold> The smooth line through the data points is the best-fitting logistic function based on Equation 1. The dotted lines indicate how the PSE (angular difference at which the two motion stimuli were judged to have the same perceived direction) was derived.</p></caption><graphic xlink:href="fncom-07-00167-g0003"/></fig><p>In Experiment 2 the fidelity of the texture motion within each spatial aperture was manipulated to investigate if the local ambiguity of the direction information contained within each region of space could govern the computation underlying perceived global direction. On each displacement, a percentage of the apertures in the <italic>standard</italic> stimulus (either 25, 37.5, 75, or 100%) contained textures that moved rigidly in a common direction whereas the remainder contained dynamic visual noise (that was uncorrelated over time) to create locally ambiguous motion. Similarly in the <italic>comparison</italic> stimulus a percentage of the apertures (same as the <italic>standard</italic>) contained textures with directions sampled (with replacement), on each displacement, from a probability distribution that was diagnostic for distinguishing between VA and ML (or WTA) predictions. The remaining apertures contained dynamic uncorrelated noise that created locally ambiguous motion. The proportion of apertures manipulated in this manner, which we term the aperture coherence, was kept the same, across trials, between <italic>standard</italic> and <italic>comparison</italic> stimuli.</p></sec><sec><title>Model predictions</title><p>We simulated the integration of local directions by reading out the responses of a set of motion-sensitive mechanisms, with biologically-plausible properties akin to neurons found in visual cortex (e.g., MT), using VA<xref ref-type="fn" rid="fn0002"><sup>2</sup></xref>, ML and WTA decoders. This model has been described in detail elsewhere (e.g., Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>) and is depicted schematically in Figure <xref ref-type="fig" rid="F4">4</xref>. Briefly, it is composed of a set of evenly spaced direction-tuned mechanisms spanning the 360&#x000b0; range, each with a Gaussian sensitivity profile to motion direction (half-width, half-height bandwidth of 45&#x000b0;) and response corrupted by Poisson noise. The sensitivity of the <italic>i</italic>th mechanism, centered on directionn &#x003b8;<sub><italic>i</italic></sub>, is given by:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>{</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>log</mml:mi><mml:mn>2</mml:mn><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>h</italic> is the tuning bandwidth (Figure <xref ref-type="fig" rid="F4">4B</xref>). The response of the <italic>i</italic>th mechanism to a moving texture stimulus <italic>D</italic> (see Figure <xref ref-type="fig" rid="F4">4A</xref>) with a distribution of directions, <italic>D</italic>(&#x003b8;), is then:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mtext>&#x0200a;</mml:mtext><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>360</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>R</italic><sub>max</sub> is the maximum mean firing rate of the mechanism (fixed at 60 spikes/s), <italic>t</italic> is the duration of the stimulus and <italic>pr</italic>{<italic>D</italic>(&#x003b8;)} is proportion of texture directions in the probability distribution. The number of spikes (<italic>n</italic><sub><italic>i</italic></sub>) evoked by a stimulus is Poisson distributed with a mean of <italic>R</italic><sub><italic>i</italic></sub>(<italic>D</italic>), similar to the firing statistics of many cortical neurons which typically exhibit Poisson-like variability (e.g., Softky and Koch, <xref ref-type="bibr" rid="B26">1993</xref>; Shadlen and Newsome, <xref ref-type="bibr" rid="B24">1998</xref>).</p><p><disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>{</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
The pattern of neuronal activity across the population of mechanisms in response to a given stimulus, as exemplified by Figure <xref ref-type="fig" rid="F4">4C</xref>, is then decoded using either a VA, ML, or WTA read-out algorithm to identify the global direction of image motion. VA estimates perceived global direction by averaging the preferred directions of all mechanisms weighted by the magnitude of their respective responses:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>tan</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>360</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mi>sin</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>360</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>cos</mml:mi></mml:mrow></mml:mstyle><mml:mtext>&#x000a0;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
ML estimates perceived direction from the weighted sum of responses of the population of mechanisms by multiplying the activity of each mechanism by the log of its tuning function (e.g., Jazayeri and Movshon, <xref ref-type="bibr" rid="B11">2006</xref>):
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mi>log</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>360</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mi>log</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
ML direction is then given by the value of &#x003b8;<sub><italic>i</italic></sub> for which log <italic>L</italic>(<italic>D</italic>), computed for all <italic>D</italic>, is maximal. Although the ML calculation is identical to that used in our previous work, in which it disambiguates neural activity related to Poisson variability, it is certainly not optimal under circumstances when sources of ambiguity also arise in the stimulus (e.g., from the presence of stimulus-based noise or from the existence of multiple directions in the scene). Although that ML computation was initially tailored to deal with coherent motion throughout the visual scene, we utilize it in the present paper because it allows us to investigate the general structure of the neural computation underlying global motion perception and compare its performance with that of the VA and WTA algorithms, across a range of conditions, using the same model architecture.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Schematic illustration of the model used to simulate trial-by-trial performance on the global direction discrimination task.</bold> The set of directions present in the stimulus <bold>(A)</bold> activates a bank of direction-tuned mechanisms, each with a Gaussian sensitivity profile corrupted by Poisson noise <bold>(B)</bold>. The population response <bold>(C)</bold>, is then decoded using either a VA, ML, or WTA read-out algorithm to identify the global direction of motion. See text for further details.</p></caption><graphic xlink:href="fncom-07-00167-g0004"/></fig><p>WTA calculates perceived global motion as the preferred direction of the most active mechanism. That is, the value of &#x003b8;<sub><italic>i</italic></sub> for which <italic>n</italic><sub><italic>i</italic></sub> in the population response is maximal.</p><p>We simulated each model's performance on the direction discrimination task, on a trial-by-trial basis, using analogous methods to those used in the psychophysical experiments. It is important to note that space is not represented explicitly in these simulations and the inputs to the model were not actual instantiations of visual images, but simply the sets of directions that would normally be assigned to the textures of the <italic>standard</italic> and <italic>comparison</italic> stimuli on each trial (those for the latter were generated by sampling, with replacement, the probability distributions depicted in Figure <xref ref-type="fig" rid="F2">2</xref>). In the case of Experiment 2, whenever an aperture contained dynamic uncorrelated noise (ambiguous motion), this was assumed to contribute equal motion energy in all directions and consequently activate all motion-sensitive mechanisms in the model uniformly to a small degree.</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><sec><title>Experiment 1</title><p>Figure <xref ref-type="fig" rid="F5">5</xref> shows how the mean perceived global direction of the observers (<italic>N</italic> = 4) and the corresponding estimates derived from the three population decoders, indicated by different symbols, changed as a function of skew for each of the 12 <italic>comparison</italic> distributions tested. It is readily apparent that the pattern of psychophysical results found using spatially-windowed arrays of moving noise textures differs markedly from that found using conventional RDKs (Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>). Indeed irrespective of whether the directions of the global noise textures were drawn from asymmetric Gaussian (Figures <xref ref-type="fig" rid="F5">5A,B</xref>) or uniform (Figure <xref ref-type="fig" rid="F5">5C</xref>) probability distributions, overall the performance of the observers was most consistent with that of the VA decoder [mean square error (MSE) between the perceived and model predictions was 12.83]. This was especially evident for the case of the <italic>comparison</italic> distributions used in Figures <xref ref-type="fig" rid="F5">5B,C</xref>, where the predictions of the ML and WTA population decoding algorithms diverged by as much as 34&#x000b0; away from the results obtained with the human observers (MSE 335.53 and 304.91, respectively) compared with only 7.6&#x000b0; for the VA prediction. Thus, the results are broadly consistent with those of Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>), in that the integration of 2-d local motion signals across space appears to be governed by a VA computation, when those motions are confined to set of fixed spatial apertures.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Results of Experiment 1 showing the perceived global direction (averaged across four observers) and the corresponding model estimates derived from the three population decoders, indicated by different symbols, as a function of skew for each of the 12 <italic>comparison</italic> distributions tested.</bold> The directions of the global noise textures were drawn from asymmetric Gaussian <bold>(A,B)</bold> or uniform <bold>(C)</bold> probability distributions, and the overall pattern of performance of the observers is most compatible with a VA decoder. The error bars above and below each data point represent &#x000b1;1 SE.</p></caption><graphic xlink:href="fncom-07-00167-g0005"/></fig></sec><sec><title>Experiment 2</title><p>To investigate whether or not the local ambiguity of the direction information contained within each region of space influences the computation underlying perceived global direction, the fidelity of the texture motion within each spatial aperture was manipulated. Dynamic, but uncorrelated, visual noise was presented within a proportion of the apertures to create locally ambiguous motion. The results for one of the diagnostic skewed (uniform) <italic>comparison</italic> distributions used previously in Figure <xref ref-type="fig" rid="F5">5C</xref> (with a CCW range of 130&#x000b0; and a CW range of 50&#x000b0;) are shown in Figure <xref ref-type="fig" rid="F6">6</xref>. The mean perceived global direction of the observers (<italic>N</italic> = 4) and the corresponding estimates derived from the three population decoders, indicated by different symbols, are plotted as a function of the aperture coherence (percentage of apertures in the stimuli that contained coherent texture directions). When the motion signals within the spatial apertures consisted of only coherent texture directions (100% aperture coherence), observers' global motion perception once again closely followed the VA prediction. However, when more than half of the apertures contained uncorrelated dynamic noise (i.e., aperture coherence &#x0003c;50%), creating substantial local ambiguity, the perceived global direction shifted progressively toward that of the ML and WTA population decoder predictions, which were similar. Hence the results of this experiment clearly show that global motion perception is dramatically affected by the local ambiguity of the direction information contained within each aperture.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Results of Experiment 2 showing the perceived global direction (averaged across four observers) and the corresponding model estimates derived from the three population decoders, indicated by different symbols, as a function of the percentage of apertures in the stimulus that contained coherent texture directions.</bold> For the <italic>comparison</italic> stimulus the individual texture directions on each displacement were sampled (with replacement) from one of the probability distributions used previously in Figure <xref ref-type="fig" rid="F5">5C</xref> (CCW range of 130&#x000b0; and CW range of 50&#x000b0;) that was diagnostic for distinguishing between VA, ML, and WTA estimates. The error bars above and below each data point represent &#x000b1;1 SE.</p></caption><graphic xlink:href="fncom-07-00167-g0006"/></fig></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>The experiments conducted in the current study were designed to further explore the nature of the principles governing the spatial pooling of 2-d motion signals in human vision. In particular we were motivated to reconcile recent contrasting findings that have led to disparate ideas on how the visual system might encode the global direction of image motion. As discussed previously, Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>) in a comprehensive examination of this issue presented evidence that the integration of local 2-d motions across space, conforms closely to the VA rule applied to the stimulus motions. However, Webb et al. (<xref ref-type="bibr" rid="B30">2007</xref>, <xref ref-type="bibr" rid="B31">2011</xref>) reached a different conclusion. They demonstrated using RDKs that the spatial pooling of motion information is in general not well characterized by any stimulus-based metric, including the VA rule. Instead they showed that a neural mechanism-based solution, such as a ML or WTA computation, was a much better predictor of the psychophysical performance. A potentially critical difference between these sets of studies lies in the choice of stimuli, display configuration and methods. Amano and colleagues used novel arrays of narrowband plaids moving behind stationary, Gaussian apertures and only considered stimulus-based solutions to global motion such as the IOC and the VA of the physical motions present in the image. Furthermore, when investigating the spatial pooling of 2-d local motions, Amano et al. based their conclusions on measures of perceived global speed and did not measure perceived global direction directly. In contrast we (e.g., Webb et al., <xref ref-type="bibr" rid="B30">2007</xref>) used conventional broadband RDKs, measured perceived global direction explicitly and compared psychophysical performance to mechanism-based predictions, based on decoding the responses of a biologically-inspired model of motion processing. Hence in the present study we sought to incorporate elements of both studies in an attempt to reconcile these very different approaches and reveal the nature of the pooling process underlying global motion perception.</p><p>In Experiment 1, we investigated if presenting rigidly-moving, spatially 2-d broadband noise textures within fixed, spatial apertures was sufficient to produce a different pooling strategy from that suggested by Webb et al. (<xref ref-type="bibr" rid="B30">2007</xref>) using RDKs. The results (Figure <xref ref-type="fig" rid="F5">5</xref>) showed that this was indeed the case and psychophysical performance under these conditions closely mirrored those of Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>), in that the perceived global motion direction coincided with the VA of the local motions within the stimulus (a VA read-out algorithm applied to a model neural population of motion-sensitive mechanisms produces the same prediction). However, this manipulation in itself does not explain why such gross differences in stimulus configuration, should lead to a qualitative change in the nature of global motion processing.</p><p>To address this issue further we designed an additional experiment to manipulate the fidelity of the motion present within each spatial aperture. In Experiment 2 we sought to explore whether or not the local directional ambiguity that characterizes each region of space within the stimulus is a key factor in determining the computational strategy used by the visual system to pool visual motion signals. That is, the human visual system could exhibit some degree of flexibility with regard to motion pooling and be able to exploit different solutions under different circumstances, as suggested by Amano et al. (<xref ref-type="bibr" rid="B4">2009</xref>) for spatially 1-d and 2-d patterns. It is important to note that even when spatial apertures are not present within a display, any spatially local region of a stochastically moving image such as the random-walk RDKs used by Webb et al. (<xref ref-type="bibr" rid="B30">2007</xref>, <xref ref-type="bibr" rid="B31">2011</xref>), will necessarily contain a range of dot directions and hence at least some degree of directional ambiguity. The results of this experiment clearly showed marked differences between spatial integration in the presence of ambiguous local direction signals and unambiguous visual motion pooling. That is, a VA decoder accurately predicted the perceived global direction of image motion when the vast majority of apertures contained rigid texture movement (Figure <xref ref-type="fig" rid="F6">6</xref>). However, as the percentage of ambiguous motion present in the stimulus was progressively increased (i.e., dynamic uncorrelated noise was introduced to more apertures on each image update), the observers' global direction judgments shifted toward the ML and WTA predictions. It is important to reiterate, however, that we are not claiming that the ML scheme implemented in the present paper is an optimal decoding strategy under the current circumstance, as the model was not tailored to the specific stimuli employed and does not take into account the considerable directional ambiguities arising from the stimuli, especially in Experiment 2. However, we were interested in the generality of the ML algorithm, that we and others have previously applied to RDKs (e.g., Jazayeri and Movshon, <xref ref-type="bibr" rid="B11">2006</xref>), and how it compares to the behavior of other (non-optimal) algorithms such as the VA, that are typically used to explain global motion processing, when implemented within a common, biologically-inspired, framework.</p><p>It is tempting to explain these findings in terms of an adaptive process that switches between a VA and a ML (or alternatively a WTA) computation depending on the degree of local ambiguity or directional uncertainty present in the stimulus. That is, the human visual system could exhibit some degree of flexibility with regard to motion pooling and is able to exploit different solutions under different circumstances. Such a general notion is not incompatible with the response properties of some neurons in macaque MT (Pack and Born, <xref ref-type="bibr" rid="B21">2001</xref>; Smith et al., <xref ref-type="bibr" rid="B25">2005</xref>; Majaj et al., <xref ref-type="bibr" rid="B17">2007</xref>), task-dependent modulations of following and pursuit eye movements (Recanzone and Wurtz, <xref ref-type="bibr" rid="B22">1999</xref>; Ferrera, <xref ref-type="bibr" rid="B7">2000</xref>; Barthelemy et al., <xref ref-type="bibr" rid="B6">2010</xref>) and recent MEG and fMRI investigations of the responses of the human homologue of MT (hMT+) to spatially-distributed 1-d motion signals (Amano et al., <xref ref-type="bibr" rid="B5">2012</xref>). This does, however, raise the issue of actually how the visual system selects the motion pooling strategy that is best suited to a particular stimulus and task. Furthermore, we have previously shown (Webb et al., <xref ref-type="bibr" rid="B31">2011</xref>) that changes in performance on global motion tasks, resulting from changes in the stimulus configuration, are not necessarily indicative of a flexible pooling system that uses different computational strategies under different circumstances. Indeed it is possible that this behavior is consistent with a generalized population vector decoding algorithm that applies a power-law non-linearity to the neural response (spike count) of each mechanism prior to pooling those responses. For example Wei and Stocker (<xref ref-type="bibr" rid="B32">2012</xref>) have recently shown that such a framework defines an entire class of decoding mechanisms that includes both the VA read-out and the WTA decoder, depending on the magnitude of the exponent of the power-law non-linearity. We are currently exploring some of these issues in our laboratory.</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>This work was funded by a Wellcome Trust Research Career Development Fellowship (Ben S. Webb).</p></ack><fn-group><fn id="fn0001"><p><sup>1</sup>Note that for simplicity the term &#x0201c;VA&#x0201d; will be used to indicate both vector average and vector sum (the two algorithms only differ in terms of their predictions concerning the perceived speed of global image motion and not perceived global direction).</p></fn><fn id="fn0002"><p><sup>2</sup>Webb et al. (<xref ref-type="bibr" rid="B30">2007</xref>) showed that predictions of perceived global direction estimated from a model-based VA decoder are very similar to those obtained by simply computing the vector average of the local physical directions within the stimulus (cf. Amano et al., <xref ref-type="bibr" rid="B4">2009</xref>).</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>E. H.</given-names></name><name><surname>Movshon</surname><given-names>J. A.</given-names></name></person-group> (<year>1982</year>). <article-title>Phenomenal coherence of moving visual patterns</article-title>. <source>Nature</source>
<volume>300</volume>, <fpage>523</fpage>&#x02013;<lpage>525</lpage>
<pub-id pub-id-type="doi">10.1038/300523a0</pub-id><pub-id pub-id-type="pmid">7144903</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D.</given-names></name><name><surname>van der Smagt</surname><given-names>M. J.</given-names></name><name><surname>van den Berg</surname><given-names>A. V.</given-names></name><name><surname>van de Grind</surname><given-names>W. A.</given-names></name></person-group> (<year>1998</year>). <article-title>Local and global factors affecting the coherent motion of gratings presented in multiple apertures</article-title>. <source>Vision Res</source>. <volume>38</volume>, <fpage>1581</fpage>&#x02013;<lpage>1591</lpage>
<pub-id pub-id-type="doi">10.1016/S0042-6989(97)00331-3</pub-id><pub-id pub-id-type="pmid">9747495</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname><given-names>T. D.</given-names></name><name><surname>Desimone</surname><given-names>R.</given-names></name></person-group> (<year>1987</year>). <article-title>Local precision of visuotopic organization in the middle temporal area (MT) of the macaque</article-title>. <source>Exp. Brain Res</source>. <volume>65</volume>, <fpage>582</fpage>&#x02013;<lpage>592</lpage>
<pub-id pub-id-type="doi">10.1007/BF00235981</pub-id><pub-id pub-id-type="pmid">3556486</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amano</surname><given-names>K.</given-names></name><name><surname>Edwards</surname><given-names>M.</given-names></name><name><surname>Badcock</surname><given-names>D. R.</given-names></name><name><surname>Nishida</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Adaptive pooling of visual motion signals by the human visual system revealed with a novel multi-element stimulus</article-title>. <source>J. Vis</source>. <volume>9</volume>, <fpage>4.1</fpage>&#x02013;<lpage>4.25</lpage>
<pub-id pub-id-type="doi">10.1167/9.3.4</pub-id><pub-id pub-id-type="pmid">19757943</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amano</surname><given-names>K.</given-names></name><name><surname>Takeda</surname><given-names>T.</given-names></name><name><surname>Haji</surname><given-names>T.</given-names></name><name><surname>Terao</surname><given-names>M.</given-names></name><name><surname>Maruya</surname><given-names>K.</given-names></name><name><surname>Matsumoto</surname><given-names>K.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Human neural responses involved in spatial pooling of locally ambiguous motion signals</article-title>. <source>J. Neurophysiol</source>. <volume>107</volume>, <fpage>3493</fpage>&#x02013;<lpage>3508</lpage>
<pub-id pub-id-type="doi">10.1152/jn.00821.2011</pub-id><pub-id pub-id-type="pmid">22442570</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barthelemy</surname><given-names>F. V.</given-names></name><name><surname>Fleuriet</surname><given-names>J.</given-names></name><name><surname>Masson</surname><given-names>G. S.</given-names></name></person-group> (<year>2010</year>). <article-title>Temporal dynamics of 2D motion integration for ocular following in macaque monkeys</article-title>. <source>J. Neurophysiol</source>. <volume>103</volume>, <fpage>1275</fpage>&#x02013;<lpage>1282</lpage>
<pub-id pub-id-type="doi">10.1152/jn.01061.2009</pub-id><pub-id pub-id-type="pmid">20032230</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrera</surname><given-names>V. P.</given-names></name></person-group> (<year>2000</year>). <article-title>Task-dependent modulation of the sensorimotor transformation for smooth pursuit eye movements</article-title>. <source>J. Neurophysiol</source>. <volume>84</volume>, <fpage>2725</fpage>&#x02013;<lpage>2738</lpage> Available online at: <ext-link ext-link-type="uri" xlink:href="http://jn.physiology.org/content/84/6/2725.long">http://jn.physiology.org/content/84/6/2725.long</ext-link>
<pub-id pub-id-type="pmid">11110803</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrera</surname><given-names>V. P.</given-names></name><name><surname>Wilson</surname><given-names>H. R.</given-names></name></person-group> (<year>1990</year>). <article-title>Perceived direction of moving two-dimensional patterns</article-title>. <source>Vision Res</source>. <volume>30</volume>, <fpage>273</fpage>&#x02013;<lpage>287</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(90)90043-K</pub-id><pub-id pub-id-type="pmid">2309462</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gattass</surname><given-names>R.</given-names></name><name><surname>Gross</surname><given-names>C. G.</given-names></name></person-group> (<year>1981</year>). <article-title>Visual topography of striate projection zone (MT) in posterior superior temporal sulcus of the macaque</article-title>. <source>J. Neurophysiol</source>. <volume>46</volume>, <fpage>621</fpage>&#x02013;<lpage>638</lpage>
<pub-id pub-id-type="pmid">7299437</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>D. H.</given-names></name><name><surname>Wiesel</surname><given-names>T. N.</given-names></name></person-group> (<year>1968</year>). <article-title>Receptive fields and functional architecture of monkey striate cortex</article-title>. <source>J. Physiol</source>. <volume>195</volume>, <fpage>215</fpage>&#x02013;<lpage>243</lpage>
<pub-id pub-id-type="pmid">4966457</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M.</given-names></name><name><surname>Movshon</surname><given-names>J. A.</given-names></name></person-group> (<year>2006</year>). <article-title>Optimal representation of sensory information by neural populations</article-title>. <source>Nat. Neurosci</source>. <volume>9</volume>, <fpage>690</fpage>&#x02013;<lpage>696</lpage>
<pub-id pub-id-type="doi">10.1038/nn1691</pub-id><pub-id pub-id-type="pmid">16617339</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>D.</given-names></name><name><surname>Bex</surname><given-names>P. J.</given-names></name><name><surname>Dakin</surname><given-names>S. C.</given-names></name></person-group> (<year>2009</year>). <article-title>The aperture problem in contoured stimuli</article-title>. <source>J. Vis</source>. <volume>9</volume>, <fpage>13.1</fpage>&#x02013;<lpage>13.17</lpage>
<pub-id pub-id-type="doi">10.1167/9.10.13</pub-id><pub-id pub-id-type="pmid">19810794</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>D.</given-names></name><name><surname>Bex</surname><given-names>P. J.</given-names></name><name><surname>Dakin</surname><given-names>S. C.</given-names></name></person-group> (<year>2011</year>). <article-title>Quantifying &#x0201c;the aperture problem&#x0201d; for judgments of motion direction in natural scenes</article-title>. <source>J. Vis</source>. <volume>11</volume>, <fpage>1</fpage>&#x02013;<lpage>20</lpage>
<pub-id pub-id-type="doi">10.1167/11.3.25</pub-id><pub-id pub-id-type="pmid">21454854</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Wilson</surname><given-names>H. R.</given-names></name></person-group> (<year>1993</year>). <article-title>Dependence of plaid motion coherence on component grating directions</article-title>. <source>Vision Res</source>. <volume>33</volume>, <fpage>2479</fpage>&#x02013;<lpage>2489</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(93)90128-J</pub-id><pub-id pub-id-type="pmid">8249328</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledgeway</surname><given-names>T.</given-names></name><name><surname>Hess</surname><given-names>R. F.</given-names></name></person-group> (<year>2002</year>). <article-title>Rules for combining the outputs of local motion detectors to define simple contours</article-title>. <source>Vision Res</source>. <volume>42</volume>, <fpage>653</fpage>&#x02013;<lpage>659</lpage>
<pub-id pub-id-type="doi">10.1016/S0042-6989(01)00247-4</pub-id><pub-id pub-id-type="pmid">11853781</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledgeway</surname><given-names>T.</given-names></name><name><surname>Smith</surname><given-names>A. T.</given-names></name></person-group> (<year>1994</year>). <article-title>Evidence for separate motion-detecting mechanisms for first- and second-order motion in human vision</article-title>. <source>Vision Res</source>. <volume>34</volume>, <fpage>2727</fpage>&#x02013;<lpage>2740</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(94)90229-1</pub-id><pub-id pub-id-type="pmid">7975310</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majaj</surname><given-names>N. J.</given-names></name><name><surname>Carandini</surname><given-names>M.</given-names></name><name><surname>Movshon</surname><given-names>J. A.</given-names></name></person-group> (<year>2007</year>). <article-title>Motion integration by neurons in macaque MT is local, not global</article-title>. <source>J. Neurosci</source>. <volume>27</volume>, <fpage>366</fpage>&#x02013;<lpage>370</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3183-06.2007</pub-id><pub-id pub-id-type="pmid">17215397</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mingolla</surname><given-names>E.</given-names></name><name><surname>Todd</surname><given-names>J. T.</given-names></name><name><surname>Norman</surname><given-names>F.</given-names></name></person-group> (<year>1992</year>). <article-title>The perception of globally coherent motion</article-title>. <source>Vision Res</source>. <volume>32</volume>, <fpage>1015</fpage>&#x02013;<lpage>1031</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(92)90003-2</pub-id><pub-id pub-id-type="pmid">1509693</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>J. A.</given-names></name></person-group> (<year>1990</year>). <article-title>Visual processing of moving images</article-title>, in <source>Images and Understanding</source>, eds <person-group person-group-type="editor"><name><surname>Barlow</surname><given-names>H.</given-names></name><name><surname>Blakemore</surname><given-names>C.</given-names></name><name><surname>Weston-Smith</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>), <fpage>122</fpage>&#x02013;<lpage>138</lpage></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pack</surname><given-names>C. C.</given-names></name><name><surname>Born</surname><given-names>R. T.</given-names></name></person-group> (<year>2001</year>). <article-title>Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain</article-title>. <source>Nature</source>
<volume>409</volume>, <fpage>1040</fpage>&#x02013;<lpage>1042</lpage>
<pub-id pub-id-type="doi">10.1038/35059085</pub-id><pub-id pub-id-type="pmid">11234012</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname><given-names>G. H.</given-names></name><name><surname>Wurtz</surname><given-names>R. H.</given-names></name></person-group> (<year>1999</year>). <article-title>Shift in smooth pursuit initiation and MT and MST neuronal activity under different stimulus conditions</article-title>. <source>J. Neurophysiol</source>. <volume>82</volume>, <fpage>1710</fpage>&#x02013;<lpage>1727</lpage>
<pub-id pub-id-type="pmid">10515961</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>M. N.</given-names></name><name><surname>Newsome</surname><given-names>W. T.</given-names></name></person-group> (<year>1998</year>). <article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title>. <source>J. Neurosci</source>. <volume>18</volume>, <fpage>3870</fpage>&#x02013;<lpage>3896</lpage>
<pub-id pub-id-type="pmid">9570816</pub-id></mixed-citation></ref><ref id="B24a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>E. P.</given-names></name><name><surname>Heeger</surname><given-names>D. J.</given-names></name></person-group> (<year>1998</year>). <article-title>A model of neuronal responses in visual area MT</article-title>. <source>Vision Res</source>. <volume>38</volume>, <fpage>743</fpage>&#x02013;<lpage>761</lpage>
<pub-id pub-id-type="doi">10.1016/S0042-6989(97)00183-1</pub-id><pub-id pub-id-type="pmid">9604103</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M. A.</given-names></name><name><surname>Majaj</surname><given-names>N. J.</given-names></name><name><surname>Movshon</surname><given-names>J. A.</given-names></name></person-group> (<year>2005</year>). <article-title>Dynamics of motion signaling by neurons in macaque area MT</article-title>. <source>Nat. Neurosci</source>. <volume>8</volume>, <fpage>220</fpage>&#x02013;<lpage>228</lpage>
<pub-id pub-id-type="doi">10.1038/nn1382</pub-id><pub-id pub-id-type="pmid">15657600</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Softky</surname><given-names>W. R.</given-names></name><name><surname>Koch</surname><given-names>C.</given-names></name></person-group> (<year>1993</year>). <article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs</article-title>. <source>J. Neurosci</source>. <volume>13</volume>, <fpage>334</fpage>&#x02013;<lpage>350</lpage>
<pub-id pub-id-type="pmid">8423479</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stumpf</surname><given-names>P.</given-names></name></person-group> (<year>1911</year>). <article-title>&#x000dc;ber die Abhangigkeit der visuellen Bewegungsrichtung und negativen Nachbildes von den Reizvorgangen auf der Netzhaut</article-title>. <source>Z. Psychol</source>. <volume>59</volume>, <fpage>321</fpage>&#x02013;<lpage>330</lpage></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>H.</given-names></name></person-group> (<year>1935</year>). <article-title>Ueber visuell wahrgenommene bewegungsrichtung</article-title>. <source>Psychol. Forsch</source>. <volume>20</volume>, <fpage>325</fpage>&#x02013;<lpage>308</lpage>
<pub-id pub-id-type="doi">10.1007/BF02409790</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watamaniuk</surname><given-names>S. N. J.</given-names></name><name><surname>Duchon</surname><given-names>A.</given-names></name></person-group> (<year>1992</year>). <article-title>The human visual system averages speed information</article-title>. <source>Vision Res</source>. <volume>32</volume>, <fpage>931</fpage>&#x02013;<lpage>941</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(92)90036-I</pub-id><pub-id pub-id-type="pmid">1604862</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>B. S.</given-names></name><name><surname>Ledgeway</surname><given-names>T.</given-names></name><name><surname>McGraw</surname><given-names>P. V.</given-names></name></person-group> (<year>2007</year>). <article-title>Cortical pooling algorithms for judging global motion direction</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>104</volume>, <fpage>3532</fpage>&#x02013;<lpage>3537</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.0611288104</pub-id><pub-id pub-id-type="pmid">17360678</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>B. S.</given-names></name><name><surname>Ledgeway</surname><given-names>T.</given-names></name><name><surname>Rocchi</surname><given-names>F.</given-names></name></person-group> (<year>2011</year>). <article-title>Neural computations governing spatiotemporal pooling of visual motion signals in humans</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>4917</fpage>&#x02013;<lpage>4925</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.6185-10.2011</pub-id><pub-id pub-id-type="pmid">21451030</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X.-X.</given-names></name><name><surname>Stocker</surname><given-names>A. A.</given-names></name></person-group> (<year>2012</year>). <article-title>Bayesian inference with efficient neural population codes</article-title>. <source>Lecture Notes Comp. Sci</source>. <volume>7552</volume>, <fpage>523</fpage>&#x02013;<lpage>530</lpage>
<pub-id pub-id-type="doi">10.1007/978-3-642-33269-2_66</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>D. W.</given-names></name><name><surname>Sekuler</surname><given-names>R.</given-names></name></person-group> (<year>1984</year>). <article-title>Coherent global motion percepts from stochastic local motions</article-title>. <source>Vision Res</source>. <volume>24</volume>, <fpage>55</fpage>&#x02013;<lpage>62</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(84)90144-5</pub-id><pub-id pub-id-type="pmid">6695508</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>H. R.</given-names></name><name><surname>Ferrera</surname><given-names>V. P.</given-names></name><name><surname>Yo</surname><given-names>C.</given-names></name></person-group> (<year>1992</year>). <article-title>A psychophysically motivated model for two-dimensional motion perception</article-title>. <source>Vis. Neurosci</source>. <volume>9</volume>, <fpage>79</fpage>&#x02013;<lpage>97</lpage>
<pub-id pub-id-type="doi">10.1017/S0952523800006386</pub-id><pub-id pub-id-type="pmid">1633129</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yo</surname><given-names>C.</given-names></name><name><surname>Wilson</surname><given-names>H. R.</given-names></name></person-group> (<year>1992</year>). <article-title>Perceived direction of moving two-dimensional patterns depends on duration, contrast and eccentricity</article-title>. <source>Vision Res</source>. <volume>32</volume>, <fpage>135</fpage>&#x02013;<lpage>147</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(92)90121-X</pub-id><pub-id pub-id-type="pmid">1502799</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohary</surname><given-names>E.</given-names></name><name><surname>Scase</surname><given-names>M. O.</given-names></name><name><surname>Braddick</surname><given-names>O. J.</given-names></name></person-group> (<year>1996</year>). <article-title>Integration across directions in dynamic random dot displays: vector summation or winner take all</article-title>. <source>Vision Res</source>. <volume>36</volume>, <fpage>2321</fpage>&#x02013;<lpage>2331</lpage>
<pub-id pub-id-type="doi">10.1016/0042-6989(95)00287-1</pub-id><pub-id pub-id-type="pmid">8776497</pub-id></mixed-citation></ref></ref-list></back></article>