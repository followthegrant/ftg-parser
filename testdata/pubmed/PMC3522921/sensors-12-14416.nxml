<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Molecular Diversity Preservation International (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23202168</article-id><article-id pub-id-type="pmc">3522921</article-id><article-id pub-id-type="doi">10.3390/s121114416</article-id><article-id pub-id-type="publisher-id">sensors-12-14416</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Towards Real-Time and Rotation-Invariant American Sign Language Alphabet Recognition Using a Range Camera</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lahamy</surname><given-names>Herv&#x000e9;</given-names></name><xref ref-type="corresp" rid="c1-sensors-12-14416"><sup>*</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lichti</surname><given-names>Derek D.</given-names></name></contrib><aff id="af1-sensors-12-14416">Department of Geomatics Engineering, The University of Calgary, 2500 University Drive N.W., Calgary, AB T2N 1N4, Canada; E-Mail: <email>ddlichti@ucalgary.ca</email></aff></contrib-group><author-notes><corresp id="c1-sensors-12-14416"><label>*</label>Author to whom correspondence should be addressed; E-Mail: <email>hdalaham@ucalgary.ca</email>; Tel.: +1-403-918-6522.</corresp></author-notes><pub-date pub-type="collection"><year>2012</year></pub-date><pub-date pub-type="epub"><day>29</day><month>10</month><year>2012</year></pub-date><volume>12</volume><issue>11</issue><fpage>14416</fpage><lpage>14441</lpage><history><date date-type="received"><day>18</day><month>7</month><year>2012</year></date><date date-type="rev-recd"><day>12</day><month>10</month><year>2012</year></date><date date-type="accepted"><day>12</day><month>10</month><year>2012</year></date></history><permissions><copyright-statement>&#x000a9; 2012 by the authors; licensee MDPI, Basel, Switzerland.</copyright-statement><copyright-year>2012</copyright-year><license><license-p><!--CREATIVE COMMONS-->This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">http://creativecommons.org/licenses/by/3.0/</ext-link>).</license-p></license></permissions><abstract><p>The automatic interpretation of human gestures can be used for a natural interaction with computers while getting rid of mechanical devices such as keyboards and mice. In order to achieve this objective, the recognition of hand postures has been studied for many years. However, most of the literature in this area has considered 2D images which cannot provide a full description of the hand gestures. In addition, a rotation-invariant identification remains an unsolved problem, even with the use of 2D images. The objective of the current study was to design a rotation-invariant recognition process while using a 3D signature for classifying hand postures. A heuristic and voxel-based signature has been designed and implemented. The tracking of the hand motion is achieved with the Kalman filter. A unique training image per posture is used in the supervised classification. The designed recognition process, the tracking procedure and the segmentation algorithm have been successfully evaluated. This study has demonstrated the efficiency of the proposed rotation invariant 3D hand posture signature which leads to 93.88% recognition rate after testing 14,732 samples of 12 postures taken from the alphabet of the American Sign Language.</p></abstract><kwd-group><kwd>posture recognition</kwd><kwd>range camera</kwd><kwd>segmentation</kwd><kwd>tracking</kwd><kwd>3D signature</kwd><kwd>rotation invariance</kwd><kwd>accuracy assessment</kwd></kwd-group></article-meta></front><body><sec><label>1.</label><title>Background and Objective</title><p>Interactions between humans and computers are typically carried out using keyboards, mice and joysticks. In addition to being different from the natural human way of communicating, these tools do not provide enough flexibility for a number of applications such as manipulating objects in a virtual environment. In order to improve the human-computer interaction, an automatic hand gesture recognition system could be used. Hand gesture recognition is the process by which gestures made by the user are automatically recognized in real-time by computer software via a camera. Hand gesture recognition has gained popularity in recent years, and could become the future tool for humans to interact effectively with computers or virtual environments.</p><p>Extensive research has been conducted in the field of gesture recognition in recent decades. Though very high recognition rates are usually claimed by authors who have used a variety of techniques (100% for [<xref ref-type="bibr" rid="b1-sensors-12-14416">1</xref>], 98.6% for [<xref ref-type="bibr" rid="b2-sensors-12-14416">2</xref>], 98% for [<xref ref-type="bibr" rid="b3-sensors-12-14416">3</xref>]), hand gesture recognition remains a timely research topic with many unresolved problems. This can be seen when taking into account the high number of papers written on the topic in 2012: more than 60 were found using only the Compendex database with a query made on 16 April 2012. Gesture recognition is performed most frequently through supervised classification processes where different features are used to predict the class membership of the considered hand image. The reference gestures are stored in a database and during a subsequent real-time image acquisition, the current gesture is matched with the most similar one available in the training dataset. To perform the classification, a huge number of classifiers such as neural networks, support vector machines, graph matching, inductive learning systems, voting theory, hidden Markov models, chamfer distance or dynamic Bayesian networks are used. Extensive training and testing are performed after acquisition of a high number of datasets from multiple users. A confusion matrix is generally presented to show the success rate. In most of the cases, a recognition rate over 98% percent is presented, but with a limited number of gestures acquired under specific conditions.</p><p>Most of the published literature on hand gesture recognition doesn't consider using the advantages that a 3D signature can provide. For example, in [<xref ref-type="bibr" rid="b4-sensors-12-14416">4</xref>], after generating a point cloud of a hand posture from data captured with four web cameras, the authors use cylindrical virtual boundaries to randomly extract five slices of the point cloud. Each slice is processed by analyzing the point cloud distribution and the hand posture is recognized from this analysis. By doing so, though the hand postures are represented by a 3D point cloud, the full 3D topology is not considered in the recognition process. Other researchers, though using a 3D sensor, do not consider at all the third dimension in the features used to represent the hand postures. That is the case of [<xref ref-type="bibr" rid="b5-sensors-12-14416">5</xref>], where the authors use a 3D depth camera but only consider the 2D outline of the hand segment in their recognition process. A 3D image describes better the hand posture than a 2D image. It provides less occlusion. The 2D image is the projection in a given plane of the 3D image. Two different 3D hand postures projected on a particular plane can provide exactly the same information in the 2D images which as a consequence cannot be used to differentiate them.</p><p>The design of a rotation invariant system has not been successfully achieved so far. Indeed many researchers consider the principal component analysis to evaluate the orientation of the 2D hand image but, as acknowledged by [<xref ref-type="bibr" rid="b6-sensors-12-14416">6</xref>], this method is not always accurate. Not only has the estimation of the rotation of a 2D hand segment not been successful so far but, furthermore, the evaluation of the orientation of a 3D hand segment is not considered in most existing approaches.</p><p>To test their hand motion classification using a multi-channel surface electromyography sensor, [<xref ref-type="bibr" rid="b7-sensors-12-14416">7</xref>] only consider five testing images per gesture. Contrary to most of the studies on this topic, a significant number of testing samples has been considered to validate the proposed algorithm. Indeed, testing more than 1,000 images per gesture in average instead of five provides more evidence on the robustness of the methodology.</p><p>The objective of the current study is to design a range camera based system where a high number of postures taken from the alphabet of the American Sign Language can be recognized in real-time. Contrary to existing methods, the current one allows hand posture recognition independently of the orientation of the user's hand. It makes use of a 3D signature, considers only one training image per posture and uses a significant number of testing images for its evaluation.</p><p>The term &#x0201c;gesture&#x0201d; means that the character considered cannot be performed without a dynamic movement of the hand while &#x0201c;posture&#x0201d; refers to a character that can be fully described with a static position of the hand. In this paper &#x0201c;Static gestures&#x0201d; doesn't mean that the user is not moving his hand. &#x0201c;Static gestures&#x0201d; or &#x0201c;postures&#x0201d; relate to the different characters of the American Sign Language Alphabet as shown in the paper except &#x0201c;Z&#x0201d; and &#x0201c;J&#x0201d;. Once the user performs one of them, he can rotate and move his hand in whatever direction he wants to. The objective is to make the system recognize this posture no matter the position of the user's hand.</p><p>This paper is structured as follows: Section 2 reviews the literature on methods used for hand gesture recognition. Section 3 describes the set up of the experiment and Section 4, the segmentation process used. The methodologies considered for tracking the hand motion are provided in Section 5. Section 6 reports on the evaluation of the segmentation as well as the tracking processes. In Section 7, the recognition principle is depicted. The rotation invariance algorithm is highlighted in Section 8. The experimental results and their analysis are shown in Section 9 while a comparison with results from other papers is discussed in Section 10. The conclusion and future work are provided in Section 11.</p></sec><sec><label>2.</label><title>Literature Review</title><p>Before describing the different steps of the algorithm applied herein, a literature review has been conducted on the methods frequently used regarding hand posture recognition. The review has focused on the input data, the sensors, the segmentation as well as the tracking processes, the features used to represent hand postures and finally the classifiers. Most of the papers selected are dealing with the American Sign Language recognition. This section ends with the limitations of current method hence the need of going further in this research and highlights the remaining structure of the current paper.</p><sec sec-type="methods"><label>2.1.</label><title>Input Data</title><p>Most of the research conducted in the field of gesture recognition makes use of 2D intensity images acquired as snapshots or at video rates [<xref ref-type="bibr" rid="b8-sensors-12-14416">8</xref>]. In very rare cases, 3D data are obtained from stereo images [<xref ref-type="bibr" rid="b9-sensors-12-14416">9</xref>]. Range data extracted from color images after analysis of the deformation of the patterns on object surfaces is used by [<xref ref-type="bibr" rid="b10-sensors-12-14416">10</xref>] while [<xref ref-type="bibr" rid="b3-sensors-12-14416">3</xref>] consider depth information obtained from a range camera.</p></sec><sec><label>2.2.</label><title>Sensors</title><p>Different sensors have been used to improve the interaction between man and machine. While [<xref ref-type="bibr" rid="b11-sensors-12-14416">11</xref>] uses the infrared time-of-flight range camera, the Logitech Messenger Webcam is the sensor considered by [<xref ref-type="bibr" rid="b12-sensors-12-14416">12</xref>]. A camera that provides a stereo pair of images is used by [<xref ref-type="bibr" rid="b13-sensors-12-14416">13</xref>]. Most researchers suggest natural interaction without any additional equipment to the user's hand, but others make use of specific gloves ([<xref ref-type="bibr" rid="b1-sensors-12-14416">1</xref>,<xref ref-type="bibr" rid="b10-sensors-12-14416">10</xref>]) or markers to derive meaningful results. In [<xref ref-type="bibr" rid="b14-sensors-12-14416">14</xref>], the images of the user's face and hand were acquired with a service robot. Most recently, some 3D sensors such as the Microsoft Kinect [<xref ref-type="bibr" rid="b15-sensors-12-14416">15</xref>] and the Leap sensor [<xref ref-type="bibr" rid="b16-sensors-12-14416">16</xref>] are currently being used by some researchers for the same purpose of improving interaction with computers by using hand gestures.</p></sec><sec><label>2.3.</label><title>Extraction of Region of Interest</title><p>In order to recognize the hand gesture, the hand information must first be extracted from the acquired images. Different approaches are available in literature to achieve this process, called segmentation.</p><p>The most commonly used technique for hand segmentation is color-based, as demonstrated in [<xref ref-type="bibr" rid="b17-sensors-12-14416">17</xref>]. The skin color is a distinctive cue of hands and is invariant to scale and rotation. Human hands have almost the same hue and saturation but vary in their brightness. This method was performed by [<xref ref-type="bibr" rid="b12-sensors-12-14416">12</xref>] by analyzing the red and green components of the skin color in the red, green, and blue color space which is then converted to the hue, saturation and intensity color space (complete range of colors that can be displayed and recorded on digital video) as the intensity component can be treated separately from the chrominance components.</p><p>Another method is based on image differencing between consecutive video frames [<xref ref-type="bibr" rid="b18-sensors-12-14416">18</xref>]. A hand gesture detection and segmentation method is proposed where video sequences are captured from a stationary camera with complex backgrounds. The hand segment is extracted based on a threshold grey value calculated from the image's intensity histogram. In [<xref ref-type="bibr" rid="b19-sensors-12-14416">19</xref>], the hand motion is detected using double difference of range images.</p><p>To extract the hand information from a range image obtained from an active time-of-flight camera [<xref ref-type="bibr" rid="b20-sensors-12-14416">20</xref>] make use of depth constraint to separate the foreground and the background of the image. The major cluster in the image at a distance smaller than a pre-defined threshold value can be treated as the hand. A simple depth keying is also used by [<xref ref-type="bibr" rid="b11-sensors-12-14416">11</xref>] to define the region of interest. In [<xref ref-type="bibr" rid="b10-sensors-12-14416">10</xref>], an initial segmentation is obtained by the means of thresholding the depth values. The segmentation of the arm is achieved by a hierarchical unsupervised clustering procedure. It is an iterative process where clusters are defined and merged based on Euclidean distance. To segment the hand from the forearm, the method used is based on a statistical modeling, the idea being to classify the point cloud based on a probability distribution modeled as a mixture of Gaussians. Experimental results demonstrate robustness of the algorithm under various orientations of the palm and fingers. In [<xref ref-type="bibr" rid="b2-sensors-12-14416">2</xref>], 3D information is used for segmentation and detection of face and hands using normal Gaussian distribution and depth information.</p><p>A robust segmentation technique based on fusion of range and intensity images using a time-of-flight camera is proposed by [<xref ref-type="bibr" rid="b21-sensors-12-14416">21</xref>]. According to the authors, none of the intensity, range and amplitude data delivered by the camera for every pixel can be used alone to make robust segmentation under variant lighting conditions. The proposed method is based on the combination of two unsupervised clustering approaches: K-means and expectation maximization. K-means clustering is a method of cluster analysis which aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. An expectation-maximization algorithm is a method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. They both attempt to find the centroids of natural clusters in the fused data. The K-means algorithm is used to define the initial clusters' centers, which reduce the sensitivity of the initial points. These initial parameters are then used by the expectation maximization technique to find the sensitivity of the local maxima. The experimental results show that the proposed gesture segmentation technique can successfully segment the hand from user's body under variant illumination conditions in real time. This idea of fusing range and intensity images for image segmentation was also considered by [<xref ref-type="bibr" rid="b22-sensors-12-14416">22</xref>]. The assumption is that pixels with similar 3D coordinates and similar intensities belong to the same physical object. The 3D position and intensity are then combined into a 4D intensity/position feature space. The object-based segmentation is then performed by clustering pixels in the new intensity/position feature space. A divisive clustering method without any prior knowledge has been considered where the cluster with the highest variance is split into two using a hyperplane.</p></sec><sec><label>2.4.</label><title>Hand Segment Tracking</title><p>The segmentation processes described in the previous section applied on every acquired image is time consuming. In order to avoid this situation and to enable a real-time application, the hand cluster is segmented from the first image acquired and tracked through the subsequently acquired images.</p><p>The Kalman filter is the most commonly used technique for tracking moving objects over time. While tracking independent clusters [<xref ref-type="bibr" rid="b22-sensors-12-14416">22</xref>] assumes the motions along the X, Y and Z directions to be decoupled and therefore predicted by separate Kalman filters. The motion of the clusters is assumed to have constant velocity. To account for slight changes in the velocity, the continuous-time acceleration is modelled as white noise. The parameters of the filter are the process noise and the measurement noise [<xref ref-type="bibr" rid="b23-sensors-12-14416">23</xref>] make use of the Kalman filter to predict the hand location in one image frame based on its location detected in the previous frame. The Kalman filter in [<xref ref-type="bibr" rid="b23-sensors-12-14416">23</xref>] is used to track the hand region centroid in order to accelerate hand segmentation and choose the correct skin region. Using a model of constant velocity motion, the filter provides and estimates the hand location, which guides the image search for the hand. Unfortunately, in most of the papers published on this topic, no detail is provided on how the process noise and the measurement noise are estimated. Similarly, the Kalman filter is not evaluated and its limits for hand motion tracking are not known.</p><p>Another technique used for tracking a hand segment within acquired images is the condensation algorithm. In [<xref ref-type="bibr" rid="b24-sensors-12-14416">24</xref>] its authors argue that trackers based on Kalman filters are of limited use because they are based on Gaussian densities which are unimodal. They suggest the condensation algorithm which is highly robust in tracking agile motion in the presence of dense background clutter. The condensation algorithm (conditional density propagation) allows quite general representations of probability. One of the most interesting facets of the algorithm is that it does not process every pixel of the image. Rather, pixels to process are chosen at random, and only a subset of these pixels ends up being processed.</p><p>The mean-shift method is a powerful and versatile, non-parametric and iterative algorithm that has been used for tracking hand motion. For each data point, the mean-shift algorithm associates it with the nearby peak of the dataset's probability density function. The mean-shift defines a window around it and computes the mean of the data point. Then it shifts the center of the window to the mean and repeats the algorithm till it converges. After each iteration, the window shifts to a denser region of the dataset. At the high level, the mean-shift algorithm can be summarized as follows: fix a window around each data point, compute the mean of data within the window and shift the window to the mean and repeat till convergence. The classic mean-shift algorithm is time intensive. Many improvements have been made to the mean shift algorithm to make it converge faster. This method has been used by [<xref ref-type="bibr" rid="b13-sensors-12-14416">13</xref>] in association with the Kalman filter.</p></sec><sec><label>2.5.</label><title>Features Used to Represent Hand Postures</title><p>The recognition of the hand gesture is performed not by using the raw image information but rather by representing it by some different types of parameters known as signatures or features. Geometric features such as fingertips [<xref ref-type="bibr" rid="b25-sensors-12-14416">25</xref>], finger directions, image moments [<xref ref-type="bibr" rid="b26-sensors-12-14416">26</xref>], orientation histograms [<xref ref-type="bibr" rid="b27-sensors-12-14416">27</xref>], depth information, hands' contours [<xref ref-type="bibr" rid="b28-sensors-12-14416">28</xref>], and labeled graphs [<xref ref-type="bibr" rid="b29-sensors-12-14416">29</xref>] are commonly used to recognize the pattern of a hand gesture. Principal component analysis has been used by [<xref ref-type="bibr" rid="b10-sensors-12-14416">10</xref>] to extract feature vectors by defining an orthogonal space and projecting an image into that space. Hand shapes and distance transformation images are used by [<xref ref-type="bibr" rid="b3-sensors-12-14416">3</xref>] to represent hand gestures. A combination of statistical features made of the seven Hu-moments derived from second and third order moments and geometric features such as rectangularity and circularity has been used by [<xref ref-type="bibr" rid="b2-sensors-12-14416">2</xref>]. Recognizing gestures not only needs spatial features, but also requires temporal features such as the hands' positions, velocities and trajectories [<xref ref-type="bibr" rid="b30-sensors-12-14416">30</xref>]. Most of these features vary depending on the lighting conditions, the size of the hand in the image, the orientation of the hand and the hand's position. For a robust gesture recognition algorithm, features independent of these elements are required.</p></sec><sec><label>2.6.</label><title>Classifiers Most Frequently Used for Recognition</title><p>Classification methods or classifiers are used to identify every gesture. They make use of different techniques to match the current gesture with the most similar one available in the reference database. While hidden Markov models are considered by [<xref ref-type="bibr" rid="b28-sensors-12-14416">28</xref>,<xref ref-type="bibr" rid="b29-sensors-12-14416">29</xref>] employ the elastic graph matching technique to classify hand postures against complex backgrounds. A multivariate fuzzy decision tree was used by [<xref ref-type="bibr" rid="b31-sensors-12-14416">31</xref>]. In [<xref ref-type="bibr" rid="b32-sensors-12-14416">32</xref>], a semi-supervised fuzzy neural network is used for recognition of static gestures while hidden Markov models are applied for online recognition of dynamic gestures. In [<xref ref-type="bibr" rid="b33-sensors-12-14416">33</xref>], the authors proposed to recognize hand gestures in a continuous video stream using a dynamic Bayesian network. The chamfer matching method is considered by [<xref ref-type="bibr" rid="b3-sensors-12-14416">3</xref>] to measure the similarities between the candidate hand image and the hand templates in the database. In [<xref ref-type="bibr" rid="b19-sensors-12-14416">19</xref>], a binary comparison of histogram bins results in a distance for each gesture used to perform the classification.</p></sec><sec><label>2.7.</label><title>Conclusion</title><p>According to [<xref ref-type="bibr" rid="b34-sensors-12-14416">34</xref>], further research in the areas of feature extraction and gesture representation is required to realize the ultimate goal of humans interfacing with machines on their own natural terms. Indeed, although [<xref ref-type="bibr" rid="b3-sensors-12-14416">3</xref>] and others claim to have used 3D data, they mainly classify 2D images with the pixel values being the range information. An exhaustive description of hand gestures can only be made with 3D data, which is not currently widely used by researchers. In addition, the feature vectors used as signatures to represent the gestures appear to not be accurate enough to describe the hand gestures. Indeed, the equivalence between the original image and its representation which is the feature vector has not been proven in any paper. This absence of equivalence is the reason why most of researchers have been trying different feature vectors and different classifiers in order to come up with the best possible combination. As a consequence, further research has to concentrate on finding the best possible signature for every gesture. The same gesture performed by the same user or different users appear to show different orientations and scales. To overcome this problem, most of the researchers chose to increase the number of reference gestures stored in the database. As a consequence, the size of the database as well as the matching time increase and the number of gestures recognizable reduces especially when dealing with real-time applications.</p><p>The proposed method in the current paper (<xref ref-type="fig" rid="f1-sensors-12-14416">Figure 1</xref>) considers the 3D point cloud provided by a range camera to build a 3D model of the hand gesture and it makes use of only one single training image with the objective of showing the robustness of the method and also its appropriateness for a real-time application. To track the hand motion during the real-time process the Kalman filter has been proposed with a detailed explanation on how the process noise and the measurement noise have been modeled. The mean-shift algorithm has been tested as well. In order to achieve these objectives, the sensor considered is the SR4000 range camera because of its ability to provide 3D images at video rates.</p></sec></sec><sec><label>3.</label><title>Experiment Setup</title><p>The hand posture recognition experiments have been conducted under laboratory conditions. The camera was mounted on a tripod situated approximately at 1.5 m from the user sitting on a chair and facing a desktop where the images acquired by the SR4000 camera as well as the results from the hand posture recognition are displayed in real-time. No particular background is required. Similarly, the user does not have to wear long sleeves or specific gloves as required in some similar experiments [<xref ref-type="bibr" rid="b10-sensors-12-14416">10</xref>]. A picture describing the setup environment is shown in <xref ref-type="fig" rid="f2-sensors-12-14416">Figure 2</xref>. The integration time which is the length of time during which the pixels are allowed to collect light was set to 1.8 ms, corresponding to a theoretical frame rate of 43 images per second.</p></sec><sec><label>4.</label><title>Segmentation</title><p>Segmentation is the process of grouping points that belong to the same object into segments. The idea here is to extract from the point cloud, the set of points that describe the user's hand (<xref ref-type="fig" rid="f3-sensors-12-14416">Figure 3</xref>). In this paper, a multiple-step range based segmentation has been designed (<xref ref-type="fig" rid="f4-sensors-12-14416">Figure 4</xref>).</p><sec><label>4.1.</label><title>Range-Based Segmentation</title><p>The underlying principle is that there shouldn't be any object between the camera and the hand. Thus the hand appears in the foreground of the image. The algorithm is designed based on the following two key points:</p><p>- Find the first 3,000 points closest to the camera using the range. This threshold was obtained from the analysis of the total number of points describing a hand posture with respect to the distance from the camera to the hand;</p><p>- Assuming that an average human's hand can fit within a 3D cube bounding box having 20 cm sides; a sub-selection is extracted to achieve this objective; the idea being to get rid of an eventual part of the user's arm.</p><p>An example of the result of this algorithm is presented in <xref ref-type="fig" rid="f5-sensors-12-14416">Figure 5(a)</xref>.</p></sec><sec><label>4.2.</label><title>Noise Removal</title><p>The results obtained contain the appropriate information but appear noisy due to the presence of hanging points (<xref ref-type="fig" rid="f5-sensors-12-14416">Figure 5(a)</xref>). The hanging points appear isolated compared to the ones that belong to the hand. The point density of the hand is much higher than the one of the hanging points. The point cloud obtained from the range-based segmentation is split into voxels (3D cells). Voxels that have a low point density are discarded from the segment.</p><p>The noise made of the hanging points is described in [<xref ref-type="bibr" rid="b35-sensors-12-14416">35</xref>] as the result of lateral and axial motion artifacts of the sensor or the target object. A range image is obtained after averaging four different images acquired by the camera with 90&#x000b0; phase-shift and at different times. For each image, four samples of the reflected light are acquired and read out by the sensor. Lateral motion and axial motion of the hand with respect to the viewing axis of the camera during the acquisition of the four samples are the main cause of the noise (hanging points) in the image. Lateral motion results in the mixture of foreground and background phase values at the boundary of moving objects while axial motion describes motion along the viewing direction and introduce additional phase changes due to non-constant object distance. To remove this noise, the authors of [<xref ref-type="bibr" rid="b35-sensors-12-14416">35</xref>] use the two raw images namely the shifted reference signal and the inverted signal, provided by the Photonic Mixing Device to compute the phase images. The objects are tracked in the latter in order to average corresponding pixels and a new depth image is computed after demodulation. The raw images used in this paper to remove the motion artifacts are not provided by the SR4000 and as a result this method which has the advantage of eliminating the noise by removing its causes cannot be applied in this research.</p></sec><sec sec-type="methods"><label>4.3.</label><title>Connected Component Analysis</title><p>This step is an additional noise removal algorithm. Connected component labeling is used to detect unconnected regions. It is an iterative process that groups neighboring elements into classes based on a distance threshold. A point belongs to a specific class if and only if it is closer within the distance threshold to another point belonging to that same class. After the noise removal, the hand segment appears to be the biggest one in the dataset. An example of the results from and connected component analysis is provided in (<xref ref-type="fig" rid="f6-sensors-12-14416">Figure 6</xref>).</p><p>The segmentation process made of the range segmentation, noise removal, and the connected component analysis operations is only applied on the very first image acquired. For the subsequent images acquired, only the noise removal steps are applied. In a real-time application, the segmentation is made only of noise removal from the region of interest obtained from the tracking process.</p><p>Every image acquired contains 25,344 3D points while every region of interest contains less than 3,000 points and in 90% of the cases, around 1,500 points. The segmentation process applied on every single image is time consuming. Indeed, the segmentation time of a dataset of 12,000 images has been reported on <xref ref-type="table" rid="t1-sensors-12-14416">Table 1</xref>. The time of the different steps of the segmentation has been recorded in second. The computation has been performed using Microsoft Visual Studio and the time has been measured with the function clock. <xref ref-type="table" rid="t1-sensors-12-14416">Table 1</xref> shows that on average 3 s are required for an independent segmentation. As a consequence, a real-time processing cannot be achieved by segmenting systematically each time the whole image. Tracking the hand movement is therefore necessary.</p></sec></sec><sec><label>5.</label><title>Tracking</title><p>In this research, two tracking methods, namely the mean-shift algorithm and the Kalman filter have been tested and evaluated.</p><sec><label>5.1.</label><title>Mean-Shift Method</title><p>A simple version of the mean-shift algorithm has been implemented. The segmentation process described earlier is used to determine the initial coordinates of the centroid of the hand segment. A 3D cube bounding box of 20 cm side centered on this initialization point is then used to collect from the first frame the points expected to belong to the user's hand. Once selected, the centroid of the new set of points is determined. In order to identify the segment in the following frame, the newly determined centroid is considered as the center on the following hand segment and thus the center of the bounding box. Iteratively, hand segments are extracted and centroids computed. This method takes its advantage from the fact that the range camera provides the x, y, z coordinates of every pixel and for every frame in the same camera frame. In addition, because of the high frequency of the frames, it is assumed that the centers of the hand segments in two consecutive frames are quite close to each other in such a way that the centroid of the hand segment in the first frame can be used as centroid of the segment in the second frame. Consequently, this method applied iteratively enables a real-time tracking that is evaluated in Section 6.</p></sec><sec><label>5.2.</label><title>Kalman Filter</title><p>Considering that the hand is undergoing a linear movement with constant velocity and applying Newton's law of motion, the linear discrete Kalman filter has been used for tracking the centroid of the hand. The state vector (<italic>x</italic>) comprises three states describing the position of the centroid of the hand segment in the camera frame (<italic>p</italic>) and three other states corresponding to the velocity of the hand movement (<italic>v</italic>). <italic>p<sub>x</sub></italic>, <italic>p<sub>y</sub></italic> and <italic>p<sub>z</sub></italic> as well as <italic>v<sub>x</sub></italic>, <italic>v<sub>y</sub></italic> and <italic>v<sub>z</sub></italic> correspond respectively to the position and velocity in <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic> directions. The initial position coordinates are obtained from an initialization process where the user indicates approximately the starting position of the hand. An image of the hand is acquired and a segmentation process described earlier is applied to extract the hand segment from which the centroid is computed. The initial velocity is assumed to be null:</p><disp-formula id="FD1"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mrow/></mml:mrow></mml:math></disp-formula><p>The continuous-time-space state representation of the system is a first order vector differential equation given by:</p><disp-formula id="FD2"><label>(2)</label><mml:math id="mm2"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:math></disp-formula><p><italic>Fx</italic> is the dynamics model and <italic>Gu</italic> the stochastic model. The dynamics model defines how the states evolve with time based on known physical relationships. The stochastic model is used primarily as a means of defining the uncertainty in the dynamics models where <italic>G</italic> is the shaping matrix. <italic>w</italic> = <italic>Gu</italic> is a vector of zero-mean, unity variance white noise. From its definition, the purpose of the stochastic model is not to model or characterize the specific variations in the states, but rather to capture their statistical properties after all of the systematic effects have been removed or accounted for. The assumption is that the underlying state vector is a random process. Based on the constant velocity model chosen:</p><disp-formula id="FD3"><label>(3)</label><mml:math id="mm3"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><p>where <italic>q<sub>&#x00076;&#x00307;</sub></italic> is the power spectral density of velocity change and thus the white noise of the acceleration. <italic>q<sub>&#x00076;&#x00307;</sub></italic> does not depend on the sampling frequency. After collecting a sample dataset with the hand moving at a nearly constant speed, <italic>q<sub>&#x00076;&#x00307;</sub></italic> will be computed as the standard deviation (<italic>&#x003c3;</italic><sub>a</sub>) of the acceleration multiplied by the square root of the sampling frequency:</p><disp-formula id="FD4"><label>(4)</label><mml:math id="mm4"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula><p>The accelerations in <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic> are computed as the corresponding differences of velocity divided by the elapsed time between three consecutive images. Similarly, the velocities are computed as differences of corresponding positions divided by the appropriate time. Accelerations can only be computed if a minimum of three consecutive positions of the hand centroid have been recorded. Once the time series of the accelerations is obtained, (<italic>&#x003c3;<sub>a</sub></italic>) is estimated as the standard variation of this set of accelerations.</p><p>The continuous-time system model presented above, although often convenient, is not well suited for the estimation of problems involving discrete time data. Therefore the continuous-time equation has to be converted into a corresponding discrete-time difference model of the following form:</p><disp-formula id="FD5"><label>(5)</label><mml:math id="mm5"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><p>where &#x000d8; is transition matrix that converts the state from epoch <italic>k</italic> &#x02212; 1 to <italic>k</italic>. It is the discrete time equivalent of the dynamics matrix. <italic>w<sub>k</sub></italic> is the process noise considered as white.</p><p>The transition matrix can be solved as follows:</p><disp-formula id="FD6"><label>(6)</label><mml:math id="mm6"><mml:mrow><mml:msub><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>&#x02009;</mml:mi><mml:msup><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="FD7"><label>(7)</label><mml:math id="mm7"><mml:mrow><mml:msub><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>with &#x00394;<italic>t</italic> being the time elapsed between the acquisition of the two consecutive frames <italic>k</italic> &#x02212; 1 and <italic>k</italic>. The covariance <italic>Q<sub>w</sub></italic> of the process noise can be computed as follows:</p><disp-formula id="FD8"><label>(8)</label><mml:math id="mm8"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>G</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9"><label>(9)</label><mml:math id="mm9"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:msubsup><mml:mi>q</mml:mi><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>q</mml:mi><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>q</mml:mi><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:msubsup><mml:mi>q</mml:mi><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>To update the state vector, the measurement (<italic>z</italic>) is made of the three position coordinates of the centroid of the hand segment:</p><disp-formula id="FD10"><label>(10)</label><mml:math id="mm10"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="FD11"><label>(11)</label><mml:math id="mm11"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>where <italic>H</italic> is the observation matrix and <italic>v</italic> is the observation white noise that is assumed to be uncorrelated with <italic>w</italic>. The measurements errors are characterized by the covariance matrice <italic>R. R</italic> is obtained by considering that the observed coordinates have a precision of <italic>&#x003c3;</italic> = 1 <italic>cm</italic> (<xref rid="FD12" ref-type="disp-formula">Equation (12)</xref>). This value has been obtained by imaging a static object (Spectralon target) hundred times within similar set up conditions as for the hand posture recognition. For several pixels, the standard deviations have been computed in <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic> directions of the camera frame. The variations for the central pixel range from 4 mm in <italic>Y</italic> direction to 10 mm in <italic>Z</italic> direction</p><p>The Kalman filter state prediction and state covariance prediction are computed as follows:</p><disp-formula id="FD12"><label>(12)</label><mml:math id="mm12"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="FD13"><label>(13)</label><mml:math id="mm13"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="FD14"><label>(14)</label><mml:math id="mm14"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mo>&#x02205;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><p>where <italic>&#x00078;&#x00302;<sub>k</sub></italic> denotes the estimated state vector; <italic>&#x00078;&#x00304;<sub>k</sub></italic> is the predicted state vector for the next epoch; <italic>&#x00050;&#x00302;<sub>k</sub></italic> is the estimated state covariance matrix; <italic>&#x00050;&#x00304;<sub>k</sub></italic> is the predicted state covariance matrix.</p><p>The Kalman filter update steps are as follows:</p><disp-formula id="FD15"><label>(15)</label><mml:math id="mm15"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>H</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>H</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="FD16"><label>(16)</label><mml:math id="mm16"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="FD17"><label>(17)</label><mml:math id="mm17"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="FD18"><label>(18)</label><mml:math id="mm18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><p>where <italic>K<sub>k</sub></italic> is the Kalman gain, which defines the updating weight between the new measurements and the prediction from the system dynamic model.</p></sec></sec><sec><label>6.</label><title>Evaluation of the Segmentation and Tracking Processes</title><p>In this section, the assessment of the segmentation process as well as the two tracking processes are reported. In this assessment, the hand was moving back and forth (translation only) in the <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic> directions of the camera. The movement of the hand is assumed to be linear with a nearly constant velocity in all three directions. In the case of the Kalman filter, any deviation from the assumptions of linearity and constant velocity are compensated by the process noise. During this experiment, 1,045 frames acquired have been saved. An individual segmentation was applied to each of them and the results used as ground truth. The mean-shift and Kalman filter algorithms were applied to the movement by considering the initial position of the hand to be the position of the centroid of the hand segment obtained from the first frame. Two parameters were considered in this evaluation: the distance between the camera and the hand and the speed of the hand movement, the objective being to check whether the accuracy of the tracking is a function of these parameters. As shown by <xref ref-type="fig" rid="f7-sensors-12-14416">Figure 7</xref>, the hand was moved away up to 2.3 m from the camera.</p><p>An example of the tracking process is shown in <xref ref-type="fig" rid="f8-sensors-12-14416">Figure 8</xref> where the tracked centroids as well as the corresponding hand segments are displayed. <xref ref-type="fig" rid="f7-sensors-12-14416">Figure 7</xref> shows three graphs where the position of the centroid of the hand segments have been plotted in the <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic> axes of the camera frame. These graphs clearly show that the segmentation process, the Kalman filter algorithm and the mean-shift tracking algorithm give accurate results and are appropriate for hand motion tracking. However, it can be noticed on the graphs that when the distance between the hand and the camera exceeds 2 m, the results obtained from the segmentation process were wrong and that the coordinates obtained indicate a centroid much closer to the camera than the hand. The main assumption of the segmentation process is that there shouldn't be any objet between the hand and the camera (Section 4.1). The experiment was performed in a small and crowded room and this assumption couldn't be satisfied over 2 m which justifies the obtained results. The object automatically segmented in the frames 266 to 374 and in the frames 718 to 816 is not the user's hand.</p><p>The two tracking algorithms give similar results and match with the results from the segmentation process, which shows their appropriateness for hand motion tracking. The accuracy of the tracking methods have been computed by considering as true values the results from the segmentation process. The frames from which the object segmented is the hand have not been included in the computation of the root mean squares. <xref ref-type="table" rid="t2-sensors-12-14416">Table 2</xref> shows that this accuracy is sub-centimeter and that both methods are equally accurate. Another conclusion to be derived is that the quality of the tracking using either of these methods is independent of the distance between the hand and the camera.</p><p>The elapsed time between two consecutive acquired images including the image acquisition and the processing time is 28 ms on average, which corresponds to 35 images per second. This frame rate is good enough for a real-time visualization as 24FPS is the progressive format now widely adopted by film and video makers [<xref ref-type="bibr" rid="b36-sensors-12-14416">36</xref>].</p><p>However, when moving the hand back and forth across the viewing direction of the camera at a velocity higher than 1 m/s (1.19 m/s for the Kalman filter and 1.33 m/s for the mean-shift) and on an approximate horizontal distance of 50 cm, it has been noticed that the designed methods fail to track properly the hand movement. The predicted position of the hand is no longer accurate. The considered methods are thus limited and in case of very fast movement of the hand (which is not the regular way for humans when moving their hands), further investigation is required.</p></sec><sec><label>7.</label><title>Principle of Posture Recognition</title><p>Several commonly used recognition methods appearing in the literature have been described and evaluated in [<xref ref-type="bibr" rid="b37-sensors-12-14416">37</xref>]. The maximum overall recognition rate obtained is 97.29% when using the outline of the 2D hand segment and its corresponding distance transformation image as features and the chamfer distance as classifier. But, unfortunately, these features are not appropriate for real-time application. In this research, the real-time recognition of the postures has been achieved by using an heuristic and voxel-based algorithm. To derive a signature of a hand posture from its point cloud, a 3D bounding box is generated and transformed into a regular grid. Only voxels containing at least one point from the hand segment's point cloud are considered in the hand posture's signature. A column vector is thus generated in which every voxel is represented by a Boolean value, true when it contains at least one point and false when it is empty. This vector contains the full 3D topology of the hand posture as voxels are stored following a predefined order. This signature is equivalent to the original posture as it can be used to reconstitute the 3D structure of the original posture. It considers the position and orientation of every single part of the hand segment. In addition, it has the advantage of containing less information and consequently it is easier to store and is more time-efficient for processing. It is a suitable parameter to measure the similarity between hand postures. With this signature, hand poestures are compared by considering their 3D structure. <xref ref-type="fig" rid="f9-sensors-12-14416">Figure 9</xref> shows an example of the generation of a hand signature. A 30 &#x000d7; 30 &#x000d7; 30 representation has been considered because after testing empirically several combinations, it appears to be the one that provides the better results.</p><p>To compare two hand postures (<xref ref-type="fig" rid="f10-sensors-12-14416">Figure 10</xref>), one of the two point clouds is translated onto the second one by matching their centroids. The idea here is to define the same bounding box for both postures to be compared. The comparison is achieved by evaluating the percentage of similarity between the two postures in other words, the percentage of voxels containing at least one point in both datasets. This percentage is a suitable parameter to measure the similarity between hand postures. Thus, hand postures are compared by considering their 3D structure.</p><p>For each of the training images, the hand segment is computed and stored in addition to the corresponding class. For every image in the testing database, the posture recognition is performed by comparing the current posture to the training ones previously stored. The similarity measure between the candidate and all the templates in the training database is calculated and the highest score indicates the recognized posture. The selected posture is the one from the training database that is closest in 3D topology to the current posture. The classifier considered is thus the nearest neighbor.</p></sec><sec><label>8.</label><title>Rotation Invariance</title><p>The performance analysis of different methods for hand posture recognition using range cameras [<xref ref-type="bibr" rid="b37-sensors-12-14416">37</xref>] shows that none of the features considered are rotation invariant, which is not realistic for real-time applications as the users will not be allowed to rotate their hand while interacting with a computer. In this research, the rotation-invariance of the posture is achieved by measuring the orientation of the segmented point cloud and by removing that rotation before entering the recognition process. Once the orientation is removed, its signature can be computed. The evaluation of the orientation is achieved in two steps. Using the principal component analysis, the primary axis of the hand segment is derived by considering the eigenvector corresponding to the largest eigenvalue. The primary axis of a hand segment can be defined as the longitudinal axis that passes through the centroid of the segment and along the segment no matter the orientation of the segment. The angle between this principal axis and the <italic>Y</italic> axis of the camera frame is used to rotate the segment by bringing them into coincidence. At this step, the rotation is not completely removed as the angle around the <italic>Y</italic> axis between the direction the posture is facing and the one it should be facing (the <italic>Z</italic> axis of the camera frame) is not yet evaluated. To measure the latter, the centroid of the hand segment is determined. All points within 3 cm of the centroid are selected and assumed to belong to the hand palm. Using least square regression, a plane is fitted within the obtained set of points. The perpendicular direction to this plane is supposed to be the direction the posture is facing. An example is provided in <xref ref-type="fig" rid="f11-sensors-12-14416">Figure 11</xref> where the original hand segment, the result of a random rotation applied to it, the result of the first rotation removal as well as the final result are shown.</p><p>One ambiguity appears in this methodology for evaluating the rotation of the hand segment: The orientations of the two axes determined. For example, after the first rotation removal, it is not always clear whether the hand segment is pointing upwards or downwards. The same problem has been notated for the second rotation removal. It has not been possible to rigorously solve this ambiguity and as a consequence for both rotation axes, the two orientations have to be considered every time; which results in four different possibilities for the un-rotated hand segment. An example of an incorrect, un-rotated hand segment that justifies this chosen solution is provided in <xref ref-type="fig" rid="f12-sensors-12-14416">Figure 12</xref>.</p></sec><sec><label>9.</label><title>Experimental Results and Analysis</title><p>To evaluate this signature in combination with the algorithm for rotation removal, only 12 postures out of the 33 (<xref ref-type="fig" rid="f13-sensors-12-14416">Figure 13</xref>) have been considered. The main reason is to avoid postures that look alike and might cause some misclassifications. For example, <xref ref-type="fig" rid="f13-sensors-12-14416">Figure 13</xref> shows the resemblance between &#x0201c;A&#x0201d;, &#x0201c;M&#x0201d;, &#x0201c;N&#x0201d;, &#x0201c;S&#x0201d; and between &#x0201c;T&#x0201d; and &#x0201c;E&#x0201d;. The same observation can be made with &#x0201c;D&#x0201d; and &#x0201c;1&#x0201d;, &#x0201c;W&#x0201d; and &#x0201c;6&#x0201d; or with &#x0201c;G&#x0201d; and &#x0201c;H&#x0201d;. In this case, only one training image has been used per posture while more than 1,000 images on average were tested for each selected posture.</p><p>The images have been collected from the same user. The range camera with its ability to collect at video rates has been used to capture the scene where the user performing one posture at a time was moving his hand in all three directions of the camera frame and rotating his hand meanwhile. The data collection was stopped once at least 1,000 images were captured for every single posture. For every posture, the set of images collected contained images where a 3D rotation has been applied to the hand movement.</p><p>To evaluate how rotated are the hand postures in the testing dataset, the rotation invariance algorithm has been applied to the images collected with the posture &#x0201c;5&#x0201d;. The purpose of this algorithm is to evaluate the two angles required to remove the orientation of the posture. <xref ref-type="fig" rid="f14-sensors-12-14416">Figures 14</xref> and <xref ref-type="fig" rid="f15-sensors-12-14416">15</xref> show respectively the histogram of the occurrence of the angle between this principal axis and the Y axis of the camera frame and the one around the Y axis between the direction the posture is facing and the Z axis of the camera frame. Theoretically the first angle can fluctuate between 0 and 180&#x000b0; while the second one can vary between &#x02212;180 and 180&#x000b0;. <xref ref-type="fig" rid="f14-sensors-12-14416">Figure 14</xref> shows that for the frames collected with the posture &#x0201c;5&#x0201d;, the first angle varies between 0 and 130&#x000b0;. <xref ref-type="fig" rid="f15-sensors-12-14416">Figure 15</xref> shows that the second angle varies between &#x02212;150 and 150&#x000b0;. It can thus been concluded that there is a high variation of rotation in the dataset collected for the posture &#x0201c;5&#x0201d; and extend this conclusion to all the testing dataset as similar demonstrations could be done for all other postures tested.</p><p>Some snapshots from the real-time application showing the original range image, the segmented rotated hand blob, the un-rotated segment as well as the recognized hand postures are presented in <xref ref-type="fig" rid="f16-sensors-12-14416">Figure 16</xref>. <xref ref-type="fig" rid="f17-sensors-12-14416">Figures 17</xref> and <xref ref-type="fig" rid="f18-sensors-12-14416">18</xref> present, respectively, the confusion matrix and the recognition rates. Because of the careful selection of the postures that avoids any resemblance, the overall recognition rate is 93.88%. Very few mismatches have been noted, as shown by the <xref ref-type="fig" rid="f17-sensors-12-14416">Figure 17</xref>. This result is similar to the one obtained using the same methodology but with a different dataset that was described in the conference paper [<xref ref-type="bibr" rid="b38-sensors-12-14416">38</xref>] where 98.24% overall recognition rate was obtained.</p></sec><sec><label>10.</label><title>Comparison with Existing Methods</title><p>In [<xref ref-type="bibr" rid="b39-sensors-12-14416">39</xref>], the authors have performed a comparative study where different shape descriptors for hand posture recognition have been evaluated using different classification methods. Using a database made of 11 postures and 1,000 images per posture taken from different users, the hand posture recognition has been performed with Hu-moments, Zernike moments and Fourier descriptors as features and Bayesian classifier, Support vector machine, k-Nearest Neighbors (k-NN) and Euclidian distance as classifiers. The best result achieved is 87.9% using the k-NN and Fourier descriptors. Another study, [<xref ref-type="bibr" rid="b40-sensors-12-14416">40</xref>] shows 88.8% overall recognition rate after testing 12 postures with 20 images for each resulting in 240 tested images. In this study, the authors use the orientation histogram as feature and the posture matching has been accomplished using Normalized Cross Correlation. <xref ref-type="table" rid="t3-sensors-12-14416">Table 3</xref> summarizes the different aspects considered in the comparison.</p><p>Though the methodology described herein is tested on similar number of postures, it provides a higher recognition rate and has several advantages over these methods: the use of 3D images, a segmentation process independent on the skin of the colour, on the background of the image and on whether the user needs to wear long sleeves or not. In addition, only one training image is required per posture compared to 500 in [<xref ref-type="bibr" rid="b40-sensors-12-14416">40</xref>] corresponding to 50% of the dataset.</p></sec><sec><label>11.</label><title>Conclusions and Future Work</title><p>The current paper addresses the following question: How to recognize hand postures independently of the hand orientation while using a better representation of the hand compared to the mostly available ones in literature? Though simplistic, the proposed signature associated with the rotation invariant algorithm has been successful in recognizing 12 postures taken from the American Sign Language alphabet. Indeed, 93.88% of the 14,732 postures tested have been correctly recognized. This method uses a 3D representation of the hand and it has been proven the robustness of the rotation invariant algorithm. In addition, the objective was to design a real-time application and thus reduce as much as possible the recognition process time. To achieve the latter, only one training image has been considered in the supervised classification.</p><p>In future work, the focus will be made on the 3D signature by improving the alignment of two hand postures before their comparison in order to always compare corresponding parts of the hand posture. To be able to achieve the final goal being the recognition of all the 33 postures appearing in the alphabet of the American Sign Language, an improvement of the noise removal is also required. Furthermore, dynamic gestures involving one or two hands and also multiple cameras will be addressed.</p></sec></body><back><ack><p>This work was supported by the Werner Graupe International Fellowship, the Computer Modelling Group LTD and the Natural Sciences and Engineering Research Council of Canada (NSERC).</p></ack><ref-list><title>References</title><ref id="b1-sensors-12-14416"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Y.F.</given-names></name><name><surname>Ho</surname><given-names>C.S.</given-names></name></person-group><article-title>A user-dependent easily-adjusted static finger language recognition system for handicapped aphasiacs</article-title><source>Appl. Artif. Intell.</source><year>2009</year><volume>23</volume><fpage>932</fpage><lpage>944</lpage></element-citation></ref><ref id="b2-sensors-12-14416"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rashid</surname><given-names>O.</given-names></name><name><surname>Al-Hamadi</surname><given-names>A.</given-names></name><name><surname>Michaelis</surname><given-names>B.</given-names></name></person-group><article-title>A Framework for the Integration of Gesture and Posture Recognition Using HMM and SVM</article-title><conf-name>Proceedings of the International Conference on Intelligent Computing and Intelligent Systems</conf-name><conf-loc>Shanghai, China</conf-loc><conf-date>20&#x02013;22 November 2009</conf-date><fpage>572</fpage><lpage>577</lpage></element-citation></ref><ref id="b3-sensors-12-14416"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhi</surname><given-names>L.</given-names></name><name><surname>Ray</surname><given-names>J.</given-names></name></person-group><article-title>Real Time Hand Gesture Recognition Using a Range Camera</article-title><conf-name>Proceedings of the Conference on Robotics and Automation (ACRA)</conf-name><conf-loc>Sydney, Australia</conf-loc><conf-date>2&#x02013;4 December 2009</conf-date></element-citation></ref><ref id="b4-sensors-12-14416"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y.</given-names></name><name><surname>Jung</surname><given-names>K.</given-names></name></person-group><article-title>3D Posture Representation Using Meshless Parameterization with Cylindrical Virtual Boundary</article-title><conf-name>Proceedings of the 2nd Pacific Rim Conference on Advances in Image and Video Technology</conf-name><conf-loc>Santiago, Chile</conf-loc><conf-date>17&#x02013;19 December 2007</conf-date><fpage>449</fpage><lpage>461</lpage></element-citation></ref><ref id="b5-sensors-12-14416"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>G.-F.</given-names></name><name><surname>Kang</surname><given-names>S.-K.</given-names></name><name><surname>Song</surname><given-names>W.-C.</given-names></name><name><surname>Jung</surname><given-names>S.-T.</given-names></name></person-group><article-title>Real-Time Gesture Recognition Using 3D Depth Camera</article-title><conf-name>Proceedings of the IEEE 2nd International Conference on Software Engineering and Service Science</conf-name><conf-loc>Beijing, China</conf-loc><conf-date>15&#x02013;17 July 2011</conf-date><fpage>187</fpage><lpage>190</lpage></element-citation></ref><ref id="b6-sensors-12-14416"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Uebersax</surname><given-names>D.</given-names></name><name><surname>Gall</surname><given-names>J.</given-names></name><name><surname>van den Bergh</surname><given-names>M.</given-names></name><name><surname>van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Real-Time Sign Language Letter and Word Recognition from Depth Data</article-title><conf-name>Proceedings of the IEEE Workshop on Human Computer Interaction</conf-name><conf-loc>Barcelona, Spain</conf-loc><conf-date>7 November 2011</conf-date><fpage>383</fpage><lpage>390</lpage></element-citation></ref><ref id="b7-sensors-12-14416"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Lv</surname><given-names>C.</given-names></name><name><surname>Sun</surname><given-names>D.</given-names></name></person-group><article-title>Hand motion classification using a multi-channel surface electromyography sensor</article-title><source>Sensors</source><year>2012</year><volume>12</volume><fpage>1130</fpage><lpage>1147</lpage><pub-id pub-id-type="pmid">22438703</pub-id></element-citation></ref><ref id="b8-sensors-12-14416"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Raheja</surname><given-names>J.L.</given-names></name><name><surname>Shyam</surname><given-names>R.</given-names></name><name><surname>Kumar</surname><given-names>U.</given-names></name></person-group><article-title>Hand Gesture Capture and Recognition Technique for Real-Time Video Stream</article-title><conf-name>Proceedings of the International Conference on Artificial Intelligence and Soft Computing</conf-name><conf-loc>Palma de Mallorca, Spain</conf-loc><conf-date>7&#x02013;9 September 2009</conf-date><fpage>40</fpage><lpage>44</lpage></element-citation></ref><ref id="b9-sensors-12-14416"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Grzeszcuk</surname><given-names>R.</given-names></name><name><surname>Bradski</surname><given-names>G.</given-names></name><name><surname>Chu</surname><given-names>M.H.</given-names></name><name><surname>Bouguet</surname><given-names>J.</given-names></name></person-group><article-title>Stereo Based Gesture Recognition Invariant to 3D Pose and Lighting</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><conf-loc>Hilton Head, SC, USA</conf-loc><conf-date>13&#x02013;15 June 2000</conf-date><fpage>826</fpage><lpage>833</lpage></element-citation></ref><ref id="b10-sensors-12-14416"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malassiotis</surname><given-names>S.</given-names></name><name><surname>Strintzis</surname><given-names>M.G.</given-names></name></person-group><article-title>Real-time hand posture recognition using range data</article-title><source>Image Vis. Comput.</source><year>2008</year><volume>26</volume><fpage>1027</fpage><lpage>1037</lpage></element-citation></ref><ref id="b11-sensors-12-14416"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Breuer</surname><given-names>P.</given-names></name><name><surname>Eckes</surname><given-names>C.</given-names></name><name><surname>Muller</surname><given-names>S.</given-names></name></person-group><article-title>Hand Gesture Recognition with a Novel IR Time-of-Flight Range Camera&#x02014;A Pilot Study</article-title><conf-name>Proceedings of the Mirage 2007 Computer Vision/Computer Graphics Collaboration Techniques and Applications</conf-name><conf-loc>Rocquencourt, France</conf-loc><conf-date>28&#x02013;30 March 2007</conf-date><fpage>247</fpage><lpage>260</lpage></element-citation></ref><ref id="b12-sensors-12-14416"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sung</surname><given-names>K.K.</given-names></name><name><surname>Mi</surname><given-names>Y.N.</given-names></name><name><surname>Phill</surname><given-names>K.R.</given-names></name></person-group><article-title>Color Based Hand and Finger Detection Technology for User Interaction</article-title><conf-name>Proceedings of the International Conference on Convergence and Hybrid Information Technology (ICHIT)</conf-name><conf-loc>Daejeon, Korea</conf-loc><conf-date>28&#x02013;29 August 2008</conf-date><fpage>229</fpage><lpage>236</lpage></element-citation></ref><ref id="b13-sensors-12-14416"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elmezain</surname><given-names>M.</given-names></name><name><surname>Al-Hamadi</surname><given-names>A.</given-names></name><name><surname>Michaelis</surname><given-names>B.</given-names></name></person-group><article-title>Hand Trajectory-Based Gesture Spotting and Recognition Using HMM</article-title><conf-name>Proceedings of the 16th IEEE International Conference on Image Processing</conf-name><conf-loc>Cairo, Egypt</conf-loc><conf-date>7&#x02013;12 November 2009</conf-date><fpage>3577</fpage><lpage>3580</lpage></element-citation></ref><ref id="b14-sensors-12-14416"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Correa</surname><given-names>M.</given-names></name><name><surname>Ruiz-del-solar</surname><given-names>J.</given-names></name><name><surname>Verschae</surname><given-names>R.</given-names></name><name><surname>Jong</surname><given-names>L.</given-names></name><name><surname>Castillo</surname><given-names>N.</given-names></name></person-group><article-title>Real-Time Hand Gesture Recognition for Human Robot Interaction</article-title><source>Lect. Notes Comput. Sci.</source><year>2009</year><volume>5949</volume><fpage>46</fpage><lpage>57</lpage></element-citation></ref><ref id="b15-sensors-12-14416"><label>15.</label><element-citation publication-type="webpage"><article-title>Hand Gesture Recognition Using the Kinect Sensor</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.kinecthacks.com/kinect-hand-gesture-recognition-hands-up/">http://www.kinecthacks.com/kinect-hand-gesture-recognition-hands-up/</ext-link> (accessed on 12 June 2012)</comment></element-citation></ref><ref id="b16-sensors-12-14416"><label>16.</label><element-citation publication-type="webpage"><article-title>The Leap Sensor</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://live.leapmotion.com/about.html">http://live.leapmotion.com/about.html</ext-link> (accessed on 12 June 2012)</comment></element-citation></ref><ref id="b17-sensors-12-14416"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>X.</given-names></name><name><surname>Ming</surname><given-names>X.</given-names></name></person-group><article-title>Hand Gesture Segmentation, Recognition and Application</article-title><conf-name>Proceedings of the IEEE International Symposium on Robotics and Automation</conf-name><conf-loc>Seoul, Korea</conf-loc><conf-date>21&#x02013;26 May 2001</conf-date><fpage>438</fpage><lpage>443</lpage></element-citation></ref><ref id="b18-sensors-12-14416"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>Hand Gesture Detection and Segmentation Based on Difference Background Image with Complex Background</article-title><conf-name>Proceedings of the International Conference on Embedded Software and Systems</conf-name><conf-loc>Sichuan, China</conf-loc><conf-date>29&#x02013;31 July 2008</conf-date><fpage>338</fpage><lpage>343</lpage></element-citation></ref><ref id="b19-sensors-12-14416"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holte</surname><given-names>M.B.</given-names></name><name><surname>Moeslund</surname><given-names>T.B.</given-names></name></person-group><source>Gesture Recognition Using a Range Camera</source><publisher-name>Technical Report for Laboratory of Computer Vision and Media Technology, Aalborg University</publisher-name><publisher-loc>Aalborg, Denmark</publisher-loc><month>2</month><year>2007</year></element-citation></ref><ref id="b20-sensors-12-14416"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>L.</given-names></name><name><surname>Fujimura</surname><given-names>K.</given-names></name></person-group><article-title>Hand Gesture Recognition Using Depth Data</article-title><conf-name>Proceedings of 6th IEEE International Conference on Automatic Face and Gesture Recognition</conf-name><conf-loc>Seoul, Korea</conf-loc><conf-date>17&#x02013;19 May 2004</conf-date><fpage>529</fpage><lpage>534</lpage></element-citation></ref><ref id="b21-sensors-12-14416"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ghobadi</surname><given-names>S.</given-names></name><name><surname>Loepprich</surname><given-names>O.</given-names></name><name><surname>Hartmann</surname><given-names>K.</given-names></name><name><surname>Loffeld</surname><given-names>O.</given-names></name></person-group><article-title>Hand Segmentation Using 2D/3D Images</article-title><conf-name>Proceedings of Conference Ivcnz 2007</conf-name><conf-loc>Hamilton, New Zealand</conf-loc><conf-date>5&#x02013;7 December 2007</conf-date></element-citation></ref><ref id="b22-sensors-12-14416"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Heisele</surname><given-names>B.</given-names></name><name><surname>Ritter</surname><given-names>W.</given-names></name></person-group><article-title>Segmentation of Range and Intensity Image Sequences by Clustering</article-title><conf-name>Proceedings of International Conference on Information Intelligence and Systems</conf-name><conf-loc>Bethesda, MD, USA</conf-loc><conf-date>31 October&#x02013;3 November 1999</conf-date><fpage>223</fpage><lpage>225</lpage></element-citation></ref><ref id="b23-sensors-12-14416"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>D.B.</given-names></name><name><surname>Enokia</surname><given-names>S.</given-names></name><name><surname>Toshiaki</surname><given-names>E.</given-names></name></person-group><article-title>Real-Time Hand Tracking and Gesture Recognition System</article-title><conf-name>Proceedings of the International Conference on Graphics, Vision and Image Processing</conf-name><conf-loc>Cairo, Egypt</conf-loc><conf-date>19&#x02013;21 December 2005</conf-date><fpage>362</fpage><lpage>368</lpage></element-citation></ref><ref id="b24-sensors-12-14416"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isard</surname><given-names>M.</given-names></name><name><surname>Blake</surname><given-names>A.</given-names></name></person-group><article-title>Contour Tracking by Stochastic Propagation of Conditional Density</article-title><conf-name>Proceedings of the 4th European Conference on Computer Vision</conf-name><conf-loc>Cambridge, UK</conf-loc><conf-date>15&#x02013;18 April 1996</conf-date><fpage>343</fpage><lpage>356</lpage></element-citation></ref><ref id="b25-sensors-12-14416"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Guan</surname><given-names>Y.</given-names></name><name><surname>Xiong</surname><given-names>H.</given-names></name></person-group><article-title>Real-time extraction of fingertip for pointing gesture recognition</article-title><source>Comput. Eng. Appl.</source><year>2011</year><volume>47</volume><fpage>219</fpage><lpage>222</lpage></element-citation></ref><ref id="b26-sensors-12-14416"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>M.</given-names></name><name><surname>Tanaka</surname><given-names>K.</given-names></name><name><surname>Weissman</surname><given-names>C.</given-names></name><name><surname>Yerazunis</surname><given-names>W.</given-names></name></person-group><article-title>Computer vision for interactive computer graphics</article-title><source>IEEE Comput. Graphic. Appl.</source><year>1998</year><volume>18</volume><fpage>42</fpage><lpage>53</lpage></element-citation></ref><ref id="b27-sensors-12-14416"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hanning</surname><given-names>Z.</given-names></name><name><surname>Dennis</surname><given-names>J.L.</given-names></name><name><surname>Thomas</surname><given-names>S.H.</given-names></name></person-group><article-title>Static Hand Gesture Recognition Based on Local Orientation Histogram Feature Distribution Model</article-title><conf-name>Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop</conf-name><conf-loc>Washington DC, USA</conf-loc><conf-date>27 June&#x02013;2 July 2004</conf-date></element-citation></ref><ref id="b28-sensors-12-14416"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Ju</surname><given-names>Z.</given-names></name><name><surname>Zhu</surname><given-names>X.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name></person-group><article-title>Dynamic Grasp Recognition Using Time Clustering, Gaussian Mixture Models and Hidden Markov Models</article-title><conf-name>Proceeding of the 1st International Conference</conf-name><conf-loc>Wuhan, China</conf-loc><conf-date>15&#x02013;17 October 2008</conf-date><fpage>669</fpage><lpage>678</lpage></element-citation></ref><ref id="b29-sensors-12-14416"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Triesch</surname><given-names>J.</given-names></name><name><surname>Von</surname><given-names>D.M.</given-names></name></person-group><article-title>Robust Classification of Hand Postures against Complex Backgrounds</article-title><conf-name>Proceedings of the 2nd International Conference on Automatic Face And Gesture Recognition</conf-name><conf-loc>Killington, VT, USA</conf-loc><conf-date>14&#x02013;16 October 1996</conf-date><fpage>170</fpage><lpage>175</lpage></element-citation></ref><ref id="b30-sensors-12-14416"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Black</surname><given-names>M.J.</given-names></name><name><surname>Jepson</surname><given-names>A.D.</given-names></name></person-group><article-title>Recognizing Temporal Trajectories Using the Condensation Algorithm</article-title><conf-name>Proceedings of 3rd IEEE International Conference on Automatic Face and Gesture Recognition</conf-name><conf-loc>Nara, Japan</conf-loc><conf-date>14&#x02013;16 April 1998</conf-date><fpage>16</fpage><lpage>21</lpage></element-citation></ref><ref id="b31-sensors-12-14416"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jeon</surname><given-names>M.-J.</given-names></name><name><surname>Yang</surname><given-names>S.-E.</given-names></name><name><surname>Bien</surname><given-names>Z.</given-names></name></person-group><article-title>User Adaptive Hand Gesture Recognition Using Multivariate Fuzzy Decision Tree and Fuzzy Garbage Model Source</article-title><conf-name>Proceedings of the 2009 IEEE International Conference on Fuzzy Systems</conf-name><conf-loc>Jeju Island, Korea</conf-loc><conf-date>20&#x02013; 24 August 2009</conf-date><fpage>474</fpage><lpage>479</lpage></element-citation></ref><ref id="b32-sensors-12-14416"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Todd</surname><given-names>C.A.</given-names></name><name><surname>Hassan</surname><given-names>S.A.</given-names></name><name><surname>Georgios</surname><given-names>C.A.</given-names></name></person-group><article-title>An Open Source Framework for Real-Time, Incremental, Static and Dynamic Hand Gesture Learning and Recognition</article-title><conf-name>Proceedings of the 13th International Conference on Human-Computer Interaction. Part II: Novel Interaction Methods and Techniques</conf-name><conf-loc>San Diego, CA, USA</conf-loc><conf-date>19&#x02013;24 July 2011</conf-date><fpage>123</fpage><lpage>130</lpage></element-citation></ref><ref id="b33-sensors-12-14416"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heung</surname><given-names>S.</given-names></name><name><surname>Bong</surname><given-names>K.S.</given-names></name><name><surname>Seong</surname><given-names>W.L.</given-names></name></person-group><article-title>Hand gesture recognition based on dynamic Bayesian network framework</article-title><source>Pattern Recogn.</source><year>2010</year><volume>43</volume><fpage>3059</fpage><lpage>3072</lpage></element-citation></ref><ref id="b34-sensors-12-14416"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pragati</surname><given-names>G.</given-names></name><name><surname>Naveen</surname><given-names>A.</given-names></name><name><surname>Sanjeev</surname><given-names>S.</given-names></name></person-group><article-title>Vision based hand gesture recognition</article-title><source>Proc. World Acad. Sci. Eng. Technol.</source><year>2009</year><volume>49</volume><fpage>972</fpage><lpage>977</lpage></element-citation></ref><ref id="b35-sensors-12-14416"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindner</surname><given-names>M.</given-names></name><name><surname>Kolb</surname><given-names>A.</given-names></name></person-group><article-title>Compensation of motion artifacts for time-of-flight cameras, dynamic 3D imaging</article-title><source>Lect. Notes Comput. Sci.</source><year>2009</year><volume>5742</volume><fpage>16</fpage><lpage>27</lpage></element-citation></ref><ref id="b36-sensors-12-14416"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Read</surname><given-names>P.</given-names></name><name><surname>Meyer</surname><given-names>M.</given-names></name></person-group><source>Restoration of Motion Picture Film, Conservation and Museology</source><publisher-name>Butterworth-Heinemann</publisher-name><publisher-loc>Oxford, UK</publisher-loc><year>2011</year><fpage>24</fpage><lpage>26</lpage></element-citation></ref><ref id="b37-sensors-12-14416"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lahamy</surname><given-names>H.</given-names></name><name><surname>Lichti</surname><given-names>D.</given-names></name></person-group><article-title>Performance Analysis of Different Methods for Hand Gesture Recognition Using Range Cameras</article-title><conf-name>Proceedings of the Videometrics, Range Imaging, and Applications Conference</conf-name><conf-loc>Munich, Germany</conf-loc><conf-date>23&#x02013;26 May 2011</conf-date><comment>Volume 8085</comment><fpage>80850B:1</fpage><lpage>80850B:14</lpage></element-citation></ref><ref id="b38-sensors-12-14416"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lahamy</surname><given-names>H.</given-names></name><name><surname>Lichti</surname><given-names>D.</given-names></name></person-group><article-title>Robust Real-Time and Rotation-Invariant American Sign Language Alphabet Recognition Using Range Camera</article-title><conf-name>Proceedings of the International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</conf-name><conf-loc>Melbourne, Australia</conf-loc><conf-date>25 August&#x02013;1 September 2012</conf-date><fpage>217</fpage><lpage>222</lpage></element-citation></ref><ref id="b39-sensors-12-14416"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourennane</surname><given-names>S.</given-names></name><name><surname>Fossati</surname><given-names>C.</given-names></name></person-group><article-title>Comparison of shape descriptors for hand posture recognition</article-title><source>Signal Image Video Process.</source><year>2012</year><volume>6</volume><fpage>147</fpage><lpage>157</lpage></element-citation></ref><ref id="b40-sensors-12-14416"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>R.</given-names></name><name><surname>Dias</surname><given-names>M.S.</given-names></name></person-group><article-title>Skin Color Profile Capture for Scale and Rotation Invariant, Hand Gesture Recognition</article-title><conf-name>Proceedings of the 7th International Gesture Workshop</conf-name><conf-loc>Lisbon, Portugal</conf-loc><conf-date>23&#x02013;25 May 2007</conf-date><fpage>81</fpage><lpage>92</lpage></element-citation></ref></ref-list></back><floats-group><fig id="f1-sensors-12-14416" position="float"><label>Figure 1.</label><caption><p>The proposed methodology for hand posture recognition.</p></caption><graphic xlink:href="sensors-12-14416f1"/></fig><fig id="f2-sensors-12-14416" position="float"><label>Figure 2.</label><caption><p>Experimental setup.</p></caption><graphic xlink:href="sensors-12-14416f2"/></fig><fig id="f3-sensors-12-14416" position="float"><label>Figure 3.</label><caption><p>Objective of segmentation.</p></caption><graphic xlink:href="sensors-12-14416f3"/></fig><fig id="f4-sensors-12-14416" position="float"><label>Figure 4.</label><caption><p>Multiple-step range based segmentation.</p></caption><graphic xlink:href="sensors-12-14416f4"/></fig><fig id="f5-sensors-12-14416" position="float"><label>Figure 5.</label><caption><p>Result of the range-based segmentation Before noise removal (<bold>a</bold>); After noise removal (<bold>b</bold>).</p></caption><graphic xlink:href="sensors-12-14416f5"/></fig><fig id="f6-sensors-12-14416" position="float"><label>Figure 6.</label><caption><p>Remaining noise removal. Before using the connected component analysis (<bold>a</bold>); after using the connected component analysis (<bold>b</bold>).</p></caption><graphic xlink:href="sensors-12-14416f6"/></fig><fig id="f7-sensors-12-14416" position="float"><label>Figure 7.</label><caption><p>Evaluation of the segmentation and tracking process while varying the distance between the hand and the camera.</p></caption><graphic xlink:href="sensors-12-14416f7"/></fig><fig id="f8-sensors-12-14416" position="float"><label>Figure 8.</label><caption><p>Tracking result: Tracked centroids and corresponding hand segments.</p></caption><graphic xlink:href="sensors-12-14416f8"/></fig><fig id="f9-sensors-12-14416" position="float"><label>Figure 9.</label><caption><p>Generation of a hand segment signature.</p></caption><graphic xlink:href="sensors-12-14416f9"/></fig><fig id="f10-sensors-12-14416" position="float"><label>Figure 10.</label><caption><p>Methodology of comparison of two hand segments.</p></caption><graphic xlink:href="sensors-12-14416f10"/></fig><fig id="f11-sensors-12-14416" position="float"><label>Figure 11.</label><caption><p>Example of rotation removal of a hand segment.</p></caption><graphic xlink:href="sensors-12-14416f11"/></fig><fig id="f12-sensors-12-14416" position="float"><label>Figure 12.</label><caption><p>Example of an incorrect rotation removal of a hand segment.</p></caption><graphic xlink:href="sensors-12-14416f12"/></fig><fig id="f13-sensors-12-14416" position="float"><label>Figure 13.</label><caption><p>The 33 considered gestures (Dynamic gestures J and Z are out of scope of this study).</p></caption><graphic xlink:href="sensors-12-14416f13"/></fig><fig id="f14-sensors-12-14416" position="float"><label>Figure 14.</label><caption><p>Values of angles between the primary axis of the hand segment and the Y axis of the camera for the frames acquired for the posture &#x0201c;5&#x0201d;.</p></caption><graphic xlink:href="sensors-12-14416f14"/></fig><fig id="f15-sensors-12-14416" position="float"><label>Figure 15.</label><caption><p>Values of angles between the direction the hand segment is facing and the Z axis of the camera for the frames acquired for the posture &#x0201c;5&#x0201d;.</p></caption><graphic xlink:href="sensors-12-14416f15"/></fig><fig id="f16-sensors-12-14416" position="float"><label>Figure 16.</label><caption><p>Considered postures Snapshots from real-time application showing the original range image, the segmented hand blob, the un-rotated segment and the recognized hand postures.</p></caption><graphic xlink:href="sensors-12-14416f16a"/><graphic xlink:href="sensors-12-14416f16b"/></fig><fig id="f17-sensors-12-14416" position="float"><label>Figure 17.</label><caption><p>Confusion matrix.</p></caption><graphic xlink:href="sensors-12-14416f17"/></fig><fig id="f18-sensors-12-14416" position="float"><label>Figure 18.</label><caption><p>Recognition rates.</p></caption><graphic xlink:href="sensors-12-14416f18"/></fig><table-wrap id="t1-sensors-12-14416" position="float"><label>Table 1.</label><caption><p>Evaluation of the time required for a segmentation of an acquired image.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"/><th align="center" valign="top" rowspan="1" colspan="1"><bold>Range-based segmentation (s)</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>Noise Removal (s)</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>Connected component analysis (s)</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>Total Time (s)</bold></th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1"><bold>Minimum</bold></td><td align="center" valign="top" rowspan="1" colspan="1">1.10</td><td align="center" valign="top" rowspan="1" colspan="1">0.00</td><td align="center" valign="top" rowspan="1" colspan="1">0.10</td><td align="center" valign="top" rowspan="1" colspan="1">1.30</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1"><bold>Average</bold></td><td align="center" valign="top" rowspan="1" colspan="1">4.00</td><td align="center" valign="top" rowspan="1" colspan="1">0.10</td><td align="center" valign="top" rowspan="1" colspan="1">1.91</td><td align="center" valign="top" rowspan="1" colspan="1">5.81</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1"><bold>Maximum</bold></td><td align="center" valign="top" rowspan="1" colspan="1">2.01</td><td align="center" valign="top" rowspan="1" colspan="1">0.00</td><td align="center" valign="top" rowspan="1" colspan="1">0.53</td><td align="center" valign="top" rowspan="1" colspan="1">2.55</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1"><bold>Standard deviation</bold></td><td align="center" valign="top" rowspan="1" colspan="1">0.39</td><td align="center" valign="top" rowspan="1" colspan="1">0.02</td><td align="center" valign="top" rowspan="1" colspan="1">0.32</td><td align="center" valign="top" rowspan="1" colspan="1">0.67</td></tr></tbody></table></table-wrap><table-wrap id="t2-sensors-12-14416" position="float"><label>Table 2.</label><caption><p>Evaluation of the tracking accuracy in cm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"/><th align="center" valign="top" rowspan="1" colspan="1"><bold>RMS(<italic>X</italic>)</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>RMS(<italic>Y</italic>)</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>RMS(<italic>Z</italic>)</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1"><bold>Kalman Filter Tracking</bold></td><td align="center" valign="top" rowspan="1" colspan="1">0.3</td><td align="center" valign="top" rowspan="1" colspan="1">0.5</td><td align="center" valign="top" rowspan="1" colspan="1">0.8</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1"><bold>Mean-Shift Tracking</bold></td><td align="center" valign="top" rowspan="1" colspan="1">0.3</td><td align="center" valign="top" rowspan="1" colspan="1">0.6</td><td align="center" valign="top" rowspan="1" colspan="1">0.7</td></tr></tbody></table></table-wrap><table-wrap id="t3-sensors-12-14416" position="float"><label>Table 3.</label><caption><p>Comparison of methods and results between the current study and two others.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="1" colspan="1"><bold>Postures considered</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>Taken from the alphabet of the American sign language</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>Taken from the alphabet of the American sign language</bold></th><th align="center" valign="top" rowspan="1" colspan="1"><bold>Taken from the alphabet of the American sign language</bold></th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Number of postures considered</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Number of testing images per posture</td><td align="center" valign="middle" rowspan="1" colspan="1">1,000</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">1,000</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Features used</td><td align="center" valign="middle" rowspan="1" colspan="1">Fourier descriptors</td><td align="center" valign="middle" rowspan="1" colspan="1">Oriented gesture descriptors</td><td align="center" valign="middle" rowspan="1" colspan="1">3D signature</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Classifier used</td><td align="center" valign="middle" rowspan="1" colspan="1">k-NN</td><td align="center" valign="middle" rowspan="1" colspan="1">Normalized cross correlation</td><td align="center" valign="middle" rowspan="1" colspan="1">k-NN</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Number of templates per posture</td><td align="center" valign="middle" rowspan="1" colspan="1">500</td><td align="center" valign="middle" rowspan="1" colspan="1">Not Provided</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Number of users for the testing database</td><td align="center" valign="middle" rowspan="1" colspan="1">Multiple users</td><td align="center" valign="middle" rowspan="1" colspan="1">Not Provided</td><td align="center" valign="middle" rowspan="1" colspan="1">1 user</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Overall recognition rate</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.88%</td></tr></tbody></table></table-wrap></floats-group></article>