<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27341524</article-id><article-id pub-id-type="pmc">4920424</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0157940</article-id><article-id pub-id-type="publisher-id">PONE-D-15-51303</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject><subj-group><subject>Beetles</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Computational Biology</subject><subj-group><subject>Computational Neuroscience</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational Neuroscience</subject><subj-group><subject>Artificial Neural Networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject><subj-group><subject>Beetles</subject><subj-group><subject>Tribolium</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal Anatomy</subject><subj-group><subject>Wings</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Dogs</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Storage and Handling</subject><subj-group><subject>Specimen Storage</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Species Identification of Food Contaminating Beetles by Recognizing Patterns in Microscopic Images of Elytra Fragments</article-title><alt-title alt-title-type="running-head">Food Pest Contamination Detection by Image Analysis</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Park</surname><given-names>Su Inn</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="author-notes" rid="currentaff001"><sup>&#x000a4;a</sup></xref></contrib><contrib contrib-type="author"><name><surname>Bisgin</surname><given-names>Halil</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="author-notes" rid="currentaff002"><sup>&#x000a4;b</sup></xref></contrib><contrib contrib-type="author"><name><surname>Ding</surname><given-names>Hongjian</given-names></name><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Semey</surname><given-names>Howard G.</given-names></name><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Langley</surname><given-names>Darryl A.</given-names></name><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Tong</surname><given-names>Weida</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5313-5847</contrib-id><name><surname>Xu</surname><given-names>Joshua</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Computer Science, Texas A&#x00026;M University, College Station, Texas, United States of America</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Division of Bioinformatics and Biostatistics, National Center for Toxicological Research, U.S. Food and Drug Administration, Jefferson, Arkansas, United States of America</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Arkansas Regional Laboratories, Office for Regulatory Affairs, U.S. Food and Drug Administration, Jefferson, Arkansas, United States of America</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Yang</surname><given-names>Jinn-Moon</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>National Chiao Tung University, TAIWAN</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist. SIP&#x02019;s current affiliation with Samsung Austin Semiconductor has no influence over the authors' adherence to PLOS ONE policies on sharing data and materials.</p></fn><fn fn-type="con" id="contrib001"><p>Conceived and designed the experiments: HD HGS JX. Performed the experiments: HD HGS DAL. Analyzed the data: SP HB JX WT. Contributed reagents/materials/analysis tools: HD HGS DAL. Wrote the paper: SP HB WT JX. Reviewed and revised the manuscript: SP HB HD HGS DAL WT JX.</p></fn><fn fn-type="current-aff" id="currentaff001"><label>&#x000a4;a</label><p>Current address: Samsung Austin Semiconductor, LLC, Austin, Texas, United States of America</p></fn><fn fn-type="current-aff" id="currentaff002"><label>&#x000a4;b</label><p>Current address: Department of Computer and Information Systems, University of Michigan-Flint, Flint, Michigan, United States of America</p></fn><corresp id="cor001">* E-mail: <email>zhihua.xu@fda.hhs.gov</email></corresp></author-notes><pub-date pub-type="epub"><day>24</day><month>6</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>11</volume><issue>6</issue><elocation-id>e0157940</elocation-id><history><date date-type="received"><day>24</day><month>11</month><year>2015</year></date><date date-type="accepted"><day>7</day><month>6</month><year>2016</year></date></history><permissions><license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/"><license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0</ext-link> public domain dedication.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0157940.pdf"/><abstract><p>A crucial step of food contamination inspection is identifying the species of beetle fragments found in the sample, since the presence of some storage beetles is a good indicator of insanitation or potential food safety hazards. The current pratice, visual examination by human analysts, is time consuming and requires several years of experience. Here we developed a species identification algorithm which utilizes images of microscopic elytra fragments. The elytra, or hardened forewings, occupy a large portion of the body, and contain distinctive patterns. In addition, elytra fragments are more commonly recovered from processed food products than other body parts due to their hardness. As a preliminary effort, we chose 15 storage product beetle species frequently detected in food inspection. The elytra were then separated from the specimens and imaged under a microscope. Both global and local characteristics were quantified and used as feature inputs to artificial neural networks for species classification. With leave-one-out cross validation, we achieved overall accuracy of 80% through the proposed global and local features, which indicates that our proposed features could differentiate these species. Through examining the overall and per species accuracies, we further demonstrated that the local features are better suited than the global features for species identification. Future work will include robust testing with more beetle species and algorithm refinement for a higher accuracy.</p></abstract><funding-group><funding-statement>SIP and HB are grateful to the National Center for Toxicological Research (NCTR) of U.S. Food and Drug Administration (FDA) for the summer research internship program and postdoc research program, respectively, through the Oak Ridge Institute for Science and Education (ORISE). This research was supported in part by internal grants from the FDA&#x02019;s Office for Regulatory Affairs (to HD, Project IR01048) and NCTR (to JX, Protocol E0759101). The views presented in this article do not necessarily reflect those of the FDA. This research was conducted while SIP was a graduate student at Texas A&#x00026;M University. The funders or SIP&#x02019;s current affiliation (Samsung Austin Semiconductor, LLC) had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the &#x0201c;Author Contributions&#x0201d; section.</funding-statement></funding-group><counts><fig-count count="13"/><table-count count="1"/><page-count count="22"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>For data access, the URL is <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.6084/m9.figshare.3398149.v1">https://dx.doi.org/10.6084/m9.figshare.3398149.v1</ext-link> and the DOI is <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.3398149.v1">10.6084/m9.figshare.3398149.v1</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>For data access, the URL is <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.6084/m9.figshare.3398149.v1">https://dx.doi.org/10.6084/m9.figshare.3398149.v1</ext-link> and the DOI is <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.3398149.v1">10.6084/m9.figshare.3398149.v1</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>In the food inspection process, insects are a crucial indicator of food sanitation, contamination, and other potential food safety problems. The insect species identification process involves time-consuming microscopic comparison of the insect fragments recovered from the food sample with reference insect specimens. Accurate identification of insects is important for regulatory purposes in order to evaluate the etiology of the food contamination as well as the degree of potential risk to the consumer [<xref rid="pone.0157940.ref001" ref-type="bibr">1</xref>]. There are about 600 insect species that attack food, the majority of which belong to the order Coleoptera (beetles) [<xref rid="pone.0157940.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0157940.ref003" ref-type="bibr">3</xref>]. Since some insect species, particularly those belonging to the same genus, are very similar in appearance, species identification of food contaminating beetles remains a major challenge.</p><p>Current attempts to recover insect fragments and identify their species rely on various micro-analytical techniques, such as chemical extraction or flotation methods [<xref rid="pone.0157940.ref004" ref-type="bibr">4</xref>]. Based on our internal lab experience, about 80% of the fragments recovered were noted to be from storage beetles, out of which about one quarter were reported to be the hardened forewings (elytra). The elytra are of special interest as they contain more recognizable patterns than other exoskeleton fragments. Due to the high degree of similarity among the beetles, many years of experience are required for food inspection analysts to master the microscopic details sufficiently. Even taking into account this consideration, the identification results may vary depending on the analyst&#x02019;s level of expertise. Compounding this difficulty is the fact that the recovered elytra fragments are usually broken, with some damage to the morphological characteristics. In the face of such complications, the task of species identification through fragments is very challenging; it is critical to develop an efficient, consistent, and reliable approach.</p><p>Our innovative solution is to take advantage of modern technologies to build a computer system to identify the species of a beetle by recognizing patterns on the elytra fragments, which are frequently detected during the food inspection process. Thus, we are taking an interdisciplinary approach consisting of image processing [<xref rid="pone.0157940.ref005" ref-type="bibr">5</xref>], machine learning [<xref rid="pone.0157940.ref006" ref-type="bibr">6</xref>], and entomology to address this food safety challenge.</p><p>As a pilot effort, we collected whole elytra specimens of 15 most common storage beetle species and their sample microscopic images; 3 to 6 specimens per species were then chosen along with their images. Next, we simulated elytra fragments by randomly excerpting subimages (i.e., with random position, width, and height) from the sample images; pattern features were subsequently extracted from each subimage. After extraction, the resulting features were utilized as inputs to train and test artificial neural networks (ANNs) to assign the class label for a given specimen. The rest of the paper is organized as follows: we describe the existing studies and systems designed for insect/fly recognition in Section 2; we explain the steps for preprocessing and feature extraction [<xref rid="pone.0157940.ref007" ref-type="bibr">7</xref>] in detail and demonstrate how ANNs are constructed with the feature sets in Section 3; we report our experimental results in Section 4; finally, we discuss the performance and findings of the proposed framework, draw conclusions, and propose some future directions in Section 5.</p></sec><sec id="sec002"><title>Related Work</title><p>Although the identification of insects can be successfully achieved through chemical or genetic procedures, computer-aided approaches have been seen as faster and more cost effective. Therefore, image analysis has been used in various studies that focused on entomology research. One of the early examples is an image-based identification system proposed by Weeks et al.; using this system, they were able to discriminate five closely related species of Ichneumonidae [<xref rid="pone.0157940.ref008" ref-type="bibr">8</xref>]. They extracted the principal components of the wing images and measured the similarities by comparing the characteristics of these components. Their model achieved 94% accuracy for 175 wings.</p><p>In another work [<xref rid="pone.0157940.ref009" ref-type="bibr">9</xref>], bee species were identified by examining forewing features and wing venation. The authors constructed features starting from the lines and intersections in the image; after refining the feature set, they applied support vector machine (SVM) and kernel discriminant analysis (KDA) methods for classification. The distinctive nature of wing patterns also inspired a classification method for fruit flies such as Rhagoletis pomonella, which has four sibling species [<xref rid="pone.0157940.ref010" ref-type="bibr">10</xref>]. The authors hypothesized that hidden biological information existed in the vein structure of wings, which in turn could make the fly distinguishable. Therefore, they utilized Bayesian and probability neural networks, adjusted for the size and shape of the wing.</p><p>An automated taxonomic identification of stoneflies was studied by Larios et al., who employed a four-step process consisting of a region of interest, representation of these regions as scale-invariant feature transform (SIFT) vectors [<xref rid="pone.0157940.ref011" ref-type="bibr">11</xref>], classification of SIFT vectors into features to form a histogram of detected features, and classification of the feature histograms [<xref rid="pone.0157940.ref012" ref-type="bibr">12</xref>]. Furthermore, the authors determined the principal curvature-based region (PCBR) for the first step; they were able to discriminate four classes at 82% accuracy. When they considered two closely-related taxa, Calineuria and Doroneuria, as one class, they reported a higher accuracy of 95%. Wang et al. presented a system which was primarily designed for identification of insects at the order level. They defined several features and used artificial neural networks (ANNs) and an SVM model to analyze these features for identification [<xref rid="pone.0157940.ref013" ref-type="bibr">13</xref>].</p><p>Besides the abovementioned techniques, semi-automated digital library systems have been also introduced. However, most of these systems require the user&#x02019;s domain knowledge and control to complete the task. For instance, Caci et al. utilizes the I3S software package [<xref rid="pone.0157940.ref014" ref-type="bibr">14</xref>], which allows the user to compare an unknown image with images in a reference library using the contours [<xref rid="pone.0157940.ref015" ref-type="bibr">15</xref>]. The authors used this system to identify the beetle Rosalia alpina for which they had 290 samples, each sample having 2 images. Another stand-alone tool, digital automated identification system (DAISY), requires the user to capture the image and perform segmentation [<xref rid="pone.0157940.ref016" ref-type="bibr">16</xref>], due to the need for identical image alignment. This requirement comes from DAISY&#x02019;s visual recognition approach, which relies on orthogonal eigenimages (principal components) [<xref rid="pone.0157940.ref017" ref-type="bibr">17</xref>]. DAISY was further used for the identification of live moths [<xref rid="pone.0157940.ref018" ref-type="bibr">18</xref>].</p><p>As a summary, prior research or systems on beetle/insect identification have been carried out with intact and macroscopic specimen sample images or using the soft wing venation patterns. However, research on bettle species identification through their elytra fragments has not been undertaken especially for a systematic and integrated feature analysis. In this study, we aim to identify the food contaminating beetles at the species level through regconizing patterns in microscopic images of their elytra fragments.</p></sec><sec id="sec003"><title>Data and Methods</title><p>Our algorithm for beetle species identification is outlined in <xref ref-type="fig" rid="pone.0157940.g001">Fig 1</xref>. As is typical for digital image processing and pattern recognition/classification, the envisioned workflow includes step-by-step procedures: preprocessing, feature extraction, and classification. As shown in <xref ref-type="fig" rid="pone.0157940.g001">Fig 1</xref>, the captured elytra fragment image is first Gaussian-filtered and histogram-equalized for quality enhancement. Then, global and local appearance characteristics are quantified as features, which are aggregated as a final feature vector. Finally, through normalization of all independent features, the (input) feature vector is fed into a neural network classifier for training. The rest of this section describes in detail data acquisition, observed elytra patterns (types), feature extraction, and classification.</p><fig id="pone.0157940.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g001</object-id><label>Fig 1</label><caption><title>The workflow of food contaminating beetle species identification by image analysis.</title></caption><graphic xlink:href="pone.0157940.g001"/></fig><sec id="sec004"><title>Data acquisition</title><p>As a preliminary effort, we chose 15 beetle species commonly found during food inspection and collected whole body specimens. The elytra were then taken off and imaged under a Leica M205 FA digital microscope (Leica Microsystems GmbH, Wetzlar, Germany); 3 to 6 elytra specimens per species were then chosen along with their images. Each specimen was manually positioned; then front and back images of each specimen were captured at magnification between 75X and 100X. In this study, only the front images were used. <xref rid="pone.0157940.t001" ref-type="table">Table 1</xref> lists 15 target species and the number of specimen images retained after discarding those of poor quality.</p><table-wrap id="pone.0157940.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.t001</object-id><label>Table 1</label><caption><title>15 target species and the number of specimens collected.</title></caption><alternatives><graphic id="pone.0157940.t001g" xlink:href="pone.0157940.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Index (Number)</th><th align="center" rowspan="1" colspan="1">Scientific Name</th><th align="center" rowspan="1" colspan="1">Number of whole elytra images</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">Cryptolestes pusillus</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">Lasioderma serricorne</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">Gnathocerus cornutus</td><td align="center" rowspan="1" colspan="1">4</td></tr><tr><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">Zabrotes subfasciatus</td><td align="center" rowspan="1" colspan="1">6</td></tr><tr><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">Oryzaephilus mercator</td><td align="center" rowspan="1" colspan="1">4</td></tr><tr><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">Oryzaephilus surinamensis</td><td align="center" rowspan="1" colspan="1">4</td></tr><tr><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">Rhyzopertha dominica</td><td align="center" rowspan="1" colspan="1">4</td></tr><tr><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">Sitophilus granarius</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">Sitophilus oryzae</td><td align="center" rowspan="1" colspan="1">4</td></tr><tr><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">Stegobium paniceum</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">11</td><td align="center" rowspan="1" colspan="1">Tribolium breviconis</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">12</td><td align="center" rowspan="1" colspan="1">Tribolium castaneum</td><td align="center" rowspan="1" colspan="1">3</td></tr><tr><td align="center" rowspan="1" colspan="1">13</td><td align="center" rowspan="1" colspan="1">Tribolium confusum</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">14</td><td align="center" rowspan="1" colspan="1">Tribolium freeman</td><td align="center" rowspan="1" colspan="1">5</td></tr><tr><td align="center" rowspan="1" colspan="1">15</td><td align="center" rowspan="1" colspan="1">Tribolium madens</td><td align="center" rowspan="1" colspan="1">5</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec005"><title>Examination and categorization of elytra patterns</title><p>To identify beetle species by recognizing patterns on elytra fragments, we first examined elytra key characteristics of the target species. In particular, we classified them into four different types or categories. The criteria for categorization are the existence of hairs, holes/grooves, lines, and unit shapes (e.g., circles, squares). <xref ref-type="fig" rid="pone.0157940.g002">Fig 2</xref> shows the categories and some example images of elytra fragments.</p><fig id="pone.0157940.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g002</object-id><label>Fig 2</label><caption><title>Categorization of elytra key characteristics.</title></caption><graphic xlink:href="pone.0157940.g002"/></fig><p>Naturally, one species may exhibit characteristics from more than one category. Individual variations and partially damaged patterns due to food processing (e.g. grinding) may result in alteration to the characteristitic and possibly different type categorization. More importantly, some species that belong to the same genus possess similar visual characteristics, presenting a great challenge for species identification. For example, Species 5 (<italic>Oryzaephilus mercator</italic>) and 6 (<italic>Oryzaephilus surinamensis</italic>) belong to the same genus and have highly similar characteristics. Another example is the set of Species 12, 13, and 14 (<italic>Tribolium castaneum</italic>, <italic>Tribolium confusum</italic>, and <italic>Tribolium freeman</italic>). Since they exhibit extremely small visual differences across species, the established procedures of identifying beetle species based on hierarchical conditions/decisions or comparison of an elytra fragment image with reference elytra images may lead to inconsisten results. Fortunately, each species has its own combinations of elytra key characteristics with particular ranges of color or mathematical (numerical) quantities/relationships of the featured components which disambiguate similar appearance between species. Therefore, in addition to the types in <xref ref-type="fig" rid="pone.0157940.g002">Fig 2</xref>, we examined species-dependent characteristics and particular ranges of color domain values presented on hairs, holes, lines, and etc.</p></sec><sec id="sec006"><title>Preprocessing and simulation of elytra fragments</title><p>Unlike the macroscopic images used in the prior research mentioned in Section 2, our elytra images are captured in a 3-D microscopic environment (i.e., depth-sensitive microscopy). Due to the curvature of the elytra, some local regions of the images are blurred or out of focus. In addition, different exposure, angle, intensity, or reflection of light when images are acquired can cause biased contrasts/intensities or saturated area. The curved area and the light effect lower the quality of images, causing blurred area, local peaks (salt noise), and varied contrasts. Thus, we preprocess the fragment images through a 2-D rotationally symmetric low-pass Gaussian filter (variance = 2, window size = 10x10) and adaptive histogram equalization to reduce local peaks and enhance the range of intensity, respectively. The preprocessing is aimed to remove noise and enhance contrast.</p><p>To segment an elytra region (foreground) from the background, two complementary binary masks are computed individually and merged. One mask is based on histogram analysis and the other is obtained from color similarity measurement as described below.</p><p>A histogram-based mask is created based on histogram peak-valley analysis. Usually, the intensity histogram of our collected images comprises two major (global) peaks; one peak indicates background-related intensities/pixels and the other indicates elytra-comprising (foreground) pixels. To facilitate finding global peaks in the histogram, a smoothing operation is needed to remove local peaks and valleys. We employ a 1-D Gaussian filter with a dynamic variance; the variance is calculated by averaging distances between adjacent local peaks. A valley of minimum value between the two global peaks on the smoothed histogram serves as a threshold to binarize the elytra region.</p><p>Based on color similarity measurements, a color-distance-based mask can also be created. Assuming the elytra object is located and captured in the middle of the image, we set a reference area (i.e. region of interest) in the middle (size: 2% of width x 2% of height) and get a reference vector from the RGB triplet histogram by averaging color component values with a fixed number of bins in the reference area. If a pixel is within a certain threshold (distance) when comparing the color component values of the pixel to the reference vector, the pixel is considered as an elytra pixel. In this manner, foreground pixels are selected for the mask.</p><p>Since there may be holes within the computed masks or background pixels falsely detected as foreground pixels in the elytra region, morphological operations, such as &#x02018;filling&#x02019; and &#x02018;opening&#x02019;, are subsequently performed on each computed mask. Then, an &#x02018;AND&#x02019; logical operation is applied on the two masks to obtain their complementary effect; this gives a more precise boundary between the elytra region and the background. After filtering the whole elytra image through the final binary mask, the elytra region is segmented. Within the detected elytra region, subimages with random sizes (i.e. random width and height) from random positions are repeatedly selected to simulate various elytra fragments; we preset minimum (= 300 pixels) and maximum (= 2500 pixels) width and height for the random size. In our simulation, we take 100 subimages per elytra specimen image in order to cover a whole elytra area in a sufficiently balanced way.</p></sec><sec id="sec007"><title>Global feature extraction</title><p>Detecting and recognizing patterns of the elytra fragments requires two stages of processing: the feature extraction stage which uses digital image processing, and the classification stage. Whole beetle elytra or the fragments include globally appearing characteristics/features such as color, and strong or directional edge, etc. In order to obtain the global features, various methods, such as Fourier transform, Difference-of-Gaussian, and Gabor filter are employed. Overall, these global characteristics are computed with mathematical metrics and defined functions.</p><p>As mentioned in the species identification procedures [<xref rid="pone.0157940.ref019" ref-type="bibr">19</xref>], the global feature approach is to describe a global appearance which contains the overall characteristics of a whole elytra fragment. To quantify the global appearance, several metrics are computed: statistical and spectra measures, color distribution, oriented edge response, and density of specific objects of interest such as hairs, holes, and lines. The computation of the individual features is addressed in more detail in the next subsections.</p><sec id="sec008"><title>Statistical (Spatial) features</title><p>To describe the global appearance, we employ several statistical measures that represent discriminant texture properties using a 1-D intensity histogram and 2-D moments. Such measures of elytra fragment images are based on texture analysis in the spatial domain. Based on previous research [<xref rid="pone.0157940.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0157940.ref021" ref-type="bibr">21</xref>], the following measures are collectively employed as one of the global appearance descriptors. Except Hu moment [<xref rid="pone.0157940.ref022" ref-type="bibr">22</xref>], each of the statistical features is computed as one dimensional numerical output value.</p><list list-type="bullet"><list-item><p>Mean: a measure of average intensity</p></list-item><list-item><p>Standard deviation: a measure of contrast</p></list-item><list-item><p>Uniformity: a measure of uniform grayness (maximum value when all gray values are equal)</p></list-item><list-item><p>Entropy: a measure of disorder/randomness</p></list-item><list-item><p>Smoothness: a measure of relative smoothness of the intensity</p></list-item><list-item><p>Third moment: a measure of the skewness of the histogram</p></list-item><list-item><p>Hu moment: 7 moment invariants with respect to translation, scale, and rotation.</p></list-item></list></sec><sec id="sec009"><title>Spectra features</title><p>With the spatial texture measures, it is difficult to discriminate whether elytra unit patterns are periodic or non-periodic. A better approach is to measure spectra properties in terms of pattern periodicity by transforming spatial domain to frequency domain; this takes into account the directionality of periodic patterns and the concentrations of low or high frequency/energy in the spectrum. Using two dimensional discrete Fast Fourier Transform [<xref rid="pone.0157940.ref023" ref-type="bibr">23</xref>], we calculate frequency responses. To describe the spectra features in two-dimensional frequency domain, we sum up absolute frequency responses (i.e. Euclidean norm of real and imaginary part coefficients) for different radii from the origin of the transformed frequency map. As shown in <xref ref-type="fig" rid="pone.0157940.g003">Fig 3</xref>, for each radius <italic>r</italic>, the corresponding spectra responses in feature map are computed by <xref ref-type="disp-formula" rid="pone.0157940.e001">Eq 1</xref>.
<disp-formula id="pone.0157940.e001"><alternatives><graphic xlink:href="pone.0157940.e001.jpg" id="pone.0157940.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo>_</mml:mo><mml:mi>F</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mstyle displaystyle="false"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>M</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>FM</italic> stands for two dimensional frequenecy map and <italic>(r</italic>, <italic>&#x003b8;)</italic> is the coordinate in polar with radius <italic>r</italic> and angle <italic>&#x003b8;</italic>.</p><fig id="pone.0157940.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g003</object-id><label>Fig 3</label><caption><title>An example of elytra image in gray scale and the corresponding frequency map.</title></caption><graphic xlink:href="pone.0157940.g003"/></fig><p>Since we exclude the component of zero frequency (i.e., the dc component) and dominant frequency patterns are observed within radius of 80, we set radius <italic>r</italic> from 1 to 80. The results of these measures serve as another global feature set.</p></sec><sec id="sec010"><title>Color features</title><p>Along with the texture analysis in spatial and spectrum domain, color distribution of elytra is invariant or relatively insensitive to surface orientation and illumination. (Note that the elytra images are collected in a lab environment.) Color is one of the key elements used to distinguish between species. <xref ref-type="fig" rid="pone.0157940.g004">Fig 4</xref> shows examples of different elytra colors. As shown in <xref ref-type="fig" rid="pone.0157940.g004">Fig 4</xref>, elytra of <italic>Tribolium madens</italic> (Species 15) have a different color from those of species <italic>Lasioderma serricorne</italic> (Species 2) or <italic>Tribolium castaneum</italic> (Species 12).</p><fig id="pone.0157940.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g004</object-id><label>Fig 4</label><caption><title>Examples of elytra colors.</title></caption><graphic xlink:href="pone.0157940.g004"/></fig><p>We digitized RGB color components in RGB domain and concatenated the histogram in each domain as feature vector elements [<xref rid="pone.0157940.ref024" ref-type="bibr">24</xref>]. The number of bins in each color domain is set to 10 not only to represent various elytra colors of 15 species, but also to cover a range of elytra color variation in each species.</p></sec><sec id="sec011"><title>Oriented responses using Gabor filters</title><p>Due to the radial or linear directional arrangement of edges in elytra compartments, another texture representation of elytra patterns involves oriented (angular) impulse responses. Gabor filters quantitatively characterize edge structures with orientations. An oriented Gabor filter <italic>G</italic><sub><italic>&#x003b8;</italic></sub> and the oriented responses <italic>C</italic><sub><italic>&#x003b8;</italic></sub> are formally defined in <xref ref-type="disp-formula" rid="pone.0157940.e003">Eq 2</xref>.
<disp-formula id="pone.0157940.e002"><alternatives><graphic xlink:href="pone.0157940.e002.jpg" id="pone.0157940.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">&#x02032;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003b3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">&#x02032;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">&#x003c0;</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">&#x02032;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x003bb;</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#x003c8;</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives></disp-formula>
<disp-formula id="pone.0157940.e003"><alternatives><graphic xlink:href="pone.0157940.e003.jpg" id="pone.0157940.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic>&#x003b8;</italic> is the orientation of the Gaussian envelope; <italic>x</italic> and <italic>y</italic> are the spatial coordiantes; <italic>x&#x02032; = xcos&#x003b8;+ysin&#x003b8;</italic> and <italic>y&#x02032; = -xsin&#x003b8;+ycos&#x003b8;</italic>; <italic>&#x003bb;</italic> is the sinusoidal factor; <italic>&#x003c8;</italic> is the phase offset; <italic>&#x003c3;</italic> is the sigma/variance of the Gaussian envelope; &#x0201c;*&#x0201d; represents the convolution operator; and <italic>I(x</italic>,<italic>y)</italic> stands for the pixel intensity at <italic>(x</italic>,<italic>y)</italic>.</p><p>We apply Gabor filters to the elytra fragment images with preset orientations [<xref rid="pone.0157940.ref025" ref-type="bibr">25</xref>]. The image is convolved with a bank of oriented Gabor filters G<sub>&#x003b8;</sub> with orientation &#x003b8;, where 0&#x000b0; &#x0003c; = &#x003b8; &#x0003c; 180&#x000b0; in increments of 30&#x000b0;. <xref ref-type="fig" rid="pone.0157940.g005">Fig 5</xref> shows a bank of Gabor filters and the corresponding filtered results after applying the filters to a fragment image (Species 10 in <xref rid="pone.0157940.t001" ref-type="table">Table 1</xref>). As shown in <xref ref-type="fig" rid="pone.0157940.g005">Fig 5</xref>, after using the oriented filters, edge structure is identified. We present the prominent edge structure with the sum, mean and standard deviation of the convolved images. The sum, mean, and standard deviation values are reflected on the feature vector. Additionally, the corresponding angle with the maximal response is added to the feature vector. Thus, the Gabor descriptor consists of 19 elements, the sum, mean and stanadard deviation for each of six oriented Gabor filters, and the max response angle.</p><fig id="pone.0157940.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g005</object-id><label>Fig 5</label><caption><title>An example of angular responses using oriented Gabor filters.</title></caption><graphic xlink:href="pone.0157940.g005"/></fig></sec><sec id="sec012"><title>Hair/Hole/Line features</title><p>As mentioned above, hairs, holes, grooves, or line patterns are a distinctive class of elytra features. These components are perceived visually since they stand in contrast to the surroundings. To segment these feature objects, we employ a Difference-of-Gaussian (DoG) filter with two variances: one large and one small, i.e., 100 and 1, respectively. A Difference-of-Gaussian filter is defined as follows:
<disp-formula id="pone.0157940.e004"><alternatives><graphic xlink:href="pone.0157940.e004.jpg" id="pone.0157940.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">&#x003c0;</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">&#x003c0;</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></alternatives><label>(3)</label></disp-formula>
where <italic>x</italic> and <italic>y</italic> are the spatial coordiantes; <italic>&#x003c3;</italic><sub><italic>1</italic></sub> and <italic>&#x003c3;</italic><sub><italic>2</italic></sub> are the two variances for the Gaussian envelopes.</p><p>The equation implies that a largely blurred image is subtracted from a less blurred one and vice versa, thus, two DoG filtered images are used. Through the DoG filter, we approximate higher and lower intensity of the objects/regions. Then, the approximated objects (i.e. regions of feature elements such as hairs and holes) are detected [<xref rid="pone.0157940.ref026" ref-type="bibr">26</xref>] and binarized; a dynamic threshold for binarizing the DoG filtered image is calculated by averaging Otsu thresholds [<xref rid="pone.0157940.ref027" ref-type="bibr">27</xref>] computed from a gray-scale original image and the DoG filtered image. <xref ref-type="fig" rid="pone.0157940.g006">Fig 6</xref> shows a sample fragment image (Species 2), the DoG filtered image, and the binary segmented image. Finally, from the binary image, color distribution, number and area density, and average and median size of the objects are computed. Specifically, color distribution is represented with 30 bins (10 bins in each domain) in RGB color histogram only for segmented hair/hole/line objects. Number and area density are obtained by dividing the total number and area of the segmented objects by the total submiage area, respectively. The final feature vector include 68 elements: 30 dimensional color disciptor, number &#x00026; area density, mean &#x00026; median for each of the two DoG filtered images. Notably, along with color distribution, the density features identify and quantify the amount of local key objects that are not attentively measured in spatial/spectra texture or color descriptors. Since they are computed throughout the whole captured area, but takes in only local key regions, we categorize this feature set into another type of global feature as &#x0201c;global feature 2&#x0201d; in Results section, which lead us to explore the impact of key objects.</p><fig id="pone.0157940.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g006</object-id><label>Fig 6</label><caption><title>An example of elytra hair like feature detection.</title></caption><graphic xlink:href="pone.0157940.g006"/></fig></sec></sec><sec id="sec013"><title>Local feature extraction</title><p>The global features are directly calculated from the whole elytra fragment images. However, these features may not accurately represent local unit patterns (e.g. rectangle, circle, and hole). Also, such features may be affected by factors such as captured/simulated area and/or the size and rotation of the simulated fragment image. In order to express the local appearance precisely, certain important points such as centroids of holes, hairs and unit shapes are selected; then a confined local area around the points of interest is processed for the local appearance analysis. We call such selected points feature points and restrict a process within the specific local area around each feature point using a filter bank. In addition, to represent repeating unit patterns or unique hair/hole/line patterns found locally, mask (window) operations around the feature points are employed. Specifically, a corner detection method for feature points and various computations (e.g. vertical/horizontal projection) around the feature points are performed.The next two sections address the feature point detection and filter-bank-based local feature extraction.</p><sec id="sec014"><title>Feature point detection</title><p>As the first step to describing local structures, we need to define interest points, i.e., feature points that can be detected in consistent and reliable positions. In particular, the points should be tolerant of scale variation to some degree, as it can often happen in microscopic images. In addition, rich local structural content around the feature points needs to exist for its topographical significance. Considering these requirements, we employ the concept of corner points. Among several mathematical approaches, we chose to use Harris corner detection [<xref rid="pone.0157940.ref028" ref-type="bibr">28</xref>] to define and detect corner points. A corner point is defined as the intersection of two edges; shifting a window in any direction from the corner yields significant changes. Thus, sufficient local structure features around the corner can be obtained.</p><p>In practice, a large number of the feature points (over 300 feature points per image) can be initially detected in the fragment images. Many adjacent points within a certain distance from each other (15 pixels) may need to be merged. To refine and merge the feature points, feature points of high density are rejected since undesired false corner points are observed densely in local peak-noise area. For this rejection, we calculate the average number of corner points per window area and set up a threshold range based on the average, i.e., 25% truncated mean calculation. The feature points are accepted only within an area within the range. Also, to merge adjacent points, the individual feature points are dilated with a disk structured element (radius = 4 pixels) and the connected components are subsequently analyzed. Centroids of the individual connected components are located as final feature points. <xref ref-type="fig" rid="pone.0157940.g007">Fig 7</xref> shows an example of detected feature points on holes in a Species 2 fragment image.</p><fig id="pone.0157940.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g007</object-id><label>Fig 7</label><caption><title>Example of detected feature points (marked as blue &#x02018;X&#x02019;s).</title></caption><graphic xlink:href="pone.0157940.g007"/></fig></sec><sec id="sec015"><title>Local feature extraction using filterbank</title><p>To represent local structures around the feature points, color gradient-based edges [<xref rid="pone.0157940.ref027" ref-type="bibr">27</xref>] are computed. <xref ref-type="fig" rid="pone.0157940.g008">Fig 8</xref> shows an elytra fragment image (Species 7) and the color-gradient-edged image.</p><fig id="pone.0157940.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g008</object-id><label>Fig 8</label><caption><title>Sample elytra fragment image of Species 7 and the corresponding color-gradient image.</title></caption><graphic xlink:href="pone.0157940.g008"/></fig><p>From the edge image, a window of preset size (101 x 101 pixels) centered at each feature point is constructed. The window (i.e. local area) is then divided into multiple grid components; <xref ref-type="fig" rid="pone.0157940.g009">Fig 9</xref> shows example captures of the local windows and the individual grid components.</p><fig id="pone.0157940.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g009</object-id><label>Fig 9</label><caption><title>Example local windows around the feature points and the grid components.</title></caption><graphic xlink:href="pone.0157940.g009"/></fig><p>After division, an array of filters and processing (i.e., filter bank) is applied to each cell. The operations within each partitioned area (20x20-pixel grid for each partition) include (1) horizontal and vertical histogram projections with interpolation (7 data points for each x,y axis), (2) mean and variance calculation, and within the individual local windows, color distribution in RGB domain (number of bins is 5 in each domain, total 15 bins). The resulted values are all concatenated in a vector. Finally, the computed vectors from individual local areas (i.e., from each interest point) are averaged for a final local feature set. The individual local feature vectors are illustrated in <xref ref-type="fig" rid="pone.0157940.g010">Fig 10</xref>; the y-axis and x-axis present the individual local windows centered at feature points and the corresponding local feature vectors, respectively. As shown in <xref ref-type="fig" rid="pone.0157940.g010">Fig 10</xref>, a discernible pattern in the local feature vectors is observed along the y-axis.</p><fig id="pone.0157940.g010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g010</object-id><label>Fig 10</label><caption><title>An example heatmap of final computed local feature vectors.</title></caption><graphic xlink:href="pone.0157940.g010"/></fig></sec></sec><sec id="sec016"><title>Species classification using artificial neural networks</title><p>To recognize beetle species, after extracting and normalizing the global and local feature sets, we employ a feed-forward neural network classifier with two hidden layers in sigmoid activation function. Both feature extraction and classifier development were implemented through MATLAB v2012a (MathWorks, Natick, Massachusetts, USA). The global feature vector is composed of the following features with the number of elements shown in parentheses: statistical (spatial texture) features (13), spectra features (80), color features (30), Gabor features (19), and hair/hole/line density feature (68). In total, the global feature vector has 210 elements. As for the local feature vector, 16 elements from each of 25 (5x5 grid) partitions are used to represent mean/variance (2), vertical/horizontal projection interpolated (14: 7 for x and y, respectively). In addition, 15 color bins in the local window (5 bins for each color domain) are extracted. The local feature vector has 415 elements. The feature extraction for each fragment image took 1 to 3 minutes, depending on the image size, using multithreaded and multiprocessor programing on a workstation with two Intel Xeon 2.3GHz 6-core CPUs and 128 gigabytes of memory. To assess the effects of the global and local features on performance, or classification accuracy, we grouped the features into four sets: (1) a subset of global features except hair/hole/line features, (2) the complementary subset of global features (i.e., hair/hole/line features), (3) the set of local features, and (4) the combined set of global and local features (all computed features). In particular, to observe how effectively the features obtained from selectively-detected objects such as hairs and holes represent the elytra characteristics, the first subset of global features does not include the hair/hole/line density feature.</p><p>The data comprise 6,900 elytra fragment images (100 fragment images per whole elytra image from the collected specimens, total about 30 gigabytes). For each round of cross validation, one specimen in each species is randomly and independently reserved to make the hold-out test set. Thus the numbers of training and test images are 5,400 and 1,500, respectively. We first explored a few ANNs with one, two, or three hidden layers for 10 rounds of cross validation and found that networks with two hidden layers usually delivered better classification performance. Following the rule of thumb method [<xref rid="pone.0157940.ref029" ref-type="bibr">29</xref>], we experimented with the number of nodes in hidden layers in multiples of 25, ranging from one fourth to two times of the dimension of each feature set. Our exploration led to the configuration choice of two hidden layers with 50 nodes at each layer. To improve neural network generalization and avoid overfitting, we adopted the early stopping technique implemented in MATALB with random division of 5,400 training subimages by 70:20:10 into three sets to monitor the training progress. This early stopping approach, together with the relatively small network configuration, also greatly reduced the training time to under 15 minutes. With each feature set, we trained one ANN and tested it by the hold-out test images. This was then repeated 100 times during cross validation to gauge the performance achieved by each feature set and how it may vary per species.</p><p>At the end, we explored two commonly used methods to select informative features and compared the performance of selected feature subsets with the whole set. The minimum redundancy maximum relevance (mRMR) method [<xref rid="pone.0157940.ref030" ref-type="bibr">30</xref>] selects a feature subset based on the combined relevance and redundancy that are defined by high-dimensional mutual information. It offers two options for residual relevance quantification to rank the selected features: mutual information difference (MID) or quotient (MIQ). We selected 50 features using each option and tested the cross-validation identification accuracy using the top K features with an increment by 5. The second method, correlation-based feature selection [<xref rid="pone.0157940.ref031" ref-type="bibr">31</xref>], selects a subset of features by considering their individual predictive ability and the degree of redundancy between them. It has a forward and backward option to output a feature subset without ranking. We acquired two feature subsets through the correlation based method. All feature subsets were then tested with the same 100 rounds of cross validation to compute identification accuracy for comparison with the results achieved without feature selection.</p></sec></sec><sec sec-type="results" id="sec017"><title>Results</title><p><xref ref-type="fig" rid="pone.0157940.g011">Fig 11</xref> shows the overall identification accuracy for each of four feature sets, which are labeled &#x0201c;global features 1&#x0201d;, &#x0201c;global features 2&#x0201d;, &#x0201c;local features&#x0201d;, and &#x0201c;all features&#x0201d;, respectively. Networks built with all features achieved the highest overall identification accuracy (with a mean accuracy at 80%). When comparing the results between &#x0201c;global features 1/2&#x0201d; and &#x0201c;local features&#x0201d;, and those between &#x0201c;local features&#x0201d; and &#x0201c;all features&#x0201d;, the global features performed worse than the local features and seemed to contribute little to the overall performance achieved by &#x0201c;all features&#x0201d;. Furthermore, except two or three rounds of cross validation, local features consistenly outperformed the global features 1 and 2 (data not shown). This result implies that the local features are more likely than the global features to capture the elytra characteristics specific to each species. In addition, it supports the interpretation that the global features might be more sensitive to overall data quality than the local features computed around the refined and merged feature points. In turn, the necessity of collecting image data with consistent quality needs to be emphasized for further research.</p><fig id="pone.0157940.g011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g011</object-id><label>Fig 11</label><caption><title>Overall accuracies of species identification using four different feature sets.</title><p>Each bar plots the mean and the standard deviation of the overall accuracy achieved during 100 rounds of cross validation.</p></caption><graphic xlink:href="pone.0157940.g011"/></fig><p>Along with the overall performance, we examined identification accuracies for each species. Each subpanel of <xref ref-type="fig" rid="pone.0157940.g012">Fig 12</xref> shows per species accuracies obtained by the global features 1 or 2, local features, or all features. The statistical, spectral, color, and angular edge response features in the &#x0201c;global features 1&#x0201d; were capable of identifying many species, but they were not sufficient to distinguish species with a high resemblance. Even with hair/hole/line quantificaiton, the &#x0201c;global featuers 2&#x0201d; rendered a meager performance improvement. The better performance of the local features demonstrated that the segmented local attributes represented in the local features were most effective for characterizing individual species that look alike. Besides a small improvement in overall performance, the inclusion of the global features led to more balanced per species performance than the local features alone. The lowest per species mean accuracy was 66% (for Species 12) by all features but only 52% (for Species 8) by the local features. With all features, 5 species (Species 1, 3, 9, 11, and 15) were identified with high accuracy (&#x02265; 85%) and 5 species (Species 2, 4, 5, 6, and 7) were well-classified (75%&#x02013;85%). Species 8, 10, 12, 13, and 14 were recognized with decent accuracy (65%&#x02013;75%). Importantly, Species 12, 13, and 14 were classified correctly at genus level due to their very similar characteristics as mentioned earlier. Thus, we confirmed that some objects of interest such as hairs, holes and lines were crucial features for species identification.</p><fig id="pone.0157940.g012" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g012</object-id><label>Fig 12</label><caption><title>Identification accuracy per species.</title><p>One panel plots the results for each feature set: (A) a subset of global features (i.e., &#x0201c;global features 1&#x0201d;), (B) the second subset of global features with hair/hole/line features (i.e., &#x0201c;global features 2&#x0201d;), (C) the set of local features (i.e., &#x0201c;local features&#x0201d;), (D) all features (i.e., &#x0201c;global and local features&#x0201d;). Each bar plots the mean and standard deviation of identification accuracy achieved during 100 rounds of cross validation.</p></caption><graphic xlink:href="pone.0157940.g012"/></fig><p>Feature selection was conducted among all global and local features. The cross validation identification accuracy was plot in <xref ref-type="fig" rid="pone.0157940.g013">Fig 13</xref> for feature subsets selected by mRMR. Since the features were ranked, we were able to test their performance incremently as more features were included for testing. For the MID scheme, there was almost no performance gain beyond the top 35 features while the gain was marginal for the MIQ scheme. The best performance, 76.9% for MIQ or 76.1% for MID, was achieved with all 50 selected features. However, both of them were still below the mean accuracy (79.5%) achieved by all features without feature selection, which is plotted as dotted lines in both sub figures. Furthermore, different network configurations such as one or two hidden layers with 25, 50, or 75 nodes were also tested for all 50 selected features. Yet no performance improvements were observed (data not shown). As mentioned above, the correlation-based feature selection method has forward and backward options when each feature is evaluated for inclusion or removal. The forward option selected 52 features while the backward option output 55 features. Since no ranking information was given for these features, we only tested each selected feature subset through the same 100 rounds of cross validation. The mean and standare deviation of identification accuracy were 72.8% and 5.5% for the forward subset, or 69.1% and 5.3% for the backward subset. Thus these two commonly used feature selection methods provided no benefits in improving the classification accuracy for our data set. mRMR appeared to work better than the correlation-based method.</p><fig id="pone.0157940.g013" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0157940.g013</object-id><label>Fig 13</label><caption><title>Cross validation identification accuracy of the features selected by mRMR.</title><p>Each panel plots the results for: (A) the top K features selected through the option of MIQ, and (B) the top K features selected through the option of MID. Tests were done for K&#x02019;s in multiplies of 5. The dot and error bar plot the mean and standard deviation of identification accuracy achieved during 100 rounds of cross validation. As a comparison, the dotted lines represent the mean identification accuracy achieved by all features without feature selection.</p></caption><graphic xlink:href="pone.0157940.g013"/></fig><p>In summary, our results demonstrated the effectiveness of using global and local features for beetle species identification, as evidenced by the high accuracy rates. In addition, the local features led to a considerable performance gain and thus were more effective than the global features.</p></sec><sec id="sec018"><title>Discussion and Future Work</title><p>We started this work with the goal of developing a software algorithm for identifying beetles from their fragments at species level; the aim of this project was to support food inspection. To this end, we selected 15 common storage beetle species and acquired microscopic images of their whole elytra specimens. We then categorized the elytra keys and explored synthetic features to model them. Finally, using ANN classifiers, we evaluated the extracted features in light of their effectiveness in beetle species identification. Our results sufficiently demonstrated the feasibility of applying image analysis to further beetle identification research.</p><p>As the elytra (or fragment) images were captured by a microscope, there are several challenges we discovered. First, we need to collect better quality data. In order to standardize the quality, a formal process for acquiring image data (i.e., data acquisition protocol) needs to be established. In addition, we need to preserve the actual fragments obtained during food inspection and build a database to store the various images.</p><p>With regards to the elytra keys and computed features, we examined and quantified the elytra characteristics, then explored the effect of various features correspondingly. We evaluated the effectiveness of the global and local features individually and in combination. The global features were effective at identifying species not of the same genus, or that have distinctive shapes, textures, and colors across a whole elytra fragment. On the other hand, due to the weakness in describing a subtle local appearance with global features, the local features were useful to differentiate between similar species that have similar hairs/holes/lines, or are of the same genus. In addition to analysis of the current features, we intend to find more candidate features that can be used for identification to further improve the accuracy for more challenging species such as Species 8 and 10. For example, the average distances from each feature point to the most adjacent K feature points or their density (i.e. distribution of corner points) can serve as new features. With the release of image data along with this publication, we hope to stir up interests in this important research field that has a tremendous impact on safeguarding our food supplies.</p><p>According to our small experiment with different configurations, ANN showed fast and fault-tolerant classification performance. With the early stopping technique implemented in MATLAB, the learning process in ANN usually took no more than an hour even for a large ANN (using the same workstation with two Intel Xeon 2.3GHz 6-core CPUs and 128 gigabytes of memory). Given the result that the performance is more dependent on feature sets and species, our future effort will be focused on data expansion and exploring additional local features. After the planned data expansion that is discussed below in detail, we will explore various approaches to optimize the ANNs and perform extensive testing. One drawback of the current ANN design is that any test image will be classified as one of the 15 species included in the study data collection. Though data expansion will help elleviate this shortcoming, we may explore more ANN design options or other classification methods to better address this issue. Some classification methods, e.g., SVM and convolutional neural network [<xref rid="pone.0157940.ref032" ref-type="bibr">32</xref>], will be tested and compared with ANN. For example, SVM was recommended for real application deployment due to its better performance over ANN while ANN would be more useful for testing new features [<xref rid="pone.0157940.ref013" ref-type="bibr">13</xref>].</p><p>It is important to note that our current study has some limitations. First, there are 3&#x02013;6 specimen images per species. A significant increase of specimen images will include more variation in specimen and thus afford a more robust test. Although the selected 15 species can identify the majority (about 70%) of storage beetle fragements recovered during food inspection, to enhance the regulatory utilities of this work, we plan to expand our specimen collection to include about 10 more species so that most (about 90%) storage beetle fragments could be accounted for. Finally, no beetle species from agriculture farms or gardens were included. Since agriculture species pose little risk to public health, we plan to select several agriculture species with elytra sizes similar to those of common storage beetles and incorporate them as negative controls. We expect these planned expansions in data collection will bring new challenges to feature discovery and classifier development.</p><p>In conclusion, our approach and efforts have yielded promising results of beetle species identification and demonstrated the feasibility to address the challenging species identification problem in food inspection with modern technologies. As our project and research continues beyond algorithm development, our focus will extend to building a beetle species identification system and a complementary elytra image retrieval system that has the potential to be useful for many different inspection and regulatory activities. We envision such a full-fledged system using state-of-art technologies: a distributed database (e.g., ElasticSearch [<xref rid="pone.0157940.ref033" ref-type="bibr">33</xref>]) for scalable and flexible data warehouse, GPU-based computation in the backend for fast computation [<xref rid="pone.0157940.ref034" ref-type="bibr">34</xref>], and an integrated user interface (e.g., [<xref rid="pone.0157940.ref035" ref-type="bibr">35</xref>]) in the front-end.</p></sec></body><back><ack><p>We appreciate Dr. Binsheng Gong for assistance in preparing the figures. We thank Drs. Zhichao Liu and Vikrant Vijay for manuscript review and discussion.</p><sec id="sec019"><title>Disclaimer</title><p>The views presented in this article do not necessarily reflect current or future opinion or policy of the US Food and Drug Administration. Any mention of commercial products is for clarification and not intended as endorsement.</p></sec></ack><ref-list><title>References</title><ref id="pone.0157940.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Olsen</surname><given-names>AR</given-names></name>, <name><surname>Gecan</surname><given-names>JS</given-names></name>, <name><surname>Ziobro</surname><given-names>GC</given-names></name>, <name><surname>Bryce</surname><given-names>JR</given-names></name>. <article-title>Regulatory Action Criteria for Filth and Other Extraneous Materials V. Strategy for Evaluating Hazardous and Nonhazardous Filth</article-title>. <source>Regulatory Toxicology and Pharmacology</source>. <year>2001</year>;<volume>33</volume>(<issue>3</issue>):<fpage>363</fpage>&#x02013;<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1006/rtph.2001.1472</pub-id>
<pub-id pub-id-type="pmid">11407939</pub-id></mixed-citation></ref><ref id="pone.0157940.ref002"><label>2</label><mixed-citation publication-type="book"><name><surname>Gorham</surname><given-names>JR</given-names></name>. <source>Insect and Mite Pests in Food, Vol. 1</source>. <series>Agric Handbook No 655</series>
<volume>1</volume>: <publisher-name>U.S. Dept. Agriculture</publisher-name>, <publisher-loc>Washington, DC.</publisher-loc>; <year>1991</year>.</mixed-citation></ref><ref id="pone.0157940.ref003"><label>3</label><mixed-citation publication-type="book"><name><surname>Sinha</surname><given-names>RN</given-names></name>, <name><surname>Watters</surname><given-names>FL</given-names></name>. <source>Insect pests of Flour Mills, Grain Elevators, and Feed Mills and Their Control</source>. <publisher-loc>Ottawa</publisher-loc>: <publisher-name>Agriculture Canada Publ.</publisher-name> 1776; <year>1985</year>.</mixed-citation></ref><ref id="pone.0157940.ref004"><label>4</label><mixed-citation publication-type="book"><name><surname>Olsen</surname><given-names>A</given-names></name>, <name><surname>Sidebottom</surname><given-names>TH</given-names></name>, <name><surname>Knight</surname><given-names>SA</given-names></name>. <source>Fundamentals of Microanalytical Entomology: A Practical Guide to Detecting and Identifying Filth in Foods</source>: <publisher-name>CRC Press</publisher-name>; <year>1996</year>.</mixed-citation></ref><ref id="pone.0157940.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Gonzalez</surname><given-names>RC</given-names></name>, <name><surname>Woods</surname><given-names>RE</given-names></name>, <name><surname>Eddins</surname><given-names>SL</given-names></name>. <source>Digital Image Processing Using MATLAB</source>. <edition>2nd ed</edition>: <publisher-name>Gatesmark Publishing</publisher-name>; <year>2009</year>.</mixed-citation></ref><ref id="pone.0157940.ref006"><label>6</label><mixed-citation publication-type="book"><name><surname>Mohri</surname><given-names>M</given-names></name>, <name><surname>Rostamizadeh</surname><given-names>A</given-names></name>, <name><surname>Talwalkar</surname><given-names>A</given-names></name>. <source>Foundations of Machine Learning</source>: <publisher-name>The MIT Press</publisher-name>; <year>2012</year>. 432 p.</mixed-citation></ref><ref id="pone.0157940.ref007"><label>7</label><mixed-citation publication-type="book"><name><surname>Nixon</surname><given-names>M</given-names></name>. <source>Feature Extraction &#x00026; Image Processing for Computer Vision</source>. <edition>3rd ed</edition>: <publisher-name>Academic Press</publisher-name>; <year>2012</year>.</mixed-citation></ref><ref id="pone.0157940.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Weeks</surname><given-names>PJD</given-names></name>, <name><surname>Gauld</surname><given-names>ID</given-names></name>, <name><surname>Gaston</surname><given-names>KJ</given-names></name>, <name><surname>ONeill</surname><given-names>MA</given-names></name>. <article-title>Automating the identification of insects: A new solution to an old problem</article-title>. <source>B Entomol Res</source>. <year>1997</year>;<volume>87</volume>(<issue>2</issue>):<fpage>203</fpage>&#x02013;<lpage>11</lpage>. .</mixed-citation></ref><ref id="pone.0157940.ref009"><label>9</label><mixed-citation publication-type="other">Arbuckle T, Schr&#x000f6;der S, Steinhage V, Wittmann D, editors. Biodiversity informatics in action: identification and monitoring of bee species using ABIS. Proceedings of the 15th International Symposium Informatics for Environmental Protection; Zurich.</mixed-citation></ref><ref id="pone.0157940.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Bi</surname><given-names>C</given-names></name>, <name><surname>Saunders</surname><given-names>MC</given-names></name>, <name><surname>Mcpheron</surname><given-names>BA</given-names></name>. <article-title>Wing Pattern-Based Classification of the Rhagoletis pomonella Species Complex Using Genetic Neural Networks</article-title>. <source>International Journal of Computer Science &#x00026; Applications</source>. <year>2007</year>;<volume>4</volume>(<issue>3</issue>):<fpage>1</fpage>&#x02013;<lpage>14</lpage>.</mixed-citation></ref><ref id="pone.0157940.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Lowe</surname><given-names>DG</given-names></name>. <article-title>Distinctive Image Features from Scale-Invariant Keypoints</article-title>. <source>Int J Comput Vision</source>. <year>2004</year>;<volume>60</volume>(<issue>2</issue>):<fpage>91</fpage>&#x02013;<lpage>110</lpage>. <pub-id pub-id-type="doi">10.1023/b:visi.0000029664.99615.94</pub-id></mixed-citation></ref><ref id="pone.0157940.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Larios</surname><given-names>N</given-names></name>, <name><surname>Deng</surname><given-names>HL</given-names></name>, <name><surname>Zhang</surname><given-names>W</given-names></name>, <name><surname>Sarpola</surname><given-names>M</given-names></name>, <name><surname>Yuen</surname><given-names>J</given-names></name>, <name><surname>Paasch</surname><given-names>R</given-names></name>, <etal>et al</etal>
<article-title>Automated insect identification through concatenated histograms of local appearance features: feature vector generation and region detection for deformable objects</article-title>. <source>Mach Vision Appl</source>. <year>2008</year>;<volume>19</volume>(<issue>2</issue>):<fpage>105</fpage>&#x02013;<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1007/s00138-007-0086-y</pub-id> .</mixed-citation></ref><ref id="pone.0157940.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>JN</given-names></name>, <name><surname>Lin</surname><given-names>CT</given-names></name>, <name><surname>Ji</surname><given-names>LQ</given-names></name>, <name><surname>Liang</surname><given-names>AP</given-names></name>. <article-title>A new automatic identification system of insect images at the order level</article-title>. <source>Knowl-Based Syst</source>. <year>2012</year>;<volume>33</volume>:<fpage>102</fpage>&#x02013;<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1016/j.knosys.2012.03.014</pub-id> .</mixed-citation></ref><ref id="pone.0157940.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Van Tienhoven</surname><given-names>AM</given-names></name>, <name><surname>Den Hartog</surname><given-names>JE</given-names></name>, <name><surname>Reijns</surname><given-names>RA</given-names></name>, <name><surname>Peddemors</surname><given-names>VM</given-names></name>. <article-title>A computer-aided program for pattern-matching of natural marks on the spotted raggedtooth shark Carcharias taurus</article-title>. <source>Journal of Applied Ecology</source>. <year>2007</year>;<volume>44</volume>(<issue>2</issue>):<fpage>273</fpage>&#x02013;<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1111/j.1365-2664.2006.01273.x</pub-id></mixed-citation></ref><ref id="pone.0157940.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Caci</surname><given-names>G</given-names></name>, <name><surname>Biscaccianti</surname><given-names>AB</given-names></name>, <name><surname>Cistrone</surname><given-names>L</given-names></name>, <name><surname>Bosso</surname><given-names>L</given-names></name>, <name><surname>Garonna</surname><given-names>AP</given-names></name>, <name><surname>Russo</surname><given-names>D</given-names></name>. <article-title>Spotting the right spot: computer-aided individual identification of the threatened cerambycid beetle Rosalia alpina</article-title>. <source>Journal of Insect Conservation</source>. <year>2013</year>.</mixed-citation></ref><ref id="pone.0157940.ref016"><label>16</label><mixed-citation publication-type="other">O'Neill MA, Gauld ID, Gaston KJ, Weeks PJD, editors. Daisy: an automated invertebrate identification system using holistic vision techniques. Inaugural Meeting BioNETINTERNATIONALGroup for Computer-AidedTaxonomy (BIGCAT); 2000; Egham.</mixed-citation></ref><ref id="pone.0157940.ref017"><label>17</label><mixed-citation publication-type="other">Turk MA, Pentland AP, editors. Face recognition using eigenfaces. IEEE Conference on Computer Vision and Pattern Recognition; 1991.</mixed-citation></ref><ref id="pone.0157940.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Watson</surname><given-names>AT</given-names></name>, <name><surname>O'Neill</surname><given-names>MA</given-names></name>, <name><surname>Kitching</surname><given-names>IJ</given-names></name>. <article-title>Automated identification of live moths (Macrolepidoptera) using Digital Automated Identification SYstem (DAISY)</article-title>. <source>Systematics and Biodiversity</source>. <year>2003</year>;<volume>1</volume>(<issue>03</issue>):<fpage>287</fpage>&#x02013;<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1017/S1477200003001208</pub-id></mixed-citation></ref><ref id="pone.0157940.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Mayo</surname><given-names>M</given-names></name>, <name><surname>Watson</surname><given-names>AT</given-names></name>. <article-title>Automatic species identification of live moths</article-title>. <source>Knowl-Based Syst</source>. <year>2007</year>;<volume>20</volume>(<issue>2</issue>):<fpage>195</fpage>&#x02013;<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1016/j.knosys.2006.11.012</pub-id></mixed-citation></ref><ref id="pone.0157940.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Clausi</surname><given-names>DA</given-names></name>. <article-title>An analysis of co-occurrence texture statistics as a function of grey level quantization</article-title>. <source>Can J Remote Sens</source>. <year>2002</year>;<volume>28</volume>(<issue>1</issue>):<fpage>45</fpage>&#x02013;<lpage>62</lpage>. .</mixed-citation></ref><ref id="pone.0157940.ref021"><label>21</label><mixed-citation publication-type="other">Hangarge M, Santosh K, Doddamani S, Pardeshi R. Statistical texture features based handwritten and printed text classification in south indian documents. arXiv preprint arXiv:13033087. 2013.</mixed-citation></ref><ref id="pone.0157940.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Ming-Kuei</surname><given-names>H</given-names></name>. <article-title>Visual pattern recognition by moment invariants</article-title>. <source>IRE Transactions on Information Theory</source>. <year>1962</year>;<volume>8</volume>(<issue>2</issue>):<fpage>179</fpage>&#x02013;<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1109/TIT.1962.1057692</pub-id></mixed-citation></ref><ref id="pone.0157940.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Cooley</surname><given-names>JW</given-names></name>, <name><surname>Tukey</surname><given-names>JW</given-names></name>. <article-title>An Algorithm for the Machine Calculation of Complex Fourier Series</article-title>. <source>Mathematics of Computation</source>. <year>1965</year>;<volume>19</volume>(<issue>90</issue>):<fpage>297</fpage>&#x02013;<lpage>301</lpage>. <pub-id pub-id-type="doi">10.2307/2003354</pub-id></mixed-citation></ref><ref id="pone.0157940.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Haldar</surname><given-names>P</given-names></name>, <name><surname>Mukherjee</surname><given-names>J</given-names></name>. <article-title>Content based Image Retrieval using Histogram, Color and Edge</article-title>. <source>International Journal of Computer Applications</source>. <year>2012</year>;<volume>48</volume>(<issue>11</issue>).</mixed-citation></ref><ref id="pone.0157940.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>J</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Park</surname><given-names>SI</given-names></name>, <name><surname>Choe</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>JR</given-names></name>, <name><surname>Han</surname><given-names>A</given-names></name>. <article-title>A microchip for quantitative analysis of CNS axon growth under localized biomolecular treatments</article-title>. <source>J Neurosci Meth</source>. <year>2014</year>;<volume>221</volume>:<fpage>166</fpage>&#x02013;<lpage>74</lpage>. .</mixed-citation></ref><ref id="pone.0157940.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Catarious</surname><given-names>DM</given-names></name>, <name><surname>Baydush</surname><given-names>AH</given-names></name>, <name><surname>Floyd</surname><given-names>CE</given-names></name>. <article-title>Characterization of difference of Gaussian filters in the detection of mammographic regions</article-title>. <source>Med Phys</source>. <year>2006</year>;<volume>33</volume>(<issue>11</issue>):<fpage>4104</fpage>&#x02013;<lpage>14</lpage>. .<pub-id pub-id-type="pmid">17153390</pub-id></mixed-citation></ref><ref id="pone.0157940.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Otsu</surname><given-names>N</given-names></name>. <article-title>A Threshold Selection Method from Gray-Level Histograms</article-title>. <source>Systems, Man and Cybernetics, IEEE Transactions on</source>. <year>1979</year>;<volume>9</volume>(<issue>1</issue>):<fpage>62</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></mixed-citation></ref><ref id="pone.0157940.ref028"><label>28</label><mixed-citation publication-type="other">Derpanis KG. The Harris corner detector2004:[1&#x02013;2 pp.].</mixed-citation></ref><ref id="pone.0157940.ref029"><label>29</label><mixed-citation publication-type="book"><name><surname>Duda</surname><given-names>R</given-names></name>, <name><surname>Hart</surname><given-names>P</given-names></name>, <name><surname>Stork</surname><given-names>D</given-names></name>. <source>Pattern Classification</source>. <edition>2nd ed</edition>: <publisher-name>Wiley</publisher-name>; <year>2000</year>.</mixed-citation></ref><ref id="pone.0157940.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Peng</surname><given-names>H</given-names></name>, <name><surname>Long</surname><given-names>F</given-names></name>, <name><surname>Ding</surname><given-names>C</given-names></name>. <article-title>Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2005</year>;<volume>27</volume>(<issue>8</issue>):<fpage>1226</fpage>&#x02013;<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2005.159</pub-id> .<pub-id pub-id-type="pmid">16119262</pub-id></mixed-citation></ref><ref id="pone.0157940.ref031"><label>31</label><mixed-citation publication-type="book"><name><surname>Hall</surname><given-names>MA</given-names></name>. <chapter-title>Correlation-based feature selection for machine learning</chapter-title>: <publisher-name>The University of Waikato</publisher-name>; <year>1999</year>.</mixed-citation></ref><ref id="pone.0157940.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Matsugu</surname><given-names>M</given-names></name>, <name><surname>Mori</surname><given-names>K</given-names></name>, <name><surname>Mitari</surname><given-names>Y</given-names></name>, <name><surname>Kaneda</surname><given-names>Y</given-names></name>. <article-title>Subject independent facial expression recognition with robust face detection using a convolutional neural network</article-title>. <source>Neural Networks</source>. <year>2003</year>;<volume>16</volume>(<issue>5&#x02013;6</issue>):<fpage>555</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/S0893-6080(03)00115-1</pub-id>
<pub-id pub-id-type="pmid">12850007</pub-id></mixed-citation></ref><ref id="pone.0157940.ref033"><label>33</label><mixed-citation publication-type="other">Kononenko O, Baysal O, Holmes R, Godfrey MW. Mining modern repositories with elasticsearch. Proceedings of the 11th Working Conference on Mining Software Repositories; Hyderabad, India. 2597091: ACM; 2014. p. 328&#x02013;31.</mixed-citation></ref><ref id="pone.0157940.ref034"><label>34</label><mixed-citation publication-type="book"><name><surname>Arenas</surname><given-names>MG</given-names></name>, <name><surname>Romero</surname><given-names>G</given-names></name>, <name><surname>Mora</surname><given-names>AM</given-names></name>, <name><surname>Castillo</surname><given-names>PA</given-names></name>, <name><surname>Merelo</surname><given-names>JJ</given-names></name>. <chapter-title>GPU Parallel Computation in Bioinspired Algorithms: A Review</chapter-title> In: <name><surname>Ko&#x00142;odziej</surname><given-names>J</given-names></name>, <name><surname>Khan</surname><given-names>US</given-names></name>, <name><surname>Burczy&#x000b4;nski</surname><given-names>T</given-names></name>, editors. <source>Advances in Intelligent Modelling and Simulation: Artificial Intelligence-Based Models and Techniques in Scalable Computing</source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2012</year> p. <fpage>113</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref><ref id="pone.0157940.ref035"><label>35</label><mixed-citation publication-type="other">Park SI, Shipman F. PerCon: a personal digital library for heterogeneous data. Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries; London, United Kingdom. 2740786: IEEE Press; 2014. p. 97&#x02013;106.</mixed-citation></ref></ref-list></back></article>