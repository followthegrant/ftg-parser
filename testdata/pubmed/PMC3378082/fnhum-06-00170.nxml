<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Hum Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Hum Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Hum. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Human Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5161</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">22723774</article-id><article-id pub-id-type="pmc">3378082</article-id><article-id pub-id-type="doi">10.3389/fnhum.2012.00170</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research Article</subject></subj-group></subj-group></article-categories><title-group><article-title>Object recognition in clutter: cortical responses depend on the type of learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hegd&#x000e9;</surname><given-names>Jay</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref></contrib><contrib contrib-type="author"><name><surname>Thompson</surname><given-names>Serena K.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Brady</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Kersten</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Ophthalmology, Vision Discovery Institute, Brain and Behavior Discovery Institute, Georgia Health Sciences University, Augusta</institution><country>GA, USA</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Psychology, University of Minnesota, Minneapolis</institution><country>MN, USA</country></aff><aff id="aff3"><sup>3</sup><institution>Pattern Recognition Systems, Johnston</institution><country>RI, USA</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Brain and Cognitive Engineering, Korea University</institution><country>Seoul, South Korea</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Srikantan S. Nagarajan, University of California, San Francisco, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Sara L. Gonzalez Andino, H&#x000f4;pitaux Universitaires de Gen&#x000e8;ve, Switzerland; Shugao Xia, Yeshiva University, USA</p></fn><corresp id="fn001">*Correspondence: Jay Hegd&#x000e9;, Department of Ophthalmology, Vision Discovery Institute, Brain and Behavior Discovery Institute, Georgia Health Sciences University, CL-3033, 1120 15th Street, Augusta, GA 30912, USA. e-mail: <email>jhegde@georgiahealth.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>6</month><year>2012</year></pub-date><pub-date pub-type="collection"><year>2012</year></pub-date><volume>6</volume><elocation-id>170</elocation-id><history><date date-type="received"><day>06</day><month>1</month><year>2012</year></date><date date-type="accepted"><day>24</day><month>5</month><year>2012</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2012 Hegd&#x000e9;, Thompson, Brady and Kersten.</copyright-statement><copyright-year>2012</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><license-p>This is an open-access article distributed under the terms of the <uri xlink:type="simple" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/">Creative Commons Attribution Non Commercial License</uri>, which permits non-commercial use, distribution, and reproduction in other forums, provided the original authors and source are credited.</license-p></license></permissions><abstract><p>Theoretical studies suggest that the visual system uses prior knowledge of visual objects to recognize them in visual clutter, and posit that the strategies for recognizing objects in clutter may differ depending on whether or not the object was learned in clutter to begin with. We tested this hypothesis using functional magnetic resonance imaging (fMRI) of human subjects. We trained subjects to recognize naturalistic, yet novel objects in strong or weak clutter. We then tested subjects' recognition performance for both sets of objects in strong clutter. We found many brain regions that were differentially responsive to objects during object recognition depending on whether they were learned in strong or weak clutter. In particular, the responses of the left fusiform gyrus (FG) reliably reflected, on a trial-to-trial basis, subjects' object recognition performance for objects learned in the presence of strong clutter. These results indicate that the visual system does not use a single, general-purpose mechanism to cope with clutter. Instead, there are two distinct spatial patterns of activation whose responses are attributable not to the visual context in which the objects were seen, but to the context in which the objects were learned.</p></abstract><kwd-group><kwd>clutter tolerance</kwd><kwd>camouflage</kwd><kwd>configural processing</kwd><kwd>crowding</kwd><kwd>perceptual learning</kwd><kwd>pop-out</kwd><kwd>visual context</kwd><kwd>visual search</kwd></kwd-group><counts><fig-count count="7"/><table-count count="3"/><equation-count count="0"/><ref-count count="76"/><page-count count="15"/><word-count count="10939"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>The visual system has a remarkable ability to recognize an object, despite the fact that we rarely see the same view of it twice (Figure <xref ref-type="fig" rid="F1">1A</xref>). This ability rests on incompletely understood brain mechanisms that discount sources of variation in the images of an object and background. Discounting is necessary because images of a single object will vary due to changes in viewpoint, lighting, material reflectance, occlusion, articulation, and background (Kersten et al., <xref ref-type="bibr" rid="B30">2004</xref>).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Effect of prior knowledge on object recognition in clutter. (A)</bold> A novel <italic>image</italic>, i.e., a pattern of intensities whose pixel representation the reader has never seen before. Yet, the toad is easily detected and recognized as a familiar object. Local input information, such as might be extracted by oriented spatial filters in V1 (simple or complex cells) used for signaling local oriented edges, provide highly ambiguous information. In typical natural images, there is no local information that uniquely distinguishes which edges belong to the boundary of an object, and which do not. <bold>(B</bold>) Different images of the same toad result in very different feature patterns. For example, it can be impossible to learn to distinguish which edge measurements near the boundary are from the toad vs. from the background, because most are ambiguous and change drastically from image to image. This variation illustrates the bootstrap learning dilemma. If detection, recognition, and segmentation rely on prior knowledge, how can this knowledge be acquired? Some images produce less ambiguous local edge features than others (bottom right two images), suggesting that object models may be learned given occasional low ambiguity conditions (although toads are not comfortable with such backgrounds). However, learning can also occur given views that are individually highly ambiguous (represented by the kinds of image input in the bottom right three images), but as a collection provide information to construct an object model.</p></caption><graphic xlink:href="fnhum-06-00170-g0001"/></fig><p>Background clutter can be particularly challenging to discount, because object features (e.g., local texture patch, or boundary curve segments) can be confusable with background features, and they may not repeat in subsequent images of the same object (Figure <xref ref-type="fig" rid="F1">1B</xref>). Recognizing an object in the presence of background objects is also made harder by crowding effects that arise when the objects are spaced too closely for a given eccentricity (for overviews, see Motter and Simoni, <xref ref-type="bibr" rid="B39">2007</xref>; Levi, <xref ref-type="bibr" rid="B35">2008</xref>; Pelli and Tillman, <xref ref-type="bibr" rid="B46">2008</xref>). In principle, ambiguity from background clutter can be resolved through bottom-up processes that use intermediate-level prior knowledge about how similar local image measurements (e.g., texture, color, edge orientation, and motion) tend to group (Stringer and Rolls, <xref ref-type="bibr" rid="B57">2000</xref>, <xref ref-type="bibr" rid="B58">2008</xref>), or by learning to use diagnostic fragments (such as an &#x0201c;eye&#x0201d;; cf. Ullman et al., <xref ref-type="bibr" rid="B61">2002</xref>; Hegd&#x000e9; et al., <xref ref-type="bibr" rid="B26">2008a</xref>). However, such bottom-up components for recognition seem to be inadequate to deal with the full range of image variation (Cavanagh, <xref ref-type="bibr" rid="B8">1991</xref>; Yuille and Kersten, <xref ref-type="bibr" rid="B69">2006</xref>). Accurate and versatile recognition seems to require analysis-by-synthesis, in which <italic>object knowledge in memory</italic> is used in a top-down fashion to resolve residual ambiguities regarding which features belong to the object and which do not (Figure <xref ref-type="fig" rid="F1">1B</xref>; also see Yuille and Kersten, <xref ref-type="bibr" rid="B69">2006</xref>; Epshtein et al., <xref ref-type="bibr" rid="B14">2008</xref>).</p><p>The requirement for prior object knowledge for versatile recognition and segmentation, however, brings an important computational problem, referred to as the <italic>bootstrap learning dilemma</italic> (Brady and Kersten, <xref ref-type="bibr" rid="B3">2003</xref>)&#x02014;how does the visual system learn an object in the first place if the object to be learned is itself ambiguous?</p><p>One possible solution is that the visual system takes advantage of opportunities in which bottom-up cues are not ambiguous, such as a view of the object in motion relative to the background, at a different stereoscopic depth, or seen against a different color&#x02014;all conditions of <italic>weak clutter</italic>. In this case, a representation of the whole object could be stored from a single view, or as needed, a representative collection of views, to allow for lighting or viewpoint changes. This is referred to as &#x0201c;opportunistic learning,&#x0201d; because of how it may arise in natural conditions (Figure <xref ref-type="fig" rid="F1">1B</xref>; Brady and Kersten, <xref ref-type="bibr" rid="B3">2003</xref>).</p><p>What if low ambiguity opportunities are scarce, as in the case of camouflage, occlusion, or in the absence of motion parallax, i.e., conditions of <italic>strong clutter</italic>, where there may be insufficient information in a given view to even localize the object? In the absence of opportunities to see an object clearly, learning would require accumulating object knowledge piecemeal over repeated and possibly infrequent exposures. Delays between views result in enormous image variations, as well as exposures to other objects in between views. Thus, learning an object given large variations would require mechanisms capable of holding candidate object features in memory obtained in one exposure for later comparison with features in other views of the same or different objects (for reviews, see Grill-Spector, <xref ref-type="bibr" rid="B21">2003</xref>; Grill-Spector and Malach, <xref ref-type="bibr" rid="B23">2004</xref>; Bussey and Saksida, <xref ref-type="bibr" rid="B7">2007</xref>; Seger and Miller, <xref ref-type="bibr" rid="B54">2010</xref>; Ungerleider and Bell, <xref ref-type="bibr" rid="B62">2011</xref>; Vann and Albasser, <xref ref-type="bibr" rid="B64">2011</xref>). Some features would need to be accepted and some rejected, until an increasingly complete description of the object model (i.e., an internal representation of an object) is assembled. Learning under such conditions of ambiguous fragmentation is referred to as bootstrapped learning (Brady and Kersten, <xref ref-type="bibr" rid="B3">2003</xref>).</p><p>These computational considerations suggest the hypothesis that substrates of object recognition in clutter may involve different sets of brain regions depending on the <italic>type</italic> of learning, i.e., on whether the object was learned in strong or weak clutter. To test this hypothesis, we designed an imaging experiment in which individual novel objects were <italic>learned</italic> under either conditions of strong or weak clutter, but the recognition of both sets of objects was <italic>tested</italic> under conditions of strong clutter.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and methods</title><sec><title>Experimental design</title><p>We designed our experiment with the following considerations:
<list list-type="order"><list-item><p>In a study such as ours that tests object recognition as a function of the type of learning, it is important to isolate the effects of learning from the potentially confounding effects of testing. For this reason, we <italic>tested</italic> all objects under the same task paradigm (i.e., object recognition in strong clutter), regardless of whether the object was <italic>learned</italic> in strong or weak clutter. (The <italic>amount</italic> of clutter, as measured by the average number of objects, remained constant. For clarity, we use the terms &#x0201c;weak clutter&#x0201d; and &#x0201c;strong clutter&#x0201d; to distinguish between weak and strong <italic>effects</italic> of clutter on figure-ground separation).</p></list-item><list-item><p>For stimuli used in training in strong clutter, we used texture mapping to mimic the natural fragmentation effects of lighting and pigment variation on both object boundaries and internal regions. The texture patterns of the target object had the same distribution of local features as the background clutter, thus encouraging the building of object models to solve the segmentation problem. The consequence was that there was insufficient information in any individual image for recognition or segmentation even with scrutiny. Preliminary measurements confirmed that observers were at chance performance prior to training (data not shown). In other words, learning to recognize an object in strong clutter required the visual system to integrate the object model information over multiple exposures to the object.</p></list-item><list-item><p>To reflect natural object properties, we synthesized 3D stimulus forms (&#x0201c;digital embryos&#x0201d;) using virtual morphogenesis to capture realistic part relations, shading, and occlusion (Brady and Kersten, <xref ref-type="bibr" rid="B3">2003</xref>; Hegd&#x000e9; et al., <xref ref-type="bibr" rid="B26">2008a</xref>; see Figure <xref ref-type="fig" rid="F2">2A</xref>).</p></list-item><list-item><p>Because of the potential for broadly different brain functions to be involved in the two types of learning, we measured the activity of the whole brain, and not just the visual areas.</p></list-item></list></p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Stimuli and task. (A)</bold> Some exemplar novel objects (&#x0201c;digital embryos&#x0201d;). <bold>(B)</bold> A weak-clutter stimulus, with the target object highlighted in color so as to minimize the effect of clutter. <bold>(C)</bold> A strong-clutter stimulus, with the target (same as the leftmost object in second row from top in panel <bold>A</bold>) camouflaged against the background. A given subject learned a given object in strong clutter or in weak clutter, but not both. Learned objects were counterbalanced among subjects. <bold>(D)</bold> The recognition task performed by the subjects during fMRI scans. The target object was always presented in the presence of clutter during the scans, regardless of whether a given object was learned with or without clutter during training. Subjects were required to report whether the stimulus contained a learned target or not, regardless of the location of the target. The panels in this figure are not drawn to the same scale.</p></caption><graphic xlink:href="fnhum-06-00170-g0002"/></fig><p>This experimental design was aimed at ruling out stimulus conditions that could otherwise lead to differential cortical responses between objects learned under conditions of strong <italic>vs</italic>. weak clutter. Systematic differences in the images, including average luminance, the level and nature of clutter, color used to highlight the target object in the sample images during training, and subject-to-subject differences in training parameters can be ruled out, since these effects were either averaged out during the experiment and/or factored out during the data analysis (see below for details). Also, the target objects were counterbalanced among subjects, so as to average out the effects of the shape variations across individual objects.</p></sec><sec><title>Subjects</title><p>Eight adult right-handed subjects (four females and four males) participated in this study. The data from one female subject were excluded because of excessive head movement in the scanner. It must be noted that this is a relatively small sample, and is therefore more subject to Type I and II errors (Cohen, <xref ref-type="bibr" rid="B9">1988</xref>; Ellis, <xref ref-type="bibr" rid="B13">2010</xref>). Each subject had normal or corrected-to-normal vision, and had no known neurological or visual disorders. Subjects gave informed consent prior to participating in the study. All protocols used in the study were approved in advance by the Institutional Review Board of the University of Minnesota.</p></sec><sec><title>Stimuli</title><p>Digital embryos were generated by starting with a uniform icosahedron, and simulating three embryonic processes in an iterative fashion: hormone-mediated cell growth, cell division and cell movement (<ext-link ext-link-type="uri" xlink:href="http://www.hegde.us/DigitalEmbryos">www.hegde.us/DigitalEmbryos</ext-link>). Because of random variations in the underlying developmental processes, each run of these processes generates a different shape. Figure <xref ref-type="fig" rid="F2">2A</xref> shows several examples. We generated &#x0003e;5000 different digital embryos from which to select target objects and produce background clutter. Each embryo was textured independently using a randomly chosen two-tone grayscale bitmap of untextured digital embryo scenes as texture maps. This process produces misleading occlusion and shading cues, similar to that employed by some animals (e.g., cuttlefish; Hanlon et al., <xref ref-type="bibr" rid="B24">2007</xref>). Digital embryo scenes were created by compositing 81 different randomly drawn embryos in 9 &#x000d7; 9 jittered rows and columns and at random depths (see Figures <xref ref-type="fig" rid="F2">2B,C</xref>). This constituted the background. All stimulus parameters, including the texture used for texture mapping and the number of background embryos, were chosen so as to yield asymptotic learning within a few hundred trials, as determined in pilot experiments (data not shown).</p><p>For scenes with a target, a new digital embryo was placed in front of the background, so that the target was in &#x0201c;plain view.&#x0201d; The embryos that were used as targets in any image never occurred in the background in any image, and vice versa. The scene was top lit and rendered using the OpenGL graphics toolkit (<ext-link ext-link-type="uri" xlink:href="http://www.opengl.org">www.opengl.org</ext-link>).</p><p>Four types of visual stimuli were generated. (1) For stimuli with strong clutter, the target object was set against a background of other digital embryos, so that the texture patterns of the individual embryos created a camouflage effect (Figure <xref ref-type="fig" rid="F2">2C</xref>). That is, the target object blended with the background, making it difficult to perceptually segment the target from the background. Note that this type of clutter is distinct from the type of clutter used in many previous studies, where the various objects in the image did not blend with each other, so that each object could be readily segmented from each other and from the background (see, e.g., Kourtzi et al., <xref ref-type="bibr" rid="B33">2005</xref>). The position of the target in the frontal plane, and the identity and position of the background embryos, varied randomly from one stimulus to the next. (2) The stimuli with weak clutter were created using an identical procedure, except that the target embryo was highlighted in monochromatic color (green or blue, depending on the subject) so as to be easily seen (Figure <xref ref-type="fig" rid="F2">2C</xref>). (3) The no-target stimuli were similar to the stimuli with strong/weak clutter, except that the target was absent. Note that the generative process for producing the backgrounds for the aforementioned three conditions was identical&#x02014;statistically, the three conditions differed only in terms of the target object. (4) In order to contrast responses to objects learned in strong/weak clutter <italic>vs.</italic> a textured background, we also created a set of scrambled control stimuli by sampling small rectangular image patches from individual stimuli chosen randomly in equal proportions from the first three conditions.</p></sec><sec><title>Training phase</title><p>During this phase of the experiment, subjects learned the target objects off-scanner, using viewing conditions that mimicked those in the scanner as closely as possible. Each subject learned five different objects presented in strong clutter, and five additional objects presented in weak clutter. The learned objects were counterbalanced across subjects. Each subject learned objects in strong or weak clutter in separate, alternating blocks of trials. Within each block, matching <italic>vs.</italic> non-matching trials (see below) occurred with equal probability in a randomly interleaved fashion.</p><p>To learn the embryos, subjects performed a delayed match-to-sample detection task with feedback, in which they reported whether or not the foreground object in the sample stimulus was found in the ensuing test stimulus (not shown). Briefly, each trial lasted 2 s, and began with a 400 ms presentation of the sample stimulus. Each sample stimulus was a visual scene described above and always contained a target object in the foreground. Depending on the trial block, the sample stimulus was a strong- or weak-clutter stimulus.</p><p>The sample stimulus was followed by a 100 ms mask drawn at random from a pool of 100 pattern mask stimuli that were created by scrambling a separate, dedicated set of strong-clutter and weak-clutter stimuli (Grill-Spector et al., <xref ref-type="bibr" rid="B22">2001</xref>; Op de Beeck et al., <xref ref-type="bibr" rid="B45">2007</xref>).</p><p>Following a 100 ms mask, the test stimulus was presented for 400 ms. The test stimulus was always a strong-clutter stimulus, regardless of whether the preceding sample stimulus was a weak-clutter stimulus or a strong-clutter stimulus. In a random 50% of the trials (matching trials), the test stimulus had the same target embryo as the sample stimulus, always at the same size and orientation, but at a randomly varying location. In another 50% of the trials, the test stimulus was a no-target stimulus. In either type of trial, the background of the test stimulus was always different from the background of the sample stimulus, so that the target, if present, was the only object common between the given test stimulus and the preceding sample stimulus.</p><p>Following a 100 ms mask, a blank screen was presented for the remainder of the trial (1000 ms). The subject had to report, using a corresponding button press, whether or not the foreground object in the sample stimulus was present in the test stimulus regardless of the object&#x02019;s position in either stimulus. Eye movements were allowed, so as to mimic natural viewing conditions as closely as possible. An audio feedback was provided.</p><p>The strong-clutter and weak-clutter blocks were repeated until the subject learned the corresponding class of objects to a criterion level (at least 75% correct for all objects in the block for at least four successive blocks). For some subjects, the learning occurred over multiple days. Although individual subjects tended to learn some objects faster than others, the asymptotic level of performance, measured as the percentage of correct responses, was comparable between objects learned in strong <italic>vs.</italic> weak clutter (Table <xref ref-type="table" rid="T1">1</xref>, top). Note that it is possible that there were systematic differences in eye movement and fixation patterns among the various conditions, and that these differences contributed to the differences in learning. Immediately before the scan, each subject performed additional &#x0201c;top-off&#x0201d; training sessions outside the scanner, so as to ensure that the subject&#x02019;s learning remained at asymptotic levels entering the scanning phase of the experiment.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Behavioral data</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"><bold>Strong clutter</bold></th><th align="left" rowspan="1" colspan="1"><bold>Weak clutter</bold></th><th align="left" rowspan="1" colspan="1"><bold>No target</bold></th></tr></thead><tbody><tr><td align="left" colspan="5" rowspan="1"><bold>PRE-SCAN TRAINING</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Asymptotic level performance (% correct &#x000b1; SEM)</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">78 &#x000b1; 2</td><td align="left" rowspan="1" colspan="1">80 &#x000b1; 5</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Number of pre-asymptotic training blocks</td><td align="left" rowspan="1" colspan="1">Median</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Range</td><td align="left" rowspan="1" colspan="1">8&#x02013;14</td><td align="left" rowspan="1" colspan="1">7&#x02013;12</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" colspan="5" rowspan="1"><bold>DURING THE SCAN</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Performance (% correct &#x000b1; SEM)</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">78 &#x000b1; 3</td><td align="left" rowspan="1" colspan="1">77 &#x000b1; 4</td><td align="left" rowspan="1" colspan="1">85 &#x000b1; 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Reaction time (Mean &#x000b1; SEM) (ms)</td><td align="left" rowspan="1" colspan="1">Correct trials</td><td align="left" rowspan="1" colspan="1">826 &#x000b1; 6</td><td align="left" rowspan="1" colspan="1">820 &#x000b1; 5</td><td align="left" rowspan="1" colspan="1">870 &#x000b1; 3</td></tr><tr><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Incorrect trials</td><td align="left" rowspan="1" colspan="1">916 + 11</td><td align="left" rowspan="1" colspan="1">903 + 8</td><td align="left" rowspan="1" colspan="1">948 + 8</td></tr></tbody></table></table-wrap></sec><sec><title>Testing phase: MRI scans</title><p>Subjects were scanned while performing an object detection task under conditions of strong clutter, regardless of whether they had learned the object under conditions of strong or weak clutter.</p><p>During the scan, each trial lasted for 2 s, and began with a 400 ms presentation of a single cluttered visual scene, followed by a 100 ms mask and a subsequent blank gray screen for the last 1500 ms of the trial (Figure <xref ref-type="fig" rid="F2">2D</xref>). Stimuli (9&#x000b0; &#x000d7; 9&#x000b0;) were back-projected via a video projector (refresh rate, 60 Hz) onto a translucent screen placed inside the scanner bore. Subjects viewed the stimuli through a mirror located above their eyes, and used a button press to report whether or not the visual scene contained an object they had learned. No feedback was provided. Eye movements were allowed.</p><p>Functional MRI data were obtained using a 3T Siemens Trio scanner with an eight-channel head array coil using a rapid event-related design. Blood oxygenation level-dependent (BOLD) signals were measured with an EPI sequence (TE: 30 ms, TR: 1000 ms, FOV: 220 &#x000d7; 220 mm<sup>2</sup>, matrix: 64 &#x000d7; 64, flip angle: 60&#x000b0;, slice thickness: 5 mm, inter-slice gap: 1 mm, number of slices: 14, slice orientation: axial). The bottom slice was positioned just below the temporal lobes, so that the slices spanned all of the brain except the crown of the motor cortex at the dorsal end. A high-resolution 3D structural data set (3D MPRAGE; 1 &#x000d7; 1 &#x000d7; 1 mm<sup>3</sup> resolution) was also collected in the same session before the functional runs.</p><p>Each scan consisted of 125 trials, representing four different conditions. (1) Twenty-five trials, referred to as &#x0201c;strong clutter&#x0201d; trials, featured an object learned under strong clutter. The objects were repeated five trials each in a randomly interleaved fashion (5 objects &#x000d7; 5 trials each = 25 trials). (2) The 25 &#x0201c;weak clutter&#x0201d; trials featured an object learned under weak clutter. The strong clutter and weak clutter trials were identical in all respects, including the pattern of background, except for the way the target object had been learned. (3) Two sets of 25 trials each (&#x0201c;no target&#x0201d; trials), one each corresponding to strong clutter and weak clutter conditions, featured a no-target stimulus. This ensured that the stimulus in any given trial had a 50% chance of containing a target. The two sets of no-target trials (defined for <italic>m</italic>-sequence purposes, see below) were otherwise identical. (4) The remaining 25 trials featured scrambled stimulus (&#x0201c;scrambled control&#x0201d; trials), during which subjects were expected to press either button.</p><p>The order of the various trials was determined using an <italic>m-sequence</italic> (Buracas and Boynton, <xref ref-type="bibr" rid="B6">2002</xref>), so that each condition was preceded and followed by each of the other conditions an equal number of times. The <italic>m</italic>-sequence was varied one scan to the next within a subject. Each subject was scanned eight times. The recognition performance did not change systematically over the scan for any subject, indicating that the subjects did not learn (or forget) during the scan (rank correlation analysis, <italic>p</italic> &#x0003e; 0.05 for each subject; not shown).</p></sec><sec><title>Data analyses</title><p>The data were analyzed using the SPM5 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">www.fil.ion.ucl.ac.uk/spm</ext-link>; Friston, <xref ref-type="bibr" rid="B18">2007</xref>) and BrainVoyager (brainvoyager.com) utilities, along with custom-written software. The EPI data were corrected for slice time and head movement, normalized to the MNI/ICBM coordinate space (<ext-link ext-link-type="uri" xlink:href="http://www.loni.ucla.edu/ICBM">www.loni.ucla.edu/ICBM</ext-link>; Evans et al., <xref ref-type="bibr" rid="B17">1993</xref>) and smoothed using a Gaussian kernel with a full width at half maximum of 8 mm.</p><p>Statistical maps were generated in two stages. The first stage was a within-subject analysis, during which statistical maps were generated for each subject individually using 13 different regressors. The effects of interest were modeled using one regressor for each of the four trial conditions (strong clutter, weak clutter, no target, scrambled control). The model also included the following nine effects of no interest (i.e., &#x0201c;nuisance&#x0201d; factors): translations and rotations of the subject&#x02019;s head along the three Cartesian axes (six regressors), the reaction times during the scan, the number of trials needed to reach the asymptotic level during the training phase prior to the scan, and the asymptotic level of learning achieved during the training phase. The last two regressors, which related to the off-scanner learning history rather than in-scanner parameters, were included to factor out the effects, if any, of the corresponding differences in off-scanner learning. In general, including nuisance factors as regressors in the model helps remove systematic, confounding contributions from them. Removing such contributions from the data is desirable, even if the contribution of any given factor may not have been statistically significant by itself (Friston, <xref ref-type="bibr" rid="B18">2007</xref>).</p><p>The second stage of analysis consisted of an across-subject random effects analysis, in which a group level statistical map was generated using the statistical maps from individual subjects.</p><p>Foci of activation that consisted of &#x02265;20 contiguous voxels at a <italic>p</italic> &#x0003c; 0.05 (corrected for multiple comparisons) in the across-subject map were identified as regions of interest (ROIs). A separate analysis (not shown) revealed no clusters of significant activation for the nuisance factors. Activation maps were graphically displayed, and the Brodmann area assignments for the activation foci were identified, using SPM5 and the Caret utility (<ext-link ext-link-type="uri" xlink:href="http://brainmap.wustl.edu/caret">brainmap.wustl.edu/caret</ext-link>; Van Essen et al., <xref ref-type="bibr" rid="B63">2001</xref>). Data from individual ROIs were analyzed using the MarsBaR utility (<ext-link ext-link-type="uri" xlink:href="http://marsbar.sourceforge.net">marsbar.sourceforge.net</ext-link>; Brett et al., <xref ref-type="bibr" rid="B5">2002</xref>) and custom-written software.</p></sec><sec><title>Estimating the bold responses</title><p>BOLD responses used as inputs to various analyses were estimated using one of the following two methods, depending on the requirements of the given analysis.</p><p><italic>Method 1 (Condition-wise method)</italic> estimated the response to each of the four conditions during each individual scan using a general linear model (GLM). To avoid potential selection biases, only half of the scans from each subject was used as input to GLM (Kriegeskorte et al., <xref ref-type="bibr" rid="B34">2009</xref>; also see Baker et al., <xref ref-type="bibr" rid="B1">2007a</xref>,<xref ref-type="bibr" rid="B2">b</xref>; Hegd&#x000e9; et al., <xref ref-type="bibr" rid="B27">2008b</xref>). The other half of the data for each subject was used to redefine the ROI for each subject individually. To do this, we determined significant voxels (at <italic>p</italic> &#x0003c; 10<sup>&#x02212;4</sup>, uncorrected) in each subject that were within a 16 mm diameter spherical volume centered on the peak voxel for the given ROI in the aforementioned second level analysis. The responses of these voxels (but in the other half of the data) were used as input to the GLM. The GLM estimated the response to the given condition across all repetitions (i.e., trials) of the condition during a given scan. The response time courses were estimated using either the canonical hemodynamic response function (HRF) or finite impulse response function (FIR). FIR makes no assumptions about the shape of the time course and essentially models each time point of the scan individually (Friston, <xref ref-type="bibr" rid="B18">2007</xref>).</p><p><italic>Method 2 (Trial-wise method)</italic> was carried out in the same fashion as Method 1, except that the response to each given trial was estimated individually using deconvolution (Ollinger et al., <xref ref-type="bibr" rid="B43">2001a</xref>,<xref ref-type="bibr" rid="B44">b</xref>; Serences, <xref ref-type="bibr" rid="B56">2004</xref>).</p></sec><sec><title>Logistic regression</title><p>Two binary logistic models were constructed, one for objects learned in strong clutter and the other for objects learned in weak clutter. The aim was to determine the extent to which the BOLD responses of given region could reliably signal the presence of a learned target. The BOLD responses used as inputs to the model were estimated using the Condition-wise method (Method 1) described above.</p><p>The model for objects learned in strong clutter compared the responses during the strong clutter trials with the responses during the corresponding no-target trials (i.e., no-target conditions designated as paired with strong-clutter trials in the <italic>m</italic>-sequence; see above) across all subjects. The probability that a given trial <italic>j</italic> featured stimulus <italic>S</italic> = <italic>g</italic> given the BOLD response <italic>X</italic><sub><italic>i</italic></sub> from the brain region <italic>i</italic> is specified by the model:
<disp-formula id="E1"><mml:math id="M1"><mml:mrow><mml:mi mathsize="11pt" mathcolor="black">P</mml:mi><mml:mrow><mml:mo mathsize="11pt" mathcolor="black">(</mml:mo><mml:mrow><mml:mi mathsize="11pt" mathcolor="black">S</mml:mi><mml:mo mathsize="11pt" mathcolor="black">=</mml:mo><mml:mi mathsize="11pt" mathcolor="black">g</mml:mi><mml:mo mathsize="11pt" mathcolor="black">|</mml:mo><mml:msub><mml:mi mathsize="11pt" mathcolor="black">X</mml:mi><mml:mrow><mml:mi mathsize="11pt" mathcolor="black">i</mml:mi><mml:mi mathsize="11pt" mathcolor="black">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="11pt" mathcolor="black">)</mml:mo></mml:mrow><mml:mo mathsize="11pt" mathcolor="black">=</mml:mo><mml:mn mathsize="11pt" mathcolor="black">1</mml:mn><mml:mo mathsize="11pt" mathcolor="black">/</mml:mo><mml:mrow><mml:mo mathsize="11pt" mathcolor="black">{</mml:mo><mml:mrow><mml:mn mathsize="11pt" mathcolor="black">1</mml:mn><mml:mo mathsize="11pt" mathcolor="black">+</mml:mo><mml:mstyle class="text" mathsize="11pt" mathcolor="black"><mml:mtext>exp</mml:mtext></mml:mstyle><mml:mrow><mml:mo mathsize="11pt" mathcolor="black">[</mml:mo><mml:mrow><mml:mo mathsize="11pt" mathcolor="black">&#x02212;</mml:mo><mml:mrow><mml:mo mathsize="11pt" mathcolor="black">(</mml:mo><mml:mrow><mml:mi mathsize="11pt" mathcolor="black">&#x003b1;</mml:mi><mml:mo mathsize="11pt" mathcolor="black">+</mml:mo><mml:mstyle displaystyle="true"><mml:mo mathsize="11pt" mathcolor="black">&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="11pt" mathcolor="black">&#x003b2;</mml:mi><mml:mi mathsize="11pt" mathcolor="black">i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi mathsize="11pt" mathcolor="black">X</mml:mi><mml:mrow><mml:mi mathsize="11pt" mathcolor="black">i</mml:mi><mml:mi mathsize="11pt" mathcolor="black">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="11pt" mathcolor="black">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="11pt" mathcolor="black">]</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="11pt" mathcolor="black">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>g</italic> &#x02208; {strong clutter, no target}, &#x003b1; is the offset and &#x003b2;<sub><italic>i</italic></sub> is the regression coefficient for brain region <italic>i</italic>. The modeling was implemented using the <italic>Design</italic> library (Harrell, <xref ref-type="bibr" rid="B25">2001</xref>) in the R software package (<ext-link ext-link-type="uri" xlink:href="http://www.r-project.org">www.r-project.org</ext-link>) and custom-written R software. The full model was then refined to retain only those regressors (i.e., regions) that contributed significantly to the model fit at <italic>p</italic> &#x0003c; 0.05 (Hosmer and Lemeshow, <xref ref-type="bibr" rid="B28">2000</xref>; Harrell, <xref ref-type="bibr" rid="B25">2001</xref>; Kleinbaum et al., <xref ref-type="bibr" rid="B31">2002</xref>).</p><p>The model for objects learned in weak clutter was similarly constructed, using the data from weak clutter conditions and the corresponding paired no-target conditions.</p></sec><sec><title>ROC analysis</title><p>Receiver operating characteristic (ROC) analysis was carried out separately for strong clutter <italic>vs.</italic> weak clutter conditions and for each subject essentially as described by Murray et al. (<xref ref-type="bibr" rid="B40">2002</xref>). The goal of this analysis was to determine the extent to which the BOLD responses in a given region were diagnostic of the subject&#x02019;s response during either clutter condition. For analyses of this sort where the diagnosticity of a given marker (or classifier) is of greater interest than the nature of the marker itself, ROC analysis is a principled, although by no means the only available, choice (Reddy et al., <xref ref-type="bibr" rid="B50">2006</xref>; Zhang et al., <xref ref-type="bibr" rid="B71">2008</xref>; Diana et al., <xref ref-type="bibr" rid="B11">2010</xref>; Sela et al., <xref ref-type="bibr" rid="B55">2011</xref>; Wee et al., <xref ref-type="bibr" rid="B66">2012</xref>).</p><p>To determine if the BOLD responses during strong clutter trials reflect the subject&#x02019;s behavioral response, the FIR estimates of BOLD responses during individual trials (Method 2 above) for the strong clutter condition were pooled across the relevant scans within each subject. The BOLD response during each given strong clutter trial was classified as a hit or false alarm using a simple decision rule. The rule tested whether BOLD responses within a 4&#x02013;10 s time window (where 0 s is the trial onset) were all above a given criterion level. If the decision rule correctly reflected the subject&#x02019;s behavioral response during that trial, the trial was considered a hit. If the rule did not, the trial was considered a false alarm. The ROC curve was determined using 100 criterion values spaced uniformly within the range of observed BOLD values. The area under the ROC curve, AUC, was determined using numerical integration. The <italic>p</italic>-value of AUC was determined using 1000 rounds of randomization. The ROC analysis was carried out similarly for the weak clutter condition.</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><p>Our experiment essentially consisted of training the subjects off-scanner under conditions of strong or weak clutter depending on the object, and scanning the BOLD responses to all objects presented under the same condition, i.e., of strong clutter (Figure <xref ref-type="fig" rid="F2">2D</xref>; Materials and Methods for details). Thus, the clutter designations (i.e., strong clutter or weak clutter) refer to the level of clutter in which a given object was seen during the <italic>training</italic> phase, and not to the level of clutter in which it was seen during the <italic>testing</italic> phase.</p><sec><title>Behavioral performance during training and testing</title><p>As expected, without learning, object recognition performance was at chance levels (not shown). After learning, the asymptotic level of learning varied significantly across subjects (Two-Way ANOVA, object type &#x000d7; subject; <italic>p</italic> &#x0003c; 0.05; not shown). However, the asymptotic level of learning for objects learned in strong <italic>vs.</italic> weak clutter was statistically indistinguishable from each other within each subject (Table <xref ref-type="table" rid="T1">1</xref>, top; binomial proportions tests, <italic>p</italic> &#x0003e; 0.05). The number of blocks needed to reach asymptotic level of learning was also statistically indistinguishable between the two conditions (<italic>p</italic> &#x0003e; 0.05).</p><p>During the testing phase, the recognition performance for the objects learned in strong <italic>vs.</italic> weak clutter (Table <xref ref-type="table" rid="T1">1</xref>, bottom) was statistically indistinguishable from each other and from the asymptotic performance during training (Mantel&#x02013;Haenszel test of independence, <italic>p</italic> &#x0003e; 0.05). The reaction times did not differ significantly between objects learned in strong <italic>vs.</italic> weak clutter (Two-Way ANOVA, learning type &#x000d7; reaction time; <italic>p</italic> &#x0003e; 0.05 for learning type and interaction factors), indicating that any systematic eye movement differences between the conditions were unlikely. However, the reaction times were faster for correct responses than for incorrect responses (<italic>p</italic> &#x0003c; 0.05 for reaction time factor).</p></sec><sec><title>Brain regions differentially active during the detection of objects learned in strong vs. weak clutter</title><p>Our analyses revealed brain regions that responded differentially to objects learned in strong <italic>vs.</italic> weak clutter across all subjects regardless of the trial outcome (&#x0201c;Materials and Methods&#x0201d;). Since all objects were seen under the same conditions of clutter during the scanning itself, these differential responses are attributable to how the objects were learned.</p><p>Twelve regions in either hemisphere showed significant activation differences. Of these, five regions were more responsive to objects learned in strong clutter than to objects learned in weak clutter, and seven regions were more responsive to objects learned in weak clutter than to objects learned in strong clutter (<italic>p</italic> &#x0003c; 0.05, corrected; Figure <xref ref-type="fig" rid="F3">3</xref> and Table <xref ref-type="table" rid="T2">2</xref>).</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Brain regions differentially responsive to visual objects learned with <italic>vs.</italic> without clutter.</bold> Activations are shown superimposed on axial slices of a standard individual brain template. <bold>(A)</bold> Regions more responsive to objects learned in strong clutter than to objects learned in weak clutter. <bold>(B)</bold> Regions more responsive to objects learned in weak clutter than to objects learned in strong clutter. BOLD responses to both classes of objects were measured in the presence of visual clutter. Abbreviations: Ant/post. Insula, anterior/posterior insular cortex; CN, caudate nucleus; DMPFC, dorsomedial prefrontal cortex; GPi, internal segment of globus pallidus; FG, fusiform gyrus; Med. Occipital, medial occipital; PHC, parahippocampal cortex; ST, superior temporal.</p></caption><graphic xlink:href="fnhum-06-00170-g0003"/></fig><table-wrap id="T2" position="float"><label>Table 2</label><caption><p><bold>Brain regions involved in object recognition in clutter</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="9" rowspan="1"><bold>Table 2A|Learned in strong clutter &#x0003e; Learned in weak clutter (<italic>p</italic> &#x0003c; 0.05, corrected)</bold>.</th></tr><tr><th align="left" rowspan="1" colspan="1"><bold>ROI<xref ref-type="table-fn" rid="TN1"><sup>&#x000b6;</sup></xref></bold></th><th align="left" rowspan="1" colspan="1"><bold>Hemi-sphere</bold></th><th align="left" rowspan="1" colspan="1"><bold>Brod-mann area</bold></th><th align="left" rowspan="1" colspan="1"><bold><italic>x<xref ref-type="table-fn" rid="TN2"><sup>&#x02020;</sup></xref></italic></bold></th><th align="left" rowspan="1" colspan="1"><bold><italic>y<xref ref-type="table-fn" rid="TN2"><sup>&#x02020;</sup></xref></italic></bold></th><th align="left" rowspan="1" colspan="1"><bold><italic>z<xref ref-type="table-fn" rid="TN2"><sup>&#x02020;</sup></xref></italic></bold></th><th align="center" colspan="3" rowspan="1"><bold>Correlation with performance<xref ref-type="table-fn" rid="TN4"><sup>&#x000a7;</sup></xref></bold></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"><bold>Strong clutter</bold></th><th align="left" rowspan="1" colspan="1"><bold>Weak clutter</bold></th><th align="left" rowspan="1" colspan="1"><bold>No target</bold></th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">9</td></tr><tr><td align="left" rowspan="1" colspan="1">Dorsal Precuneus</td><td align="left" rowspan="1" colspan="1">L&#x00026;R<xref ref-type="table-fn" rid="TN3"><sup>&#x02021;</sup></xref></td><td align="left" rowspan="1" colspan="1">BA7/31</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">&#x02212;75</td><td align="left" rowspan="1" colspan="1">38</td><td align="left" rowspan="1" colspan="1">0.10</td><td align="left" rowspan="1" colspan="1">0.23<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">&#x02212;0.38<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Medial Occipital</td><td align="left" rowspan="1" colspan="1">L&#x00026;R<xref ref-type="table-fn" rid="TN3"><sup>&#x02021;</sup></xref></td><td align="left" rowspan="1" colspan="1">BA17/18/19</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">&#x02212;92</td><td align="left" rowspan="1" colspan="1">28</td><td align="left" rowspan="1" colspan="1">&#x02212;0.23<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">&#x02212;0.50<xref ref-type="table-fn" rid="TN6"><sup>**</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Caudate Nucleus (CN)/Globus</td><td align="left" rowspan="1" colspan="1">L&#x00026;R<xref ref-type="table-fn" rid="TN3"><sup>&#x02021;</sup></xref></td><td align="left" rowspan="1" colspan="1">(unassigned)</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">0.00</td><td align="left" rowspan="1" colspan="1">&#x02212;0.40<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">&#x02212;0.08</td></tr><tr><td align="left" rowspan="1" colspan="1">Pallidus Internal Segment (GPi)</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Fusiform Gyrus (FG)</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">BA37</td><td align="left" rowspan="1" colspan="1">&#x02212;54</td><td align="left" rowspan="1" colspan="1">&#x02212;51</td><td align="left" rowspan="1" colspan="1">&#x02212;21</td><td align="left" rowspan="1" colspan="1">0.48<xref ref-type="table-fn" rid="TN6"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">0.15</td><td align="left" rowspan="1" colspan="1">&#x02212;0.20</td></tr><tr><td align="left" rowspan="1" colspan="1">Fusiform Gyrus (FG)</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">BA37</td><td align="left" rowspan="1" colspan="1">57</td><td align="left" rowspan="1" colspan="1">&#x02212;57</td><td align="left" rowspan="1" colspan="1">&#x02212;18</td><td align="left" rowspan="1" colspan="1">0.53<xref ref-type="table-fn" rid="TN6"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">0.04</td></tr></tbody><tbody><tr><td align="left" colspan="9" rowspan="1"><bold>Table 2B|Learned in weak clutter &#x0003e; Learned in strong clutter (<italic>p</italic> &#x0003c; 0.05, corrected)</bold>.</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>ROI<xref ref-type="table-fn" rid="TN1"><sup>&#x000b6;</sup></xref></bold></td><td align="left" rowspan="1" colspan="1"><bold>Hemi-sphere</bold></td><td align="left" rowspan="1" colspan="1"><bold>Brod-mann area</bold></td><td align="left" rowspan="1" colspan="1"><bold><italic>x<xref ref-type="table-fn" rid="TN2"><sup>&#x02020;</sup></xref></italic></bold></td><td align="left" rowspan="1" colspan="1"><bold><italic>y<xref ref-type="table-fn" rid="TN2"><sup>&#x02020;</sup></xref></italic></bold></td><td align="left" rowspan="1" colspan="1"><bold><italic>z<xref ref-type="table-fn" rid="TN2"><sup>&#x02020;</sup></xref></italic></bold></td><td align="center" colspan="3" rowspan="1"><bold>Correlation with performance<xref ref-type="table-fn" rid="TN4"><sup>&#x000a7;</sup></xref></bold></td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"><bold>Strong clutter</bold></td><td align="left" rowspan="1" colspan="1"><bold>Weak clutter</bold></td><td align="left" rowspan="1" colspan="1"><bold>No target</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">9</td></tr><tr><td align="left" rowspan="1" colspan="1">Dorsomedial Prefrontal (DMPFC)</td><td align="left" rowspan="1" colspan="1">L&#x00026;R<xref ref-type="table-fn" rid="TN3"><sup>&#x02021;</sup></xref></td><td align="left" rowspan="1" colspan="1">BA32</td><td align="left" rowspan="1" colspan="1">&#x02212;15</td><td align="left" rowspan="1" colspan="1">45</td><td align="left" rowspan="1" colspan="1">33</td><td align="left" rowspan="1" colspan="1">0.18</td><td align="left" rowspan="1" colspan="1">&#x02212;0.01</td><td align="left" rowspan="1" colspan="1">0.50<xref ref-type="table-fn" rid="TN6"><sup>**</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Parahippocampal Cortex (PHC)</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">BA20/BA37</td><td align="left" rowspan="1" colspan="1">&#x02212;30</td><td align="left" rowspan="1" colspan="1">&#x02212;42</td><td align="left" rowspan="1" colspan="1">&#x02212;12</td><td align="left" rowspan="1" colspan="1">&#x02212;0.03</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">&#x02212;0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">Anterior Insula</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">(unassigned)</td><td align="left" rowspan="1" colspan="1">&#x02212;39</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">&#x02212;0.31<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">0.25<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">&#x02212;0.28<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Parahippocampal Cortex (PHC)</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">BA20/BA37</td><td align="left" rowspan="1" colspan="1">27</td><td align="left" rowspan="1" colspan="1">&#x02212;39</td><td align="left" rowspan="1" colspan="1">&#x02212;15</td><td align="left" rowspan="1" colspan="1">&#x02212;0.12</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">0.24<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Precentral</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">BA4/6</td><td align="left" rowspan="1" colspan="1">39</td><td align="left" rowspan="1" colspan="1">&#x02212;3</td><td align="left" rowspan="1" colspan="1">42</td><td align="left" rowspan="1" colspan="1">&#x02212;0.08</td><td align="left" rowspan="1" colspan="1">&#x02212;0.28<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">&#x02212;0.26<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Superior Temporal (ST)</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">BA39/BA22</td><td align="left" rowspan="1" colspan="1">48</td><td align="left" rowspan="1" colspan="1">&#x02212;33</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">&#x02212;0.48<xref ref-type="table-fn" rid="TN6"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">&#x02212;0.31<xref ref-type="table-fn" rid="TN5"><sup>*</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Posterior Insula</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">BA42</td><td align="left" rowspan="1" colspan="1">48</td><td align="left" rowspan="1" colspan="1">&#x02212;48</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">0.11</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">&#x02212;0.03</td></tr></tbody></table><table-wrap-foot><fn id="TN1"><label>&#x000b6;</label><p>ROIs highlighted in green, red, or yellow denoted brain regions whose activity correlated with the recognition of objects learned in strong clutter, objects learned in weak clutter, or both types of objects, as determined by logistic regression at the level of stimulus types. The BOLD responses used in the models were estimated using the Condition-wise method (Method 1; Materials and Methods). This analysis was separate from the behavioral level analyses shown in columns 7&#x02013;9. See text for details.</p></fn><fn id="TN2"><label>&#x02020;</label><p>The stereotactic coordinates represent the center of mass of the given ROI in the MNI space.</p></fn><fn id="TN3"><label>&#x02021;</label><p>ROIs that were contiguous because they were on the medial wall of the hemispheres were treated as a single ROI, i.e., the voxels were not partitioned into two hemispheres.</p></fn><fn id="TN4"><label>&#x000a7;</label><p>The numbers represent the correlation coefficient r between the estimated BOLD response of a given ROI to a given condition during each scan and the corresponding behavioral performance of the subject (measured as % correct trials). The BOLD responses during a given condition were estimated using the Condition-wise method (Method 1; Materials and Methods).</p></fn><fn id="TN5"><label>*</label><p>p &#x0003c; 0.05 (df = 54);</p></fn><fn id="TN6"><label>**</label><p>p &#x0003c; 0.001 (df = 54).</p></fn></table-wrap-foot></table-wrap><p>Regions significantly more responsive to objects learned in strong clutter than to objects learned in weak clutter were found in the fusiform cortex, dorsal precuneus, medial occipital cortex, and basal ganglia [especially the posterior aspect of the head of the caudate nucleus (CN), and the medial aspect of the internal segment of globus pallidus (GPi)] (Figure <xref ref-type="fig" rid="F3">3A</xref>; also see Table <xref ref-type="table" rid="T2">2A</xref>). Regions with the opposite response pattern (i.e., weak clutter &#x0003e; strong clutter) were found in the bilateral parahippocampal cortex (PHC), bilateral dorsomedial frontal cortex (DMPFC), right precentral, right superior temporal (ST), and insular cortices (Figure <xref ref-type="fig" rid="F3">3B</xref> and Table <xref ref-type="table" rid="T2">2B</xref>). CN, GPi, and the fusiform gyrus (FG) are part of a memory subsystem called the corticostriatal loop. PHC is part of another memory subsystem based in the medial temporal lobe (MTL). Both of these memory subsystems are known to be involved in object category learning and object recognition (Poldrack et al., <xref ref-type="bibr" rid="B47">2001</xref>; Seger, <xref ref-type="bibr" rid="B53">2006</xref>; Murray et al., <xref ref-type="bibr" rid="B40a">2007</xref>; Poldrack and Foerde, <xref ref-type="bibr" rid="B48">2008</xref>; Baxter, <xref ref-type="bibr" rid="B7a">2009</xref>; Suzuki and Baxter, <xref ref-type="bibr" rid="B59a">2009</xref>; Seger and Miller, <xref ref-type="bibr" rid="B54">2010</xref>). We will discuss later the plausible roles of these subsystems in recognizing objects learned in strong <italic>vs.</italic> weak clutter.</p><p>Note that ROIs identified by either of the above contrasts showed some hemispheric asymmetry. A previous study of learning in clutter did not find any differences between the two hemispheres [(Kourtzi et al., <xref ref-type="bibr" rid="B33">2005</xref>), p. 1324]. Additional analyses in our case indicated that even ROIs in seemingly symmetrical locations in the two hemispheres sometimes had substantially different response patterns (e.g., the right and left FG; also see below). The only exception to this were the four ROIs located on or near the hemispheric midline (DMPFC, dorsal precuneus, medial occipital cortex, and CN/GPi; Figure <xref ref-type="fig" rid="F3">3</xref> and Table <xref ref-type="table" rid="T2">2</xref>), which failed to show significant variations across the two hemispheres and therefore were not split according to hemisphere.</p><p>The differential activation in the retinotopic medial occipital cortex is somewhat surprising, since our paradigm allowed free eye movements. The time course of the BOLD response in this ROI indicates that the visual stimulation reduced responses in this ROI below background levels, such that at their most suppressed, the responses showed a no target &#x0003c; strong clutter &#x0003c; weak clutter &#x0003c; scrambled control pattern (see Figure <xref ref-type="fig" rid="FA1">A1</xref>). Response suppression in the occipito-temporal visual areas is believed to reflect the fine-tuning (i.e., sharpening) of the responses induced by prior object knowledge although there is some debate about the precise mechanisms (Murray et al., <xref ref-type="bibr" rid="B40">2002</xref>; Ganel et al., <xref ref-type="bibr" rid="B19">2006</xref>; Schacter et al., <xref ref-type="bibr" rid="B52">2007</xref>; Yotsumoto et al., <xref ref-type="bibr" rid="B68">2008</xref>).</p><p>Involvement of retinotopic visual areas and of fusiform cortex in learning objects in visual clutter has been reported previously (Kourtzi et al., <xref ref-type="bibr" rid="B33">2005</xref>; Zhang and Kourtzi, <xref ref-type="bibr" rid="B70">2010</xref>). The anatomical location of the PHC activation in this study roughly corresponded to that of the parahippocampal place area that has been previously reported to play an important role in scene perception, although not in the context of visual clutter (Epstein and Kanwisher, <xref ref-type="bibr" rid="B16">1998</xref>; Epstein et al., <xref ref-type="bibr" rid="B15">1999</xref>; also see Grill-Spector and Malach, <xref ref-type="bibr" rid="B23">2004</xref>). None of the remaining regions identified in the present study have been previously reported to play a role in object recognition in clutter (see below).</p></sec><sec><title>Role of the ROIs in recognizing objects learned in strong vs. weak clutter</title><p>The fact that a given brain region is more responsive to objects learned in strong clutter does not, by itself, mean that it plays no role in representing objects learned in weak clutter (or vice versa). For instance, it is possible that the two classes of objects are represented in a distributed fashion, with one or more regions contributing significantly to recognizing both classes of objects in clutter.</p><p>We examined this possibility using logistic regression modeling to compare the BOLD responses to strong/weak clutter conditions with the responses to the no target conditions (&#x0201c;Materials and Methods&#x0201d;). For the strong clutter <italic>vs</italic>. no target comparison, the slope of the fitted logistic function is a measure of the extent to which a given region is more responsive to a stimulus that contained an object learned in strong clutter than to a comparable stimulus that contained no target (Hosmer and Lemeshow, <xref ref-type="bibr" rid="B28">2000</xref>; Harrell, <xref ref-type="bibr" rid="B25">2001</xref>; Kleinbaum et al., <xref ref-type="bibr" rid="B31">2002</xref>). By this measure, DMPFC, right PHC, CN/GPi, and left FG reliably signaled the presence in a cluttered background of an object learned in strong clutter (green and yellow rows in Table <xref ref-type="table" rid="T2">2</xref>; see footnotes for additional details). Similarly, DMPFC, left and right PHC, and STS reliably signaled the presence of an object learned in weak clutter (red and yellow rows). Two regions, DMPFC and the right PHC, signaled the presence of a target object in cluttered background regardless of whether the object was learned in strong or weak clutter (yellow rows). Taken together, these results indicate that the two sets of objects are represented in two partially overlapping, distributed sets of brain regions.</p><p>To determine the extent to which the different brain regions reflect the subject&#x02019;s behavioral responses, we correlated the BOLD response of a given ROI to the strong clutter, weak clutter and no target conditions with the subject&#x02019;s overall behavioral performance during the given condition. The resulting correlation coefficient values are shown in Table <xref ref-type="table" rid="T2">2</xref> (columns 7&#x02013;9; see footnotes for additional details). The time courses of the BOLD responses for the six ROIs in Table <xref ref-type="table" rid="T2">2</xref> with significant correlations (i.e., ROIs highlighted in color) are shown in Figure <xref ref-type="fig" rid="FA2">A2</xref>.</p><p>Three aspects of these two analyses are particularly noteworthy. First, many ROIs (e.g., ST, FG) showed significant correlation with performance either for objects learned in strong clutter or in weak clutter, but not for both, indicating that these ROIs process the two types of stimuli in a qualitatively different manner. This is particularly important, because it indicates that the responses are not entirely attributable to the internal object model (or representation) being more detailed or better segmented for the two types of objects because in that case, the responses would be expected to vary systematically between the two conditions. Second, note that the BOLD response was anticorrelated with performance for many regions (e.g., anterior insula, precentral, medial occipital), indicating that the BOLD response of these regions to a given condition decreased with an increase in performance. Third, although the response to the no-target condition played no part in ROI selection in either analysis (and the subjects did not explicitly learn no-target stimuli during training; see &#x0201c;Materials and Methods&#x0201d;), the response to no-target stimuli showed significant correlation with performance for many ROIs. This suggests that these ROIs carry information about the presence or absence of a target in a given cluttered scene. Together, the above results suggest that objects seen in clutter are processed by the visual system in a distributed fashion.</p></sec><sec><title>Responses of left FG is diagnostic of correct detection of objects learned in strong clutter on a trial-to-trial basis</title><p>The above analyses reveal many regions for which the average magnitude of the BOLD response during a given condition was correlated with the average behavioral performance of the subject on a scan-to-scan basis. We further examined all regions, regardless of whether they showed significant correlation in the above scan-level analysis, individually using an ROC analysis to determine the accuracy with which the BOLD response estimated the subject&#x02019;s response on a trial-to-trial basis (&#x0201c;Materials and Methods&#x0201d;). By this analysis, the responses in one ROI, left FG, reflected a given subject&#x02019;s behavioral response during individual trials featuring an object learned in strong clutter (see below).</p><p>Figure <xref ref-type="fig" rid="F4">4</xref> shows the results of this analysis for one representative subject. The extent to which the ROC curve (blue line) deviated from the chance level performance (diagonal) is a measure of the accuracy with which the BOLD response successfully estimated the subject&#x02019;s response. Greater overall deviations, i.e., the extent to which the area under the ROC curve (AUC) deviates from 0.5, signify correspondingly greater accuracy. The AUC values were statistically significant in subjects 1&#x02013;6 (AUC range, 0.57&#x02013;0.63; <italic>p</italic> &#x0003c; 0.05). In the remaining subject (Subject 7), the <italic>p</italic>-value was 0.058 (AUC, 0.5812; not shown). Together, these results indicate that, in general, the BOLD response during a given trial in left FG reliably estimates the subject&#x02019;s behavioral response for that trial.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Responses in left FG estimate the subject&#x02019;s response on a trial-by-trial basis.</bold> ROC analyses of trial-by-trial responses were carried out using the BOLD responses to individual trials estimated using FIR modeling in Method 2 (see &#x0201c;Materials and Methods&#x0201d; for details). This figure shows the ROC curve (<italic>blue</italic>), the area under the ROC curve (AUC) and the <italic>p</italic>-value of AUC for one subject. The <italic>diagonal</italic> represents chance level performance (AUC = 0.5). The data from the remaining subjects are reported in the text. The average time course of the responses in FG across all sessions and subjects are shown in Figure <xref ref-type="fig" rid="FA2">A2</xref>.</p></caption><graphic xlink:href="fnhum-06-00170-g0004"/></fig><p>However, the responses in left FG failed to estimate the subject&#x02019;s response during the weak clutter condition, consistently across all subjects. Furthermore, no other ROI consistently estimated the behavioral response of the subjects during strong-clutter or weak clutter conditions on a trial-to-trial basis by this measure. This may mean that the processing of the objects learned in weak clutter is even more distributed than the processing of objects learned in strong clutter.</p></sec><sec><title>Additional evidence for overlapping streams of processing: brain regions responsive to both types of object</title><p>The ROIs in Figure <xref ref-type="fig" rid="F3">3</xref> were specifically identified by their differential responses to strong- <italic>vs.</italic> weak-clutter conditions. However, it is also useful to identify ROIs that distinguish both conditions from the scrambled control regardless of whether they distinguish between the strong- <italic>vs.</italic> weak-clutter condition. To do this, we identified those regions in which the responses to both strong clutter and weak clutter conditions were greater than to the scrambled control condition using a conjunction analysis (strong clutter &#x0003e; scrambled control and weak clutter &#x0003e; scrambled control; Figure <xref ref-type="fig" rid="F5">5</xref> (Joseph et al., <xref ref-type="bibr" rid="B29">2002</xref>; Friston, <xref ref-type="bibr" rid="B18">2007</xref>). We used the scrambled control condition, rather than the no target condition, as the reference condition because our aim was to identify regions that play a role in the processing of cluttered scenes, and not just the target object.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Brain regions that are more responsive to objects in cluttered scenes than to their scrambled counterparts.</bold> Two separate statistical maps were generated, one using the strong clutter condition &#x0003e; scrambled control condition, and the other using the weak clutter condition &#x0003e; scrambled control condition, each at <italic>p</italic> &#x0003c; 0.05 (corrected). This figure shows the regions that were significantly activated in both statistical maps by a conjunction analysis (<italic>p</italic> &#x0003c; 0.05). Activations are shown superimposed on axial slices of a standard individual brain template. Abbreviations: IOG, inferior occipital gyrus; IPS, intraparietal sulcus; LOC, lateral occipital complex; VLPFC, ventrolateral prefrontal cortex.</p></caption><graphic xlink:href="fnhum-06-00170-g0005"/></fig><p>The regions revealed by this analysis were largely symmetrical between the two hemispheres, although significant activation of lateral occipital complex (LOC) and inferior occipital gyrus (IOG) was evident only in the right hemisphere. In the left hemisphere, the activation of these regions was not extensive enough to be identified as an ROI by our criteria (data not shown). Both LOC and IOG, especially in the right hemisphere, are known to play key roles in object recognition (Grill-Spector and Malach, <xref ref-type="bibr" rid="B23">2004</xref>). Two other regions in this map, anterior insula and dorsomedial cingulate, have been previously reported to play important roles in executive control (Stuss and Knight, <xref ref-type="bibr" rid="B59">2002</xref>; Koechlin and Hyafil, <xref ref-type="bibr" rid="B32">2007</xref>). Thalamus is known to play a generic role in many forms of perceptual learning (Seger, <xref ref-type="bibr" rid="B53">2006</xref>).</p></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><sec><title>Functional specialization in the substrates for object recognition in clutter</title><p>Our results reveal that recognition of visual objects in clutter involves distinct, albeit overlapping, sets of brain regions. Moreover, the differences in the two sets of regions were not attributable to differences in object category, since these objects were randomly drawn from the same object category and counter-rotated across subjects. Rather, the differences were a function of whether or not the objects were learned in clutter. Thus, our results confirm the hypothesis that the differences in the underlying substrates can arise as a function of the <italic>type</italic> of learning.</p><p>These findings are consistent with the computational notion that learning an object in strong <italic>vs</italic>. weak clutter results in different processing mechanisms possibly involving different learned object representations. As noted above, object features that are useful for detecting an object differ when the object is learned in strong clutter <italic>vs.</italic> weak clutter. Modeling studies show that neural networks learn different properties of the image when the input images contain background clutter <italic>vs</italic>. when they do not (Stringer and Rolls, <xref ref-type="bibr" rid="B57">2000</xref>; Rolls et al., <xref ref-type="bibr" rid="B51">2008</xref>). Psychophysical studies using dot patterns have shown that training on shapes in isolation did not later facilitate segmentation of the same shapes in noisy backgrounds (Yi et al., <xref ref-type="bibr" rid="B67a">2006</xref>). Thus, it is plausible that two different corresponding sets of &#x0201c;task-relevant&#x0201d; features come to be represented, and the brain regions that help represent these features will be correspondingly different. The differential activations revealed by our study may reflect these differences in how the two sets of features come to be represented after training in strong <italic>vs.</italic> weak clutter.</p><p>Our results identify left FG as a brain region that plays a key role in the recognition of objects learned in strong clutter, in that activity of this one region by itself is a reasonably good indicator of the outcome of the recognition process on a trial-to-trial basis. On the other hand, our results also indicate that the mechanisms for recognizing objects learned in weak clutter may be more distributed. Previous monkey neurophysiological studies have reported neurons in the inferotemporal cortex that show considerable tolerance to clutter (Zoccolan et al., <xref ref-type="bibr" rid="B72">2007</xref>; Li et al., <xref ref-type="bibr" rid="B36">2009</xref>; also see Levi, <xref ref-type="bibr" rid="B35">2008</xref>; Nandy and Tjan, <xref ref-type="bibr" rid="B41">2008</xref>), although the extent to which type of learning, or learning <italic>per se</italic>, affects the responses of these neurons remains unclear.</p><p>Three caveats about our results are especially worth noting. First, while our results reveal many brain regions that show differential learning type-dependent effects, it is possible that our study missed additional brain regions that play key roles in this process, because of the relatively low spatial resolution of fMRI and/or the relatively small size of our sample. Second, while our results generally designate each given brain region as being selective to one condition or the other (<italic>e.g.</italic>, left FG as selective to strong clutter condition), it is possible that selectivity for both conditions exist within the same given region at the level of individual neurons or neuronal subpopulations. Techniques with far greater spatial resolution, such as monkey electrophysiological recordings, would be needed to explore this scenario. Third, the extent to which our findings are generalizable to other learning situations, especially under natural viewing conditions, remains to be seen. Thus, the significance of our results is that they provide an &#x0201c;existence-proof&#x0201d; for a learning type-dependent object recognition process, and not necessarily that it is a common mechanism of object learning and recognition.</p></sec><sec><title>A hypothetical mechanism by which functional specialization may arise in a learning type-dependent fashion</title><p>Our experiments were not designed to address the mechanisms by which brain regions come to be differentially responsive to the objects learned in strong <italic>vs</italic>. weak clutter. But it is worth pointing out the possibility that the differential responses may arise as a result of the interplay between two known memory subsystems, the MTL and the corticostriatal loop.</p><p>Both of these subsystems are known to play prominent roles in perceptual learning of visual objects (for reviews, see Seger, <xref ref-type="bibr" rid="B53">2006</xref>; also see Poldrack et al., <xref ref-type="bibr" rid="B47">2001</xref>; Poldrack and Foerde, <xref ref-type="bibr" rid="B48">2008</xref>; Seger and Miller, <xref ref-type="bibr" rid="B54">2010</xref>). Previous human and monkey studies indicate these two systems play different, somewhat complementary roles in visual learning: MTL is important for learning easy visual discriminations, whereas the corticostriatal loop is necessary for learning more difficult and gradually learned discrimination tasks (Teng et al., <xref ref-type="bibr" rid="B60">2000</xref>; Poldrack et al., <xref ref-type="bibr" rid="B47">2001</xref>, <xref ref-type="bibr" rid="B49">2005</xref>; Seger, <xref ref-type="bibr" rid="B53">2006</xref>). The involvement of the two subsystems is known to change dynamically over the course of learning (Poldrack et al., <xref ref-type="bibr" rid="B47">2001</xref>; Voermans et al., <xref ref-type="bibr" rid="B65">2004</xref>; Nomura and Reber, <xref ref-type="bibr" rid="B42">2008</xref>; Daniel et al., <xref ref-type="bibr" rid="B10">2011</xref>). In general, as object uncertainty decreases during the course of learning, the striatal loop becomes less active and MTL becomes more active (Teng et al., <xref ref-type="bibr" rid="B60">2000</xref>; Daniel et al., <xref ref-type="bibr" rid="B10">2011</xref>).</p><p>We posit that the corticostriatal loop plays a comparatively more prominent role than the MTL in object learning in strong clutter, and the reverse is true for the learning of objects in weak clutter, so that they come to preferentially represent the two sets of objects. The differential responses observed using identical testing conditions may reflect a differential engagement of these two subsystems and of other brain regions with close functional connections with either subsystem. Conversely, the regions activated by both sets of objects presumably reflect overlapping roles played by the two subsystems.</p></sec><sec><title>General implications for neural mechanisms of knowledge-based visual disambiguation</title><p>Previous studies of the processes by which prior learning and knowledge of objects help resolve image ambiguities have shown that the underlying pattern of activity can differ based on whether the subject has learned to recognize a given object or not, based on the category of the given object or its visual context (Dolan et al., <xref ref-type="bibr" rid="B12">1997</xref>; Moore and Engel, <xref ref-type="bibr" rid="B38">2001</xref>; Gauthier et al., <xref ref-type="bibr" rid="B20">2003</xref>; Grill-Spector and Malach, <xref ref-type="bibr" rid="B23">2004</xref>; Kourtzi et al., <xref ref-type="bibr" rid="B33">2005</xref>; McKeeff and Tong, <xref ref-type="bibr" rid="B37">2007</xref>; Wong et al., <xref ref-type="bibr" rid="B67">2009</xref>; Brascamp et al., <xref ref-type="bibr" rid="B4">2010</xref>). More directly relevant to the present context, Kourtzi et al. (<xref ref-type="bibr" rid="B33">2005</xref>; also see Zhang and Kourtzi, <xref ref-type="bibr" rid="B70">2010</xref>) have shown that many regions in the visual cortex are more responsive to trained than untrained shapes when the shapes are learned in clutter.</p><p>Our results further show that knowledge-based visual disambiguation of object category is not mediated by a single set of regions or a single pathway, but that different brain regions are activated during recognition depending on the <italic>type</italic> of learning even when the object category and task remain the same. Understanding the precise neural mechanisms by which the type of learning affects knowledge-based visual disambiguation&#x02014;the process by which prior learning and knowledge resolves ambiguities in visual input&#x02014;is likely to be a fruitful area for future research.</p></sec><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>This work was supported by grants ONR N00014-05-1-0124 and NIH R01 EY015261 to Daniel Kersten. This research was partially supported by WCU (World Class University) program funded by the Ministry of Education, Science, and Technology through the National Research Foundation of Korea (R31-10008). This work was also supported in part by the US Army Research Laboratory and the US Army Research Office grant W911NF-11-1-0105 to Jay Hegd&#x000e9;. The 3T scanner at the University of Minnesota Center for Magnetic Resonance Research was supported by BTRR P41 008079 and by the MIND Institute. We thank Drs. Elizabeth Buffalo, Yuhong Jiang, Wilma Koutstaal, Chad Marsolek, and Edmund Rolls for helpful discussions.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>C. I.</given-names></name><name><surname>Hutchison</surname><given-names>T. L.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2007a</year>). <article-title>Does the fusiform face area contain subregions highly selective for nonfaces?</article-title>
<source>Nat. Neurosci</source>. <volume>10</volume>, <fpage>3</fpage>&#x02013;<lpage>4</lpage>
<pub-id pub-id-type="doi">10.1038/nn0107-3</pub-id><pub-id pub-id-type="pmid">17189940</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>C. I.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Wald</surname><given-names>L. L.</given-names></name><name><surname>Kwong</surname><given-names>K. K.</given-names></name><name><surname>Benner</surname><given-names>T.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2007b</year>). <article-title>Visual word processing and experiential origins of functional selectivity in human extrastriate cortex</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>104</volume>, <fpage>9087</fpage>&#x02013;<lpage>9092</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.0703300104</pub-id><pub-id pub-id-type="pmid">17502592</pub-id></mixed-citation></ref><ref id="B7a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baxter</surname><given-names>M. G.</given-names></name></person-group> (<year>2009</year>). <article-title>Involvement of medial temporal lobe structures in memory and perception</article-title>. <source>Neuron</source>
<volume>61</volume>, <fpage>667</fpage>&#x02013;<lpage>677</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.007</pub-id><pub-id pub-id-type="pmid">19285463</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>M. J.</given-names></name><name><surname>Kersten</surname><given-names>D.</given-names></name></person-group> (<year>2003</year>). <article-title>Bootstrapped learning of novel objects</article-title>. <source>J. Vis</source>. <volume>3</volume>, <fpage>413</fpage>&#x02013;<lpage>422</lpage>
<pub-id pub-id-type="doi">10.1167/3.6.2</pub-id><pub-id pub-id-type="pmid">12901712</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brascamp</surname><given-names>J. W.</given-names></name><name><surname>Kanai</surname><given-names>R.</given-names></name><name><surname>Walsh</surname><given-names>V.</given-names></name><name><surname>van Ee</surname><given-names>R.</given-names></name></person-group> (<year>2010</year>). <article-title>Human middle temporal cortex, perceptual bias, and perceptual memory for ambiguous three-dimensional motion</article-title>. <source>J. Neurosci</source>. <volume>30</volume>, <fpage>760</fpage>&#x02013;<lpage>766</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4171-09.2010</pub-id><pub-id pub-id-type="pmid">20071541</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brett</surname><given-names>M.</given-names></name><name><surname>Anton</surname><given-names>J.-L.</given-names></name><name><surname>Valabregue</surname><given-names>R.</given-names></name><name><surname>Poline</surname><given-names>J.-B.</given-names></name></person-group> (<year>2002</year>). <article-title>&#x0201c;Region of interest analysis using an SPM toolbox,&#x0201d;</article-title> in <source>Paper Presented at: 8th International Conference on Functional Mapping of the Human Brain</source>, June 2&#x02013;6, 2002, (Sendai, Japan).</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buracas</surname><given-names>G. T.</given-names></name><name><surname>Boynton</surname><given-names>G. M.</given-names></name></person-group> (<year>2002</year>). <article-title>Efficient design of event-related fMRI experiments using M-sequences</article-title>. <source>Neuroimage</source>
<volume>16</volume>, <fpage>801</fpage>&#x02013;<lpage>813</lpage>
<pub-id pub-id-type="doi">10.1006/nimg.2002.1116</pub-id><pub-id pub-id-type="pmid">12169264</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>T. J.</given-names></name><name><surname>Saksida</surname><given-names>L. M.</given-names></name></person-group> (<year>2007</year>). <article-title>Memory, perception, and the ventral visual-perirhinal-hippocampal stream: thinking outside of the boxes</article-title>. <source>Hippocampus</source>
<volume>17</volume>, <fpage>898</fpage>&#x02013;<lpage>908</lpage>
<pub-id pub-id-type="doi">10.1002/hipo.20320</pub-id><pub-id pub-id-type="pmid">17636546</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>P.</given-names></name></person-group> (<year>1991</year>). <article-title>&#x0201c;What&#x02019;s up in top-down processing?,&#x0201d;</article-title> in <source>Representations of Vision: Trends and Tacit Assumptions in Vision Research</source>, ed <person-group person-group-type="editor"><name><surname>Gorea</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>), <fpage>295</fpage>&#x02013;<lpage>304</lpage></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J.</given-names></name></person-group> (<year>1988</year>). <source>Statistical Power Analysis for the Behavioral Sciences</source>, 2nd Edn. <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>L. Erlbaum Associates</publisher-name></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daniel</surname><given-names>R.</given-names></name><name><surname>Wagner</surname><given-names>G.</given-names></name><name><surname>Koch</surname><given-names>K.</given-names></name><name><surname>Reichenbach</surname><given-names>J. R.</given-names></name><name><surname>Sauer</surname><given-names>H.</given-names></name><name><surname>Schlosser</surname><given-names>R. G.</given-names></name></person-group> (<year>2011</year>). <article-title>Assessing the neural basis of uncertainty in perceptual category learning through varying levels of distortion</article-title>. <source>J. Cogn. Neurosci</source>. <volume>23</volume>, <fpage>1781</fpage>&#x02013;<lpage>1793</lpage>
<pub-id pub-id-type="doi">10.1162/jocn.2010.21541</pub-id><pub-id pub-id-type="pmid">20617884</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diana</surname><given-names>R. A.</given-names></name><name><surname>Yonelinas</surname><given-names>A. P.</given-names></name><name><surname>Ranganath</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>Medial temporal lobe activity during source retrieval reflects information type, not memory strength</article-title>. <source>J. Cogn. Neurosci</source>. <volume>22</volume>, <fpage>1808</fpage>&#x02013;<lpage>1818</lpage>
<pub-id pub-id-type="doi">10.1162/jocn.2009.21335</pub-id><pub-id pub-id-type="pmid">19702458</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>R. J.</given-names></name><name><surname>Fink</surname><given-names>G. R.</given-names></name><name><surname>Rolls</surname><given-names>E.</given-names></name><name><surname>Booth</surname><given-names>M.</given-names></name><name><surname>Holmes</surname><given-names>A.</given-names></name><name><surname>Frackowiak</surname><given-names>R. S.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>1997</year>). <article-title>How the brain learns to see objects and faces in an impoverished context</article-title>. <source>Nature</source>
<volume>389</volume>, <fpage>596</fpage>&#x02013;<lpage>599</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.12.023</pub-id><pub-id pub-id-type="pmid">9335498</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>P. D.</given-names></name></person-group> (<year>2010</year>). <source>The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results</source>. <publisher-loc>Cambridge, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epshtein</surname><given-names>B.</given-names></name><name><surname>Lifshitz</surname><given-names>I.</given-names></name><name><surname>Ullman</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Image interpretation by a single bottom-up top-down cycle</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>105</volume>, <fpage>14298</fpage>&#x02013;<lpage>14303</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.0800968105</pub-id><pub-id pub-id-type="pmid">18796607</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R.</given-names></name><name><surname>Harris</surname><given-names>A.</given-names></name><name><surname>Stanley</surname><given-names>D.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>1999</year>). <article-title>The parahippocampal place area: recognition, navigation, or encoding?</article-title>
<source>Neuron</source>
<volume>23</volume>, <fpage>115</fpage>&#x02013;<lpage>125</lpage>
<pub-id pub-id-type="doi">10.1016/S0896-6273(00)80758-8</pub-id><pub-id pub-id-type="pmid">10402198</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>1998</year>). <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>
<volume>392</volume>, <fpage>598</fpage>&#x02013;<lpage>601</lpage>
<pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>A. C.</given-names></name><name><surname>Collins</surname><given-names>D. L.</given-names></name><name><surname>Mills</surname><given-names>S. R.</given-names></name><name><surname>Brown</surname><given-names>E. D.</given-names></name><name><surname>Kelly</surname><given-names>R. L.</given-names></name><name><surname>Peters</surname><given-names>T. M.</given-names></name></person-group> (<year>1993</year>). <article-title>&#x0201c;3D statistical neuroanatomical models from 305 MRI volumes,&#x0201d;</article-title> in <source>Proceedings of IEEE-Nuclear Science Symposium and Medical Imaging Conference</source>. (<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>), <fpage>1813</fpage>&#x02013;<lpage>1817</lpage></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2007</year>). <source>Statistical Parametric Mapping: The Analysis of Funtional Brain Images</source>, 1st Edn. <publisher-loc>Amsterdam; Boston</publisher-loc>: <publisher-name>Elsevier/Academic Press</publisher-name></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganel</surname><given-names>T.</given-names></name><name><surname>Gonzalez</surname><given-names>C. L.</given-names></name><name><surname>Valyear</surname><given-names>K. F.</given-names></name><name><surname>Culham</surname><given-names>J. C.</given-names></name><name><surname>Goodale</surname><given-names>M. A.</given-names></name><name><surname>Kohler</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <article-title>The relationship between fMRI adaptation and repetition priming</article-title>. <source>Neuroimage</source>
<volume>32</volume>, <fpage>1432</fpage>&#x02013;<lpage>1440</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.039</pub-id><pub-id pub-id-type="pmid">16854597</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I.</given-names></name><name><surname>Curran</surname><given-names>T.</given-names></name><name><surname>Curby</surname><given-names>K. M.</given-names></name><name><surname>Collins</surname><given-names>D.</given-names></name></person-group> (<year>2003</year>). <article-title>Perceptual interference supports a non-modular account of face processing</article-title>. <source>Nat. Neurosci</source>. <volume>6</volume>, <fpage>428</fpage>&#x02013;<lpage>432</lpage>
<pub-id pub-id-type="doi">10.1038/nn1029</pub-id><pub-id pub-id-type="pmid">12627167</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name></person-group> (<year>2003</year>). <article-title>The neural basis of object perception</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>13</volume>, <fpage>159</fpage>&#x02013;<lpage>166</lpage>
<pub-id pub-id-type="doi">10.1016/S0959-4388(03)00040-0</pub-id><pub-id pub-id-type="pmid">12744968</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name><name><surname>Kourtzi</surname><given-names>Z.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2001</year>). <article-title>The lateral occipital complex and its role in object recognition</article-title>. <source>Vision Res</source>. <volume>41</volume>, <fpage>1409</fpage>&#x02013;<lpage>1422</lpage>
<pub-id pub-id-type="doi">10.1016/S0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name><name><surname>Malach</surname><given-names>R.</given-names></name></person-group> (<year>2004</year>). <article-title>The human visual cortex</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>27</volume>, <fpage>649</fpage>&#x02013;<lpage>677</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144220</pub-id><pub-id pub-id-type="pmid">15217346</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanlon</surname><given-names>R. T.</given-names></name><name><surname>Naud</surname><given-names>M. J.</given-names></name><name><surname>Forsythe</surname><given-names>J. W.</given-names></name><name><surname>Hall</surname><given-names>K.</given-names></name><name><surname>Watson</surname><given-names>A. C.</given-names></name><name><surname>McKechnie</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>Adaptable night camouflage by cuttlefish</article-title>. <source>Am. Nat</source>. <volume>169</volume>, <fpage>543</fpage>&#x02013;<lpage>551</lpage>
<pub-id pub-id-type="doi">10.1086/512106</pub-id><pub-id pub-id-type="pmid">17427123</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harrell</surname><given-names>F. E.</given-names></name></person-group> (<year>2001</year>). <source>Regression Modeling Strategies</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegd&#x000e9;</surname><given-names>J.</given-names></name><name><surname>Bart</surname><given-names>E.</given-names></name><name><surname>Kersten</surname><given-names>D.</given-names></name></person-group> (<year>2008a</year>). <article-title>Fragment-based learning of visual object categories</article-title>. <source>Curr. Biol</source>. <volume>18</volume>, <fpage>597</fpage>&#x02013;<lpage>601</lpage>
<pub-id pub-id-type="doi">10.1016/j.cub.2008.03.058</pub-id><pub-id pub-id-type="pmid">18424145</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegd&#x000e9;</surname><given-names>J.</given-names></name><name><surname>Fang</surname><given-names>F.</given-names></name><name><surname>Murray</surname><given-names>S. O.</given-names></name><name><surname>Kersten</surname><given-names>D.</given-names></name></person-group> (<year>2008b</year>). <article-title>Preferential responses to occluded objects in the human visual cortex</article-title>. <source>J. Vis</source>. <volume>8</volume>, <fpage>16.11</fpage>&#x02013;<lpage>16.16</lpage>
<pub-id pub-id-type="doi">10.1167/8.4.16</pub-id><pub-id pub-id-type="pmid">18484855</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hosmer</surname><given-names>D. W.</given-names></name><name><surname>Lemeshow</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <source>Applied Logistic Regression</source>, 2nd Edn. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joseph</surname><given-names>J. E.</given-names></name><name><surname>Partin</surname><given-names>D. J.</given-names></name><name><surname>Jones</surname><given-names>K. M.</given-names></name></person-group> (<year>2002</year>). <article-title>Hypothesis testing for selective, differential, and conjoined brain activation</article-title>. <source>J. Neurosci. Methods</source>
<volume>118</volume>, <fpage>129</fpage>&#x02013;<lpage>140</lpage>
<pub-id pub-id-type="doi">10.1016/S0165-0270(02)00122-X</pub-id><pub-id pub-id-type="pmid">12204304</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten</surname><given-names>D.</given-names></name><name><surname>Mamassian</surname><given-names>P.</given-names></name><name><surname>Yuille</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Object perception as Bayesian inference</article-title>. <source>Annu. Rev. Psychol</source>. <volume>55</volume>, <fpage>271</fpage>&#x02013;<lpage>304</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142005</pub-id><pub-id pub-id-type="pmid">14744217</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kleinbaum</surname><given-names>D. G.</given-names></name><name><surname>Klein</surname><given-names>M.</given-names></name><name><surname>Pryor</surname><given-names>E. R.</given-names></name></person-group> (<year>2002</year>). <source>Logistic Regression: A Self-learning Text</source>, 2nd Edn. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname><given-names>E.</given-names></name><name><surname>Hyafil</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). <article-title>Anterior prefrontal function and the limits of human decision-making</article-title>. <source>Science</source>
<volume>318</volume>, <fpage>594</fpage>&#x02013;<lpage>598</lpage>
<pub-id pub-id-type="doi">10.1126/science.1142995</pub-id><pub-id pub-id-type="pmid">17962551</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z.</given-names></name><name><surname>Betts</surname><given-names>L. R.</given-names></name><name><surname>Sarkheil</surname><given-names>P.</given-names></name><name><surname>Welchman</surname><given-names>A. E.</given-names></name></person-group> (<year>2005</year>). <article-title>Distributed neural plasticity for shape learning in the human visual cortex</article-title>. <source>PLoS Biol</source>. <volume>3</volume>:<fpage>e204</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pbio.0030204</pub-id><pub-id pub-id-type="pmid">15934786</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name><name><surname>Simmons</surname><given-names>W. K.</given-names></name><name><surname>Bellgowan</surname><given-names>P. S.</given-names></name><name><surname>Baker</surname><given-names>C. I.</given-names></name></person-group> (<year>2009</year>). <article-title>Circular analysis in systems neuroscience: the dangers of double dipping</article-title>. <source>Nat. Neurosci</source>. <volume>12</volume>, <fpage>535</fpage>&#x02013;<lpage>540</lpage>
<pub-id pub-id-type="doi">10.1038/nn.2303</pub-id><pub-id pub-id-type="pmid">19396166</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levi</surname><given-names>D. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Crowding&#x02013;an essential bottleneck for object recognition: a mini-review</article-title>. <source>Vision Res</source>. <volume>48</volume>, <fpage>635</fpage>&#x02013;<lpage>654</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2007.12.009</pub-id><pub-id pub-id-type="pmid">18226828</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N.</given-names></name><name><surname>Cox</surname><given-names>D. D.</given-names></name><name><surname>Zoccolan</surname><given-names>D.</given-names></name><name><surname>DiCarlo</surname><given-names>J. J.</given-names></name></person-group> (<year>2009</year>). <article-title>What response properties do individual neurons need to underlie position and clutter &#x0201c;invariant&#x0201d; object recognition?</article-title>
<source>J. Neurophysiol</source>. <volume>102</volume>, <fpage>360</fpage>&#x02013;<lpage>376</lpage>
<pub-id pub-id-type="doi">10.1152/jn.90745.2008</pub-id><pub-id pub-id-type="pmid">19439676</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKeeff</surname><given-names>T. J.</given-names></name><name><surname>Tong</surname><given-names>F.</given-names></name></person-group> (<year>2007</year>). <article-title>The timing of perceptual decisions for ambiguous face stimuli in the human ventral visual cortex</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>669</fpage>&#x02013;<lpage>678</lpage>
<pub-id pub-id-type="doi">10.1093/cercor/bhk015</pub-id><pub-id pub-id-type="pmid">16648454</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>C.</given-names></name><name><surname>Engel</surname><given-names>S. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Neural response to perception of volume in the lateral occipital complex</article-title>. <source>Neuron</source>
<volume>29</volume>, <fpage>277</fpage>&#x02013;<lpage>286</lpage>
<pub-id pub-id-type="doi">10.1016/S0896-6273(01)00197-0</pub-id><pub-id pub-id-type="pmid">11182098</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname><given-names>B. C.</given-names></name><name><surname>Simoni</surname><given-names>D. A.</given-names></name></person-group> (<year>2007</year>). <article-title>The roles of cortical image separation and size in active visual search performance</article-title>. <source>J. Vis</source>. <volume>7</volume>, <fpage>6.1</fpage>&#x02013;<lpage>6.15</lpage>
<pub-id pub-id-type="doi">10.1167/7.2.6</pub-id><pub-id pub-id-type="pmid">18217821</pub-id></mixed-citation></ref><ref id="B40a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>E. A.</given-names></name><name><surname>Bussey</surname><given-names>T. J.</given-names></name><name><surname>Saksida</surname><given-names>L. M.</given-names></name></person-group> (<year>2007</year>). <article-title>Visual perception and memory: a new view of medial temporal lobe function in primates and rodents</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>30</volume>, <fpage>99</fpage>&#x02013;<lpage>122</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113046</pub-id><pub-id pub-id-type="pmid">17417938</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>S. O.</given-names></name><name><surname>Kersten</surname><given-names>D.</given-names></name><name><surname>Olshausen</surname><given-names>B. A.</given-names></name><name><surname>Schrater</surname><given-names>P.</given-names></name><name><surname>Woods</surname><given-names>D. L.</given-names></name></person-group> (<year>2002</year>). <article-title>Shape perception reduces activity in human primary visual cortex</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>99</volume>, <fpage>15164</fpage>&#x02013;<lpage>15169</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.192579399</pub-id><pub-id pub-id-type="pmid">12417754</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nandy</surname><given-names>A. S.</given-names></name><name><surname>Tjan</surname><given-names>B. S.</given-names></name></person-group> (<year>2008</year>). <article-title>Efficient integration across spatial frequencies for letter identification in foveal and peripheral vision</article-title>. <source>J. Vis</source>. <volume>8</volume>, <fpage>3.1</fpage>&#x02013;<lpage>3.20</lpage>
<pub-id pub-id-type="doi">10.1167/8.13.3</pub-id><pub-id pub-id-type="pmid">19146333</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nomura</surname><given-names>E. M.</given-names></name><name><surname>Reber</surname><given-names>P. J.</given-names></name></person-group> (<year>2008</year>). <article-title>A review of medial temporal lobe and caudate contributions to visual category learning</article-title>. <source>Neurosci. Biobehav. Rev</source>. <volume>32</volume>, <fpage>279</fpage>&#x02013;<lpage>291</lpage>
<pub-id pub-id-type="doi">10.1016/j.neubiorev.2007.07.006</pub-id><pub-id pub-id-type="pmid">17868867</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollinger</surname><given-names>J. M.</given-names></name><name><surname>Corbetta</surname><given-names>M.</given-names></name><name><surname>Shulman</surname><given-names>G. L.</given-names></name></person-group> (<year>2001a</year>). <article-title>Separating processes within a trial in event-related functional MRI</article-title>. <source>Neuroimage</source>
<volume>13</volume>, <fpage>218</fpage>&#x02013;<lpage>229</lpage>
<pub-id pub-id-type="doi">10.1006/nimg.2000.0711</pub-id><pub-id pub-id-type="pmid">11133324</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollinger</surname><given-names>J. M.</given-names></name><name><surname>Shulman</surname><given-names>G. L.</given-names></name><name><surname>Corbetta</surname><given-names>M.</given-names></name></person-group> (<year>2001b</year>). <article-title>Separating processes within a trial in event-related functional MRI</article-title>. <source>Neuroimage</source>
<volume>13</volume>, <fpage>210</fpage>&#x02013;<lpage>217</lpage>
<pub-id pub-id-type="doi">10.1006/nimg.2000.0710</pub-id><pub-id pub-id-type="pmid">11133323</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>H. P.</given-names></name><name><surname>Wagemans</surname><given-names>J.</given-names></name><name><surname>Vogels</surname><given-names>R.</given-names></name></person-group> (<year>2007</year>). <article-title>Effects of perceptual learning in visual backward masking on the responses of macaque inferior temporal neurons</article-title>. <source>Neuroscience</source>
<volume>145</volume>, <fpage>775</fpage>&#x02013;<lpage>789</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.12.058</pub-id><pub-id pub-id-type="pmid">17293053</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>D. G.</given-names></name><name><surname>Tillman</surname><given-names>K. A.</given-names></name></person-group> (<year>2008</year>). <article-title>The uncrowded window of object recognition</article-title>. <source>Nat. Neurosci</source>. <volume>11</volume>, <fpage>1129</fpage>&#x02013;<lpage>1135</lpage>
<pub-id pub-id-type="pmid">18828191</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>R. A.</given-names></name><name><surname>Clark</surname><given-names>J.</given-names></name><name><surname>Pare-Blagoev</surname><given-names>E. J.</given-names></name><name><surname>Shohamy</surname><given-names>D.</given-names></name><name><surname>Creso Moyano</surname><given-names>J.</given-names></name><name><surname>Myers</surname><given-names>C.</given-names></name><name><surname>Gluck</surname><given-names>M. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Interactive memory systems in the human brain</article-title>. <source>Nature</source>
<volume>414</volume>, <fpage>546</fpage>&#x02013;<lpage>550</lpage>
<pub-id pub-id-type="doi">10.1038/35107080</pub-id><pub-id pub-id-type="pmid">11734855</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>R. A.</given-names></name><name><surname>Foerde</surname><given-names>K.</given-names></name></person-group> (<year>2008</year>). <article-title>Category learning and the memory systems debate</article-title>. <source>Neurosci. Biobehav. Rev</source>. <volume>32</volume>, <fpage>197</fpage>&#x02013;<lpage>205</lpage>
<pub-id pub-id-type="doi">10.1016/j.neubiorev.2007.07.007</pub-id><pub-id pub-id-type="pmid">17869339</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>R. A.</given-names></name><name><surname>Sabb</surname><given-names>F. W.</given-names></name><name><surname>Foerde</surname><given-names>K.</given-names></name><name><surname>Tom</surname><given-names>S. M.</given-names></name><name><surname>Asarnow</surname><given-names>R. F.</given-names></name><name><surname>Bookheimer</surname><given-names>S. Y.</given-names></name><name><surname>Knowlton</surname><given-names>B. J.</given-names></name></person-group> (<year>2005</year>). <article-title>The neural correlates of motor skill automaticity</article-title>. <source>J. Neurosci</source>. <volume>25</volume>, <fpage>5356</fpage>&#x02013;<lpage>5364</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3880-04.2005</pub-id><pub-id pub-id-type="pmid">15930384</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>L.</given-names></name><name><surname>Quiroga</surname><given-names>R. Q.</given-names></name><name><surname>Wilken</surname><given-names>P.</given-names></name><name><surname>Koch</surname><given-names>C.</given-names></name><name><surname>Fried</surname><given-names>I.</given-names></name></person-group> (<year>2006</year>). <article-title>A single-neuron correlate of change detection and change blindness in the human medial temporal lobe</article-title>. <source>Curr. Biol</source>. <volume>16</volume>, <fpage>2066</fpage>&#x02013;<lpage>2072</lpage>
<pub-id pub-id-type="doi">10.1016/j.cub.2006.08.064</pub-id><pub-id pub-id-type="pmid">17055988</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>E. T.</given-names></name><name><surname>Tromans</surname><given-names>J. M.</given-names></name><name><surname>Stringer</surname><given-names>S. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Spatial scene representations formed by self-organizing learning in a hippocampal extension of the ventral visual system</article-title>. <source>Eur. J. Neurosci</source>. <volume>28</volume>, <fpage>2116</fpage>&#x02013;<lpage>2127</lpage>
<pub-id pub-id-type="doi">10.1111/j.1460-9568.2008.06486.x</pub-id><pub-id pub-id-type="pmid">19046392</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>D. L.</given-names></name><name><surname>Wig</surname><given-names>G. S.</given-names></name><name><surname>Stevens</surname><given-names>W. D.</given-names></name></person-group> (<year>2007</year>). <article-title>Reductions in cortical activity during priming</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>17</volume>, <fpage>171</fpage>&#x02013;<lpage>176</lpage>
<pub-id pub-id-type="doi">10.1016/j.conb.2007.02.001</pub-id><pub-id pub-id-type="pmid">17303410</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seger</surname><given-names>C. A.</given-names></name></person-group> (<year>2006</year>). <article-title>The basal ganglia in human learning</article-title>. <source>Neuroscientist</source>
<volume>12</volume>, <fpage>285</fpage>&#x02013;<lpage>290</lpage>
<pub-id pub-id-type="doi">10.1177/1073858405285632</pub-id><pub-id pub-id-type="pmid">16840704</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seger</surname><given-names>C. A.</given-names></name><name><surname>Miller</surname><given-names>E. K.</given-names></name></person-group> (<year>2010</year>). <article-title>Category learning in the brain</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>33</volume>, <fpage>203</fpage>&#x02013;<lpage>219</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135546</pub-id><pub-id pub-id-type="pmid">20572771</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sela</surname><given-names>Y.</given-names></name><name><surname>Freiman</surname><given-names>M.</given-names></name><name><surname>Dery</surname><given-names>E.</given-names></name><name><surname>Edrei</surname><given-names>Y.</given-names></name><name><surname>Safadi</surname><given-names>R.</given-names></name><name><surname>Pappo</surname><given-names>O.</given-names></name><name><surname>Joskowicz</surname><given-names>L.</given-names></name><name><surname>Abramovitch</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>fMRI-based hierarchical SVM model for the classification and grading of liver fibrosis</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <volume>58</volume>, <fpage>2574</fpage>&#x02013;<lpage>2581</lpage>
<pub-id pub-id-type="doi">10.1109/TBME.2011.2159501</pub-id><pub-id pub-id-type="pmid">21672670</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>J. T.</given-names></name></person-group> (<year>2004</year>). <article-title>A comparison of methods for characterizing the event-related BOLD timeseries in rapid fMRI</article-title>. <source>Neuroimage</source>
<volume>21</volume>, <fpage>1690</fpage>&#x02013;<lpage>1700</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.12.021</pub-id><pub-id pub-id-type="pmid">15050591</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>S. M.</given-names></name><name><surname>Rolls</surname><given-names>E. T.</given-names></name></person-group> (<year>2000</year>). <article-title>Position invariant recognition in the visual system with cluttered environments</article-title>. <source>Neural Netw</source>. <volume>13</volume>, <fpage>305</fpage>&#x02013;<lpage>315</lpage>
<pub-id pub-id-type="doi">10.1016/S0893-6080(00)00017-4</pub-id><pub-id pub-id-type="pmid">10937964</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>S. M.</given-names></name><name><surname>Rolls</surname><given-names>E. T.</given-names></name></person-group> (<year>2008</year>). <article-title>Learning transform invariant object recognition in the visual system with multiple stimuli present during training</article-title>. <source>Neural Netw</source>. <volume>21</volume>, <fpage>888</fpage>&#x02013;<lpage>903</lpage>
<pub-id pub-id-type="doi">10.1016/j.neunet.2007.11.004</pub-id><pub-id pub-id-type="pmid">18440774</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stuss</surname><given-names>D. T.</given-names></name><name><surname>Knight</surname><given-names>R. T.</given-names></name></person-group> (<year>2002</year>). <source>Principles of Frontal Lobe Function</source>. <publisher-loc>Oxford, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name></mixed-citation></ref><ref id="B59a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>W. A.</given-names></name><name><surname>Baxter</surname><given-names>M. G.</given-names></name></person-group> (<year>2009</year>). <article-title>Memory, perception, and the medial temporal lobe: a synthesis of opinions</article-title>. <source>Neuron</source>
<volume>61</volume>, <fpage>678</fpage>&#x02013;<lpage>679</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.009</pub-id><pub-id pub-id-type="pmid">19285464</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>E.</given-names></name><name><surname>Stefanacci</surname><given-names>L.</given-names></name><name><surname>Squire</surname><given-names>L. R.</given-names></name><name><surname>Zola</surname><given-names>S. M.</given-names></name></person-group> (<year>2000</year>). <article-title>Contrasting effects on discrimination learning after hippocampal lesions and conjoint hippocampal-caudate lesions in monkeys</article-title>. <source>J. Neurosci</source>. <volume>20</volume>, <fpage>3853</fpage>&#x02013;<lpage>3863</lpage>
<pub-id pub-id-type="pmid">10804225</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S.</given-names></name><name><surname>Vidal-Naquet</surname><given-names>M.</given-names></name><name><surname>Sali</surname><given-names>E.</given-names></name></person-group> (<year>2002</year>). <article-title>Visual features of intermediate complexity and their use in classification</article-title>. <source>Nat. Neurosci</source>. <volume>5</volume>, <fpage>682</fpage>&#x02013;<lpage>687</lpage>
<pub-id pub-id-type="doi">10.1038/nn870</pub-id><pub-id pub-id-type="pmid">12055634</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>L. G.</given-names></name><name><surname>Bell</surname><given-names>A. H.</given-names></name></person-group> (<year>2011</year>). <article-title>Uncovering the visual &#x0201c;alphabet&#x0201d;: advances in our understanding of object perception</article-title>. <source>Vision Res</source>. <volume>51</volume>, <fpage>782</fpage>&#x02013;<lpage>799</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2010.10.002</pub-id><pub-id pub-id-type="pmid">20971130</pub-id></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>D. C.</given-names></name><name><surname>Drury</surname><given-names>H. A.</given-names></name><name><surname>Dickson</surname><given-names>J.</given-names></name><name><surname>Harwell</surname><given-names>J.</given-names></name><name><surname>Hanlon</surname><given-names>D.</given-names></name><name><surname>Anderson</surname><given-names>C. H.</given-names></name></person-group> (<year>2001</year>). <article-title>An integrated software suite for surface-based analyses of cerebral cortex</article-title>. <source>J. Am. Med. Inform. Assoc</source>. <volume>8</volume>, <fpage>443</fpage>&#x02013;<lpage>459</lpage>
<pub-id pub-id-type="doi">10.1136/jamia.2001.0080443</pub-id><pub-id pub-id-type="pmid">11522765</pub-id></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vann</surname><given-names>S. D.</given-names></name><name><surname>Albasser</surname><given-names>M. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Hippocampus and neocortex: recognition and spatial memory</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>21</volume>, <fpage>440</fpage>&#x02013;<lpage>445</lpage>
<pub-id pub-id-type="doi">10.1016/j.conb.2011.02.002</pub-id><pub-id pub-id-type="pmid">21353527</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voermans</surname><given-names>N. C.</given-names></name><name><surname>Petersson</surname><given-names>K. M.</given-names></name><name><surname>Daudey</surname><given-names>L.</given-names></name><name><surname>Weber</surname><given-names>B.</given-names></name><name><surname>Van Spaendonck</surname><given-names>K. P.</given-names></name><name><surname>Kremer</surname><given-names>H. P.</given-names></name><name><surname>Fernandez</surname><given-names>G.</given-names></name></person-group> (<year>2004</year>). <article-title>Interaction between the human hippocampus and the caudate nucleus during route recognition</article-title>. <source>Neuron</source>
<volume>43</volume>, <fpage>427</fpage>&#x02013;<lpage>435</lpage>
<pub-id pub-id-type="doi">10.1016/j.nlm.2009.06.002</pub-id><pub-id pub-id-type="pmid">15294149</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wee</surname><given-names>C. Y.</given-names></name><name><surname>Yap</surname><given-names>P. T.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Denny</surname><given-names>K.</given-names></name><name><surname>Browndyke</surname><given-names>J. N.</given-names></name><name><surname>Potter</surname><given-names>G. G.</given-names></name><name><surname>Welsh-Bohmer</surname><given-names>K. A.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Identification of MCI individuals using structural and functional connectivity networks</article-title>. <source>Neuroimage</source>
<volume>59</volume>, <fpage>2045</fpage>&#x02013;<lpage>2056</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.015</pub-id><pub-id pub-id-type="pmid">22019883</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>A. C.</given-names></name><name><surname>Palmeri</surname><given-names>T. J.</given-names></name><name><surname>Rogers</surname><given-names>B. P.</given-names></name><name><surname>Gore</surname><given-names>J. C.</given-names></name><name><surname>Gauthier</surname><given-names>I.</given-names></name></person-group> (<year>2009</year>). <article-title>Beyond shape: how you learn about objects affects how they are represented in visual cortex</article-title>. <source>PLoS ONE</source>
<volume>4</volume>:<fpage>e8405</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0008405</pub-id><pub-id pub-id-type="pmid">20027229</pub-id></mixed-citation></ref><ref id="B67a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>D.</given-names></name><name><surname>Olson</surname><given-names>I. R.</given-names></name><name><surname>Chun</surname><given-names>M. M.</given-names></name></person-group> (<year>2006</year>). <article-title>Shape-specific perceptual learning in a figure-ground segregation task</article-title>. <source>Vis. Res</source>. <volume>46</volume>, <fpage>914</fpage>&#x02013;<lpage>924</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2005.09.009</pub-id><pub-id pub-id-type="pmid">16242752</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yotsumoto</surname><given-names>Y.</given-names></name><name><surname>Watanabe</surname><given-names>T.</given-names></name><name><surname>Sasaki</surname><given-names>Y.</given-names></name></person-group> (<year>2008</year>). <article-title>Different dynamics of performance and brain activation in the time course of perceptual learning</article-title>. <source>Neuron</source>
<volume>57</volume>, <fpage>827</fpage>&#x02013;<lpage>833</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.034</pub-id><pub-id pub-id-type="pmid">18367084</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuille</surname><given-names>A.</given-names></name><name><surname>Kersten</surname><given-names>D.</given-names></name></person-group> (<year>2006</year>). <article-title>Vision as Bayesian inference: analysis by synthesis?</article-title>
<source>Trends Cogn. Sci</source>. <volume>10</volume>, <fpage>301</fpage>&#x02013;<lpage>308</lpage>
<pub-id pub-id-type="doi">10.1016/j.tics.2006.05.002</pub-id><pub-id pub-id-type="pmid">16784882</pub-id></mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Kourtzi</surname><given-names>Z.</given-names></name></person-group> (<year>2010</year>). <article-title>Learning-dependent plasticity with and without training in the human brain</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>107</volume>, <fpage>13503</fpage>&#x02013;<lpage>13508</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.1002506107</pub-id><pub-id pub-id-type="pmid">20628009</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Liang</surname><given-names>L.</given-names></name><name><surname>Anderson</surname><given-names>J. R.</given-names></name><name><surname>Gatewood</surname><given-names>L.</given-names></name><name><surname>Rottenberg</surname><given-names>D. A.</given-names></name><name><surname>Strother</surname><given-names>S. C.</given-names></name></person-group> (<year>2008</year>). <article-title>Evaluation and comparison of GLM- and CVA-based fMRI processing pipelines with Java-based fMRI processing pipeline evaluation system</article-title>. <source>Neuroimage</source>
<volume>41</volume>, <fpage>1242</fpage>&#x02013;<lpage>1252</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.034</pub-id><pub-id pub-id-type="pmid">18482849</pub-id></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D.</given-names></name><name><surname>Kouh</surname><given-names>M.</given-names></name><name><surname>Poggio</surname><given-names>T.</given-names></name><name><surname>DiCarlo</surname><given-names>J. J.</given-names></name></person-group> (<year>2007</year>). <article-title>Trade-off between object selectivity and tolerance in monkey inferotemporal cortex</article-title>. <source>J. Neurosci</source>. <volume>27</volume>, <fpage>12292</fpage>&#x02013;<lpage>12307</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1897-07.2007</pub-id><pub-id pub-id-type="pmid">17989294</pub-id></mixed-citation></ref></ref-list><app-group><app id="A1"><title>Appendix</title><fig id="FA1" position="anchor"><label>Figure A1</label><caption><p><bold>Time course of BOLD responses in bilateral medial occipital cortex.</bold> The time course of each given condition from each given ROI was calculated as the mean response (&#x000b1;SEM) across all sessions and subjects as determined by FIR modeling (Method 1; Materials and Methods). During the strong clutter and weak clutter conditions, the correct response was a &#x0201c;Target Present&#x0201d; report, whereas during the no target condition, the correct response was a &#x0201c;Target Absent&#x0201d; report. For these three conditions, the BOLD time courses are shown separately for correct vs. incorrect responses. For the &#x0201c;Scrambled&#x0201d; condition, all subjects provided the expected response in all trials, so that incorrect responses were not available.</p></caption><graphic xlink:href="fnhum-06-00170-a0001"/></fig><fig id="FA2" position="anchor"><label>Figure A2</label><caption><p><bold>Time course of BOLD responses for selected ROIs in Table <xref ref-type="table" rid="T2">2</xref> in the main text.</bold> BOLD time courses are shown for those six ROIs in Table <xref ref-type="table" rid="T2">2</xref> (color-highlighted in Table <xref ref-type="table" rid="T2">2</xref>) that showed significant correlation with the behavioral responses. Time courses were calculated and plotted as described in the legend to Figure <xref ref-type="fig" rid="FA1">A1</xref>.</p></caption><graphic xlink:href="fnhum-06-00170-a0002"/></fig></app></app-group></back></article>