<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Integr Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Integr Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Integr. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Integrative Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5145</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26909028</article-id><article-id pub-id-type="pmc">4754445</article-id><article-id pub-id-type="doi">10.3389/fnint.2016.00006</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Magnitude Estimation with Noisy Integrators Linked by an Adaptive Reference</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Thurley</surname><given-names>Kay</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/54287/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department Biology II, Ludwig-Maximilians-Universit&#x000e4;t M&#x000fc;nchen</institution><country>M&#x000fc;nchen, Germany</country></aff><aff id="aff2"><sup>2</sup><institution>Bernstein Center for Computational Neuroscience</institution><country>Munich, Germany</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Henry H. Yin, Duke University, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Xin Jin, The Salk Institute for Biological Studies, USA; Trevor B. Penney, National University of Singapore, Singapore</p></fn><corresp id="fn001">*Correspondence: Kay Thurley <email xlink:type="simple">thurley@bio.lmu.de</email></corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>2</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>10</volume><elocation-id>6</elocation-id><history><date date-type="received"><day>19</day><month>10</month><year>2015</year></date><date date-type="accepted"><day>02</day><month>2</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016 Thurley.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Thurley</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Judgments of physical stimuli show characteristic biases; relatively small stimuli are overestimated whereas relatively large stimuli are underestimated (regression effect). Such biases likely result from a strategy that seeks to minimize errors given noisy estimates about stimuli that itself are drawn from a distribution, i.e., the statistics of the environment. While being conceptually well described, it is unclear how such a strategy could be implemented neurally. The present paper aims toward answering this question. A theoretical approach is introduced that describes magnitude estimation as two successive stages of noisy (neural) integration. Both stages are linked by a reference memory that is updated with every new stimulus. The model reproduces the behavioral characteristics of magnitude estimation and makes several experimentally testable predictions. Moreover, the model identifies the regression effect as a means of minimizing estimation errors and explains how this optimality strategy depends on the subject's discrimination abilities and on the stimulus statistics. The latter influence predicts another property of magnitude estimation, the so-called range effect. Beyond being successful in describing decision-making, the present work suggests that noisy integration may also be important in processing magnitudes.</p></abstract><kwd-group><kwd>magnitude estimation</kwd><kwd>interval timing</kwd><kwd>drift-diffusion model</kwd><kwd>uncertainty</kwd><kwd>regression effect</kwd><kwd>range effect</kwd><kwd>optimality</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">Bundesministerium f&#x000fc;r Bildung und Forschung<named-content content-type="fundref-id">10.13039/501100002347</named-content></funding-source><award-id rid="cn001">01GQ1004A</award-id></award-group></funding-group><counts><fig-count count="4"/><table-count count="0"/><equation-count count="26"/><ref-count count="55"/><page-count count="11"/><word-count count="7815"/></counts></article-meta></front><body><sec id="s1"><title>1. Introduction</title><p>In daily life we continuously need to process the physical conditions of our environment; we make judgements about the magnitude of sensory stimuli, represent them neurally and base decisions upon them. Judgements about magnitudes are inherently unreliable due to noise from different sources such as the statistics of the physical world, the judgement process itself, the neural representation of the stimulus and finally the computations that drive behavior. A large body of experimental work highlights that magnitude estimation is subject to characteristic psychophysical effects. These effects are strikingly similar across different sensory modalities, suggesting common processing mechanisms that are shared by different sensory systems (for a recent review see Petzschner et al., <xref rid="B42" ref-type="bibr">2015</xref>). Amongst the behavioral characteristics the most astonishing yet unresolved is the <italic>regression effect</italic> also known as regression to the mean, central tendency, or Vierordt's law (von Vierordt, <xref rid="B52" ref-type="bibr">1868</xref>; Hollingworth, <xref rid="B19" ref-type="bibr">1910</xref>; Shi et al., <xref rid="B48" ref-type="bibr">2013</xref>). It states that over a range of stimuli, small stimuli are overestimated whereas large stimuli are underestimated (Figure <xref ref-type="fig" rid="F1">1A</xref>). Regression becomes more pronounced for ranges that comprise larger stimulus values (<italic>range effect</italic>; Teghtsoonian and Teghtsoonian, <xref rid="B50" ref-type="bibr">1978</xref>). As a consequence the same stimuli lead to different responses on average when embedded in different but overlapping stimulus distributions (Figure <xref ref-type="fig" rid="F1">1A</xref>) &#x02014; the responses depend on the stimulus context (Jazayeri and Shadlen, <xref rid="B21" ref-type="bibr">2010</xref>). Another omnipresent effect in magnitude estimation experiments is <italic>scalar variability</italic>, i.e., errors monotonically increase with the size of the stimulus, attributed to the famous Weber-Fechner law Figure <xref ref-type="fig" rid="F1">1B</xref>; (Weber, <xref rid="B53" ref-type="bibr">1851</xref>; Fechner, <xref rid="B12" ref-type="bibr">1860</xref>). Finally, magnitude estimation is influenced by the sequence in which stimuli are presented (Cross, <xref rid="B9" ref-type="bibr">1973</xref>; Hellstr&#x000f6;m, <xref rid="B17" ref-type="bibr">2003</xref>; Dyjas et al., <xref rid="B11" ref-type="bibr">2012</xref>). According to such <italic>sequential effects</italic> the estimate of the stimulus in a particular trial is affected by the previous trial. This results in under- or overestimation of the current stimulus depending on the previous stimulus (Figure <xref ref-type="fig" rid="F1">1C</xref>).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Psychophysical characteristics of magnitude estimation</bold>. The typical properties of magnitude estimation are illustrated as they are reproduced by the model presented in this paper. The description is based on subsecond interval timing (cf., Jazayeri and Shadlen, <xref rid="B21" ref-type="bibr">2010</xref>). <bold>(A)</bold> Individual reproduced values for each trial and stimulus (small dots, 100 per stimulus value), and their averages (large circles connected by lines) are shown for a simulation with three stimulus ranges. The regression effect is the deviation of the averages from the line of equality (diagonal gray dashed line) toward the mean of the respective stimulus range. It becomes stronger with larger means of the stimulus range, i.e., range effect. The analytical approximation of the model is in line with the simulated data (black solid lines). The memory parameter <italic>a</italic> was chosen to minimize MSE<sub><italic>r</italic></sub> for each range (derived in Section &#x0201c;3.1&#x0201d;). Stimulus ranges and memory weights <italic>a</italic> are given in the top-left corner of the plot. Other parameters are <italic>A</italic><sub><italic>m</italic></sub> = <italic>A</italic><sub><italic>r</italic></sub> = 0.25, &#x003c3;<sub><italic>m</italic></sub> = 1, and &#x003c3;<sub><italic>r</italic></sub> = 0.5. <italic>Inset:</italic> Average deviations (BIAS) from the line of equality for each stimulus and test range. Solid lines are again analytical predictions. <bold>(B)</bold> Standard deviation and coefficient of variation (standard deviation divided by the mean) corresponding to <bold>(A)</bold>. Black solid lines are again analytical predictions. <bold>(C)</bold> Sequential effects. Plotting the response bias for a certain stimulus as a function of the stimulus in the previous trial, reveals effects of stimulus order in the simulations (thick lines). The simulation results can be analytically approximated (thin lines). Results for the range 494 &#x02212; 847 ms are displayed. For each stimulus value 10,000 trials were simulated.</p></caption><graphic xlink:href="fnint-10-00006-g0001"/></fig><p>The above behavioral characteristics likely result from an optimal strategy when noisy estimates are made about stimuli that itself depend on the statistics of the environment. Recently such optimality strategies were successfully explained in Bayesian frameworks (Jazayeri and Shadlen, <xref rid="B21" ref-type="bibr">2010</xref>; Petzschner and Glasauer, <xref rid="B41" ref-type="bibr">2011</xref>; Cicchini et al., <xref rid="B8" ref-type="bibr">2012</xref>). Bayesian models incorporate a-priori knowledge about the stimuli into the estimation process, which seems to be crucial in explaining the aforementioned behavioral phenomena. However, the cited Bayesian approaches represent conceptual descriptions; inference about brain implementation is challenging.</p><p>The present paper introduces a theoretical approach that formulates magnitude estimation with noisy integrators (drift-diffusion processes). The model comprises two successive stages, measurement and reproduction. During measurement the current stimulus is estimated via noisy integration. The estimate is then combined with information from previous trials and used as threshold in the reproduction stage. The first passage of the threshold during reproduction determines the magnitude of the reproduced stimulus. Since the threshold depends on both the current and previous trials, it acts as an internal reference memory that is updated with every new stimulus. As we will see below, the model reproduces the behavioral characteristics of magnitude estimation (Figure <xref ref-type="fig" rid="F1">1</xref> anticipates these results) and interprets them as a consequence of an optimization strategy to minimize reproduction errors given noisy estimates and stimulus statistics.</p></sec><sec id="s2"><title>2. Materials and methods</title><p>The analytical methods employed in this paper rely on standard mathematical and statistical techniques. Simulation and numerical analysis was performed with Python 2.7 using the packages: Numpy 1.9, Scipy 0.15, Statsmodels 0.6 (Seabold and Perktold, <xref rid="B45" ref-type="bibr">2010</xref>), and Matplotlib 1.4 (Hunter, <xref rid="B20" ref-type="bibr">2007</xref>). The model's stochastic differential equations, Equations (1, 4), were simulated via the approximation</p><disp-formula id="E1"><mml:math id="M1"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:msqrt><mml:mrow><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="-tex-caligraphic">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>A time step &#x00394;<italic>t</italic> = 5 ms was used, to appropriately sample the Gaussian process <inline-formula><mml:math id="M2"><mml:mrow><mml:mi mathvariant="-tex-caligraphic">N</mml:mi></mml:mrow></mml:math></inline-formula>(0, 1), and capture noise sources on fast time scales like sensory noise and irregular spiking dynamics, at reasonable computing times.</p><sec><title>2.1. Definition of the model</title><p>Estimating the magnitude of a stimulus comprises two stages: First the stimulus is measured and afterwards the measurement is reported, e.g., reproduced by matching the strength of the stimulus. In the present paper, both measurement and reproduction are modeled as drift-diffusion processes (e.g., Bogacz et al., <xref rid="B4" ref-type="bibr">2006</xref>). During measurement drift-diffusion is left running as long as the stimulus is presented. Whereas, in the reproduction stage the drift-diffusion process is not stopped from outside but lasts until it hits a threshold from below. This threshold depends on the stimulus as estimated in the measurement stage and also includes the history of thresholds from previous trials, serving as an internal reference. Figure <xref ref-type="fig" rid="F2">2</xref> gives an overview of the model. For simplicity, the description below focuses on the estimation of temporal intervals (<italic>interval timing</italic>; Merchant et al., <xref rid="B38" ref-type="bibr">2013</xref>). Numbers refer to interval timing in the subsecond range after Jazayeri and Shadlen (<xref rid="B21" ref-type="bibr">2010</xref>). However, application to the estimation of, e.g., sound intensity or spatial distances, is straightforward by reinterpreting the variables accordingly.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Architecture of the model</bold>. The model comprises the measurement of the stimulus followed by its reproduction. Both stages are connected via the threshold &#x003b8; for the reproduction stage (dashed lines), which combines the measurement of the current stimulus <italic>m</italic><sub><italic>n</italic></sub> with the threshold &#x003b8;<sub><italic>n</italic>&#x02212;1</sub> from the previous trial, i.e., the reference. Example traces are displayed for intervals of 494 ms (blue) and 847 ms (red). Kernel density estimates are provided for the distributions of the model's stochastic variables (derived from 100 simulation runs for each stimulus in the range 49 4&#x02212; 847 ms from Figure <xref ref-type="fig" rid="F1">1</xref>). Thick shaded lines in the measurement stage are theoretical distributions. Dotted vertical lines and shaded areas in the reproduction stage give predicted mean &#x000b1; std.</p></caption><graphic xlink:href="fnint-10-00006-g0002"/></fig><sec><title>2.1.1. Measurement</title><p>The measurement stage comprises a drift process with rate <italic>A</italic><sub><italic>m</italic></sub> that is corrupted by noise (diffusion) realized as a Wiener process <italic>W</italic> with an amplitude &#x003c3;<sub><italic>m</italic></sub>. The dynamics are described by a stochastic differential equation</p><disp-formula id="E2"><label>(1)</label><mml:math id="M3"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mstyle mathvariant="normal"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mstyle mathvariant="normal"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mstyle mathvariant="normal"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The process is assumed to finish with the end of the stimulus and its final state yields the measurement. We can calculate the latter by integrating the above formula between stimulus start at <italic>t</italic> = 0 and end at <italic>t</italic> = <italic>T</italic> (Broderick et al., <xref rid="B5" ref-type="bibr">2009</xref>) and obtain</p><disp-formula id="E3"><label>(2)</label><mml:math id="M4"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msqrt><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="-tex-caligraphic">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>For convenience let us write <italic>m</italic><sub><italic>T</italic></sub> when we are considering a trial in which the interval <italic>T</italic> was presented, i.e., <italic>m</italic>(<italic>T</italic>). The final value <italic>m</italic><sub><italic>T</italic></sub> of the measurement process is Gauss-distributed <inline-formula><mml:math id="M5"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="-tex-caligraphic">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with mean <inline-formula><mml:math id="M6"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:math></inline-formula> and variance <inline-formula><mml:math id="M7"><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi></mml:math></inline-formula>. This value is incorporated into the threshold of the reproduction phase as detailed below.</p><p>For later use, let us also derive the overall variance of the measurement Var (<italic>m</italic>) here. To calculate Var (<italic>m</italic>), we apply the law of total variance and get</p><disp-formula id="E4"><label>(3)</label><mml:math id="M8"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></sec><sec><title>2.1.2. Reproduction</title><p>Similarly to the measurement stage, reproduction is modeled as drift-diffusion with corresponding drift <italic>A</italic><sub><italic>r</italic></sub> and noise amplitude &#x003c3;<sub><italic>r</italic></sub>. However, here, the process is not stopped after a certain time but limited by an upper bound, i.e., a threshold &#x003b8; (Broderick et al., <xref rid="B5" ref-type="bibr">2009</xref>),</p><disp-formula id="E5"><label>(4)</label><mml:math id="M9"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mstyle mathvariant="normal"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mstyle mathvariant="normal"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mstyle mathvariant="normal"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>&#x000a0;and&#x000a0;</mml:mtext><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The time of threshold crossing from below, i.e., the first-passage time of the drift-diffusion process, represents the response or the reproduced stimulus interval, respectively. Since we have a drift-diffusion process with a single threshold &#x003b8; &#x0003e; 0, the distribution of its first-passage times has an inverse Gaussian density <inline-formula><mml:math id="M10"><mml:mi>I</mml:mi><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo class="qopname">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and is characterized by <italic>X</italic> ~ <italic>IG</italic>(&#x003bc;, &#x003bb;) : E (<italic>X</italic>) = &#x003bc;, Var (<italic>X</italic>) = &#x003bc;<sup>3</sup>&#x02215;&#x003bb; (Tuckwell, <xref rid="B51" ref-type="bibr">1988</xref>). In the present case, we have (cf. Simen et al., <xref rid="B49" ref-type="bibr">2011</xref>):</p><disp-formula id="E6"><label>(5)</label><mml:math id="M11"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>I</mml:mi><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The reproduced stimulus interval that corresponds to the presentation of a stimulus <italic>T</italic> is denoted by <italic>r</italic><sub><italic>T</italic></sub> and &#x003b8;<sub><italic>T</italic></sub> is the threshold in this trial.</p></sec><sec><title>2.1.3. Threshold for reproduction</title><p>As already mentioned above, in a trial <italic>n</italic> the threshold &#x003b8;<sub><italic>n</italic></sub> in the reproduction stage depends on the stimulus' measurement <italic>m</italic><sub><italic>n</italic></sub> and the threshold &#x003b8;<sub><italic>n</italic>&#x02212;1</sub> of the previous trial</p><disp-formula id="E7"><label>(6)</label><mml:math id="M12"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The weight <italic>a</italic> is limited to the interval 0 &#x0003c; <italic>a</italic> &#x02264; 1. A value of <italic>a</italic> = 0 has to be excluded since for <italic>a</italic> = 0 only the first stimulus would be taken into account and has an everlasting memory. The formulation in Equation (6) effectively introduces a weighted average preventing unbounded growth. A reference memory is formed and updated on a single trial basis. As we will see later, the memory weight <italic>a</italic> has an immediate impact on the relation between stimulus and response. The recursive definition in Equation (6) can also be given as an iterative formula</p><disp-formula id="E8"><label>(7)</label><mml:math id="M13"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></sec><sec><title>2.1.4. Further conditions for the model</title><p>It is assumed that drifts <italic>A</italic><sub><italic>m</italic></sub> and <italic>A</italic><sub><italic>r</italic></sub> are positive numbers. In addition, the drift-diffusion processes are supposed to act in drift-dominated regimes with <inline-formula><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02215;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02215;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msqrt><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="-tex-caligraphic">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Otherwise, the measurement stage may yield negative values, resulting in negative thresholds &#x003b8;, which can not be hit from below. For the sake of simplicity and without loss of generality, the model is not formulated with a lower bound that only allows for positive values. An account of the influence of a lower bound on the first passage time distribution of a drift-diffusion process can be found in Simen et al. (<xref rid="B49" ref-type="bibr">2011</xref>).</p></sec></sec><sec><title>2.2. Analytical approximations</title><p>Reproduced stimuli in the model are random variables drawn from the distribution of first passage times in the reproduction stage (Equation 5). Determining the distribution of these first passage times <italic>p</italic>(<italic>r</italic><sub><italic>T</italic></sub>) is complicated since the threshold &#x003b8;<sub><italic>T</italic></sub> itself is a random variable. Obtaining <italic>p</italic>(<italic>r</italic><sub><italic>T</italic></sub>) would thus require calculating <italic>p</italic>(<italic>r</italic><sub><italic>T</italic></sub>) = &#x0222b; <italic>d&#x003b8;</italic><sub><italic>T</italic></sub><italic>p</italic>(<italic>r</italic><sub><italic>T</italic></sub> &#x02223; &#x003b8;<sub><italic>T</italic></sub>)<italic>p</italic>(&#x003b8;<sub><italic>T</italic></sub>) for which a general solution can not be provided. For a Gaussian threshold distribution the calculations are exemplified by Simen et al. (<xref rid="B49" ref-type="bibr">2011</xref>), resulting in smeared-out inverse Gaussian distributions. Qualitatively this results also holds true for other &#x0201c;reasonable&#x0201d; threshold distributions (cf. Figure <xref ref-type="fig" rid="F2">2</xref>). To provide generic analytical solutions for the present model, the section below focuses on expected values and variances.</p><sec><title>2.2.1. Expected value of the threshold</title><p>With randomized stimulus presentations and sufficiently large numbers of preceding trials, we obtain the expected value of the threshold in the current trial from Equation (7)</p><disp-formula id="E9"><label>(8)</label><mml:math id="M15"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>with &#x02329;&#x000b7;&#x0232a; denoting the trial average. The sum in Equation (8) is a geometric series and can be rewritten to</p><disp-formula id="E10"><mml:math id="M16"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>We further simplify by taking the limit <italic>n</italic> &#x02192; &#x0221e; and get <inline-formula><mml:math id="M17"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mrow><mml:mo>lim</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>&#x0221e;</mml:mi></mml:mrow></mml:msub><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:math></inline-formula>. From the last expression we derive the expected value of the threshold in a trial in which the interval <italic>T</italic> was presented, i.e., <inline-formula><mml:math id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003d1;</mml:mi><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo class="qopname">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003d1;</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Using &#x02329;<italic>m</italic>&#x0232a; = <italic>A</italic><sub><italic>m</italic></sub> &#x02329; <italic>T</italic>&#x0232a; and <inline-formula><mml:math id="M19"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:math></inline-formula>, we end up with</p><disp-formula id="E11"><label>(9)</label><mml:math id="M20"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Note, that the average threshold <inline-formula><mml:math id="M21"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> depends on both the current stimulus <italic>T</italic> and the trial average &#x02329;<italic>T</italic>&#x0232a;. The latter is equal to the mean of the stimulus distribution &#x02329;<italic>T</italic>&#x0232a; = E (<italic>T</italic>). The description below therefore uses E (<italic>T</italic>) instead of &#x02329;<italic>T</italic>&#x0232a;.</p></sec><sec><title>2.2.2. Variance of the threshold</title><p>The above calculations only gave the mean threshold for a particular trial. In a next step let us derive from Equation (7) the corresponding variance. Calculating the variance of Equation (7) we obtain a slightly more elaborate geometric series</p><disp-formula id="E12"><mml:math id="M22"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Taking the limit <italic>n</italic> &#x02192; &#x0221e;, yields</p><disp-formula id="E13"><mml:math id="M23"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">lim</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>&#x0221e;</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>From the last expression we determine the variance of the threshold in a trial with stimulus interval <italic>T</italic>, i.e., Var (&#x003d1;<sub><italic>T</italic></sub>). The variance Var (<italic>m</italic><sub><italic>n</italic></sub>) is given by the variance of the current measurement <inline-formula><mml:math id="M24"><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi></mml:math></inline-formula>, see Equation (2), and Var (<italic>m</italic>) is given by Equation (3), i.e., <inline-formula><mml:math id="M25"><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Insertion into the above formula yields</p><disp-formula id="E14"><label>(10)</label><mml:math id="M26"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Thus, similarly to the average threshold (9) its variance also depends on both the current stimulus <italic>T</italic> and the mean of the stimulus distribution E (<italic>T</italic>). A third influence comes from the variance of the stimuli Var (<italic>T</italic>).</p></sec><sec><title>2.2.3. Expected value and variance for the reproduction</title><p>We can use the solutions for expected value and variance of the threshold &#x003d1;<sub><italic>T</italic></sub> from Equations (9, 10) to extend the formulas for the expected value and variance during reproduction in Equation (5). To determine the average reproduced value <inline-formula><mml:math id="M27"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for a stimulus <italic>T</italic>, we apply the law of total expectation and obtain</p><disp-formula id="E15"><label>(11)</label><mml:math id="M28"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mo class="qopname">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>From Equation (11) we also find an expression for the bias corresponding to a stimulus <italic>T</italic></p><disp-formula id="E16"><label>(12)</label><mml:math id="M29"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>BIAS</mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mtext>&#x02009;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>T</mml:mi><mml:mtext>&#x02009;</mml:mtext><mml:mo>=</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>&#x000a0;</mml:mtext><mml:mi>E</mml:mi><mml:mi>T</mml:mi><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Equations (11, 12) directly relate the stimulus <italic>T</italic> to its reproduced value. Expected value and bias of the reproduced stimuli not only depend on the current stimulus <italic>T</italic> but also on the expected value of the stimulus distribution E (<italic>T</italic>). The latter adds an offset to the linear relations. The memory weight <italic>a</italic> contributes to the slope of the relations and thus determines the strength of the regression effect. Values of <italic>a</italic> closer to zero result in stronger regression to mean; for values of <italic>a</italic> closer to one, regression vanishes and reproduction is veridical. As we will see in Section 3, the weight <italic>a</italic> can be constrained by other model parameters to minimize reproduction errors. Regression and range effects are consequences of such optimization efforts.</p><p>Expected value and bias of the reproduction according to Equations (11, 12) also depend on the ratio of drifts from both production and reproduction, <italic>A</italic><sub><italic>m</italic></sub> and <italic>A</italic><sub><italic>r</italic></sub>, respectively. Calculating the expectations</p><disp-formula id="E17"><label>(13)</label><mml:math id="M30"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo class="qopname">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle class="mbox"><mml:mtext>and</mml:mtext></mml:mstyle><mml:mtext>&#x000a0;</mml:mtext><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo class="qopname">BIAS</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>shows that for mismatches between the drifts <italic>A</italic><sub><italic>m</italic></sub> and <italic>A</italic><sub><italic>r</italic></sub> we get overall deviations between stimuli <italic>T</italic> and reproductions <italic>r</italic><sub><italic>T</italic></sub>. These non-zero average biases may explain overall over-estimation (for <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub> &#x0003e; 1) and overall under-estimation (<italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub> &#x0003c; 1), respectively.</p><p>To determine the variance Var (<italic>r</italic><sub><italic>T</italic></sub>) : = Var (<italic>r</italic>&#x02223;<italic>T</italic>) in a trial in which the stimulus <italic>T</italic> was presented, we apply the law of total variance and obtain</p><disp-formula id="E18"><label>(14)</label><mml:math id="M31"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Like the variance of the threshold, also the variance Var (<italic>r</italic><sub><italic>T</italic></sub>) depends on the current stimulus <italic>T</italic> and the statistics of the stimulus distribution given by E (<italic>T</italic>) and Var (<italic>T</italic>). Note that the monotonic relation (14) between stimulus <italic>T</italic> and variance Var (<italic>r</italic><sub><italic>T</italic></sub>) of its reproduction is equivalent to scalar variability.</p><p>With formulas (11&#x02013;14), we have a full characterization of the model linking the stimuli <italic>T</italic> to their reproduced values <italic>r</italic><sub><italic>T</italic></sub>. The description also details the dependence on the different model parameters, i.e., the internal processing. Figure <xref ref-type="fig" rid="F1">1</xref> gives examples how formulas (11&#x02013;14) fit to simulations of the model.</p></sec></sec></sec><sec id="s3"><title>3. Results</title><p>As displayed in Figure <xref ref-type="fig" rid="F1">1</xref>, the model described in Section 2 can reproduce the typical psychophysical findings for magnitude estimation: regression effect, range effect, scalar variability, and sequential effects. However, it remains open how we can motivate the choice of parameters that fit the psychophysical findings. The upcoming paragraphs focus on this question.</p><sec><title>3.1. How to minimize reproduction errors?</title><p>Different factors of uncertainty challenge precise magnitude estimation as it is formulated by the model &#x02014; such as the statistics of the stimuli and internal sources of noise &#x003c3;<sub><italic>m</italic></sub> and &#x003c3;<sub><italic>r</italic></sub>. How could a subject cope with these noise sources to minimize estimation errors?</p><p>For optimal magnitude reproduction one needs to minimize the mean squared error between a stimulus <italic>T</italic> and its reproduced value <italic>r</italic><sub><italic>T</italic></sub>, i.e., <inline-formula><mml:math id="M32"><mml:mo class="qopname">MSE</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo class="qopname">E</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. The mean squared error can be partitioned into a variance and a bias term</p><disp-formula id="E19"><label>(15)</label><mml:math id="M33"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mo class="qopname">MSE</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo class="qopname">BIAS</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The description of the variance Var (<italic>r</italic>) in Equation (15) depends on the purpose of optimization. In fact, it is not the total variance for the reproduction that should be minimized here. Rather subjects would want to minimize the variability of individual measurements of a particular stimulus E (Var (<italic>r</italic><sub><italic>T</italic></sub>)) = E (Var (<italic>r</italic>&#x02223;<italic>T</italic>)); cf. Jazayeri and Shadlen (<xref rid="B21" ref-type="bibr">2010</xref>). From Equation (14) the variance E (Var (<italic>r</italic><sub><italic>T</italic></sub>)) is given by</p><disp-formula id="E20"><label>(16)</label><mml:math id="M34"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;</mml:mtext><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The term <inline-formula><mml:math id="M35"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>BIAS</mml:mtext></mml:mrow><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> in Equation (15) refers to the mean squared or quadratic mean of all biases in a test range, i.e., <inline-formula><mml:math id="M36"><mml:mrow><mml:mtext>E&#x02009;</mml:mtext><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>BIAS</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Using Equation (12) it is given by</p><disp-formula id="E21"><label>(17)</label><mml:math id="M37"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mtext>BIAS</mml:mtext><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>BIAS</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>E</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x02009;&#x02009;&#x02009;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>E</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>With Equations (16, 17) the MSE<sub><italic>r</italic></sub> reads as follows</p><disp-formula id="E22"><label>(18)</label><mml:math id="M38"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mtext>MSE</mml:mtext><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>|</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mtext>BIAS</mml:mtext><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>E</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>E</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Let us explore the possibility that the memory <italic>a</italic> of the system can be adapted to minimize the mean squared error, i.e., <inline-formula><mml:math id="M39"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mstyle class="mbox"><mml:mtext>min</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo class="qopname">MSE</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Recall that <italic>a</italic> is connected to the slope of the relation between stimulus <italic>T</italic> and its average reproduction <inline-formula><mml:math id="M40"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and thus determines the strength of the regression effect; cf. Equation (11). To find <italic>a</italic><sub>min</sub> we take the first derivative with respect to <italic>a</italic> of Equation (18) and set it to zero</p><disp-formula id="E23"><mml:math id="M41"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mtext>&#x000a0;</mml:mtext><mml:munder accentunder="true"><mml:munder accentunder="true"><mml:mo>!</mml:mo><mml:mo stretchy="true">_</mml:mo></mml:munder><mml:mo stretchy="true">_</mml:mo></mml:munder><mml:mtext>&#x000a0;</mml:mtext><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mtext>MSE</mml:mtext><mml:mi>r</mml:mi></mml:msub><mml:mtext>&#x02009;</mml:mtext><mml:mo>=</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:mtext>E&#x000a0;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>|</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mtext>BIAS</mml:mtext><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;&#x02009;E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>&#x02009;Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Solving <italic>a</italic> = <italic>a</italic><sub>min</sub>, weobtain</p><disp-formula id="E24"><label>(19)</label><mml:math id="M42"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mrow><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>Simulation results confirm the derivation that led to Equation (19); cf. Figure <xref ref-type="fig" rid="F3">3A</xref>.</p><p>According to Equation (19) a subject may reduce its overall reproduction error by adjusting the strength of regression, depending on the values of three different relations: (i) the drift ratio <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub>, which may account for overall biases, cf. Equation (13); (ii) the inverse signal-to-noise ratio (SNR) of the measurement &#x003c3;<sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>m</italic></sub>, quantifying internal noise; and (iii) the inverse of the index of dispersion (variance-to-mean ratio, Fano factor) of the stimulus distribution E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>), characterizing the stimulus distribution, and constituting an external source of uncertainty &#x02014; in contrast to the other two ratios that are due to internal processing. Note that noise in the reproduction, i.e., &#x003c3;<sub><italic>r</italic></sub>, does not influence <italic>a</italic><sub>min</sub>, which intuitively makes sense since the update-step of the memory weight <italic>a</italic> precedes the reproduction stage.</p></sec><sec><title>3.2. Optimality predicts range and regression effects</title><p>To evaluate how the optimal memory weight <italic>a</italic><sub>min</sub> depends on the above ratios, let us consider their individual influences on the reproduction error (Figure <xref ref-type="fig" rid="F3">3</xref>) and determine their interaction (Figure <xref ref-type="fig" rid="F4">4</xref>). Figure <xref ref-type="fig" rid="F3">3</xref> displays the reproduction error as a function of <italic>a</italic><sub>min</sub> for different choices of the model parameters. Instead of the mean squared error its square root <inline-formula><mml:math id="M43"><mml:msub><mml:mtext>RMSE</mml:mtext><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mtext>MSE</mml:mtext><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:math></inline-formula> is plotted, which allows for the intuitive visualization of the Pythagorean sum (15) on quarter circles of similar MSE<sub><italic>r</italic></sub>-levels (Figure <xref ref-type="fig" rid="F3">3</xref>, right panels).</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Reproduction error and model parameters</bold>. Root-mean-squared error (left panels) and its representation on a quarter circle <inline-formula><mml:math id="M44"><mml:msqrt><mml:mrow><mml:mo class="qopname">Var</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:math></inline-formula> vs. BIAS<sub><italic>r</italic></sub> (right panels) are displayed for the optimal memory weight <italic>a</italic><sub>min</sub> conditioned on E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>) <bold>(A)</bold>, the inverse SNR of measurement &#x003c3;<sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>m</italic></sub>
<bold>(B)</bold>, the drift ratio <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub>
<bold>(C)</bold>, and the noise level &#x003c3;<sub><italic>r</italic></sub> during reproduction <bold>(D)</bold>. Solid lines show the predictions of <italic>a</italic><sub>min</sub> for different values of the respective ratio or parameter. Small numbers mark the range of values. Large dots mark the theoretical predictions from Equation (19) and correspond to the memory weights <italic>a</italic> taken in Figure <xref ref-type="fig" rid="F1">1</xref>. Colors as in Figure <xref ref-type="fig" rid="F1">1</xref>. In <bold>(A)</bold> also simulation results are displayed for the three stimulus ranges from Figure <xref ref-type="fig" rid="F1">1</xref> and different values of <italic>a</italic> (small dots, fainter colors correspond to smaller values of <italic>a</italic>). The simulations confirm the theoretical predictions for the optimal values <italic>a</italic><sub>min</sub>.</p></caption><graphic xlink:href="fnint-10-00006-g0003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Optimality characteristics</bold>. <bold>(A,B)</bold> Optimal weight <italic>a</italic><sub>min</sub> in dependence on the stimulus distribution. <bold>(A)</bold> Optimal weight <italic>a</italic><sub>min</sub> with SNR<sup>&#x02212;1</sup> fixed to 4 (<italic>upper panel</italic>) and drift ratio of 1 (<italic>lower panel</italic>), respectively. Same color bar as in (<bold>C</bold>, left). <bold>(B)</bold> Regions of <italic>a</italic><sub>min</sub> &#x02208; (0, 1] for three different SNR<sup>&#x02212;1</sup> and drift ratios, respectively; thick colored lines <italic>a</italic><sub>min</sub> = 0, thin colored lines <italic>a</italic><sub>min</sub> = 1. <bold>(C)</bold> Optimal memory weight <italic>a</italic><sub>min</sub> as a function of drift ratio and inverse signal-to-noise ratio. <italic>Left:</italic> Optimal weight <italic>a</italic><sub>min</sub> for the stimulus range 494 &#x02212; 847 ms. <italic>Right:</italic> Regions of <italic>a</italic><sub>min</sub> &#x02208; (0, 1] for each stimulus range from Figure <xref ref-type="fig" rid="F1">1</xref>. Colors as in Figure <xref ref-type="fig" rid="F1">1</xref>. <bold>(D)</bold> Optimal weight <italic>a</italic><sub>min</sub> as a function of the drift ratio and inverse SNR, respectively, for all three stimulus ranges from Figure <xref ref-type="fig" rid="F1">1</xref>. Again an inverse SNR of 4 and a drift ratio of 1, respectively, have been used. Gray dashed lines mark those values. Simulation data (1000 runs per stimulus) confirm the theoretical prediction (shaded areas of same color, 10% percentile for minimal MSEs).</p></caption><graphic xlink:href="fnint-10-00006-g0004"/></fig><p>The MSE<sub><italic>r</italic></sub> increases with larger ratio E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>); Figure <xref ref-type="fig" rid="F3">3A</xref>. The dependence serves as an explanation of range effects in magnitude estimation, i.e., dependencies on the stimulus statistics, &#x02014; an experimentally testable prediction (cf. 4). A larger ratio E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>) corresponds to a narrower stimulus distribution and thus smaller differences between particular stimuli, which in turn are harder to distinguish. This increases uncertainty about the stimuli, which a subject could balance by increasing regression and hence treat different stimuli as more similar (closer to their mean) as they in fact are. Stronger regression is obtained by letting the memory weight <italic>a</italic><sub>min</sub> tend to zero. Note that stronger regression, i.e., smaller <italic>a</italic><sub>min</sub>, results in a stronger change in the BIAS<sub><italic>r</italic></sub>-component compared to the variance component <inline-formula><mml:math id="M45"><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:math></inline-formula> (Figure <xref ref-type="fig" rid="F3">3A</xref>, right panel). Figures <xref ref-type="fig" rid="F4">4A,B</xref> examines the relation between E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>) and the other model parameters with regard to the optimal weight <italic>a</italic><sub>min</sub>. Only regions with <italic>a</italic><sub>min</sub> &#x02208; (0, 1] are displayed to obtain parameter combinations where optimization is possible. The parameter regions where MSE<sub><italic>r</italic></sub> could be optimized shrink with larger E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>) and are further diminished when conditioned on the drift ratio and SNR<sup>&#x02212;1</sup> (Figures <xref ref-type="fig" rid="F4">4A,B</xref>).</p><p>Larger measurement noise, i.e., SNR<sup>&#x02212;</sup> = &#x003c3;<sub><italic>m</italic></sub>/<italic>A</italic><sub><italic>m</italic></sub>, increases MSE<sub><italic>r</italic></sub> (Figure <xref ref-type="fig" rid="F3">3B</xref>); to balance this the optimal memory weight <italic>a</italic><sub>min</sub> decreases accordingly (Figures <xref ref-type="fig" rid="F3">3B</xref>, <xref ref-type="fig" rid="F4">4C,D</xref>). For larger measurement noise, reproduction errors are minimized by increasing regression. The regression effect can thus be interpreted as a strategy to reduce reproduction errors given noisy estimates. In contrast, very precise estimation would lead to veridical judgements about the stimuli. Note, the connection between the inverse SNR and the Weber fraction from psychophysics. Larger SNR<sup>&#x02212;1</sup> corresponds to reduced sensory resolution, i.e., lower discriminability, which results in a larger Weber fraction.</p><p>The optimal weight <italic>a</italic><sub>min</sub> also depends on the drift ratio <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub>, which if not equal to one, leads to systematic biases, i.e., overall under- or overestimation; cf. Equation (13), and thus larger MSE<sub><italic>r</italic></sub> (Figure <xref ref-type="fig" rid="F3">3C</xref>). To compensate for the introduced overall bias (Figure <xref ref-type="fig" rid="F3">3C</xref>, right panel), drift ratios greater than one require smaller <italic>a</italic><sub>min</sub> and drift ratios smaller than one require larger <italic>a</italic><sub>min</sub> (Figures <xref ref-type="fig" rid="F4">4C,D</xref>). Note that the impact of the drift ratio <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub> on <italic>a</italic><sub>min</sub> might point in the opposite direction as that of the external and internal uncertainties, E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>) and &#x003c3;<sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>m</italic></sub>, respectively.</p><p>In summary, the dependence of <italic>a</italic><sub>min</sub> on the noise level during measurement &#x003c3;<sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>m</italic></sub> predicts the regression effect and the dependence on the stimulus statistics E (<italic>T</italic>)&#x02215;Var (<italic>T</italic>) explains the range effect. The dependence on the ratio of drifts <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub> explains systematic effects like overall over- and underestimation. As already mentioned above noise in the reproduction &#x003c3;<sub><italic>r</italic></sub> does not affect <italic>a</italic><sub>min</sub>; cf. Equation (19). Nevertheless, MSE<sub><italic>r</italic></sub> gets increased with larger reproduction noise (Figure <xref ref-type="fig" rid="F3">3D</xref>). Noise in measurement and reproduction therefore differently affects the bias and the variance of stimulus reproduction.</p></sec><sec><title>3.3. Explaining sequential effects</title><p>A fourth class of psychophysical characteristics that was mentioned in the introduction was not considered so far, i.e., effects related to stimulus order (Cross, <xref rid="B9" ref-type="bibr">1973</xref>; Petzschner et al., <xref rid="B42" ref-type="bibr">2015</xref>). Due to the trial-by-trial update rule incorporated in the model, previous trials unavoidably affect the reproduction of the current stimulus. Figure <xref ref-type="fig" rid="F1">1C</xref> exemplifies this via the biases for a particular stimulus conditioned on the stimulus in the previous trial. In general the bias for the current stimulus is proportional to the immediately preceding stimulus. To evaluate this effect analytically, let us reconsider Equation (7). We take out trial <italic>n</italic> &#x02212; 1 from the sum and proceed in similar steps to the derivation in Section &#x0201c;2.2,&#x0201d; which lead to Equation (9) and finally to Equation (11). The average response to stimulus <italic>T</italic> given that stimulus <italic>T</italic><sub><italic>n</italic>&#x02212;1</sub> was presented in the preceding trial is obtained from</p><disp-formula id="E25"><label>(20)</label><mml:math id="M46"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>&#x00394;</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>&#x00394;</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>We express the previous stimulus relative to the mean E (<italic>T</italic>) here, i.e., &#x00394;<italic>T</italic><sub><italic>n</italic>&#x02212;1</sub> = <italic>T</italic><sub><italic>n</italic>&#x02212;1</sub> &#x02212; E (<italic>T</italic>). The effect of the previous onto the current trial, we evaluate by the corresponding BIAS</p><disp-formula id="E26"><label>(21)</label><mml:math id="M47"><mml:mrow><mml:msub><mml:mrow><mml:mtext>BIAS</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>T</mml:mi><mml:mtext>&#x02009;</mml:mtext><mml:mo>=</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:msub><mml:mrow><mml:mtext>&#x000a0;BIAS</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>&#x00394;</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>Thus, when a stimulus value <italic>T</italic><sub><italic>n</italic>&#x02212;1</sub> larger than E (<italic>T</italic>) was presented in the previous trial a positive term is added to BIAS<sub><italic>rT</italic></sub>. For a stimulus <italic>T</italic><sub><italic>n</italic>&#x02212;1</sub> &#x0003c; E (<italic>T</italic>) the term is negative and the bias will become smaller (Figure <xref ref-type="fig" rid="F1">1C</xref>).</p></sec></sec><sec id="s4"><title>4. Discussion</title><p>The model introduced in the present paper describes magnitude estimation as a two-stage process, measurement and reproduction, consisting of noisy integrators linked by an internal reference (<italic>implicit standard</italic> or <italic>prior</italic>) that is updated on a trial-by-trial basis.</p><p>Trial-by-trial update rules have been used by others to explain aspects of magnitude estimation (Hellstr&#x000f6;m, <xref rid="B17" ref-type="bibr">2003</xref>; Dyjas et al., <xref rid="B11" ref-type="bibr">2012</xref>; Bausenhart et al., <xref rid="B1" ref-type="bibr">2014</xref>) and are also at the core of the Bayesian model by Petzschner and Glasauer (<xref rid="B41" ref-type="bibr">2011</xref>), where such updating is used to adjust prior knowledge about the stimulus distribution. Iterative updating in the present model estimates the moments of the stimulus distribution to form an internal reference. At least humans are known to be able to maintain (Morgan et al., <xref rid="B39" ref-type="bibr">2000</xref>) and to quickly adapt such an internal reference (Berniker et al., <xref rid="B2" ref-type="bibr">2010</xref>).</p><p>Noisy integrative processes well describe decision-making at the behavioral level (Brunton et al., <xref rid="B6" ref-type="bibr">2013</xref>). Moreover, several brain regions show noisy integration during decision-making (Shadlen and Newsome, <xref rid="B47" ref-type="bibr">2001</xref>; Liu and Pleskac, <xref rid="B31" ref-type="bibr">2011</xref>; Shadlen and Kiani, <xref rid="B46" ref-type="bibr">2013</xref>; Hanks et al., <xref rid="B16" ref-type="bibr">2015</xref>) at least at the population level. Whether noisy integration is generated by ramp-like noisy integration in single neurons has been questioned recently (Latimer et al., <xref rid="B28" ref-type="bibr">2015</xref>). In any case, the present model suggests that noisy integration is also crucial to non-binary cognitive demands such as the representation and processing of magnitudes.</p><sec><title>4.1. Connection to psychophysical effects of magnitude estimation</title><p>The presented model reproduces the main behavioral characteristics of magnitude estimation Figure <xref ref-type="fig" rid="F1">1</xref>; (Petzschner et al., <xref rid="B42" ref-type="bibr">2015</xref>): Estimates tend toward the mean (regression effect) and this effect scales with the range of stimuli chosen (range effect). Errors monotonically increase with the size of the stimulus (scalar variability). In addition, the sequence in which stimuli are presented influences magnitude judgments. Such sequential effects are by design captured by the model due to the trial-by-trial update of the internal reference. The major insight from this paper therefore is that iterative updating can explain regression and range effects (see also Bausenhart et al., <xref rid="B1" ref-type="bibr">2014</xref>). As such both effects are consequences of strategies to minimize reproduction errors. With larger uncertainty about the stimuli, stronger regression helps to minimize reproduction errors and hence optimizes judgements. Uncertainty may stem from both internal and external sources, whose influence can be evaluated separately by the presented approach.</p><p>Internal noise is quantified by the signal-to-noise ratio (SNR) during measurement, i.e., inverse SNR in Equation (19), which corresponds to the Weber fraction in psychophysics and thus the discrimination abilities of a subject. Weber fractions depend on the stimulus modality and are subject-specific. &#x0201c;Modality effects&#x0201d; and individual differences are well known in interval timing literature (Shi et al., <xref rid="B48" ref-type="bibr">2013</xref>). Cicchini et al. (<xref rid="B8" ref-type="bibr">2012</xref>) showed that percussionists precisely reproduce temporal intervals and display very weak regression effects in contrast to normal subjects. In addition, the results depended on stimulus modality. For all subject groups, performance was better when intervals were given by auditory rather then visual stimulation. The results of Cicchini et al. (<xref rid="B8" ref-type="bibr">2012</xref>) are in line with the present model due to the connection between regression effect and Weber fraction. To explain their experimental data, Cicchini et al. (<xref rid="B8" ref-type="bibr">2012</xref>) proposed a Bayesian model that included information about the discrimination abilities (Weber fractions) and obtained very similar results to the present work. Increasing SNR (decreasing Weber fraction) during measurement would require adjusting the drift rate <italic>A</italic><sub><italic>m</italic></sub> such that it is as large as possible compared to the noise &#x003c3;<sub><italic>m</italic></sub>. However, the drift rate <italic>A</italic><sub><italic>m</italic></sub> will be limited from above by neuronal and network processes, and related time constants (Murray et al., <xref rid="B40" ref-type="bibr">2014</xref>). Analogous constraints were derived by Cicchini et al. (<xref rid="B8" ref-type="bibr">2012</xref>) on the width of the prior distribution. Parkinson patients tested off of their medication display strong regression effects (Malapani et al., <xref rid="B36" ref-type="bibr">1998</xref>, <xref rid="B35" ref-type="bibr">2002</xref>). In addition, the precision of the responses is reduced. This is in line with the present model, since stronger regression is predicted with reduced precision (i.e., increased variance or reduced SNR).</p><p>External uncertainty is due to stimulus context, i.e., the statistics of the stimuli, which is quantified by the ratio between mean and variance of the stimulus distribution in the present model. Larger ratios (narrower stimulus distributions) should lead to stronger regression. Intuitively this means that the width of the stimulus distribution becomes small compared to its mean and individual stimuli can not be discriminated anymore, hence uncertainty increases. The regression effect counteracts this by treating different stimuli similar to their mean. Note the similarity to the Weber-Fechner law, which predicts decreased discriminability with larger stimuli. In line with this view, more difficult magnitude estimation tasks should display stronger regression effects (Teghtsoonian and Teghtsoonian, <xref rid="B50" ref-type="bibr">1978</xref>; Petzschner et al., <xref rid="B42" ref-type="bibr">2015</xref>).</p><p>Systematic over- or underestimation are often found in magnitude estimation experiments (for examples see Jazayeri and Shadlen, <xref rid="B21" ref-type="bibr">2010</xref>; Petzschner and Glasauer, <xref rid="B41" ref-type="bibr">2011</xref>; Cicchini et al., <xref rid="B8" ref-type="bibr">2012</xref>). Such differences may, e.g., occur due to attentional and subject-related factors. In the model this would be attributed to differences in the drift rates from measurement and reproduction. Note that only differences are important, absolute scales of (neural) processing (Kiebel et al., <xref rid="B24" ref-type="bibr">2008</xref>; Murray et al., <xref rid="B40" ref-type="bibr">2014</xref>) are not crucial as long as they are similar across processing stages.</p><p>The standard deviation is a monotonically increasing function of the stimulus strength in the model presented here; cf. Equation (14) and Figure <xref ref-type="fig" rid="F1">1B</xref>. As such the model is in line with the Weber-Fechner law (scalar variability). However, the Weber-Fechner law predicts a linear increase of variability (standard deviation) as a function of magnitude. According to Equation (14) the increase of the standard deviation is sub-linear (square root) in the present model. This sub-linearity may be rather weak (cf. simulation data and theoretical predictions in Figure <xref ref-type="fig" rid="F1">1B</xref>) and thus may still be in line with experimental data, i.e., differentiating between linearity and weak sub-linearity may be hard from real data. Certain extensions of the model may help to obtain a linear relation. One possibility is introducing a drift ratio <italic>A</italic><sub><italic>m</italic></sub>&#x02215;<italic>A</italic><sub><italic>r</italic></sub> that scales with the stimulus <italic>T</italic>. Whether scalar variability applies to magnitude estimation without restrictions and across all ranges is not clear. This question is, for example, still a matter of debate in interval timing literature, where non-scalar variability has been reported for specific tasks or situations (like timing while counting or singing; Hinton and Rao, <xref rid="B18" ref-type="bibr">2004</xref>; Grondin and Killeen, <xref rid="B15" ref-type="bibr">2009</xref>).</p></sec><sec><title>4.2. Predictions</title><p>The formulation of the optimal memory weight <italic>a</italic><sub>min</sub> according to Equation (19) allows for a number of experimentally testable predictions: (i) Reproduced magnitudes should depend on the stimulus distribution. The experimental studies by Jazayeri and Shadlen (<xref rid="B21" ref-type="bibr">2010</xref>), Petzschner and Glasauer (<xref rid="B41" ref-type="bibr">2011</xref>), Cicchini et al. (<xref rid="B8" ref-type="bibr">2012</xref>) only increased the mean of the stimulus distribution between ranges, which would increase the mean-to-variance ratio and predict stronger regression, i.e., a decrease in <italic>a</italic> (cf. Figure <xref ref-type="fig" rid="F1">1A</xref>). Stimulus distributions with the same mean but larger variances should result in less regression. Indeed, for their experiments on range effects in loudness and distance estimation, Teghtsoonian and Teghtsoonian (<xref rid="B50" ref-type="bibr">1978</xref>) varied the width of the stimulus distribution instead of the mean. They found increasing power exponents with wider stimulus distributions. (ii) Regression to the mean should depend on the discrimination abilities of the individual. Subjects with precise perception of the stimulus magnitude under investigation should show less regression than subjects with reduced abilities; (e.g., Cicchini et al., <xref rid="B8" ref-type="bibr">2012</xref>). This should depend on stimulus modality (Cicchini et al., <xref rid="B8" ref-type="bibr">2012</xref>; Shi et al., <xref rid="B48" ref-type="bibr">2013</xref>) and change with training for a specific task. (iii) Seldom stimuli with a low probability of occurrence and with a magnitude way below or way above the stimulus distribution, should not influence the internal reference. (iv) For strong regression the convergence dynamics of the reference should be much slower then for subjects showing weak regression. The influence of previous stimuli should correlate with the level of regression as well as updating of the references after changing the stimulus distribution within an experimental session.</p></sec><sec><title>4.3. Connection to bayesian models of magnitude estimation</title><p>Magnitude estimation has been successfully explained by Bayesian models (Jazayeri and Shadlen, <xref rid="B21" ref-type="bibr">2010</xref>; Petzschner and Glasauer, <xref rid="B41" ref-type="bibr">2011</xref>; Cicchini et al., <xref rid="B8" ref-type="bibr">2012</xref>; Petzschner et al., <xref rid="B42" ref-type="bibr">2015</xref>). The relation between the present work and the Bayesian approaches is not investigated in detail. Nevertheless, some connections shall be discussed. An equivalence between drift-diffusion models and Bayesian frameworks has been described for modeling perceptual decision making (Bitzer et al., <xref rid="B3" ref-type="bibr">2014</xref>) and may also be possible to be established for the model presented here. The measurement phase results in an internal estimate <italic>m</italic> of a stimulus <italic>T</italic> drawn from a likelihood distribution <italic>p</italic>(<italic>m</italic> &#x02223; <italic>T</italic>). The reproduction process gives a posterior estimate, the reproduced stimulus <italic>r</italic>, drawn from the distribution <italic>p</italic>(<italic>r</italic> &#x02223; <italic>m</italic>). It has to be explored, however, (i) whether the update rule Equation (6) implements a way of connecting both the likelihood <italic>p</italic>(<italic>m</italic> &#x02223; <italic>T</italic>) and the posterior <italic>p</italic>(<italic>r</italic> &#x02223; <italic>m</italic>) in a Bayes-optimal way; (ii) in how far the update-rules used here and in Petzschner and Glasauer (<xref rid="B41" ref-type="bibr">2011</xref>) correspond to each other; and (iii) if the remarkable agreement between the present results and that of the Bayesian description by Cicchini et al. (<xref rid="B8" ref-type="bibr">2012</xref>) indicates more than conceptual conformity, i.e., the connection between minimization of reproduction errors and strength of regression, and their modulation by the precision of sensory representations and by the stimulus distribution.</p><p>In general, interpreting the regression effect as a means of error minimization shares similarities with concepts like the free-energy principle (Friston, <xref rid="B13" ref-type="bibr">2010</xref>) and information maximization (Linsker, <xref rid="B30" ref-type="bibr">1990</xref>). Error minimization corresponds to the idea of minimizing surprise (free energy) or prediction error and hence maximizing reward.</p></sec><sec><title>4.4. Neural implementation?</title><p>Noisy integrative activation patterns are found in several brain regions during decision-making tasks (for a recent review see, e.g., Shadlen and Kiani, <xref rid="B46" ref-type="bibr">2013</xref>). It remains open, however, if such patterns are also present during magnitude estimation as proposed by the model presented here. Neurons sensitive to elapsed time have been shown, for instance, in parietal cortex (Leon and Shadlen, <xref rid="B29" ref-type="bibr">2003</xref>), hippocampus (MacDonald et al., <xref rid="B34" ref-type="bibr">2011</xref>; Sakon et al., <xref rid="B44" ref-type="bibr">2014</xref>), and basal ganglia (Jin et al., <xref rid="B22" ref-type="bibr">2009</xref>; Mello et al., <xref rid="B37" ref-type="bibr">2015</xref>). Neurons in rat hippocampus code for distance covered (Kraus et al., <xref rid="B26" ref-type="bibr">2013</xref>). Single neurons in rat prefrontal cortex show temporally modulated activation patterns during interval timing (Kim et al., <xref rid="B25" ref-type="bibr">2013</xref>; Xu et al., <xref rid="B55" ref-type="bibr">2014</xref>). Such single cell activation patterns may form a set of basis functions to drive noisy integrative processes (c.f. Ludvig et al., <xref rid="B33" ref-type="bibr">2008</xref>; Goldman, <xref rid="B14" ref-type="bibr">2009</xref>; Mello et al., <xref rid="B37" ref-type="bibr">2015</xref>) and may arise in neural networks with balanced excitation and inhibition (Simen et al., <xref rid="B49" ref-type="bibr">2011</xref>), from firing rate adaptation (Reutimann et al., <xref rid="B43" ref-type="bibr">2004</xref>), or from single neuron dynamics (Durstewitz, <xref rid="B10" ref-type="bibr">2003</xref>) &#x02014; although it has been questioned recently if ramp-like activity is present in single cells (Latimer et al., <xref rid="B28" ref-type="bibr">2015</xref>). It is, furthermore, conceivable to obtain processes akin to noisy integration from state dependent networks (Karmarkar and Buonomano, <xref rid="B23" ref-type="bibr">2007</xref>; Buonomano and Laje, <xref rid="B7" ref-type="bibr">2010</xref>; Laje and Buonomano, <xref rid="B27" ref-type="bibr">2013</xref>). Another question that arises when thinking about a neural implementation of the model introduced in this paper concerns the implementation of the adaptive threshold. It has been suggested from network models of perceptual decision making that adaptive thresholds for noisy integrative processes may be implemented with the help of synaptic plasticity in cortico-striatal circuits (Lo and Wang, <xref rid="B32" ref-type="bibr">2006</xref>; Wei et al., <xref rid="B54" ref-type="bibr">2015</xref>).</p></sec></sec><sec id="s5"><title>5. Conclusions</title><p>The model presented in this paper describes magnitude estimation as two-stages of noisy integration linked by an iteratively updated internal reference memory. Behavioral characteristics well known from magnitude estimation experiments are not only reproduced but also explained as a means of minimizing errors given estimates corrupted by internal and external sources of noise. This paper thus shows that noisy integrative processes may be crucial for cognitive demands beyond perceptual decision making, such as the processing of magnitudes &#x02014; suggesting an overall computational principle and likely common neural mechanisms that we use to perceive and interpret our environment.</p></sec><sec id="s6"><title>Author contributions</title><p>The author confirms being the sole contributor of this work and approved it for publication.</p><sec><title>Conflict of interest statement</title><p>The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>This work was supported by Bernstein Center Munich; grant number 01GQ1004A, BMBF (Federal Ministry of Education and Research, Germany).</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bausenhart</surname><given-names>K. M.</given-names></name><name><surname>Dyjas</surname><given-names>O.</given-names></name><name><surname>Ulrich</surname><given-names>R.</given-names></name></person-group> (<year>2014</year>). <article-title>Temporal reproductions are influenced by an internal reference: explaining the Vierordt effect</article-title>. <source>Acta Psychol. (Amst.)</source>
<volume>147</volume>, <fpage>60</fpage>&#x02013;<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2013.06.011</pub-id><pub-id pub-id-type="pmid">23896562</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berniker</surname><given-names>M.</given-names></name><name><surname>Voss</surname><given-names>M.</given-names></name><name><surname>Kording</surname><given-names>K.</given-names></name></person-group> (<year>2010</year>). <article-title>Learning priors for bayesian computations in the nervous system</article-title>. <source>PLoS ONE</source>
<volume>5</volume>:<fpage>e12686</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0012686</pub-id><pub-id pub-id-type="pmid">20844766</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitzer</surname><given-names>S.</given-names></name><name><surname>Park</surname><given-names>H.</given-names></name><name><surname>Blankenburg</surname><given-names>F.</given-names></name><name><surname>Kiebel</surname><given-names>S. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Perceptual decision making: drift-diffusion model is equivalent to a Bayesian model</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>81</volume>:<issue>102</issue>
<pub-id pub-id-type="doi">10.3389/fnhum.2014.00102</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R.</given-names></name><name><surname>Brown</surname><given-names>E.</given-names></name><name><surname>Moehlis</surname><given-names>J.</given-names></name><name><surname>Holmes</surname><given-names>P.</given-names></name><name><surname>Cohen</surname><given-names>J. D.</given-names></name></person-group> (<year>2006</year>). <article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title>. <source>Psychol. Rev.</source>
<volume>113</volume>, <fpage>700</fpage>&#x02013;<lpage>765</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>T.</given-names></name><name><surname>Wong-Lin</surname><given-names>K. F.</given-names></name><name><surname>Holmes</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>Closed-form approximations of first-passage distributions for a stochastic decision-making model</article-title>. <source>Appl. Math. Res. Express</source>
<volume>2009</volume>, <fpage>123</fpage>&#x02013;<lpage>141</lpage>. <pub-id pub-id-type="doi">10.1093/amrx/abp008</pub-id><pub-id pub-id-type="pmid">23105943</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>B. W.</given-names></name><name><surname>Botvinick</surname><given-names>M. M.</given-names></name><name><surname>Brody</surname><given-names>C. D.</given-names></name></person-group> (<year>2013</year>). <article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title>. <source>Science</source>
<volume>340</volume>, <fpage>95</fpage>&#x02013;<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1126/science.1233912</pub-id><pub-id pub-id-type="pmid">23559254</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname><given-names>D. V.</given-names></name><name><surname>Laje</surname><given-names>R.</given-names></name></person-group> (<year>2010</year>). <article-title>Population clocks: motor timing with neural dynamics</article-title>. <source>Trends Cogn. Sci.</source>
<volume>14</volume>, <fpage>520</fpage>&#x02013;<lpage>527</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2010.09.002</pub-id><pub-id pub-id-type="pmid">20889368</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cicchini</surname><given-names>G. M.</given-names></name><name><surname>Arrighi</surname><given-names>R.</given-names></name><name><surname>Cecchetti</surname><given-names>L.</given-names></name><name><surname>Giusti</surname><given-names>M.</given-names></name><name><surname>Burr</surname><given-names>D. C.</given-names></name></person-group> (<year>2012</year>). <article-title>Optimal encoding of interval timing in expert percussionists</article-title>. <source>J. Neurosci.</source>
<volume>32</volume>, <fpage>1056</fpage>&#x02013;<lpage>1060</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3411-11.2012</pub-id><pub-id pub-id-type="pmid">22262903</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>D. V.</given-names></name></person-group> (<year>1973</year>). <article-title>Sequential dependencies and regression in psychophysical judgments</article-title>. <source>Percept. Psychophys.</source>
<volume>14</volume>, <fpage>547</fpage>&#x02013;<lpage>552</lpage>.</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D.</given-names></name></person-group> (<year>2003</year>). <article-title>Self-organizing neural integrator predicts interval times through climbing activity</article-title>. <source>J. Neurosci</source>. <volume>23</volume>, <fpage>5342</fpage>&#x02013;<lpage>5353</lpage>. <pub-id pub-id-type="pmid">12832560</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyjas</surname><given-names>O.</given-names></name><name><surname>Bausenhart</surname><given-names>K. M.</given-names></name><name><surname>Ulrich</surname><given-names>R.</given-names></name></person-group> (<year>2012</year>). <article-title>Trial-by-trial updating of an internal reference in discrimination tasks: evidence from effects of stimulus order and trial sequence</article-title>. <source>Atten. Percept. Psychophys.</source>
<volume>74</volume>, <fpage>1819</fpage>&#x02013;<lpage>1841</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-012-0362-4</pub-id><pub-id pub-id-type="pmid">23055085</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fechner</surname><given-names>G. T.</given-names></name></person-group> (<year>1860</year>). <source>Elemente der Psychophysik.</source>
<publisher-loc>Leipzig</publisher-loc>: <publisher-name>Breitkopf und H&#x000e4;rtel</publisher-name>.</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name></person-group> (<year>2010</year>). <article-title>The free-energy principle: a unified brain theory?</article-title>
<source>Nat. Rev. Neurosci.</source>
<volume>11</volume>, <fpage>127</fpage>&#x02013;<lpage>138</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>M. S.</given-names></name></person-group> (<year>2009</year>). <article-title>Memory without feedback in a neural network</article-title>. <source>Neuron</source>
<volume>61</volume>, <fpage>621</fpage>&#x02013;<lpage>634</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.012</pub-id><pub-id pub-id-type="pmid">19249281</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grondin</surname><given-names>S.</given-names></name><name><surname>Killeen</surname><given-names>P. R.</given-names></name></person-group> (<year>2009</year>). <article-title>Tracking time with song and count: different Weber functions for musicians and nonmusicians</article-title>. <source>Atten. Percept. Psychophys.</source>
<volume>71</volume>, <fpage>1649</fpage>&#x02013;<lpage>1654</lpage>. <pub-id pub-id-type="doi">10.3758/APP.71.7.1649</pub-id><pub-id pub-id-type="pmid">19801624</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>T. D.</given-names></name><name><surname>Kopec</surname><given-names>C. D.</given-names></name><name><surname>Brunton</surname><given-names>B. W.</given-names></name><name><surname>Duan</surname><given-names>C. A.</given-names></name><name><surname>Erlich</surname><given-names>J. C.</given-names></name><name><surname>Brody</surname><given-names>C. D.</given-names></name></person-group> (<year>2015</year>). <article-title>Distinct relationships of parietal and prefrontal cortices to evidence accumulation</article-title>. <source>Nature</source>
<volume>520</volume>, <fpage>220</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1038/nature14066</pub-id><pub-id pub-id-type="pmid">25600270</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hellstr&#x000f6;m</surname><given-names>A.</given-names></name></person-group> (<year>2003</year>). <article-title>Comparison is not just subtraction: effects of time- and space-order on subjective stimulus difference</article-title>. <source>Percept. Psychophys.</source>
<volume>65</volume>, <fpage>1161</fpage>&#x02013;<lpage>1177</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194842</pub-id><pub-id pub-id-type="pmid">14674641</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>S. C.</given-names></name><name><surname>Rao</surname><given-names>S. M.</given-names></name></person-group> (<year>2004</year>). <article-title>&#x0201c;One-thousand one&#x02026;one-thousand two&#x02026;&#x0201d;: chronometric counting violates the scalar property in interval timing</article-title>. <source>Psychon. Bull. Rev.</source>
<volume>11</volume>, <fpage>24</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.3758/BF03206456</pub-id><pub-id pub-id-type="pmid">15116982</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollingworth</surname><given-names>H. L.</given-names></name></person-group> (<year>1910</year>). <article-title>The central tendency of judgment</article-title>. <source>J. Philos. Psychol. Sci. Methods</source>
<volume>7</volume>, <fpage>461</fpage>&#x02013;<lpage>469</lpage>.</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>J. D.</given-names></name></person-group> (<year>2007</year>). <article-title>Matplotlib: A 2D graphics environment</article-title>. <source>Comput. Sci. Eng.</source>
<volume>9</volume>, <fpage>90</fpage>&#x02013;<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M.</given-names></name><name><surname>Shadlen</surname><given-names>M. N.</given-names></name></person-group> (<year>2010</year>). <article-title>Temporal context calibrates interval timing</article-title>. <source>Nat. Neurosci.</source>
<volume>13</volume>, <fpage>1020</fpage>&#x02013;<lpage>1026</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2590</pub-id><pub-id pub-id-type="pmid">20581842</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>D. Z.</given-names></name><name><surname>Fujii</surname><given-names>N.</given-names></name><name><surname>Graybiel</surname><given-names>A. M.</given-names></name></person-group> (<year>2009</year>). <article-title>Neural representation of time in cortico-basal ganglia circuits</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>
<volume>106</volume>, <fpage>19156</fpage>&#x02013;<lpage>19161</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0909881106</pub-id><pub-id pub-id-type="pmid">19850874</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karmarkar</surname><given-names>U. R.</given-names></name><name><surname>Buonomano</surname><given-names>D. V.</given-names></name></person-group> (<year>2007</year>). <article-title>Timing in the absence of clocks: encoding time in neural network states</article-title>. <source>Neuron</source>
<volume>53</volume>, <fpage>427</fpage>&#x02013;<lpage>438</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.01.006</pub-id><pub-id pub-id-type="pmid">17270738</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname><given-names>S. J.</given-names></name><name><surname>Daunizeau</surname><given-names>J.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2008</year>). <article-title>A hierarchy of time-scales and the brain</article-title>. <source>PLoS Comput. Biol.</source>
<volume>4</volume>:<fpage>e1000209</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id><pub-id pub-id-type="pmid">19008936</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Ghim</surname><given-names>J.-W.</given-names></name><name><surname>Lee</surname><given-names>J. H.</given-names></name><name><surname>Jung</surname><given-names>M. W.</given-names></name></person-group> (<year>2013</year>). <article-title>Neural correlates of interval timing in rodent prefrontal cortex</article-title>. <source>J. Neurosci.</source>
<volume>33</volume>, <fpage>13834</fpage>&#x02013;<lpage>13847</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1443-13.2013</pub-id><pub-id pub-id-type="pmid">23966703</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraus</surname><given-names>B. J.</given-names></name><name><surname>Robinson</surname><given-names>R. J.</given-names><suffix>II.</suffix></name><name><surname>White</surname><given-names>J. A.</given-names></name><name><surname>Eichenbaum</surname><given-names>H.</given-names></name><name><surname>Hasselmo</surname><given-names>M. E.</given-names></name></person-group> (<year>2013</year>). <article-title>Hippocampal &#x0201c;time cells&#x0201d;: time versus path integration</article-title>. <source>Neuron</source>
<volume>78</volume>, <fpage>1090</fpage>&#x02013;<lpage>1101</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.015</pub-id><pub-id pub-id-type="pmid">23707613</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname><given-names>R.</given-names></name><name><surname>Buonomano</surname><given-names>D. V.</given-names></name></person-group> (<year>2013</year>). <article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title>. <source>Nat. Neurosci.</source>
<volume>16</volume>, <fpage>925</fpage>&#x02013;<lpage>933</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><pub-id pub-id-type="pmid">23708144</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latimer</surname><given-names>K. W.</given-names></name><name><surname>Yates</surname><given-names>J. L.</given-names></name><name><surname>Meister</surname><given-names>M. L. R.</given-names></name><name><surname>Huk</surname><given-names>A. C.</given-names></name><name><surname>Pillow</surname><given-names>J. W.</given-names></name></person-group> (<year>2015</year>). <article-title>Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>. <source>Science</source>
<volume>349</volume>, <fpage>184</fpage>&#x02013;<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1126/science.aaa4056</pub-id><pub-id pub-id-type="pmid">26160947</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leon</surname><given-names>M. I.</given-names></name><name><surname>Shadlen</surname><given-names>M. N.</given-names></name></person-group> (<year>2003</year>). <article-title>Representation of time by neurons in the posterior parietal cortex of the macaque</article-title>. <source>Neuron</source>
<volume>38</volume>, <fpage>317</fpage>&#x02013;<lpage>327</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(03)00185-5</pub-id><pub-id pub-id-type="pmid">12718864</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linsker</surname><given-names>R.</given-names></name></person-group> (<year>1990</year>). <article-title>Perceptual neural organization: some approaches based on network models and information theory</article-title>. <source>Annu. Rev. Neurosci.</source>
<volume>13</volume>, <fpage>257</fpage>&#x02013;<lpage>281</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.13.030190.001353</pub-id><pub-id pub-id-type="pmid">2183677</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Pleskac</surname><given-names>T. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Neural correlates of evidence accumulation in a perceptual decision task</article-title>. <source>J. Neurophysiol.</source>
<volume>106</volume>, <fpage>2383</fpage>&#x02013;<lpage>2398</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00413.2011</pub-id><pub-id pub-id-type="pmid">21849612</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lo</surname><given-names>C.-C.</given-names></name><name><surname>Wang</surname><given-names>X.-J.</given-names></name></person-group> (<year>2006</year>). <article-title>Cortico-basal ganglia circuit mechanism for a decision threshold in reaction time tasks</article-title>. <source>Nat. Neurosci.</source>
<volume>9</volume>, <fpage>956</fpage>&#x02013;<lpage>963</lpage>. <pub-id pub-id-type="doi">10.1038/nn1722</pub-id><pub-id pub-id-type="pmid">16767089</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname><given-names>E. A.</given-names></name><name><surname>Sutton</surname><given-names>R. S.</given-names></name><name><surname>Kehoe</surname><given-names>E. J.</given-names></name></person-group> (<year>2008</year>). <article-title>Stimulus representation and the timing of reward-prediction errors in models of the dopamine system</article-title>. <source>Neural Comput.</source>
<volume>20</volume>, <fpage>3034</fpage>&#x02013;<lpage>3054</lpage>. <pub-id pub-id-type="doi">10.1162/neco.2008.11-07-654</pub-id><pub-id pub-id-type="pmid">18624657</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDonald</surname><given-names>C. J.</given-names></name><name><surname>Lepage</surname><given-names>K. Q.</given-names></name><name><surname>Eden</surname><given-names>U. T.</given-names></name><name><surname>Eichenbaum</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). <article-title>Hippocampal &#x0201c;time cells&#x0201d; bridge the gap in memory for discontiguous events</article-title>. <source>Neuron</source>
<volume>71</volume>, <fpage>737</fpage>&#x02013;<lpage>749</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.012</pub-id><pub-id pub-id-type="pmid">21867888</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malapani</surname><given-names>C.</given-names></name><name><surname>Deweer</surname><given-names>B.</given-names></name><name><surname>Gibbon</surname><given-names>J.</given-names></name></person-group> (<year>2002</year>). <article-title>Separating storage from retrieval dysfunction of temporal memory in Parkinson's disease</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>14</volume>, <fpage>311</fpage>&#x02013;<lpage>322</lpage>. <pub-id pub-id-type="doi">10.1162/089892902317236920</pub-id><pub-id pub-id-type="pmid">11970794</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malapani</surname><given-names>C.</given-names></name><name><surname>Rakitin</surname><given-names>B.</given-names></name><name><surname>Levy</surname><given-names>R. S.</given-names></name><name><surname>Meck</surname><given-names>W. H.</given-names></name><name><surname>Deweer</surname><given-names>B.</given-names></name><name><surname>Dubois</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>1998</year>). <article-title>Coupled temporal memories in Parkinson's disease: a dopamine-related dysfunction</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>10</volume>, <fpage>316</fpage>&#x02013;<lpage>331</lpage>. <pub-id pub-id-type="pmid">9869707</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname><given-names>G. B. M.</given-names></name><name><surname>Soares</surname><given-names>S.</given-names></name><name><surname>Paton</surname><given-names>J. J.</given-names></name></person-group> (<year>2015</year>). <article-title>A scalable population code for time in the striatum</article-title>. <source>Curr. Biol.</source>
<volume>25</volume>, <fpage>1113</fpage>&#x02013;<lpage>1122</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2015.02.036</pub-id><pub-id pub-id-type="pmid">25913405</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merchant</surname><given-names>H.</given-names></name><name><surname>Harrington</surname><given-names>D. L.</given-names></name><name><surname>Meck</surname><given-names>W. H.</given-names></name></person-group> (<year>2013</year>). <article-title>Neural basis of the perception and estimation of time</article-title>. <source>Annu. Rev. Neurosci.</source>
<volume>36</volume>, <fpage>313</fpage>&#x02013;<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170349</pub-id><pub-id pub-id-type="pmid">23725000</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>M. J.</given-names></name><name><surname>Watamaniuk</surname><given-names>S. N.</given-names></name><name><surname>McKee</surname><given-names>S. P.</given-names></name></person-group> (<year>2000</year>). <article-title>The use of an implicit standard for measuring discrimination thresholds</article-title>. <source>Vision Res.</source>
<volume>40</volume>, <fpage>2341</fpage>&#x02013;<lpage>2349</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(00)00093-6</pub-id><pub-id pub-id-type="pmid">10927119</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>J. D.</given-names></name><name><surname>Bernacchia</surname><given-names>A.</given-names></name><name><surname>Freedman</surname><given-names>D. J.</given-names></name><name><surname>Romo</surname><given-names>R.</given-names></name><name><surname>Wallis</surname><given-names>J. D.</given-names></name><name><surname>Cai</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>A hierarchy of intrinsic timescales across primate cortex</article-title>. <source>Nat. Neurosci.</source>
<volume>17</volume>, <fpage>1661</fpage>&#x02013;<lpage>1663</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3862</pub-id><pub-id pub-id-type="pmid">25383900</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petzschner</surname><given-names>F. H.</given-names></name><name><surname>Glasauer</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <article-title>Iterative Bayesian estimation as an explanation for range and regression effects: a study on human path integration</article-title>. <source>J. Neurosci.</source>
<volume>31</volume>, <fpage>17220</fpage>&#x02013;<lpage>17229</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2028-11.2011</pub-id><pub-id pub-id-type="pmid">22114288</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petzschner</surname><given-names>F. H.</given-names></name><name><surname>Glasauer</surname><given-names>S.</given-names></name><name><surname>Stephan</surname><given-names>K. E.</given-names></name></person-group> (<year>2015</year>). <article-title>A Bayesian perspective on magnitude estimation</article-title>. <source>Trends Cogn. Sci.</source>
<volume>19</volume>, <fpage>285</fpage>&#x02013;<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2015.03.002</pub-id><pub-id pub-id-type="pmid">25843543</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reutimann</surname><given-names>J.</given-names></name><name><surname>Yakovlev</surname><given-names>V.</given-names></name><name><surname>Fusi</surname><given-names>S.</given-names></name><name><surname>Senn</surname><given-names>W.</given-names></name></person-group> (<year>2004</year>). <article-title>Climbing neuronal activity as an event-based cortical representation of time</article-title>. <source>J. Neurosci.</source>
<volume>24</volume>, <fpage>3295</fpage>&#x02013;<lpage>3303</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4098-03.2004</pub-id><pub-id pub-id-type="pmid">15056709</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakon</surname><given-names>J. J.</given-names></name><name><surname>Naya</surname><given-names>Y.</given-names></name><name><surname>Wirth</surname><given-names>S.</given-names></name><name><surname>Suzuki</surname><given-names>W. A.</given-names></name></person-group> (<year>2014</year>). <article-title>Context-dependent incremental timing cells in the primate hippocampus</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>111</volume>, <fpage>18351</fpage>&#x02013;<lpage>18356</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1417827111</pub-id><pub-id pub-id-type="pmid">25489071</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Seabold</surname><given-names>S.</given-names></name><name><surname>Perktold</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Statsmodels: econometric and statistical modeling with python</article-title>, in <source>Proceedings of the 9th Python in Science Conference</source>, eds <person-group person-group-type="editor"><name><surname>van der Walt</surname><given-names>S.</given-names></name><name><surname>Millman</surname><given-names>J.</given-names></name></person-group>, <fpage>57</fpage>&#x02013;<lpage>61</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf">http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf</ext-link></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>M. N.</given-names></name><name><surname>Kiani</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Decision making as a window on cognition</article-title>. <source>Neuron</source>
<volume>80</volume>, <fpage>791</fpage>&#x02013;<lpage>806</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.047</pub-id><pub-id pub-id-type="pmid">24183028</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>M. N.</given-names></name><name><surname>Newsome</surname><given-names>W. T.</given-names></name></person-group> (<year>2001</year>). <article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title>. <source>J. Neurophysiol.</source>
<volume>86</volume>, <fpage>1916</fpage>&#x02013;<lpage>1936</lpage>. <pub-id pub-id-type="doi">10.3410/f.1001494.23207</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Z.</given-names></name><name><surname>Church</surname><given-names>R. M.</given-names></name><name><surname>Meck</surname><given-names>W. H.</given-names></name></person-group> (<year>2013</year>). <article-title>Bayesian optimization of time perception</article-title>. <source>Trends Cogn. Sci.</source>
<volume>17</volume>, <fpage>556</fpage>&#x02013;<lpage>564</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2013.09.009</pub-id><pub-id pub-id-type="pmid">24139486</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simen</surname><given-names>P.</given-names></name><name><surname>Balci</surname><given-names>F.</given-names></name><name><surname>de Souza</surname><given-names>L.</given-names></name><name><surname>Cohen</surname><given-names>J. D.</given-names></name><name><surname>Holmes</surname><given-names>P.</given-names></name></person-group> (<year>2011</year>). <article-title>A model of interval timing by neural integration</article-title>. <source>J. Neurosci.</source>
<volume>31</volume>, <fpage>9238</fpage>&#x02013;<lpage>9253</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3121-10.2011</pub-id><pub-id pub-id-type="pmid">21697374</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teghtsoonian</surname><given-names>R.</given-names></name><name><surname>Teghtsoonian</surname><given-names>M.</given-names></name></person-group> (<year>1978</year>). <article-title>Range and regression effects in magnitude scaling</article-title>. <source>Percept. Psychophys.</source>
<volume>24</volume>, <fpage>305</fpage>&#x02013;<lpage>314</lpage>. <pub-id pub-id-type="pmid">750977</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tuckwell</surname><given-names>H. C.</given-names></name></person-group> (<year>1988</year>). <source>Introduction to Theoretical Neurobiology: Volume 2, Nonlinear and Stochastic Theories.</source>
<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref><ref id="B52"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>von Vierordt</surname><given-names>K.</given-names></name></person-group> (<year>1868</year>). <source>Der Zeitsinn nach Versuchen.</source>
<publisher-loc>T&#x000fc;bingen</publisher-loc>: <publisher-name>Laupp</publisher-name>.</mixed-citation></ref><ref id="B53"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>E. H.</given-names></name></person-group> (<year>1851</year>). <article-title>Die lehre vom tastsinne und gemeingef&#x000fc;hle auf versuche gegr&#x000fc;ndet</article-title>, in <source>Handw&#x000f6;rterbuch der Physiologie</source>, ed <person-group person-group-type="editor"><name><surname>Wagner</surname><given-names>R.</given-names></name></person-group> (<publisher-loc>Braunschweig</publisher-loc>: <publisher-name>Vieweg</publisher-name>), <fpage>481</fpage>&#x02013;<lpage>588</lpage>.</mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>W.</given-names></name><name><surname>Rubin</surname><given-names>J. E.</given-names></name><name><surname>Wang</surname><given-names>X.-J.</given-names></name></person-group> (<year>2015</year>). <article-title>Role of the indirect pathway of the basal ganglia in perceptual decision making</article-title>. <source>J. Neurosci.</source>
<volume>35</volume>, <fpage>4052</fpage>&#x02013;<lpage>4064</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3611-14.2015</pub-id><pub-id pub-id-type="pmid">25740532</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>S.-y.</given-names></name><name><surname>Dan</surname><given-names>Y.</given-names></name><name><surname>Poo</surname><given-names>M.-M.</given-names></name></person-group> (<year>2014</year>). <article-title>Representation of interval timing by temporally scalable firing patterns in rat prefrontal cortex</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>111</volume>, <fpage>480</fpage>&#x02013;<lpage>485</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1321314111</pub-id><pub-id pub-id-type="pmid">24367075</pub-id></mixed-citation></ref></ref-list></back></article>