<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="iso-abbrev">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="hwp">tvst</journal-id><journal-id journal-id-type="pmc">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="publisher-id">TVST</journal-id><journal-title-group><journal-title>Translational Vision Science &#x00026; Technology</journal-title></journal-title-group><issn pub-type="epub">2164-2591</issn><publisher><publisher-name>The Association for Research in Vision and Ophthalmology</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31788350</article-id><article-id pub-id-type="pmc">6871542</article-id><article-id pub-id-type="doi">10.1167/tvst.8.6.21</article-id><article-id pub-id-type="sici">tvst-08-05-26</article-id><article-id pub-id-type="publisher-id">TVST-19-1677</article-id><article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories><title-group><article-title>Deep Learning for Automatically Visual Evoked Potential Classification During Surgical Decompression of Sellar Region Tumors</article-title><alt-title alt-title-type="runhead">Qiao et al.</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Qiao</surname><given-names>Nidan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="author-notes" rid="n102">*</xref></contrib><contrib contrib-type="author"><name><surname>Song</surname><given-names>Mengju</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="author-notes" rid="n102">*</xref></contrib><contrib contrib-type="author"><name><surname>Ye</surname><given-names>Zhao</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Wenqiang</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ma</surname><given-names>Zengyi</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yongfei</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Yuyan</given-names></name><xref ref-type="aff" rid="aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Shou</surname><given-names>Xuefei</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><aff id="aff1">
<label>1</label>Shanghai Pituitary Tumor Center, Shanghai Neurosurgical Research Institute, Department of Neurosurgery, Huashan Hospital, Shanghai Medical College, Fudan University, Shanghai, China &#x00026; Neuroendocrine Unit, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA
</aff><aff id="aff2">
<label>2</label>Department of Ophthalmology, Huashan Hospital, Shanghai Medical College, Fudan University, Shanghai, China &#x00026; Putuo Oculopathy Dental Disease Prevention &#x00026; Cure Clinic, Shanghai, China
</aff><aff id="aff3">
<label>3</label>Shanghai Pituitary Tumor Center, Shanghai Neurosurgical Research Institute, Department of Neurosurgery, Huashan Hospital, Shanghai Medical College, Fudan University, Shanghai, China
</aff><aff id="aff4">
<label>4</label>Department of Ophthalmology, Huashan Hospital, Shanghai Medical College, Fudan University, Shanghai, China
</aff></contrib-group><author-notes><corresp id="cor1"><bold>Correspondence:</bold> Yuyan Zhang, 12 Wulumuqi Zhong Road, Department of Ophthalmology, Huashan Hospital, Shanghai, China. e-mail: <email>yuyan8688@163.com</email></corresp><corresp id="cor2">Xuefei Shou, 12 Wulumuqi Zhong Road, Department of Neurosurgery, Huashan Hospital, Shanghai, China. e-mail: <email>shouxf@hotmail.com</email></corresp><fn id="n102" fn-type="equal"><label>*</label><p>Nidan Qiao and Mengju Song contributed equally to the manuscript.</p></fn></author-notes><pub-date pub-type="collection"><month>11</month><year>2019</year></pub-date><pub-date pub-type="epub"><day>20</day><month>11</month><year>2019</year></pub-date><volume>8</volume><issue>6</issue><elocation-id>21</elocation-id><history><date date-type="received"><day>10</day><month>6</month><year>2019</year></date><date date-type="accepted"><day>17</day><month>9</month><year>2019</year></date></history><permissions><copyright-statement>Copyright 2019 The Authors</copyright-statement><copyright-year>2019</copyright-year><license license-type="cc-by-nc-nd" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="i2164-2591-8-6-21.pdf"/><abstract><sec id="st1"><title>Purpose</title><p>Detection of the huge amount of data generated in real-time visual evoked potential (VEP) requires labor-intensive work and experienced electrophysiologists. This study aims to build an automatic VEP classification system by using a deep learning algorithm.</p></sec><sec id="st2"><title>Methods</title><p>Patients with sellar region tumor and optic chiasm compression were enrolled. Flash VEP monitoring was applied during surgical decompression. Sequential VEP images were fed into three neural network algorithms to train VEP classification models.</p></sec><sec id="st3"><title>Results</title><p>We included 76 patients. During surgical decompression, we observed 68 eyes with increased VEP amplitude, 47 eyes with a transient decrease, and 37 eyes without change. We generated 2,843 sequences (39,802 images) in total (887 sequences with increasing VEP, 276 sequences with decreasing VEP, and 1680 sequences without change). The model combining convolutional and recurrent neural network had the highest accuracy (87.4%; 95% confidence interval, 84.2%&#x02013;90.1%). The sensitivity of predicting no change VEP, increasing VEP, and decreasing VEP was 92.6%, 78.9%, and 83.7%, respectively. The specificity of predicting no change VEP, increasing VEP, and decreasing VEP was 80.5%, 93.3%, and 100.0%, respectively. The class activation map visualization technique showed that the P2-N3-P3 complex was important in determining the output.</p></sec><sec id="st4"><title>Conclusions</title><p>We identified three VEP responses (no change, increase, and decrease) during transsphenoidal surgical decompression of sellar region tumors. We developed a deep learning model to classify the sequential changes of intraoperative VEP.</p></sec><sec id="st5"><title>Translational Relevance</title><p>Our model may have the potential to be applied in real-time monitoring during surgical resection of sellar region tumors.</p></sec></abstract><kwd-group><kwd>artificial intelligence</kwd><kwd>optic chiasm</kwd><kwd>intraoperative monitoring</kwd><kwd>neural network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>More than 20% of tumors in the central nervous system originate in the sellar region, which houses the pituitary gland.<xref rid="i2164-2591-8-6-21-b01" ref-type="bibr">1</xref>,<xref rid="i2164-2591-8-6-21-b02" ref-type="bibr">2</xref> The most common among these sellar tumors are pituitary adenomas, craniopharyngiomas, and meningiomas. Visual dysfunctions are usually one of the complaints in these patients and are indications for surgical decompression. The close relationship of tumors and the optic nerve or chiasm makes the latter vulnerable to any direct intraoperative injury, blood supply reduction, and indirect insult due to heat conduction during coagulation. Although the visual outcomes are favorable in most patients, 10% of the patients are not able to fully recover after surgical decompression.<xref rid="i2164-2591-8-6-21-b03" ref-type="bibr">3</xref></p><p>Electrophysiological monitoring by the intraoperative real-time recording of visual evoked potential (VEP) can detect any possible VEP deviations during the surgical procedure and act as a surrogate for optic nerve injury. Several studies showed that this procedure had the potential to improve visual outcome.<xref rid="i2164-2591-8-6-21-b04" ref-type="bibr">4</xref><xref rid="i2164-2591-8-6-21-b05" ref-type="bibr"/>&#x02013;<xref rid="i2164-2591-8-6-21-b06" ref-type="bibr">6</xref> Nevertheless, detection of the massive amount of data generated in real-time requires labor-intensive work, which limits its wide usage in clinical settings. Moreover, the lack of experts who are experienced with electrophysiology and variations in different statuses of anesthesia make the real-time monitoring of VEP recordings difficult. The computer-aided analysis may have a solution to these issues.</p><p>A source of this assistance may lie in the rapidly progressing domain of artificial intelligence known as the neural network: a recent study has found that neural network algorithm performed well in classifying images from multifocal VEP data.<xref rid="i2164-2591-8-6-21-b07" ref-type="bibr">7</xref> Several studies also used an algorithm to predict steady-state VEP data.<xref rid="i2164-2591-8-6-21-b08" ref-type="bibr">8</xref>,<xref rid="i2164-2591-8-6-21-b09" ref-type="bibr">9</xref> Whereas in the dynamic field, multiple studies applied artificial intelligence in real-time: a study used hospital-wide deployment of artificial intelligence to discern real-time clinician behaviors,<xref rid="i2164-2591-8-6-21-b10" ref-type="bibr">10</xref> and another study focused on early diagnosis of patients with heart failure.<xref rid="i2164-2591-8-6-21-b11" ref-type="bibr">11</xref> No study was performed on the dynamic detection of abnormal VEP in real-time by using neural network algorithms. This study aims to determine whether deep learning algorithms can detect abnormal VEP. We hypothesize that VEP changing tendency can be identified by a deep learning algorithm during surgical decompression of sellar region tumors.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><p>We enrolled 76 patients (36 female, 47.3%; mean age, 45.7 years old) with sellar region tumor, and the chiasm compression was indicated on the preoperative magnetic resonance imaging scan. All the patients underwent transsphenoidal tumor resection. The study was approved by Huashan Hospital Institutional Review Board and was in adherence to the Declaration of Helsinki. Informed consents were obtained from all the participants. Preoperational evaluations included thorough ophthalmic examinations as well as visual acuity and visual field examination to exclude patients with fundus disease, glaucoma, or traumatic optic injury. Among 152 eyes included in this study, 60 eyes had decreased visual acuity, including 19 eyes that had visual acuity less than 0.1, and 87 eyes had an abnormal visual field (36 eyes had quadrantanopia and 51 eyes had hemianopia or worse).</p><sec id="s2a"><title>Intraoperative VEP Recording</title><p>Flash VEP monitoring was applied using the NIM Eclipse system (Medtronic, Minneapolis, MN), with the International Society for Clinical Electrophysiology of Vision VEP standard.<xref rid="i2164-2591-8-6-21-b12" ref-type="bibr">12</xref> The scalp for electrode placement (the midline active electrode was located at Oz, two lateral active electrodes were located at O<sub>1</sub> and O<sub>2</sub>, and the reference electrode was located at Fz) was prepared by shaving and cleaning before recording channels were connected with gold cup electrodes. Conductive paste was applied to decrease the impedance of the electrodes. Light-proof goggles with a flashing light-emitting diode for visual stimulation were placed over the two eyes. Monocular flashing stimulation was made at a rate of 1 Hz alternatively.</p><p>The preoperational recording was obtained before the anesthesia. Propofol was used to induce anesthesia with a 3-mg/kg/h maintaining dosage. We did not use inhaling anesthesia. The baseline of postanesthesia was obtained 5 minutes after successful anesthesia. VEPs were recorded during the surgical process in three channels per eye.</p></sec><sec id="s2b"><title>Preanalysis Processing</title><p>We measured amplitude from the positive P2 peak at around 120 ms to the preceding N2 negative peak at around 90 ms. Latency was measured as the time from stimulus onset to the P2 amplitude. To minimize the noise during monitoring, we used the average response over 5 minutes after stable anesthesia to measure the amplitude and latency. Then, the amplitude and latency after anesthesia were used as the baseline, and change from the baseline was calculated in status after tumor resection.</p><p>To train the model for detecting VEP change, we identified no change, increase, and decrease (the criterion was defined as a &#x0003e;25% increase or &#x0003e;25% decrease in amplitude compared with the baseline) VEP sequences during surgical decompression. Preanalysis processing of the VEP sequences included extracting VEP images from all three channels in each eye and combining 14 VEP images over 5 minutes into a sequence. Images with huge noise and artifacts were excluded. All the sequences were transformed into 350 &#x000d7; 90-pixel size and vectorized using the value of each pixel.</p></sec><sec id="s2c"><title>Statistical Analysis</title><p>We applied three neural network models (a three-layer convolutional neural network, a pretrained convolutional neural network, and a combination of a convolutional and recurrent neural network) to detect different VEP responses. In the first model (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s02">Supplementary Table S1</xref>), convolutional neural network alone has the potential to distinguish among different responses. In this model, we used three convolutional layers and max-pooling layers. In the second model (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s02">Supplementary Table S2</xref>), a VGG19 architecture with preinitialization weights from the same network trained in our previous work<xref rid="i2164-2591-8-6-21-b07" ref-type="bibr">7</xref> was used. In the third model (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s02">Supplementary Table S3</xref>), the convolutional neural network first recognized the images from each sequence; then, the recurrent neural network distinguished the sequence. In this model, multiple convolutional layers were followed by a long-short memory layer. We used another simpler model (one long-short memory layer) with amplitude and latency from every single VEP image as inputs for comparison. Workflow for VEP analysis is provided in <xref ref-type="fig" rid="i2164-2591-8-6-21-f01">Figure 1</xref>.</p><fig id="i2164-2591-8-6-21-f01" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The proposed workflow of analyzing intraoperative visual evoked potential. Time sequential visual evoked potentials were inputted to a convolutional neural network, followed by a recurrent neural network to predict no change, increasing, or decreasing. CNN, convolutional neural network; RNN, recurrent neural network.</p></caption><graphic xlink:href="i2164-2591-8-6-21-f01"/></fig><p>During the training process, random dropout was used to prevent overfitting. Data were randomly split into training dataset (60%), validation dataset (20%), and test dataset (20%). We used bootstrap (a new dataset was created by sampling from the original data in every step of the bootstrap) to test the robustness of our built models in the test dataset. Confusion matrix of the whole cohort was calculated with 5-fold cross-validation. Mean and 95% confidence intervals of the accuracy were provided. To provide model interpretability, we used class activation map (CAM) visualization technique to demonstrate the model explanation.<xref rid="i2164-2591-8-6-21-b13" ref-type="bibr">13</xref> The CAM computes how important each location is concerning the classification. For instance, given an image fed into our &#x0201c;no change versus decreasing versus increasing&#x0201d; model, CAM visualization allows generation of an &#x0201c;importance&#x0201d; heatmap for any input image. All the analyses were performed on Python 3.6 with the Keras package (version 2.1.1).</p><p>All the post-processing data can be assessed by request. The code will be available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/norikaisa/DeepiVEP">https://github.com/norikaisa/DeepiVEP</ext-link>).</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>Characteristics of VEPs of the Cohort</title><p>Preoperation baseline VEP amplitude was 4.0 &#x000b1; 2.4 &#x003bc;v, and the latency was 99.3 &#x000b1; 27.2 ms. After anesthesia, VEP amplitude decreased to 1.5 &#x000b1; 1.4 &#x003bc;v, and the latency increased to 104.7 &#x000b1; 29.3 ms (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s01">Supplementary Figure S1</xref>). The change was significant in VEP amplitude (&#x02212;107%; interquartile range, &#x02212;291% to &#x02212;5%) but not in VEP latency (7%; interquartile range, &#x02212;25% to 24%).</p><p>During surgical decompression of the optic chiasm, we observed 68 eyes had increased VEP amplitude. We also identified 47 eyes with a transient decrease in VEP amplitude and 37 eyes without change during the surgical decompression (<xref rid="i2164-2591-8-6-21-t01" ref-type="table">Table 1</xref>). These statuses were used as the ground truth for future training. Eyes with transient VEP amplitude decrease had better preoperative visual acuity. On the contrary, eyes without obvious change had worse visual acuity.</p><table-wrap id="i2164-2591-8-6-21-t01" content-type="2col" orientation="portrait" position="float"><label>Table 1</label><caption><p>Gold Standard of the Classification in Visual Evoked Potential</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col id="tb1col1" align="left" charoff="0" char="" span="1"/><col id="tb1col2" align="center" charoff="0" char="" span="1"/><col id="tb1col3" align="center" charoff="0" char="" span="1"/><col id="tb1col4" align="center" charoff="0" char="" span="1"/><col id="tb1col5" align="char" charoff="0" char="." span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Visual Measurements<hr/></td><td rowspan="1" colspan="1">No Change (<italic>n</italic> = 37)<hr/></td><td rowspan="1" colspan="1">Increasing (<italic>n</italic> = 68)<hr/></td><td rowspan="1" colspan="1">Decreasing (<italic>n</italic> = 47)<hr/></td><td rowspan="1" colspan="1"><italic>P</italic><hr/></td></tr></thead><tbody><tr style="stubhead"><td colspan="5" rowspan="1">Visual acuity, no. (%)</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Normal</td><td rowspan="1" colspan="1">11 (29.7)</td><td rowspan="1" colspan="1">33 (48.5)</td><td rowspan="1" colspan="1">29 (61.7)</td><td rowspan="2" colspan="1">0.003</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Abnormal</td><td rowspan="1" colspan="1">26 (70.3)</td><td rowspan="1" colspan="1">35 (51.5)</td><td rowspan="1" colspan="1">18 (38.3)</td></tr><tr style="stubhead"><td colspan="5" rowspan="1">Visual field, no. (%)</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Normal</td><td rowspan="1" colspan="1">15 (41.7)</td><td rowspan="1" colspan="1">29 (42.6)</td><td rowspan="1" colspan="1">21 (44.7)</td><td rowspan="2" colspan="1">0.484</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Abnormal</td><td rowspan="1" colspan="1">22 (58.3)</td><td rowspan="1" colspan="1">39 (57.2)</td><td rowspan="1" colspan="1">26 (55.3)</td></tr><tr style="stubhead"><td colspan="5" rowspan="1">Preanesthesia VEP, mean &#x000b1; SD</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Amplitude (&#x003bc;v)</td><td rowspan="1" colspan="1">4.4 &#x000b1; 2.1</td><td rowspan="1" colspan="1">3.6 &#x000b1; 2.1</td><td rowspan="1" colspan="1">4.2 &#x000b1; 2.9</td><td rowspan="1" colspan="1">0.253</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Latency (ms)</td><td rowspan="1" colspan="1">103.3 &#x000b1; 28.4</td><td rowspan="1" colspan="1">97.2 &#x000b1; 26.3</td><td rowspan="1" colspan="1">99.5 &#x000b1; 27.9</td><td rowspan="1" colspan="1">0.586</td></tr><tr style="stubhead"><td colspan="5" rowspan="1">VEP after stable anesthesia (baseline)</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Amplitude (&#x003bc;v), mean &#x000b1; SD</td><td rowspan="1" colspan="1">2.2 &#x000b1; 0.9</td><td rowspan="1" colspan="1">1.2 &#x000b1; 0.7</td><td rowspan="1" colspan="1">1.5 &#x000b1; 2.0</td><td rowspan="1" colspan="1">0.001</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Amplitude change from preanesthesia, (CI), %</td><td rowspan="1" colspan="1">&#x02212;53 (&#x02212;70, &#x02212;20)</td><td rowspan="1" colspan="1">&#x02212;67 (&#x02212;79, &#x02212;45)</td><td rowspan="1" colspan="1">&#x02212;71 (&#x02212;107, &#x02212;21)</td><td rowspan="1" colspan="1">0.037</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Latency (ms), mean &#x000b1; SD</td><td rowspan="1" colspan="1">112.8 &#x000b1; 28.3</td><td rowspan="1" colspan="1">102.2 &#x000b1; 28.7</td><td rowspan="1" colspan="1">102.0 &#x000b1; 30.4</td><td rowspan="1" colspan="1">0.153</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Latency change from preanesthesia, (CI), %</td><td rowspan="1" colspan="1">13 (&#x02212;5, 31)</td><td rowspan="1" colspan="1">8 (&#x02212;23, 40)</td><td rowspan="1" colspan="1">2 (&#x02212;19, 32)</td><td rowspan="1" colspan="1">0.892</td></tr><tr style="stubhead"><td colspan="5" rowspan="1">VEP during tumor decompression</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Amplitude (&#x003bc;v), mean &#x000b1; SD</td><td rowspan="1" colspan="1">2.1 &#x000b1; 0.9</td><td rowspan="1" colspan="1">3.0 &#x000b1; 1.9</td><td rowspan="1" colspan="1">1.4 &#x000b1; 1.2</td><td rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Amplitude change from baseline, (CI), %</td><td rowspan="1" colspan="1">0 (&#x02212;10, 0)</td><td rowspan="1" colspan="1">146 (69, 270)</td><td rowspan="1" colspan="1">&#x02212;75 (&#x02212;270, &#x02212;43)</td><td rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Latency (ms), mean &#x000b1; SD</td><td rowspan="1" colspan="1">99.1 &#x000b1; 29.3</td><td rowspan="1" colspan="1">95.6 &#x000b1; 27.1</td><td rowspan="1" colspan="1">100.6 &#x000b1; 25.7</td><td rowspan="1" colspan="1">0.604</td></tr><tr><td rowspan="1" colspan="1">&#x02003;Latency change from baseline, (CI), %</td><td rowspan="1" colspan="1">&#x02212;5 (&#x02212;17, 0)</td><td rowspan="1" colspan="1">&#x02212;3 (&#x02212;15, 5)</td><td rowspan="1" colspan="1">0 (&#x02212;12, 17)</td><td rowspan="1" colspan="1">0.058</td></tr></tbody></table><table-wrap-foot><fn id="nt101"><p>no., number; CI, 95% confidence interval.</p></fn></table-wrap-foot></table-wrap><p>A sequence comprised 14 VEP images over 5 minutes, including the changing amplitude (<xref ref-type="fig" rid="i2164-2591-8-6-21-f02">Fig. 2</xref>). We created 1931 sequences during surgical decompression (887 sequences with increasing VEP, 276 sequences with decreasing VEP, and 768 sequences without change). Thus, on average, 4.2 sequences were generated per eye per channel. We further increased the sample size of VEP without change to 1680 sequences (adding 912 sequences from the baseline). The total sample size of our study was 2843 sequences (39,802 images).</p><fig id="i2164-2591-8-6-21-f02" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Examples of preprocessed visual evoked potential sequences.</p></caption><graphic xlink:href="i2164-2591-8-6-21-f02"/></fig></sec><sec id="s3b"><title>Deep Learning Models</title><p>First, we built a simple convolutional neural network (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s02">Supplementary Table S1</xref>) to classify the three VEP status. The model got an accuracy of 82.7% (95% confidence interval [CI], 78.3%&#x02013;86.5%). The sensitivity of predicting no change VEP, increasing VEP, and decreasing VEP was 84.6%, 79.3%, and 88.5%, respectively. The specificity of predicting no change VEP, increasing VEP, and decreasing VEP was 82.4%, 88.3%, and 99.3%, respectively.</p><p>In transfer learning using pretrained VGG16 structure (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s02">Supplementary Table S2</xref>), the accuracy was 84.1% (95% CI, 79.8%&#x02013;87.8%). The sensitivity of predicting no change VEP, increasing VEP, and decreasing VEP was 90.4%, 80.5%, and 86.0%, respectively. The specificity of predicting no change VEP, increasing VEP, and decreasing VEP was 83.3%, 90.4%, and 99.3%, respectively.</p><p>In the model combining convolutional and recurrent neural network (<xref ref-type="supplementary-material" rid="tvst-08-06-26_s02">Supplementary Table S3</xref>), the accuracy was 87.4% (95% CI, 84.2%&#x02013;90.1%). The sensitivity of predicting no change VEP, increasing VEP, and decreasing VEP was 92.6%, 78.9%, and 83.7%, respectively. The specificity of predicting no change VEP, increasing VEP, and decreasing VEP was 80.5%, 93.3%, and 100.0%, respectively. The 5-fold cross-validation of this model demonstrated that 2488 sequences were correctly classified (<xref ref-type="fig" rid="i2164-2591-8-6-21-f03">Fig. 3</xref>).</p><fig id="i2164-2591-8-6-21-f03" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Confusion matrix of the whole cohort after cross-validation.</p></caption><graphic xlink:href="i2164-2591-8-6-21-f03"/></fig><p>The simpler model only using amplitude and latency from every single VEP images as prediction features yielded an accuracy of 83.1% (95% CI, 81.7%&#x02013;84.5%).</p></sec><sec id="s3c"><title>Explanation of the Model</title><p>We calculated CAM in a typical image, and the visualization showed that VEP images in the bottom area (later time) and the P2-N3-P3 complex were more important in determining the output (<xref ref-type="fig" rid="i2164-2591-8-6-21-f04">Fig. 4</xref>).</p><fig id="i2164-2591-8-6-21-f04" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Class activation map visualization technique to demonstrate the model explanation. The visualization showed that visual evoked potential images in the bottom area (later time) and in the P2-N3-P3 complex were more important in determining the output.</p></caption><graphic xlink:href="i2164-2591-8-6-21-f04"/></fig></sec></sec><sec id="s4"><title>Discussion</title><p>In this paper, we built a workflow (extracting, preprocessing, and analysis) of the intraoperative monitoring VEP. Deep learning models were trained to detect the sequential change of intraoperative VEP. The model performance was comparable to human intelligence level in terms of differentiating VEPs without change, with increasing amplitude, or with decreasing amplitude. The results suggest our models can potentially assist or partially substitute human labor for VEP monitoring during surgical resection of sellar region tumors.</p><p>Intraoperative VEP monitoring was introduced to the field of neurosurgery in the 1970s<xref rid="i2164-2591-8-6-21-b14" ref-type="bibr">14</xref> when multiple publications demonstrated the usefulness of this technique to protect optic nerve from surgical injury.<xref rid="i2164-2591-8-6-21-b15" ref-type="bibr">15</xref>,<xref rid="i2164-2591-8-6-21-b16" ref-type="bibr">16</xref> But a few publications indicated that this technique might be susceptible nonspecific influences, such as anesthesia,<xref rid="i2164-2591-8-6-21-b17" ref-type="bibr">17</xref> blood pressure, oxygen saturation, and bone procedures.<xref rid="i2164-2591-8-6-21-b18" ref-type="bibr">18</xref>,<xref rid="i2164-2591-8-6-21-b19" ref-type="bibr">19</xref> We observed a huge discrepancy before the anesthesia and after anesthesia&#x02014;the amplitude decreased by roughly 60% and the latency increased by roughly 5%, which corresponded to most of the previous studies.</p><p>Several publications have made criteria to predict postoperative visual functions; for example, Harding et al.<xref rid="i2164-2591-8-6-21-b16" ref-type="bibr">16</xref> argued that the absence of a previously normal VEP for more than 4 minutes during surgical manipulation within the orbit showed a correlation with postoperative impairment of vision. The decrease in latency was correlated with better visual prognosis in several publications.<xref rid="i2164-2591-8-6-21-b13" ref-type="bibr">13</xref> Several groups used a 20% to 50% increase or decrease in amplitude as the criterion.<xref rid="i2164-2591-8-6-21-b20" ref-type="bibr">20</xref><xref rid="i2164-2591-8-6-21-b21" ref-type="bibr"/>&#x02013;<xref rid="i2164-2591-8-6-21-b22" ref-type="bibr">22</xref> Recent studies also argued the effectiveness of intraoperative VEP monitoring during surgeries that might have the risk of optic injury.<xref rid="i2164-2591-8-6-21-b04" ref-type="bibr">4</xref><xref rid="i2164-2591-8-6-21-b05" ref-type="bibr"/>&#x02013;<xref rid="i2164-2591-8-6-21-b06" ref-type="bibr">6</xref> On the contrary, several studies concluded that this procedure had no predictive value for postoperative prognosis.<xref rid="i2164-2591-8-6-21-b18" ref-type="bibr">18</xref>,<xref rid="i2164-2591-8-6-21-b19" ref-type="bibr">19</xref> We cannot rule out publication bias where a negative result was less likely to be published.</p><p>In the previous paper, we built a convolutional neural network to differentiate normal VEPs from abnormal VEPs from signals obtained from multifocal VEP examination.<xref rid="i2164-2591-8-6-21-b07" ref-type="bibr">7</xref> Still images are more suitable for the convolutional neural network. In data with dynamic properties, a combination of the convolutional and recurrent neural network was more suitable. The recurrent neural network has been proven to be useful in analyzing data, such as clinical notes,<xref rid="i2164-2591-8-6-21-b23" ref-type="bibr">23</xref>,<xref rid="i2164-2591-8-6-21-b24" ref-type="bibr">24</xref> anesthesia parameters,<xref rid="i2164-2591-8-6-21-b25" ref-type="bibr">25</xref> and cardiographs.<xref rid="i2164-2591-8-6-21-b26" ref-type="bibr">26</xref> Here, we combined a convolutional neural network and recurrent neural network with the assumption that the former can differentiate static images and the latter can recognize dynamic patterns. We chose the long-short memory layer because of its property of selectively remembering and forgetting patterns for long and short durations of time. The performance of the combining model outranged that of a simpler model using only amplitude and latency, the single convolutional neural network, and even the pretrained convolutional neural network, which suggested the usefulness of our model. We also argue that our model is simple enough in practice because the workflow (recording VEP sequences, extracting pixels, and feeding models) was automated.</p><p>Tumors for which craniotomy was warranted decades ago can now be resected using neuroendoscope with less risk of optic nerve injury. In our included cases, we always used the endoscopic transsphenoidal approach where direct injury to the optic apparatus was less likely to happen. Although we did not have enough cases for VEP decrease during monitoring, the differentiating power in these cases was not compromised. We will use generative adversarial networks to generate simulated decreasing VEP responses in future studies.</p><p>We have several limitations to our study. We did not have postoperative visual outcomes, but we argued the correlation of intraoperative monitoring and the outcome had been investigated by other studies,<xref rid="i2164-2591-8-6-21-b04" ref-type="bibr">4</xref><xref rid="i2164-2591-8-6-21-b05" ref-type="bibr"/>&#x02013;<xref rid="i2164-2591-8-6-21-b06" ref-type="bibr">6</xref>,<xref rid="i2164-2591-8-6-21-b14" ref-type="bibr">14</xref><xref rid="i2164-2591-8-6-21-b15" ref-type="bibr"/><xref rid="i2164-2591-8-6-21-b16" ref-type="bibr"/><xref rid="i2164-2591-8-6-21-b17" ref-type="bibr"/><xref rid="i2164-2591-8-6-21-b18" ref-type="bibr"/><xref rid="i2164-2591-8-6-21-b19" ref-type="bibr"/><xref rid="i2164-2591-8-6-21-b20" ref-type="bibr"/>&#x02013;<xref rid="i2164-2591-8-6-21-b21" ref-type="bibr">21</xref> and our study did not focus on this topic. We have a relatively small sample size for a deep learning study. But, we generated more than 10,000 images from these samples, which were sufficient for deep learning training. The developed system should be investigated in the real-time setting to discern if the system can truly detect the increasing or decreasing amplitude, especially for those signals with noise artifacts. Future studies might include &#x0201c;noise&#x0201d; as one of the outcomes and let the net decide if these artifacts should be excluded or not. The generalizability of the model should be studied in a wider broad extend including other institutions.</p><sec id="s4a"><title>Conclusion</title><p>In this paper, we developed a deep learning model to monitor the sequential change of intraoperative VEP. The model performance was comparable to human intelligence level in terms of differentiating VEP with increasing amplitude, decreasing amplitude, or with no change.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="tvst-08-06-26_s01"><label>Supplement 1</label><media xlink:href="tvst-08-06-26_s01.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="tvst-08-06-26_s02"><label>Supplement 2</label><media xlink:href="tvst-08-06-26_s02.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>This study is supported by Shanghai Committee of Science and Technology, China (grant numbers 18441901400, 16ZR1404500, and 17JC1402100). Nidan Qiao is supported by the 2018 Milstein Medical Asian American Partnership Foundation translational medicine fellowship. We thank Brooke Swearingen for his language editing on the manuscript.</p><p>Disclosure: <bold>N. Qiao</bold>, None; <bold>M. Song</bold>, None; <bold>Z. Ye</bold>, None; <bold>W. He</bold>, None; <bold>Z. Ma</bold>, None; <bold>Y. Wang</bold>, None; <bold>Y. Zhang</bold>, None; <bold>X. Shou</bold>, None</p></ack><ref-list><title>References</title><ref id="i2164-2591-8-6-21-b01"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bresson</surname><given-names>D</given-names></name><name><surname>Herman</surname><given-names>P</given-names></name><name><surname>Polivka</surname><given-names>M</given-names></name><name><surname>Froelich</surname><given-names>S</given-names></name></person-group><article-title>Sellar lesions/pathology</article-title><source><italic toggle="yes">Otolaryngol Clin North Am</italic></source><year>2016</year><volume>49</volume><fpage>63</fpage><lpage>93</lpage><pub-id pub-id-type="pmid">26614829</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b02"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freda</surname><given-names>PU</given-names></name><name><surname>Post</surname><given-names>KD</given-names></name></person-group><article-title>Differential diagnosis of sellar masses</article-title><source><italic toggle="yes">Endocrinol Metab Clin North Am</italic></source><year>1999</year><volume>28</volume><fpage>81</fpage><lpage>117</lpage><comment>vi</comment><pub-id pub-id-type="pmid">10207686</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b03"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>N</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Shou</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Zhao</surname><given-names>Y</given-names></name></person-group><article-title>Discrepancy between structural and functional visual recovery in patients after trans-sphenoidal pituitary adenoma resection</article-title><source><italic toggle="yes">Clin Neurol Neurosurg</italic></source><year>2016</year><volume>151</volume><fpage>9</fpage><lpage>17</lpage><pub-id pub-id-type="pmid">27728836</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b04"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>R</given-names></name><name><surname>Schwartz</surname><given-names>J</given-names></name><name><surname>Loewenstern</surname><given-names>J</given-names></name><etal/></person-group><article-title>The predictive role of intra-operative visual evoked potentials (VEP) in visual improvement after endoscopic pituitary tumor resection in large and complex tumors: description and validation of a method</article-title><source><italic toggle="yes">World Neurosurg</italic></source><year>2019</year><volume>126</volume><fpage>e136</fpage><lpage>e143</lpage><pub-id pub-id-type="pmid">30794978</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b05"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutzwiller</surname><given-names>EM</given-names></name><name><surname>Cabrilo</surname><given-names>I</given-names></name><name><surname>Radovanovic</surname><given-names>I</given-names></name><name><surname>Schaller</surname><given-names>K</given-names></name><name><surname>Bo&#x000eb;x</surname><given-names>C</given-names></name></person-group><article-title>Intraoperative monitoring with visual evoked potentials for brain surgeries</article-title><source><italic toggle="yes">J Neurosurg</italic></source><year>2018</year><volume>130</volume><fpage>654</fpage><lpage>660</lpage><pub-id pub-id-type="pmid">29600911</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b06"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyama</surname><given-names>K</given-names></name><name><surname>Wanibuchi</surname><given-names>M</given-names></name><name><surname>Honma</surname><given-names>T</given-names></name><name><surname>Komatsu</surname><given-names>K</given-names></name><name><surname>Akiyama</surname><given-names>Y</given-names></name><name><surname>Mikami</surname><given-names>T</given-names></name><name><surname>Mikuni</surname><given-names>N</given-names></name></person-group><article-title>Effectiveness of intraoperative visual evoked potential in avoiding visual deterioration during endonasal transsphenoidal surgery for pituitary tumors</article-title><source><italic toggle="yes">Neurosurg Rev</italic></source><year>2018</year><volume>93</volume><fpage>311</fpage><lpage>317</lpage></element-citation></ref><ref id="i2164-2591-8-6-21-b07"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>N</given-names></name></person-group><article-title>Using deep learning for the classification of images generated by multifocal visual evoked potential</article-title><source><italic toggle="yes">Front Neurol</italic></source><year>2018</year><volume>9</volume><fpage>905</fpage><lpage>904</lpage><pub-id pub-id-type="pmid">30405526</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b08"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palaniappan</surname><given-names>R</given-names></name><name><surname>Mandic</surname><given-names>DP</given-names></name></person-group><article-title>Biometrics from brain electrical activity: a machine learning approach</article-title><source><italic toggle="yes">IEEE Trans Pattern Anal Mach Intell</italic></source><year>2007</year><volume>29</volume><fpage>738</fpage><lpage>742</lpage><pub-id pub-id-type="pmid">17299228</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b09"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>T</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Lv</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Yao</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>P</given-names></name></person-group><article-title>The extraction of motion-onset VEP BCI features based on deep learning and compressed sensing</article-title><source><italic toggle="yes">J Neurosci Methods</italic></source><year>2017</year><volume>275</volume><fpage>80</fpage><lpage>92</lpage><pub-id pub-id-type="pmid">27845150</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>S</given-names></name><name><surname>Downing</surname><given-names>NL</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Milstein</surname><given-names>A</given-names></name></person-group><article-title>Bedside computer vision&#x02014;moving artificial intelligence from driver assistance to patient safety</article-title><source><italic toggle="yes">N Engl J Med</italic></source><year>2018</year><volume>378</volume><fpage>1271</fpage><lpage>1273</lpage><pub-id pub-id-type="pmid">29617592</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>E</given-names></name><name><surname>Schuetz</surname><given-names>A</given-names></name><name><surname>Stewart</surname><given-names>WF</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Using recurrent neural network models for early detection of heart failure onset</article-title><source><italic toggle="yes">J Am Med Inform Assoc</italic></source><year>2017</year><volume>24</volume><fpage>361</fpage><lpage>370</lpage><pub-id pub-id-type="pmid">27521897</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odom</surname><given-names>JV</given-names></name><name><surname>Bach</surname><given-names>M</given-names></name><name><surname>Brigell</surname><given-names>M</given-names></name><etal/></person-group><article-title>ISCEV standard for clinical visual evoked potentials: (2016 update)</article-title><source><italic toggle="yes">Doc Ophthalmol</italic></source><year>2016</year><volume>133</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="i2164-2591-8-6-21-b13"><label>13.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Ramakrishna</surname><given-names>V</given-names></name><name><surname>Michael</surname><given-names>C</given-names></name><name><surname>Abhishek</surname><given-names>D</given-names></name><name><surname>Devi</surname><given-names>P</given-names></name><name><surname>Dhruv</surname><given-names>B</given-names></name></person-group><article-title>Grad-CAM: visual explanations from deep networks via gradient-based localization</article-title><source><italic toggle="yes">2017 IEEE International Conference on Computer Vision (ICCV)</italic></source><pub-id pub-id-type="doi">10.1109/ICCV.2017.74</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>JE</given-names></name><name><surname>Arden</surname><given-names>G</given-names></name><name><surname>Jones</surname><given-names>BR</given-names></name></person-group><article-title>Continuous monitoring of the visually evoked response during intra-orbital surgery</article-title><source><italic toggle="yes">Trans Ophthalmol Soc UK</italic></source><year>1973</year><volume>93</volume><fpage>311</fpage><lpage>314</lpage><pub-id pub-id-type="pmid">4526452</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussain</surname><given-names>SS</given-names></name><name><surname>Laljee</surname><given-names>HC</given-names></name><name><surname>Horrocks</surname><given-names>JM</given-names></name><name><surname>Tec</surname><given-names>H</given-names></name><name><surname>Grace</surname><given-names>AR</given-names></name></person-group><article-title>Monitoring of intra-operative visual evoked potentials during functional endoscopic sinus surgery (FESS) under general anaesthesia</article-title><source><italic toggle="yes">J Laryngol Otol</italic></source><year>1996</year><volume>110</volume><fpage>31</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">8745778</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harding</surname><given-names>GF</given-names></name><name><surname>Bland</surname><given-names>JD</given-names></name><name><surname>Smith</surname><given-names>VH</given-names></name></person-group><article-title>Visual evoked potential monitoring of optic nerve function during surgery</article-title><source><italic toggle="yes">J Neurol Neurosurg Psychiatr</italic></source><year>1990</year><volume>53</volume><fpage>890</fpage><lpage>895</lpage></element-citation></ref><ref id="i2164-2591-8-6-21-b17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiedemayer</surname><given-names>H</given-names></name><name><surname>Fauser</surname><given-names>B</given-names></name><name><surname>Armbruster</surname><given-names>W</given-names></name><name><surname>Gasser</surname><given-names>T</given-names></name><name><surname>Stolke</surname><given-names>D</given-names></name></person-group><article-title>Visual evoked potentials for intraoperative neurophysiologic monitoring using total intravenous anesthesia</article-title><source><italic toggle="yes">J Neurosurg Anesthesiol</italic></source><year>2003</year><volume>15</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">12499978</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chacko</surname><given-names>AG</given-names></name><name><surname>Babu</surname><given-names>KS</given-names></name><name><surname>Chandy</surname><given-names>MJ</given-names></name></person-group><article-title>Value of visual evoked potential monitoring during trans-sphenoidal pituitary surgery</article-title><source><italic toggle="yes">Br J Neurosurg</italic></source><year>1996</year><volume>10</volume><fpage>275</fpage><lpage>278</lpage><pub-id pub-id-type="pmid">8799538</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cedzich</surname><given-names>C</given-names></name><name><surname>Schramm</surname><given-names>J</given-names></name><name><surname>Fahlbusch</surname><given-names>R</given-names></name></person-group><article-title>Are flash-evoked visual potentials useful for intraoperative monitoring of visual pathway function?</article-title><source><italic toggle="yes">Neurosurgery</italic></source><year>1987</year><volume>21</volume><fpage>709</fpage><lpage>715</lpage><pub-id pub-id-type="pmid">3696406</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S-B</given-names></name><name><surname>Park</surname><given-names>C-W</given-names></name><name><surname>Seo</surname><given-names>D-W</given-names></name><name><surname>Kong</surname><given-names>D-S</given-names></name><name><surname>Park</surname><given-names>S-K</given-names></name></person-group><article-title>Intraoperative visual evoked potential has no association with postoperative visual outcomes in transsphenoidal surgery</article-title><source><italic toggle="yes">Acta Neurochir (Wien)</italic></source><year>2012</year><volume>154</volume><fpage>1505</fpage><lpage>1510</lpage><pub-id pub-id-type="pmid">22739773</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamio</surname><given-names>Y</given-names></name><name><surname>Sakai</surname><given-names>N</given-names></name><name><surname>Sameshima</surname><given-names>T</given-names></name><etal/></person-group><article-title>Usefulness of intraoperative monitoring of visual evoked potentials in transsphenoidal surgery</article-title><source><italic toggle="yes">Neurol Med Chir (Tokyo)</italic></source><year>2014</year><volume>54</volume><fpage>606</fpage><lpage>611</lpage></element-citation></ref><ref id="i2164-2591-8-6-21-b22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>Y</given-names></name><name><surname>Regli</surname><given-names>L</given-names></name><name><surname>Bozinov</surname><given-names>O</given-names></name><name><surname>Sarnthein</surname><given-names>J</given-names></name></person-group><article-title>Clinical utility and limitations of intraoperative monitoring of visual evoked potentials</article-title><source><italic toggle="yes">PLoS One</italic></source><year>2015</year><volume>10</volume><fpage>e0120525</fpage><pub-id pub-id-type="pmid">25803287</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Che</surname><given-names>Z</given-names></name><name><surname>Purushotham</surname><given-names>S</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Sontag</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name></person-group><article-title>Recurrent neural networks for multivariate time series with missing values</article-title><source><italic toggle="yes">Sci Rep</italic></source><year>2018</year><volume>8</volume><elocation-id>6085</elocation-id><pub-id pub-id-type="pmid">29666385</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>R</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><etal/></person-group><article-title>Detection of bleeding events in electronic health record notes using convolutional neural network models enhanced with recurrent neural network autoencoders: deep learning approach</article-title><source><italic toggle="yes">JMIR Med Inform</italic></source><year>2019</year><volume>7</volume><fpage>e10788</fpage><pub-id pub-id-type="pmid">30735140</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>SC</given-names></name><name><surname>Nair</surname><given-names>B</given-names></name><name><surname>Vavilala</surname><given-names>MS</given-names></name><etal/></person-group><article-title>Explainable machine-learning predictions for the prevention of hypoxaemia during surgery</article-title><source><italic toggle="yes">Nat Biomed Eng</italic></source><year>2018</year><volume>2</volume><fpage>749</fpage><lpage>760</lpage><pub-id pub-id-type="pmid">31001455</pub-id></element-citation></ref><ref id="i2164-2591-8-6-21-b26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>Z</given-names></name><name><surname>Nash</surname><given-names>MP</given-names></name><name><surname>Cheng</surname><given-names>E</given-names></name><name><surname>Fedorov</surname><given-names>VV</given-names></name><name><surname>Stiles</surname><given-names>MK</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name></person-group><article-title>ECG signal classification for the detection of cardiac arrhythmias using a convolutional recurrent neural network</article-title><source><italic toggle="yes">Physiol Meas</italic></source><year>2018</year><volume>39</volume><fpage>094006</fpage><pub-id pub-id-type="pmid">30102248</pub-id></element-citation></ref></ref-list></back></article>