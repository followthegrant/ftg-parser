<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31905916</article-id><article-id pub-id-type="pmc">6983164</article-id><article-id pub-id-type="doi">10.3390/s20010206</article-id><article-id pub-id-type="publisher-id">sensors-20-00206</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>JsrNet: A Joint Sampling&#x02013;Reconstruction Framework for Distributed Compressive Video Sensing</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Can</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00206">1</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Yutong</given-names></name><xref ref-type="aff" rid="af2-sensors-20-00206">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Chao</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00206">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Dengyin</given-names></name><xref ref-type="aff" rid="af3-sensors-20-00206">3</xref><xref ref-type="aff" rid="af4-sensors-20-00206">4</xref><xref rid="c1-sensors-20-00206" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-20-00206"><label>1</label>College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing 210003, China; <email>chencan15@126.com</email> (C.C.); <email>zhouchaogl@163.com</email> (C.Z.)</aff><aff id="af2-sensors-20-00206"><label>2</label>College of Information Technology, Shanghai Ocean University, Shanghai 201306, China; <email>wuyutong20000801@126.com</email></aff><aff id="af3-sensors-20-00206"><label>3</label>College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing 210003, China</aff><aff id="af4-sensors-20-00206"><label>4</label>Jiangsu Key Laboratory of Broadband Wireless Communication and Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing 210003, China</aff><author-notes><corresp id="c1-sensors-20-00206"><label>*</label>Correspondence: <email>zhangdy@njupt.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>30</day><month>12</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2020</year></pub-date><volume>20</volume><issue>1</issue><elocation-id>206</elocation-id><history><date date-type="received"><day>01</day><month>12</month><year>2019</year></date><date date-type="accepted"><day>27</day><month>12</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Huge video data has posed great challenges on computing power and storage space, triggering the emergence of distributed compressive video sensing (DCVS). Hardware-friendly characteristics of this technique have consolidated its position as one of the most powerful architectures in source-limited scenarios, namely, wireless video sensor networks (WVSNs). Recently, deep convolutional neural networks (DCNNs) are successfully applied in DCVS because traditional optimization-based methods are computationally elaborate and hard to meet the requirements of real-time applications. In this paper, we propose a joint sampling&#x02013;reconstruction framework for DCVS, named &#x0201c;JsrNet&#x0201d;. JsrNet utilizes the whole group of frames as the reference to reconstruct each frame, regardless of key frames and non-key frames, while the existing frameworks only utilize key frames as the reference to reconstruct non-key frames. Moreover, different from the existing frameworks which only focus on exploiting complementary information between frames in joint reconstruction, JsrNet also applies this conception in joint sampling by adopting learnable convolutions to sample multiple frames jointly and simultaneously in an encoder. JsrNet fully exploits spatial&#x02013;temporal correlation in both sampling and reconstruction, and achieves a competitive performance in both the quality of reconstruction and computational complexity, making it a promising candidate in source-limited, real-time scenarios.</p></abstract><kwd-group><kwd>distributed compressive video sensing</kwd><kwd>deep convolutional neural networks</kwd><kwd>video signal processing</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-20-00206"><title>1. Introduction</title><p>Compressive sensing (CS) [<xref rid="B1-sensors-20-00206" ref-type="bibr">1</xref>,<xref rid="B2-sensors-20-00206" ref-type="bibr">2</xref>] is a powerful framework for signal acquisition and processing. By adopting a measurement matrix, CS integrates sampling and compression, making it desirable in many applications such as magnetic resonance imaging (MRI) [<xref rid="B3-sensors-20-00206" ref-type="bibr">3</xref>] and cognitive radio communication [<xref rid="B4-sensors-20-00206" ref-type="bibr">4</xref>]. CS states that if the measurement matrix satisfies the restricted isometry property (RIP), we can recover one sparse or compressible signal from fewer measurements than that suggested by the Nyquist theory [<xref rid="B5-sensors-20-00206" ref-type="bibr">5</xref>]. Frame-based sampling [<xref rid="B6-sensors-20-00206" ref-type="bibr">6</xref>,<xref rid="B7-sensors-20-00206" ref-type="bibr">7</xref>] is impractical due to limited storage space. To overcome this problem, Lu [<xref rid="B8-sensors-20-00206" ref-type="bibr">8</xref>] proposed block-based CS that reduced much of the implementation costs.</p><p>Over the past decade, CS has been successfully applied in video signal processing because compared to still images, video signals contain more spatial and temporal redundancies which can be further exploited. One of the most powerful architectures in video CS reconstruction in the literature is distributed compressive video sensing (DCVS), which is desirable in source-limited scenarios because of its hardware-friendly characteristics. In DCVS, the first frame of a given group of frames is classified as the key frame and the remaining frames are classified as non-key frames. In an encoder, each frame is sampled independently; in a decoder, key frames are reconstructed independently and served as references in the recovery of non-key frames. A large number of algorithms have been proposed for DCVS, which focus on how to further exploit spatial&#x02013;temporal correlation in decoders to improve reconstruction performance. Inspired by motion estimation (ME) and motion compensation (MC), the multi-hypothesis (MH) prediction algorithm [<xref rid="B9-sensors-20-00206" ref-type="bibr">9</xref>] utilizes a combination of blocks to generate a prediction for the target block. Combining MH and residual reconstruction [<xref rid="B10-sensors-20-00206" ref-type="bibr">10</xref>], the MH-BCS-SPL algorithm [<xref rid="B11-sensors-20-00206" ref-type="bibr">11</xref>] yields state-of-the-art results for DCVS. Further improvements based on MH are proposed in [<xref rid="B12-sensors-20-00206" ref-type="bibr">12</xref>,<xref rid="B13-sensors-20-00206" ref-type="bibr">13</xref>]. Zhao [<xref rid="B14-sensors-20-00206" ref-type="bibr">14</xref>] proposed a reweighted residual sparsity (RRS) model which not only takes full advantage of spatial correlation of videos to produce good initial recoveries, but also utilizes temporal correlation between frames to further enhance the reconstruction quality. To enhance the robustness of MH prediction, Chen [<xref rid="B15-sensors-20-00206" ref-type="bibr">15</xref>] proposed a reweighted Tikhonov regularization which considers the impact of each hypothesis. Although these methods can yield competitive reconstruction quality, they are time-consuming and do not easily meet the requirements of real-time applications. Thus, MH-BCS-SPL is commonly adopted in DCVS for its acceptable reconstruction performance and low computational complexity [<xref rid="B16-sensors-20-00206" ref-type="bibr">16</xref>,<xref rid="B17-sensors-20-00206" ref-type="bibr">17</xref>,<xref rid="B18-sensors-20-00206" ref-type="bibr">18</xref>].</p><p>Iterative optimization-based methods used in traditional DCVS are computationally elaborate and do not easily meet the requirements of real-time applications. Fortunately, as deep convolutional neural networks (DCNNs) have shown great potential in solving computer vision tasks, such as classification and object detection, applying DCNN to solve CS problem has attracted considerable attention. Different from traditional approaches, DCNN-based approaches utilize deep learning techniques to directly recover the original signal from the measurement vector, achieving a better trade-off between reconstruction quality and computational complexity. A stacked denoising autoencoder (SDA) [<xref rid="B19-sensors-20-00206" ref-type="bibr">19</xref>] was first proposed to efficiently estimate a signal. DeepInverse [<xref rid="B20-sensors-20-00206" ref-type="bibr">20</xref>] was first proposed to utilize a DCNN to learn inverse transformation. Inspired from the denoising-based approximate message passing (D-AMP) algorithm [<xref rid="B7-sensors-20-00206" ref-type="bibr">7</xref>], Metzler [<xref rid="B21-sensors-20-00206" ref-type="bibr">21</xref>] developed Learned D-AMP (LDAMP), which unrolls D-AMP algorithm into a novel neural network architecture. Reconnet [<xref rid="B22-sensors-20-00206" ref-type="bibr">22</xref>] first reconstructs each block using a DCNN architecture and assembles reconstructed blocks to feed into an off-the-shelf denoiser. In Deepcodec [<xref rid="B23-sensors-20-00206" ref-type="bibr">23</xref>], the sensing process of images is non-linear and learned from the training data. Recently, several video frameworks were proposed. Combining DCNNs and long short-term memory (LSTM) networks, CSVideoNet [<xref rid="B24-sensors-20-00206" ref-type="bibr">24</xref>] achieves a promising performance in DCVS. Blocking artifacts were introduced in these methods because they neglect edge continuity between blocks. To reduce blocking artifacts, instead of utilizing post-processing [<xref rid="B22-sensors-20-00206" ref-type="bibr">22</xref>], a novel network in which all measurements of blocks from one image are used simultaneously to reconstruct the full image was proposed in [<xref rid="B25-sensors-20-00206" ref-type="bibr">25</xref>]. A multi-frame quality enhancement (MFQE) [<xref rid="B26-sensors-20-00206" ref-type="bibr">26</xref>] approach based on LSTM networks was proposed, which enhances the quality of low-quality frames by using their neighboring high-quality frames.</p><p>The promise of the existing DCNN-based frameworks has been offset by two problems. First, the existing frameworks only utilize key frames as the reference to reconstruct non-key frames. Secondly, the conception of exploiting complementary information between frames is only applied in joint reconstruction. To address these problems, we propose a joint sampling&#x02013;reconstruction framework for DCVS, named &#x0201c;JsrNet&#x0201d;. The main contributions of our work are three-fold:<list list-type="order"><list-item><p>JsrNet utilizes the whole group of frames as the reference to reconstruct each frame, regardless of key frames and non-key frames.</p></list-item><list-item><p>JsrNet not only applies the conception of exploiting complementary information between frames in joint reconstruction, but also in joint sampling by adopting learnable convolutions to sample multiple frames jointly and simultaneously in an encoder.</p></list-item><list-item><p>JsrNet exploits spatial&#x02013;temporal correlation in both sampling and reconstruction, and achieves a competitive performance on both the quality of reconstruction and computational complexity, making it a promising candidate in source-limited, real-time scenarios.</p></list-item></list></p><p>The remainder of this paper is organized as follows. In <xref ref-type="sec" rid="sec2-sensors-20-00206">Section 2</xref>, we review the backgrounds of our work. <xref ref-type="sec" rid="sec3-sensors-20-00206">Section 3</xref> introduces a detailed description of the proposed JsrNet. In <xref ref-type="sec" rid="sec4-sensors-20-00206">Section 4</xref>, we provide the experimental results. Conclusions are drawn in <xref ref-type="sec" rid="sec5-sensors-20-00206">Section 5</xref>.</p></sec><sec id="sec2-sensors-20-00206"><title>2. Backgrounds</title><sec id="sec2dot1-sensors-20-00206"><title>2.1. Preliminary of CS Theory</title><p>CS theory states that we can measure a signal <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> with a sub-Nyquist rate through a measurement matrix <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD1-sensors-20-00206"><label>(1)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the measurements vector and <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sampling rate. In block-based CS, <italic>n</italic> is equal to <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>B</italic> denotes the block size. Since <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x0226a;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the recovery of <italic>x</italic> from <italic>y</italic> is ill-posed. Regularized iterative algorithms [<xref rid="B7-sensors-20-00206" ref-type="bibr">7</xref>,<xref rid="B27-sensors-20-00206" ref-type="bibr">27</xref>] have become the standard approach to this ill-posed inverse problem in the past few decades:<disp-formula id="FD2-sensors-20-00206"><label>(2)</label><mml:math id="mm8"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mo>&#x003bb;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a non-negative constant and <italic>R</italic>(<italic>x</italic>) represents some priors about the signal structure, such as sparse priors [<xref rid="B28-sensors-20-00206" ref-type="bibr">28</xref>,<xref rid="B29-sensors-20-00206" ref-type="bibr">29</xref>] and low-rank priors [<xref rid="B30-sensors-20-00206" ref-type="bibr">30</xref>,<xref rid="B31-sensors-20-00206" ref-type="bibr">31</xref>]. These methods suffer from high computational complexity and parameter-tuning issues. Due to the powerful learning capability of deep networks, deep learning-based algorithms [<xref rid="B19-sensors-20-00206" ref-type="bibr">19</xref>,<xref rid="B20-sensors-20-00206" ref-type="bibr">20</xref>,<xref rid="B21-sensors-20-00206" ref-type="bibr">21</xref>,<xref rid="B22-sensors-20-00206" ref-type="bibr">22</xref>,<xref rid="B23-sensors-20-00206" ref-type="bibr">23</xref>,<xref rid="B24-sensors-20-00206" ref-type="bibr">24</xref>,<xref rid="B25-sensors-20-00206" ref-type="bibr">25</xref>,<xref rid="B32-sensors-20-00206" ref-type="bibr">32</xref>] have successfully shown great potential in solving this inverse problem.</p></sec><sec id="sec2dot2-sensors-20-00206"><title>2.2. Unsupervised Learning</title><p>Both supervised learning and unsupervised learning have been successfully applied in image CS frameworks; however, we highlight the need for using unsupervised learning to find and represent structure in video CS frameworks because videos contain a large amount of spatial and temporal redundancies which makes them particularly suitable for building unsupervised learning models. This is consistent with one of the motivations of our work that we aim to apply the conception of exploiting complementary information between frames in joint sampling.</p><p>Given a <italic>T</italic>-length group of pictures <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we use mean square error (MSE) as the loss function which favors high peak signal-to-noise ratio (PSNR):<disp-formula id="FD3-sensors-20-00206"><label>(3)</label><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x00398;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mo>&#x00398;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#x00398; represents the parameters in the designed network and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mo>&#x00398;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the output of the network. One advantage of these algorithms is low computational complexity because signals are reconstructed by feeding to a single forward model, instead of optimizing iteratively.</p></sec></sec><sec id="sec3-sensors-20-00206"><title>3. The Proposed JsrNet</title><p>In this section, we propose a joint sampling&#x02013;reconstruction framework for DCVS, named &#x0201c;JsrNet&#x0201d;. JsrNet measures signals in a block-based manner, but reconstructs signals in a frame-based manner. <xref ref-type="fig" rid="sensors-20-00206-f001">Figure 1</xref> shows the overview architecture of JsrNet which contains three modules: (1) a convolutional neural network (CNN) for joint sampling, in which multiple frames are sampled jointly and simultaneously by using learnable convolutions in a block-based manner; (2) a spatial DCNN for initial recovery, in which all measurements of blocks from one image are used simultaneously to output the intermediate reconstructed image; and (3) a temporal DCNN for joint reconstruction, in which each frame is reconstructed by exploiting temporal correlation within the whole group of frames. These three modules consist of an integrated end-to-end model whose parameters are jointly trained.</p><sec id="sec3dot1-sensors-20-00206"><title>3.1. CNN for Joint Sampling</title><p>Different from traditional approaches which commonly utilize the random Gaussian matrix [<xref rid="B22-sensors-20-00206" ref-type="bibr">22</xref>] as the measurement matrix, we use a convolutional layer [<xref rid="B33-sensors-20-00206" ref-type="bibr">33</xref>] in which parameters only depend on the size and number of convolution kernels to mimic the sampling operation. <xref ref-type="fig" rid="sensors-20-00206-f002">Figure 2</xref> shows the structure of the encoder for joint sampling. First, video sequences are divided into several <italic>T</italic>-length groups of frames, in which a key frame <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is followed by some non-key frames <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each frame goes through a specific convolution layer in which rectified linear units (ReLU) activation [<xref rid="B34-sensors-20-00206" ref-type="bibr">34</xref>] was removed to obtain measurements in a block-based manner. High sampling rates, <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi mathvariant="normal">K</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, are allocated to key frames, whereas relatively low sampling rates, <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, are allocated to non-key frames. During the training process, the sampling of multiple frames is jointly optimized, fully exploiting spatial&#x02013;temporal correlation in the encoder. Different from the existing frameworks which only focus on exploiting complementary information between frames in joint reconstruction, JsrNet also applies this conception in joint sampling by adopting learnable convolutions to sample multiple frames jointly and simultaneously in the encoder.</p></sec><sec id="sec3dot2-sensors-20-00206"><title>3.2. Spatial DCNN for Initial Recovery</title><p>In this subsection, we design a spatial DCNN for the initial recovery of each frame which is shown in <xref ref-type="fig" rid="sensors-20-00206-f003">Figure 3</xref>. Inspired by [<xref rid="B25-sensors-20-00206" ref-type="bibr">25</xref>] which effectively removes the blocking artifacts, all measurements of blocks from one image are used simultaneously to reconstruct the full image. Different from typical DCNNs used for classification and segmentation, we remove the pooling layer which can cause information loss. We first use a convolutional layer which uses <italic>n</italic> convolution kernels of size 1 &#x000d7; 1 with stride 1 and a reshape layer to transform the measurements to the feature map which has the same dimension as the final reconstructed frame. Then, we stack 12 convolutional layers to obtain the intermediate reconstruction <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. All the convolutional layers are followed by ReLU activation, except the final layer, and each frame has its corresponding spatial DCNN, instead of a universal one.</p></sec><sec id="sec3dot3-sensors-20-00206"><title>3.3. Temporal DCNN for Joint Reconstruction</title><p>JsrNet utilizes the whole group of frames as the reference to reconstruct each frame, regardless of key frames and non-key frames, while the existing frameworks only utilize key frames as the reference to reconstruct non-key frames. <xref ref-type="fig" rid="sensors-20-00206-f004">Figure 4</xref> shows the structure of the temporal DCNN for joint reconstruction, which is made up of several basic units (BUs). As shown in <xref ref-type="fig" rid="sensors-20-00206-f005">Figure 5</xref>, BU consists of a concatenating layer, an inception layer, and a convolutional layer. In the concatenating layer, we concatenate the intermediately reconstructed key frame and the output of the previous layer into a single tensor. Adaptively exploiting temporal correlation is the key to improve the overall reconstruction quality in traditional DCVS [<xref rid="B17-sensors-20-00206" ref-type="bibr">17</xref>,<xref rid="B18-sensors-20-00206" ref-type="bibr">18</xref>]. Therefore, we adopt the inception module [<xref rid="B35-sensors-20-00206" ref-type="bibr">35</xref>] in the inception layer to let DCNN adaptively select the optimal size to exploit temporal correlation. In the last convolutional layer, 3 &#x000d7; 3 convolution kernels are utilized to reduce the number of channels from <italic>T</italic> to <italic>T</italic> &#x02212; 1. ReLU activation is removed in this convolutional layer. After stacking 5 BUs, we add a shortcut connection to the plain network, making the DCNN easier to train [<xref rid="B36-sensors-20-00206" ref-type="bibr">36</xref>]. Then, we de-concatenate the output to obtain the final reconstructed frames.</p></sec></sec><sec id="sec4-sensors-20-00206"><title>4. Experiments</title><sec id="sec4dot1-sensors-20-00206"><title>4.1. Training Settings</title><p>We implemented the proposed JsrNet with Tensorflow framework using NVIDIA Titan XP GPU. UCF-101 dataset [<xref rid="B37-sensors-20-00206" ref-type="bibr">37</xref>] was used to benchmark the proposed network because there is no standard dataset designed for DCVS. Due to limited GPU memory, we cropped the central 160 &#x000d7; 160 patch from each frame and retained only the luminance component. The size of group of frames was set to 4 and the batch size was set to 16. Groups were randomly split into 80% for training, 10% for validation, and the remaining for testing. The sampling rate of key frames <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">K</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was set to 0.25, whereas the sampling rate of non-key frames <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was set to 0.01, 0.04, and 0.1. We adopted the Adam optimizer [<xref rid="B38-sensors-20-00206" ref-type="bibr">38</xref>] with a learning rate of 0.0001 to train JsrNet for 50 epochs.</p><p>In DCVS, the reconstruction quality of key frames plays a significant role in improving the overall reconstruction performance, because key frames are allocated with high sampling rates for guaranteed high reconstruction quality to serve as references in the recovery of non-key frames. The reconstruction quality of key frames, however, can be easily degraded by the poor reconstruction quality of non-key frames in joint optimizations. Hence, we pre-trained the sampling part and the spatial DCNN for key frames based on VOC dataset [<xref rid="B39-sensors-20-00206" ref-type="bibr">39</xref>]. The learning rate was set to 0.0001 and the batch size was set to 128. We pre-trained the subnetwork for 200 epochs.</p></sec><sec id="sec4dot2-sensors-20-00206"><title>4.2. Performance Comparisons</title><p>We compared the proposed JsrNet with four state-of-the-art algorithms experimentally: (1) D-AMP [<xref rid="B7-sensors-20-00206" ref-type="bibr">7</xref>], which is a representative of the state-of-the-art iterative algorithms developed for CS; (2) Reconnet [<xref rid="B22-sensors-20-00206" ref-type="bibr">22</xref>], which is a dedicated DCNN-based approach for block-based CS; (3) FIR [<xref rid="B25-sensors-20-00206" ref-type="bibr">25</xref>], which is a novel full image recovery CS framework for block-based CS; and (4) MH-BCS-SPL [<xref rid="B11-sensors-20-00206" ref-type="bibr">11</xref>], which achieves the state-of-the-art performance in DCVS. CSVideoNet [<xref rid="B24-sensors-20-00206" ref-type="bibr">24</xref>] is another architecture designed for DCVS and was intended to be compared; however, we could not present the results of CSVideoNet due to limited GPU memory. The parameters used in these methods were set as default to keep fairness.</p><p>We adopted PSNR and structural similarity (SSIM) as objective standards to measure reconstruction performance. <xref rid="sensors-20-00206-t001" ref-type="table">Table 1</xref> shows the average PSNR and SSIM of the test set. JsrNet outperformed the other four algorithms. For example, in experiments with <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.01, JsrNet outperformed Reconnet, MH-BCS-SPL, FIR, and D-AMP by 8.37 dB, 2.91 dB, 4.03 dB, and 16.69 dB, respectively. Furthermore, <xref ref-type="fig" rid="sensors-20-00206-f006">Figure 6</xref> and <xref ref-type="fig" rid="sensors-20-00206-f007">Figure 7</xref> present examples of visual comparisons with different sampling rates. Reconnet, D-AMP, and MH-BCS-SPL suffered from blocking artifacts, especially when having low sampling rates. The main reason was that they compressed and recovered signals in a block-wise manner, but ignored edge continuity between blocks. Benefiting from exploiting temporal correlation instead of treating each frame independently, MH-BCS-SPL slightly alleviated the blocking artifacts and achieved an acceptable performance. Although FIR succeeded in reducing the blocking artifacts because all the measurements of blocks from one image were used to simultaneously reconstruct the full image, FIR failed in preserving image details. It can be seen clearly that JsrNet achieved the best performance. There were several factors contributing to this improvement. First, combining the advantages of FIR and MH-BCS-SPL, JsrNet utilized the whole group of frames as the reference to reconstruct each frame, regardless of key frames and non-key frames. JsrNet further applied the conception of exploiting complementary information between frames in joint sampling by adopting learnable convolutions to sample multiple frames jointly and simultaneously in the encoder.</p><p><xref rid="sensors-20-00206-t002" ref-type="table">Table 2</xref> shows the comparisons of average reconstruction speed of each frame. Compared with MH-BCS-SPL and DAMP, the reconstruction time of JsrNet was nearly 1000 times faster. This was because DCNN-based approaches reconstruct video sequences via a forward model instead of solving an iterative optimization problem. More importantly, the speed of DCNN-based approaches depends only on the model capacity, whereas traditional approaches depend on the sampling rate. Compared with Reconnet and FIR, which treat each frame independently, JsrNet reconstructed frames simultaneously, and achieved the best performance.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-20-00206"><title>5. Conclusions</title><p>A DCNN-based learning framework, named &#x0201c;JsrNet&#x0201d;, is proposed with the aim to apply DCVS in real-time applications. JsrNet utilizes the whole group of frames as the reference to reconstruct each frame, regardless of key frames and non-key frames. Moreover, JsrNet applies the conception of exploiting complementary information between frames in joint sampling by adopting learnable convolutions to sample multiple frames jointly and simultaneously in an encoder. Benefiting from fully exploiting spatial&#x02013;temporal correlation in both sampling and reconstruction, JsrNet achieves a satisfying reconstruction quality without the blocking artifacts. Moreover, the non-iterative nature of DCNNs leads to low computational complexity, making JsrNet a promising candidate in source-limited, real-time scenarios. In future, we will focus on utilizing generative models for the representation and reconstruction of video sequences.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank the editors and anonymous reviewers for providing helpful suggestions for improving the quality of this manuscript, and the MDPI English Editing Team.</p></ack><notes><title>Author Contributions</title><p>Conceptualization, C.C.; methodology, C.C.; software, C.C.; validation, C.C.; formal analysis, C.C.; investigation, C.C.; resources, C.C.; data curation, C.C.; writing&#x02014;original draft preparation, C.C.; writing&#x02014;review and editing, Y.W. and C.Z.; visualization, C.C.; supervision, D.Z.; project administration, D.Z.; funding acquisition, D.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>This work was partially supported by the National Natural Science Foundation of China (No. 61571241 and 61872423), the Industry Prospective Primary Research &#x00026; Development Plan of Jiangsu Province (No. BE2017111), the Scientific Research Foundation of the Higher Education Institutions of Jiangsu Province (No. 19KJA180006), and Postgraduate Research &#x00026; Practice Innovation Program of Jiangsu Province (No. KYCX18_0889).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-20-00206"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cand&#x000e8;s</surname><given-names>E.J.</given-names></name><name><surname>Romberg</surname><given-names>J.</given-names></name><name><surname>Tao</surname><given-names>T.</given-names></name></person-group><article-title>Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</article-title><source>IEEE Trans. Inf. Theory</source><year>2006</year><volume>52</volume><fpage>489</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1109/TIT.2005.862083</pub-id></element-citation></ref><ref id="B2-sensors-20-00206"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoho</surname><given-names>D.L.</given-names></name></person-group><article-title>Compressed sensing</article-title><source>IEEE Trans. Inf. Theory</source><year>2006</year><volume>52</volume><fpage>1289</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1109/TIT.2006.871582</pub-id></element-citation></ref><ref id="B3-sensors-20-00206"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>J.Y.</given-names></name><name><surname>Chen</surname><given-names>F.</given-names></name><name><surname>Sandino</surname><given-names>C.</given-names></name><name><surname>Mardani</surname><given-names>M.</given-names></name><name><surname>Pauly</surname><given-names>J.M.</given-names></name><name><surname>Vasanawala</surname><given-names>S.S.</given-names></name></person-group><article-title>Compressed Sensing: From Research to Clinical Practice with Data-Driven Learning</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1903.07824</pub-id></element-citation></ref><ref id="B4-sensors-20-00206"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>S.K.</given-names></name><name><surname>Lagunas</surname><given-names>E.</given-names></name><name><surname>Chatzinotas</surname><given-names>S.</given-names></name><name><surname>Ottersten</surname><given-names>B.</given-names></name></person-group><article-title>Application of compressive sensing in cognitive radio communications: A survey</article-title><source>IEEE Commun. Surv. Tutor.</source><year>2016</year><volume>18</volume><fpage>1838</fpage><lpage>1860</lpage><pub-id pub-id-type="doi">10.1109/COMST.2016.2524443</pub-id></element-citation></ref><ref id="B5-sensors-20-00206"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>H.J.</given-names></name></person-group><article-title>Sampling, data transmission, and the Nyquist rate</article-title><source>Proc. IEEE</source><year>1967</year><volume>55</volume><fpage>1701</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1109/PROC.1967.5962</pub-id></element-citation></ref><ref id="B6-sensors-20-00206"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Figueiredo</surname><given-names>M.A.T.</given-names></name><name><surname>Nowak</surname><given-names>R.D.</given-names></name><name><surname>Wright</surname><given-names>S.J.</given-names></name></person-group><article-title>Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</article-title><source>IEEE J. Sel. Top. Signal Process.</source><year>2007</year><volume>1</volume><fpage>586</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1109/JSTSP.2007.910281</pub-id></element-citation></ref><ref id="B7-sensors-20-00206"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzler</surname><given-names>C.A.</given-names></name><name><surname>Maleki</surname><given-names>A.</given-names></name><name><surname>Baraniuk</surname><given-names>R.G.</given-names></name></person-group><article-title>From denoising to compressed sensing</article-title><source>IEEE Trans. Inf. Theory</source><year>2016</year><volume>62</volume><fpage>5117</fpage><lpage>5144</lpage><pub-id pub-id-type="doi">10.1109/TIT.2016.2556683</pub-id></element-citation></ref><ref id="B8-sensors-20-00206"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gan</surname><given-names>L.</given-names></name></person-group><article-title>Block compressed sensing of natural images</article-title><source>Proceedings of the 2007 IEEE 15th International Conference on Digital Signal Processing</source><conf-loc>Cardiff, UK</conf-loc><conf-date>1&#x02013;4 July 2007</conf-date></element-citation></ref><ref id="B9-sensors-20-00206"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Do</surname><given-names>T.T.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Nguyen</surname><given-names>D.T.</given-names></name><name><surname>Nguyen</surname><given-names>N.</given-names></name><name><surname>Gan</surname><given-names>L.</given-names></name><name><surname>Tran</surname><given-names>T.D.</given-names></name></person-group><article-title>Distributed compressed video sensing</article-title><source>Proceedings of the 2009 IEEE 16th International Conference on Image Processing</source><conf-loc>Cairo, Egypt</conf-loc><conf-date>7&#x02013;10 November 2009</conf-date></element-citation></ref><ref id="B10-sensors-20-00206"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mun</surname><given-names>S.</given-names></name><name><surname>Fowler</surname><given-names>J.E.</given-names></name></person-group><article-title>Residual reconstruction for block-based compressed sensing of video</article-title><source>Proceedings of the 2011 IEEE Data Compression Conference</source><conf-loc>Snowbird, UT, USA</conf-loc><conf-date>29&#x02013;31 March 2011</conf-date></element-citation></ref><ref id="B11-sensors-20-00206"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fowler</surname><given-names>J.E.</given-names></name><name><surname>Mun</surname><given-names>S.</given-names></name><name><surname>Tramel</surname><given-names>E.W.</given-names></name></person-group><article-title>Block-based compressed sensing of images and video</article-title><source>Found. Trends<sup>&#x000ae;</sup> Signal Process.</source><year>2012</year><volume>4</volume><fpage>297</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1561/2000000033</pub-id></element-citation></ref><ref id="B12-sensors-20-00206"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azghani</surname><given-names>M.</given-names></name><name><surname>Karimi</surname><given-names>M.</given-names></name><name><surname>Marvasti</surname><given-names>F.</given-names></name></person-group><article-title>Multihypothesis compressed video sensing technique</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2015</year><volume>26</volume><fpage>627</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2015.2418586</pub-id></element-citation></ref><ref id="B13-sensors-20-00206"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Qin</surname><given-names>D.</given-names></name><name><surname>Kuo</surname><given-names>Y.</given-names></name></person-group><article-title>An elastic net-based hybrid hypothesis method for compressed video sensing</article-title><source>Multimed. Tools Appl.</source><year>2015</year><volume>74</volume><fpage>2085</fpage><lpage>2108</lpage><pub-id pub-id-type="doi">10.1007/s11042-013-1743-y</pub-id></element-citation></ref><ref id="B14-sensors-20-00206"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>C.</given-names></name><name><surname>Ma</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Xiong</surname><given-names>R.</given-names></name><name><surname>Gao</surname><given-names>W.</given-names></name></person-group><article-title>Video compressive sensing reconstruction via reweighted residual sparsity</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2016</year><volume>27</volume><fpage>1182</fpage><lpage>1195</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2016.2527181</pub-id></element-citation></ref><ref id="B15-sensors-20-00206"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Zhou</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>P.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Iterative Reweighted Tikhonov-regularized Multihypothesis Prediction Scheme for Distributed Compressive Video Sensing</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2018</year><pub-id pub-id-type="doi">10.1109/TCSVT.2018.2886310</pub-id></element-citation></ref><ref id="B16-sensors-20-00206"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Resample-based hybrid multi-hypothesis scheme for distributed compressive video sensing</article-title><source>IEICE Trans. Inf. Syst.</source><year>2017</year><volume>100</volume><fpage>3073</fpage><lpage>3076</lpage><pub-id pub-id-type="doi">10.1587/transinf.2017EDL8133</pub-id></element-citation></ref><ref id="B17-sensors-20-00206"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>N.</given-names></name><name><surname>Xue</surname><given-names>F.</given-names></name><name><surname>Gao</surname><given-names>Y.</given-names></name></person-group><article-title>Distributed compressed video sensing based on the optimization of hypothesis set update technique</article-title><source>Multimed. Tools Appl.</source><year>2017</year><volume>76</volume><fpage>15735</fpage><lpage>15754</lpage><pub-id pub-id-type="doi">10.1007/s11042-016-3866-4</pub-id></element-citation></ref><ref id="B18-sensors-20-00206"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>K.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>A scheme for distributed compressed video sensing based on hypothesis set optimization techniques</article-title><source>Multimed. Tools Appl.</source><year>2017</year><volume>28</volume><fpage>129</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1007/s11045-015-0337-4</pub-id></element-citation></ref><ref id="B19-sensors-20-00206"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mousavi</surname><given-names>A.</given-names></name><name><surname>Patel</surname><given-names>A.B.</given-names></name><name><surname>Baraniuk</surname><given-names>R.G.</given-names></name></person-group><article-title>A deep learning approach to structured signal recovery</article-title><source>Proceedings of the 2015 IEEE 53rd Annual Allerton Conference on Communication, Control, and Computing</source><conf-loc>Monticello, IL, USA</conf-loc><conf-date>29 September&#x02013;2 October 2015</conf-date></element-citation></ref><ref id="B20-sensors-20-00206"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mousavi</surname><given-names>A.</given-names></name><name><surname>Baraniuk</surname><given-names>R.G.</given-names></name></person-group><article-title>Learning to invert: Signal recovery via deep convolutional networks</article-title><source>Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and Signal Processing</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>5&#x02013;9 March 2017</conf-date></element-citation></ref><ref id="B21-sensors-20-00206"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzler</surname><given-names>C.</given-names></name><name><surname>Mousavi</surname><given-names>A.</given-names></name><name><surname>Baraniuk</surname><given-names>R.</given-names></name></person-group><article-title>Learned D-AMP: Principled neural network based compressive image recovery</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>2017</volume><fpage>1773</fpage><lpage>1784</lpage></element-citation></ref><ref id="B22-sensors-20-00206"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kulkarni</surname><given-names>K.</given-names></name><name><surname>Lohit</surname><given-names>S.</given-names></name><name><surname>Turaga</surname><given-names>P.</given-names></name><name><surname>Kerviche</surname><given-names>R.</given-names></name><name><surname>Ashok</surname><given-names>A.</given-names></name></person-group><article-title>Reconnet: Non-iterative reconstruction of images from compressively sensed measurements</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date></element-citation></ref><ref id="B23-sensors-20-00206"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mousavi</surname><given-names>A.</given-names></name><name><surname>Dasarathy</surname><given-names>G.</given-names></name><name><surname>Baraniuk</surname><given-names>R.G.</given-names></name></person-group><article-title>Deepcodec: Adaptive sensing and recovery via deep convolutional neural networks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1707.03386</pub-id></element-citation></ref><ref id="B24-sensors-20-00206"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>K.</given-names></name><name><surname>Ren</surname><given-names>F.</given-names></name></person-group><article-title>Csvideonet: A real-time end-to-end learning framework for high-frame-rate video compressive sensing</article-title><source>Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision</source><conf-loc>Lake Tahoe, NV, USA</conf-loc><conf-date>12&#x02013;15 March 2018</conf-date></element-citation></ref><ref id="B25-sensors-20-00206"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Du</surname><given-names>J.</given-names></name><name><surname>Shi</surname><given-names>G.</given-names></name></person-group><article-title>Full image recover for block-based compressive sensing</article-title><source>Proceedings of the 2018 IEEE International Conference on Multimedia and Expo</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>23&#x02013;27 July 2018</conf-date></element-citation></ref><ref id="B26-sensors-20-00206"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guan</surname><given-names>Z.</given-names></name><name><surname>Xing</surname><given-names>Q.</given-names></name><name><surname>Xu</surname><given-names>M.</given-names></name><name><surname>Yang</surname><given-names>R.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1902.09707</pub-id><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2944806</pub-id></element-citation></ref><ref id="B27-sensors-20-00206"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>A.</given-names></name><name><surname>Teboulle</surname><given-names>M.</given-names></name></person-group><article-title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems</article-title><source>SIAM J. Imaging Sci.</source><year>2009</year><volume>2</volume><fpage>183</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1137/080716542</pub-id></element-citation></ref><ref id="B28-sensors-20-00206"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Dai</surname><given-names>W.</given-names></name><name><surname>Zou</surname><given-names>J.</given-names></name><name><surname>Xiong</surname><given-names>H.</given-names></name><name><surname>Zheng</surname><given-names>Y.F.</given-names></name></person-group><article-title>Structured sparse representation with union of data-driven linear and multilinear subspaces model for compressive video sampling</article-title><source>IEEE Trans. Signal Process.</source><year>2017</year><volume>65</volume><fpage>5062</fpage><lpage>5077</lpage><pub-id pub-id-type="doi">10.1109/TSP.2017.2721905</pub-id></element-citation></ref><ref id="B29-sensors-20-00206"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Chien</surname><given-names>T.</given-names></name><name><surname>Dinh</surname><given-names>K.Q.</given-names></name><name><surname>Jeon</surname><given-names>B.</given-names></name><name><surname>Burger</surname><given-names>M.</given-names></name></person-group><article-title>Block compressive sensing of image and video with nonlocal Lagrangian multiplier and patch-based sparse representation</article-title><source>Signal Process. Image Commun.</source><year>2017</year><volume>54</volume><fpage>93</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.image.2017.02.012</pub-id></element-citation></ref><ref id="B30-sensors-20-00206"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>K.</given-names></name><name><surname>Ding</surname><given-names>P.L.K.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>Compressive sensing reconstruction of correlated images using joint regularization</article-title><source>IEEE Signal Process. Lett.</source><year>2016</year><volume>23</volume><fpage>449</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1109/LSP.2016.2527680</pub-id></element-citation></ref><ref id="B31-sensors-20-00206"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>Z.</given-names></name><name><surname>Hou</surname><given-names>B.</given-names></name><name><surname>Jiao</surname><given-names>L.</given-names></name></person-group><article-title>Joint sparse recovery with semisupervised MUSIC</article-title><source>IEEE Signal Process. Lett.</source><year>2017</year><volume>24</volume><fpage>629</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1109/LSP.2017.2680603</pub-id></element-citation></ref><ref id="B32-sensors-20-00206"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>H.</given-names></name><name><surname>Dai</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Tian</surname><given-names>Q.</given-names></name><name><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>Dr2-net: Deep residual reconstruction network for image compressive sensing</article-title><source>Neurocomputing</source><year>2019</year><pub-id pub-id-type="doi">10.1016/j.neucom.2019.05.006</pub-id></element-citation></ref><ref id="B33-sensors-20-00206"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>J.</given-names></name><name><surname>Xie</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Shi</surname><given-names>G.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Fully convolutional measurement network for compressive sensing image reconstruction</article-title><source>Neurocomputing</source><year>2019</year><volume>328</volume><fpage>105</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2018.04.084</pub-id></element-citation></ref><ref id="B34-sensors-20-00206"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>V.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>Rectified linear units improve restricted boltzmann machines</article-title><source>Proceedings of the 2010 27th International Conference on Machine Learning</source><conf-loc>Haifa, Israel</conf-loc><conf-date>21&#x02013;24 June 2010</conf-date></element-citation></ref><ref id="B35-sensors-20-00206"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>Y.</given-names></name><name><surname>Sermanet</surname><given-names>P.</given-names></name><name><surname>Reed</surname><given-names>S.</given-names></name><name><surname>Anguelov</surname><given-names>D.</given-names></name><name><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><article-title>Going deeper with convolutions</article-title><source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date></element-citation></ref><ref id="B36-sensors-20-00206"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the 2016 IEEE conference on computer vision and pattern recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date></element-citation></ref><ref id="B37-sensors-20-00206"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soomro</surname><given-names>K.</given-names></name><name><surname>Zamir</surname><given-names>A.R.</given-names></name><name><surname>Shah</surname><given-names>M.</given-names></name></person-group><article-title>UCF101: A dataset of 101 human actions classes from videos in the wild</article-title><source>arXiv</source><year>2012</year><pub-id pub-id-type="arxiv">1212.0402</pub-id></element-citation></ref><ref id="B38-sensors-20-00206"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D.P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref><ref id="B39-sensors-20-00206"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>M.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name><name><surname>Williams</surname><given-names>C.K.</given-names></name><name><surname>Winn</surname><given-names>J.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>The pascal visual object classes (voc) challenge</article-title><source>Int. J. Comput. Vis.</source><year>2010</year><volume>88</volume><fpage>303</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-20-00206-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Overview architecture of JsrNet.</p></caption><graphic xlink:href="sensors-20-00206-g001"/></fig><fig id="sensors-20-00206-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>CNN for joint sampling. In a <italic>T</italic>-length group of pictures, the key frame <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the remaining non-key frames <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> go through specific convolution layers to generate corresponding measurements <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-20-00206-g002"/></fig><fig id="sensors-20-00206-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Spatial DCNN for initial recovery. Each intermediate reconstruction <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is recovered from corresponding measurements <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> through its corresponding spatial DCNN.</p></caption><graphic xlink:href="sensors-20-00206-g003"/></fig><fig id="sensors-20-00206-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Temporal DCNN for joint reconstruction. Intermediate reconstructions <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> go through this temporal DCNN together to generate the final outputs.</p></caption><graphic xlink:href="sensors-20-00206-g004"/></fig><fig id="sensors-20-00206-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>BU. The inputs are the intermediate reconstruction of key frame <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the outputs of the previous layer.</p></caption><graphic xlink:href="sensors-20-00206-g005"/></fig><fig id="sensors-20-00206-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Visual comparisons of <italic>WallPushups_g20</italic>. (<bold>a</bold>) JsrNet, (<bold>b</bold>) Reconnet, (<bold>c</bold>) MH-BCS-SPL, (<bold>d</bold>) FIR, (<bold>e</bold>) D-AMP.</p></caption><graphic xlink:href="sensors-20-00206-g006a"/><graphic xlink:href="sensors-20-00206-g006b"/></fig><fig id="sensors-20-00206-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Visual comparisons of <italic>WallPushups_g10</italic>. (<bold>a</bold>) JsrNet, (<bold>b</bold>) Reconnet, (<bold>c</bold>) MH-BCS-SPL, (<bold>d</bold>) FIR, (<bold>e</bold>) D-AMP.</p></caption><graphic xlink:href="sensors-20-00206-g007a"/><graphic xlink:href="sensors-20-00206-g007b"/></fig><table-wrap id="sensors-20-00206-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-00206-t001_Table 1</object-id><label>Table 1</label><caption><p>Reconstruction performance comparisons (PSNR/SSIM).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm33"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold">N</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">JsrNet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reconnet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MH-BCS-SPL</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FIR</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">D-AMP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">29.81 dB/0.8604</td><td align="center" valign="middle" rowspan="1" colspan="1">21.44 dB/0.5766</td><td align="center" valign="middle" rowspan="1" colspan="1">26.90 dB/0.7837</td><td align="center" valign="middle" rowspan="1" colspan="1">25.78 dB/0.7419</td><td align="center" valign="middle" rowspan="1" colspan="1">13.12 dB/0.2283</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">31.99 dB/0.9018</td><td align="center" valign="middle" rowspan="1" colspan="1">23.58 dB/0.6554</td><td align="center" valign="middle" rowspan="1" colspan="1">29.02 dB/0.8372</td><td align="center" valign="middle" rowspan="1" colspan="1">29.27 dB/0.8499</td><td align="center" valign="middle" rowspan="1" colspan="1">20.36 dB/0.6284</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.15 dB/0.9390</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.44 dB/0.7371</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.21 dB/0.8604</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.71 dB/0.9107</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.56 dB/0.7625</td></tr></tbody></table></table-wrap><table-wrap id="sensors-20-00206-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-00206-t002_Table 2</object-id><label>Table 2</label><caption><p>Reconstruction speed comparisons (s).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm34"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold">N</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">JsrNet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reconnet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MH-BCS-SPL</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FIR</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">D-AMP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.003</td><td align="center" valign="middle" rowspan="1" colspan="1">0.008</td><td align="center" valign="middle" rowspan="1" colspan="1">4.631</td><td align="center" valign="middle" rowspan="1" colspan="1">0.034</td><td align="center" valign="middle" rowspan="1" colspan="1">14.935</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.003</td><td align="center" valign="middle" rowspan="1" colspan="1">0.008</td><td align="center" valign="middle" rowspan="1" colspan="1">3.805</td><td align="center" valign="middle" rowspan="1" colspan="1">0.033</td><td align="center" valign="middle" rowspan="1" colspan="1">14.822</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.003</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.008</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.932</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.034</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.097</td></tr></tbody></table></table-wrap></floats-group></article>