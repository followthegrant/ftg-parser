<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1367-4811</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">32049335</article-id><article-id pub-id-type="pmc">7214041</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btaa088</article-id><article-id pub-id-type="publisher-id">btaa088</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Papers</subject><subj-group subj-group-type="category-toc-heading"><subject>Data and Text Mining</subject></subj-group></subj-group></article-categories><title-group><article-title>Polar labeling: silver standard algorithm for training disease classifiers</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6219-861X</contrib-id><name><surname>Wagholikar</surname><given-names>Kavishwar B</given-names></name><xref ref-type="corresp" rid="btaa088-cor1"/><xref ref-type="aff" rid="btaa088-aff1">b1</xref><!--<email>kwagholikar@mgh.harvard.edu</email>--></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0204-8978</contrib-id><name><surname>Estiri</surname><given-names>Hossein</given-names></name><xref ref-type="aff" rid="btaa088-aff1">b1</xref></contrib><contrib contrib-type="author"><name><surname>Murphy</surname><given-names>Marykate</given-names></name><xref ref-type="aff" rid="btaa088-aff2">b2</xref></contrib><contrib contrib-type="author"><name><surname>Murphy</surname><given-names>Shawn N</given-names></name><xref ref-type="aff" rid="btaa088-aff1">b1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Wren</surname><given-names>Jonathan</given-names></name><role>Associate Editor</role></contrib></contrib-group><aff id="btaa088-aff1"><label>b1</label>
<institution>Laboratory of Computer Science</institution>, Massachusetts General Hospital, Boston, MA 02114, USA</aff><aff id="btaa088-aff2"><label>b2</label>
<institution>Partners Healthcare</institution>, Somerville, MA 02145, USA</aff><author-notes><corresp id="btaa088-cor1">To whom correspondence should be addressed. <email>kwagholikar@mgh.harvard.edu</email></corresp></author-notes><pub-date pub-type="ppub"><day>15</day><month>5</month><year>2020</year></pub-date><pub-date pub-type="epub" iso-8601-date="2020-02-12"><day>12</day><month>2</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>12</day><month>2</month><year>2020</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>36</volume><issue>10</issue><fpage>3200</fpage><lpage>3206</lpage><history><date date-type="received"><day>14</day><month>7</month><year>2019</year></date><date date-type="rev-recd"><day>30</day><month>1</month><year>2020</year></date><date date-type="accepted"><day>04</day><month>2</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020. Published by Oxford University Press.</copyright-statement><copyright-year>2020</copyright-year><license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="btaa088.pdf"/><abstract><title>Abstract</title><sec id="s1"><title>Motivation</title><p>Expert-labeled data are essential to train phenotyping algorithms for cohort identification. However expert labeling is time and labor intensive, and the costs remain prohibitive for scaling phenotyping to wider use-cases.</p></sec><sec id="s2"><title>Results</title><p>We present an approach referred to as polar labeling (PL), to create silver standard for training machine learning (ML) for disease classification. We test the hypothesis that ML models trained on the silver standard created by applying PL on unlabeled patient records, are comparable in performance to the ML models trained on gold standard, created by clinical experts through manual review of patient records. We perform experimental validation using health records of 38&#x000a0;023 patients spanning six diseases. Our results demonstrate the superior performance of the proposed approach.</p></sec><sec id="s3"><title>Availability and implementation</title><p>We provide a Python implementation of the algorithm and the Python code developed for this study on Github.</p></sec><sec id="s5"><title>Supplementary information</title><p>
<xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p></sec></abstract><funding-group><award-group award-type="grant"><funding-source><institution-wrap><institution>National Institute of Health</institution></institution-wrap></funding-source></award-group><award-group award-type="grant"><funding-source><institution-wrap><institution>NIH</institution><institution-id institution-id-type="DOI">10.13039/100000002</institution-id></institution-wrap></funding-source><award-id>R01-HG009174</award-id><award-id>R00-LM011575</award-id></award-group></funding-group><counts><page-count count="7"/></counts></article-meta></front><body><sec><title>1 Introduction</title><p>Phenotyping forms the basis of utilizing statistical or machine learning (ML) for research and clinical workflows (<xref rid="btaa088-B14" ref-type="bibr">Shivade <italic>et al.</italic>, 2014</xref>; <xref rid="btaa088-B18" ref-type="bibr">Xu <italic>et al.</italic>, 2015</xref>). However, data in electronic health records (EHR) are considered noisy for training ML algorithms (<xref rid="btaa088-B7" ref-type="bibr">Hripcsak and Albers, 2013</xref>). To address this gap, expert labeling of the data is widely used to train phenotyping algorithms (see <xref ref-type="fig" rid="btaa088-F1">Fig.&#x000a0;1</xref>; <xref rid="btaa088-B4" ref-type="bibr">Geraci <italic>et al.</italic>, 2017</xref>; <xref rid="btaa088-B10" ref-type="bibr">Kagawa <italic>et al.</italic>, 2017</xref>; <xref rid="btaa088-B17" ref-type="bibr">Wood <italic>et al.</italic>, 2015</xref>). However expert labeling is time and labor intensive, and the costs are prohibitive for scaling phenotyping to wider use-cases (<xref rid="btaa088-B13" ref-type="bibr">Richesson <italic>et al.</italic>, 2013</xref>, <xref rid="btaa088-B12" ref-type="bibr">2017</xref>) The lack of data quality is considered as the primary obstacle for utilization of AI in healthcare (<xref rid="btaa088-B7" ref-type="bibr">Hripcsak and Albers, 2013</xref>; <xref rid="btaa088-B9" ref-type="bibr">Jason, The Mitre Corporation, 2017</xref>).</p><fig id="btaa088-F1" orientation="portrait" position="float"><label>Fig. 1.</label><caption><p>Experimental design</p></caption><graphic xlink:href="btaa088f1"/></fig><p>Several approaches have been investigated to avert the high costs of expert labeling. Methods to discover optimal inference algorithms, as well as feature selection and feature engineering techniques to combine different data modalities have been shown to improve performance of supervised ML (<xref rid="btaa088-B2" ref-type="bibr">Carroll <italic>et al.</italic>, 2011</xref>; <xref rid="btaa088-B8" ref-type="bibr">Huang <italic>et al.</italic>, 2004</xref>; <xref rid="btaa088-B15" ref-type="bibr">Teixeira <italic>et al.</italic>, 2017</xref>). These approaches serve to minimize the size of required expert labels. Active learning has been observed to be similarly useful in reducing the labeling effort (<xref rid="btaa088-B3" ref-type="bibr">Chen <italic>et al.</italic>, 2013</xref>). Unsupervised learning-based tensor factorization techniques called Limestone and Marble are able to generate phenotype clusters with no predefined phenotype definitions but require manual validation by experts (<xref rid="btaa088-B5" ref-type="bibr">Ho <italic>et al.</italic>, 2014a</xref>, <xref rid="btaa088-B6" ref-type="bibr">b</xref>).</p><p>On the other hand, several weakly supervised learning approaches have been investigated. <xref rid="btaa088-B20" ref-type="bibr">Yu <italic>et al.</italic> (2015</xref>) described an automated feature-selection algorithm (AFEP) that uses medical knowledge sources. For rheumatoid arthritis and coronary artery disease cohorts, models trained using the features selected by AFEP achieved comparable or slightly higher accuracy than those trained with exper-curated features. Yu <italic>et al.</italic> improved upon AFEP to develop Surrogate-assisted feature extraction (SAFE). SAFE refined the feature selection using high coefficients of variables generated in ML models trained on a silver standard that was derived from frequency distribution of diagnostic codes (<xref rid="btaa088-B19" ref-type="bibr">Yu <italic>et al.</italic>, 2017</xref>). The performance of algorithms trained using features identified by SAFE was significantly higher than that of those trained on expert curated features for four disease cohorts. However, this approach is complex to implement in contrast to the XPRESS framework developed by Aggarwal <italic>et al.</italic> (2016), in which the silver standard labels are generated by searching disease-related anchor terms in clinical notes. Models trained on this silver standard were reported to be comparable to rule-based models for heart attack and type 2 diabetes mellitus. Furthermore, the XPRESS framework has been implemented as a R software (R Foundation for Statistical Computing, Vienna, Austria) package by Banda and is widely used in the Observational Medical Outcomes Partnership (OMOP) consortium. Deriving from XPRESS, <xref rid="btaa088-B21" ref-type="bibr">Yu <italic>et al.</italic> (2017)</xref> developed Phenorm, which includes SAFE for feature selection but trains ML directly on silver standard labels from frequency distribution of diagnoses, and applies self-regression to incorporate related features for refining the silver standard. However, a downside of this approach is that it involves multiple steps and parameters, and is not available as an opensource implementation. Consequently Phenorm is difficult to implement or replicate by other researchers.</p><p>In this paper, we detail our approach for creating a silver standard to bypass the effort for creating human annotation for training ML models. Ours is a weakly supervised learning approach, (<xref rid="btaa088-B22" ref-type="bibr">Zhou, 2018</xref>) that leverages the intuitive concept that even though health data are noisy, patients who are repeatedly ascribed to particular categories are more likely to have true membership of that category. For example, patients with a large number of visits with documentation of codes for asthma are more likely to truly have asthma than those patients who have a lesser number of such codes in comparison. In other words, patients near the high pole of asthma distribution counts will tend to truly suffer from asthma. Our main contribution is that we describe an algorithm implementing this notion, which we refer to as polar labeling (PL) algorithm. Furthermore, we perform experimental validation using expert annotations on patient records for six diseases. Specifically, we test the hypothesis that ML models trained on silver standard (created by applying PL on unlabeled patient records), are comparable in performance to the ML models trained on gold standard, created by clinical experts through manual review of patient records.</p><p>We provide a Python implementation of the algorithm and the Python code developed for this study on Github. We hope that our results and source code will enable the research community to further study and use the proposed approach.</p></sec><sec><title>2 Materials and methods</title><p>The experiment described in this paper was carried out on health records of 38 023 patients at Partners Healthcare in Boston who had consented for inclusion of their health records into the institutional biobank. The study was approved by the institutional review board. The patient records were obtained by querying the institutional research patient data repository (<xref rid="btaa088-B11" ref-type="bibr">Nalichowski <italic>et al.</italic>, 2006</xref>; <xref rid="btaa088-B16" ref-type="bibr">Wattanasin <italic>et al.</italic>, 2008</xref>). For conducting this study, we selected six diseases of varying prevalence including asthma, breast cancer, chronic obstructive pulmonary disease (COPD), depression, epilepsy and hypertension. See <xref ref-type="supplementary-material" rid="sup1">Supplementary Appendix</xref> B for details on disease prevalence.</p><sec><title>2.1 Experimental design</title><p>
<xref ref-type="fig" rid="btaa088-F1">Figure&#x000a0;1</xref> outlines the experimental design. For each of the chosen diseases, we selected patients from the study dataset, to create a gold-standard set labeled by a clinical expert. The unselected patients formed the unlabeled set (Step 1 in <xref ref-type="fig" rid="btaa088-F1">Fig.&#x000a0;1</xref>). The gold-standard set was further split randomly and equally into a development set and a validation set (Step 2). A silver-standard set was created by applying the PL algorithm to the unlabeled set.</p><p>Development set (Step 3), and the silver-standard set (Step 6) were used to train ML models. To facilitate readability, we henceforth refer to the models as gold-ML and silver-ML, respectively. The validation set was used to measure the performance of both of these models (Steps 4, 7 and 8).</p><p>Creation of the validation set was repeated 15 times. In each of the runs, new gold-ML and silver-ML models were trained on the development set and silver-standard set respectively, and then scored on the validation set. These multiple runs were essential to average the performance scores.</p><p>What follows is the detailed methodology for (i) creation of gold standard, (ii) creation of silver standard, (iii) PL algorithm, (iv) feature extraction, (v) training of ML models on the development-set and silver standards and (vi) comparison of the performance of the models on the validation set.</p></sec><sec><title>2.2 Gold standard creation</title><p>Samples of 540 patients were drawn from the study dataset for each of the six diseases. Each of the sample consisted of 540 randomly selected patients, such that 100 of these patients had an International Classification of Diseases, ninth revision (ICD-9) code for the disease in question and 365 had at least one other disease. The remaining 75 patients were selected completely at random. For each of these samples, a single clinical expert (nurse practitioner) performed a chart review to annotate the presence of the disease. This expert labeled the patients with one of four classes: Y: present, N: Absent, P: possible, U: unknown/can&#x02019;t say. The criteria for the disease annotations are provided in <xref ref-type="supplementary-material" rid="sup1">Supplementary Appendix</xref> A. For our experiments we included only the Y and N labels, which provided more certainty (<xref rid="btaa088-T1" ref-type="table">Table&#x000a0;1</xref>).</p><table-wrap id="btaa088-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Distribution of expert annotations for the six disease cohorts</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="left" span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1"/><th colspan="4" rowspan="1">Human/expert annotations<hr/></th><th rowspan="1" colspan="1"/><th rowspan="2" colspan="1">Estimated prevalence in USA (%)</th></tr><tr><th rowspan="1" colspan="1">Cohort</th><th rowspan="1" colspan="1">N</th><th rowspan="1" colspan="1">P</th><th rowspan="1" colspan="1">U</th><th rowspan="1" colspan="1">Y</th><th rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Asthma</td><td rowspan="1" colspan="1">446</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">81</td><td rowspan="1" colspan="1">540</td><td rowspan="1" colspan="1">7.9</td></tr><tr><td rowspan="1" colspan="1">Breast cancer</td><td rowspan="1" colspan="1">463</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">64</td><td rowspan="1" colspan="1">540</td><td rowspan="1" colspan="1">0.5</td></tr><tr><td rowspan="1" colspan="1">COPD</td><td rowspan="1" colspan="1">468</td><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">39</td><td rowspan="1" colspan="1">540</td><td rowspan="1" colspan="1">4 &#x02013; 9</td></tr><tr><td rowspan="1" colspan="1">Depression</td><td rowspan="1" colspan="1">383</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">143</td><td rowspan="1" colspan="1">540</td><td rowspan="1" colspan="1">8.1</td></tr><tr><td rowspan="1" colspan="1">Epilepsy</td><td rowspan="1" colspan="1">479</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">52</td><td rowspan="1" colspan="1">540</td><td rowspan="1" colspan="1">1.2</td></tr><tr><td rowspan="1" colspan="1">Hypertension</td><td rowspan="1" colspan="1">222</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">307</td><td rowspan="1" colspan="1">540</td><td rowspan="1" colspan="1">29</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p>Y, present; N, absent; P, possible; U, unknown/can&#x02019;t say.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>2.3 Silver-standard creation</title><p>The silver-standard is generated from the unlabeled set (Step 5) by applying the PL algorithm.</p></sec><sec><title>2.4 PL algorithm</title><p>The steps are elaborated as follows (see <xref ref-type="fig" rid="btaa088-F2">Fig.&#x000a0;2</xref>):</p><fig id="btaa088-F2" orientation="portrait" position="float"><label>Fig. 2.</label><caption><p>(<bold>a</bold>) Counts of ICD-9 codes follow a log normal distribution. (<bold>b</bold>) Patients to left-side of low-pole are silver negatives and right-side of high pole are silver positives</p></caption><graphic xlink:href="btaa088f2"/></fig><list list-type="order"><list-item><p>Obtain diagnostic billing counts: The total number of ICD-9 counts that each patient has forms the input for the algorithm.</p></list-item><list-item><p>Distribution of log normalized counts: As the distribution shows logarithmic distribution the counts are normalized to log normal form log (<italic>x</italic> + 1). Note that the addition of 1 to the count obviates computation of log for zero counts.</p></list-item><list-item><p>Identify &#x02018;potential silver positives&#x02019;: Patients with log(count+1) &#x0003e; 0 i.e. count &#x0003e; =1 are potential silver positives.</p></list-item><list-item><p>Cut-off (alpha) for delineating high pole: The mean and standard deviation for the log normal distribution for &#x02018;potential silver positives&#x02019; is computed, and for an arbitrary factor alpha, the &#x02018;high pole cut-off&#x02019; is computed as mean + (cut-off parameter <italic>x</italic> standard deviation). The lower pole cut-off is always at 1.0.</p></list-item><list-item><p>Silver set: Patients on and to the right of the high pole cut-off are labeled as &#x02018;silver positive&#x02019;, and patients on and to left of &#x02018;low pole cut-off&#x02019; are labeled as &#x02018;silver negative&#x02019;. Note that all patient with log(count + 1) = 0 i.e. count = 0 are silver negatives.</p></list-item><list-item><p>Balance negative silver set: The &#x02018;silver negative&#x02019; set is reduced to a smaller subset of randomly selected y patients, where <italic>y</italic> is the size of silver set <italic>x</italic> (size of silver positive/size of potential silver positive). This step ensures that relative distribution of positive and negative cases in the &#x02018;balanced silver set&#x02019; is close to that in the entire dataset.</p></list-item><list-item><p>Create &#x02018;balanced silver&#x02019; set as a union of the &#x02018;positive silver&#x02019; set and the &#x02018;balanced negative silver&#x02019; set.</p></list-item><list-item><p>Output a random sample of the balanced silver set, by creating a random subset of a specified &#x02018;train size&#x02019; parameter.</p></list-item></list><p>Hence, there are two parameters for the PL algorithm: cut-off parameter (Step 4) and train size parameter (Step 8).</p></sec><sec><title>2.5 Feature extraction</title><p>For applying ML to the dataset, we transformed the health records for each patient to a feature vector, as follows. Clinical experts hand-picked concepts that are relevant for the six diseases, including diagnosis, medications, laboratory results and procedures (<xref ref-type="supplementary-material" rid="sup1">Supplementary Appendix</xref> C). We computed the total number of times each of these concepts occurs in the health record of a patient, to construct a count vector of the features (<italic>n</italic>&#x02009;=&#x02009;175) for the patient. For example, PRC_ERVisit and DX_COPD indicate the number of emergency room visits and number of times the diagnosis of COPD was made in the patients record, respectively. The feature vectors were then used for training and scoring the ML algorithm.</p></sec><sec><title>2.6 ML model training</title><p>The gold and silver ML models were trained on the development set (part of gold standard) and silver-standard, respectively. We trained two types of ML models&#x02014;random forest (RF) and logistic regression (LR). The RF classifiers were trained using 1000 estimators, using the Gini score to split the trees. The LR model was fit with L2 regularization, with maximum threshold of 10 000 iterations for optimization. The coefficient of regularization was selected by performing 5-fold cross-validation using the area under the receiver operating characteristics (AUROC) curve for scoring. The developed models were output in Steps 3 and 6 in <xref ref-type="fig" rid="btaa088-F1">Figure&#x000a0;1</xref>.</p></sec><sec><title>2.7 Performance comparison</title><p>We compared the AUROC of the gold and silver ML models (<xref ref-type="fig" rid="btaa088-F4">Fig.&#x000a0;4</xref> and <xref rid="btaa088-T2" ref-type="table">Table 2</xref>). T-test was used to determine the significance of the difference in <italic>P</italic>-values, with a <italic>P</italic>-value of 0.05 acting as a cut-off for statistical significance. For baseline, we measured the AUROC for the simple rule of predicting a disease for a patient if the diagnosis code for the disease is present in the patient&#x02019;s health record.</p></sec><sec><title>2.8 Opensource code</title><p>We have included a Github link to our Python implementation of the PL algorithm, along with the code for carrying out the experiment described in this paper, which includes a dummy dataset. In addition, a live Jupyter-python notebook for conducting the experiment is available as mybinder link.</p></sec></sec><sec><title>3 Results</title><p>The PL algorithm for the creation of silver standard leverages the intuitive concept that even though medical data are noisy, patients that are repeatedly ascribed to particular categories are more likely to hold true membership of that category. <xref ref-type="fig" rid="btaa088-F3">Figure&#x000a0;3</xref> demonstrates this concept on the labeled dataset.</p><fig id="btaa088-F3" orientation="portrait" position="float"><label>Fig. 3.</label><caption><p>Log-normal distribution of counts of billing codes, using the labeled dataset, shows distinct but overlapping log-normal distribution for both true-positive and true-negative patients. The poles of the distributions show areas of enriched true-positive (blue colored bars)/true-negative (red colored bars) patients. The <italic>y</italic>-axis is the count of patients and the <italic>x</italic>-axis is the log of diagnostic ICD-9 count. (Color version of this figure is available at <italic>Bioinformatics</italic> online.)</p></caption><graphic xlink:href="btaa088f3"/></fig><p>
<xref ref-type="fig" rid="btaa088-F4">Figure&#x000a0;4</xref> and <xref rid="btaa088-T2" ref-type="table">Table&#x000a0;2</xref> show that RF model trained on silver standard is superior or equivalent in performance to the RF model trained on the gold standard for five of the six diseases (i.e. not depression). Similarly, the LR model trained on the silver standard is superior or equivalent in performance to the LR model trained on gold standard for five of the six diseases (i.e. not breast cancer).</p><fig id="btaa088-F4" orientation="portrait" position="float"><label>Fig. 4.</label><caption><p>AUROC for the phenotyping models on the validation sets. The suffix of gold-ML refers to the ML models trained on the &#x02018;gold-standard development set&#x02019;, and silver-ML refers to the ML models trained on the silver standard set. The Dx-code-count refers to the rule of predicting the disease if the diagnosis code for the disease is present in the health record</p></caption><graphic xlink:href="btaa088f4"/></fig><table-wrap id="btaa088-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Table of the AUROC algorithm</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><tbody><tr><td rowspan="1" colspan="1">
<inline-graphic xlink:href="btaa088ilf1.jpg" mimetype="image"/>
</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><label>a</label><p>Indicates that the value is significantly different from that in the column to the immediate left. Background green shading indicates the algorithm with the highest AUROC for the cohort.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>4 Discussion</title><p>Our results show that the performance of ML models trained on silver standard created using PL are superior or equivalent to those trained on gold standard created by clinical experts in five of the six cohorts, for both RF and LR classifiers. The reason for the superior performance is likely due to the large size of training sets (<italic>n</italic>&#x02009;=&#x02009;8000) yielded by PL, as compared to the training set of size approximately 263 from human labeling.</p><p>The latter size of 263 is because the size of the gold standard for each cohort has average size 525 (range 507&#x02013;536), after excluding the uncertain categories (<xref rid="btaa088-T1" ref-type="table">Table&#x000a0;1</xref>). As 50% of this set was used for generating the test set, the development set was restricted to (50% of 525=) 263 patients. In contrast, a total of 37 763 patients (38 026 total &#x02212; 263 human labeled) were available for PL, from which training set of size 8000 could be consistently constructed for all cohorts using a polar cut-off parameter of 0.5.</p><p>The advantage of PL is that it is an intuitive approach that is easy to implement. It can be easily applied to coded data to generate silver standard. Moreover, the use of billing codes is pervasive throughout the healthcare systems across the world. However, a potential downside to PL is that the developed models maybe insensitive to newly diagnosed patients, as the polar set is inherently biased towards patients that have large number of repeated encounters with the same diagnosis. In our dataset, the count of events per patient averaged at 1330 with standard deviation of 1902. Further research is necessary to investigate the effect of data density on performance of PL. Our implementation code in the Github repository will facilitate the application of the algorithm on other datasets by the research community. To ensure reproducibility of the study, we applied PL to identify stroke patients as described in Supplementary Appendix D.</p><p>PL may be applied in situations where data points have been labeled multiple times in a noisy/biased manner. For example, in the case of healthcare data, one or more care providers evaluate a patient multiple times for presence of a disease, with each instance recorded indirectly in form of the billing codes. Hence, crowd sourcing experiments that have multiple but biased or noisy reviewers for each data point may be able to utilize PL.</p><p>The PL approach requires two parameters: the size of training set and the cut-off factor. Higher cut-off factor restricts the size of possible training set. This is because the higher cut-off pole is shifted more to the right with higher cut-off values, so there are lesser samples left available to choose from as the right-side tail diminishes.</p><p>We carried out the following experiment to examine the sensitivity of the algorithm to different parameter values. We varied the training size from 300 to 8000, and the cut-off factor from &#x02212;3.0 to +3.0 in 0.5 increments to generate silver standard using the unlabeled data. LR models were trained on the silver standard. The performance of the models was measured by computing AUC_ROC for predictions made on the labeled data. <xref ref-type="fig" rid="btaa088-F5">Figure&#x000a0;5</xref> shows the variation in AUROC for the different combinations of the PL parameters (averaged over five runs). Generally higher training size improved performance of resulting ML models. The optimal cut-off parameter lies between &#x02212;0.5 and 0.5, for most of the disease cohorts.</p><fig id="btaa088-F5" orientation="portrait" position="float"><label>Fig. 5.</label><caption><p>Sensitivity analysis was performed by varying &#x02018;train size&#x02019; and &#x02018;cut-off/alpha&#x02019; parameters for PL to create silver standard. LR models were trained on the silver standard and the performance of the models was measured by computing AUC_ROC on labeled data. The plots show that performance of ML models trained on the silver standard generally increases with higher train size</p></caption><graphic xlink:href="btaa088f5"/></fig><sec><title>4.1 Limitations</title><p>A major limitation of our study is that the features selected for our datasets were based on expert judgment, which limits the generalization of our results. For high-throughput phenotyping, it is desirable to have complete automation for phenotyping and the need to engage with clinical experts for feature selection is a limitation.</p><p>In future work, we plan to expand our study dataset to include all the EHR data and to leverage automated methods for feature selection.</p><p>A second limitation of our study is that only a single clinical expert performed the annotations. In the absence of additional annotators, the reliability of the clinical expert could not be assessed.</p><p>The third limitation of the study is that we only experimented with LR and RF models and did not utilize other ML algorithms like na&#x000ef;ve Bayes, support vector machines, neural networks, etc. Such a comparison is necessary to establish the use of PL to augmenting other ML approaches. One challenge in using the other ML algorithms is their sensitivity to data normalization. As our current PL implementation requires absolute ICD counts, we could not normalize/scale the data to evaluate other ML approaches. In future work, we will modify our implementation to allow the use of normalized datasets.</p></sec></sec><sec><title>5 Conclusion</title><p>We have described a PL approach to create silver standard labels for training ML algorithms for disease classification. Our results demonstrate that ML models trained on silver standard created using PL are similar in performance to those trained on expert labeled data. Directions for future work include replication of the results on other cohorts and using a diversity of other ML approaches.</p></sec><sec><title>Author contributions</title><p>S.N.M. and K.B.W. envisioned this study. K.B.W. developed and implemented the algorithm and carried out the experiments. H.E. provided inputs on the study design. M.K.M. performed chart review to annotate the dataset. All authors made significant contributions for drafting the manuscript and approved the final version.</p></sec><sec><title>Funding</title><p>This study was supported by National Institute of Health (NIH) (R01-HG009174 and R00-LM011575).</p><p>
<italic>Conflict of Interest</italic>: none declared.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1"><label>btaa088_Supplementary_Data</label><media xlink:href="btaa088_supplementary_data.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list id="ref1"><title>References</title><ref id="btaa088-B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Agarwal</surname><given-names>V.</given-names></name></person-group>
<etal>et al</etal> (<year>2016</year>) 
<article-title>Learning statistical models of phenotypes using noisy labeled training data</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>23</volume>, <fpage>1166</fpage>&#x02013;<lpage>1173</lpage>.<pub-id pub-id-type="pmid">27174893</pub-id></mixed-citation></ref><ref id="btaa088-B2"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Carroll</surname><given-names>R.J.</given-names></name></person-group>
<etal>et al</etal> (<year>2011</year>) Na&#x000ef;ve Electronic Health Record phenotype identification for rheumatoid arthritis. In <italic>Annual Symposium Proceedings/AMIA Symposium. AMIA Symposium 2011</italic>, pp. <fpage>189</fpage>&#x02013;<lpage>196</lpage>.</mixed-citation></ref><ref id="btaa088-B3"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Applying active learning to high-throughput phenotyping algorithms for electronic health records data</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>20</volume>, <fpage>e253</fpage>&#x02013;<lpage>e259</lpage>.<pub-id pub-id-type="pmid">23851443</pub-id></mixed-citation></ref><ref id="btaa088-B4"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Geraci</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Applying deep neural networks to unstructured text notes in electronic medical records for phenotyping youth depression</article-title>. <source>Evid. Based Mental Health</source>, <volume>20</volume>, <fpage>83</fpage>&#x02013;<lpage>87</lpage>.</mixed-citation></ref><ref id="btaa088-B5"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>J.C.</given-names></name></person-group>
<etal>et al</etal> (<year>2014a</year>) 
<article-title>Limestone: high-throughput candidate phenotype generation via tensor factorization</article-title>. <source>J. Biomed. Inform</source>., <volume>52</volume>, <fpage>199</fpage>&#x02013;<lpage>211</lpage>.<pub-id pub-id-type="pmid">25038555</pub-id></mixed-citation></ref><ref id="btaa088-B6"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2014b</year>) Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization. In Marble: <italic>H</italic>igh-Throughput Phenotyping from Electronic Health Records via Sparse Nonnegative Tensor Factorization, ACM, pp. <fpage>115</fpage>&#x02013;<lpage>124</lpage>.</mixed-citation></ref><ref id="btaa088-B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hripcsak</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Albers</surname><given-names>D.J.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Next-generation phenotyping of electronic health records</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>20</volume>, <fpage>117</fpage>&#x02013;<lpage>121</lpage>.<pub-id pub-id-type="pmid">22955496</pub-id></mixed-citation></ref><ref id="btaa088-B8"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name></person-group>
<etal>et al</etal> (<year>2004</year>) Feature selection and classification model construction on type 2 diabetic patient&#x02019;s data. In Feature Selection and Classification Model Construction on Type 2 Diabetic Patient&#x02019;s Data, Springer, pp. <fpage>153</fpage>&#x02013;<lpage>162</lpage>.</mixed-citation></ref><ref id="btaa088-B9"><mixed-citation publication-type="other">Jason, The Mitre Corporation (2017) Artificial Intelligence for Health and Health Care.</mixed-citation></ref><ref id="btaa088-B10"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Kagawa</surname><given-names>R.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Development of type 2 diabetes mellitus phenotyping framework using expert knowledge and machine learning approach</article-title>. <source>J. Diabet. Sci. Technol</source>., <volume>11</volume>, <fpage>791</fpage>&#x02013;<lpage>799</lpage>.</mixed-citation></ref><ref id="btaa088-B11"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Nalichowski</surname><given-names>R.</given-names></name></person-group>
<etal>et al</etal> (<year>2006</year>) Calculating the benefits of a research patient data repository. In <italic>AMIA Annu Symp Proc</italic>, pp. <fpage>1044</fpage>.</mixed-citation></ref><ref id="btaa088-B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Richesson</surname><given-names>R.L.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Pragmatic (trial) informatics: a perspective from the NIH Health Care Systems Research Collaboratory</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>24</volume>, <fpage>996</fpage>&#x02013;<lpage>1001</lpage>.<pub-id pub-id-type="pmid">28340241</pub-id></mixed-citation></ref><ref id="btaa088-B13"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Richesson</surname><given-names>R.L.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Electronic health records based phenotyping in next-generation clinical trials: a perspective from the NIH Health Care Systems Collaboratory</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>20</volume>, <fpage>e226</fpage>&#x02013;<lpage>e231</lpage>.<pub-id pub-id-type="pmid">23956018</pub-id></mixed-citation></ref><ref id="btaa088-B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Shivade</surname><given-names>C.</given-names></name></person-group>
<etal>et al</etal> (<year>2014</year>) 
<article-title>A review of approaches to identifying patient phenotype cohorts using electronic health records</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>21</volume>, <fpage>221</fpage>&#x02013;<lpage>230</lpage>.<pub-id pub-id-type="pmid">24201027</pub-id></mixed-citation></ref><ref id="btaa088-B15"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Teixeira</surname><given-names>P.L.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Evaluating electronic health record data sources and algorithmic approaches to identify hypertensive individuals</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>24</volume>, <fpage>162</fpage>&#x02013;<lpage>171</lpage>.<pub-id pub-id-type="pmid">27497800</pub-id></mixed-citation></ref><ref id="btaa088-B16"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Wattanasin</surname><given-names>N.</given-names></name></person-group>
<etal>et al</etal> (<year>2008</year>) E-facts: business process management in clinical data repositories. In AMIA Annu Symp Proc , pp. <fpage>1170</fpage>.</mixed-citation></ref><ref id="btaa088-B17"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wood</surname><given-names>W.A.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Emerging uses of patient generated health data in clinical research</article-title>. <source>Mol. Oncol</source>., <volume>9</volume>, <fpage>1018</fpage>&#x02013;<lpage>1024</lpage>.<pub-id pub-id-type="pmid">25248998</pub-id></mixed-citation></ref><ref id="btaa088-B18"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Review and evaluation of electronic health records-driven phenotype algorithm authoring tools for clinical and translational research</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>22</volume>, <fpage>1251</fpage>&#x02013;<lpage>1260</lpage>.<pub-id pub-id-type="pmid">26224336</pub-id></mixed-citation></ref><ref id="btaa088-B19"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Surrogate-assisted feature extraction for high-throughput phenotyping</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>24</volume>, e143&#x02013;e149.</mixed-citation></ref><ref id="btaa088-B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Toward high-throughput phenotyping: unbiased automated feature extraction and selection from knowledge sources</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>22</volume>, <fpage>993</fpage>&#x02013;<lpage>1000</lpage>.<pub-id pub-id-type="pmid">25929596</pub-id></mixed-citation></ref><ref id="btaa088-B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>Enabling phenotypic big data with PheNorm</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>25</volume>, <fpage>54</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="btaa088-B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Z.-H.</given-names></name></person-group> (<year>2018</year>) 
<article-title>A brief introduction to weakly supervised learning</article-title>. <source>Natl. Sci. Rev</source>., <volume>5</volume>, <fpage>44</fpage>&#x02013;<lpage>53</lpage>.</mixed-citation></ref></ref-list></back></article>