<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31096584</article-id><article-id pub-id-type="pmc">6567322</article-id><article-id pub-id-type="doi">10.3390/s19102242</article-id><article-id pub-id-type="publisher-id">sensors-19-02242</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Colour Constancy for Image of Non-Uniformly Lit Scenes <xref ref-type="author-notes" rid="fn1-sensors-19-02242">&#x02020;</xref></article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1364-9151</contrib-id><name><surname>Hussain</surname><given-names>Md Akmol</given-names></name><xref ref-type="aff" rid="af1-sensors-19-02242">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0677-7083</contrib-id><name><surname>Sheikh-Akbari</surname><given-names>Akbar</given-names></name><xref ref-type="aff" rid="af2-sensors-19-02242">2</xref><xref rid="c1-sensors-19-02242" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Mporas</surname><given-names>Iosif</given-names></name><xref ref-type="aff" rid="af2-sensors-19-02242">2</xref></contrib></contrib-group><aff id="af1-sensors-19-02242"><label>1</label>School of Computing, Creative Technology and Engineering, Leeds Beckett University, Leeds LS1 3HE, UK; <email>m.hussein7766@student.leedsbeckett.ac.uk</email></aff><aff id="af2-sensors-19-02242"><label>2</label>Communications and Intelligent Systems Group, School of Engineering and Computer Science, University of Hertfordshire, Hatfield AL10 9EU, UK; <email>i.mporas@herts.ac.uk</email></aff><author-notes><corresp id="c1-sensors-19-02242"><label>*</label>Correspondence: <email>A.Sheikh-Akbari@leedsbeckett.ac.uk</email></corresp><fn id="fn1-sensors-19-02242"><label>&#x02020;</label><p>This paper is extension version of the conference paper: Hussain, M.A.; Sheikh-Akbari, A. Colour Constancy for Image of Non-Uniformly Lit Scenes. In Proceedings of the 2018 IEEE International Conference on Imaging Systems and Techniques (IST), Krakow, Poland, 16&#x02013;18 October 2018.</p></fn></author-notes><pub-date pub-type="epub"><day>15</day><month>5</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>5</month><year>2019</year></pub-date><volume>19</volume><issue>10</issue><elocation-id>2242</elocation-id><history><date date-type="received"><day>21</day><month>2</month><year>2019</year></date><date date-type="accepted"><day>10</day><month>5</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Digital camera sensors are designed to record all incident light from a captured scene, but they are unable to distinguish between the colour of the light source and the true colour of objects. The resulting captured image exhibits a colour cast toward the colour of light source. This paper presents a colour constancy algorithm for images of scenes lit by non-uniform light sources. The proposed algorithm uses a histogram-based algorithm to determine the number of colour regions. It then applies the K-means<sup>++</sup> algorithm on the input image, dividing the image into its segments. The proposed algorithm computes the Normalized Average Absolute Difference (NAAD) for each segment and uses it as a measure to determine if the segment has sufficient colour variations. The initial colour constancy adjustment factors for each segment with sufficient colour variation is calculated. The Colour Constancy Adjustment Weighting Factors (CCAWF) for each pixel of the image are determined by fusing the CCAWFs of the segments, weighted by their normalized Euclidian distance of the pixel from the center of the segments. Results show that the proposed method outperforms the statistical techniques and its images exhibit significantly higher subjective quality to those of the learning-based methods. In addition, the execution time of the proposed algorithm is comparable to statistical-based techniques and is much lower than those of the state-of-the-art learning-based methods.</p></abstract><kwd-group><kwd>charge-coupled device sensor</kwd><kwd>colour constancy</kwd><kwd>multi-illuminants</kwd><kwd>k-means segmentation</kwd><kwd>fusion</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-19-02242"><title>1. Introduction</title><p>Image sensors built into today&#x02019;s digital cameras mostly use either the Charge Coupled Device (CCD) or Complementary Metal-Oxide Semiconductor (CMOS) technology. Both CCD and CMOS are semiconductor devices that serve as &#x0201c;electronic eyes&#x0201d;. While they both use photodiodes, they differ in terms of manufacturing. A CCD sensor is an array of capacitors, each of which can store its own electrical charge. Groups of capacitors on the CCD form pixels, which are charged using the photoelectric effect. This happens when the capacitor converts an incident photon of light into an electrical charge. When a circuit is connected to the pixels, the value of the charge is then numerically measured and recorded in a computer file. After recording, the image can be displayed. Most CCD sensors use a Bayer filtration pattern for their pixels; each pixel is covered with either a red, blue, or green filter, which only allow light that is that colour to reach the capacitor. In this filtration pattern, there are twice as many green sensors as red or blue, because human eyes are more sensitive to green than to other colours. Since it is known which sensor has which colour filter, the intensity of red, green, and blue light can be determined anywhere on the CCD sensor [<xref rid="B1-sensors-19-02242" ref-type="bibr">1</xref>]. The formed raw image using the red, green and blue signals generates the 16 million colours of the RGB colour space. The same principle is used in some other image sensors when they use a single-layer colour array [<xref rid="B2-sensors-19-02242" ref-type="bibr">2</xref>].</p><p>CMOS sensors are much cheaper to produce than CCD sensors, as they are made of the same semiconductor fabrication lines used in microprocessor and static RAM memory chips. Consequently, they have largely replaced CCD sensors, which rely on more specialised fabrication methods. Each photosensitive pixel within the CMOS sensor is comprised of a photodiode and three transistors. One transistor is responsible for activating and resetting the pixel, the second amplifies and converts the stored charge within the photodiode to voltage and the third performs selection and multiplexing. The signals within each pixel are multiplexed by row and column to multiple on chip analog-to-digital converters, resulting in a high-speed yet low-sensitivity image capture system. Furthermore, CMOS sensors are susceptible to high fixed-pattern noise due to manufacturing flaws in the charge to voltage conversion circuits. As a result of their multiplexing design, CMOS sensors are often paired with electronic rolling shutters, although versions with additional transistors can be used in conjunction with global shutters and simultaneous exposure. The low sensitivity of a CMOS sensor results in a lower power consumption and the ability to handle higher light levels than a CCD sensor, resulting in their use in special high dynamic range cameras [<xref rid="B3-sensors-19-02242" ref-type="bibr">3</xref>]. CCD and CMOS sensors are susceptible to different problems such as CCD sensors are more susceptible to vertical smear from bright light sources, while CMOS sensors are susceptible to skewing, wobbling and partial exposure. Research has shown that CCD and CMOS camera sensors are unable to recognise all colours within the scene [<xref rid="B4-sensors-19-02242" ref-type="bibr">4</xref>].</p><p>Furthermore, digital camera sensors are designed to record all incident light from the scene but they are unable to distinguish between the colour of the light source and the true colour of objects. This results in the captured image exhibiting a colour cast, representing the colour of the light source [<xref rid="B5-sensors-19-02242" ref-type="bibr">5</xref>,<xref rid="B6-sensors-19-02242" ref-type="bibr">6</xref>,<xref rid="B7-sensors-19-02242" ref-type="bibr">7</xref>,<xref rid="B8-sensors-19-02242" ref-type="bibr">8</xref>,<xref rid="B9-sensors-19-02242" ref-type="bibr">9</xref>]. Consequently, the colour constancy problem is underconstrained, and determining the true colour of objects when the scene is illuminated by non-canonical light sources is a challenge, so digital cameras use colour constancy adjustment techniques to estimate the true colour of objects [<xref rid="B10-sensors-19-02242" ref-type="bibr">10</xref>,<xref rid="B11-sensors-19-02242" ref-type="bibr">11</xref>,<xref rid="B12-sensors-19-02242" ref-type="bibr">12</xref>]. The primary aim of all colour constancy adjustment algorithms is to remove the colour cast from digital images caused by the colour of scene illuminants [<xref rid="B13-sensors-19-02242" ref-type="bibr">13</xref>,<xref rid="B14-sensors-19-02242" ref-type="bibr">14</xref>,<xref rid="B15-sensors-19-02242" ref-type="bibr">15</xref>] In this paper, existing colour constancy algorithms are discussed and a method to address colour correction in images containing large uniform colour areas and/or images of scenes illuminated by multiple light sources are presented, which produces significantly higher colour constancy than in the existing state-of-the-art methods.</p><sec><title>Related Work</title><p>Researchers have proposed various colour constancy adjustment methods to address the problem of colour constancy in the presence of both single and multiple light sources in digital images [<xref rid="B16-sensors-19-02242" ref-type="bibr">16</xref>,<xref rid="B17-sensors-19-02242" ref-type="bibr">17</xref>,<xref rid="B18-sensors-19-02242" ref-type="bibr">18</xref>,<xref rid="B19-sensors-19-02242" ref-type="bibr">19</xref>,<xref rid="B20-sensors-19-02242" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-02242" ref-type="bibr">21</xref>,<xref rid="B22-sensors-19-02242" ref-type="bibr">22</xref>,<xref rid="B23-sensors-19-02242" ref-type="bibr">23</xref>,<xref rid="B24-sensors-19-02242" ref-type="bibr">24</xref>,<xref rid="B25-sensors-19-02242" ref-type="bibr">25</xref>,<xref rid="B26-sensors-19-02242" ref-type="bibr">26</xref>,<xref rid="B27-sensors-19-02242" ref-type="bibr">27</xref>,<xref rid="B28-sensors-19-02242" ref-type="bibr">28</xref>,<xref rid="B29-sensors-19-02242" ref-type="bibr">29</xref>,<xref rid="B30-sensors-19-02242" ref-type="bibr">30</xref>,<xref rid="B31-sensors-19-02242" ref-type="bibr">31</xref>,<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>,<xref rid="B33-sensors-19-02242" ref-type="bibr">33</xref>,<xref rid="B34-sensors-19-02242" ref-type="bibr">34</xref>,<xref rid="B35-sensors-19-02242" ref-type="bibr">35</xref>,<xref rid="B36-sensors-19-02242" ref-type="bibr">36</xref>,<xref rid="B37-sensors-19-02242" ref-type="bibr">37</xref>,<xref rid="B38-sensors-19-02242" ref-type="bibr">38</xref>,<xref rid="B39-sensors-19-02242" ref-type="bibr">39</xref>,<xref rid="B40-sensors-19-02242" ref-type="bibr">40</xref>,<xref rid="B41-sensors-19-02242" ref-type="bibr">41</xref>,<xref rid="B42-sensors-19-02242" ref-type="bibr">42</xref>,<xref rid="B43-sensors-19-02242" ref-type="bibr">43</xref>,<xref rid="B44-sensors-19-02242" ref-type="bibr">44</xref>,<xref rid="B45-sensors-19-02242" ref-type="bibr">45</xref>]. The existing colour constancy techniques can be grouped into five categories: statistics-based, gamut-based, physics-based, learning-based and biologically inspired methods. In this section, the key colour constancy adjustment methods are discussed.</p><p>The Grey World [<xref rid="B16-sensors-19-02242" ref-type="bibr">16</xref>], the Max-RGB [<xref rid="B17-sensors-19-02242" ref-type="bibr">17</xref>] and the Shades of Grey [<xref rid="B18-sensors-19-02242" ref-type="bibr">18</xref>] are the main statistical-based colour constancy methods. These techniques are based on some assumptions on the statistics of the image data, such as achromaticity. Van de Weijer et al. [<xref rid="B19-sensors-19-02242" ref-type="bibr">19</xref>] proposed the Grey Edge hypothesis, which assumes that the average edge difference of a scene&#x02019;s image data is achromatic. The authors developed a framework that unified the aforementioned methods, which is shown in Equation (1):<disp-formula id="FD1-sensors-19-02242"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x0222b;</mml:mo></mml:mstyle><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>&#x02202;</mml:mo><mml:mi>n</mml:mi></mml:msup><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msubsup><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>&#x000b7;</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the Forbenius norm, <italic>c</italic> is the image colour component and <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
<italic>n</italic> is the order of the derivative, <italic>p</italic> is the Minkowski-norm, and <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>&#x003c3;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the convolution of the image with a Gaussian filter with scale parameter <inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Equation (1) can be used to represent different statistical colour constancy algorithms by using various values of <italic>p</italic>. When <italic>p</italic> = 1, Equation (1) becomes the Grey World method, which assumes the average value of all colour components within an image are achromatic. If <italic>p</italic> = &#x0221e;, Equation (1) represents the Max-RGB colour constancy algorithm, which assumes that the maximum values of the image colour components are achromatic. By setting <italic>p</italic> to be equal to 6, Equation (1) becomes the Shades of Gray algorithm, which is less data dependent than both the Gray World and White Patch colour correction methods. Equation (1) can also represent higher order colour constancy methods, including Grey Edge-1 and Grey Edge-2 by setting <italic>p =</italic> 1, <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic>n</italic> = 1 or 2, respectively. These two approaches assume that derivatives of the image colour components are achromatic. The Weighted Grey Edge method was proposed by Gisenji et al. in [<xref rid="B20-sensors-19-02242" ref-type="bibr">20</xref>]. This colour correction technique is an extension of the Grey Edge algorithm, which incorporates the general weighting scheme of the Grey Edge method as well as the edge of the shadows within the image to achieve colour correction. A moment-based colour balancing method, which uses several higher order moments of the colour features on a fixed 3 &#x000d7; 3 matrix transformation, was also introduced by Finlayson in [<xref rid="B21-sensors-19-02242" ref-type="bibr">21</xref>].</p><p>Forsyth [<xref rid="B22-sensors-19-02242" ref-type="bibr">22</xref>] introduced the gamut mapping method, which assumes that only a limited number of colours can be observed for any given illuminant in real-world images. Gijsenij et al. [<xref rid="B23-sensors-19-02242" ref-type="bibr">23</xref>] proposed an extended version of gamut mapping by incorporating the differential nature of the image. The authors have shown that the failure of the diagonal model of the gamut mapping framework can be barred by adjusting the diagonal-offset model proposed in [<xref rid="B24-sensors-19-02242" ref-type="bibr">24</xref>].</p><p>Several learning-based colour constancy adjustment methods have recently been reported in the literature. The colour constancy problem as a 2D spatial localization task in a log-chrominance space was formulated by Baron in [<xref rid="B25-sensors-19-02242" ref-type="bibr">25</xref>]. Baron observed that scaling the image colour components induces a translation in the log-chromaticity histogram of the image, allowing a colour constancy adjustment to be performed using learning-based methods such as Convolutional Neural Network (CNN). A light source estimation algorithm, which employs a CNN that contains one convolutional layer, one fully connected layer and three output nodes was proposed by Bianco et al. in [<xref rid="B26-sensors-19-02242" ref-type="bibr">26</xref>]. The proposed CNN-based technique samples several non-overlapping patches from the input image and then applies a histogram-stretching algorithm to neutralize the contrast of the image. It then fuses the patch scores, which are obtained by extracting activation values of the last hidden layer to guess the light source. They reported satisfactory performance of the algorithm on a specific raw image dataset. An extension of this neural network-based colour correction method was introduced by Fourure et al. in [<xref rid="B27-sensors-19-02242" ref-type="bibr">27</xref>], which uses a mix-pooling method to determine the availability of accurate features to be learned to perform colour correction. Another CNN-based method, which determines the number of image scene illuminants using a Kernel Density Estimator (KDE) algorithm, was proposed in [<xref rid="B28-sensors-19-02242" ref-type="bibr">28</xref>]. The proposed technique assigns local estimations to the density peaks to compute supports for local regression and back-project the results to the original image based on a distance transform. Finally, the estimated illuminants are refined by non-linear local aggregation to produce a universal estimate for each scene illuminant. Qian et al. [<xref rid="B29-sensors-19-02242" ref-type="bibr">29</xref>] proposed the recurrent colour constancy network (RCC-Net), which consists of two parallel convolutional long-term temporal memory networks, to process the original frame sequence and simulated spatial sequence to learn compositional representations in space and time. This end-to-end RCC-Net is equipped with a simulated sequence module, which boosts its performance for temporal colour constancy task.</p><p>To perform colour constancy for images of scenes illuminated by multiple light sources, researchers have proposed techniques which estimate the local incident light based on the assumption that the colour of the incident light of a mini region is uniform [<xref rid="B30-sensors-19-02242" ref-type="bibr">30</xref>,<xref rid="B31-sensors-19-02242" ref-type="bibr">31</xref>,<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>,<xref rid="B33-sensors-19-02242" ref-type="bibr">33</xref>]. Riess et al. [<xref rid="B30-sensors-19-02242" ref-type="bibr">30</xref>] have estimated the source light for each mini region of superpixels using a graph-based algorithm. Their approach creates an illuminant map that is coloured by the local estimates and merges areas with similar light colour together using the quick shift algorithm to obtain a new estimate for each region. In a comparable work, Blier et al. have applied five state-of-the-art algorithms to estimate the local illuminant for superpixels of areas of approximately similar colour [<xref rid="B31-sensors-19-02242" ref-type="bibr">31</xref>]. They choose the best estimates based on the error statistics and combine the estimates for per superpixel using a machine-learning-based regression algorithm. Mazin et al. [<xref rid="B34-sensors-19-02242" ref-type="bibr">34</xref>] proposed a technique to extract a set of gray pixels from the image to estimate a set of possible light source for the pixels by using the Planckian locus of black-body radiators. In [<xref rid="B35-sensors-19-02242" ref-type="bibr">35</xref>], Bianco and Schettini proposed a method to estimate the colour of the light from human faces within the image by using a scale-space histogram filtering method. Their algorithm performs well; however, its application is limited to images that contain a clear visible human face. In [<xref rid="B36-sensors-19-02242" ref-type="bibr">36</xref>], a 3D scene geometry model was created by applying hard and soft segmentation methods to the input image. It then links the statistical information of the different layers of the model to suitable colour constancy techniques. A colour balancing technique was proposed in [<xref rid="B37-sensors-19-02242" ref-type="bibr">37</xref>] that employs a diagonal 3 &#x000d7; 3 matrix on the ground truth illuminant of a 24-colour patch, the method of which generates superior results compared to other diagonal algorithms. Arguing the robustness of the abovementioned methods, a user-guided colour correction method for multiple illuminants was proposed in [<xref rid="B38-sensors-19-02242" ref-type="bibr">38</xref>].</p><p>A biologically inspired model for colour constancy was reported by Gao et al. [<xref rid="B39-sensors-19-02242" ref-type="bibr">39</xref>] that exploits the response of the Double Opponent cells in different channels and estimates the colour of the illuminant by <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> pooling mechanism in long, medium and short wavelength colour space. An improved retinal-mechanism-based model was proposed by Zhang et al. [<xref rid="B40-sensors-19-02242" ref-type="bibr">40</xref>]. The authors imitated the function of Horizontal Cell (HC) modulation that provides global colour correction with cone-specific lateral gain control. Akbarnia and Parraga [<xref rid="B41-sensors-19-02242" ref-type="bibr">41</xref>] proposed a colour constancy model using two overlapping asymmetric Gaussian kernels, where the contrast of the surrounding pixels is used to adjust the kernels&#x02019; sizes (approximating the change of visual neuron&#x02019;s receptive field size). Finally, the outputs of the most activated visual neuron&#x02019;s receptive fields are used to estimate the colour of the light.</p><p>Yang et al. [<xref rid="B42-sensors-19-02242" ref-type="bibr">42</xref>] proposed a grey pixel-based colour constancy method by using the illuminant-invariant measure. Joze and Drew [<xref rid="B43-sensors-19-02242" ref-type="bibr">43</xref>] showed that the texture feature of an image is ideal candidate for illuminant estimation. Their approach took the weakly colour constant RGB values from the texture to find the neighbour surface based on histogram matching from the training data. Male et al. [<xref rid="B44-sensors-19-02242" ref-type="bibr">44</xref>] employed an automatic human eyes detection method and extracted the scelera pixels to estimate the scenes&#x02019; illuminant.</p><p>The aforementioned algorithms work reasonably well when the scene is illuminated by a uniform light source and lacks large uniform colour areas within the image, but the performance of existing algorithms deteriorates in the presence of large uniform colour areas and when the scene is illuminated by multiple non-uniform light sources. This paper presents a colour constancy algorithm for images of scenes lit by multiple non-uniform light sources (this paper is an extended version of a previous paper [<xref rid="B45-sensors-19-02242" ref-type="bibr">45</xref>]). The proposed algorithm first applies a histogram-based approach to determine the number of segments that represent different colour variations of the image. The K-means<sup>++</sup> clustering method [<xref rid="B46-sensors-19-02242" ref-type="bibr">46</xref>] is then used to divide the input image into this number of segments. The proposed method then calculates the Normalized Average Absolute Difference (NAAD) of each resulting segment and uses it as a criterion to discard segments of near-uniform colour, which could potentially bias the colour-balanced image toward those colours. The initial colour constancy adjustment weighting factors for each of the remaining segments are computed based on the assumption that the achromatic values of the colour components of the segment are neutral. The colour constancy adjustment weighting factors for each pixel are finally determined by fusing the colour constancy of all selected segments adjusted by the normalized Euclidian distance of the pixel from the centroids of the selected segments. Experimental results on the images of five benchmark standard image datasets show the merit of the proposed algorithm over existing techniques. The rest of this paper is organized as follows: In <xref ref-type="sec" rid="sec2-sensors-19-02242">Section 2</xref>, the proposed algorithm is described, experimental results and their evaluation are given in <xref ref-type="sec" rid="sec3-sensors-19-02242">Section 3</xref> and <xref ref-type="sec" rid="sec4-sensors-19-02242">Section 4</xref> concludes the paper.</p></sec></sec><sec id="sec2-sensors-19-02242"><title>2. Image Colour Constancy Adjustment by Fusion of Image Segments&#x02019; Initial Colour Correction Factors</title><p>The proposed Colour Constancy Adjustment by Fusion of Image Segments&#x02019; initial colour correction factors (CCAFIS) algorithm is divided into three steps: automatic image segmentation, segment selection and calculation of initial colour constancy weighting factors for each segment and calculation of the colour adjustment factors for each pixel. Each of these three steps is detailed in the following sub-sections.</p><sec id="sec2dot1-sensors-19-02242"><title>2.1. Automatic Image Segmentation</title><p>The proposed automatic image segmentation algorithm first converts the input RGB image into a grey image. It then splits the coefficients within the resulting grey image into a histogram with 256 bins. The resulting histogram is then filtered using the following Gaussian low-pass filter: <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x02026;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>7</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x02026;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>7</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are equal to 0.0002, 0.0029, 0.0161, 0.0537, 0.1208, 0.1934 and 0.2256, respectively. The algorithm then counts the number of the local maxima found in the smoothed histogram to determine the required number of the segments for the proposed colour constancy algorithm. To do this, the minimum distance between two local maxima and the minimum local maxima height were set to 0.05 and 0.001<sup>th</sup> of the total number of image pixels, respectively. These numbers were empirically found to be effective in finding the reasonable number of segments when dealing with images of five different image datasets. The calculated number of segments and the <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mrow><mml:mo>*</mml:mo></mml:msup><mml:msup><mml:mi mathvariant="normal">b</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> format of the input RGB image are then fed into a K-means<sup>++</sup> clustering algorithm, which divides the input image pixels into several segments based on their colour properties.</p></sec><sec id="sec2dot2-sensors-19-02242"><title>2.2. Segments&#x02019; Selection and Calculation of Initial Colour Constancy Weighting Factors for each Segment</title><p>In this section, each of the resulting segments are processed independently to circumvent the segments containing uniform areas and to calculate initial colour adjustment factors for each segment, as follows:</p><p>The proposed technique first calculates the Normalized Average Absolute Difference (NAAD) of pixels for each colour component of the segment using Equation (2):<disp-formula id="FD2-sensors-19-02242"><label>(2)</label><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>NAAD</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>A</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the Normalized Average Absolute Difference of the segments colour component <inline-formula><mml:math id="mm15"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, T is the total number of pixels in the segment, <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents component <inline-formula><mml:math id="mm18"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>&#x02019;s coefficients of the segment at location <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the average value of the component <inline-formula><mml:math id="mm22"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> of the segment&#x02019;s coefficients.</p><p>The resulting <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>A</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> value is compared with an empirically pre-determined threshold value for that colour component. The threshold values for the colour components R, G and B are named <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">G</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. If the calculated <inline-formula><mml:math id="mm27"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>A</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> values of the three colour components of the segment are greater than their respective threshold values, the segment represents a non-uniform colour area. Hence, this segment is selected to contribute to the colour correction of the whole image and a bit representing this segment within the Decision Vector (DV) is set. The proposed technique then calculates the initial colour adjustment factors for the selected segment using the Grey World algorithm [<xref rid="B16-sensors-19-02242" ref-type="bibr">16</xref>], as written in Equation (3):<disp-formula id="FD3-sensors-19-02242"><label>(3)</label><mml:math id="mm28"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac bevelled="true"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is component C&#x02019;s initial colour adjustment factor, <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the average value of the segment&#x02019;s coefficients, and <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the average value of the segment&#x02019;s component C&#x02019;s coefficients, where <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In this research, the Grey World colour constancy method, which is one of the most effective and yet computationally inexpensive techniques compared to other statistical colour constancy algorithms [<xref rid="B2-sensors-19-02242" ref-type="bibr">2</xref>,<xref rid="B3-sensors-19-02242" ref-type="bibr">3</xref>], is used to compute the initial colour constancy weighting factors for segments. However, other statistical colour constancy methods can be used.</p></sec><sec id="sec2dot3-sensors-19-02242"><title>2.3. Calculation of the Colour Adjustment Factors for each Pixel</title><p>In this section, the fusion of the initially calculated colour constancy adjustment factors for the selected segments to calculate per pixel colour correction weighting factors is discussed. The algorithm is then fed the calculated initial colour constancy adjustment factors of the selected segments, the gravitational centroid of each selected segment&#x02019;s pixels and the Decision Vector (DV). The proposed algorithm calculates the Euclidian distance of each pixel from the centers of the selected segments and uses them to regulate initially calculated colour adjustment factors to determine per pixel weighting factors using Equation (4):<disp-formula id="FD4-sensors-19-02242"><label>(4)</label><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the colour constancy adjustment factor for component C of the pixel i, C <inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic>d</italic><sub>1</sub>, <italic>d</italic><sub>2</sub>, &#x02026;, and <italic>d</italic><sub>n</sub> are the Euclidian distance of the pixel i from the centroid of the segments 1, 2,.., <italic>n</italic>, respectively.</p><p>This balances the effect of colours of different light sources on the colour of each pixel. The resulting weighting factors are used to colour balance the input image using The Von-Kries Diagonal model [<xref rid="B47-sensors-19-02242" ref-type="bibr">47</xref>], as shown in Equation (5):<disp-formula id="FD6-sensors-19-02242"><label>(5)</label><mml:math id="mm37"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the colour balanced pixel i, <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm41"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the calculated weighting factors for pixel i and <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the R, G and B colour components of the pixel i of the input image.</p></sec></sec><sec id="sec3-sensors-19-02242"><title>3. Experimental Results and Evaluation</title><p>In this section, the performance of the proposed Colour Constancy Adjustment by Fusion of Image Segments&#x02019; initial colour correction factors (CCAFIS) is assessed on five benchmark image datasets, namely: Multiple Light Source (MLS) [<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>], The Multiple Illuminant and Multiple Object (MIMO) [<xref rid="B33-sensors-19-02242" ref-type="bibr">33</xref>], The Colour Checker [<xref rid="B48-sensors-19-02242" ref-type="bibr">48</xref>], Grey Ball [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>] and the UPenn Natural image dataset [<xref rid="B50-sensors-19-02242" ref-type="bibr">50</xref>]. The Evaluation procedures are discussed in <xref ref-type="sec" rid="sec3dot1-sensors-19-02242">Section 3.1</xref>, an example of image segmentation and segment selection is given in <xref ref-type="sec" rid="sec3dot2-sensors-19-02242">Section 3.2</xref> and the experimental results are discussed in <xref ref-type="sec" rid="sec3dot3-sensors-19-02242">Section 3.3</xref>.</p><sec id="sec3dot1-sensors-19-02242"><title>3.1. Evaluation Procedure</title><p>The performance of the colour constancy algorithms are generally assessed both objectively and subjectively. The angular error, also known as the recovery angular error, is an objective criterion that is widely used to assess the colour constancy of the images which measures the distance between the colour-corrected image and its ground truth [<xref rid="B51-sensors-19-02242" ref-type="bibr">51</xref>]. A lower resulting mean or median angular error of the images of an algorithm indicates that the algorithm&#x02019;s performance is superior. The recovery angular error of an image can be calculated using Equation (6):<disp-formula id="FD7-sensors-19-02242"><label>(6)</label><mml:math id="mm45"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>cos</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02016;</mml:mo><mml:mo>&#x02016;</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm46"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> shows the dot product of the ground-truth and the colour-corrected image vectors, respectively, and <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mo>.</mml:mo><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Euclidian norm of the vector.</p><p>Recently, Finlayson et al. [<xref rid="B52-sensors-19-02242" ref-type="bibr">52</xref>] have critiqued the application of the (recovery) angular error measure based on the argument that it produces different results for identical scenes viewed under different colour light sources. They proposed an improved version of the recovery angular error measure, called the reproduction angular error, which is defined as the angle between the image RGB of a white surface when the actual and estimated illuminations are &#x02018;divided out&#x02019;. The reproduction angular error metric can be calculated using Equation (7):<disp-formula id="FD8-sensors-19-02242"><label>(7)</label><mml:math id="mm48"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="italic">cos</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mi>e</mml:mi><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>.</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#x0221a;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> is the true colour of the white reference.</p><p>Both recovery and reproduction angular error have been used to assess the objective quality of the colour-corrected image by computing the average of the mean or median recovery/reproduction angular errors of different methods on a large set of colour-corrected images and using them for comparison purpose. The images of the method that have the lowest average of the mean or median recovery/reproduction angular errors have the highest colour constancy.</p><p>Because human eyes are the final judge for assessing the colour constancy of images, a subjective evaluation is generally considered to be the most reliable assessment method when evaluating colour constancy algorithms. Mean Opinion Score (MOS) is a popular subjective evaluation method that is widely used to compare the visual quality of the images. To determine the MOS for images subjected to different colour constancy adjustment methods, a set of images of different scenes containing diverse backgrounds, foregrounds, objects and a range of colour variations that are taken under various lighting conditions is first chosen. These images are then colour-corrected using different colour balancing techniques. The resulting images are shown to observers who score the images based on their colour constancy (the same laptop was used to present the images to all observers). The MOS of each method finally calculated by computing the average score of its images.</p><p>Objective criteria are widely used to assess the performance of different colour constancy techniques due to their simplicity. However, there is significant debate on the merit of objective measurements and their relation to subjective assessment. Some researchers have argued that objective measurements may not always be in agreement with the subjective quality of the image [<xref rid="B53-sensors-19-02242" ref-type="bibr">53</xref>,<xref rid="B54-sensors-19-02242" ref-type="bibr">54</xref>]. Consequently, in this paper, both subjective and objective assessment methods have been used to assess the performance of the colour constancy algorithms.</p></sec><sec id="sec3dot2-sensors-19-02242"><title>3.2. Example of Image Segmentation and Segment Selection</title><p>To visualize the effectiveness of the proposed method in dividing the image into several segments and identifying segments with uniform colour areas, a sample image from the UPenn images dataset [<xref rid="B50-sensors-19-02242" ref-type="bibr">50</xref>] is taken and processed by the proposed method. <xref ref-type="fig" rid="sensors-19-02242-f001">Figure 1</xref>a shows the input image and <xref ref-type="fig" rid="sensors-19-02242-f001">Figure 1</xref>b&#x02013;e show its resulting four segments, with the pixels excluded from each segment coloured black. From <xref ref-type="fig" rid="sensors-19-02242-f001">Figure 1</xref>, it is obvious that <xref ref-type="fig" rid="sensors-19-02242-f001">Figure 1</xref>d represents the segment with uniform colour areas.</p><p>The Normalized Average Absolute Differences (NAAD) of the three colour components of the segments of the image shown in <xref ref-type="fig" rid="sensors-19-02242-f001">Figure 1</xref> have been calculated using Equation (2) and tabulated in <xref rid="sensors-19-02242-t001" ref-type="table">Table 1</xref>. From this table, it is clear that the calculated NAAD values for the three colour components of segment 3 (highlighted with a green border), which represents the uniform colour area of the image, are below the empirically determined thresholds&#x02019; values. As a pre-processing step, extensive experiments were performed using the benchmark image dataset to empirically determine the threshold values. The results showed that a threshold value of 0.01 for the T<sub>R</sub>, T<sub>G</sub> and T<sub>B</sub> components can efficiently eliminate segments with uniform areas [<xref rid="B55-sensors-19-02242" ref-type="bibr">55</xref>]. Hence, the proposed technique will exclude this segment&#x02019;s coefficients from contributing into the colour correction of the whole image.</p></sec><sec id="sec3dot3-sensors-19-02242"><title>3.3. Experimental Results</title><p>In the next two sub-sections, subjective and objective results for the proposed method will be presented.</p><sec id="sec3dot3dot1-sensors-19-02242"><title>3.3.1. Subjective Result</title><p>To demonstrate the subjective performance of the proposed Colour Constancy Adjustment by Fusion of Image Segments&#x02019; initial colour correction factors (CCAFIS) method and to compare the quality of its colour-corrected images with those of the state of the art colour constancy techniques, two sample images from the Colour Checker and the Upenn Natural Image benchmark image datasets are selected and colour-balanced using different colour correction methods. <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref> shows a sample image from the Colour Checker image dataset, its corresponding ground truth and its colour-balanced images using the Weighted Grey Edge [<xref rid="B20-sensors-19-02242" ref-type="bibr">20</xref>], Corrected Moment [<xref rid="B21-sensors-19-02242" ref-type="bibr">21</xref>], Cheng et al. [<xref rid="B38-sensors-19-02242" ref-type="bibr">38</xref>] and the proposed CCAFIS techniques. The resulting images have been linearly gamma-corrected to improve their visual qualities. From <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref>a, it can be seen that the input image has a significant green colour cast and the scene is illuminated by multiple indoor and outdoor light sources. <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref>b shows the ground truth of the image. <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref>c shows the Weighted Grey Edge method&#x02019;s image, which demonstrates a slightly lower green colour cast than the input image. <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref>d illustrates the Corrected Moment&#x02019;s image, which exhibits a yellow to orange colour cast. Cheng et al.&#x02019;s method&#x02019;s image is shown in <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref>e. This image suffers from the presence of a deep yellow-orange colour cast. The proposed CCAFIS method&#x02019;s image, shown in <xref ref-type="fig" rid="sensors-19-02242-f002">Figure 2</xref>f, exhibits high colour constancy and appears to have the closest colour constancy to that of the ground truth image. The recovery angular error of the images are also shown on the images; from these figures it can be seen that the recovery angular error of the proposed method&#x02019;s image is the lowest among all other methods. This implies that the objective qualities of the images are consistent with their subjective qualities.</p><p><xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref> shows a sample image from the UPenn dataset [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>], its ground truth and colour-balanced images using the Max-RGB, Shades of Grey, Grey Edge-1, Grey Edge-2, Weighted Grey Edge and the proposed CCAFIS&#x02019; methods&#x02019; images. From <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>a, it can be noted that the input image exhibits a yellow colour cast. The tree&#x02019;s green leaves and the colour chart exhibit yellow colour cast. <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>b is its ground truth image. <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>c shows the Max-RGB method&#x02019;s image. From this image, it can be seen that the image has a slightly higher yellow colour cast than its original input image. The Shades of Grey method&#x02019;s image is shown in <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>d. This figure demonstrates significantly higher yellow colour cast than the original image, particularly on the tree&#x02019;s green leaves area of the image. <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>e is the Grey Edge-1 method&#x02019;s image. This image also suffers from an increased colour cast on the tree&#x02019;s green leaves and the colour chart areas of the image. The Grey Edge-method&#x02019;s image is shown in <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>f. This image demonstrates a slightly higher colour constancy than its original image. The tree and the deciduous plants on the left side of the image have a slightly lower colour cast than the original image. The Weighted Grey Edge method&#x02019;s image is illustrated in <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>g. This image is appeared to be very alike to that of the Grey Edge-1 method&#x02019;s image, shown in <xref ref-type="fig" rid="sensors-19-02242-f003">Figure 3</xref>e.</p><p><xref ref-type="fig" rid="sensors-19-02242-f004">Figure 4</xref> illustrates a sample image from the Gray Ball dataset with a yellow colour cast, its respective ground truth image and its colour-balanced images using Edge-based gamut [<xref rid="B23-sensors-19-02242" ref-type="bibr">23</xref>], Grey pixel [<xref rid="B42-sensors-19-02242" ref-type="bibr">42</xref>], RCC-Net [<xref rid="B29-sensors-19-02242" ref-type="bibr">29</xref>], and the proposed CCAFIS methods&#x02019; images. From <xref ref-type="fig" rid="sensors-19-02242-f004">Figure 4</xref>c, it can be seen that the Gamut-based method exhibits a high level of red colour casts. <xref ref-type="fig" rid="sensors-19-02242-f004">Figure 4</xref>d and <xref ref-type="fig" rid="sensors-19-02242-f004">Figure 4</xref>e are the images of the Grey Pixel and the RCC-Net methods, respectively. These images demonstrate an improved colour balance to that of the input image. However, the images have still some levels of yellow colour cast. The proposed CCAFIS method&#x02019;s image (<xref ref-type="fig" rid="sensors-19-02242-f004">Figure 4</xref>f) appears to be a shot under canonical light as the presence of the source illuminant is significantly reduced. Moreover, the median recovery angular error of the proposed technique&#x02019;s image is the lowest among all other techniques&#x02019; images, which means the proposed technique&#x02019;s image has the highest objective colour constancy.</p><p><xref ref-type="fig" rid="sensors-19-02242-f005">Figure 5</xref> illustrates a sample image from the Gray Ball image dataset [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>], which has a yellow colour cast, and its colour-balanced images using the Edge-based gamut [<xref rid="B23-sensors-19-02242" ref-type="bibr">23</xref>], Grey pixel [<xref rid="B42-sensors-19-02242" ref-type="bibr">42</xref>], RCC-Net [<xref rid="B29-sensors-19-02242" ref-type="bibr">29</xref>] and the proposed CCAFIS methods&#x02019; images. From <xref ref-type="fig" rid="sensors-19-02242-f005">Figure 5</xref>c, it can be seen that the Gamut-based method&#x02019;s image exhibits both blue and reddish colour cast. However, this image exhibits lower colour constancy to those of the Grey pixel and the RCC-Net techniques&#x02019; images, shown in <xref ref-type="fig" rid="sensors-19-02242-f005">Figure 5</xref>d and <xref ref-type="fig" rid="sensors-19-02242-f005">Figure 5</xref>e, respectively. Nevertheless, the images of all these three techniques still have visible yellow colour cast. The proposed CCAFIS method&#x02019;s image (<xref ref-type="fig" rid="sensors-19-02242-f005">Figure 5</xref>f) appears as if being taken under a white illuminant. The recovery angular errors of the images are also calculated and displayed on the images. By comparing the images&#x02019; recovery angular error, it can be seen that the proposed method&#x02019;s image has the lowest recovery angular error, which means it exhibits the highest objective colour quality to other techniques images.</p><p>To give the reader a better understanding in the performance of the proposed CCAFIS method on images of scenes with spatially varying illuminant distribution, an image from the MLS image dataset [<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>] that represents a scene lit by spatially varying illumination is taken and colour corrected using the proposed technique, Grey Edge-2, Weighted Grey Edge and Gisenji et al. The original image, its ground truth and the resulting colour-corrected images using the proposed CCAFIS and other techniques are shown in <xref ref-type="fig" rid="sensors-19-02242-f006">Figure 6</xref>. From this figure, it can be noted that the proposed technique&#x02019;s image exhibits the highest colour constancy. In addition, it has the lowest median angular error among all other techniques&#x02019; images, which implies that the proposed technique&#x02019;s image has the highest objective quality.</p><p>To generate the Mean Opinion Score (MOS) for the images of the proposed and the state-of-the-art colour constancy methods, a set of images from the Grey Ball, the Colour Checker and the MIMO image datasets, which contain images of scenes lit by either single or multiple light sources, was chosen. The selected images were colour-corrected using the proposed CCAFIS method, as well as other state-of-the art techniques including the multiple illuminant methods such as: Gijsenij et al. [<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>], MIRF [<xref rid="B33-sensors-19-02242" ref-type="bibr">33</xref>] and Cheng et al. [<xref rid="B38-sensors-19-02242" ref-type="bibr">38</xref>]. Ten independent observers subjectively evaluated the resulting colour-balanced images. The viewers scored the colour constancy of each image from 1 to 5, where higher numbers correspond to increased colour constancy. The average MOS of different methods&#x02019; images were then calculated and tabulated in <xref rid="sensors-19-02242-t002" ref-type="table">Table 2</xref>. From <xref rid="sensors-19-02242-t002" ref-type="table">Table 2</xref>, it can be noted that the proposed method&#x02019;s images have the highest average MOS when compared to the other techniques&#x02019; images. This implies that the proposed method&#x02019;s images have the uppermost subjective colour constancy.</p></sec><sec id="sec3dot3dot2-sensors-19-02242"><title>3.3.2. Objective Result</title><p>To evaluate the objective performance of the proposed method, Grey world [<xref rid="B16-sensors-19-02242" ref-type="bibr">16</xref>], Max-RGB [<xref rid="B17-sensors-19-02242" ref-type="bibr">17</xref>], Grey Edge-1 [<xref rid="B19-sensors-19-02242" ref-type="bibr">19</xref>], Grey Edge-2 [<xref rid="B19-sensors-19-02242" ref-type="bibr">19</xref>], Gijsenij et al. [<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>], MIRF [<xref rid="B33-sensors-19-02242" ref-type="bibr">33</xref>], ASM [<xref rid="B41-sensors-19-02242" ref-type="bibr">41</xref>], Grey Pixel [<xref rid="B42-sensors-19-02242" ref-type="bibr">42</xref>], Exemplar [<xref rid="B43-sensors-19-02242" ref-type="bibr">43</xref>] and CNN+SVR [<xref rid="B28-sensors-19-02242" ref-type="bibr">28</xref>] methods were used to colour balance the images of the MIMO [<xref rid="B33-sensors-19-02242" ref-type="bibr">33</xref>], the Grey Ball [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>] and the Colour Checker [<xref rid="B48-sensors-19-02242" ref-type="bibr">48</xref>] image datasets, as well as 9 outdoor images of the Multiple light sources image dataset [<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>]. The average mean and median of both recovery and reproduction angular errors of the colour-balanced images of the Grey Ball and the Colour Checker dataset are tabulated in <xref rid="sensors-19-02242-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-19-02242-t004" ref-type="table">Table 4</xref>, respectively.</p><p>From <xref rid="sensors-19-02242-t003" ref-type="table">Table 3</xref>, the proposed technique&#x02019;s images have the lowest average mean and median recovery and reproduction angular errors among all the statistics-based colour constancy methods, implying that the proposed technique outperforms statistics- and gamut-based techniques with respect to objective colour constancy (the mean and the median angular errors of the Deep Learning, Natural Image Statistics and Spectral Statistics were taken from [<xref rid="B41-sensors-19-02242" ref-type="bibr">41</xref>]). When compared to the learning-based methods, the proposed technique&#x02019;s average median angular error equals 4.0&#x000b0;, which is slightly higher than that of the learning-based methods. The proposed algorithm&#x02019;s median reproduction angular error is 2.6&#x000b0;, which is the lowest among all techniques apart from ASM with a median angular error of 2.3&#x000b0;. This demonstrates that the proposed method produces very competitive objective results compared to those of the learning-based methods.</p><p>According to <xref rid="sensors-19-02242-t004" ref-type="table">Table 4</xref>, the proposed CCAFIS technique&#x02019;s images have the lowest average mean and median recovery and reproduction angular errors among all the statistics-based colour constancy methods, which implies that the proposed technique outperforms statistics- and gamut-based techniques with respect to objective colour constancy. With respect to the learning-based methods, the proposed technique&#x02019;s average median angular error equals 2.7&#x000b0;, which is slightly higher than some of the learning-based methods (the mean and the median angular error of the AAC, HLVIBU, HLVI BU &#x00026; TD, CCDL, EB, BDP, CM, FB+GM, PCL, SF, CCP, CCC, AlexNet + SVR, CNN-Per patch, CNN average-pooling, CNN median-pooling and CNN fine-tuned methods have been taken from [<xref rid="B28-sensors-19-02242" ref-type="bibr">28</xref>]). The proposed algorithm&#x02019;s median reproduction angular error is 2.9<inline-formula><mml:math id="mm110"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>, which is the lowest among all techniques except the Exemplar-based method with a median angular error of 2.6<inline-formula><mml:math id="mm111"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>. This demonstrates that the proposed method produces very competitive objective results compared to those of the learning-based methods.</p><p>The average mean and median recovery angular errors of the colour-balanced images from the MIMO image dataset for different techniques were computed and tabulated in <xref rid="sensors-19-02242-t005" ref-type="table">Table 5</xref> and the median angular errors for 9 outdoor image of multiple light source dataset for different algorithms were determined and tabulated in <xref rid="sensors-19-02242-t006" ref-type="table">Table 6</xref>.</p><p>From <xref rid="sensors-19-02242-t005" ref-type="table">Table 5</xref>, it can be seen that the proposed CCAFIS method&#x02019;s mean recovery angular error for real world images of the MIMO dataset is 4.2&#x000b0;. Please note that the mean and the median recovery angular error for the MLS + GW, MLS + WP, MIRF + GW, MIRF + WP and MIsRF + IEbV methods have been taken from [<xref rid="B28-sensors-19-02242" ref-type="bibr">28</xref>]. This is the same as the Grey World&#x02019;s and MLS + GW mean recovery angular error and slightly higher than that of the MIRF methods, which produced images having the smallest mean angular error of 4.1&#x000b0;. The median recovery angular error of the proposed CCAFIS method is 4.3&#x000b0;, which is the lowest among all of the statistics-based methods. For the laboratory images, the proposed CCAFIS method&#x02019;s mean recovery angular error is 2.1&#x000b0; and the median recovery angular is 2.7&#x000b0;, which are the lowest recovery angular errors compared to all other methods. This implies that the proposed CCAFIS method has the highest objective performance when dealing with lab images of MIMO dataset.</p><p>From <xref rid="sensors-19-02242-t006" ref-type="table">Table 6</xref>, it can be noted that the proposed CCAFIS method&#x02019;s images exhibit the lowest median recovery angular error among all statistical and the state of art techniques. This implies that the proposed CCAFIS method outperforms other methods in adjusting the colour constancy of the images taken from scenes illuminated by multiple light sources.</p></sec></sec></sec><sec id="sec4-sensors-19-02242"><title>4. Execution Time</title><p>To enable the reader to compare the execution time of the proposed Colour Constancy Adjustment by Fusion of Image Segments&#x02019; initial colour correction factors (CCAFIS) algorithm with both statistical-based and learning-based state of the art colour constancy techniques, the proposed method, Gray World, Max-RGB (White Patch), Gray Edge-1, Gray Edge-2, Exemplar-based [<xref rid="B43-sensors-19-02242" ref-type="bibr">43</xref>], Gray Pixel (std) [<xref rid="B42-sensors-19-02242" ref-type="bibr">42</xref>] and ASM [<xref rid="B41-sensors-19-02242" ref-type="bibr">41</xref>] algorithms were implemented in MATLAB. They were then run on the same Microsoft Windows 10 based personal computer, running on Intel<sup>&#x000ae;</sup> Core (TM) i3-6006U CPU with a 1.99 GHz processor, 4.00 GB of RAM, and without any additional dedicated graphic processing unit. These methods were timed when applied to colour balance the first 100 images of the Colour Checker benchmark image dataset. For the learning-based techniques, the second 100 images from the Colour Checker benchmark image dataset were used for training. The cumulative execution time for testing the proposed CCAFIS, statistical-based and learning-based methods were measured and tabulated in <xref rid="sensors-19-02242-t007" ref-type="table">Table 7</xref>. From <xref rid="sensors-19-02242-t007" ref-type="table">Table 7</xref>, it can be seen that the proposed CCAFIS technique requires slightly more execution time to colour balance the images in comparison to other statistical-based methods. This is consistent with the fact that the proposed technique calculates colour adjustment factors for each pixel of the input image separately. Moreover, this is the price for achieving higher colour constancy in the presence of large uniform colour patches within the image and when the scene is illuminated by multiple non-uniform light sources. However, the proposed method demonstrates very competitive colour correction at a significantly lower computation time when compared to the learning-based state of the art techniques. In summary, it can be concluded that the proposed CCAFIS algorithm provides significantly higher performance to those of statistical-based colour constancy adjustment techniques at slightly higher computational cost, while generating very competitive results that are comparable to those of the state-of-the-art learning-based methods, but at a hugely lower computational cost. In addition, the proposed CCAFIS technique is data independent and produces accurate results without prior knowledge of the image dataset, unlike the learning-based algorithms, which achieve high performance when dealing with the images of the dataset that was used for training. The performance of the learning-based techniques usually deteriorates when used for cross-dataset image testing.</p></sec><sec sec-type="conclusions" id="sec5-sensors-19-02242"><title>5. Conclusions</title><p>This paper presented a colour constancy algorithm for non-uniformly lit scenes&#x02019; images. The algorithm uses the K-means<sup>++</sup> clustering algorithm along with a histogram-based method to divide the input image into several segments with similar colour variations. The Normalized Average Absolute Difference (NAAD) of the resulting segments are then calculated and used as a measure to identify segments with uniform colour areas. These segments are then excluded from the calculation of colour constancy adjustment factors for the whole image. The initial colour constancy-weighting factor for each of the remaining segments is then calculated using the Grey World method. The colour constancy adjustment factors for each pixel is finally computed by fusing the initial colour constancy of the remaining segments, regulated by the Euclidian distances of the pixel from the centroids of all remaining segments. Experimental results on both single and multiple illuminant benchmark image datasets showed that the proposed method gives a significantly higher performance to those of the state-of-the-art statistical-based techniques. Furthermore, the proposed techniques gave higher or very competitive results to those of learning-based techniques at a fraction of the computational cost.</p></sec></body><back><notes><title>Author Contributions</title><p>Supervision, A.S.A.; Writing&#x02014;original draft, M.A.H.; Writing&#x02014;review &#x00026; editing, I.M.</p></notes><notes><title>Funding</title><p>This research received no external funding.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-19-02242"><label>1.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Bowdle</surname><given-names>D.</given-names></name></person-group><article-title>Background Science for Colour Imaging and CCDs, Faulkes Telescope Project</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.euhou.net/docupload/files/euhou_colour_images_ccds.pdf.">http://www.euhou.net/docupload/files/euhou_colour_images_ccds.pdf.</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2019-02-12">(accessed on 12 February 2019)</date-in-citation></element-citation></ref><ref id="B2-sensors-19-02242"><label>2.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Urbaniak</surname><given-names>A.</given-names></name></person-group><article-title>&#x0201c;Monochroming&#x0201d; a Colour Sensor and Colour Photography with the Monochrom. Wilddog</article-title><year>2017</year><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://wilddogdesign.co.uk/blog/monochroming-colour-sensor-colour-photography-monochrom.">https://wilddogdesign.co.uk/blog/monochroming-colour-sensor-colour-photography-monochrom.</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2019-02-08">(accessed on 8 February 2019)</date-in-citation></element-citation></ref><ref id="B3-sensors-19-02242"><label>3.</label><element-citation publication-type="web"><person-group person-group-type="author"><collab>Applications, E. Inc.</collab><collab>Edmund Optics Inc. and E. Inc.</collab></person-group><article-title>&#x0201c;Imaging Electronics 101: Understanding Camera Sensors for Machine Vision Applications&#x0201d;, Edmundoptics.com, 2019</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.edmundoptics.com/resources/application-notes/imaging/understanding-camera-sensors-for-machine-vision-applications/">https://www.edmundoptics.com/resources/application-notes/imaging/understanding-camera-sensors-for-machine-vision-applications/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2019-03-22">(accessed on 22 March 2019)</date-in-citation></element-citation></ref><ref id="B4-sensors-19-02242"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliver</surname><given-names>S.</given-names></name><name><surname>Peter</surname><given-names>S.L.</given-names></name></person-group><article-title>A True-Color Sensor and Suitable Evaluation Algorithm for Plant Recognition</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1823</elocation-id><pub-id pub-id-type="doi">10.3390/s17081823</pub-id></element-citation></ref><ref id="B5-sensors-19-02242"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woo</surname><given-names>S.M.</given-names></name><name><surname>Lee</surname><given-names>S.H.</given-names></name><name><surname>Yoo</surname><given-names>J.S.</given-names></name><name><surname>Kim</surname><given-names>J.O.</given-names></name></person-group><article-title>Improving Color Constancy in an Ambient Light Environment Using the Phong Reflection Model</article-title><source>IEEE Trans. Image Process.</source><year>2018</year><volume>27</volume><fpage>1862</fpage><lpage>1877</lpage><pub-id pub-id-type="doi">10.1109/TIP.2017.2785290</pub-id><pub-id pub-id-type="pmid">29346100</pub-id></element-citation></ref><ref id="B6-sensors-19-02242"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tai</surname><given-names>S.</given-names></name><name><surname>Liao</surname><given-names>T.</given-names></name><name><surname>Chang</surname><given-names>C.Y.</given-names></name></person-group><article-title>Automatic White Balance algorithm through the average equalization and threshold</article-title><source>Proceedings of the 8th International Conference on Information Science and Digital Content Technology</source><conf-loc>Jeju, Korea</conf-loc><conf-date>26&#x02013;28 June 2012</conf-date><fpage>571</fpage><lpage>576</lpage></element-citation></ref><ref id="B7-sensors-19-02242"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>S.J.J.</given-names></name></person-group><article-title>Robust Algorithm for Computational Color Constancy</article-title><source>Proceedings of the 2014 International Conference on Technologies and Applications of Artificial Intelligence</source><conf-loc>Hsinchu, Taiwan</conf-loc><conf-date>18&#x02013;20 November 2010</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B8-sensors-19-02242"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Banic</surname><given-names>N.</given-names></name><name><surname>Lon&#x0010d;ari&#x00107;</surname><given-names>S.</given-names></name></person-group><article-title>Improving the white patch method by subsampling</article-title><source>Proceedings of the 2014 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Paris, France</conf-loc><conf-date>27&#x02013;30 October 2014</conf-date><fpage>605</fpage><lpage>609</lpage></element-citation></ref><ref id="B9-sensors-19-02242"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bani&#x00107;</surname><given-names>N.</given-names></name><name><surname>Lon&#x0010d;ari&#x00107;</surname><given-names>S.</given-names></name></person-group><article-title>Color Cat: Remembering Colors for Illumination Estimation</article-title><source>IEEE Signal Process. Lett.</source><year>2013</year><volume>22</volume><fpage>651</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1109/LSP.2014.2366973</pub-id></element-citation></ref><ref id="B10-sensors-19-02242"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Joze</surname><given-names>H.R.V.</given-names></name><name><surname>Drew</surname><given-names>M.S.</given-names></name></person-group><article-title>White Patch Gamut Mapping Colour Constancy</article-title><source>Proceedings of the 2012 19th IEEE International Conference on Image Processing (ICIP 2012)</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>30 September&#x02013;3 October 2012</conf-date><fpage>801</fpage><lpage>804</lpage></element-citation></ref><ref id="B11-sensors-19-02242"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sim&#x000e3;o</surname><given-names>J.</given-names></name><name><surname>Schneebeli</surname><given-names>H.J.A.</given-names></name><name><surname>Vassallo</surname><given-names>R.F.</given-names></name></person-group><article-title>An Iterative Approach for Colour Constancy</article-title><source>Proceedings of the 2014 Joint Conference on Robotics: SBR-LARS Robotics Symposium and Robocontrol</source><conf-loc>Sao Carlos, Sao Paolo, Brazil</conf-loc><conf-date>18&#x02013;23 October 2014</conf-date><fpage>130</fpage><lpage>135</lpage></element-citation></ref><ref id="B12-sensors-19-02242"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gijsenij</surname><given-names>A.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Van de Weijer</surname><given-names>J.</given-names></name></person-group><article-title>Computational Colour Constancy: Survey and Experiments</article-title><source>IEEE Trans. Image Process.</source><year>2011</year><volume>20</volume><fpage>2475</fpage><lpage>2489</lpage><pub-id pub-id-type="doi">10.1109/TIP.2011.2118224</pub-id><pub-id pub-id-type="pmid">21342844</pub-id></element-citation></ref><ref id="B13-sensors-19-02242"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Aytekin</surname><given-names>C.</given-names></name><name><surname>Nikkanen</surname><given-names>J.</given-names></name><name><surname>Gabbouj</surname><given-names>M.</given-names></name></person-group><article-title>Deep multi-resolution color constancy</article-title><source>Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Beijing, China</conf-loc><conf-date>17&#x02013;20 September 2017</conf-date><fpage>3735</fpage><lpage>3739</lpage></element-citation></ref><ref id="B14-sensors-19-02242"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lam</surname><given-names>E.Y.</given-names></name></person-group><article-title>Combining gray world and retinex theory for automatic white balance in digital photography</article-title><source>Proceedings of the Ninth International Symposium on Consumer Electronics, ISCE 2005</source><conf-loc>Macau, China</conf-loc><conf-date>14&#x02013;16 June 2005</conf-date><fpage>134</fpage><lpage>139</lpage></element-citation></ref><ref id="B15-sensors-19-02242"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Batur</surname><given-names>A.U.</given-names></name></person-group><article-title>A real-time auto white balance algorithm for mobile phone cameras</article-title><source>Proceedings of the 2012 IEEE International Conference on Consumer Electronics (ICCE)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>13&#x02013;16 January 2012</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B16-sensors-19-02242"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchsbaum</surname><given-names>G.</given-names></name></person-group><article-title>A spatial processor model for object colour perception</article-title><source>J. Frankl. Inst.</source><year>1980</year><volume>32</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/0016-0032(80)90058-7</pub-id></element-citation></ref><ref id="B17-sensors-19-02242"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>E.</given-names></name></person-group><article-title>The retinex theory of colour vision</article-title><source>Sci. Am.</source><year>1977</year><volume>237</volume><fpage>28</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/scientificamerican1277-108</pub-id></element-citation></ref><ref id="B18-sensors-19-02242"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Finlayson</surname><given-names>G.D.</given-names></name><name><surname>Trezzi</surname><given-names>E.</given-names></name></person-group><source>Shades of Grey and Colour Constancy</source><publisher-name>Society for Imaging Science and Technology</publisher-name><publisher-loc>Scottsdale, AZ, USA</publisher-loc><year>2004</year><fpage>37</fpage><lpage>41</lpage></element-citation></ref><ref id="B19-sensors-19-02242"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Weijer</surname><given-names>J.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Gijsenij</surname><given-names>A.</given-names></name></person-group><article-title>Edge-based colour constancy</article-title><source>IEEE Trans Image Process.</source><year>2007</year><volume>16</volume><fpage>2207</fpage><lpage>2217</lpage><pub-id pub-id-type="doi">10.1109/TIP.2007.901808</pub-id><pub-id pub-id-type="pmid">17784594</pub-id></element-citation></ref><ref id="B20-sensors-19-02242"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gijsenij</surname><given-names>A.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Van de Weijer</surname><given-names>J.</given-names></name></person-group><article-title>Improving Colour Constancy by Photometric Edge Weighting</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2012</year><volume>34</volume><fpage>918</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.197</pub-id><pub-id pub-id-type="pmid">22442121</pub-id></element-citation></ref><ref id="B21-sensors-19-02242"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finlayson</surname><given-names>G.D.</given-names></name></person-group><article-title>Corrected-Moment Illuminant Estimation</article-title><source>2013 IEEE Int. Conf. Comput. Vis.</source><year>2013</year><pub-id pub-id-type="doi">10.1109/ICCV.2013.239</pub-id></element-citation></ref><ref id="B22-sensors-19-02242"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forsyth</surname><given-names>D.A.</given-names></name></person-group><article-title>A novel algorithm for color constancy</article-title><source>Int. J. Comput. Vis.</source><year>1990</year><volume>5</volume><fpage>5</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1007/BF00056770</pub-id></element-citation></ref><ref id="B23-sensors-19-02242"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gijsenij</surname><given-names>A.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Van de Weijer</surname><given-names>J.</given-names></name></person-group><article-title>Generalized gamut mapping using image derivative structures for color constancy</article-title><source>Int. J. Comput. Vis.</source><year>2010</year><volume>86</volume><fpage>127</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1007/s11263-008-0171-3</pub-id></element-citation></ref><ref id="B24-sensors-19-02242"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finlayson</surname><given-names>G.</given-names></name><name><surname>Hordley</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>R.</given-names></name></person-group><article-title>Convex programming colour constancy with a diagonal-offset model</article-title><source>IEEE Int. Conf. Image Process.</source><year>2005</year><volume>3</volume><fpage>948</fpage><lpage>951</lpage></element-citation></ref><ref id="B25-sensors-19-02242"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>J.T.</given-names></name></person-group><article-title>Convolutional Color Constancy</article-title><source>Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Santiago, Chile</conf-loc><conf-date>13&#x02013;16 December 2015</conf-date><fpage>379</fpage><lpage>387</lpage></element-citation></ref><ref id="B26-sensors-19-02242"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>S.</given-names></name><name><surname>Cusano</surname><given-names>C.</given-names></name><name><surname>Schettini</surname><given-names>R.</given-names></name></person-group><article-title>Color constancy using CNNs</article-title><source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>81</fpage><lpage>89</lpage></element-citation></ref><ref id="B27-sensors-19-02242"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fourure</surname><given-names>D.</given-names></name><name><surname>Emonet</surname><given-names>R.</given-names></name><name><surname>Fromont</surname><given-names>E.</given-names></name><name><surname>Muselet</surname><given-names>D.</given-names></name><name><surname>Tr&#x000e9;meau</surname><given-names>A.</given-names></name><name><surname>Wolf</surname><given-names>C.</given-names></name></person-group><article-title>Mixed pooling neural networks for colour constancy</article-title><source>Proceedings of the IEEE International Conference on Image Processing</source><conf-loc>Phoenix, AZ, USA</conf-loc><conf-date>25&#x02013;28 September 2016</conf-date><fpage>3997</fpage><lpage>4001</lpage></element-citation></ref><ref id="B28-sensors-19-02242"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>S.</given-names></name><name><surname>Cusano</surname><given-names>C.</given-names></name><name><surname>Schettini</surname><given-names>R.</given-names></name></person-group><article-title>Single and Multiple Illuminant Estimation Using Convolutional Neural Networks</article-title><source>IEEE Trans. Image Process.</source><year>2017</year><volume>26</volume><fpage>4347</fpage><lpage>4362</lpage><pub-id pub-id-type="doi">10.1109/TIP.2017.2713044</pub-id><pub-id pub-id-type="pmid">28600246</pub-id></element-citation></ref><ref id="B29-sensors-19-02242"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Nikkanen</surname><given-names>J.</given-names></name><name><surname>K&#x000e4;m&#x000e4;r&#x000e4;inen</surname><given-names>J.</given-names></name><name><surname>Matas</surname><given-names>J.</given-names></name></person-group><article-title>Recurrent Color Constancy</article-title><source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>5459</fpage><lpage>5467</lpage></element-citation></ref><ref id="B30-sensors-19-02242"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Riess</surname><given-names>C.</given-names></name><name><surname>Eibenberger</surname><given-names>E.</given-names></name><name><surname>Angelopoulou</surname><given-names>E.</given-names></name></person-group><article-title>Illuminant color estimation for real-world mixed-illuminant scenes</article-title><source>Proceedings of the 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>6&#x02013;13 November 2011</conf-date><fpage>782</fpage><lpage>7813</lpage></element-citation></ref><ref id="B31-sensors-19-02242"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bleier</surname><given-names>M.</given-names></name><name><surname>Riess</surname><given-names>C.</given-names></name><name><surname>Beigpour</surname><given-names>S.</given-names></name><name><surname>Eibenberger</surname><given-names>E.</given-names></name><name><surname>Angelopoulou</surname><given-names>E.</given-names></name><name><surname>Tr&#x000f6;ger</surname><given-names>T.</given-names></name><name><surname>Kaup</surname><given-names>A.M.</given-names></name></person-group><article-title>Color constancy and non-uniform illumination: Can existing algorithms work?</article-title><source>Proceedings of the 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>6&#x02013;13 November 2011</conf-date><fpage>774</fpage><lpage>781</lpage></element-citation></ref><ref id="B32-sensors-19-02242"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gijsenij</surname><given-names>A.</given-names></name><name><surname>Lu</surname><given-names>R.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name></person-group><article-title>Color Constancy for Multiple Light Sources</article-title><source>IEEE Trans. Image. Process.</source><year>2012</year><volume>21</volume><fpage>697</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1109/TIP.2011.2165219</pub-id><pub-id pub-id-type="pmid">21859624</pub-id></element-citation></ref><ref id="B33-sensors-19-02242"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beigpour</surname><given-names>S.</given-names></name><name><surname>Riess</surname><given-names>C.</given-names></name><name><surname>Van De Weijer</surname><given-names>J.</given-names></name><name><surname>Angelopoulou</surname><given-names>E.</given-names></name></person-group><article-title>Multi-Illuminant Estimation with Conditional Random Fields</article-title><source>IEEE Trans. Image Process.</source><year>2014</year><volume>23</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1109/TIP.2013.2286327</pub-id><pub-id pub-id-type="pmid">24144663</pub-id></element-citation></ref><ref id="B34-sensors-19-02242"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazin</surname><given-names>B.</given-names></name><name><surname>Delon</surname><given-names>J.</given-names></name><name><surname>Gousseau</surname><given-names>Y.</given-names></name></person-group><article-title>Estimation of Illuminants from Projections on the Planckian Locus</article-title><source>IEEE Trans. Image Process.</source><year>2013</year><volume>24</volume><fpage>11344</fpage><lpage>11355</lpage></element-citation></ref><ref id="B35-sensors-19-02242"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>S.</given-names></name><name><surname>Schettini</surname><given-names>R.</given-names></name></person-group><article-title>Adaptive Color Constancy Using Faces</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2014</year><volume>36</volume><fpage>1505</fpage><lpage>1518</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.2297710</pub-id><pub-id pub-id-type="pmid">26353334</pub-id></element-citation></ref><ref id="B36-sensors-19-02242"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ElFiky</surname><given-names>N.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Gijsenij</surname><given-names>A.</given-names></name><name><surname>Gonz&#x000e0;lez</surname><given-names>J.</given-names></name></person-group><article-title>Color Constancy Using 3D Scene Geometry Derived from a Single Image</article-title><source>IEEE Trans. Image Process.</source><year>2014</year><volume>23</volume><fpage>3855</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1109/TIP.2014.2336545</pub-id><pub-id pub-id-type="pmid">25051548</pub-id></element-citation></ref><ref id="B37-sensors-19-02242"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>D.</given-names></name><name><surname>Price</surname><given-names>B.</given-names></name><name><surname>Cohen</surname><given-names>S.</given-names></name><name><surname>Brown</surname><given-names>M.S.</given-names></name></person-group><article-title>Beyond White: Ground Truth Colors for Color Constancy Correction</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>1&#x02013;8 December 2013</conf-date><fpage>2138</fpage><lpage>2306</lpage></element-citation></ref><ref id="B38-sensors-19-02242"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>D.</given-names></name><name><surname>Kamel</surname><given-names>A.</given-names></name><name><surname>Price</surname><given-names>B.</given-names></name><name><surname>Cohen</surname><given-names>S.</given-names></name><name><surname>Brown</surname><given-names>M.S.</given-names></name></person-group><article-title>Two Illuminant Estimation and User Correction Preference</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, LV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date><fpage>469</fpage><lpage>477</lpage></element-citation></ref><ref id="B39-sensors-19-02242"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>S.B.</given-names></name><name><surname>Yang</surname><given-names>K.F.</given-names></name><name><surname>Li</surname><given-names>C.Y.</given-names></name><name><surname>Li</surname><given-names>Y.J.</given-names></name></person-group><article-title>Color Constancy Using Double-Opponency</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2013</year><volume>37</volume><fpage>11373</fpage><lpage>11385</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2396053</pub-id></element-citation></ref><ref id="B40-sensors-19-02242"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X.S.</given-names></name><name><surname>Gao</surname><given-names>S.B.</given-names></name><name><surname>Li</surname><given-names>R.X.</given-names></name><name><surname>Du</surname><given-names>X.Y.</given-names></name><name><surname>Li</surname><given-names>C.Y.</given-names></name><name><surname>Li</surname><given-names>Y.J.</given-names></name></person-group><article-title>A Retinal Mechanism Inspired Color Constancy Model</article-title><source>IEEE Trans. Image Process.</source><year>2016</year><volume>25</volume><fpage>1219</fpage><lpage>1232</lpage><pub-id pub-id-type="doi">10.1109/TIP.2016.2516953</pub-id><pub-id pub-id-type="pmid">26766375</pub-id></element-citation></ref><ref id="B41-sensors-19-02242"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akbarinia</surname><given-names>A.</given-names></name><name><surname>Parraga</surname><given-names>C.A.</given-names></name></person-group><article-title>Color Constancy Beyond the Classical Receptive Field</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2016</year><volume>14</volume><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="B42-sensors-19-02242"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>K.F.</given-names></name><name><surname>Gao</surname><given-names>S.B.</given-names></name><name><surname>Li</surname><given-names>Y.J.</given-names></name></person-group><article-title>Efficient illuminant estimation for colour constancy using grey pixels</article-title><source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>2254</fpage><lpage>2263</lpage></element-citation></ref><ref id="B43-sensors-19-02242"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joze</surname><given-names>H.R.V.</given-names></name><name><surname>Drew</surname><given-names>M.S.</given-names></name></person-group><article-title>Exemplar-Based Color Constancy and Multiple Illumination</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2014</year><volume>36</volume><fpage>860</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.169</pub-id><pub-id pub-id-type="pmid">26353222</pub-id></element-citation></ref><ref id="B44-sensors-19-02242"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Males</surname><given-names>M.</given-names></name><name><surname>Hedi</surname><given-names>A.</given-names></name><name><surname>Grgic</surname><given-names>M.</given-names></name></person-group><article-title>Color balancing using sclera color</article-title><source>IET Image Process.</source><year>2018</year><volume>12</volume><fpage>416</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1049/iet-ipr.2017.0182</pub-id></element-citation></ref><ref id="B45-sensors-19-02242"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hussain</surname><given-names>M.A.</given-names></name><name><surname>Sheikh Akbari</surname><given-names>A.</given-names></name></person-group><article-title>Colour Constancy for Image of Non-Uniformly Lit Scenes</article-title><source>Proceedings of the IEEE International Conference on Imaging Systems and Techniques (IST 2018)</source><conf-loc>Krak&#x000f3;w, Poland</conf-loc><conf-date>16&#x02013;18 October 2018</conf-date></element-citation></ref><ref id="B46-sensors-19-02242"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Arthur</surname><given-names>D.</given-names></name><name><surname>Vassilvitskii</surname><given-names>S.</given-names></name></person-group><article-title>K-means++: The Advantages of Careful Seeding</article-title><source>Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA &#x02019;07)</source><conf-loc>Louisiana, USA</conf-loc><conf-date>7&#x02013;9 January, 2007</conf-date><fpage>227</fpage><lpage>235</lpage></element-citation></ref><ref id="B47-sensors-19-02242"><label>47.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Von Kries</surname><given-names>J.</given-names></name></person-group><source>Influence of Adaptation on the Effects Produced by Luminous Stimuli. Sources of Colour Vision</source><person-group person-group-type="editor"><name><surname>MacAdam</surname><given-names>D.</given-names></name></person-group><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>1970</year><fpage>29</fpage><lpage>119</lpage></element-citation></ref><ref id="B48-sensors-19-02242"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gehler</surname><given-names>P.</given-names></name><name><surname>Rother</surname><given-names>C.</given-names></name><name><surname>Blake</surname><given-names>A.</given-names></name><name><surname>Sharp</surname><given-names>T.</given-names></name><name><surname>Minka</surname><given-names>T.</given-names></name></person-group><article-title>Bayesian colour constancy revisited</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>24&#x02013;26 June 2008</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B49-sensors-19-02242"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ciurea</surname><given-names>F.</given-names></name><name><surname>Funt</surname><given-names>B.</given-names></name></person-group><article-title>A Large Image Database for Colour Constancy Research</article-title><source>Proceedings of the Imaging Science and Technology Eleventh Colour Imaging Conference</source><conf-loc>Scottsdale, AZ, USA</conf-loc><conf-date>3 November 2003</conf-date><fpage>160</fpage><lpage>164</lpage></element-citation></ref><ref id="B50-sensors-19-02242"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tka&#x0010d;ik</surname><given-names>G.</given-names></name><name><surname>Garrigan</surname><given-names>P.</given-names></name><name><surname>Ratliff</surname><given-names>C.</given-names></name><name><surname>Mil&#x0010d;inski</surname><given-names>G.</given-names></name><name><surname>Klein</surname><given-names>J.M.</given-names></name><name><surname>Seyfarth</surname><given-names>L.H.</given-names></name><name><surname>Sterling</surname><given-names>P.</given-names></name><name><surname>Brainard</surname><given-names>D.H.</given-names></name><name><surname>Balasubramanian</surname><given-names>V.</given-names></name></person-group><article-title>Natural Images from the Birthplace of the Human Eye</article-title><source>PLoS ONE</source><year>2011</year><volume>6</volume><elocation-id>e20409</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0020409</pub-id><pub-id pub-id-type="pmid">21698187</pub-id></element-citation></ref><ref id="B51-sensors-19-02242"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hordley</surname><given-names>S.D.</given-names></name><name><surname>Finlayson</surname><given-names>G.D.</given-names></name></person-group><article-title>Re-evaluating color constancy algorithms</article-title><source>Proceedings of the 17th International Conference on Pattern Recognition, ICPR 2004</source><conf-loc>Cambridge, UK</conf-loc><conf-date>23&#x02013;26 August 2004</conf-date><fpage>76</fpage><lpage>79</lpage></element-citation></ref><ref id="B52-sensors-19-02242"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finlayson</surname><given-names>G.D.</given-names></name><name><surname>Zakizadeh</surname><given-names>R.</given-names></name><name><surname>Gijsenij</surname><given-names>A.</given-names></name></person-group><article-title>The Reproduction Angular Error for Evaluating the Performance of Illuminant Estimation Algorithms</article-title><source>IEEE Trans. Pattern. Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>1482</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2582171</pub-id><pub-id pub-id-type="pmid">27333601</pub-id></element-citation></ref><ref id="B53-sensors-19-02242"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gijsenij</surname><given-names>A.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Lucassen</surname><given-names>M.P.</given-names></name></person-group><article-title>Perceptual Analysis of Distance Measures for Colour Constancy Algorithms</article-title><source>J. Optical Soc. Am. A</source><year>2009</year><volume>26</volume><fpage>2243</fpage><lpage>2256</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.26.002243</pub-id></element-citation></ref><ref id="B54-sensors-19-02242"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>L.</given-names></name><name><surname>Xiong</surname><given-names>W.</given-names></name><name><surname>Funt</surname><given-names>B.</given-names></name></person-group><article-title>Illumination estimation via thin-plate spline interpolation</article-title><source>J. Opt. Soc. A</source><year>2011</year><volume>28</volume><fpage>940</fpage><pub-id pub-id-type="doi">10.1364/JOSAA.28.000940</pub-id></element-citation></ref><ref id="B55-sensors-19-02242"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussain</surname><given-names>M.A.</given-names></name><name><surname>Akbari</surname><given-names>A.S.</given-names></name></person-group><article-title>Color Constancy Algorithm for Mixed-Illuminant Scene Images</article-title><source>IEEE Access</source><year>2018</year><volume>6</volume><fpage>8964</fpage><lpage>8976</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2808502</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-19-02242-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>A sample image from the UPenn dataset [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>] and its four resulting segments: (<bold>a</bold>) original image, (<bold>b</bold>&#x02013;<bold>e</bold>) segment 1-4.</p></caption><graphic xlink:href="sensors-19-02242-g001"/></fig><fig id="sensors-19-02242-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Original, ground truth and its colour-balanced images using different colour correction methods: (<bold>a</bold>) Original image from the Colour Checker dataset [<xref rid="B48-sensors-19-02242" ref-type="bibr">48</xref>], (<bold>b</bold>) Ground truth image, (<bold>c</bold>) Weighted Grey Edge, (<bold>d</bold>) Corrected Moment, (<bold>e</bold>) Cheng et al. and (<bold>f</bold>) Proposed CCAFIS&#x02019; methods&#x02019; images.</p></caption><graphic xlink:href="sensors-19-02242-g002"/></fig><fig id="sensors-19-02242-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Original, ground truth and its colour-balanced images using different colour correction methods: (<bold>a</bold>) Original image from the UPenn Natural Image dataset [<xref rid="B50-sensors-19-02242" ref-type="bibr">50</xref>], (<bold>b</bold>) Ground truth, (<bold>c</bold>) Max-RGB, (<bold>d</bold>) Shades of Grey, (<bold>e</bold>) Grey Edge-1, (<bold>f</bold>) Grey Edge-2, (<bold>g</bold>) Weighted Grey Edge and (<bold>h</bold>) Proposed CCAFIS&#x02019; methods&#x02019; images.</p></caption><graphic xlink:href="sensors-19-02242-g003"/></fig><fig id="sensors-19-02242-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Original, ground truth and its colour-balanced images using different colour correction methods: (<bold>a</bold>) Original image from the Grey Ball dataset [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>], (<bold>b</bold>) Ground truth, (<bold>c</bold>) Edge-based-gamut, (<bold>d</bold>) Grey Pixel, (<bold>e</bold>) RCC-Net, (<bold>f</bold>) Proposed CCAFIS&#x02019; methods&#x02019; images.</p></caption><graphic xlink:href="sensors-19-02242-g004"/></fig><fig id="sensors-19-02242-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Original, ground truth and its colour-balanced images using different colour correction methods: (<bold>a</bold>) Original image from the Grey Ball dataset [<xref rid="B49-sensors-19-02242" ref-type="bibr">49</xref>], (<bold>b</bold>) Ground truth, (<bold>c</bold>) Edge-based-gamut, (<bold>d</bold>) Grey Pixel, (<bold>e</bold>) RCC-Net, (<bold>f</bold>) Proposed CCAFIS&#x02019; methods&#x02019; images.</p></caption><graphic xlink:href="sensors-19-02242-g005"/></fig><fig id="sensors-19-02242-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Original, ground truth and its colour-balanced images using different colour correction methods: (<bold>a</bold>) Original image from the MLS dataset [<xref rid="B32-sensors-19-02242" ref-type="bibr">32</xref>], (<bold>b</bold>) Ground truth image, (<bold>c</bold>) Grey Edge-2, (<bold>d</bold>) Weighted Grey Edge, (<bold>e</bold>) Gisenji et al. and (<bold>f</bold>) Proposed CCAFIS methods&#x02019; images.</p></caption><graphic xlink:href="sensors-19-02242-g006"/></fig><table-wrap id="sensors-19-02242-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t001_Table 1</object-id><label>Table 1</label><caption><p>Normalised Average Absolute Difference (NAAD) values of different colour components of the image segments.</p></caption><graphic xlink:href="sensors-19-02242-i001"/></table-wrap><table-wrap id="sensors-19-02242-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t002_Table 2</object-id><label>Table 2</label><caption><p>Mean Opinion Score (MOS) of the proposed and the state-of-the-art techniques.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Dataset (Number of Images)</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Method</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WGE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gisenji et al.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIRF</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cheng et al.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed CCAFIS</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLS (9 outdoor)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIMO (78)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.24</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey Ball (200)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Colour Checker (100)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.29</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UPenn (57)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.12</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-02242-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t003_Table 3</object-id><label>Table 3</label><caption><p>Average mean and median recovery and reproduction angular errors of colour constancy methods&#x02019; images of the Grey Ball dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Recovery Error</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Reproduction Error</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th></tr></thead><tbody><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Statistics-based methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray World</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.5&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.7&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.5&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shades of Gray</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.9&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Edge-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.7&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.6&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Edge-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.6&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed CCAFIS </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.6&#x000b0;</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Learning-based methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Exemplar-based </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.4&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.4&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.7&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Pixel (std)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.6&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep Learning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.7&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Natural Image Statistics</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.5&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.3&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral Statistics</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.9&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.7&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.3&#x000b0;</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-02242-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t004_Table 4</object-id><label>Table 4</label><caption><p>Average mean and median recovery angular errors of colour constancy methods&#x02019; images of the colour checker dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Recovery Error</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Reproduction Error</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th></tr></thead><tbody><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Statistics-based methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray World</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.4&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mn>7.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:mn>6.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:mn>8.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mn>6.5</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shades of Gray</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mn>5.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mn>4.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Edge-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.5&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:mn>6.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.9<inline-formula><mml:math id="mm57"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Edge-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mn>6.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mn>4.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed CCAFIS </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mn>3.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mn>2.7</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:mn>4.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mn>2.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Learning-based methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Exemplar-based </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:mn>2.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:mn>3.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.6<inline-formula><mml:math id="mm67"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Pixel (std)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm68"><mml:mrow><mml:mrow><mml:mn>3.2</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.7<inline-formula><mml:math id="mm69"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mn>3.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mn>2.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mn>4.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:mn>3.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AAC </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:mn>2.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:mn>3.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HLVI BU </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm77"><mml:mrow><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HLVI BU &#x00026; TD </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm78"><mml:mrow><mml:mrow><mml:mn>2.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCDL </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mn>3.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mn>2.2</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mn>2.7</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BDP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm84"><mml:mrow><mml:mrow><mml:mn>2.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm85"><mml:mrow><mml:mrow><mml:mn>3.5</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CM </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm86"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm87"><mml:mrow><mml:mrow><mml:mn>2.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FB+GM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm88"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm89"><mml:mrow><mml:mrow><mml:mn>3.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PCL</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm90"><mml:mrow><mml:mrow><mml:mn>1.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm91"><mml:mrow><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm92"><mml:mrow><mml:mrow><mml:mn>1.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm93"><mml:mrow><mml:mrow><mml:mn>2.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm95"><mml:mrow><mml:mrow><mml:mn>2.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCC </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:mn>1.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AlexNet+SVR </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm98"><mml:mrow><mml:mrow><mml:mn>3.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm99"><mml:mrow><mml:mrow><mml:mn>4.7</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-Per patch</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm100"><mml:mrow><mml:mrow><mml:mn>2.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm101"><mml:mrow><mml:mrow><mml:mn>3.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN average--pooling</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm102"><mml:mrow><mml:mrow><mml:mn>2.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm103"><mml:mrow><mml:mrow><mml:mn>3.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN median-pooling</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm104"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm105"><mml:mrow><mml:mrow><mml:mn>3.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN fine-tuned</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm106"><mml:mrow><mml:mrow><mml:mn>1.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm107"><mml:mrow><mml:mrow><mml:mn>2.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN + SVR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm108"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm109"><mml:mrow><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-02242-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t005_Table 5</object-id><label>Table 5</label><caption><p>Mean and median recovery angular error of various methods on images of the MIMO dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MIMO (real)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MIMO (lab)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th></tr></thead><tbody><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Statistical methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey world </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.9&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.6&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.8&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.6&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey Edge-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey Edge-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.7&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.9&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIRF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.6&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.6&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey Pixel</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.7&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.5&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed CCAFIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.3&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.1&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm112"><mml:mrow><mml:mrow><mml:mn>2.7</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Learning-based methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLS + GW</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm113"><mml:mrow><mml:mrow><mml:mn>4.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.3<inline-formula><mml:math id="mm114"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm115"><mml:mrow><mml:mrow><mml:mn>6.4</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm116"><mml:mrow><mml:mrow><mml:mn>5.9</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLS + WP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm117"><mml:mrow><mml:mrow><mml:mn>4.2</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm118"><mml:mrow><mml:mrow><mml:mn>3.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm119"><mml:mrow><mml:mrow><mml:mn>5.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2<inline-formula><mml:math id="mm120"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIRF + GW</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm121"><mml:mrow><mml:mrow><mml:mn>3.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm122"><mml:mrow><mml:mrow><mml:mn>2.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm123"><mml:mrow><mml:mrow><mml:mn>3.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm124"><mml:mrow><mml:mrow><mml:mn>2.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIRF + WP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm125"><mml:mrow><mml:mrow><mml:mn>4.1</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm126"><mml:mrow><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm127"><mml:mrow><mml:mrow><mml:mn>3.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm128"><mml:mrow><mml:mrow><mml:mn>2.8</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIRF + IEbV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm129"><mml:mrow><mml:mrow><mml:mn>5.6</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm130"><mml:mrow><mml:mrow><mml:mn>4.3</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.5<inline-formula><mml:math id="mm131"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm132"><mml:mrow><mml:mrow><mml:mn>3.0</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-02242-t006" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t006_Table 6</object-id><label>Table 6</label><caption><p>Median recovery angular errors for 9 outdoor images of the multiple light source dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Median Error</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.8&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey World</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.9&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey Edge-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.4&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grey Edge-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gisenji et al.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.1&#x000b0;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed CCAFIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.6&#x000b0;</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-02242-t007" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-02242-t007_Table 7</object-id><label>Table 7</label><caption><p>Cumulative execution time for the first 100 images of the Colour Checker benchmark image dataset for the proposed CCAFIS, statistical-based and learning-based colour constancy adjustment techniques.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Time (s)</th></tr></thead><tbody><tr><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Statistics-Based Methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray World</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.01</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max-RGB (White Patch)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.21</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Edge-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Edge-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.21</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed CCAFIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.37</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Learning-Nased Methods</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Exemplar-based </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2827</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray Pixel (std)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1165</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2500</td></tr></tbody></table></table-wrap></floats-group></article>