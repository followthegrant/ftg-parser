<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Hum Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Hum Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Hum. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Human Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5161</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30792634</article-id><article-id pub-id-type="pmc">6374327</article-id><article-id pub-id-type="doi">10.3389/fnhum.2019.00029</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Gender Identification of Human Cortical 3-D Morphology Using Hierarchical Sparsity</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Zhiguo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/595716/overview"/></contrib><contrib contrib-type="author"><name><surname>Hou</surname><given-names>Chenping</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Lubin</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/210245/overview"/></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Dewen</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="c001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/42723/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>College of Mechatronics and Automation, National University of Defense Technology</institution>, <addr-line>Changsha</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>College of Science, National University of Defense Technology</institution>, <addr-line>Changsha</addr-line>, <country>China</country></aff><aff id="aff3"><sup>3</sup><institution>Cognitive and Mental Health Research Center, Beijing Institute of Basic Medical Science</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Meng-Chuan Lai, University of Toronto, Canada</p></fn><fn fn-type="edited-by"><p>Reviewed by: Miao Cao, Fudan University, China; Erin W. Dickie, Centre for Addiction and Mental Health (CAMH), Canada</p></fn><corresp id="c001">*Correspondence: Dewen Hu <email>dwhu@nudt.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>2</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>13</volume><elocation-id>29</elocation-id><history><date date-type="received"><day>02</day><month>8</month><year>2018</year></date><date date-type="accepted"><day>21</day><month>1</month><year>2019</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2019 Luo, Hou, Wang and Hu.</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Luo, Hou, Wang and Hu</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Difference exists widely in cognition, behavior and psychopathology between males and females, while the underlying neurobiology is still unclear. As brain structure is the fundament of its function, getting insight into structural brain may help us to better understand the functional mechanism of gender difference. Previous structural studies of gender difference in Magnetic Resonance Imaging (MRI) usually focused on gray matter (GM) concentration and structural connectivity (SC), leaving cortical morphology not characterized properly. In this study a large dataset is used to explore whether cortical three-dimensional (3-D) morphology can offer enough discriminative morphological features to effectively identify gender. Data of all available healthy controls (<italic>N</italic> = 1113) from the Human Connectome Project (HCP) were utilized. We suggested a multivariate pattern analysis method called Hierarchical Sparse Representation Classifier (HSRC) and got an accuracy of 96.77% for gender identification. Permutation tests were used to testify the reliability of gender discrimination (<italic>p</italic> &#x0003c; 0.001). Cortical 3-D morphological features within the frontal lobe were found the most important contributors to gender difference of human brain morphology. Moreover, we investigated gender discriminative ability of cortical 3-D morphology in predefined Anatomical Automatic Labeling (AAL) and Resting-State Networks (RSN) templates, and found the superior frontal gyrus the most discriminative in AAL and the default mode network the most discriminative in RSN. Gender difference of surface-based morphology was also discussed. The frontal lobe, as well as the default mode network, was widely reported of gender difference in previous structural and functional MRI studies, which suggested that morphology indeed affect human brain function. Our study indicates that gender can be identified on individual level by using cortical 3-D morphology and offers a new approach for structural MRI research, as well as highlights the importance of gender balance in brain imaging studies.</p></abstract><kwd-group><kwd>cortical three-dimensional morphology</kwd><kwd>gender difference</kwd><kwd>hierarchical sparse representation classifier</kwd><kwd>Magnetic Resonance Imaging</kwd><kwd>multivariate pattern analysis</kwd></kwd-group><counts><fig-count count="6"/><table-count count="4"/><equation-count count="12"/><ref-count count="62"/><page-count count="11"/><word-count count="7492"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>1. Introduction</title><p>Gender difference has been widely reported in psychiatric and neurological diseases (Piccinelli and Wilkinson, <xref rid="B38" ref-type="bibr">2000</xref>; Baron-Cohen et al., <xref rid="B6" ref-type="bibr">2005</xref>; Shulman, <xref rid="B48" ref-type="bibr">2007</xref>; Eranti et al., <xref rid="B13" ref-type="bibr">2013</xref>; Lai et al., <xref rid="B29" ref-type="bibr">2015</xref>), cognitive functions (Ren et al., <xref rid="B39" ref-type="bibr">2009</xref>; Ohla and Lundstr, <xref rid="B37" ref-type="bibr">2013</xref>; Yin et al., <xref rid="B58" ref-type="bibr">2017</xref>; Chen et al., <xref rid="B11" ref-type="bibr">2018</xref>) and behaviors (Christov-Moore et al., <xref rid="B12" ref-type="bibr">2014</xref>), while its neurobiological mechanism is unclear yet (Giudice, <xref rid="B18" ref-type="bibr">2009</xref>). As neural function has its structural basis, studying brain neuroanatomy may provide us new insights and understandings of gender difference.</p><p>Previous reports tend to explain gender difference in the view of GM concentration, SC and Functional Connectivity (FC). Wang et al. (<xref rid="B55" ref-type="bibr">2012</xref>) applied multivariate pattern analysis on GM concentration and resting state fMRI from healthy young adults and got an accuracy of 89%, and they found the occipital lobe and the cerebellum the most discriminative regions of gender difference; Yuan et al. (<xref rid="B59" ref-type="bibr">2018a</xref>) proposed a three-dimensional weighted histogram of gradient orientation to describe the complex spatial structure of human brain image, and they got an over 90% accuracy of gender classification on 527 healthy adults from four research sites; Ruigrok et al. (<xref rid="B44" ref-type="bibr">2014</xref>) reported gender difference in the amygdala, hippocampus, and insula after meta-analysis in human brain structure; Goldstein et al. (<xref rid="B20" ref-type="bibr">2001</xref>) found females had higher percentage of GM than males, while Gur et al. (<xref rid="B22" ref-type="bibr">1999</xref>) got a converse result in white matter; Feis et al. (<xref rid="B15" ref-type="bibr">2013</xref>) used multimodal gender classification of T1-weighted, T2-weighted and fractional anisotropy images and indicated the frontal lobe the most discriminative lobe. Gong et al. (<xref rid="B21" ref-type="bibr">2009</xref>) found greater overall cortical connectivity and more efficient cortical network organizations in women; Ingalhalikar et al. (<xref rid="B25" ref-type="bibr">2013</xref>) reported that males had stronger intra-hemispheric SC while females had stronger inter-hemispheric SC using diffusion tensor imaging. Zhang et al. (<xref rid="B62" ref-type="bibr">2018</xref>) used 4 fMRI runs of 820 healthy controls from the HCP and got the accuracy of 87% using FC features for gender prediction, and they suggested that FC within the default, fronto-parietal and sensorimotor networks had the greatest gender prediction abilities while the right fusiform gyrus and the right ventromedial prefrontal cortex contributed the most in the default mode network.</p><p>Recently, gender difference in surface-based morphology such as cortical thickness, surface area, cortical curvature and cortical volume has attracted much attention. Im et al. (<xref rid="B24" ref-type="bibr">2006</xref>) indicated that women showed more significant localized cortical thickening in the frontal, parietal and occipital lobes, which were also reported of significant gender-related difference by Lv et al. (<xref rid="B33" ref-type="bibr">2010</xref>) using graph theoretical approaches; Sowell et al. (<xref rid="B49" ref-type="bibr">2007</xref>) found women had thicker cortices in posterior temporal and right inferior parietal regions, while men showed larger brain in all locations, especially in the frontal and occipital poles of both hemispheres; Sepehrband et al. (<xref rid="B47" ref-type="bibr">2018</xref>) developed a multivariate statistical learning model to predict gender from regional neuroanatomical features on different brain atlases, and they got an 83% cross-validated prediction accuracy and found the middle occipital lobes and the angular gyri the major predictors of gender.</p><p>Despite studies of gender difference in surface-based morphology, few paid attention to the original cortical 3-D morphology, which is defined as the voxel-based morphology of the cerebral cortex without gray matter concentration in the standard MNI space. Clearly the original cortical 3-D morphology contains more abundant and complete morphological information, and most surface-based morphology such as cortical thickness and curvature are measured on the cortical 3-D morphology (cortical volume and surface area are measured in the subject's undistorted native volume space). Moreover, most previous morphology studies focused on finding gender difference using statistical analysis while few of them have effectively discriminated males from females with high classification accuracy using those morphological features to support their conclusions.</p><p>In this study, we aimed to find gender difference of cortical 3-D morphology and focused on two questions: (a) Can gender be discriminated with a high accuracy using cortical 3-D morphology? (b) What is the most discriminative region of gender in cortical 3-D morphology?</p></sec><sec sec-type="materials and methods" id="s2"><title>2. Materials and Methods</title><sec><title>2.1. Data Acquisition and Preprocessing</title><p>Structural MRI was acquired from the HCP S1200 release, and details about the HCP can be seen in Essen et al. (<xref rid="B14" ref-type="bibr">2012</xref>). Subjects were scanned on a customized 3T Siemens scanner (Connectome Skyra) with a standard 32-channel head coil and a body transmission coil and scan parameters were as follows: TR = 2400 ms, TE = 2.14 ms, Voxel Size = 0.7 mm isotropic. All 1113 available subjects (age: 22&#x02013;37 years, gender: 507 males and 606 females) were selected for our gender difference study.</p><p>Data were initially preprocessed by the HCP structural pipelines in this study, and a highlight of the HCP pipelines is that it uses T2-weighted structural images for registration so as to get more precise registration and segmentation results. The main preprocessing steps include gradient distortion correction, brain extracting, readout distortion correction, boundarybased cross-modal registration, bias field correction, recon-all pipeline in FreeSurfer, and native to MNI nonlinear volume transformation, and detailed preprocessing steps can be seen in Glasser et al. (<xref rid="B19" ref-type="bibr">2013</xref>). One of the outputs, the wmparc, is an accurate subject-specific human brain mask of the gray matter and white matter in the MNI space. In the file &#x0201c;MNINonLinear/wmparc.nii&#x0201d; of each subject of the HCP, the scattered integers between 251 and 2035 stand for different subregions of the cerebral cortex, and when they were defined as 1 and others as 0, the original 3-D morphology of the cerebral cortex were obtained (<xref ref-type="fig" rid="F1">Figure 1A</xref>). We also attempted to analyse the discriminative abilities of both anatomical and functional subregions, so atlas-based morphology analysis (Meyer et al., <xref rid="B35" ref-type="bibr">2017</xref>) was conducted with two predefined atlas: the AAL template (Tzourio-Mazoyer et al., <xref rid="B54" ref-type="bibr">2002</xref>) was used as structural atlas and the 7 RSN template (Thomas Yeo et al., <xref rid="B52" ref-type="bibr">2011</xref>) was used as functional atlas (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation_Yeo2011">https://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation_Yeo2011</ext-link>, &#x0201c;Yeo2011_7Networks_MNI152_FreeSurfer-Conformed1mm_LiberalMask.nii,&#x0201d; downsampled to 1.4 mm isotropic). All the MRI files and templates were in the standard MNI space for comparisons across subjects.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p>Framework of gender identification of cortical 3-D morphology via HSRC. <bold>(A)</bold> Process of cortical 3-D morphology extraction. For each subject, T1w and T2w were used in the HCP structural pipelines to generate a normalized volume parcellation&#x02014;the wmparc, which is an accurate subject-specific human brain mask of the gray matter and white matter in the MNI space. We defined the value of the gray matter voxels as 1 and others as 0, and got the original cortical 3-D morphology. <bold>(B)</bold> Gender classification with cortical 3-D morphology using HSRC. The original cortical 3-D morphology (0.7 mm) of each subject was first downsampled into 1.4 and 2.8 mm, then gender classification was conducted on the 2.8 mm 3-D morphology with 10-fold cross validation, RFS was used on the training data to select voxels in each fold. We set the overall classification accuracy as a function of the number of selected voxels in each fold, and selected the union of the selected voxels in each fold corresponding to the highest accuracy as discriminative voxels, the corresponding voxels in 1.4 mm morphology were selected as the initial input for the next 10-fold across validation. The same operation was conducted in 0.7 mm data.</p></caption><graphic xlink:href="fnhum-13-00029-g0001"/></fig><p>As surface-based morphology was discussed in this study, we obtained 4 surface-based morphological features (thickness, curvature, sulc and myelinmap) in the HCP for gender difference analysis. They were all spatially downsampled to a ~32k mesh of each hemisphere (average vertex spacing of ~2 mm).</p></sec><sec><title>2.2. Hierarchical Sparsity Feature Selection</title><p>Considering the scale of the dataset in this study, a 10-fold cross validation was conducted for gender classification, and in consideration of numerous features of MRI data (dimensionality=1,113 &#x000d7; 4,352,560 after abandon all-0 and all-1 columns for 0.7 mm data matrix), dimensionality reduction is essential to alleviate or avoid the curse of dimensionality (Liu and Motoda, <xref rid="B31" ref-type="bibr">1998</xref>).</p><p>Feature extraction algorithms like Principal Component Analysis (PCA) combine all features to create new dimensionality reduced features in a new feature space, and general statistical tests like <italic>t</italic>-test are unsuitable to filter 0-1 distributed features. Comparatively, sparse representations select typical features from the original feature space directly, so that we can maintain the original physical meanings of the cortical morphological features and have a better explanation.</p><p>Since sparse representation is not good at dealing with data with too large dimensionality (Su et al., <xref rid="B50" ref-type="bibr">2012</xref>), we proposed a Hierarchical Sparse Representation Classifier (HSRC) algorithm for informative feature selection and classification (<xref ref-type="fig" rid="F1">Figure 1B</xref>). MRI data were downsampled to voxel size=1.4 mm isotropic (feature dimensionality=544,069 after abandon all-0 and all-1 columns) and voxel size=2.8 mm isotropic (feature dimensionality=67,994 after abandon all-0 and all-1 columns). The 10-fold cross-validation classification was first conducted in 2.8 mm data. In each fold, we aligned all the 67,994 features of the training set using sparse representation and empirically select the first 10,000 features in 200 intervals, and thus we had 50 (10,000/200) classification results in each fold. The overall classification accuracy was the average accuracy of classification with the same number of training data features across folds, and when the highest overall classification accuracy was got, the union of the selected features in each fold were regarded as the most discriminative features of 2.8 mm data. The corresponding 1.4 mm features of all the selected features in 2.8 mm data were defined as the original features (8 times the dimensionality of the selected 2.8 mm features) for the next sparse representation operation. The same operation was conducted in 1.4&#x02013;0.7 mm data.</p><p>Given training data <inline-formula><mml:math id="M1"><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mspace width="0.3em" class="thinspace"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211d;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and the associated class labels <bold><italic>y</italic></bold> &#x02208; &#x0211d;<sup><italic>n</italic></sup>, the sparse representation algorithm can be modeled as follows:</p><disp-formula id="E1"><label>(1)</label><mml:math id="M2"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <bold><italic>w</italic></bold> &#x02208; &#x0211d;<sup><italic>d</italic></sup> is the weight vector to be solved and it should be as sparse as possible. It can be described as the following optimization problem:</p><disp-formula id="E2"><label>(2)</label><mml:math id="M3"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mo class="qopname">min</mml:mo><mml:mo>&#x02225;</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>it is a &#x02113;<sub>0</sub>-norm problem which is difficult to get the solution although the solution is the most desirable to Equation 1.</p><p>Under practical conditions, the &#x02113;<sub>0</sub>-norm problem is equivalent or approximately equivalent to the &#x02113;<sub>1</sub>-norm problem. It is convex and thus can be easily optimized. Besides, the utility of &#x02113;<sub>1</sub>-norm makes <bold><italic>w</italic></bold> less sensitive to noise. Consequently, we can get <bold><italic>w</italic></bold> by solving the following problem:</p><disp-formula id="E3"><label>(3)</label><mml:math id="M4"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mo class="qopname">min</mml:mo><mml:mo>&#x02225;</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>considering that the constraint condition <bold><italic>X</italic></bold><sup><italic>T</italic></sup><bold><italic>w</italic></bold> = <bold><italic>y</italic></bold> makes <bold><italic>w</italic></bold> sensitive to outliers of <bold><italic>X</italic></bold>, we suggested a new equation:</p><disp-formula id="E4"><label>(4)</label><mml:math id="M5"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:munder></mml:mstyle><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>-</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x02225;</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>thus we can get the approximate solution of Equation 1, and make sparse representation more robust.</p><p>We find Equation 4 is a specific form of the Robust Feature Selection (RFS) algorithm proposed by Nie et al. (<xref rid="B36" ref-type="bibr">2010</xref>). The RFS is based on regression and &#x02113;<sub>2, 1</sub>-norm sparsity regularization. Unlike the traditional least square regression which uses the squared &#x02113;<sub>2</sub>-norm loss, RFS emphasizes joint &#x02113;<sub>2, 1</sub>-norm minimization on both loss function and regularization. Before introducing RFS method, we first present the definition of the &#x02113;<sub>2, 1</sub>-norm of a matrix.</p><p>For the matrix <bold><italic>M</italic></bold> &#x02208; <italic>R</italic><sup><italic>n</italic>&#x000d7;<italic>m</italic></sup>, its &#x02113;<sub>2, 1</sub>-norm is defined as:</p><disp-formula id="E5"><label>(5)</label><mml:math id="M6"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mo>&#x02225;</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>m</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <bold><italic>m</italic></bold><sup><italic>i</italic></sup> is the <italic>i</italic>-th row of <bold><italic>M</italic></bold>.</p><p>Given training data <inline-formula><mml:math id="M7"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mspace width="0.3em" class="thinspace"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211d;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, the RFS algorithm employs the one-vs-rest binary coding scheme to encode the class labels. Denote the total number of classes as <italic>c</italic>. The label vector of training data <bold><italic>x</italic></bold><sub><italic>i</italic></sub> is represented by <inline-formula><mml:math id="M8"><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, such that <italic>y</italic><sub><italic>i</italic></sub>(<italic>j</italic>) = 1 if <bold><italic>x</italic></bold><sub><italic>i</italic></sub> belongs to the <italic>j</italic>-th category and <italic>y</italic><sub><italic>i</italic></sub>(<italic>j</italic>) = 0 otherwise. The associated class labels of all data points are <inline-formula><mml:math id="M9"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mspace width="0.3em" class="thinspace"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211d;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. RFS optimizes the following robust loss function:</p><disp-formula id="E6"><label>(6)</label><mml:math id="M10"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>b</mml:mi></mml:mstyle><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <bold><italic>W</italic></bold> &#x02208; &#x0211d;<sup><italic>d</italic>&#x000d7;<italic>c</italic></sup> is the projection matrix and <bold><italic>b</italic></bold> &#x02208; &#x0211d;<sup><italic>c</italic></sup> is the bias vector.</p><p>For simplicity, the bias <bold><italic>b</italic></bold> can be absorbed into <bold><italic>W</italic></bold> when the constant value 1 is added as an additional dimension for each data <bold><italic>x</italic></bold><sub><italic>i</italic></sub>(1 &#x02264; <italic>i</italic> &#x02264; <italic>n</italic>). Thus, the problem becomes:</p><disp-formula id="E7"><label>(7)</label><mml:math id="M11"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>For the sake of feature selection, we will add a sparse regularizer. Essentially, the <italic>i</italic>-row vector of <bold><italic>W</italic></bold> corresponds to the transformation vector of the <italic>i</italic>-th feature in regression. It can also be regarded as a vector that measures the importance of the <italic>i</italic>-th feature. Considering the task of feature selection, we expect that the transformation matrix holds the sparsity property for feature selection. More concretely, we expect that only a small number of row vectors of <bold><italic>W</italic></bold> are non-zeros. As a result, the corresponding features are selected since these features are enough to regress the original data <bold><italic>x</italic></bold><sub><italic>i</italic></sub> to its label vector <bold><italic>y</italic></bold><sub><italic>i</italic></sub>. When we employ the &#x02113;<sub>2</sub>-norm of each row vector as a metrix to measure its contribution in this regression, the sparsity property, i.e., a small number of row vectors that are non-zeros, indicates the following RFS objective function:</p><disp-formula id="E8"><label>(8)</label><mml:math id="M12"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <bold><italic>w</italic></bold><sup><italic>i</italic></sup> denotes the <italic>i</italic>-th row of <bold><italic>W</italic></bold>. The parameter &#x003b3; is to balance the regression loss and the influence of sparse regularizer, and it was set to be the default value 0.01 suggested by Nie et al. (<xref rid="B36" ref-type="bibr">2010</xref>) through a series of empirical studies.</p><p>Denote data matrix <inline-formula><mml:math id="M13"><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mspace width="0.3em" class="thinspace"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211d;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and label matrix <inline-formula><mml:math id="M14"><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mspace width="0.3em" class="thinspace"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211d;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, the objective function becomes:</p><disp-formula id="E9"><label>(9)</label><mml:math id="M15"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle></mml:munder><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The &#x02113;<sub>2, 1</sub>-norm based loss function makes RFS robust to outliers in data points and the &#x02113;<sub>2, 1</sub>-norm regularization enables RFS to select features across all data points with joint sparsity. Though both terms of the objective function are non-smooth, the problem can be solved efficiently with the reweighted method, which has been proved to be convergent. More details about the RFS algorithm can be seen in Nie et al. (<xref rid="B36" ref-type="bibr">2010</xref>).</p><p>After obtaining the solution of <bold><italic>W</italic></bold>, features are ranked according to the value of <inline-formula><mml:math id="M16"><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. In other words, the larger value of <inline-formula><mml:math id="M17"><mml:mo>&#x02225;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> denotes that the <italic>i</italic>-th feature are more important. The features with less importance are then discarded.</p></sec><sec><title>2.3. Classification and Cross Validation</title><p>In each of the 10-fold cross validation, 90% samples were regarded as the training set and the remaining 10% samples were served as the testing set. The classifier used in this study was linear support vector machine (SVM), whose goal is to find a decision function:</p><disp-formula id="E10"><label>(10)</label><mml:math id="M18"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:msup><mml:mi>h</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>x</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula><p>by solving the following optimization problem:</p><disp-formula id="E11"><label>(11)</label><mml:math id="M19"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>h</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>&#x003b5;</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>h</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>s</mml:mtext><mml:mo>.</mml:mo><mml:mtext>t</mml:mtext><mml:mo>.</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:msup><mml:mi>h</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <bold><italic>h</italic></bold> denotes the normal of the hyperplane, <bold><italic>x</italic></bold><sub><italic>i</italic></sub> denotes the <italic>i</italic>-th training vector and <italic>y</italic><sub><italic>i</italic></sub> is its corresponding lebel, &#x003be;<sub><italic>i</italic></sub> is the misclassification errors of non-separable cases, and C is the empirical risk and model complexity which was set to be 1 in this study. Females were labeled as -1 and males were labeled as 1, and thus the classification threshold was 0. The classification accuracy and the area under curve (AUC) of the receiver operating characteristic (ROC) curve were used as the classification performance index, and 1,000 times of permutation tests and 1,000 times of bootstrap tests were conducted to access the overall statistical significance of the classification results. In the permutation test of each fold, gender labels were randomly permuted when gender features kept stable, and 1,000 AUC values were used to construct a null distribution and compare with AUC value of using true gender labels. In each bootstrap test, 90% of the training set were randomly chosen as new training set, and inspired by the back projection stage of Wang et al. (<xref rid="B55" ref-type="bibr">2012</xref>), the weight of voxels was defined as the absolute of <bold><italic>h</italic></bold>, and detailed equation was as follows:</p><disp-formula id="E12"><label>(12)</label><mml:math id="M20"><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>g</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>h</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mtext>abs</mml:mtext><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula><p>where <bold><italic>g</italic></bold> denotes the weight vector of voxels, &#x003b1;<sub><italic>i</italic></sub> is the <italic>i</italic>-th value of alpha coefficient vector <bold><italic>&#x003b1;</italic></bold> in SVM, and <italic>N</italic> is the number of subjects in the training set. The mean of <bold><italic>g</italic></bold> in 1,000 times of bootstrap tests was the final weight vector <inline-formula><mml:math id="M21"><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mover accent="false" class="mml-overline"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo accent="true">&#x000af;</mml:mo></mml:mover></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec sec-type="results" id="s3"><title>3. Results</title><sec><title>3.1. Gender Classification Results: AUC and Accuracy</title><p>Results of gender classification using HSRC of three resolutions are provided in the top two rows of <xref rid="T1" ref-type="table">Table 1</xref>. The highest AUC and accuracy, both of which are got from 0.7 mm data, are 0.9925 and 96.77%, respectively. The relationship of classification accuracy and the number of selected features in each fold are provided in <xref ref-type="fig" rid="F2">Figure 2B</xref>, which indicates that the classification accuracy of all the three resolutions improves rapidly up to 0.9 with a few voxels and with the same number of voxels, the higher resolution data always have higher classification accuracies with much less computation time (platform: Linux server with 2 Inter(R) Xeon(R) CUP @ 2.10 GHz, 28 kernels, 260 GiB Memory. CentOS 6.7, MATLAB R2015b, 1 fold RFS: 151.3 (0.7 mm) +158.8 (1.4 mm) +64.3 (2.8 mm) = 374.4 s for HSRC; 5682.6 s (0.7 mm) for direct sparsity) and storage demanded, but when direct sparsity is conducted in different resolution data, we do not see improvement of overall classification performance in higher resolution data, which proves that our HSRC algorithm indeed plays a part. The outcomes of conducting direct sparsity in different resolution data are in the median two rows of <xref rid="T1" ref-type="table">Table 1</xref> and <xref ref-type="fig" rid="F2">Figure 2A</xref>.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>AUC and accuracy for gender classification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>0.7 mm</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>1.4 mm</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>2.8 mm</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">HSRC</td><td valign="top" align="center" rowspan="1" colspan="1">AUC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9925</td><td valign="top" align="center" rowspan="1" colspan="1">0.9868</td><td valign="top" align="center" rowspan="1" colspan="1">0.9821</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">96.77</td><td valign="top" align="center" rowspan="1" colspan="1">95.69</td><td valign="top" align="center" rowspan="1" colspan="1">94.49</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Direct sparsity</td><td valign="top" align="center" rowspan="1" colspan="1">AUC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9829</td><td valign="top" align="center" rowspan="1" colspan="1">0.9831</td><td valign="top" align="center" rowspan="1" colspan="1">0.9821</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">94.34</td><td valign="top" align="center" rowspan="1" colspan="1">94.70</td><td valign="top" align="center" rowspan="1" colspan="1">94.49</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PCA</td><td valign="top" align="center" rowspan="1" colspan="1">AUC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9874</td><td valign="top" align="center" rowspan="1" colspan="1">0.9870</td><td valign="top" align="center" rowspan="1" colspan="1">0.9844</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">94.43</td><td valign="top" align="center" rowspan="1" colspan="1">94.52</td><td valign="top" align="center" rowspan="1" colspan="1">94.07</td></tr></tbody></table></table-wrap><fig id="F2" position="float"><label>Figure 2</label><caption><p>Classification results of the sparse representation, and the classification was a function of the number of voxels selected in each fold. In HSRC, the higher resolution data always have the higher classification accuracy, while in direct sparsity the classification accuracys of three resolution data are roughly the same. The highest accuracy is 96.77% which is got from 0.7 mm data using HSRC.</p></caption><graphic xlink:href="fnhum-13-00029-g0002"/></fig><p>Gender classification using PCA was also conducted for comparing, and results are provided in the bottom two rows of <xref rid="T1" ref-type="table">Table 1</xref>, the classification performance of using PCA is comparable with using direct sparsity, but poor than using HSRC.</p><p>We conducted 1,000 times of permutation tests to testify the statistical significance of overall gender classification performance, and detailed results for all three resolution data are in <xref ref-type="fig" rid="F3">Figure 3</xref>. Concurring with expectations, null distributions of the AUCs scattered around 0.5, which implied that the performance of the classifier for the randomly permuted data sets whose subjects were randomly labeled was just no better than the probability of getting positive side in random coin tossing. All of the AUC values for permuted labels fell behind the AUCs of real labels, which demonstrated high statistical significance of gender classification (<italic>p</italic> &#x0003c; 0.001) for all three resolutions.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Permutation tests of AUC index for gender classification. <bold>(A)</bold> 0.7 mm; <bold>(B)</bold> 1.4 mm; <bold>(C)</bold> 2.8 mm. The light blue histograms indicates the null distributions of AUC for randomly permuted gender labels and the solid red line show the AUC when gender labels were true.</p></caption><graphic xlink:href="fnhum-13-00029-g0003"/></fig></sec><sec><title>3.2. Important 3-D Morphological Features in Gender Discrimination</title><p>As the best classification performance was obtained from 0.7 mm data, and other resolution data were downsampled from them, we conducted 1,000 times of bootstrap tests in 0.7 mm data, and the outcome is shown in <xref ref-type="fig" rid="F4">Figure 4</xref> and detailed information of the main clusters is in <xref rid="T2" ref-type="table">Table 2</xref>.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p>Surface rendering of discriminative regions of gender difference derived from normalized mean bootstrap result [visualized by BrainNet Viewer Xia et al., <xref rid="B57" ref-type="bibr">2013</xref>]. The main morphology difference for gender exists mainly in the Frontal Lobe and the Limbic Lobe, others scattered in the Parietal Lobe, the Temporal Lobe, the Corpus Callosum, and the Precuneus.</p></caption><graphic xlink:href="fnhum-13-00029-g0004"/></fig><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>The main locations of the voxels that were selected by HSRC in 0.7 mm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Cluster</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Voxels</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Hemisphere</bold></th><th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1"><bold>MNI Coordinate</bold></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>x</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>y</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>z</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Uncus</td><td valign="top" align="center" rowspan="1" colspan="1">120</td><td valign="top" align="center" rowspan="1" colspan="1">L</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;25.5</td><td valign="top" align="center" rowspan="1" colspan="1">2.1</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;21.6</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Uncus</td><td valign="top" align="center" rowspan="1" colspan="1">143</td><td valign="top" align="center" rowspan="1" colspan="1">R</td><td valign="top" align="center" rowspan="1" colspan="1">20</td><td valign="top" align="center" rowspan="1" colspan="1">4.2</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;25.8</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Superior Temporal Gyrus</td><td valign="top" align="center" rowspan="1" colspan="1">103</td><td valign="top" align="center" rowspan="1" colspan="1">L</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;48.6</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;23.8</td><td valign="top" align="center" rowspan="1" colspan="1">2.9</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Superior Frontal Gyrus</td><td valign="top" align="center" rowspan="1" colspan="1">271</td><td valign="top" align="center" rowspan="1" colspan="1">R</td><td valign="top" align="center" rowspan="1" colspan="1">14.4</td><td valign="top" align="center" rowspan="1" colspan="1">69.3</td><td valign="top" align="center" rowspan="1" colspan="1">14.1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Corpus Callosum</td><td valign="top" align="center" rowspan="1" colspan="1">141</td><td valign="top" align="center" rowspan="1" colspan="1">R</td><td valign="top" align="center" rowspan="1" colspan="1">2.5</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;35</td><td valign="top" align="center" rowspan="1" colspan="1">7.1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Superior Frontal Gyrus</td><td valign="top" align="center" rowspan="1" colspan="1">139</td><td valign="top" align="center" rowspan="1" colspan="1">L</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;8.7</td><td valign="top" align="center" rowspan="1" colspan="1">67.2</td><td valign="top" align="center" rowspan="1" colspan="1">20.4</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Middle Frontal Gyrus</td><td valign="top" align="center" rowspan="1" colspan="1">117</td><td valign="top" align="center" rowspan="1" colspan="1">L</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;25.5</td><td valign="top" align="center" rowspan="1" colspan="1">21.7</td><td valign="top" align="center" rowspan="1" colspan="1">42.8</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Precuneus</td><td valign="top" align="center" rowspan="1" colspan="1">101</td><td valign="top" align="center" rowspan="1" colspan="1">L</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;9.4</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;75.6</td><td valign="top" align="center" rowspan="1" colspan="1">55.4</td></tr></tbody></table></table-wrap><p>The main morphology difference for gender exists mainly in the frontal lobe and the limbic lobe, others scattered in the parietal lobe, the temporal lobe, the corpus callosum and the precuneus. Considering the high relevance of cortical 3-D morphology and GM, we compared our study and previous studies of gender difference with GM concentration, and found that our study had high accordance with the study of gender difference using T1w, T2w, and FA (Feis et al., <xref rid="B15" ref-type="bibr">2013</xref>) and using GM concentration and fMRI (Wang et al., <xref rid="B55" ref-type="bibr">2012</xref>), and also those using cortical thickness (Im et al., <xref rid="B24" ref-type="bibr">2006</xref>; Sowell et al., <xref rid="B49" ref-type="bibr">2007</xref>; Lv et al., <xref rid="B33" ref-type="bibr">2010</xref>) in reporting the main gender difference in the frontal lobe, the limbic lobe, the parietal lobe and the temporal lobe. Moreover, there are reports of gender difference in the precuneus (Kaiser et al., <xref rid="B28" ref-type="bibr">2008</xref>; Taki et al., <xref rid="B51" ref-type="bibr">2011</xref>; Semrud-Clikeman et al., <xref rid="B46" ref-type="bibr">2012</xref>) and the corpus callosum (Witelson, <xref rid="B56" ref-type="bibr">1989</xref>; Allen et al., <xref rid="B1" ref-type="bibr">1991</xref>; Bishop and Wahlsten, <xref rid="B8" ref-type="bibr">1997</xref>).</p></sec><sec><title>3.3. Discriminative Ability of Brain Subregions</title><p>The accuracy of each brain subregion in AAL for gender classification is in <xref ref-type="fig" rid="F5">Figure 5</xref>, and the top and bottom 5 discriminative subregions and their classification accuracy are in <xref rid="T3" ref-type="table">Table 3</xref>. The most discriminative regions of gender exist in the front of the brain and the least discriminative regions are the temporal gyrus. It can be seen from <xref ref-type="fig" rid="F5">Figure 5</xref> that the accuracy distribution of two hemi-spheres is roughly bilateral symmetrical, which means that the corresponding brain areas of two hemi-spheres have approximately equal discriminative abilities in gender difference.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p>Surface rendering of gender classification accuracy of brain region in AAL. The discriminative ability of brain area improves roughly from behind to front, and the accuracy distribution of two hemi-spheres is roughly bilateral symmetrical.</p></caption><graphic xlink:href="fnhum-13-00029-g0005"/></fig><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>The top and bottom 5 discriminative regions of AAL template and accuracy for gender classification, the highest gender classification accuracy distributed in the Frontal Lobe while the bottommost gender classification accuracy distributed in the Temporal Lobe.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Top 5 discriminative regions [acc (%)]</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Bottom 5 discriminative regions [acc (%)]</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_Medial_R (87.87)</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Inf_R (62.89)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_Medial_L (87.78)</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Pole_Mid_R (63.07)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_R (87.78)</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Inf_L (63.34)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Supp_Motor_Area_R (87.42)</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Pole_Mid_L (63.61)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_L (87.42)</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Mid_R (64.06)</td></tr></tbody></table></table-wrap><p>An interesting phenomenon which should be paid attention to is that the brain subregions' discriminative ability for gender arises from posterior to anterior in the brain, and this phenomenon has high accordance with the evolution regular of human brain: these brain areas located in the anterior of the brain evolved first, while these posterior brain areas evolved later (Buckner and Krienen, <xref rid="B9" ref-type="bibr">2013</xref>). A possible explanation is that these brain areas evolving advanced and better in human evolution history have more abundant and complex function, so they should develop first in individual brain to ensure the basic function, and with evolution the functional difference of gender grows thus the structural difference grows, too. And those brain areas evolving not so full have less functions and those functions are common among human beings.</p><p>The accuracies and AUCs of 7 RSN for gender classification are in <xref rid="T4" ref-type="table">Table 4</xref>. Considering the dimensionality of data, the classification of 7 RSN was conducted in 1.4 mm data. The most discriminative brain areas of gender difference mainly distribute in the default mode network, which is also indicated in Zhang et al. (<xref rid="B62" ref-type="bibr">2018</xref>). While a majority of the least discriminative regions belong to the visual network and dorsal attention network. The outcome offers a new evidence of the accordance between structural and functional brain.</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>AUC and accuracy for gender classification of 7 RSN Networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>RSN network</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>1</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>2</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>3</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>4</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>5</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>6</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>7</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">AUC</td><td valign="top" align="center" rowspan="1" colspan="1">0.8782</td><td valign="top" align="center" rowspan="1" colspan="1">0.9257</td><td valign="top" align="center" rowspan="1" colspan="1">0.8849</td><td valign="top" align="center" rowspan="1" colspan="1">0.9359</td><td valign="top" align="center" rowspan="1" colspan="1">93.68</td><td valign="top" align="center" rowspan="1" colspan="1">0.9285</td><td valign="top" align="center" rowspan="1" colspan="1">0.9568</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">80.05</td><td valign="top" align="center" rowspan="1" colspan="1">86.16</td><td valign="top" align="center" rowspan="1" colspan="1">81.22</td><td valign="top" align="center" rowspan="1" colspan="1">86.25</td><td valign="top" align="center" rowspan="1" colspan="1">86.88</td><td valign="top" align="center" rowspan="1" colspan="1">85.44</td><td valign="top" align="center" rowspan="1" colspan="1">90.21</td></tr></tbody></table><table-wrap-foot><p><italic>(1) Visual network; (2) Somatomotor network; (3) Dorsal attention network; (4) Ventral network; (5) Limbic network; (6) Frontoparietal contral network; (7) Default mode network</italic>.</p></table-wrap-foot></table-wrap><p>Surface-based gender difference is in <xref ref-type="fig" rid="F6">Figure 6</xref> which shows that gender difference is most obvious in myelinmap of all the 4 surface-based morphology. The average gender classification accuracy in 10 times of 10-fold cross-validation of thickness, curvature, sulc and myelinmap are 0.8740, 0.8022, 0.8431, and 0.8820, respectively. The details of the most discriminative areas are as follows: isthmuscingulate, left superiortemporal, and right insula for cortical thickness; posteriorcingulate and insula for sulc; inferiorparietal, isthmuscingulate and left posteriorcingulate for curvtura; precuneus, rostralmiddlefrontal and superiorfrontal for myelinmap. Interestingly, myelinmap showed greater gender difference and those discriminative areas of myelinmap have high accordance with those areas we find in cortical 3-D morphology, especially in the frontal lobe and the precuneus; those discriminative areas in the other 3 surface-based morphology are mainly in the insula, which is also found in cortical 3-D morphology.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p>Gender difference of surface-based morphology denoted by absolute value of the <italic>t</italic>-value of two sample <italic>t</italic>-test (<italic>p</italic> &#x0003c; 0.01, visualized by workbench of the HCP).</p></caption><graphic xlink:href="fnhum-13-00029-g0006"/></fig></sec></sec><sec sec-type="discussion" id="s4"><title>4. Discussion</title><p>In this study, we investigated gender difference of cortical 3-D morphology by proposing an HSRC approach, and got an accuracy of 96.77% in a 10-fold cross-validation. The robustness of classification was testified by permutation tests, and the frontal lobe was found the most discriminative region of gender difference in cortical 3-D morphology selected by HSRC. The superior frontal gyrus in AAL and the default mode network in RSN got the highest accuracy in template based classification. Moreover, the advantages of our proposed HSRC method were mentioned. Discussions are in the following.</p><p>There are reports of gender difference in cortical morphology (Im et al., <xref rid="B24" ref-type="bibr">2006</xref>; Sowell et al., <xref rid="B49" ref-type="bibr">2007</xref>; Lv et al., <xref rid="B33" ref-type="bibr">2010</xref>; Sepehrband et al., <xref rid="B47" ref-type="bibr">2018</xref>) and brain morphology changes in aging (Resnick et al., <xref rid="B40" ref-type="bibr">2000</xref>; Bigler et al., <xref rid="B7" ref-type="bibr">2002</xref>; Rusinek et al., <xref rid="B45" ref-type="bibr">2003</xref>; Fjell et al., <xref rid="B16" ref-type="bibr">2009</xref>) and multiple inherent brain disorders (Lieberman et al., <xref rid="B30" ref-type="bibr">2001</xref>; Ashburner et al., <xref rid="B4" ref-type="bibr">2003</xref>; Thompson et al., <xref rid="B53" ref-type="bibr">2004</xref>; Jouvent et al., <xref rid="B27" ref-type="bibr">2008</xref>; Aylward et al., <xref rid="B5" ref-type="bibr">2010</xref>), and our proposed method may have the potential in auxiliary diagnosis of those disorders combined with other modalities. Theoretically brain morphology is less sensitive to the scan variables than GM concentration, which may help the fusion of sMRI data from different datasets, and thus our discovery may also offer a new thinking in dealing with multi-site MRI data (Ma et al., <xref rid="B34" ref-type="bibr">2018</xref>; Yuan et al., <xref rid="B60" ref-type="bibr">2018b</xref>; Zeng et al., <xref rid="B61" ref-type="bibr">2018</xref>).</p><p>As far as we know, this work is the first to classify gender with original cortical 3-D morphology and to get an accuracy of over 95% in gender classification using morphological features. It encouraged us to draw a conclusion that genders can be distinguished on individual level by cortical 3-D morphology features, and supported those opinions in the aspect of brain morphology that males and females can be effectively classified (Chekroud et al., <xref rid="B10" ref-type="bibr">2016</xref>; Rosenblatt, <xref rid="B43" ref-type="bibr">2016</xref>; Anderson et al., <xref rid="B2" ref-type="bibr">2018</xref>), as well as challenged these suggestions that brains are essentially indistinguishable in gender (Joel et al., <xref rid="B26" ref-type="bibr">2015</xref>).</p><p>The result of bootstrap tests showed that those discriminative regions of gender difference found by cortical 3-D morphology had high accordance with those found by GM concentration and surface-based morphology in previous studies, especially in the frontal lobe, the limbic lobe and the partial lobe. We suggested a hypothesis that those gender difference of GM concentration, to some extent, may be the result of morphology difference.</p><p>Atlas-based morphology analysis indicated different discriminative abilities among brain areas, that is to say, some brain areas contributed much to the gender difference, while some areas exert a smaller influence, and even some areas had no contribution for gender difference, which may be referred to as so-called mosaic areas (Rippon et al., <xref rid="B41" ref-type="bibr">2014</xref>; Joel et al., <xref rid="B26" ref-type="bibr">2015</xref>). According to the brain areas classification results, those brain areas with complex functions and functions related to gender reap high accuracy in gender classification. The bootstrap results also show that the high difference voxels are located in the high difference brain areas, which is comprehensible and consistent with the classification results. Moreover, we found good symmetry in AAL-based morphology analysis which is rarely mentioned in previous studies of gender difference; RSN-based morphology analysis suggested that the default mode network is the most discriminative network, and the same result was also reported in the studies of gender difference using fMRI Zhang et al. (<xref rid="B62" ref-type="bibr">2018</xref>).</p><p>Considering that sample size was emphasized in recent studies (Ritchie et al., <xref rid="B42" ref-type="bibr">2018</xref>), we particularly compared our findings with those using more than 1,000 samples (Chekroud et al., <xref rid="B10" ref-type="bibr">2016</xref>; Gur and Gur, <xref rid="B23" ref-type="bibr">2016</xref>; Anderson et al., <xref rid="B2" ref-type="bibr">2018</xref>; Ritchie et al., <xref rid="B42" ref-type="bibr">2018</xref>), and we found considerable accordance. First, the reported classification accuracies were more than 90% to support the opinions of sexual dimorphism with different MRI modalities. Second, the most discriminative areas/networks of gender difference were found to be the frontal lobe (Gur and Gur, <xref rid="B23" ref-type="bibr">2016</xref>; Anderson et al., <xref rid="B2" ref-type="bibr">2018</xref>; Ritchie et al., <xref rid="B42" ref-type="bibr">2018</xref>) and the default mode network (Gur and Gur, <xref rid="B23" ref-type="bibr">2016</xref>; Ritchie et al., <xref rid="B42" ref-type="bibr">2018</xref>), further indicating high relevance of cortical morphology, GM concentration and fMRI based on large sample size.</p><p>The proposed HSRC algorithm was testified to be helpful in improving classification accuracy while reducing computation and storage resource for high-dimensional MRI data. It also selected features directly, making discriminative voxels more explainable in MRI data and may help to accurately locate lesion of diseased brain (Antel et al., <xref rid="B3" ref-type="bibr">2003</xref>; Llad&#x000f3; et al., <xref rid="B32" ref-type="bibr">2012</xref>).</p><p>We noticed several possible limitations in this work. Firstly, there are papers suggesting that important gender difference also exists in subcortical structures like cerebellum, amygdala and hippocampus (Giedd et al., <xref rid="B17" ref-type="bibr">2012</xref>; Ruigrok et al., <xref rid="B44" ref-type="bibr">2014</xref>). As cortical thickness of these subcortical structures is much less than that of the cerebral cortex, it cannot be automatically segmented by the pipelines offered by the HCP at present (Glasser et al., <xref rid="B19" ref-type="bibr">2013</xref>). Since morphology data provided by the HCP did not include these subcortical structures so far, the influence of subcortical morphology to gender difference was not studied. Secondly, the effect of aging on brain morphology was not discussed because of narrow age range of adults (22&#x02013;37 years old) in our study. Thirdly, because of the lack of T2w images, we have not conducted multi-site experiment to test the robustness of brain morphology by now. Moreover, although we have conducted dimension reduction, linear SVM and cross-validation to alleviate the risk of overfitting in the classification methodology as far as possible, an independent dataset is still required to validate the generalizability of our proposed model, which should be done once possible in the future.</p></sec><sec id="s5"><title>Ethics Statement</title><p>This study was carried out in accordance with the recommendations of &#x0201c;name of guidelines, name of committee&#x0201d; with written informed consent from all subjects. All subjects gave written informed consent in accordance with the Declaration of Helsinki. The protocol was approved by the &#x0201c;name of committee.&#x0201d;</p></sec><sec id="s6"><title>Author Contributions</title><p>DH designed the study. ZL conducted the experiment. ZL, CH, and LW wrote the article.</p><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>We thank the HCP for data collection and sharing. This work was supported by the National Natural Science Foundation of China (61420106001).</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>L.</given-names></name><name><surname>Richey</surname><given-names>M.</given-names></name><name><surname>Chai</surname><given-names>Y.</given-names></name><name><surname>Gorski</surname><given-names>R.</given-names></name></person-group> (<year>1991</year>). <article-title>Sex differences in the corpus callosum of the living human being</article-title>. <source>J. Neurosci.</source>
<volume>11</volume>, <fpage>933</fpage>&#x02013;<lpage>942</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.11-04-00933.1991</pub-id><pub-id pub-id-type="pmid">2010816</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>N. E.</given-names></name><name><surname>Harenski</surname><given-names>K. A.</given-names></name><name><surname>Harenski</surname><given-names>C. L.</given-names></name><name><surname>Koenigs</surname><given-names>M. R.</given-names></name><name><surname>Decety</surname><given-names>J.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Machine learning of brain gray matter differentiates sex in a large forensic sample</article-title>. <source>Hum. Brain Mapp.</source> [Epub ahead of print] <pub-id pub-id-type="doi">10.1002/hbm.24462</pub-id>. <pub-id pub-id-type="pmid">30430711</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antel</surname><given-names>S. B.</given-names></name><name><surname>Collins</surname><given-names>D.</given-names></name><name><surname>Bernasconi</surname><given-names>N.</given-names></name><name><surname>Andermann</surname><given-names>F.</given-names></name><name><surname>Shinghal</surname><given-names>R.</given-names></name><name><surname>Kearney</surname><given-names>R. E.</given-names></name><etal/></person-group>. (<year>2003</year>). <article-title>Automated detection of focal cortical dysplasia lesions using computational models of their MRI characteristics and texture analysis</article-title>. <source>Neuroimage</source>
<volume>19</volume>, <fpage>1748</fpage>&#x02013;<lpage>1759</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00226-X</pub-id><pub-id pub-id-type="pmid">12948729</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Csernansk</surname><given-names>J. G.</given-names></name><name><surname>Davatzikos</surname><given-names>C.</given-names></name><name><surname>Fox</surname><given-names>N. C.</given-names></name><name><surname>Frisoni</surname><given-names>G. B.</given-names></name><name><surname>Thompson</surname><given-names>P. M.</given-names></name></person-group> (<year>2003</year>). <article-title>Computer-assisted imaging to assess brain structure in healthy and diseased brains</article-title>. <source>Lancet Neurol.</source>
<volume>2</volume>, <fpage>79</fpage>&#x02013;<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/S1474-4422(03)00304-1</pub-id><pub-id pub-id-type="pmid">12849264</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aylward</surname><given-names>E. H.</given-names></name><name><surname>Nopoulos</surname><given-names>P. C.</given-names></name><name><surname>Ross</surname><given-names>C. A.</given-names></name><name><surname>Langbehn</surname><given-names>D. R.</given-names></name><name><surname>Pierson</surname><given-names>R. K.</given-names></name><name><surname>Mills</surname><given-names>J. A.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Longitudinal change in regional brain volumes in prodromal huntington disease</article-title>. <source>J. Neurol. Neurosurg. Psychiatry</source>
<volume>82</volume>, <fpage>405</fpage>&#x02013;<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1136/jnnp.2010.208264</pub-id><pub-id pub-id-type="pmid">20884680</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname><given-names>S.</given-names></name><name><surname>Knickmeyer</surname><given-names>R. C.</given-names></name><name><surname>Belmonte</surname><given-names>M. K.</given-names></name></person-group> (<year>2005</year>). <article-title>Sex differences in the brain: implications for explaining autism</article-title>. <source>Science</source>
<volume>310</volume>, <fpage>819</fpage>&#x02013;<lpage>823</lpage>. <pub-id pub-id-type="doi">10.1126/science.1115455</pub-id><pub-id pub-id-type="pmid">16272115</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigler</surname><given-names>E. D.</given-names></name><name><surname>Andersob</surname><given-names>C. V.</given-names></name><name><surname>Blatter</surname><given-names>D. D.</given-names></name></person-group> (<year>2002</year>). <article-title>Temporal lobe morphology in normal aging and traumatic brain injury</article-title>. <source>Am. J. Neuroradiol.</source>
<volume>23</volume>, <fpage>255</fpage>&#x02013;<lpage>266</lpage>. <pub-id pub-id-type="pmid">11847051</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>K. M.</given-names></name><name><surname>Wahlsten</surname><given-names>D.</given-names></name></person-group> (<year>1997</year>). <article-title>Sex differences in the human corpus callosum: myth or reality?</article-title>
<source>Neurosci. Biobehav. Rev.</source>
<volume>21</volume>, <fpage>581</fpage>&#x02013;<lpage>601</lpage>. <pub-id pub-id-type="doi">10.1016/S0149-7634(96)00049-8</pub-id><pub-id pub-id-type="pmid">9353793</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>R. L.</given-names></name><name><surname>Krienen</surname><given-names>F. M.</given-names></name></person-group> (<year>2013</year>). <article-title>The evolution of distributed association networks in the human brain</article-title>. <source>Trends Cogn. Sci.</source>
<volume>17</volume>, <fpage>648</fpage>&#x02013;<lpage>665</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2013.09.017</pub-id><pub-id pub-id-type="pmid">24210963</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chekroud</surname><given-names>A. M.</given-names></name><name><surname>Ward</surname><given-names>E. J.</given-names></name><name><surname>Rosenberg</surname><given-names>M. D.</given-names></name><name><surname>Holmes</surname><given-names>A. J.</given-names></name></person-group> (<year>2016</year>). <article-title>Patterns in the human brain mosaic discriminate males from females</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>113</volume>, <fpage>1968</fpage>&#x02013;<lpage>1968</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1523888113</pub-id><pub-id pub-id-type="pmid">26884168</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Yuan</surname><given-names>H.</given-names></name><name><surname>Zheng</surname><given-names>T.</given-names></name><name><surname>Chang</surname><given-names>Y.</given-names></name><name><surname>Luo</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>Females are more sensitive to opponent's emotional feedback: evidence from event-related potentials</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>12</volume>:<fpage>275</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2018.00275</pub-id><pub-id pub-id-type="pmid">30042666</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christov-Moore</surname><given-names>L.</given-names></name><name><surname>Simpson</surname><given-names>E. A.</given-names></name><name><surname>Coud</surname><given-names>G.</given-names></name><name><surname>Grigaityte</surname><given-names>K.</given-names></name><name><surname>Iacoboni</surname><given-names>M.</given-names></name><name><surname>Ferrari</surname><given-names>P. F.</given-names></name></person-group> (<year>2014</year>). <article-title>Empathy: gender effects in brain and behavior</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>46</volume>, <fpage>604</fpage>&#x02013;<lpage>627</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.09.001</pub-id><pub-id pub-id-type="pmid">25236781</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eranti</surname><given-names>S. V.</given-names></name><name><surname>MacCabe</surname><given-names>J. H.</given-names></name><name><surname>Bundy</surname><given-names>H.</given-names></name><name><surname>Murray</surname><given-names>R. M.</given-names></name></person-group> (<year>2013</year>). <article-title>Gender difference in age at onset of schizophrenia: a meta-analysis</article-title>. <source>Psychol. Med.</source>
<volume>43</volume>, <fpage>155</fpage>&#x02013;<lpage>167</lpage>. <pub-id pub-id-type="doi">10.1017/S003329171200089X</pub-id><pub-id pub-id-type="pmid">22564907</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Essen</surname><given-names>D. V.</given-names></name><name><surname>Ugurbil</surname><given-names>K.</given-names></name><name><surname>Auerbach</surname><given-names>E.</given-names></name><name><surname>Barch</surname><given-names>D.</given-names></name><name><surname>Behrens</surname><given-names>T.</given-names></name><name><surname>Bucholz</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>The Human Connectome Project: a data acquisition perspective</article-title>. <source>Neuroimage</source>
<volume>62</volume>, <fpage>2222</fpage>&#x02013;<lpage>2231</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id><pub-id pub-id-type="pmid">22366334</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feis</surname><given-names>D.-L.</given-names></name><name><surname>Brodersen</surname><given-names>K. H.</given-names></name><name><surname>von Cramon</surname><given-names>D. Y.</given-names></name><name><surname>Luders</surname><given-names>E.</given-names></name><name><surname>Tittgemeyer</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>Decoding gender dimorphism of the human brain using multimodal anatomical and diffusion MRI data</article-title>. <source>Neuroimage</source>
<volume>70</volume>, <fpage>250</fpage>&#x02013;<lpage>257</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.12.068</pub-id><pub-id pub-id-type="pmid">23298750</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fjell</surname><given-names>A. M.</given-names></name><name><surname>Walhovd</surname><given-names>K. B.</given-names></name><name><surname>Fennema-Notestine</surname><given-names>C.</given-names></name><name><surname>McEvoy</surname><given-names>L. K.</given-names></name><name><surname>Hagler</surname><given-names>D. J.</given-names></name><name><surname>Holland</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>One-year brain atrophy evident in healthy aging</article-title>. <source>J. Neurosci.</source>
<volume>29</volume>, <fpage>15223</fpage>&#x02013;<lpage>15231</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3252-09.2009</pub-id><pub-id pub-id-type="pmid">19955375</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giedd</surname><given-names>J. N.</given-names></name><name><surname>Raznahan</surname><given-names>A.</given-names></name><name><surname>Mills</surname><given-names>K. L.</given-names></name><name><surname>Lenroot</surname><given-names>R. K.</given-names></name></person-group> (<year>2012</year>). <article-title>Review: magnetic resonance imaging of male/female differences in human adolescent brain anatomy</article-title>. <source>Biol. Sex Differ.</source>
<volume>3</volume>:<fpage>19</fpage>. <pub-id pub-id-type="doi">10.1186/2042-6410-3-19</pub-id><pub-id pub-id-type="pmid">22908911</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giudice</surname><given-names>M. D.</given-names></name></person-group> (<year>2009</year>). <article-title>On the real magnitude of psychological sex differences</article-title>. <source>Evol. Psychol.</source>
<volume>7</volume>:<fpage>264</fpage>
<pub-id pub-id-type="doi">10.1177/147470490900700209</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>M. F.</given-names></name><name><surname>Sotiropoulos</surname><given-names>S. N.</given-names></name><name><surname>Wilson</surname><given-names>J. A.</given-names></name><name><surname>Coalson</surname><given-names>T. S.</given-names></name><name><surname>Fischl</surname><given-names>B.</given-names></name><name><surname>Andersson</surname><given-names>J. L.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title>. <source>Neuroimage</source>
<volume>80</volume>, <fpage>105</fpage>&#x02013;<lpage>124</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id><pub-id pub-id-type="pmid">23668970</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldstein</surname><given-names>J.</given-names></name><name><surname>Seidman</surname><given-names>L.</given-names></name><name><surname>Horton</surname><given-names>N.</given-names></name><name><surname>Makris</surname><given-names>N.</given-names></name><name><surname>Kennedy</surname><given-names>D.</given-names></name><name><surname>Caviness</surname><given-names>V.</given-names></name><etal/></person-group>. (<year>2001</year>). <article-title>Normal sexual dimorphism of the adult human brain assessed by <italic>in vivo</italic> magnetic resonance imaging</article-title>. <source>Cereb. Cortex</source>
<volume>11</volume>, <fpage>490</fpage>&#x02013;<lpage>497</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/11.6.490</pub-id><pub-id pub-id-type="pmid">11375910</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>G.</given-names></name><name><surname>Rosa-Neto</surname><given-names>P.</given-names></name><name><surname>Carbonell</surname><given-names>F.</given-names></name><name><surname>Chen</surname><given-names>Z. J.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Evans</surname><given-names>A. C.</given-names></name></person-group> (<year>2009</year>). <article-title>Age- and gender-related differences in the cortical anatomical network</article-title>. <source>J. Neurosci.</source>
<volume>29</volume>, <fpage>15684</fpage>&#x02013;<lpage>15693</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2308-09.2009</pub-id><pub-id pub-id-type="pmid">20016083</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gur</surname><given-names>R. C.</given-names></name><name><surname>Turetsky</surname><given-names>B. I.</given-names></name><name><surname>Matsui</surname><given-names>M.</given-names></name><name><surname>Yan</surname><given-names>M.</given-names></name><name><surname>Bilker</surname><given-names>W.</given-names></name><name><surname>Hughett</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>1999</year>). <article-title>Sex differences in brain gray and white matter in healthy young adults: correlations with cognitive performance</article-title>. <source>J. Neurosci.</source>
<volume>19</volume>, <fpage>4065</fpage>&#x02013;<lpage>4072</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-10-04065.1999</pub-id><pub-id pub-id-type="pmid">10234034</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gur</surname><given-names>R. E.</given-names></name><name><surname>Gur</surname><given-names>R. C.</given-names></name></person-group> (<year>2016</year>). <article-title>Sex differences in brain and behavior in adolescence: findings from the philadelphia neurodevelopmental cohort</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>70</volume>, <fpage>159</fpage>&#x02013;<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.07.035</pub-id><pub-id pub-id-type="pmid">27498084</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Im</surname><given-names>K.</given-names></name><name><surname>Lee</surname><given-names>J.-M.</given-names></name><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>Shin</surname><given-names>Y.-W.</given-names></name><name><surname>Kim</surname><given-names>I. Y.</given-names></name><name><surname>Kwon</surname><given-names>J. S.</given-names></name><etal/></person-group>. (<year>2006</year>). <article-title>Gender difference analysis of cortical thickness in healthy young adults with surface-based methods</article-title>. <source>Neuroimage</source>
<volume>31</volume>, <fpage>31</fpage>&#x02013;<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.11.042</pub-id><pub-id pub-id-type="pmid">16426865</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ingalhalikar</surname><given-names>M.</given-names></name><name><surname>Smith</surname><given-names>A.</given-names></name><name><surname>Parker</surname><given-names>D.</given-names></name><name><surname>Satterthwaite</surname><given-names>T. D.</given-names></name><name><surname>Elliott</surname><given-names>M. A.</given-names></name><name><surname>Ruparel</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>Sex differences in the structural connectome of the human brain</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>111</volume>, <fpage>823</fpage>&#x02013;<lpage>828</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1316909110</pub-id><pub-id pub-id-type="pmid">24297904</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joel</surname><given-names>D.</given-names></name><name><surname>Berman</surname><given-names>Z.</given-names></name><name><surname>Tavor</surname><given-names>I.</given-names></name><name><surname>Wexler</surname><given-names>N.</given-names></name><name><surname>Gaber</surname><given-names>O.</given-names></name><name><surname>Stein</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Sex beyond the genitalia: the human brain mosaic</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>112</volume>, <fpage>15468</fpage>&#x02013;<lpage>15473</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1509654112</pub-id><pub-id pub-id-type="pmid">26621705</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jouvent</surname><given-names>E.</given-names></name><name><surname>Mangin</surname><given-names>J.-F.</given-names></name><name><surname>Porcher</surname><given-names>R.</given-names></name><name><surname>Viswanathan</surname><given-names>A.</given-names></name><name><surname>O'Sullivan</surname><given-names>M.</given-names></name><name><surname>Guichard</surname><given-names>J.-P.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Cortical changes in cerebral small vessel diseases: a 3D MRI study of cortical morphology in CADASIL</article-title>. <source>Brain</source>
<volume>131</volume>, <fpage>2201</fpage>&#x02013;<lpage>2208</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awn129</pub-id><pub-id pub-id-type="pmid">18577545</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>S.</given-names></name><name><surname>Walther</surname><given-names>S.</given-names></name><name><surname>Nennig</surname><given-names>E.</given-names></name><name><surname>Kronmller</surname><given-names>K.</given-names></name><name><surname>Mundt</surname><given-names>C.</given-names></name><name><surname>Weisbrod</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Gender-specific strategy use and neural correlates in a spatial perspective taking task</article-title>. <source>Neuropsychologia</source>
<volume>46</volume>, <fpage>2524</fpage>&#x02013;<lpage>2531</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.04.013</pub-id><pub-id pub-id-type="pmid">18514745</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>M.-C.</given-names></name><name><surname>Lombardo</surname><given-names>M. V.</given-names></name><name><surname>Auyeung</surname><given-names>B.</given-names></name><name><surname>Chakrabarti</surname><given-names>B.</given-names></name><name><surname>Baron-Cohen</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Sex/gender differences and autism: setting the scene for future research</article-title>. <source>J. Am. Acad. Child Adolesc. Psychiatry</source>
<volume>54</volume>, <fpage>11</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1016/j.jaac.2014.10.003</pub-id><pub-id pub-id-type="pmid">25524786</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieberman</surname><given-names>J.</given-names></name><name><surname>Chakos</surname><given-names>M.</given-names></name><name><surname>Wu</surname><given-names>H.</given-names></name><name><surname>Alvir</surname><given-names>J.</given-names></name><name><surname>Hoffman</surname><given-names>E.</given-names></name><name><surname>Robinson</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2001</year>). <article-title>Longitudinal study of brain morphology in first episode schizophrenia</article-title>. <source>Biol. Psychiatry</source>
<volume>49</volume>, <fpage>487</fpage>&#x02013;<lpage>499</lpage>. <pub-id pub-id-type="doi">10.1016/S0006-3223(01)01067-8</pub-id><pub-id pub-id-type="pmid">11257234</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Motoda</surname><given-names>H.</given-names></name></person-group> (<year>1998</year>). <source>Feature Selection for Knowledge Discovery and Data Mining</source>. <publisher-loc>Norwell, MA</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>.</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llad&#x000f3;</surname><given-names>X.</given-names></name><name><surname>Oliver</surname><given-names>A.</given-names></name><name><surname>Cabezas</surname><given-names>M.</given-names></name><name><surname>Freixenet</surname><given-names>J.</given-names></name><name><surname>Vilanova</surname><given-names>J. C.</given-names></name><name><surname>Quiles</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Segmentation of multiple sclerosis lesions in brain MRI: a review of automated approaches</article-title>. <source>Inform. Sci.</source>
<volume>186</volume>, <fpage>164</fpage>&#x02013;<lpage>185</lpage>. <pub-id pub-id-type="doi">10.1016/j.ins.2011.10.011</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lv</surname><given-names>B.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>He</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Zhao</surname><given-names>M.</given-names></name><name><surname>Ai</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Gender consistency and difference in healthy adults revealed by cortical thickness</article-title>. <source>Neuroimage</source>
<volume>53</volume>, <fpage>373</fpage>&#x02013;<lpage>382</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.020</pub-id><pub-id pub-id-type="pmid">20493267</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>T.</given-names></name><name><surname>Zanetti</surname><given-names>M. V.</given-names></name><name><surname>Shen</surname><given-names>H.</given-names></name><name><surname>Satterthwaite</surname><given-names>T. D.</given-names></name><name><surname>Wolf</surname><given-names>D. H.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Classification of multi-site MR images in the presence of heterogeneity using multi-task learning</article-title>. <source>Neuroimage</source>
<volume>19</volume>, <fpage>476</fpage>&#x02013;<lpage>486</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2018.04.037</pub-id><pub-id pub-id-type="pmid">29984156</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>C. E.</given-names></name><name><surname>Kurth</surname><given-names>F.</given-names></name><name><surname>Lepore</surname><given-names>S.</given-names></name><name><surname>Gao</surname><given-names>J. L.</given-names></name><name><surname>Johnsonbaugh</surname><given-names>H.</given-names></name><name><surname>Oberoi</surname><given-names>M. R.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title><italic>In vivo</italic> magnetic resonance images reveal neuroanatomical sex differences through the application of voxel-based morphometry in c57bl/6 mice</article-title>. <source>Neuroimage</source>
<volume>163</volume>, <fpage>197</fpage>&#x02013;<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.027</pub-id><pub-id pub-id-type="pmid">28923275</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nie</surname><given-names>F.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Cai</surname><given-names>X.</given-names></name><name><surname>Ding</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>Efficient and robust feature selection via joint l2,1-norms minimization</article-title>, in <source>Proceedings of the 23rd International Conference on Neural Information Processing Systems</source>, <volume>Vol. 2</volume>, NIPS'10 (<publisher-loc>Whistler</publisher-loc>: <publisher-name>Curran Associates Inc.</publisher-name>), <fpage>1813</fpage>&#x02013;<lpage>1821</lpage>.</mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohla</surname><given-names>K.</given-names></name><name><surname>Lundstr</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Sex differences in chemosensation: sensory or emotional?</article-title>
<source>Front. Hum. Neurosci.</source>
<volume>7</volume>:<fpage>607</fpage>
<pub-id pub-id-type="doi">10.3389/fnhum.2013.00607</pub-id><pub-id pub-id-type="pmid">24133429</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piccinelli</surname><given-names>M.</given-names></name><name><surname>Wilkinson</surname><given-names>G.</given-names></name></person-group> (<year>2000</year>). <article-title>Gender differences in depression: critical review</article-title>. <source>Br. J. Psychiatry</source>
<volume>177</volume>, <fpage>486</fpage>&#x02013;<lpage>492</lpage>. <pub-id pub-id-type="doi">10.1192/bjp.177.6.486</pub-id><pub-id pub-id-type="pmid">11102321</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>D.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Fu</surname><given-names>X.</given-names></name></person-group> (<year>2009</year>). <article-title>A deeper look at gender difference in multitasking: gender-specific mechanism of cognitive control</article-title>, in <source>Proceedings of the 2009 Fifth International Conference on Natural Computation</source>, <volume>Vol 5</volume>, ICNC '09, (<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>), <fpage>13</fpage>&#x02013;<lpage>17</lpage>.</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resnick</surname><given-names>S. M.</given-names></name><name><surname>Goldszal</surname><given-names>A. F.</given-names></name><name><surname>Davatzikos</surname><given-names>C.</given-names></name><name><surname>Golski</surname><given-names>S.</given-names></name><name><surname>Kraut</surname><given-names>M. A.</given-names></name><name><surname>Metter</surname><given-names>E. J.</given-names></name><etal/></person-group>. (<year>2000</year>). <article-title>One-year age changes in MRI brain volumes in older adults</article-title>. <source>Cereb. Cortex</source>
<volume>10</volume>, <fpage>464</fpage>&#x02013;<lpage>472</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/10.5.464</pub-id><pub-id pub-id-type="pmid">10847596</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rippon</surname><given-names>G.</given-names></name><name><surname>Jordan-Young</surname><given-names>R.</given-names></name><name><surname>Kaiser</surname><given-names>A.</given-names></name><name><surname>Fine</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>Recommendations for sex/gender neuroimaging research: key principles and implications for research design, analysis, and interpretation</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>8</volume>:<fpage>650</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2014.00650</pub-id><pub-id pub-id-type="pmid">25221493</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>S. J.</given-names></name><name><surname>Cox</surname><given-names>S. R.</given-names></name><name><surname>Shen</surname><given-names>X.</given-names></name><name><surname>Lombardo</surname><given-names>M. V.</given-names></name><name><surname>Reus</surname><given-names>L. M.</given-names></name><name><surname>Alloza</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Sex differences in the adult human brain: evidence from 5216 UK biobank participants</article-title>. <source>Cereb. Cortex</source>
<volume>28</volume>, <fpage>2959</fpage>&#x02013;<lpage>2975</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhy109</pub-id><pub-id pub-id-type="pmid">29771288</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>J. D.</given-names></name></person-group> (<year>2016</year>). <article-title>Multivariate revisit to &#x0201c;sex beyond the genitalia&#x0201d;</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>113</volume>, <fpage>1966</fpage>&#x02013;<lpage>1967</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1523961113</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruigrok</surname><given-names>A. N.</given-names></name><name><surname>Salimi-Khorshidi</surname><given-names>G.</given-names></name><name><surname>Lai</surname><given-names>M.-C.</given-names></name><name><surname>Baron-Cohen</surname><given-names>S.</given-names></name><name><surname>Lombardo</surname><given-names>M. V.</given-names></name><name><surname>Tait</surname><given-names>R. J.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>A meta-analysis of sex differences in human brain structure</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>39</volume>, <fpage>34</fpage>&#x02013;<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.12.004</pub-id><pub-id pub-id-type="pmid">24374381</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rusinek</surname><given-names>H.</given-names></name><name><surname>De Santi</surname><given-names>S.</given-names></name><name><surname>Frid</surname><given-names>D.</given-names></name><name><surname>Tsui</surname><given-names>W.-H.</given-names></name><name><surname>Tarshish</surname><given-names>C. Y.</given-names></name><name><surname>Convit</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2003</year>). <article-title>Regional brain atrophy rate predicts future cognitive decline: 6-year longitudinal MR imaging study of normal aging</article-title>. <source>Radiology</source>
<volume>229</volume>, <fpage>691</fpage>&#x02013;<lpage>696</lpage>. <pub-id pub-id-type="doi">10.1148/radiol.2293021299</pub-id><pub-id pub-id-type="pmid">14657306</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semrud-Clikeman</surname><given-names>M.</given-names></name><name><surname>Fine</surname><given-names>J. G.</given-names></name><name><surname>Bledsoe</surname><given-names>J.</given-names></name><name><surname>Zhu</surname><given-names>D. C.</given-names></name></person-group> (<year>2012</year>). <article-title>Gender differences in brain activation on a mental rotation task</article-title>. <source>Int. J. Neurosci.</source>
<volume>122</volume>, <fpage>590</fpage>&#x02013;<lpage>597</lpage>. <pub-id pub-id-type="doi">10.3109/00207454.2012.693999</pub-id><pub-id pub-id-type="pmid">22651549</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sepehrband</surname><given-names>F.</given-names></name><name><surname>Lynch</surname><given-names>K. M.</given-names></name><name><surname>Cabeen</surname><given-names>R. P.</given-names></name><name><surname>Gonzalez-Zacarias</surname><given-names>C.</given-names></name><name><surname>Zhao</surname><given-names>L.</given-names></name><name><surname>D'Arcy</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Neuroanatomical morphometric characterization of sex differences in youth using statistical learning</article-title>. <source>Neuroimage</source>
<volume>172</volume>, <fpage>217</fpage>&#x02013;<lpage>227</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.065</pub-id><pub-id pub-id-type="pmid">29414494</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shulman</surname><given-names>L. M.</given-names></name></person-group> (<year>2007</year>). <article-title>Gender differences in Parkinson's disease</article-title>. <source>Gender Med.</source>
<volume>4</volume>, <fpage>8</fpage>&#x02013;<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/S1550-8579(07)80003-9</pub-id><pub-id pub-id-type="pmid">17584622</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sowell</surname><given-names>E. R.</given-names></name><name><surname>Peterson</surname><given-names>B. S.</given-names></name><name><surname>Kan</surname><given-names>E.</given-names></name><name><surname>Woods</surname><given-names>R. P.</given-names></name><name><surname>Yoshii</surname><given-names>J.</given-names></name><name><surname>Bansal</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Sex differences in cortical thickness mapped in 176 healthy individuals between 7 and 87 years of age</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>1550</fpage>&#x02013;<lpage>1560</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhl066</pub-id><pub-id pub-id-type="pmid">16945978</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>F.</given-names></name><name><surname>Shen</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Hu</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Sparse representation of brain aging: Extracting covariance patterns from structural MRI</article-title>. <source>PLoS ONE</source>
<volume>7</volume>:<fpage>e36147</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0036147</pub-id><pub-id pub-id-type="pmid">22590522</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taki</surname><given-names>Y.</given-names></name><name><surname>Hashizume</surname><given-names>H.</given-names></name><name><surname>Sassa</surname><given-names>Y.</given-names></name><name><surname>Takeuchi</surname><given-names>H.</given-names></name><name><surname>Wu</surname><given-names>K.</given-names></name><name><surname>Asano</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Gender differences in partial-volume corrected brain perfusion using brain MRI in healthy children</article-title>. <source>Neuroimage</source>
<volume>58</volume>, <fpage>709</fpage>&#x02013;<lpage>715</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.07.020</pub-id><pub-id pub-id-type="pmid">21782958</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas Yeo</surname><given-names>B. T.</given-names></name><name><surname>Krienen</surname><given-names>F. M.</given-names></name><name><surname>Sepulcre</surname><given-names>J.</given-names></name><name><surname>Sabuncu</surname><given-names>M. R.</given-names></name><name><surname>Lashkari</surname><given-names>D.</given-names></name><name><surname>Hollinshead</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>The organization of the human cerebral cortex estimated by intrinsic functional connectivity</article-title>. <source>J. Neurophysiol.</source>
<volume>106</volume>, <fpage>1125</fpage>&#x02013;<lpage>1165</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00338.2011</pub-id><pub-id pub-id-type="pmid">21653723</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>P. M.</given-names></name><name><surname>Hayashi</surname><given-names>K. M.</given-names></name><name><surname>Sowell</surname><given-names>E. R.</given-names></name><name><surname>Gogtay</surname><given-names>N.</given-names></name><name><surname>Giedd</surname><given-names>J. N.</given-names></name><name><surname>Rapoport</surname><given-names>J. L.</given-names></name><etal/></person-group>. (<year>2004</year>). <article-title>Mapping cortical change in Alzheimer's disease, brain development, and schizophrenia</article-title>. <source>Neuroimage</source>
<volume>23</volume>, <fpage>2</fpage>&#x02013;<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.071</pub-id><pub-id pub-id-type="pmid">15501091</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname><given-names>N.</given-names></name><name><surname>Landeau</surname><given-names>B.</given-names></name><name><surname>Papathanassiou</surname><given-names>D.</given-names></name><name><surname>Crivello</surname><given-names>F.</given-names></name><name><surname>Etard</surname><given-names>O.</given-names></name><name><surname>Delcroix</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2002</year>). <article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>. <source>Neuroimage</source>
<volume>15</volume>, <fpage>273</fpage>&#x02013;<lpage>289</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id><pub-id pub-id-type="pmid">11771995</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Shen</surname><given-names>H.</given-names></name><name><surname>Tang</surname><given-names>F.</given-names></name><name><surname>Zang</surname><given-names>Y.</given-names></name><name><surname>Hu</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Combined structural and resting-state functional MRI analysis of sexual dimorphism in the young adult human brain: An MVPA approach</article-title>. <source>Neuroimage</source>
<volume>61</volume>, <fpage>931</fpage>&#x02013;<lpage>940</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.03.080</pub-id><pub-id pub-id-type="pmid">22498657</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witelson</surname><given-names>S.</given-names></name></person-group> (<year>1989</year>). <article-title>Hand and sex differences in the isthmus and genu of the human corpus callosum. A postmortem morphological study</article-title>. <source>Brain</source>
<volume>112</volume>(<issue>Pt 3</issue>), <fpage>799</fpage>&#x02013;<lpage>835</lpage>. <pub-id pub-id-type="doi">10.1093/brain/112.3.799</pub-id><pub-id pub-id-type="pmid">2731030</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name></person-group> (<year>2013</year>). <article-title>Brainnet viewer: a network visualization tool for human brain connectomics</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e68910</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0068910</pub-id><pub-id pub-id-type="pmid">23861951</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>L.</given-names></name><name><surname>Fan</surname><given-names>M.</given-names></name><name><surname>Lin</surname><given-names>L.</given-names></name><name><surname>Sun</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name></person-group> (<year>2017</year>). <article-title>Attractiveness modulates neural processing of infant faces differently in males and females</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>11</volume>:<fpage>551</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2017.00551</pub-id><pub-id pub-id-type="pmid">29184490</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>F.</given-names></name><name><surname>Zeng</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>D.</given-names></name></person-group> (<year>2018a</year>). <article-title>Gender identification of human brain image with a novel 3D descriptor</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
<volume>15</volume>, <fpage>551</fpage>&#x02013;<lpage>561</lpage>. <pub-id pub-id-type="doi">10.1109/tcbb.2015.2448081</pub-id><pub-id pub-id-type="pmid">29610103</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>L.</given-names></name><name><surname>Wei</surname><given-names>X.</given-names></name><name><surname>Shen</surname><given-names>H.</given-names></name><name><surname>Zeng</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>D.</given-names></name></person-group> (<year>2018b</year>). <article-title>Multi-center brain imaging classification using a novel 3D CNN approach</article-title>. <source>IEEE Access</source>
<volume>6</volume>, <fpage>49925</fpage>&#x02013;<lpage>49934</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2868813</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>L.-L.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Hu</surname><given-names>P.</given-names></name><name><surname>Yang</surname><given-names>B.</given-names></name><name><surname>Pu</surname><given-names>W.</given-names></name><name><surname>Shen</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Multi-site diagnostic classification of schizophrenia using discriminant deep learning with functional connectivity MRI</article-title>. <source>EBioMedicine</source>
<volume>30</volume>, <fpage>74</fpage>&#x02013;<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.ebiom.2018.03.017</pub-id><pub-id pub-id-type="pmid">29622496</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Dougherty</surname><given-names>C. C.</given-names></name><name><surname>Baum</surname><given-names>S. A.</given-names></name><name><surname>White</surname><given-names>T</given-names></name><name><surname>Micheal</surname><given-names>A.M.</given-names></name></person-group> (<year>2018</year>). <article-title>Functional connectivity predicts gender: evidence for gender differences in resting brain connectivity</article-title>. <source>Hum. Brain Mapp.</source>
<volume>39</volume>, <fpage>1765</fpage>&#x02013;<lpage>1776</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.23950</pub-id><pub-id pub-id-type="pmid">29322586</pub-id></mixed-citation></ref></ref-list></back></article>