<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">22247755</article-id><article-id pub-id-type="pmc">3256131</article-id><article-id pub-id-type="publisher-id">PONE-D-11-07807</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0026551</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Neuroscience</subject></subj-group><subj-group><subject>Sensory Systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Medicine</subject><subj-group><subject>Mental Health</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and Behavioral Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>The Vividness of Happiness in Dynamic Facial Displays of Emotion</article-title><alt-title alt-title-type="running-head">Dynamic Expression Detection</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Becker</surname><given-names>D. Vaughn</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>&#x0002a;</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Neel</surname><given-names>Rebecca</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Srinivasan</surname><given-names>Narayanan</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref><xref ref-type="aff" rid="aff3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Neufeld</surname><given-names>Samantha</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Kumar</surname><given-names>Devpriya</given-names></name><xref ref-type="aff" rid="aff3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fouse</surname><given-names>Shannon</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>Cognitive Science and Engineering, Arizona State University, Mesa, Arizona, United States of America</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>Department of Psychology, Arizona State University, Tempe, Arizona, United States of America</addr-line>
</aff><aff id="aff3">
<label>3</label>
<addr-line>Centre of Behavioural and Cognitive Sciences, University of Allahabad, Allahabad, India</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Op de Beeck</surname><given-names>Hans P.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University of Leuven, Belgium</aff><author-notes><corresp id="cor1">&#x0002a; E-mail: <email>vaughn.becker@asu.edu</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: DVB RN NS. Performed the experiments: RN SN . Analyzed the data: DVB NS. Contributed reagents/materials/analysis tools: DK SF. Wrote the paper: DVB RN NS SN DK SF.</p></fn></author-notes><pub-date pub-type="collection"><year>2012</year></pub-date><pub-date pub-type="epub"><day>11</day><month>1</month><year>2012</year></pub-date><volume>7</volume><issue>1</issue><elocation-id>e26551</elocation-id><history><date date-type="received"><day>4</day><month>5</month><year>2011</year></date><date date-type="accepted"><day>28</day><month>9</month><year>2011</year></date></history><permissions><copyright-statement>Becker et al.</copyright-statement><copyright-year>2012</copyright-year><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>Rapid identification of facial expressions can profoundly affect social interactions, yet most research to date has focused on static rather than dynamic expressions. In four experiments, we show that when a non-expressive face <italic>becomes</italic> expressive, happiness is detected more rapidly anger. When the change occurs peripheral to the focus of attention, however, dynamic anger is better detected when it appears in the left visual field (LVF), whereas dynamic happiness is better detected in the right visual field (RVF), consistent with hemispheric differences in the processing of approach- and avoidance-relevant stimuli. The central advantage for happiness is nevertheless the more robust effect, persisting even when information of either high or low spatial frequency is eliminated. Indeed, a survey of past research on the visual search for emotional expressions finds better support for a happiness detection advantage, and the explanation may lie in the coevolution of the signal and the receiver.</p></abstract><counts><page-count count="6"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>One of the most cited ideas in the emotion perception literature is that angry faces &#x0201c;pop out&#x0201d; of crowds&#x02014;that they can be detected equally rapidly regardless of the number of other faces in the crowd <xref rid="pone.0026551-Hansen1" ref-type="bibr">[1]</xref>, <xref rid="pone.0026551-hman1" ref-type="bibr">[2]</xref>. From one perspective, this effect makes adaptive sense, because rapidly detecting facial cues of impending interpersonal violence would facilitate the avoidance of said violence. On the other hand, however, the most successful ancestral attackers were likely those that concealed their threatening intentions, which likely would have selected <italic>against</italic> a vivid display of anger <xref rid="pone.0026551-Fridlund1" ref-type="bibr">[3]</xref>. Furthermore, a growing chorus of voices in cognitive science question whether previous demonstrations of an anger superiority effect (ASE) might not reflect idiosyncrasies of the stimuli used in particular experiments <xref rid="pone.0026551-Becker1" ref-type="bibr">[4]</xref>&#x02013;<xref rid="pone.0026551-Hunt1" ref-type="bibr">[6]</xref>. Critically, the stimuli that show the ASE are often static schematic images only slightly more complicated than the iconic smiley face, and are thus susceptible to the criticism that equally simple low-level perceptual features drive the detection asymmetries <xref rid="pone.0026551-Horstmann2" ref-type="bibr">[7]</xref>. For example, if a schematic angry face has more angular features, and if feature detectors of the visual cortex detect angularity faster than curviness, such stimuli would give rise to an apparent anger advantage even in subjects who were not attending to the emotion of the display. Indeed, one stimulus set (used in <xref rid="pone.0026551-hman1" ref-type="bibr">[2]</xref>) has been used to show anger detection advantages in subjects with autism <xref rid="pone.0026551-Ashwin1" ref-type="bibr">[8]</xref> as well as in elderly subjects <xref rid="pone.0026551-Mather1" ref-type="bibr">[9]</xref>, despite the fact that these populations generally have more difficultly processing emotional expressions. This raises the possibility that these results may simply show that basic feature detectors&#x02014;which are more plausibly still intact in these participants&#x02014;are readily activated by the features of the schematic stimulus, and <italic>not</italic> that expressions of anger have been preferentially detected.</p><p>Clearly, support for the claim that angry faces are more efficiently detected requires stimuli that are more ecologically valid. Unfortunately, more realistic faces often give rise to a happiness detection advantage relative to both angry <xref rid="pone.0026551-Becker1" ref-type="bibr">[4]</xref>, <xref rid="pone.0026551-Juth1" ref-type="bibr">[10]</xref> and sad <xref rid="pone.0026551-Srivastava1" ref-type="bibr">[11]</xref> faces. In fact, even schematic happy faces are better identified in a flanker task relative to schematic angry/threatening faces <xref rid="pone.0026551-Srinivasan1" ref-type="bibr">[12]</xref>. In short, the expression detection literature is not only inconsistent in its conclusions, but also rife with criticisms that particular effects arise only from idiosyncrasies of unrealistic and ecologically-invalid visual displays.</p><p>It is surprising, then, that most of this work uses <italic>static</italic> facial displays of emotion&#x02014;the more realistic experience of seeing a face become angry or happy has received almost no attention. Given that effective social communication often depends on the need to quickly detect the dynamic aspects of expressive change, it is important that researchers begin to fill in this gap in the literature. Horstmann &#x00026; Ansorge <xref rid="pone.0026551-Horstmann3" ref-type="bibr">[13]</xref> made a laudable effort in this regard (and failed to find an ASE that was not confounded with other display attributes), but they did not use real faces. Therefore, in this paper, we conduct four experiments using dynamic changes in expression from neutral to happy or angry. We investigated both identification performance with singleton faces as well as a single changing expression in the context of multiple faces (as in visual search tasks). In addition, we also investigated the role of spatial frequency information in the identification of dynamic changes in expression, which has recently been shown to enhance the detectibility of both positive and negative images.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s2a"><title>General Method and Materials</title><p>To explore the detection of dynamic expressions of anger and happiness, we first selected photographs of actors portraying closed-mouth expressions of anger, neutrality and happiness from the MacBrain stimulus set <xref rid="pone.0026551-Tottenham1" ref-type="bibr">[14]</xref>. Using closed-mouth expressions had the advantage of eliminating certain high-contrast features in the lower part of the face, particularly the exposed teeth of a full smile, a feature that has confounded many previous demonstrations of happiness detection advantages (e.g., <xref rid="pone.0026551-Juth1" ref-type="bibr">[10]</xref>). For each actor, we then created a morph-continuum running from the neutral exemplar to each expressive extreme. Presenting static images from this continuum in a rapid sequence (see <xref ref-type="fig" rid="pone-0026551-g001">Figure 1</xref>) generated the appearance of a video clip of a person becoming angry or happy. One advantage of this approach was that the timing of the dynamic change was precisely controlled and was made up of components that changed in a linear fashion, maximizing the realism of the expressive dynamics without sacrificing the ability to equate the onset and offset of the timing.</p><fig id="pone-0026551-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0026551.g001</object-id><label>Figure 1</label><caption><title>Two possible morph sequences, timed to give the impression of a video clip in real time (endpoint images adapted from Tottenham, et al., 2009; the people displayed provided consent for publication of the photos in publications and on the web).</title></caption><graphic xlink:href="pone.0026551.g001"/></fig><p>The faces selected included both African and Caucasian male and female faces. Using the Nimstim naming conventions, we selected individuals 1, 5, 7, 9, 11, 12, 13, 14, 20, 21, 22, 23, 39, 40, 41, and 43. Pilot work confirmed that subjects perceived these to be realistic video recordings&#x02014;none suspected that we were showing them a succession of morphs.</p><p>Across experiments we used the software package DirectRT to display the experimental stimuli and collect reaction times. In all experiments, participants provide informed consent and then sat approximately 60 cm away from a flat screen monitor in a cubicle, and decisions were recorded by pressing keys on the computer keyboard.</p><sec id="s2a1"><title>Ethics Statement</title><p>This research was approved by the Arizona State University Institutional Review Board, and all participants read and signed statements of informed consent.</p></sec></sec><sec id="s2b"><title>Experiment 1</title><p>We first investigated emotion identification of singleton faces to see whether happy or angry emotional changes are detected better.</p><sec id="s2b1"><title>Participants</title><p>Seventy-eight subjects participated, but ten exited the program before all of the data were collected. Only the 68 participants with complete data were retained for the analysis.</p></sec><sec id="s2b2"><title>Procedure</title><p>In this first Experiment, participants were presented with fairly large (6&#x000d7;9 cm) displays of these dynamic expressions, presented one at a time at the center of a computer screen. There were 96 trials, with faces presented in a randomized order. Participants had to rapidly indicate that the face was angry or happy as soon as the expressive change was apparent to them.</p></sec><sec id="s2b3"><title>Experiment 1 Results</title><p>Participants were 37 ms faster identifying the change to a happy expression (M&#x0200a;=&#x0200a;453 ms; SD&#x0200a;=&#x0200a;86 ms) than the change to anger (M&#x0200a;=&#x0200a;490 ms; SD&#x0200a;=&#x0200a;122 ms), <italic>t</italic>(67)&#x0200a;=&#x0200a;3.30, <italic>p</italic>&#x0200a;=&#x0200a;.0016. Thus, in spite of the fact that more muscles are recruited by anger than by happiness, happiness appears to involve the more vivid expressive change.</p><p>Because each face appeared bearing both dynamic expressions over the course of the trials, we also conducted a dependent samples t-test with faces as the unit of analysis, and confirmed that the happiness advantage was significant, <italic>t</italic>(15)&#x0200a;=&#x0200a;4.91, <italic>p</italic>&#x0003c;.0001 (in the experiments that follow, such item analyses are not reported but were routinely consistent with subject analyses). There were no differences for accuracy. Follow-up analyses did not reveal any other factors or interactions that compromised the conclusion that happy expressions were better identified than angry expressions. The happy advantage is consistent with previous research using static images of anger and happiness <xref rid="pone.0026551-Becker1" ref-type="bibr">[4]</xref>, as well as findings using happy and sad faces <xref rid="pone.0026551-Kumar1" ref-type="bibr">[15]</xref>. The results of Experiment 1 raise questions about the studies that claim superiority of detection of negative emotional expressions especially given the more ecological and dynamic nature of the present stimuli.</p></sec></sec><sec id="s2c"><title>Experiment 2</title><p>Experiment 1 presented a promising finding, but the majority of demonstrations that facial displays of anger are more detectible come from searches for expressions in <italic>crowds</italic> of faces <xref rid="pone.0026551-Hansen1" ref-type="bibr">[1]</xref>, <xref rid="pone.0026551-hman1" ref-type="bibr">[2]</xref>. This &#x0201c;visual search&#x0201d; paradigm affords stronger inferences about whether stimuli more quickly draw attention to their location, and so may be better suited to reveal an adaptive advantage for angry faces.</p><sec id="s2c1"><title>Participants</title><p>Although 45 subjects participated, 5 were removed for abnormally low accuracy (less than 2.5 SD below the grand mean) and one participant was eliminated for exceptionally long reaction times (more than 3 SD above the mean).</p></sec><sec id="s2c2"><title>Stimuli and Procedure</title><p>Because we wanted to show each stimulus in each location more than once without vastly increasing the number of trials, for this experiment we used only the four white male stimuli (20, 21, 22, 23) from the first study, which resulted in a total of 192 trials (admittedly, this is a small number of stimuli, but we wanted to ensure that no location by identity interaction could compromise the results; Furthermore, we should note that almost every past demonstration of the ASE used a single angry&#x02014;and generally schematic&#x02014;stimulus, so our four exemplars afford more generalizability than most past studies&#x02014;see <xref rid="pone.0026551-Becker1" ref-type="bibr">[4]</xref> for a more rigorous survey of the previous studies and their shortcomings). Participants were told that they would see either a single face at the central fixation point or four faces (of different identities), one in each quadrant of the screen (each at an equal distance&#x02014;approximately 5 cm&#x02014;from the central fixation point). The task was to attend to and identify the expression as rapidly as possible with a key press. We assessed detection efficiency as a function of whether peripheral faces showed up on the left or the right; past work suggests that this may moderate the detection of positive and negative signals <xref rid="pone.0026551-Srivastava1" ref-type="bibr">[11]</xref>, <xref rid="pone.0026551-Baijal1" ref-type="bibr">[16]</xref>.</p><p>
<xref ref-type="fig" rid="pone-0026551-g002">Figure 2</xref> shows the result of Experiment 2. When faces were presented on their own in the center of the display, we found a 28 ms advantage (SD&#x0200a;=&#x0200a;81 ms) for happy faces, <italic>t</italic>(38)&#x0200a;=&#x0200a;2.13, p&#x0200a;=&#x0200a;.04, replicating experiment 1. When the faces were presented in the periphery, however, there was no expression advantage in the right visual field (<italic>t</italic>&#x0003c;1), while in the left visual field, there was an 85 ms (SD&#x0200a;=&#x0200a;112 ms) <italic>advantage for angry faces</italic>, <italic>t</italic>(38)&#x0200a;=&#x0200a;4.74, p&#x0003c;.0001. It bears emphasizing that these were exactly the same faces, appearing on the left, right and center. Contrasting the detection of the same expression across right and left locations showed that angry faces presented on the left were detected 36 ms (SD&#x0200a;=&#x0200a;100) faster than on the right, <italic>t</italic>(38)&#x0200a;=&#x0200a;2.24, p&#x0200a;=&#x0200a;.031. Moreover, happy faces presented on the left were detected 53 ms more slowly (SD&#x0200a;=&#x0200a;137 ms) than on the right, <italic>t</italic>(38)&#x0200a;=&#x0200a;&#x02212;2.40, <italic>p</italic>&#x0200a;=&#x0200a;.021. In other words, presentation in the LVF improved the detectibility of angry faces while it hurt the detectibility of happy faces.</p><fig id="pone-0026551-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0026551.g002</object-id><label>Figure 2</label><caption><title>Reaction times to correctly identify the stimulus as a function of the type of dynamic expression (becoming angry vs. becoming happy) and its location.</title><p>Standard error bars are included to provide a sense of variability across subjects, but do not correspond to the within-subjects hypothesis tests reported in the text. <italic>Experiment 2 Results.</italic>
</p></caption><graphic xlink:href="pone.0026551.g002"/></fig><p>This lateralization effect is consistent with previous research showing that the right hemisphere of the brain&#x02014;which receives visual input from the LVF&#x02014;shows a specialization for processing information that we want to avoid <xref rid="pone.0026551-Davidson1" ref-type="bibr">[17]</xref>, while the left hemisphere&#x02014;receiving input from the RVF&#x02014;is specialized for approach-related emotions and stimuli (note here that while the experience of anger may be an approach&#x02013;related emotion <xref rid="pone.0026551-Coan1" ref-type="bibr">[18]</xref>, the angry <italic>face</italic> is a stimulus that we likely want to avoid).</p></sec></sec><sec id="s2d"><title>Experiment 3</title><p>While these peripheral results are intriguing, location is confounded with perceptual load, because the peripheral faces only appear within 4-face crowds, while the central faces appear alone. To verify that these lateralization effects generalized to single presentations, Experiment 3 was conducted.</p><sec id="s2d1"><title>Participants</title><p>Although 59 subjects participated, 2 were removed for abnormally low accuracy (less than 2.5 SD below the grand mean) and one participant was eliminated for exceptionally long reaction times (more than 3 SD above the mean).</p></sec><sec id="s2d2"><title>Stimuli and Procedure</title><p>This study replicated the design of Experiment 2, but also included trials in which a single neutral face appeared in one of the peripheral locations, which increased the number of trials to 240. Upon appearing, the peripheral expressions immediately began to transform to anger or happiness (i.e. 35 ms after the neutral face onset, it was replaced by a slightly expressive image). This was necessary because any single brief onset stimulus automatically grabs attention <xref rid="pone.0026551-Muller1" ref-type="bibr">[19]</xref>, and we wanted to ensure that the expressive dynamics were underway before the person made a saccade to the stimulus location. We also reincorporated the White female stimuli used in study 1 (images 1, 5, 7, 9), to broaden the selection of items and ensure that expression lateralization effect was not contingent on masculine gender. Note however, that every face and expression combination appeared in every location for every subject.</p></sec><sec id="s2d3"><title>Experiment 3 Results</title><p>Experiment 3 again revealed a 36 ms (SD&#x0200a;=&#x0200a;70 ms) advantage for detection of happy faces vs. angry faces when targets were presented at the center of the display, <italic>t</italic>(55)&#x0200a;=&#x0200a;3.82, p&#x0200a;=&#x0200a;.0003. The lateralization benefit was again seen for angry targets, which showed a significant left-side advantage in both crowds (M&#x0200a;=&#x0200a;27 ms, SD&#x0200a;=&#x0200a;106 ms), <italic>t</italic>(55)&#x0200a;=&#x0200a;1.88, p<sub>one-tailed</sub>&#x0200a;=&#x0200a;.032, and when appearing on their own (M&#x0200a;=&#x0200a;40 ms, SD&#x0200a;=&#x0200a;113 ms) <italic>t</italic>(55)&#x0200a;=&#x0200a;2.66, p<sub>one-tailed</sub>&#x0200a;=&#x0200a;.005. Peripheral happy targets showed evidence of a nonsignificant trend for a right-side advantage when embedded in crowds, <italic>t</italic>&#x0003c;1, but did show a significant advantage when presented peripherally on their own (M&#x0200a;=&#x0200a;30 ms, SD&#x0200a;=&#x0200a;129 ms) <italic>t</italic>(55)&#x0200a;=&#x0200a;1.72, p<sub>one-tailed</sub>&#x0200a;=&#x0200a;.046.</p><p>These results suggest that perceptual load cannot account for the lateralization effects. We should be wary of adaptive explanations for this result, however, because any plausible adaptation for detecting peripheral anger (or happiness) should have produced equivalently fast detections regardless of location. This result instead provides new evidence for a hemispheric asymmetry in approach vs. avoidance processing. In contrast, the happiness detection advantage at the central (foveated) location may suggest a legitimate adaption at the level of the signal design: Happy faces have a form that is more detectible. Because this form emerged via natural selection, it suggests that the facial display of happiness is a social signal that wants to be seen, and seen rapidly, and accordingly, it has appropriated signaling qualities that make use of basic feature detectors in order to maximize the likelihood that its prosocial message gets through. It follows then, that the happiness advantage should be robust to stimulus degradation.</p></sec></sec><sec id="s2e"><title>Experiments 4a &#x00026; 4b</title><p>One factor that has been shown to differentially influence the detection of positive and negative stimuli is the spatial frequency of the information presented <xref rid="pone.0026551-Mermillod1" ref-type="bibr">[20]</xref>. Fourier analysis can be used to decompose visual images into their component spatial frequencies, and consequently filter out all of the high frequency information&#x02014;the sharp lines and contours that convey much of the detail of images&#x02014;leaving a low pass filtered image that is a very blurry replica of the original (see <xref ref-type="fig" rid="pone-0026551-g003">Figure 3</xref>). Researchers have found that threatening images show a detection advantage that persists after Low Pass Filtering (LPF) <xref rid="pone.0026551-Mermillod1" ref-type="bibr">[20]</xref>, and have claimed that a fast subcortical route to the emotional centers of the brain processes this coarse threat-relevant information, an adaptation that enables us to rapidly respond to threat.</p><fig id="pone-0026551-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0026551.g003</object-id><label>Figure 3</label><caption><title>Reaction times to correctly identify the stimulus as a function of the type of dynamic expression (becoming angry vs. becoming happy), its location, and whether it was high or low pass filtered.</title><p>Standard error bars are included to provide a sense of variability across subjects, but do not correspond to the within-subjects hypothesis tests reported in the text.</p></caption><graphic xlink:href="pone.0026551.g003"/></fig><p>Recently we (DK &#x00026; NS) have shown that the removal of low spatial frequency information significantly decreased the speed at which static happy expressions were identified <xref rid="pone.0026551-Kumar1" ref-type="bibr">[15]</xref>. In contrast, filtering out low frequency information with a High Pass Filter (HPF) benefits the detectibility of negative (sad) expressions relative to happy expressions. HPF may therefore wipe out the happy face advantage at central locations. If, however, the happy expression evolved to vividly and unambiguously signal positive affordances, we might expect to see the advantage for dynamic happiness persisting across both low and high pass filtering.</p><sec id="s2e1"><title>Participants</title><p>Experiment 4A included 84 subjects, but 8 were removed for low accuracy (less than 2.5 SD below the grand mean). Experiment 4B included 86 participants, but 4 were removed for error rates&#x0003c;&#x02212;2.5 SD below the grand mean and 1 was removed for mean current reaction times &#x0003e;3 SD above the mean.</p></sec><sec id="s2e2"><title>Stimuli and Procedure</title><p>For the image filtering in this pair of experiments, we used a Gaussian filter. The low pass filter cutoff was 3.33 cycles per degree, which amounted to 8 cycles per face; the high pass filter cutoff was 13.13 cycles per degree or 32 cycles per face.</p><p>The design consisted of two replications of Experiment 3, substituting dynamic LPF faces in Experiment 4a and dynamic HPF faces in Experiment 4b&#x02014;in all other respects the designs were exactly the same.</p></sec><sec id="s2e3"><title>Experiment 4a &#x00026; 4b Results</title><p>The results in this pair of experiments provided consistent support for the happy face advantage, and showed a robust lateralization effect for happy faces as well (we collapse over single vs. multiple peripheral conditions, for ease of interpretation). In Experiment 4a, centrally presented LPF dynamic expressions of happiness were detected 60 ms faster (SD&#x0200a;=&#x0200a;85) than their similarly filtered angry counterparts, <italic>t</italic>(75)&#x0200a;=&#x0200a;6.18, p&#x0003c;.0001. There was no difference in the speed with which angry faces were detected when they appeared on the left vs. the right, <italic>t</italic>&#x0003c;1. There was, however, a big reaction time difference for happy faces, with those on the right detected 101 ms faster (SD&#x0200a;=&#x0200a;136 ms) than those on the left, <italic>t</italic>(75)&#x0200a;=&#x0200a;6.53, p&#x0003c;.0001. This happy face asymmetry consists of both a left-side cost&#x02014;relative to LVF anger , a 69 ms slow-down (SD&#x0200a;=&#x0200a;185 ms) in response times, <italic>t</italic>(75)&#x0200a;=&#x0200a;3.28, p&#x0003c;.001&#x02014;and a right side benefit&#x02014;relative to RVF anger, a 35 ms facilitation (SD&#x0200a;=&#x0200a;106 ms) in response times, <italic>t</italic>(75)&#x0200a;=&#x0200a;2.90, p&#x0200a;=&#x0200a;.002.</p><p>In Experiment 4b, centrally presented HPF dynamic expressions of happiness were detected 68 ms faster (SD&#x0200a;=&#x0200a;176 ms) than their similarly filtered angry counterparts, <italic>t</italic>(80)&#x0200a;=&#x0200a;3.49, p&#x0003c;.001. There was no difference in the speed with which angry faces were detected when they appeared on the left vs. the right, <italic>t</italic>&#x0003c;1. There was again a big reaction time difference for happy faces, with those on the right detected 120 ms faster (SD&#x0200a;=&#x0200a;251 ms) than those on the left, <italic>t</italic>(80)&#x0200a;=&#x0200a;3.7, p&#x0003c;.001. Again, this happy face asymmetry is a consequence of both a left side cost&#x02014;relative to LVF anger, a 99 ms slow-down (SD&#x0200a;=&#x0200a;316 ms) in response times, <italic>t</italic>(80)&#x0200a;=&#x0200a;2.83, p&#x0200a;=&#x0200a;.006&#x02014;as well as the suggestion of a right side benefit&#x02014;relative to RVF anger, a 39 ms facilitation (SD&#x0200a;=&#x0200a;106 ms) in response times, <italic>t</italic>(75)&#x0200a;=&#x0200a;1.54, p&#x0200a;=&#x0200a;.125. The lateralization advantage for angry expression compared to the happy expression in the left visual field is consistent with other findings that show advantage for negative emotions in the LVF/right hemisphere <xref rid="pone.0026551-Baijal1" ref-type="bibr">[16]</xref>. The laterality effect is much more pronounced in the right hemisphere compared to the left hemisphere.</p></sec></sec></sec><sec id="s3"><title>Results and Discussion</title><p>The present research produced two principal effects. First and foremost, the consistent advantage for detecting happiness at the focus of attention does appear to speak to the adaptive properties of this signal. Indeed, the fact that these advantages persist even for LPF and HPF images suggests that the dynamics of the happy expression have evolved to better appropriate the processing efficiencies of the human visual system at a number of different levels. For example, becoming happy involves an expansion of the face while becoming angry involves a contraction, and psychophysical work has determined that expansion is more efficiently detected than contraction <xref rid="pone.0026551-Takeuchi1" ref-type="bibr">[21]</xref> (perhaps because expansion is something that approaching, <italic>looming</italic> objects do, see <xref rid="pone.0026551-Schiff1" ref-type="bibr">[22]</xref>). To be clear, we are not postulating that the human perceptual apparatus evolved to be on the look-out for happiness (although there may be a perceptual readiness for the receiver to detect it as well), but rather that the form of the expression evolved to appropriate pre-existing efficiencies in the visual system. Although facial displays of emotion are ancient signals, they are not eternal&#x02014;human facial expressions have evolved as signals in a coevolutionary &#x0201c;arms race&#x0201d; with perceptual receivers, and here both the signaler and the receiver benefit from the rapid detection of prosocial (or submissive) intentions. Indeed, even in chimpanzees, bared-teeth displays are now thought to communicate benign intent and function to reduce uncertainty rapidly in both aggressive and affiliative interactions <xref rid="pone.0026551-Waller1" ref-type="bibr">[23]</xref>.</p><p>We should also note that the changes in expression in our dynamic images occur fairly rapidly and it is plausible that the pathways that are sensitive to high temporal frequencies or changes would respond to this change in expression. In terms of visual pathways, the magnocellular pathway is more sensitive to high temporal frequencies and also low spatial frequencies. The consistent advantage for dynamic happy faces indicates that they might be subserved by the magnocellular pathway. This is also consistent with experimental results using static faces which indicate the importance of low spatial frequencies in detecting happy expression <xref rid="pone.0026551-Kumar1" ref-type="bibr">[15]</xref>. While this is hypothetical, it provides a plausible neural substrate for a happy face advantage that may have evolved for better social communication.</p><p>The second principal finding is that whether these expressions appear on the left or the right has a significant impact on their detectibility. This is consistent with properties of hemispheric specialization that have already been suggested in the literature <xref rid="pone.0026551-Davidson1" ref-type="bibr">[17]</xref>, but ultimately does not reveal much about the adaptive design of expression perception. However, the results are fairly consistent with other findings using static emotional faces indicating a preference for negative expressions by the left hemisphere. We also find a bias for happy expression in the left hemisphere. If there is evolutionary advantage for detecting happy expressions (in social communication), then perhaps that might have become linked to the language specialization in the left hemisphere. It is also possible that approach emotions are linked with speech acts and hence might underlie a left hemispheric bias.</p><sec id="s3a"><title>The vividness of happy facial expressions in the broader literature</title><p>Our results may come as a surprise to many, for as we noted at the outset, the belief that angry faces are efficiently&#x02014;and even preattentively&#x02014;detected is widespread. Indeed, two of the more prominent studies <xref rid="pone.0026551-Hansen1" ref-type="bibr">[1]</xref> &#x00026; <xref rid="pone.0026551-hman1" ref-type="bibr">[2]</xref> have each been cited over 400 times. A careful examination of the literature supporting the ASE, however, shows a problematic tendency to rely on simple schematic line drawings of anger and/or single target faces used repeatedly over hundreds of trials; both of these design features (or rather flaws) make it likely that participants learn to use idiosyncratic stimulus elements to perform the detection task without emotion perception coming into play at all. Horstmann and colleagues (e.g., <xref rid="pone.0026551-Horstmann3" ref-type="bibr">[13]</xref>) have done an admirable job of experimentally demonstrating the shortcomings of various schematic stimuli. In contrast, when a variety of more realistic and ecologically valid photographic images are used and participants actually have to attend holistically to the emotional expressions to perform the task, happiness is more rapidly and accurately detected (see <xref rid="pone.0026551-Becker1" ref-type="bibr">[4]</xref>, for both a review of the literature and experimental evidence for this contention). We therefore feel confident that when the empirical findings are weighted by the ecological validity of the experimental designs, there is overwhelming evidence that happy faces are detected more efficiently than angry faces. We call this a <italic>vividness</italic> effect because we believe that the signal has evolved a detectable form in the same way that, for example, the black and yellow stripes of a hornet evolved to make use the perceptual mechanisms of potential predators and other threats. But we should be careful to note that these vividness effects occur early in perception (and possibly without the conscious application of attention), and that at later stages of information processing we may well see advantages for angry faces. For example, once seen and attended, angry faces may hold on to that attention and resist attentional disengagement (e.g. <xref rid="pone.0026551-Fox1" ref-type="bibr">[24]</xref>). Thus, while happy faces are vividly detected, angry faces may be quite vivid once attended and in memory.</p></sec><sec id="s3b"><title>Conclusion</title><p>These results should compel cognitive scientists to begin thinking about what facial expressions evolved to signal, and the costs and benefits of the signals' detectibility. Expressions of happiness convey approachability, friendship, possibilities for affiliation, trade and coalition building, and the sight of a happy face can de-escalate tension, all of which has caused the facial display of happiness to converge on salient and detectible forms (indeed, Hagar and Ekman <xref rid="pone.0026551-Hager1" ref-type="bibr">[25]</xref> made a similar case about happiness over 30 years ago). Expressions of anger, on the other hand, have less reason to be salient at the earliest stages of visual perception. Sometimes anger communicates frustration and strong disapproval with the aim of holding attention. But if the expression accompanies a desire to attack another, a visually salient facial display of rage is only adaptive if its perception causes the target to back down; if there is no opportunity to preempt physical conflict, if one must aggress against another who is relatively equal in power/ability, then concealing one's intention&#x02014;masking one's anger&#x02014;is more beneficial <xref rid="pone.0026551-Fridlund1" ref-type="bibr">[3]</xref>. It is therefore difficult to make the case that angry facial expressions would have evolved a form that could draw attention to their location, because the advantage to the perceiver is balanced by the cost to the displayer, and so the selective pressure would not promote visual salience. Communicating happiness, on the other hand, benefits the perceiver and the displayer, and would be expected to converge on forms and dynamics that are clearly and rapidly detected.</p><p>The present results thus exemplify a more principled approach to emotional signal detection that takes into account the ecologically valid form of the signal as well as the design of the receiver. It also represents one of the first explorations of the detectibility of dynamic facial expressions of emotion. We hope these results spur similar advances in theorizing and research, because until cognitive science wrestles with the coevolved nature of social signals and their perceivers, it provides an incomplete picture of why the mind works the way that it does.</p></sec></sec></body><back><fn-group><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding: </bold>This work was supported by National Science Foundation grant(BCS-0642873) to Vaughn Becker. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></fn></fn-group><ref-list><title>References</title><ref id="pone.0026551-Hansen1"><label>1</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hansen</surname><given-names>CH</given-names></name><name><surname>Hansen</surname><given-names>RD</given-names></name></person-group>
<year>1988</year>
<article-title>Finding the face in the crowd: an anger superiority effect.</article-title>
<source>Journal of Personality and Social Psychology</source>
<volume>54</volume>
<fpage>917</fpage>
<lpage>924</lpage>
<pub-id pub-id-type="pmid">3397866</pub-id></element-citation></ref><ref id="pone.0026551-hman1"><label>2</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>&#x000d6;hman</surname><given-names>A</given-names></name><name><surname>Lundqvist</surname><given-names>D</given-names></name><name><surname>Esteves</surname><given-names>F</given-names></name></person-group>
<year>2001</year>
<article-title>The face in the crowd revisited: a threat advantage with schematic stimuli.</article-title>
<source>Journal of Personality and Social Psychology</source>
<volume>80</volume>
<fpage>381</fpage>
<lpage>396</lpage>
<pub-id pub-id-type="pmid">11300573</pub-id></element-citation></ref><ref id="pone.0026551-Fridlund1"><label>3</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Fridlund</surname><given-names>AJ</given-names></name></person-group>
<year>1994</year>
<source>Human facial expression: An evolutionary view</source>
<publisher-loc>San Diego, CA</publisher-loc>
<publisher-name>Academic Press</publisher-name>
</element-citation></ref><ref id="pone.0026551-Becker1"><label>4</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Becker</surname><given-names>DV</given-names></name><name><surname>Anderson</surname><given-names>US</given-names></name><name><surname>Mortensen</surname><given-names>CR</given-names></name><name><surname>Neufeld</surname><given-names>S</given-names></name><name><surname>Neel</surname><given-names>R</given-names></name></person-group>
<year>2011</year>
<article-title>The face in the crowd effect unconfounded: Happy faces, not angry faces, are more efficiently detected in the visual search task.</article-title>
<source>Journal of Experimental Psychology: General</source>
<volume>140</volume>
<fpage>637</fpage>
<lpage>59</lpage>
<pub-id pub-id-type="pmid">21744984</pub-id></element-citation></ref><ref id="pone.0026551-Horstmann1"><label>5</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Horstmann</surname><given-names>G</given-names></name></person-group>
<year>2009</year>
<article-title>Visual search for schematic affective faces: Stability and variability of search slopes with different instances.</article-title>
<source>Cognition and Emotion</source>
<volume>23</volume>
<fpage>355</fpage>
<lpage>379</lpage>
</element-citation></ref><ref id="pone.0026551-Hunt1"><label>6</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hunt</surname><given-names>AR</given-names></name><name><surname>Cooper</surname><given-names>RM</given-names></name><name><surname>Hungr</surname><given-names>C</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group>
<year>2007</year>
<article-title>The effect of emotional faces on eye movements and attention.</article-title>
<source>Visual Cognition</source>
<volume>15</volume>
<fpage>513</fpage>
<lpage>531</lpage>
</element-citation></ref><ref id="pone.0026551-Horstmann2"><label>7</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Horstmann</surname><given-names>G</given-names></name></person-group>
<year>2007</year>
<article-title>Preattentive face processing: What do visual search experiments with schematic faces tell us?</article-title>
<source>Visual Cognition</source>
<volume>15</volume>
<fpage>799</fpage>
<lpage>833</lpage>
</element-citation></ref><ref id="pone.0026551-Ashwin1"><label>8</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ashwin</surname><given-names>C</given-names></name><name><surname>Wheelwright</surname><given-names>SJ</given-names></name><name><surname>Baron-Cohen</surname><given-names>S</given-names></name></person-group>
<year>2006</year>
<article-title>Finding a face in the crowd: Testing the anger superiority effect in autism.</article-title>
<source>Brain and Cognition</source>
<volume>61</volume>
<fpage>78</fpage>
<lpage>95</lpage>
<pub-id pub-id-type="pmid">16455174</pub-id></element-citation></ref><ref id="pone.0026551-Mather1"><label>9</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mather</surname><given-names>M</given-names></name><name><surname>Knight</surname><given-names>M</given-names></name></person-group>
<year>2006</year>
<article-title>Angry faces get noticed quickly: Threat detection is not impaired among older adults.</article-title>
<source>Journal of Gerontology: Psychological Sciences</source>
<volume>61</volume>
<fpage>54</fpage>
<lpage>57</lpage>
</element-citation></ref><ref id="pone.0026551-Juth1"><label>10</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Juth</surname><given-names>P</given-names></name><name><surname>Lundqvist</surname><given-names>D</given-names></name><name><surname>Karlsson</surname><given-names>A</given-names></name><name><surname>&#x000d6;hman</surname><given-names>A</given-names></name></person-group>
<year>2005</year>
<article-title>Looking for foes and friends: Perceptual and emotional factors when finding a face in the crowd.</article-title>
<source>Emotion</source>
<volume>5</volume>
<fpage>379</fpage>
<lpage>395</lpage>
<pub-id pub-id-type="pmid">16366743</pub-id></element-citation></ref><ref id="pone.0026551-Srivastava1"><label>11</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>P</given-names></name><name><surname>Srinivasan</surname><given-names>N</given-names></name></person-group>
<year>2010</year>
<article-title>Time course of visual attention with emotional faces.</article-title>
<source>Attention, Perception, &#x00026; Psychophysics</source>
<volume>72</volume>
<fpage>369</fpage>
<lpage>377</lpage>
</element-citation></ref><ref id="pone.0026551-Srinivasan1"><label>12</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>N</given-names></name><name><surname>Baijal</surname><given-names>S</given-names></name><name><surname>Khetrapal</surname><given-names>N</given-names></name></person-group>
<year>2010</year>
<article-title>Effect of emotions on selective attention and control.</article-title>
<person-group person-group-type="editor"><name><surname>Srinivasan</surname><given-names>N</given-names></name><name><surname>Kar</surname><given-names>B</given-names></name><name><surname>Pandey</surname><given-names>J</given-names></name></person-group>
<source>Advances in Cognitive Science: Volume 2</source>
<fpage>132</fpage>
<lpage>149</lpage>
<comment>New Delhi, India: Sage Publications</comment>
</element-citation></ref><ref id="pone.0026551-Horstmann3"><label>13</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Horstmann</surname><given-names>G</given-names></name><name><surname>Ansorge</surname><given-names>U</given-names></name></person-group>
<year>2009</year>
<article-title>Visual search for facial expressions of emotions: A comparison of dynamic and static faces.</article-title>
<source>Emotion</source>
<volume>9</volume>
<fpage>29</fpage>
<lpage>38</lpage>
<pub-id pub-id-type="pmid">19186914</pub-id></element-citation></ref><ref id="pone.0026551-Tottenham1"><label>14</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tottenham</surname><given-names>N</given-names></name><name><surname>Tanaka</surname><given-names>J</given-names></name><name><surname>Leon</surname><given-names>AC</given-names></name><name><surname>McCarry</surname><given-names>T</given-names></name><name><surname>Nurse</surname><given-names>M</given-names></name><etal/></person-group>
<year>2009</year>
<article-title>The NimStim set of facial expressions: judgments from untrained research participants.</article-title>
<source>Psychiatry Research</source>
<volume>168</volume>
<fpage>242</fpage>
<lpage>249</lpage>
<pub-id pub-id-type="pmid">19564050</pub-id></element-citation></ref><ref id="pone.0026551-Kumar1"><label>15</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kumar</surname><given-names>D</given-names></name><name><surname>Srinivasan</surname><given-names>N</given-names></name></person-group>
<year>2011</year>
<article-title>Emotion perception is mediated by spatial frequency content.</article-title>
<source>Emotion</source>
<volume>11</volume>
<fpage>1144</fpage>
<lpage>1151</lpage>
<pub-id pub-id-type="pmid">21942699</pub-id></element-citation></ref><ref id="pone.0026551-Baijal1"><label>16</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Baijal</surname><given-names>S</given-names></name><name><surname>Srinivasan</surname><given-names>N</given-names></name></person-group>
<year>2011</year>
<article-title>Emotional and hemispheric asymmetries in shifts of attention: An ERP study.</article-title>
<source>Cognition &#x00026; Emotion</source>
<volume>25</volume>
<fpage>280</fpage>
<lpage>294</lpage>
<pub-id pub-id-type="pmid">21432671</pub-id></element-citation></ref><ref id="pone.0026551-Davidson1"><label>17</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Davidson</surname><given-names>RJ</given-names></name></person-group>
<year>1988</year>
<article-title>EEG measures of cerebral asymmetry: Conceptual and methodological issues.</article-title>
<source>International Journal of Neuroscience</source>
<volume>39</volume>
<fpage>71</fpage>
<lpage>89</lpage>
<pub-id pub-id-type="pmid">3290140</pub-id></element-citation></ref><ref id="pone.0026551-Coan1"><label>18</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Coan</surname><given-names>JA</given-names></name><name><surname>Allen</surname><given-names>JJB</given-names></name></person-group>
<year>2003</year>
<article-title>Frontal EEG asymmetry and the behavioral activation and inhibition systems.</article-title>
<source>Psychophysiology</source>
<volume>40</volume>
<fpage>106</fpage>
<lpage>114</lpage>
<pub-id pub-id-type="pmid">12751808</pub-id></element-citation></ref><ref id="pone.0026551-Muller1"><label>19</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Muller</surname><given-names>HJ</given-names></name><name><surname>Rabbit</surname><given-names>PMA</given-names></name></person-group>
<year>1989</year>
<article-title>Reflexive and voluntary orienting of visual attention: Time course of activation and resistance to interruption.</article-title>
<source>Journal of Experimental Psychology: Human Perception and Performance</source>
<volume>15</volume>
<fpage>315</fpage>
<lpage>330</lpage>
<pub-id pub-id-type="pmid">2525601</pub-id></element-citation></ref><ref id="pone.0026551-Mermillod1"><label>20</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mermillod</surname><given-names>M</given-names></name><name><surname>Droit-Volet</surname><given-names>S</given-names></name><name><surname>Devaux</surname><given-names>D</given-names></name><name><surname>Schaefer</surname><given-names>A</given-names></name><name><surname>Vermeulen</surname><given-names>N</given-names></name></person-group>
<year>2010</year>
<article-title>Are Coarse Scales Sufficient for Fast Detection of Visual Threat?</article-title>
<source>Psychological Science</source>
<volume>21</volume>
<fpage>1429</fpage>
<lpage>1437</lpage>
<pub-id pub-id-type="pmid">20817781</pub-id></element-citation></ref><ref id="pone.0026551-Takeuchi1"><label>21</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Takeuchi</surname><given-names>T</given-names></name></person-group>
<year>1997</year>
<article-title>Visual search of expansion and contraction.</article-title>
<source>Vision Research</source>
<volume>37</volume>
<fpage>2083</fpage>
<lpage>2090</lpage>
<pub-id pub-id-type="pmid">9327056</pub-id></element-citation></ref><ref id="pone.0026551-Schiff1"><label>22</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Schiff</surname><given-names>W</given-names></name><name><surname>Caviness</surname><given-names>JA</given-names></name><name><surname>Gibson</surname><given-names>JJ</given-names></name></person-group>
<year>1962</year>
<article-title>Persistent fear responses in rhesus monkeys to the optical stimulus of &#x02018;looming&#x02019;.</article-title>
<source>Science</source>
<volume>136</volume>
<fpage>982</fpage>
<lpage>983</lpage>
<pub-id pub-id-type="pmid">14498362</pub-id></element-citation></ref><ref id="pone.0026551-Waller1"><label>23</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Dunbar</surname><given-names>RIM</given-names></name></person-group>
<year>2005</year>
<article-title>Differential behavioural effects of silent bared teeth display and relaxed open mouth display in chimpanzees (Pan troglodytes).</article-title>
<source>Ethology</source>
<volume>111</volume>
<fpage>129</fpage>
<lpage>142</lpage>
</element-citation></ref><ref id="pone.0026551-Fox1"><label>24</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Russo</surname><given-names>R</given-names></name><name><surname>Dutton</surname><given-names>K</given-names></name></person-group>
<year>2002</year>
<article-title>Evidence for delayed disengagement from emotional faces.</article-title>
<source>Cognition and emotion</source>
<volume>16</volume>
<fpage>355</fpage>
<lpage>379</lpage>
<pub-id pub-id-type="pmid">18273395</pub-id></element-citation></ref><ref id="pone.0026551-Hager1"><label>25</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hager</surname><given-names>JC</given-names></name><name><surname>Ekman</surname><given-names>P</given-names></name></person-group>
<year>1979</year>
<article-title>Long-distance transmission of facial affect signals.</article-title>
<source>Ethology and Sociobiology</source>
<volume>1</volume>
<fpage>77</fpage>
<lpage>82</lpage>
</element-citation></ref></ref-list></back></article>