<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31398946</article-id><article-id pub-id-type="pmc">6719009</article-id><article-id pub-id-type="doi">10.3390/s19163470</article-id><article-id pub-id-type="publisher-id">sensors-19-03470</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Improving Stockline Detection of Radar Sensor Array Systems in Blast Furnaces Using a Novel Encoder&#x02013;Decoder Architecture</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Xiaopeng</given-names></name><xref ref-type="aff" rid="af1-sensors-19-03470">1</xref><xref ref-type="aff" rid="af2-sensors-19-03470">2</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yan</given-names></name><xref ref-type="aff" rid="af1-sensors-19-03470">1</xref><xref ref-type="aff" rid="af2-sensors-19-03470">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Meng</given-names></name><xref ref-type="aff" rid="af1-sensors-19-03470">1</xref><xref ref-type="aff" rid="af3-sensors-19-03470">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4113-6994</contrib-id><name><surname>Chen</surname><given-names>Xianzhong</given-names></name><xref ref-type="aff" rid="af1-sensors-19-03470">1</xref><xref ref-type="aff" rid="af2-sensors-19-03470">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2288-7901</contrib-id><name><surname>Li</surname><given-names>Jiangyun</given-names></name><xref ref-type="aff" rid="af1-sensors-19-03470">1</xref><xref ref-type="aff" rid="af2-sensors-19-03470">2</xref><xref rid="c1-sensors-19-03470" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-19-03470"><label>1</label>School of Automation &#x00026; Electrical Engineering, University of Science and Technology Beijing, Beijing 100083, China</aff><aff id="af2-sensors-19-03470"><label>2</label>Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, Beijing 100083, China</aff><aff id="af3-sensors-19-03470"><label>3</label>Instrument Science &#x00026; Technology, Beijing 100083, China</aff><author-notes><corresp id="c1-sensors-19-03470"><label>*</label>Correspondence: <email>leejy@ustb.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>08</day><month>8</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>8</month><year>2019</year></pub-date><volume>19</volume><issue>16</issue><elocation-id>3470</elocation-id><history><date date-type="received"><day>16</day><month>6</month><year>2019</year></date><date date-type="accepted"><day>05</day><month>8</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>The stockline, which describes the measured depth of the blast furnace (BF) burden surface with time, is significant to the operator executing an optimized charging operation. For the harsh BF environment, noise interferences and aberrant measurements are the main challenges of stockline detection. In this paper, a novel encoder&#x02013;decoder architecture that consists of a convolution neural network (CNN) and a long short-term memory (LSTM) network is proposed, which suppresses the noise interferences, classifies the distorted signals, and regresses the stockline in a learning way. By leveraging the LSTM, we are able to model the longer historical measurements for robust stockline tracking. Compared to traditional hand-crafted denoising processing, the time and efforts could be greatly saved. Experiments are conducted on an actual eight-radar array system in a blast furnace, and the effectiveness of the proposed method is demonstrated on the real recorded data.</p></abstract><kwd-group><kwd>sensor array system</kwd><kwd>blast furnace</kwd><kwd>stockline detection</kwd><kwd>radar signal processing</kwd><kwd>LSTM</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-19-03470"><title>1. Introduction</title><p>Blast furnaces (BFs) are the key reactors of iron and steel smelting, which consumes about 70% of the energy (i.e., coal, electricity, fuel oil, and natural gas) in the steel-making process [<xref rid="B1-sensors-19-03470" ref-type="bibr">1</xref>,<xref rid="B2-sensors-19-03470" ref-type="bibr">2</xref>]. In iron-making, solid raw materials, e.g., iron ore, coke, limestone, are violently burned and consumed from time to time, and the charging operation needs to be executed by accurately estimating the current depth of the burden surface. Burden surface monitoring is crucial to ensuring high quality steel production as it affects the optimization of charging operations and the utilization ratio of heat and chemical energy [<xref rid="B3-sensors-19-03470" ref-type="bibr">3</xref>].</p><p>The measurement of the BF burden surface is a longstanding and challenging task because of the harsh in-furnace environment, which is lightless, high-pressure, high-dust, high-humidity, and extremely high in temperature [<xref rid="B4-sensors-19-03470" ref-type="bibr">4</xref>]. With the advantage of its contact-free nature, high precision, and high penetrability, the frequency-modulated continuous wave (FMCW) radar has become widely popular with its pointwise measuring method to locate the burden level in real time [<xref rid="B5-sensors-19-03470" ref-type="bibr">5</xref>]. As shown in <xref ref-type="fig" rid="sensors-19-03470-f001">Figure 1</xref>, the employed eight-radar array system is introduced, where the radar sensors are placed scientifically in consideration of the practical industrial field. The burden surface is bilaterally symmetrical because of the uniform rotation of the charging chute, so only half of the surface needs to be measured. The trajectory of the measuring points taken by the radar over time is termed the stockline and reflects the depth of the burden surface at every timestamp. Improving the precision of stockline detection, the operator can optimize the burden distribution in a way that is conducive to a stable iron-making process.</p><p>Noise interferences are crucial issues in the BF radar system. The radar signals usually suffer heavy low-frequency noises due to strong electromagnetic scattering. At times, the shadows of the rotating chute and falling materials, the influence of natural deviations, instrument errors, fraudulent behaviors, and other unexpected interferences result in the appearance of distorted signals and make stockline detection more challenging.</p><p>In the study of BFs, a variety of algorithms of noisy data processing have been developed so far, covering principal component analysis [<xref rid="B6-sensors-19-03470" ref-type="bibr">6</xref>], support vector machines (SVM) [<xref rid="B7-sensors-19-03470" ref-type="bibr">7</xref>,<xref rid="B8-sensors-19-03470" ref-type="bibr">8</xref>], neural network models [<xref rid="B9-sensors-19-03470" ref-type="bibr">9</xref>], extreme learning machines [<xref rid="B10-sensors-19-03470" ref-type="bibr">10</xref>], etc. It is acknowledged by remarkable previous works that the noises occurring within the BF reactor are still an extremely complex issue. The existing works of stockline detection are reviewed [<xref rid="B5-sensors-19-03470" ref-type="bibr">5</xref>,<xref rid="B11-sensors-19-03470" ref-type="bibr">11</xref>,<xref rid="B12-sensors-19-03470" ref-type="bibr">12</xref>,<xref rid="B13-sensors-19-03470" ref-type="bibr">13</xref>,<xref rid="B14-sensors-19-03470" ref-type="bibr">14</xref>,<xref rid="B15-sensors-19-03470" ref-type="bibr">15</xref>]. Generally, peak searching (PS) is used to extract the stockline, which corresponds to the maximum amplitude component in the signal spectrum [<xref rid="B12-sensors-19-03470" ref-type="bibr">12</xref>]. In the case of actual BF environments, the collected signals are usually corrupted with noises that bury the target features under fraudulent peaks and bring outliers to the stockline. A simple approach to eliminate the noise influence is threshold clipping, of which the adaptive threshold method is an example [<xref rid="B13-sensors-19-03470" ref-type="bibr">13</xref>], where the noises outside the thresholds would be ignored rudely. Effective filtering methods are being developed to exclude the empirical interval of noise distribution, e.g., the infinite impulse response (IIR) filter used in [<xref rid="B14-sensors-19-03470" ref-type="bibr">14</xref>] and the windowed finite impulse response (FIR) filter used in [<xref rid="B16-sensors-19-03470" ref-type="bibr">16</xref>]. Ongoing efforts are being made to improve noise robustness by taking historical observations into consideration, e.g., Kalman tracking methods [<xref rid="B17-sensors-19-03470" ref-type="bibr">17</xref>,<xref rid="B18-sensors-19-03470" ref-type="bibr">18</xref>]. Several stockline smoothing methods are presented to reduce noise fluctuation, e.g., mean shift and spectrum average [<xref rid="B13-sensors-19-03470" ref-type="bibr">13</xref>]. The CLEAN and clustering algorithms, as reported in [<xref rid="B5-sensors-19-03470" ref-type="bibr">5</xref>], can omit the falsely noisy targets on the burden surface but are unsuitable for the task of continuous detection as such. Traditional noise abatement requires domain expertise to construct the feature selector, involving, for example, threshold selection or noise filtering, to the extent that it limits further performance improvement. Another bottleneck is the limited stockline tracking capability.</p><p>Recently, deep neural networks have made considerable progress on diverse kinds of data processing, such as image, video, speech, and text [<xref rid="B19-sensors-19-03470" ref-type="bibr">19</xref>]. The convolutional neural network (CNN) is widely believed to have a powerful learning ability for feature selection and extraction. Besides, with the advantage of long-range memory, the long short-term memory (LSTM) network is broadly used in time-series data modeling. An increasing number of hybrid architectures that combine the CNN and LSTM as an encoder&#x02013;decoder pair have achieved great success in promoting long-term information learning, such as the image caption [<xref rid="B20-sensors-19-03470" ref-type="bibr">20</xref>], speech signal processing [<xref rid="B21-sensors-19-03470" ref-type="bibr">21</xref>,<xref rid="B22-sensors-19-03470" ref-type="bibr">22</xref>], sensory signal estimation [<xref rid="B23-sensors-19-03470" ref-type="bibr">23</xref>], etc.</p><p>In this paper, we propose a novel encoder&#x02013;decoder architecture for effective stockline detection. The encoder is a one-dimensional convolutional neural network (1D-CNN), and the decoder is a cascade multi-layer LSTM network. Between the encoder and decoder, a binary classifier is constructed to mitigate the negative impacts of distorted signals. The hybrid architecture has an excellent anti-noise learning ability and a long-range stockline tracking ability. Our contributions can be summarized as follows:<list list-type="bullet"><list-item><p>To present a novel encoder&#x02013;decoder architecture to improve stockline detection, which learns desired features from noisy data adaptively. We save time and effort compared to traditional hand-crafted denoising processing.</p></list-item><list-item><p>To present an effective stockline tracking strategy by leveraging the LSTM network to model longer range historical signals. A large tracking capability brings better robustness of noise randomness.</p></list-item><list-item><p>The experiments are validated on actual industrial BF data. In particular, the experiments are carried out on an intact multi-radar scenario rather than a single radar scenario.</p></list-item></list></p><p>The rest of the paper is organized as follows. In the second section, the issues of stockline detection and the necessity of the encoder&#x02013;decoder architecture are described. The proposed algorithm and the loss function are explained in <xref ref-type="sec" rid="sec3-sensors-19-03470">Section 3</xref>. We conduct the experiments on actual BF data collected from the eight-radar array system in <xref ref-type="sec" rid="sec4-sensors-19-03470">Section 4</xref>. A conclusion is provided in <xref ref-type="sec" rid="sec5-sensors-19-03470">Section 5</xref> to summarize this work.</p></sec><sec id="sec2-sensors-19-03470"><title>2. Issue Description and Necessity Of Encoder-Decoder Architecture</title><p>The signals are collected individually and sequentially among different radars. They are 1024-dimensional vectors quantified by the 10-bit analog&#x02013;digital converter. There are some examples of the input signals shown in <xref ref-type="fig" rid="sensors-19-03470-f002">Figure 2</xref>. Stocklines are fluctuant curves that reflect the changing depth of the burden surface, as shown in the radar temporal-frequency spectrum in <xref ref-type="fig" rid="sensors-19-03470-f003">Figure 3</xref> where the vertical coordinates have been converted to the measuring distance using
<disp-formula id="FD1-sensors-19-03470"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000b7;</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>d</italic> is the distance in m, <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> m/s is the velocity of electromagnetic waves, and <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1.64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> GHz represents the radar bandwidth. <italic>f</italic> is the beat frequency in Hz. More introductions about the FMCW radar can be found in [<xref rid="B24-sensors-19-03470" ref-type="bibr">24</xref>].</p><p>Several typical noisy conditions are depicted in <xref ref-type="fig" rid="sensors-19-03470-f003">Figure 3</xref>. <xref ref-type="fig" rid="sensors-19-03470-f003">Figure 3</xref>A shows a condition where the stockline is able to be detected. On the contrary, the stockline in <xref ref-type="fig" rid="sensors-19-03470-f003">Figure 3</xref>B appears to be buried under the strong low-frequency noises (the luminous yellow band above the stockline). In <xref ref-type="fig" rid="sensors-19-03470-f003">Figure 3</xref>C, the stockline is so discontinuous that numerous absent measurements occurred due to the interruption of distorted signals. In general, the collected data close to the furnace center suffer heavier noisy impacts compared to those close to the furnace wall.</p><p>As declared in [<xref rid="B2-sensors-19-03470" ref-type="bibr">2</xref>,<xref rid="B7-sensors-19-03470" ref-type="bibr">7</xref>], the unknown statistic properties of the actual BF noises are always a dilemma for noise abatement. Different from traditional methods, which perform data filtering processing that is heuristic, repetitive, and knowledge-based for a multi-radar system, the proposed method circumvents the noise interferences in a learning fashion, significantly reducing the time and effort of hand-engineered data filtering. The front CNN is used to extract representative features from the raw signals; the middle classifier is used to separate the fraudulent distorted signals; the trailing LSTM is utilized to capture the dependencies of time series measurements and decode the feature. Such an effective encoder&#x02013;decoder (CNN&#x02013;LSTM) backbone architecture tailored to stockline detection is presented.</p></sec><sec sec-type="methods" id="sec3-sensors-19-03470"><title>3. Methodology</title><sec id="sec3dot1-sensors-19-03470"><title>3.1. Architecture</title><p>The proposed architecture is shown in <xref ref-type="fig" rid="sensors-19-03470-f004">Figure 4</xref>. It consists of four operations, including CNN feature extraction, radar identification (ID) embedding, distorted signal separation, and temporal feature decoding.</p><p><bold>CNN Encoder</bold>. A five-layer CNN is constructed as the encoder. It receives the raw signal (1024-D) as input. The structure is shown in <xref ref-type="fig" rid="sensors-19-03470-f005">Figure 5</xref>a.</p><p>Mathematically, given the input of the convolutional layer, <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is <italic>C</italic>-channel and <italic>D</italic>-dimensional. Let <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> be the weight, where <italic>K</italic> is the number of convolutional kernels and <italic>L</italic> is the kernel length. The 1D-convolutional calculation can be formulated as
<disp-formula id="FD2-sensors-19-03470"><label>(2)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfenced separators="" open="&#x02308;" close="&#x02309;"><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mfenced><mml:mi>s</mml:mi></mml:mrow></mml:mover></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the symbol &#x02217; stands for the convolutional operation, <italic>s</italic> is the sliding stride, and <inline-formula><mml:math id="mm7"><mml:mrow><mml:mfenced open="&#x02308;" close="&#x02309;"><mml:mo>&#x000b7;</mml:mo></mml:mfenced></mml:mrow></mml:math></inline-formula> is the ceiling function.</p><p>We apply batch normalization (BN) and nonlinearity after each convolutional layer, as shown in <xref ref-type="fig" rid="sensors-19-03470-f005">Figure 5</xref>b. BN is widely used in the neural networks and can speed up convergence and slightly improve performance [<xref rid="B25-sensors-19-03470" ref-type="bibr">25</xref>]. Assuming <italic>x</italic> is the input of BN, we have
<disp-formula id="FD3-sensors-19-03470"><label>(3)</label><mml:math id="mm8"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula> is a scaling factor and <inline-formula><mml:math id="mm10"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> is a shifting factor. It is noted that <inline-formula><mml:math id="mm11"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> also acts as a bias term to the convolutional layer. <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean, and <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>[</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the variance. <inline-formula><mml:math id="mm14"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></inline-formula> is a small positive constant to prevent division by zero, e.g., <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi>&#x003f5;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The leaky rectified linear unit (LRelu) is adopted as nonlinearity [<xref rid="B26-sensors-19-03470" ref-type="bibr">26</xref>]. It is
<disp-formula id="FD4-sensors-19-03470"><label>(4)</label><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>x</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi></mml:mrow><mml:mspace width="1.em"/><mml:mi>x</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>otherwise</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the LRelu function and <inline-formula><mml:math id="mm18"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> is a constant, e.g., <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Max-pooling layers with a stride of 2 are used in the encoder. After encoding by the CNN, the <italic>m</italic>-dimensional feature is extracted. <italic>m</italic> is a optional hyperparameter, and <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is used.</p><p><bold>ID Embedding</bold>. The characteristics of noisy signal data are different from radar to radar. For multi-radar data, we embed the radar ID information into feature <italic>F</italic> to slightly improve performance. The radar ID is encoded using one-hot coding. Formally, given <italic>R</italic> radars, the corresponding ID is denoted by <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="script">I</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="script">I</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm23"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the <italic>i</italic>-th component of <inline-formula><mml:math id="mm24"><mml:mrow><mml:msup><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. For the <italic>r</italic>-th radar, we have
<disp-formula id="FD5-sensors-19-03470"><label>(5)</label><mml:math id="mm25"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="script">I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi></mml:mrow><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>otherwise</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm26"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27"><mml:mrow><mml:msup><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are concatenated together in dimensionality, the superscript denotes the signal derived from the <italic>r</italic>-th radar.<disp-formula id="FD6-sensors-19-03470"><label>(6)</label><mml:math id="mm28"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><bold>Distorted Signal Separation</bold>. We construct a binary classifier using a fully connected layer to classify the normal signals and distorted signals before feeding them into the decoder. The fully connected layer maps the feature into a <italic>V</italic>-class decision space (<inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for the 2-class task). Let <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> be the weight, with <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> as the bias. We have
<disp-formula id="FD7-sensors-19-03470"><label>(7)</label><mml:math id="mm32"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mfenced></mml:mrow></mml:munder></mml:mstyle><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33"><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is <italic>j</italic>-th element of <inline-formula><mml:math id="mm34"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm35"><mml:mrow><mml:mfenced separators="" open="|" close="|"><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mfenced></mml:mrow></mml:math></inline-formula> is the dimensionality of <inline-formula><mml:math id="mm36"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the category predicting the probability by the softmax function. For the binary classification, we have the expectation <inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for the normal signals and <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for distorted signals. The distorted signals carry confusing information and do not make any meaningful contributions to stockline estimation. We mask them by zero to reduce their negative impacts for the next calculation step.</p><p><bold>LSTM Decoder</bold>. LSTM is an improved variant of recurrent neural networks (RNNs) [<xref rid="B27-sensors-19-03470" ref-type="bibr">27</xref>,<xref rid="B28-sensors-19-03470" ref-type="bibr">28</xref>]. Define <italic>T</italic> be the tracking length of the stockline, including <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> historical signals and one current signal. Let the subscript <italic>t</italic> be the index of the <italic>T</italic> sequential signals that are fed into the LSTM, so <inline-formula><mml:math id="mm41"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. LSTM makes use of an effective gate mechanism to control the context information flow, which is comprised of the input gate <inline-formula><mml:math id="mm42"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the forget gate <inline-formula><mml:math id="mm43"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the output gate <inline-formula><mml:math id="mm44"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Its inner gate mechanism is shown in <xref ref-type="fig" rid="sensors-19-03470-f006">Figure 6</xref>. The cell state <inline-formula><mml:math id="mm45"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> stores the information of each time step, the input gate determines whether to add new information to <inline-formula><mml:math id="mm46"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the forget gate selectively forgets the uninteresting previous information involved in <inline-formula><mml:math id="mm47"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and they control the update of the cell state <inline-formula><mml:math id="mm48"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The output gate controls the emission from <inline-formula><mml:math id="mm49"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to the hidden state <inline-formula><mml:math id="mm50"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm51"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is mapped to the decoder output <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm53"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the weight matrix and <inline-formula><mml:math id="mm54"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the bias.</p><p>The output <inline-formula><mml:math id="mm55"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of LSTM would correspond to one point of the stockline. Equally, <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>LSTM</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot2-sensors-19-03470"><title>3.2. Loss Function</title><p><bold>Classification loss</bold>. The classifier is trained by minimizing the cross-entropy loss. It can be formulated as
<disp-formula id="FD8-sensors-19-03470"><label>(8)</label><mml:math id="mm57"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><bold>Regression loss</bold>. The output values <inline-formula><mml:math id="mm58"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are regressed to the target values <inline-formula><mml:math id="mm59"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> by minimizing their square errors, and the form of regression loss is
<disp-formula id="FD9-sensors-19-03470"><label>(9)</label><mml:math id="mm60"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:msub><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>T</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mfenced></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>&#x003c1;</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder></mml:mstyle><mml:msubsup><mml:mrow><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mfenced></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm61"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> is the discount factor, <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is used.</p><p><bold>Joint loss</bold>. It is a joint training task of classification and regression. The general way of constructing a joint loss is using a linear weighted sum of each subtask loss. The total loss can be written as <disp-formula id="FD10-sensors-19-03470"><label>(10)</label><mml:math id="mm63"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02113;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the weighted factors of each subtask. <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="&#x02225;" close="&#x02225;"><mml:mi>&#x003b8;</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the L2 norm loss.</p><p>When it simply presets <inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm67"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in our experiments, the classification loss tends to steer the training process and damage the regression performance. Thus, we use the strategy of homoscedastic uncertainty to search for the suitable weight factors to balance their performance [<xref rid="B29-sensors-19-03470" ref-type="bibr">29</xref>,<xref rid="B30-sensors-19-03470" ref-type="bibr">30</xref>]. The joint loss is rewritten as
<disp-formula id="FD11-sensors-19-03470"><label>(11)</label><mml:math id="mm68"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm70"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are two learnable parameters that stand for the observed variances of the subtasks. The bound term <inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> discourages the variances from increasing too much. Assuming <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. On implementation, the network is trained to predict the log variance because it is more numerically stable; for instance, letting <inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, we use <inline-formula><mml:math id="mm76"><mml:mrow><mml:msqrt><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msup></mml:msqrt></mml:mrow></mml:math></inline-formula> to replace <inline-formula><mml:math id="mm77"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> in (11), and so for <inline-formula><mml:math id="mm78"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="sec4-sensors-19-03470"><title>4. Experiment</title><p>In this section, the experiments are carried out and the experimental setups are provided in detail. The experimental results consist of four parts: performance, scientificity of the architecture, the effect of the tracking strategy, and the running time.</p><sec id="sec4dot1-sensors-19-03470"><title>4.1. Experiment Setup</title><p><bold>Dataset</bold>. There are <inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mn>504</mml:mn><mml:mo>,</mml:mo><mml:mn>488</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> signals collected from the industrial BF eight-radar array system as posed in <xref rid="sensors-19-03470-t001" ref-type="table">Table 1</xref>. The data is divided into three parts, including the training set, validation set, and testing set; they are 3:1:1, respectively. The training set is used to train the model, the validation set is used to validate the performance and tune the hyperparameters, and the testing set is used to test its performance. The results are from the testing set as default, unless otherwise stated. The radar signals are normalized between <inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in order to eliminate the effect of the amplitude.</p><p><bold>Hyperparameters</bold>. The encoder with 5 convolutional layers, the decoder with 3 hidden layers, 72 time steps (i.e., <inline-formula><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>72</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), and 128 hidden nodes are adopted. The convolutional layers and fully connected layers are initialized with zero bias and a Gaussian weight filled with <inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The LSTM cells are zero-initialized. We use the Adam solver [<xref rid="B31-sensors-19-03470" ref-type="bibr">31</xref>], which is an improved stochastic gradient descent algorithm, to optimize the model. A dynamic learning rate is used,. It starts with 0.001 and decays by 0.98 per every 100 iterations. The model is trained for 20 epochs with a batch size of 120. We perform the dropout with a <inline-formula><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> dropout rate to the LSTM as regularization [<xref rid="B28-sensors-19-03470" ref-type="bibr">28</xref>,<xref rid="B32-sensors-19-03470" ref-type="bibr">32</xref>]. When the training is completed, the adaptive parameters <inline-formula><mml:math id="mm84"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm85"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are leveled off to <inline-formula><mml:math id="mm86"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4.09</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm87"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.34</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively; in other words, <inline-formula><mml:math id="mm88"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>29.87</mml:mn><mml:mo>:</mml:mo><mml:mn>1.40</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><bold>Evaluation</bold>. We use mean absolute error (MAE) and root mean square error (RMSE) to evaluate the detection performance. Formally, MAE is defined as
<disp-formula id="FD12-sensors-19-03470"><label>(12)</label><mml:math id="mm89"><mml:mrow><mml:mrow><mml:mi>MAE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mfenced><mml:mspace width="3.33333pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The RMSE is defined as
<disp-formula id="FD13-sensors-19-03470"><label>(13)</label><mml:math id="mm90"><mml:mrow><mml:mrow><mml:mi>RMSE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:msubsup><mml:mrow><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mfenced></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mspace width="3.33333pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We use accuracy, precision, recall, and F1 score to evaluate the classification performance. Accuracy indicates the proportion of correctly classified samples among all samples. Precision indicates the proportion of true positive samples among true positive samples (TPs) and false positive samples (FPs), that is,
<disp-formula id="FD14-sensors-19-03470"><label>(14)</label><mml:math id="mm91"><mml:mrow><mml:mrow><mml:mi>precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mspace width="4.pt"/><mml:mo>+</mml:mo><mml:mspace width="4.pt"/><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Recall indicates the proportion of true positive samples among true positive samples and false negative samples (FNs).
<disp-formula id="FD15-sensors-19-03470"><label>(15)</label><mml:math id="mm92"><mml:mrow><mml:mrow><mml:mi>recall</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mspace width="4.pt"/><mml:mo>+</mml:mo><mml:mspace width="4.pt"/><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The F1 score is the harmonic mean of precision and recall.
<disp-formula id="FD16-sensors-19-03470"><label>(16)</label><mml:math id="mm93"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mi>precision</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>recall</mml:mi></mml:mrow><mml:mrow><mml:mi>precision</mml:mi><mml:mspace width="4.pt"/><mml:mo>+</mml:mo><mml:mspace width="4.pt"/><mml:mi>recall</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="results" id="sec4dot2-sensors-19-03470"><title>4.2. Results</title><p>Different experiments were conducted as shown below. The proposed method was implemented on the Tensorflow framework [<xref rid="B33-sensors-19-03470" ref-type="bibr">33</xref>].</p><p><bold>Performance</bold>. The traditional peak searching method with pass-band FIR filters and Kalman filters were implemented for comparison. The settings of the FIR filters are shown in <xref rid="sensors-19-03470-t002" ref-type="table">Table 2</xref>, and the Kalman algorithm is introduced in [<xref rid="B34-sensors-19-03470" ref-type="bibr">34</xref>]. Respectively, the single CNN model and single LSTM model were constructed for another baseline comparison and their structures were identical to the corresponding part of the hybrid model.</p><p>The estimation stocklines are displayed in <xref ref-type="fig" rid="sensors-19-03470-f007">Figure 7</xref>, including the proposed method and FIR-Kalman methods. The regression curves of the proposed method exhibited less fluctuation around the expectation curves. In contrast, the filter-based approaches tended to be influenced by interferences.</p><p>As shown in <xref rid="sensors-19-03470-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-19-03470-t004" ref-type="table">Table 4</xref>, with an MAE of 0.0432 and a RMSE of 0.0581, the presented architecture is capable of learning features and suppressing noise interferences of noisy data. It could be observed that the proposed method significantly outperforms traditional experience-dependent denoising methods. From the table, the traditional method without denoising processing is extremely sensitive to heavy noisy impacts and achieves a poor performance. In contrast, the proposed method shows itself to be more efficient and robust.</p><p>The classifier performance is shown in <xref rid="sensors-19-03470-t005" ref-type="table">Table 5</xref>. With scores of 95.90%, 96.00%, 99.48%, and 97.68% for the accuracy, precision, recall, and F1 score indicators, respectively, it achieves a decent classification result that can classify the distorted signals well.</p><p><bold>Scientificity of architecture</bold>. A series of experiments were carried out to verify the scientificity of such an architecture. First is the validity of such a CNN&#x02013;LSTM mixed architecture. Two simpler baseline architectures are provided, i.e., a single CNN architecture and a single LSTM architecture, as shown earlier in <xref rid="sensors-19-03470-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-19-03470-t004" ref-type="table">Table 4</xref>. The detached architectures both show worse performances compared to the proposed mixed architecture due to losing the advantages of each. For the simple CNN architecture, it shows a weak performance and is even worse than traditional denoising methods. For the simple LSTM architecture, we observe that LSTM gets into trouble learning the feature from noisy signals without the help of the CNN encoder.</p><p>Secondly, we conduct experiments to examine the complexity of the encoder and decoder. The impacts of picking a different number of CNN layers and a different number of LSTM layers are shown in <xref ref-type="fig" rid="sensors-19-03470-f008">Figure 8</xref>a,b, respectively. Based on the experiments, we choose a 5-layer CNN and a 3-layer LSTM as the encoder and decoder for optimal performance. The experimental results also indicate that a deep architecture is not always necessary.</p><p>Thirdly, the stockline tracking length <italic>T</italic> is examined. The experiments are shown in <xref ref-type="fig" rid="sensors-19-03470-f008">Figure 8</xref>c; as <italic>T</italic> increases, errors decrease in a general trend. Due to the limited memory of the LSTM network, it is found that when the tracking length <inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>72</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, it brings a marginal improvement of performance but conspicuously increases computational time, so a proper <inline-formula><mml:math id="mm95"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>72</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is used.</p><p><bold>Effect of tracking strategy</bold>. To further validate the effect of the stockline tracking ability provided by the LSTM, a snapshot-based model as designed by setting the time step <inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, implying that the network has no visible historical signal. We compare its performance with the tracking-based model (i.e., <inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>72</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) with fair training settings. The results are shown in <xref ref-type="fig" rid="sensors-19-03470-f009">Figure 9</xref>, where the black dotted line stands for the expectation, and the one whose distribution is closer to the black line shows a better performance. The snapshot-based tracking strategy, with a MAE of 0.1019 and a RMSE of 0.1854, is more sensitive to the interruptive noises and is even worse than the FIR-Kalman approach. As shown earlier in <xref ref-type="fig" rid="sensors-19-03470-f008">Figure 8</xref>c, a longer tracking capability has a positive effect on performance. The proposed method takes a longer range of previous stockline into consideration; however, the Kalman tracking approach only makes use of one previous moment. An effective tracking strategy brings a better tolerance to noisy randomness and disturbances.</p><p><bold>Running time</bold>. In addition, with an average computational time of <inline-formula><mml:math id="mm98"><mml:mrow><mml:mrow><mml:mn>0.522</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms, the proposed architecture is lightweight and fully meets the real-time requirements of the industrial process.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-19-03470"><title>5. Conclusions</title><p>In this paper, we have successfully developed a hybrid CNN&#x02013;LSTM architecture for the challenging stockline detection problem. Its effectiveness has been demonstrated by experiments on BF eight-radar array data. The success of the proposed method is attributable to three reasons: its effective learning ability, its ability to classify distorted signals, and its excellent stockline tracking ability.</p><p>In the industrial process, most of the monitoring data or variables have close time-series dependencies. The general contribution of this work is that we improve the long-range history learning by leveraging the novel LSTM network. It is a very promising direction to achieve more accurate and robust industrial control, if we can make the most of the context relationship of successive measurements.</p><p>In future work, we will dedicate our efforts to the image reconstruction of BF burden surfaces. The inner transparency of the neural network methodology will be investigated, which will allow us to have an in-depth qualitative or quantitative analysis of noise influences.</p></sec></body><back><notes><title>Author Contributions</title><p>Methodology, X.L.; Project administration, J.L.; Resources, X.C.; Validation, M.Z.; Writing&#x02014;review &#x00026; editing, Y.L.</p></notes><notes><title>Funding</title><p>This work was supported by the Natural Science Foundation of Beijing Municipality (No. 4182038), the National Natural Science Foundation of China (No. 61671054), the Fundamental Research Funds for the China Central Universities of USTB (FRF-BR-17-004A, FRF-GF-17-B49), and the Open Project Program of the National Laboratory of Pattern Recognition (No. 201800027).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-19-03470"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z.</given-names></name><name><surname>Fu</surname><given-names>Z.</given-names></name></person-group><article-title>Current situation of energy consumption and measures taken for energy saving in the iron and steel industry in China</article-title><source>Energy</source><year>2010</year><volume>35</volume><fpage>4356</fpage><lpage>4360</lpage><pub-id pub-id-type="doi">10.1016/j.energy.2009.04.008</pub-id></element-citation></ref><ref id="B2-sensors-19-03470"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P.</given-names></name><name><surname>Lv</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Chai</surname><given-names>T.</given-names></name></person-group><article-title>Data-Driven Robust RVFLNs Modeling of a Blast Furnace Iron-Making Process Using Cauchy Distribution Weighted M-Estimation</article-title><source>IEEE Trans. Ind. Electron.</source><year>2017</year><volume>64</volume><fpage>7141</fpage><lpage>7151</lpage><pub-id pub-id-type="doi">10.1109/TIE.2017.2686369</pub-id></element-citation></ref><ref id="B3-sensors-19-03470"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.L.</given-names></name><name><surname>Liu</surname><given-names>D.X.</given-names></name><name><surname>Jia</surname><given-names>C.</given-names></name><name><surname>Chen</surname><given-names>X.Z.</given-names></name></person-group><article-title>Multi-model control of blast furnace burden surface based on fuzzy SVM</article-title><source>Neurocomputing</source><year>2015</year><volume>148</volume><fpage>209</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2013.09.067</pub-id></element-citation></ref><ref id="B4-sensors-19-03470"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name></person-group><article-title>A dielectric-filled waveguide antenna element for 3D imaging radar in high temperature and excessive dust conditions</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>1339</elocation-id><pub-id pub-id-type="doi">10.3390/s16081339</pub-id><pub-id pub-id-type="pmid">27556469</pub-id></element-citation></ref><ref id="B5-sensors-19-03470"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zankl</surname><given-names>D.</given-names></name><name><surname>Schuster</surname><given-names>S.</given-names></name><name><surname>Feger</surname><given-names>R.</given-names></name><name><surname>Stelzer</surname><given-names>A.</given-names></name><name><surname>Scheiblhofer</surname><given-names>S.</given-names></name><name><surname>Schmid</surname><given-names>C.M.</given-names></name><name><surname>Ossberger</surname><given-names>G.</given-names></name><name><surname>Stegfellner</surname><given-names>L.</given-names></name><name><surname>Lengauer</surname><given-names>G.</given-names></name><name><surname>Feilmayr</surname><given-names>C.</given-names></name><etal/></person-group><article-title>BLASTDAR&#x02014;A large radar sensor array system for blast furnace burden surface imaging</article-title><source>IEEE Sens. J.</source><year>2015</year><volume>15</volume><fpage>5893</fpage><lpage>5909</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2015.2445494</pub-id></element-citation></ref><ref id="B6-sensors-19-03470"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B.</given-names></name><name><surname>Ye</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name></person-group><article-title>Process monitoring of iron-making process in a blast furnace with PCA-based methods</article-title><source>Control. Eng. Pract.</source><year>2016</year><volume>47</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.conengprac.2015.11.006</pub-id></element-citation></ref><ref id="B7-sensors-19-03470"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>C.</given-names></name><name><surname>Jian</surname><given-names>L.</given-names></name><name><surname>Luo</surname><given-names>S.</given-names></name></person-group><article-title>Modeling of the thermal state change of blast furnace hearth with support vector machines</article-title><source>IEEE Trans. Ind. Electron.</source><year>2012</year><volume>59</volume><fpage>1134</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1109/TIE.2011.2159693</pub-id></element-citation></ref><ref id="B8-sensors-19-03470"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jian</surname><given-names>L.</given-names></name><name><surname>Gao</surname><given-names>C.</given-names></name></person-group><article-title>Binary coding SVMs for the multiclass problem of blast furnace system</article-title><source>IEEE Trans. Ind. Electron.</source><year>2013</year><volume>60</volume><fpage>3846</fpage><lpage>3856</lpage><pub-id pub-id-type="doi">10.1109/TIE.2012.2206336</pub-id></element-citation></ref><ref id="B9-sensors-19-03470"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pettersson</surname><given-names>F.</given-names></name><name><surname>Chakraborti</surname><given-names>N.</given-names></name><name><surname>Sax&#x000e9;n</surname><given-names>H.</given-names></name></person-group><article-title>A genetic algorithms based multi-objective neural net applied to noisy blast furnace data</article-title><source>Appl. Soft Comput.</source><year>2007</year><volume>7</volume><fpage>387</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2005.09.001</pub-id></element-citation></ref><ref id="B10-sensors-19-03470"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Yin</surname><given-names>Y.</given-names></name><name><surname>Xiao</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>A novel online sequential extreme learning machine for gas utilization ratio prediction in blast furnaces</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1847</elocation-id><pub-id pub-id-type="doi">10.3390/s17081847</pub-id></element-citation></ref><ref id="B11-sensors-19-03470"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>D.</given-names></name><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Bai</surname><given-names>C.</given-names></name><name><surname>Qiu</surname><given-names>G.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name></person-group><article-title>Effect of burden material size on blast furnace stockline profile of bell-less blast furnace</article-title><source>Ironmak. Steelmak.</source><year>2009</year><volume>36</volume><fpage>217</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1179/174328108X369107</pub-id></element-citation></ref><ref id="B12-sensors-19-03470"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Qingwen</surname><given-names>H.</given-names></name><name><surname>Xianzhong</surname><given-names>C.</given-names></name><name><surname>Ping</surname><given-names>C.</given-names></name></person-group><article-title>Radar data processing of blast furnace stock-line based on spatio-temporal data association</article-title><source>Proceedings of the 2015 34th Chinese Control Conference (CCC)</source><conf-loc>Hangzhou, China</conf-loc><conf-date>28&#x02013;30 July 2015</conf-date><fpage>4604</fpage><lpage>4609</lpage></element-citation></ref><ref id="B13-sensors-19-03470"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Wei</surname><given-names>J.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name><name><surname>Hou</surname><given-names>Q.</given-names></name><name><surname>Bai</surname><given-names>Z.</given-names></name></person-group><article-title>3-Dimension imaging system of burden surface with 6-radars array in a blast furnace</article-title><source>ISIJ Int.</source><year>2012</year><volume>52</volume><fpage>2048</fpage><lpage>2054</lpage><pub-id pub-id-type="doi">10.2355/isijinternational.52.2048</pub-id></element-citation></ref><ref id="B14-sensors-19-03470"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A.</given-names></name><name><surname>Huang</surname><given-names>J.</given-names></name><name><surname>Qiu</surname><given-names>J.</given-names></name><name><surname>Ke</surname><given-names>Y.</given-names></name><name><surname>Zheng</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Signal processing for a FMCW material level measurement system</article-title><source>Proceedings of the 2016 IEEE 13th International Conference on Signal Processing (ICSP)</source><conf-loc>Chengdu, China</conf-loc><conf-date>6&#x02013;10 November 2016</conf-date><fpage>244</fpage><lpage>247</lpage></element-citation></ref><ref id="B15-sensors-19-03470"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malmberg</surname><given-names>D.</given-names></name><name><surname>Hahlin</surname><given-names>P.</given-names></name><name><surname>Nilsson</surname><given-names>E.</given-names></name></person-group><article-title>Microwave technology in steel and metal industry, an overview</article-title><source>ISIJ Int.</source><year>2007</year><volume>47</volume><fpage>533</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.2355/isijinternational.47.533</pub-id></element-citation></ref><ref id="B16-sensors-19-03470"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomes</surname><given-names>F.S.</given-names></name><name><surname>C&#x000f4;co</surname><given-names>K.F.</given-names></name><name><surname>Salles</surname><given-names>J.L.F.</given-names></name></person-group><article-title>Multistep forecasting models of the liquid level in a blast furnace hearth</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><year>2017</year><volume>14</volume><fpage>1286</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1109/TASE.2016.2538560</pub-id></element-citation></ref><ref id="B17-sensors-19-03470"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Br&#x000e4;nnbacka</surname><given-names>J.</given-names></name><name><surname>Saxen</surname><given-names>H.</given-names></name></person-group><article-title>Novel model for estimation of liquid levels in the blast furnace hearth</article-title><source>Chem. Eng. Sci.</source><year>2004</year><volume>59</volume><fpage>3423</fpage><lpage>3432</lpage><pub-id pub-id-type="doi">10.1016/j.ces.2004.05.007</pub-id></element-citation></ref><ref id="B18-sensors-19-03470"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Kelly</surname><given-names>J.</given-names></name><name><surname>Cui</surname><given-names>Y.</given-names></name></person-group><article-title>Blast furnace stockline measurement using radar</article-title><source>Ironmak. Steelmak.</source><year>2015</year><volume>42</volume><fpage>533</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1179/1743281214Y.0000000258</pub-id></element-citation></ref><ref id="B19-sensors-19-03470"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B20-sensors-19-03470"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>K.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name><name><surname>Kiros</surname><given-names>R.</given-names></name><name><surname>Cho</surname><given-names>K.</given-names></name><name><surname>Courville</surname><given-names>A.</given-names></name><name><surname>Salakhudinov</surname><given-names>R.</given-names></name><name><surname>Zemel</surname><given-names>R.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Show, attend and tell: Neural image caption generation with visual attention</article-title><source>Proceedings of the International Conference on Machine Learning (ICML)</source><conf-loc>Lille, France</conf-loc><conf-date>6&#x02013;11 July 2015</conf-date><fpage>2048</fpage><lpage>2057</lpage></element-citation></ref><ref id="B21-sensors-19-03470"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sainath</surname><given-names>T.N.</given-names></name><name><surname>Vinyals</surname><given-names>O.</given-names></name><name><surname>Senior</surname><given-names>A.</given-names></name><name><surname>Sak</surname><given-names>H.</given-names></name></person-group><article-title>Convolutional, long short-term memory, fully connected deep neural networks</article-title><source>Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>South Brisbane, Australia</conf-loc><conf-date>19&#x02013;24 April 2015</conf-date><fpage>4580</fpage><lpage>4584</lpage></element-citation></ref><ref id="B22-sensors-19-03470"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Trigeorgis</surname><given-names>G.</given-names></name><name><surname>Ringeval</surname><given-names>F.</given-names></name><name><surname>Brueckner</surname><given-names>R.</given-names></name><name><surname>Marchi</surname><given-names>E.</given-names></name><name><surname>Nicolaou</surname><given-names>M.A.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name><name><surname>Zafeiriou</surname><given-names>S.</given-names></name></person-group><article-title>Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network</article-title><source>Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Shanghai, China</conf-loc><conf-date>20&#x02013;25 March 2016</conf-date><fpage>5200</fpage><lpage>5204</lpage></element-citation></ref><ref id="B23-sensors-19-03470"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Zhao</surname><given-names>R.</given-names></name><name><surname>Zhu</surname><given-names>Q.</given-names></name><name><surname>Masood</surname><given-names>M.K.</given-names></name><name><surname>Soh</surname><given-names>Y.C.</given-names></name><name><surname>Mao</surname><given-names>K.</given-names></name></person-group><article-title>Building occupancy estimation with environmental sensors via CDBLSTM</article-title><source>IEEE Trans. Ind. Electron.</source><year>2017</year><volume>64</volume><fpage>9549</fpage><lpage>9559</lpage><pub-id pub-id-type="doi">10.1109/TIE.2017.2711530</pub-id></element-citation></ref><ref id="B24-sensors-19-03470"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stove</surname><given-names>A.G.</given-names></name></person-group><article-title>Linear FMCW radar techniques</article-title><source>IEE Proceedings F (Radar and Signal Processing)</source><publisher-name>IET</publisher-name><publisher-loc>London, UK</publisher-loc><year>1992</year><volume>Volume 139</volume><fpage>343</fpage><lpage>350</lpage></element-citation></ref><ref id="B25-sensors-19-03470"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title><source>Proceedings of the International Conference on Machine Learning (ICML)</source><conf-loc>Lille, France</conf-loc><conf-date>6&#x02013;11 July 2015</conf-date><fpage>448</fpage><lpage>456</lpage></element-citation></ref><ref id="B26-sensors-19-03470"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#x02013;13 December 2015</conf-date><fpage>1026</fpage><lpage>1034</lpage></element-citation></ref><ref id="B27-sensors-19-03470"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S.</given-names></name><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group><article-title>Long short-term memory</article-title><source>Neural Comput.</source><year>1997</year><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="B28-sensors-19-03470"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaremba</surname><given-names>W.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Vinyals</surname><given-names>O.</given-names></name></person-group><article-title>Recurrent neural network regularization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.2329</pub-id></element-citation></ref><ref id="B29-sensors-19-03470"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kendall</surname><given-names>A.</given-names></name><name><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>Geometric loss functions for camera pose regression with deep learning</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>5974</fpage><lpage>5983</lpage></element-citation></ref><ref id="B30-sensors-19-03470"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kendall</surname><given-names>A.</given-names></name><name><surname>Gal</surname><given-names>Y.</given-names></name></person-group><article-title>What uncertainties do we need in bayesian deep learning for computer vision?</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2017</year><fpage>5574</fpage><lpage>5584</lpage></element-citation></ref><ref id="B31-sensors-19-03470"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P.</given-names></name><name><surname>Kingma</surname><given-names>J.B.</given-names></name></person-group><article-title>Adam: A Method for Stochastic Optimization</article-title><source>Proceedings of the 3rd International Conference on Learning Representations (ICLR)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>7&#x02013;9 May 2015</conf-date></element-citation></ref><ref id="B32-sensors-19-03470"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group><article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title><source>J. Mach. Learn. Res.</source><year>2014</year><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="B33-sensors-19-03470"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Davis</surname><given-names>A.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name><name><surname>Devin</surname><given-names>M.</given-names></name><name><surname>Ghemawat</surname><given-names>S.</given-names></name><name><surname>Irving</surname><given-names>G.</given-names></name><name><surname>Isard</surname><given-names>M.</given-names></name><etal/></person-group><article-title>TensorFlow: A System for Large-Scale Machine Learning</article-title><source>Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI&#x02019;16)</source><conf-loc>Savannah, GA, USA</conf-loc><conf-date>2&#x02013;4 November 2016</conf-date><volume>Volume 16</volume><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="B34-sensors-19-03470"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>G.</given-names></name><name><surname>Welch</surname><given-names>G.</given-names></name></person-group><article-title>An introduction to the Kalman filter</article-title><source>Proc. Siggraph Course</source><year>2001</year><volume>8</volume><fpage>59</fpage></element-citation></ref></ref-list></back><floats-group><fig id="sensors-19-03470-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Prototype of the employed eight-radar array system. The frequency-modulated continuous wave (FMCW) radars work in the band of 24 GHz&#x0223c;26 GHz, with a bandwidth of 1.64 GHz. The radar sensors are placed by scientifically considering the practical industrial field.</p></caption><graphic xlink:href="sensors-19-03470-g001"/></fig><fig id="sensors-19-03470-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Examples of the input signals. Each signal is 1024-dimensional. (<bold>a</bold>&#x02013;<bold>c</bold>) are examples from radar #2; (<bold>d</bold>&#x02013;<bold>f</bold>) are examples from radar #4; (<bold>g</bold>&#x02013;<bold>i</bold>) are examples from radar #6.</p></caption><graphic xlink:href="sensors-19-03470-g002"/></fig><fig id="sensors-19-03470-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The time&#x02013;frequency spectrum and the expectation stockline of different radar signals. (<bold>A</bold>&#x02013;<bold>C</bold>) are from radar #2, #4, and #6, respectively. The horizontal axis represents the number of time series signals, and the vertical axis represents the distance between the radar sensor and the measured point.</p></caption><graphic xlink:href="sensors-19-03470-g003"/></fig><fig id="sensors-19-03470-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The proposed encoder&#x02013;decoder architecture unrolled in time. LSTM: long short-term memory; 1D-CNN: one-dimensional convolutional neural network.</p></caption><graphic xlink:href="sensors-19-03470-g004"/></fig><fig id="sensors-19-03470-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>(<bold>a</bold>) The structure of the 1D-CNN encoder. It is composed of 5 convolutional layers and 4 pooling layers. Eight-length convolutional kernels are used at the first four layers, while 4-length kernels are used at the last layer. (<bold>b</bold>) We perform batch normalization (BN) and a leaky rectified linear unit (LRelu) function after each convolutional layer.</p></caption><graphic xlink:href="sensors-19-03470-g005"/></fig><fig id="sensors-19-03470-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Diagram of an LSTM cell with its inner memory mechanism, including the input gate <inline-formula><mml:math id="mm99"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, forget gate <inline-formula><mml:math id="mm100"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and output gate <inline-formula><mml:math id="mm101"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The dashed line represents the information from the last time step.</p></caption><graphic xlink:href="sensors-19-03470-g006"/></fig><fig id="sensors-19-03470-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>The estimation stocklines of the eight-radar array system measured in the same period.</p></caption><graphic xlink:href="sensors-19-03470-g007"/></fig><fig id="sensors-19-03470-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Performance on the validation set. (<bold>a</bold>) Performance of selecting different encoder layers. (<bold>b</bold>) Performance of selecting different decoder layers. (<bold>c</bold>) Performance of selecting different tracking lengths <italic>T</italic>.</p></caption><graphic xlink:href="sensors-19-03470-g008"/></fig><fig id="sensors-19-03470-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>The distribution of the estimated values by the snapshot-based model and tracking-based model. As can be seen in the figure, a group of fixed false estimated points (red) occurred at &#x0223c;8.3 m, perhaps caused by a fixed noisy target within the blast furnace (BF).</p></caption><graphic xlink:href="sensors-19-03470-g009"/></fig><table-wrap id="sensors-19-03470-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-03470-t001_Table 1</object-id><label>Table 1</label><caption><p>The number of normal signals and distorted signals.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Radar ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normal</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distorted</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1#</td><td align="center" valign="middle" rowspan="1" colspan="1">61,904</td><td align="center" valign="middle" rowspan="1" colspan="1">1157</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2#</td><td align="center" valign="middle" rowspan="1" colspan="1">61,267</td><td align="center" valign="middle" rowspan="1" colspan="1">1794</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3#</td><td align="center" valign="middle" rowspan="1" colspan="1">61,715</td><td align="center" valign="middle" rowspan="1" colspan="1">1346</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4#</td><td align="center" valign="middle" rowspan="1" colspan="1">50,490</td><td align="center" valign="middle" rowspan="1" colspan="1">12,571</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5#</td><td align="center" valign="middle" rowspan="1" colspan="1">53,599</td><td align="center" valign="middle" rowspan="1" colspan="1">9462</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6#</td><td align="center" valign="middle" rowspan="1" colspan="1">52,150</td><td align="center" valign="middle" rowspan="1" colspan="1">10,911</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7#</td><td align="center" valign="middle" rowspan="1" colspan="1">52,719</td><td align="center" valign="middle" rowspan="1" colspan="1">10,342</td><td align="center" valign="middle" rowspan="1" colspan="1">63,061</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8#</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53,839</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9222</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63,061</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-03470-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-03470-t002_Table 2</object-id><label>Table 2</label><caption><p>Configuration of the FIR filters with the Blackman&#x02013;Harris windows.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Radar ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">1#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">2#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">3#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">4#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">5#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">6#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">7#</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">8#</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Window Length</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">264</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">136</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">136</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">184</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">104</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-03470-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-03470-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of the proposed method with different methods on the testing set (mean absolute error, MAE). PS: peak searching.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Radar ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PS (w/o Denoising)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FIR-PS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FIR-Kalman-PS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CNN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LSTM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CNN-LSTM (Ours)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1#</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2575</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0825</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0733</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1153</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1675</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0320</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2#</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7230</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0621</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0502</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1751</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1952</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0434</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3#</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5422</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0898</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0583</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2403</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1938</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0520</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4#</td><td align="center" valign="middle" rowspan="1" colspan="1">5.1153</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1330</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0892</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1986</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2304</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0645</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5#</td><td align="center" valign="middle" rowspan="1" colspan="1">5.1069</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1030</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0778</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1434</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1906</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0418</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6#</td><td align="center" valign="middle" rowspan="1" colspan="1">1.4011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1629</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0938</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1741</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2822</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0318</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7#</td><td align="center" valign="middle" rowspan="1" colspan="1">5.8221</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1433</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1032</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2044</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0414</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8#</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9030</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1301</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1132</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1601</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2967</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0385</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8097</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1133</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0824</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1760</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3451</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0432</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-03470-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-03470-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of the proposed method with different methods on testing set (root mean square error, RMSE).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Radar ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PS (w/o Denoising)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FIR-PS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FIR-Kalman-PS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CNN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LSTM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CNN-LSTM (Ours)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1#</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8002</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2776</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1130</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2770</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2082</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0423</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2#</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2089</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1321</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0640</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2900</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2384</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0564</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3#</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3763</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1445</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0738</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2997</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2458</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0675</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4#</td><td align="center" valign="middle" rowspan="1" colspan="1">5.1337</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2300</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1145</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2666</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2853</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0869</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5#</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2721</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2191</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1033</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2264</td><td align="center" valign="middle" rowspan="1" colspan="1">1.5000</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0588</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6#</td><td align="center" valign="middle" rowspan="1" colspan="1">2.7862</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3662</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1235</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2645</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3404</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0427</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7#</td><td align="center" valign="middle" rowspan="1" colspan="1">5.8500</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2282</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1317</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2907</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2509</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0559</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8#</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0667</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2994</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1438</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2547</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3605</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0540</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.5598</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2371</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1084</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2712</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4287</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0581</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-03470-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-03470-t005_Table 5</object-id><label>Table 5</label><caption><p>Classification performance on testing set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Radar ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1#</td><td align="center" valign="middle" rowspan="1" colspan="1">98.33%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.36%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.98%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.16%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2#</td><td align="center" valign="middle" rowspan="1" colspan="1">98.84%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.97%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.86%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.42%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3#</td><td align="center" valign="middle" rowspan="1" colspan="1">97.97%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.14%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.97%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4#</td><td align="center" valign="middle" rowspan="1" colspan="1">94.48%</td><td align="center" valign="middle" rowspan="1" colspan="1">96.08%</td><td align="center" valign="middle" rowspan="1" colspan="1">97.67%</td><td align="center" valign="middle" rowspan="1" colspan="1">96.87%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5#</td><td align="center" valign="middle" rowspan="1" colspan="1">89.07%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.05%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.99%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.64%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6#</td><td align="center" valign="middle" rowspan="1" colspan="1">95.83%</td><td align="center" valign="middle" rowspan="1" colspan="1">95.41%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">97.56%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7#</td><td align="center" valign="middle" rowspan="1" colspan="1">95.62%</td><td align="center" valign="middle" rowspan="1" colspan="1">96.14%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.85%</td><td align="center" valign="middle" rowspan="1" colspan="1">97.48%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8#</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.08%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.88%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.90%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.37%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.90%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.48%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.68%</td></tr></tbody></table></table-wrap></floats-group></article>