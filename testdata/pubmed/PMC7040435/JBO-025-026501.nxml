<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Biomed Opt</journal-id><journal-id journal-id-type="iso-abbrev">J Biomed Opt</journal-id><journal-id journal-id-type="coden">JBOPFO</journal-id><journal-id journal-id-type="publisher-id">JBO</journal-id><journal-title-group><journal-title>Journal of Biomedical Optics</journal-title></journal-title-group><issn pub-type="ppub">1083-3668</issn><issn pub-type="epub">1560-2281</issn><publisher><publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">32100492</article-id><article-id pub-id-type="pmc">7040435</article-id><article-id pub-id-type="doi">10.1117/1.JBO.25.2.026501</article-id><article-id pub-id-type="publisher-manuscript">JBO-190401R</article-id><article-id pub-id-type="publisher-id">190401R</article-id><article-categories><subj-group subj-group-type="heading"><subject>Microscopy</subject></subj-group><subj-group subj-group-type="SPIE-art-type"><subject>Paper</subject></subj-group></article-categories><title-group><article-title>Automatic motion compensation for structured illumination endomicroscopy using a flexible fiber bundle</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0777-6218</contrib-id><name><surname>Thrapp</surname><given-names>Andrew D.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="b1"/><email>at600@kent.ac.uk</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5301-2492</contrib-id><name><surname>Hughes</surname><given-names>Michael R.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="b2"/><email>m.r.hughes@kent.ac.uk</email></contrib><aff id="aff1"><institution>University of Kent</institution>, School of Physical Sciences, Applied Optics Group, Canterbury, <country>United Kingdom</country></aff></contrib-group><author-notes><corresp id="cor1"><label>*</label>Address all correspondence to Andrew D. Thrapp, E-mail: <email>at600@kent.ac.uk</email></corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>2</month><year>2020</year></pub-date><pub-date pub-type="ppub"><month>2</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>25</day><month>2</month><year>2020</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>25</volume><issue>2</issue><elocation-id>026501</elocation-id><history><date date-type="received"><day>19</day><month>11</month><year>2019</year></date><date date-type="accepted"><day>21</day><month>1</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2020 The Authors</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>The Authors</copyright-holder><license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="JBO_25_2_026501.pdf"/><abstract><title>Abstract.</title><p><bold>Significance</bold>: Confocal laser scanning enables optical sectioning in clinical fiber bundle endomicroscopes, but lower-cost, simplified endomicroscopes use widefield incoherent illumination instead. Optical sectioning can be introduced in these simple systems using structured illumination microscopy (SIM), a multiframe digital subtraction process. However, SIM results in artifacts when the probe is in motion, making the technique difficult to use <italic>in vivo</italic> and preventing the use of mosaicking to synthesize a larger effective field of view (FOV).</p><p><bold>Aim</bold>: We report and validate an automatic motion compensation technique to overcome motion artifacts and allow generation of mosaics in SIM endomicroscopy.</p><p><bold>Approach</bold>: Motion compensation is achieved using image registration and real-time pattern orientation correction via a digital micromirror device. We quantify the similarity of moving probe reconstructions to those acquired with a stationary probe using the relative mean of the absolute differences (MAD). We further demonstrate mosaicking with a moving probe in mechanical and freehand operation.</p><p><bold>Results</bold>: Reconstructed SIM images show an improvement in the MAD from 0.85 to 0.13 for lens paper and from 0.27 to 0.12 for bovine tissue. Mosaics also show vastly reduced artifacts.</p><p><bold>Conclusion</bold>: The reduction in motion artifacts in individual SIM reconstructions leads to mosaics that more faithfully represent the morphology of tissue, giving clinicians a larger effective FOV than the probe itself can provide.</p></abstract><kwd-group><title>Keywords:</title><kwd>endomicroscopy</kwd><kwd>structured illumination</kwd><kwd>motion compensation</kwd><kwd>mosaicking</kwd><kwd>digital micromirror device</kwd></kwd-group><funding-group><award-group id="sp1"><funding-source>Engineering and Physical Sciences Research Council (EPSRC)<named-content content-type="fundref:id">https://doi.org/10.13039/501100000266</named-content></funding-source><award-id>EP/R019274/1</award-id></award-group></funding-group><counts><fig-count count="8"/><table-count count="2"/><ref-count count="35"/><page-count count="13"/></counts><custom-meta-group><custom-meta><meta-name>running-head</meta-name><meta-value>Thrapp and Hughes: Automatic motion compensation for structured illumination endomicroscopy&#x02026;</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p>Histopathology is the current &#x0201c;gold standard&#x0201d; for the diagnosis of epithelial cancers. Typically, the process involves a clinician removing one or more tissue samples from a patient. The tissue is then fixed, sliced, and stained before being examined under a microscope. This lengthy process introduces difficulties for rapid diagnosis or interoperative surgical guidance. A less-invasive alternative, an optical biopsy obtained using a fiber bundle-based endomicroscope, allows real-time microscopic imaging in otherwise inaccessible hollow tissue tracts or deep within hollow organs.<xref rid="r1" ref-type="bibr"><sup>1</sup></xref></p><p>Conventional histology has the benefit of providing physical sectioning by thinly slicing the tissue, allowing it to be observed under a conventional microscope. In an optical biopsy, as there is no physical sectioning, light from out-of-focus planes degrades the resulting images.<xref rid="r2" ref-type="bibr"><sup>2</sup></xref> This out-of-focus contribution can be rejected by introducing optical sectioning. One common technique is to use a confocal pinhole to block out-of-focus light from returning to the detector;<xref rid="r3" ref-type="bibr"><sup>3</sup></xref><named-content content-type="online"><xref rid="r4" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r5" ref-type="bibr"><sup>5</sup></xref> several commercial confocal endomicroscopy systems employ this approach such as the Mauna Kea Cellvizio<xref rid="r6" ref-type="bibr"><sup>6</sup></xref> (Paris, France) and Optiscan<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> system (Victoria, Australia). These systems are both fiber-based, use lasers, and require fast, precise scanning mechanisms.<xref rid="r8" ref-type="bibr"><sup>8</sup></xref> The scanning is either performed via a miniaturized fiber scanner sitting at the distal tip of the probe or using conventional scanning mirrors sitting proximal to the probe. In the latter case, the scanning pattern is relayed to the tissue via a coherent fiber imaging bundle with typically up to 30,000 cores.</p><p>Alternatively, a low-cost nonscanning endomicroscope can be built using widefield LED illumination delivered directly to the tissue via a fiber bundle with an image collected by the same bundle and imaged onto a camera via fluorescence filters.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref><named-content content-type="online"><xref rid="r10" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r11" ref-type="bibr"><sup>11</sup></xref> The fiber has no distal optics and is always in direct contact with the tissue, thus the surface of the sample is essentially deformed to be in constant contact with the probe tip. Such a system has virtually no inherent optical sectioning, but this can be added using structured-illumination microscopy (SIM),<xref rid="r12" ref-type="bibr"><sup>12</sup></xref><named-content content-type="online"><xref rid="r13" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r14" ref-type="bibr"><sup>14</sup></xref> requiring only a modification to the illumination optics to project periodic symmetric line patterns onto the bundle, and thus onto the tissue.</p><p>In SIM, a thin slice around focus is modulated by the illumination pattern while out-of-focus regions see a defocused pattern, which approximates uniform illumination. Three images are acquired, each with the modulation pattern shifted by <inline-formula><mml:math id="math1"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> of a period. The three images are then processed to recover an optically sectioned image.<xref rid="r15" ref-type="bibr"><sup>15</sup></xref><named-content content-type="online"><xref rid="r16" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r17" ref-type="bibr"><sup>17</sup></xref> In the ideal case, we can express the modulation <inline-formula><mml:math id="math2"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as <disp-formula id="e001"><mml:math id="math3"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mtext>&#x02009;</mml:mtext><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(1)</label></disp-formula>where <inline-formula><mml:math id="math4"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> is the modulation depth, which ranges from 0 (no modulation) to 1 (complete modulation), <inline-formula><mml:math id="math5"><mml:mrow><mml:mi>&#x003bd;</mml:mi></mml:mrow></mml:math></inline-formula> is the spatial frequency of modulation, chosen based on desired optical-sectioning strength, and <inline-formula><mml:math id="math6"><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> is the phase of the modulation for <inline-formula><mml:math id="math7"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>.<xref rid="r18" ref-type="bibr"><sup>18</sup></xref></p><p>Once the three raw images, <inline-formula><mml:math id="math8"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, have been acquired, the modulation is removed, and an optically sectioned image <inline-formula><mml:math id="math9"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>sim</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is obtained via three-phase demodulation:<xref rid="r18" ref-type="bibr"><sup>18</sup></xref>
<disp-formula id="e002"><mml:math id="math10"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>sim</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(2)</label></disp-formula></p><p>Endomicroscopes are used in several configurations, including through the working channel of an endoscope,<xref rid="r19" ref-type="bibr"><sup>19</sup></xref> as part of a robotic or mechanical system,<xref rid="r20" ref-type="bibr"><sup>20</sup></xref><sup>,</sup><xref rid="r21" ref-type="bibr"><sup>21</sup></xref> or in freehand operation.<xref rid="r22" ref-type="bibr"><sup>22</sup></xref> These three configurations all tend to involve a probe that is constantly, or often, in motion. In some circumstances, motion is deliberately introduced. A conventional biopsy is typically around <inline-formula><mml:math id="math11"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> in size, while a fiber endomicroscope has a typical probe diameter of <inline-formula><mml:math id="math12"><mml:mrow><mml:mo form="prefix">&#x0003c;</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>. This size difference can make it difficult for clinicians to get a broader view of the tissue morphology.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref> To increase the effective field of view (FOV), and consequently the amount of clinically relevant information, it is possible to merge sequentially acquired images during motion into a mosaic.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref><named-content content-type="online"><xref rid="r24" ref-type="bibr"/><xref rid="r25" ref-type="bibr"/><xref rid="r26" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r27" ref-type="bibr"><sup>27</sup></xref></p><p>However, since SIM is a three-frame acquisition process, motion between acquisition frames makes the recovered optically sectioned images susceptible to artifacts. The artifacts appear as blurring or chopping, explaining why mosaicking in structured illumination endomicroscopy has not been previously demonstrated. The shift between raw images due to motion of the probe can be corrected using registration providing it is sufficiently small relative to the frame rate. However, the consequence of this is that the line pattern is then no longer correctly positioned in the three images.</p><p>We have previously reported provisional results for cases when the motion is known in advance and hence where the lines can be aligned parallel to the motion, keeping the line pattern correctly positioned when images are registered and shifted. Once images have been shifted, SIM images can then be reconstructed in an overlapping area with reduced artifacts.<xref rid="r28" ref-type="bibr"><sup>28</sup></xref> The usefulness of this approach, however, is limited as motion is not typically known in advance and any change in the direction of motion would require changing the orientation of the line pattern. We now report an automatic feedback mechanism to compensate for a probe moving in any direction. We also demonstrate real-time mosaicking for lens tissue paper and bovine tissue using mechanical translation and freehand probe operation.</p></sec><sec id="sec2"><label>2</label><title>Methods</title><p>The basic operational principle in SIM endomicroscopy involves placing the tip of a multicore fiber-optic bundle (30,000 cores) at the focus of a microscope objective, projecting a periodic pattern of lines through the bundle and onto a sample, shifting the line pattern <inline-formula><mml:math id="math13"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> of a spatial period for two additional images, and processing the returning raw images. Various approaches have been used to generate the line pattern<xref rid="r12" ref-type="bibr"><sup>12</sup></xref><sup>,</sup><xref rid="r29" ref-type="bibr"><sup>29</sup></xref><sup>,</sup><xref rid="r30" ref-type="bibr"><sup>30</sup></xref> and similar to some previous reports, we use a digital micromirror device (DMD) (Texas Instruments DLP LightCrafter 3000). The automatic motion compensation technique we introduce has two components&#x02014;image registration and pattern correction. Image registration is the process of determining the angle and magnitude of motion, while pattern orientation correction is the reorientation of lines parallel to the motion.</p><sec id="sec2.1"><label>2.1</label><title>System Description</title><p>Light from the blue LED of the DMD&#x02019;s built-in illumination system is directed through a <inline-formula><mml:math id="math14"><mml:mrow><mml:mn>20</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mo>/</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> NA finite-conjugate objective (Newport), a 20-mm collimating lens (Thorlabs ACL2520U), an excitation filter (Thorlabs FESH0450), and reflected off of a 490-nm cut-on dichroic mirror (Thorlabs DMLP490) onto the back aperture of a <inline-formula><mml:math id="math15"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mo>/</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> NA finite-conjugate objective. It is then transferred through a fiber optic bundle (Sumitomo IGN-08/30) in pixelated form. The returning fluorescence emission is imaged onto a camera (Point Grey Flea3) via the <inline-formula><mml:math id="math16"><mml:mrow><mml:mn>4</mml:mn><mml:mo form="postfix">&#x000d7;</mml:mo></mml:mrow></mml:math></inline-formula> objective, dichroic, and an emission filter (Thorlabs FELH0500). A diagram, showing the components as well as optical path, is presented in <xref ref-type="fig" rid="f1">Fig.&#x000a0;1</xref>. A microcontroller (Pyboard 1.0) is used to control triggering of the camera and DMD (<xref ref-type="fig" rid="f3">Fig.&#x000a0;3</xref>). A data acquisition (DAQ) card (NI USB 6008) provides communication between the controlling personal computer (PC) and the microcontroller. At the object plane, each camera pixel corresponds to <inline-formula><mml:math id="math17"><mml:mrow><mml:mn>0.747</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>0.747</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with the <inline-formula><mml:math id="math18"><mml:mrow><mml:mn>4</mml:mn><mml:mtext>-</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> intercore spacing of the bundle, which provides better than Nyquist sampling. Each DMD pixel corresponds to <inline-formula><mml:math id="math19"><mml:mrow><mml:mn>4.407</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>4.407</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> measured at the object plane. The power at the fiber tip was measured to be <inline-formula><mml:math id="math20"><mml:mrow><mml:mn>95</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:math></inline-formula>, and the fiber coupling losses were measured to be 89%. The system has no optics on the distal end of the fiber, meaning images are acquired in contact mode only.</p><p>A custom-built LabVIEW (National Instruments) VI controls acquisition, registration, reconstruction, and pattern orientation. A summary of the program control loop is: (i)&#x000a0;the Labview program sends the required pattern angle to the microcontroller via the DAQ card. (ii)&#x000a0;The microcontroller selects the pattern orientation and triggers the necessary patterns on the DMD. (iii)&#x000a0;A four-frame image sequence is acquired (<xref ref-type="fig" rid="f2">Fig.&#x000a0;2</xref>) and a median filter is applied to the images. (iv)&#x000a0;The angle and magnitude of motion between widefield illumination frames are determined by image registration using template matching. The shift is used for reconstruction and mosaicking, and the angle is used for the next update of the pattern orientation. (v)&#x000a0;The raw frames are shifted to correct for motion between acquisition of each frame and an SIM image is reconstructed using Eq.&#x000a0;(2). (vi)&#x000a0;The reconstructed image is added into the mosaic.</p><fig id="f1" orientation="portrait" position="float"><label>Fig. 1</label><caption><p>Schematic of endomicroscope. DMD, digital micromirror device; OBJ 1, <inline-formula><mml:math id="math21"><mml:mrow><mml:mn>20</mml:mn><mml:mo form="postfix">&#x000d7;</mml:mo></mml:mrow></mml:math></inline-formula> objective; OBJ 2, <inline-formula><mml:math id="math22"><mml:mrow><mml:mn>4</mml:mn><mml:mo form="postfix">&#x000d7;</mml:mo></mml:mrow></mml:math></inline-formula> objective; L1, 20&#x000a0;mm collimating lens; L-DMD, lens built into DMD; SPF, short-pass filter; LPF, long-pass filter; DBS, dichroic beam splitter; and FB, fiber bundle.</p></caption><graphic xlink:href="JBO-025-026501-g001"/></fig></sec><sec id="sec2.2"><label>2.2</label><title>Image Acquisition</title><p>Using the custom built VI, a four-frame sequence is acquired using LabVIEW image acquisition libraries (IMAQdx). This consists of three SIM raw images and one widefield (i.e., unstructured) illumination image (<xref ref-type="fig" rid="f2">Fig.&#x000a0;2</xref>). The reason for introducing a widefield image is discussed in Sec.&#x000a0;<xref ref-type="sec" rid="sec2.3">2.3</xref>. Once images are captured, the honeycomb structure from the fiber bundle needs to be removed. There are several ways to accomplish this including two-dimensional (2-D) Gaussian filtering,<xref rid="r25" ref-type="bibr"><sup>25</sup></xref> spatial averaging,<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> and core localization and interpolation methods.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref> We filter the raw images using LabVIEW&#x02019;s IMAQ Nth-Order filter as a median filter with a <inline-formula><mml:math id="math23"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>pixel</mml:mi></mml:mrow></mml:math></inline-formula> neighborhood (<inline-formula><mml:math id="math24"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>)&#x02014;pixel values are arranged by intensity in descending order and the central intensity value is selected.</p><fig id="f2" orientation="portrait" position="float"><label>Fig. 2</label><caption><p>The four-frame acquisition sequence. The three SIM frames, with shifted line patterns, are book-ended by widefield illuminated frames used for image registration.</p></caption><graphic xlink:href="JBO-025-026501-g002"/></fig></sec><sec id="sec2.3"><label>2.3</label><title>Template Matching</title><p>To determine the probe&#x02019;s movement between image frames, we need to use template matching between overlapping frames. However, attempting to register two images with an overlapping grid pattern leads to incorrect registration when using a correlation-based approach. We instead register two widefield illumination images. Operationally, we accomplish this by adding one image to the end of the three-frame sequence and template matching the widefield illumination image from the current sequence with that of the previous (<xref ref-type="fig" rid="f2">Fig.&#x000a0;2</xref>). Using an approach previously reported by Lewis for fast template matching,<xref rid="r32" ref-type="bibr"><sup>32</sup></xref> we extract a <inline-formula><mml:math id="math25"><mml:mrow><mml:mn>250</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>250</mml:mn><mml:mtext>&#x02009;&#x02009;pixel</mml:mtext></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math26"><mml:mrow><mml:mn>187.5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>187.5</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) square from the center of the current widefield illumination frame as a template, then calculate the normalized cross-correlation (NCC) matrix against the previous widefield illumination frame of <inline-formula><mml:math id="math27"><mml:mrow><mml:mn>958</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>958</mml:mn><mml:mtext>&#x02009;&#x02009;pixels</mml:mtext></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math28"><mml:mrow><mml:mn>718.5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>718.5</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). The estimated shift is then taken to be the location of the maximum peak in the NCC matrix. Since the pulse structure (<xref ref-type="fig" rid="f3">Fig.&#x000a0;3</xref>) is designed to give uniformly spaced camera trigger pulses, we then assume uniform motion between each raw frame over the acquisition sequence and estimate the interframe shift between the raw SIM frames as <inline-formula><mml:math id="math29"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> the shift between the widefield illumination frames. Finally, we shift the first and third SIM images relative to the second and generate optically sectioned reconstructions using Eq.&#x000a0;(2).</p><fig id="f3" orientation="portrait" position="float"><label>Fig. 3</label><caption><p>Pattern trigger structure. Top pane shows the sequence of patterns loaded onto the DMD. Each pattern must be triggered in order, but the exposure can be varied. Nonuniform pulses are sent to the DMD, with longer exposures for the required patterns synchronized with camera exposure trigger pulses, as shown in lower pane. The pattern is designed so that the camera receives a uniform pulse structure.</p></caption><graphic xlink:href="JBO-025-026501-g003"/></fig></sec><sec id="sec2.4"><label>2.4</label><title>Pattern Orientation Correction and Image Reconstruction</title><p>As discussed above, simply registering and shifting the images is insufficient to prevent artifacts since the line patterns are no longer correctly in phase. To reduce or eliminate the artifacts, the lines need to be oriented parallel to the motion of the probe.</p><p>The DMD can be used in several ways. A PC can output patterns to the DMD via a high-definition multimedia interface or patterns can be preloaded onboard. Onboard pattern storage was used to take advantage of the faster pattern switching possible with this approach and to avoid software lag when sending patterns over a video stream. We loaded patterns of different angles (0&#x000a0;deg to 180 deg) onto the DMD. The onboard storage supports storing 96 patterns, meaning 31 angles are supported (also allowing for a widefield frame), permitting a set of 3 patterns every 5.625&#x000a0;deg.</p><p>Patterns are selected by the microcontroller using the angle determined in template matching. Although the DMD does not allow us to select individual patterns at the hardware level, it does allow us to &#x0201c;skip&#x0201d; patterns after a minimum display time. As shown graphically in <xref ref-type="fig" rid="f3">Fig.&#x000a0;3</xref>, the total acquisition time of a frame is equal to the exposure time (<inline-formula><mml:math id="math30"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>exp</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) added to the product of the minimum display time (<inline-formula><mml:math id="math31"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and the maximum number of unwanted frames between SIM patterns (<inline-formula><mml:math id="math32"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mtext>skipped</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). When fewer patterns are skipped, delays need to be introduced to keep the camera&#x02019;s frame rate constant. The formula for the theoretical maximum raw frame rate (<inline-formula><mml:math id="math33"><mml:mrow><mml:msub><mml:mrow><mml:mi>fps</mml:mi></mml:mrow><mml:mrow><mml:mtext>raw</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) is then given by <disp-formula id="e003"><mml:math id="math34"><mml:mrow><mml:msub><mml:mrow><mml:mi>fps</mml:mi></mml:mrow><mml:mrow><mml:mtext>raw</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mtext>skipped</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(3)</label></disp-formula></p><p>The reconstructed frame rate is then <inline-formula><mml:math id="math35"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> of the raw frame rate. With 30 skipped frames, a (<inline-formula><mml:math id="math36"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) of <inline-formula><mml:math id="math37"><mml:mrow><mml:mn>325</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula>, and an exposure of <inline-formula><mml:math id="math38"><mml:mrow><mml:mn>1000</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula>, this supports a theoretical maximum raw frame rate of 93 or 23.25&#x000a0;fps per reconstructed image. We acknowledge that this exposure is not ideal for many applications, including the acriflavine stained bovine stomach tissue issue images reported below, where we used an exposure of <inline-formula><mml:math id="math39"><mml:mrow><mml:mn>25,000</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula>. The theoretical maximum frame rate for this exposure duration is 28&#x000a0;fps raw frame rate or 7&#x000a0;fps per reconstructed image. For consistency, we operated at 10 or 2.5&#x000a0;fps reconstructed throughout all experiments reported here.</p><p>During template matching, the pattern angle is scaled by a factor of 180 and sent as an analog voltage using a DAQ (National Instruments USB-6008) to a Pyboard. The Pyboard converts the received voltage back to the angle and, using the pulse structure shown in <xref ref-type="fig" rid="f3">Fig.&#x000a0;3</xref>, selects the appropriate angle of illumination pattern and outputs trigger pulses to the camera and DMD. During unwanted patterns, the Pyboard triggers the DMD every <inline-formula><mml:math id="math40"><mml:mrow><mml:mn>325</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula> and does not trigger the camera. To ensure the four frames are equally spaced in time, the camera must be triggered uniformly and additional delays are introduced in DMD triggering to achieve this.</p><p>Once a sequence has completed and the VI has received the final widefield illumination image in a sequence, pattern orientation is not updated instantaneously, which takes 135&#x000a0;ms for template matching and the required angle to be received by the Pyboard. At frame rates below 20&#x000a0;fps, this voltage update is received by the Pyboard during the next cycle and the pattern orientation is changed after that cycle has completed.</p></sec><sec id="sec2.5"><label>2.5</label><title>Mosaicking</title><p>In order to assemble a mosaic as the probe moves, the position of each reconstructed frame relative to the previous is determined from the registration between widefield images. The SIM reconstructed image is cropped to a user-selected circle and added dead-leaf into the mosaic, overwriting any previous pixel values.</p></sec><sec id="sec2.6"><label>2.6</label><title>Validating Uniform Linear Probe Motion</title><p>Our approach assumes probe positions, which in five-frame sets are uniformly spaced and approximately linear. To verify these two assumptions, we capture widefield images with a probe being translated across a fluorescently stained lens paper sample, register the images, and plot a probe trajectory. We take neighboring sets of five points and interpolate the ends. To verify uniformity, we report the average distance between frames as well as the mean of the absolute deviations. To verify linearity, we calculate the distance between the interpolated line and the probe position. We then calculate combined error by determining the location of three evenly spaced points on the interpolated line and calculate the distance from the actual probe positions. Frames are captured at 120&#x000a0;fps and we also downsample the data to obtain a similar interframe shift to <xref ref-type="fig" rid="f7">Figs.&#x000a0;7</xref> and <xref ref-type="fig" rid="f8">8</xref>. The results of this study are shown in <xref rid="t001" ref-type="table">Table&#x000a0;1</xref>.</p><table-wrap id="t001" orientation="portrait" position="float"><label>Table 1</label><caption><p>Translation stage and freehand probe deviation from uniform and linear motion. Dist, interframe distance; MD, mean deviation from uniform motion; ED, mean Euclidean distance from interpolated line; CE, mean combined error from predicted probe positions. TS, translation stage; FH, freehand; ds, downsampled. Neighboring sets of five coordinates. Values less than <inline-formula><mml:math id="math41"><mml:mrow><mml:mn>6</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> are less than the resolution of the probe.</p></caption><!--OASIS TABLE HERE--><table frame="hsides" rules="groups"><colgroup><col/><col/><col/><col/><col/><col/><col/><col/></colgroup><thead><tr><th valign="top">&#x000a0;</th><th valign="top">Dist (<inline-formula><mml:math id="math42"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>)</th><th valign="top">MD (<inline-formula><mml:math id="math43"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>)</th><th valign="top">% MD</th><th valign="top">ED (<inline-formula><mml:math id="math44"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>)</th><th valign="top">% ED</th><th valign="top">CE (<inline-formula><mml:math id="math45"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>)</th><th valign="top">% CE</th></tr></thead><tbody><tr><td>TS</td><td>19.3</td><td>0.9</td><td>4.4</td><td>0.6</td><td>2.9</td><td>1.2</td><td>6.1</td></tr><tr><td><inline-formula><mml:math id="math46"><mml:mrow><mml:msub><mml:mrow><mml:mi>TS</mml:mi></mml:mrow><mml:mrow><mml:mi>ds</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td>77.0</td><td>1.6</td><td>2.1</td><td>1.4</td><td>1.8</td><td>2.5</td><td>3.2</td></tr><tr><td>FH</td><td>8.6</td><td>1.1</td><td>16.8</td><td>0.3</td><td>4.0</td><td>1.4</td><td>21.2</td></tr><tr><td><inline-formula><mml:math id="math47"><mml:mrow><mml:msub><mml:mrow><mml:mi>FH</mml:mi></mml:mrow><mml:mrow><mml:mi>ds</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td>77.5</td><td>19.3</td><td>27.2</td><td>2.4</td><td>3.3</td><td>22.7</td><td>32.9</td></tr></tbody></table></table-wrap><p>Using a translation stage with mean interframe shifts of 77.0 and <inline-formula><mml:math id="math48"><mml:mrow><mml:mn>19.3</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>, the combined errors are, respectively, 3.2% (<inline-formula><mml:math id="math49"><mml:mrow><mml:mn>2.5</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>) and 6.1% (<inline-formula><mml:math id="math50"><mml:mrow><mml:mn>1.2</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>). In freehand with interframe shifts of 77.6 and <inline-formula><mml:math id="math51"><mml:mrow><mml:mn>8.6</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>, the errors are, respectively, 32.9% (<inline-formula><mml:math id="math52"><mml:mrow><mml:mn>22.7</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>) and 21.2% (<inline-formula><mml:math id="math53"><mml:mrow><mml:mn>1.4</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>). Error is calculated in each five-frame set and averaged over all sets. With motion compensation, this predicts improvement in alignment in SIM reconstructions in all cases.</p></sec><sec id="sec2.7"><label>2.7</label><title>Line Spacing Selection</title><p>The line spacing is chosen based on a number of criteria. Since the line spacing determines the strength of the optical sectioning, the spacing must be small enough to sufficiently remove out-of-focus light,<xref rid="r33" ref-type="bibr"><sup>33</sup></xref> but large enough to minimize the effects of pixelation and discretization and for the approach to remain robust to motion.</p><p>To determine how much out-of-focus light is rejected at each line spacing, we characterized the axial response of the system by measuring the relative intensity drop-off as a function of distance (<xref ref-type="fig" rid="f4">Fig.&#x000a0;4</xref>). The study was performed by translating a smooth metal plate, stained with fluorescent marker, away from the tip of the fiber bundle at a known velocity using a translation stage (Newport M-UTM25PP1HL). The reconstruction intensity was then recorded as a function of distance by a custom-built LabVIEW VI.</p><fig id="f4" orientation="portrait" position="float"><label>Fig. 4</label><caption><p>Half-width half-maximum of axial response as a function of line spacing. Data acquired using a smooth metal plate, fluorescently labeled with yellow highlighter, translated away from fiber bundle tip using a linear translation stage. Error bars reported as precision of stage (<inline-formula><mml:math id="math54"><mml:mrow><mml:mn>5</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>).</p></caption><graphic xlink:href="JBO-025-026501-g004"/></fig><p>Before being sent to the DMD, lines need to be discretized, leading to errors at the line boundaries, which can degrade reconstructions. Each DMD pixel is a square arranged on a grid and, when the lines are aligned with this grid, line spacings can be chosen with an integer number of pixels with common factors of 2 (to allow for half on and off mirrors), and 3 (to allow for three phase positions), eliminating discretization error. However, when the lines are rotated on the square grid, as required for the motion compensation approach described, this leads to discretization artifacts near the boundaries. Since these only occur at boundaries, the relative effect of this can be reduced at the expense of optical-sectioning strength by increasing the spacing of the lines (in pixels).</p><p>Even in cases where there is no discretization error, the fiber bundle is also susceptible to pixelation artifacts at the line boundary since the cores are laid out in a pseudohoneycomb structure.<xref rid="r34" ref-type="bibr"><sup>34</sup></xref> Selecting a line spacing much greater than the intercore spacing similarly reduces the number of boundaries and minimizes these artifacts at the expense of optical-sectioning strength.</p><p>Since the DMD only supports patterns every 5.625&#x000a0;deg, probe motion often introduces additional misalignment, thus the line spacing selected must generate reconstructions that are robust to probe motion when the motion is not exactly parallel to the line pattern orientation. This was characterized via a numerical simulation of SIM reconstructions with a shift between raw frames. In the simulation, a circle representing a probe in contact with a uniform single-layered, in-focus sample was superimposed with a line pattern of the form of Eq.&#x000a0;(1). Three raw SIM frames were then acquired with the required phase shifts, with the probe also translated at some angle relative to the lines. The mean of the absolute differences (MAD) was then calculated between the moving SIM reconstruction and an ideal reconstruction in an overlapping area of the bundle. A <inline-formula><mml:math id="math55"><mml:mrow><mml:mn>5000</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5000</mml:mn><mml:mtext>&#x02009;&#x02009;pixel</mml:mtext></mml:mrow></mml:math></inline-formula> grid was used, the modulation depth was assumed to be 1 (i.e., ideal), and pixelation effects from the fiber bundle and discretization effects from the DMD were ignored. The circle&#x02019;s diameter was selected to match the fiber bundle (<inline-formula><mml:math id="math56"><mml:mrow><mml:mn>707</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>) and interframe shift (<inline-formula><mml:math id="math57"><mml:mrow><mml:mn>50</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>) used in the lens paper and bovine tissue studies below. The results showing the MAD as a function of line pair spacing and angular alignment error are shown in <xref ref-type="fig" rid="f5">Fig.&#x000a0;5</xref>.</p><fig id="f5" orientation="portrait" position="float"><label>Fig. 5</label><caption><p>Simulation of SIM reconstructions when probe translation is not exactly parallel to the line orientation. The angle is measured between the direction of movement and line orientation. The plot shows relative MAD of reconstructions with different line spacings. Simulation assumed a <inline-formula><mml:math id="math58"><mml:mrow><mml:mn>707</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> probe diameter and a <inline-formula><mml:math id="math59"><mml:mrow><mml:mn>50</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> shift between frames.</p></caption><graphic xlink:href="JBO-025-026501-g005"/></fig><p>Based on the results shown in <xref ref-type="fig" rid="f4">Figs.&#x000a0;4</xref> and <xref ref-type="fig" rid="f5">5</xref>, we chose 6.9 line-pairs per millimeter (<inline-formula><mml:math id="math60"><mml:mrow><mml:mi>lp</mml:mi><mml:mo>/</mml:mo><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>) or a line pair spacing of <inline-formula><mml:math id="math61"><mml:mrow><mml:mn>145</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="f4">Figure&#x000a0;4</xref> shows that with lines at <inline-formula><mml:math id="math62"><mml:mrow><mml:mn>6.9</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>lp</mml:mi><mml:mo>/</mml:mo><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, out-of-focus light emanating from <inline-formula><mml:math id="math63"><mml:mrow><mml:mo form="prefix">&#x0003e;</mml:mo><mml:mn>108</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> from focus is heavily attenuated. The simulation using <inline-formula><mml:math id="math64"><mml:mrow><mml:mn>6.9</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>lp</mml:mi><mml:mo>/</mml:mo><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> showed 95% similarity to static reference images (<inline-formula><mml:math id="math65"><mml:mrow><mml:mi>MAD</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) with an interframe shift of <inline-formula><mml:math id="math66"><mml:mrow><mml:mn>50</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> and line pair orientation off-parallel by 3&#x000a0;deg. At this line spacing, pixelation artifacts are also minimal, and since the <inline-formula><mml:math id="math67"><mml:mrow><mml:mo form="prefix">&#x0223c;</mml:mo><mml:mn>30</mml:mn><mml:mtext>&#x02009;&#x02009;pixels</mml:mtext></mml:mrow></mml:math></inline-formula> per line pair is much greater than the minimum 6 DMD pixels (<inline-formula><mml:math id="math68"><mml:mrow><mml:mn>4.5</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>) required for SIM, it is also much greater than the intercore spacing of the fiber bundle (<inline-formula><mml:math id="math69"><mml:mrow><mml:mn>3</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>). At this spacing, the modulation depth was measured to be 0.42 using the method reported by Hagen.<xref rid="r15" ref-type="bibr"><sup>15</sup></xref></p></sec></sec><sec id="sec3"><label>3</label><title>Results and Discussion</title><p>To demonstrate the improvement of SIM endomicroscopy over widefield, we first mounted the probe to be stationary and in contact with a piece of lens paper stained with yellow highlighter. To simulate a fluorescent background signal, the lens paper was placed on top of a piece of paper also stained with highlighter, and the two were separated by a thin plastic sheet. <xref ref-type="fig" rid="f6">Figure&#x000a0;6</xref> clearly shows the effect of optical sectioning and the improvement in contrast.</p><fig id="f6" orientation="portrait" position="float"><label>Fig. 6</label><caption><p>Fluorescently stained lens paper separated from a fluorescent background by a plastic sheet acquired with a stationary probe. Comparison of (a)&#x000a0;widefield and (b)&#x000a0;SIM images acquired with a fiber bundle probe. Bundle diameter: <inline-formula><mml:math id="math70"><mml:mrow><mml:mn>707</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>. Line spacing: 6.4&#x000a0;lines per mm. Scale bar <inline-formula><mml:math id="math71"><mml:mrow><mml:mn>100</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="JBO-025-026501-g006"/></fig><p>To validate motion compensation with a moving probe, lens paper and bovine stomach were prepared for imaging. The lens paper was stained with yellow highlighter and the bovine tissue with acriflavine hydrochloride.<xref rid="r22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="r35" ref-type="bibr"><sup>35</sup></xref> The probe was placed in light contact with the sample, then translated across the sample at a velocity giving <inline-formula><mml:math id="math72"><mml:mrow><mml:mn>50</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> between each raw image, corresponding to about 7% of the bundle&#x02019;s diameter. This interframe shift, and hence velocity, was chosen as it is an example of a case of greatest degradation due to motion artifacts (certain velocities lead to less degradation if the interframe shift happens to be close to an integer multiple of the line spacing). The shift between the two widefield frames used for template matching was <inline-formula><mml:math id="math73"><mml:mrow><mml:mn>200</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>, sufficiently small to provide an overlapping area for image registration. The maximum of the NCC as well as the relative MAD for the images in <xref ref-type="fig" rid="f7">Figs.&#x000a0;7</xref> and <xref ref-type="fig" rid="f8">8</xref> are reported in <xref rid="t002" ref-type="table">Table&#x000a0;2</xref>. Examples of individual images are shown above the generated mosaics in <xref ref-type="fig" rid="f7">Figs.&#x000a0;7</xref> and <xref ref-type="fig" rid="f8">8</xref>.</p><fig id="f7" orientation="portrait" position="float"><label>Fig. 7</label><caption><p>Moving probe SIM reconstructions and mosaics of lens paper stained with fluorescent highlighter. Lines at <inline-formula><mml:math id="math74"><mml:mrow><mml:mn>6.9</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>lp</mml:mi><mml:mo>/</mml:mo><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, frame rate of 2.5&#x000a0;fps per reconstructed image. Images (a)&#x02013;(d)&#x000a0;acquired with a translation stage (velocity was <inline-formula><mml:math id="math75"><mml:mrow><mml:mn>0.25</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>mm</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula>). (a)&#x000a0;Pattern correction off/image registration off, circle shows cropped area for mosaicking, (b)&#x000a0;pattern correction on/image registration off, (c)&#x000a0;pattern correction off/image registration on, (d)&#x000a0;pattern correction on/image registration on, (e)&#x000a0;freehand images&#x02014;pattern correction on/image registration on. Cropped area used for mosaicking to eliminate residual lines near edges of reconstructions.</p></caption><graphic xlink:href="JBO-025-026501-g007"/></fig><fig id="f8" orientation="portrait" position="float"><label>Fig. 8</label><caption><p>Moving probe SIM reconstructions and mosaics of bovine stomach labeled with acriflavine. Lines at <inline-formula><mml:math id="math76"><mml:mrow><mml:mn>6.9</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>lp</mml:mi><mml:mo>/</mml:mo><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, frame rate of 2.5&#x000a0;fps per reconstructed image. Images (a)&#x02013;(d)&#x000a0;acquired with a translation stage (velocity was <inline-formula><mml:math id="math77"><mml:mrow><mml:mn>0.25</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>mm</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula>). (a)&#x000a0;Pattern correction off/image registration off, circle shows cropped area for mosaicking, (b)&#x000a0;pattern correction on/image registration off, (c)&#x000a0;pattern correction off/image registration on, (d)&#x000a0;pattern correction on/image registration on, (e)&#x000a0;freehand images&#x02014;pattern correction on/image registration on. Intensity values uniformly scaled by 2.5. Cropped area used for mosaicking to eliminate residual lines near edges of reconstructions.</p></caption><graphic xlink:href="JBO-025-026501-g008"/></fig><table-wrap id="t002" orientation="portrait" position="float"><label>Table 2</label><caption><p>Relative MAD measurements as well as maximum of NCC matrix for lens paper and bovine stomach tissue. Obtained for lines at <inline-formula><mml:math id="math78"><mml:mrow><mml:mn>6.9</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>lp</mml:mi><mml:mo>/</mml:mo><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> and with translation of <inline-formula><mml:math id="math79"><mml:mrow><mml:mn>50</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> between raw images. Pat., pattern orientation correction and Reg., image registration.</p></caption><!--OASIS TABLE HERE--><table frame="hsides" rules="groups"><colgroup><col/><col/><col/><col/><col/></colgroup><thead><tr><th valign="top">&#x000a0;</th><th valign="top">Pat. off, reg. off</th><th valign="top">Pat. on, reg. off</th><th valign="top">Pat. off, reg. on</th><th valign="top">Pat. on, reg. on</th></tr></thead><tbody><tr><td colspan="5">Lens paper</td></tr><tr><td>NCC</td><td>0.44</td><td>0.39</td><td>0.97</td><td>0.97</td></tr><tr><td>MAD</td><td>0.85</td><td>0.91</td><td>0.14</td><td>0.13</td></tr><tr><td colspan="5">Bovine stomach</td></tr><tr><td>NCC</td><td>0.56</td><td>0.66</td><td>0.54</td><td>0.94</td></tr><tr><td>MAD</td><td>0.27</td><td>0.26</td><td>0.75</td><td>0.12</td></tr></tbody></table></table-wrap><p>It is clear from <xref ref-type="fig" rid="f2">Fig.&#x000a0;2</xref> that introducing the motion compensation approach significantly reduced motion artifacts in both the lens paper and bovine stomach reconstructions. The improvement in the MAD value for lens paper was a decrease from 0.85 to 0.13, and there was a decrease from 0.27 to 0.12 for the bovine stomach tissue. NCC scores increased from 0.44 to 0.97 for lens paper and from 0.56 to 0.94 for bovine stomach tissue. The mosaics also show optically sectioned images with reduced motion artifacts. As discussed below, cropping before mosaicking was sufficient to ensure residual artifacts were also reduced. In both cases, the combination of registration and pattern orientation correction yielded the greatest improvement over uncorrected reconstructions and reconstructions where only pattern orientation or registration was active.</p><p>However, some motion that cannot be corrected by this technique includes motion that occurs during the exposure. Effects from this occur as they do in SIM endomicroscopy, widefield endomicroscopy, and confocal endomicroscopy. In SIM endomicroscopy and widefield endomicroscopy, they appear as blur, and in confocal they appear as distortion. When features in a sample do not move with the probe, they may appear as repeated features as in <xref ref-type="fig" rid="f8">Fig.&#x000a0;8</xref>, which, as can be seen, did not break the continuity of the mosaic.</p><p>Some artifacts toward the edges of the images were observed. These are seen as more pronounced residual grid patterns in the image periphery, as marked with an arrow in <xref ref-type="fig" rid="f8">Fig.&#x000a0;8(d)</xref>. The effect arises because, in a single widefield image, the intensity profile from out-of-focus depths is roughly uniform in the center of the probe but drops off near the edges. This is due to the effect of the edge of the bundle; out-of-focus depths nearer the edge receive less illumination than more central points. This does not matter for conventional SIM imaging as this is common to all three images, and the out-of-focus light is subsequently removed. However, it introduces a complication when reconstructing SIM images of a moving probe, as the three registered and shifted SIM images now have slightly different intensity profiles for the out-of-focus light contributions, leading to incomplete removal. These artifacts can be mitigated at the expense of a smaller FOV by cropping the raw images to the overlapping region with roughly uniform intensity. This has a similar effect to introducing compensating illumination surrounding a smaller diameter bundle. It may also be possible to computationally correct the raw images, which is a topic for further investigation.</p><p>Artifacts also occur when the motion of the probe is not perfectly parallel to the alignment of the patterns. The effects of slight misalignment caused by translation at an angle from parallel can be seen in <xref ref-type="fig" rid="f7">Fig.&#x000a0;7(e)</xref> as streaks in the final mosaic. (These are unlikely to be edge artifacts as the cropping circle was set to half of the probe&#x02019;s diameter where the widefield images had roughly uniform intensity.) The misaligment artifact partly arises due to the finite number of angles supported by the DMD, meaning that the direction of translation has to be rounded to the nearest supported angle.</p><p>Although the FOV of the individual motion-compensated moving SIM images is reduced, compared with conventional SIM there is no effective reduction of the usable FOV in any case. When the probe is stationary, the device performs as a conventional SIM; when the probe is in motion, a conventional SIM does not work at all. With motion compensation, the FOV is reduced and mosaicking is then possible, allowing for a larger effective FOV.</p><p>Other artifacts include repeated features in the freehand mosaics for bovine stomach. These were not observed in the tissue paper study, and so a likely cause is misregistration of tissue deformed during translation of the probe in contact mode (only rigid transformations were allowed for by the algorithm). Additionally, the latency associated with pattern selection meant that pattern orientation was always one acquisition sequence behind, leading to artifacts when the direction of travel changed between acquisition sequences. This is mostly caused by software limitations. Although template matching was benchmarked faster than the maximum camera frame rate (60&#x000a0;Hz), the updated angle was still received by the Pyboard shortly after the next sequence had started. Another source of latency was the speed at which LabVIEW determined the angle and sent the angle to the Pyboard via the DAQ. This could be reduced using a serial port to communicate with the Pyboard directly instead of analog pins, removing any delay in communication with the DAQ. A topic for further investigation is devising a scheme that would take advantage of both the fast template matching and serial data port to eliminate the pattern orientation lag.</p><p>With a mounted operation using a translation stage, in a configuration where angles do not need to be updated each cycle, artifacts caused by latency are mostly eliminated since the angle is the same for a large number of acquisition sequences. Similarly, handheld operation is more susceptible to artifacts resulting from deviations in movement parallel to line orientation. These would be reduced with a faster camera and DMD pattern selection time.</p><p>The frame rate of 2.5&#x000a0;fps was due to the lower duty cycle at higher frame rates forcing the camera exposure to be reduced. This arises from use of a low-cost off-the-shelf DMD with an onboard LED, which, without adding active cooling, had optical power limits and a DMD controller that does not allow patterns to be selected arbitrarily. Unfortunately, as each pattern has a minimum display time of <inline-formula><mml:math id="math80"><mml:mrow><mml:mn>325</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula>, as the frame rate is increased, the duty cycle decreases due to an increased fraction of the overall time needed to wait while unwanted frames are skipped. Pattern selection time and optical power are not fundamental limitations of this approach. Pattern selection time could be increased with a more sophisticated DMD controller that allowed selection of arbitrary patterns. The power at the fiber tip was measured to be <inline-formula><mml:math id="math81"><mml:mrow><mml:mn>95</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:math></inline-formula>, and the fiber coupling losses were measured to be 89%. Power could be increased by adding active cooling to the DMD, optimizing lost light at the objective/bundle interface, or exchanging the DMD&#x02019;s built-in lens and LED with a high-power light source.</p></sec><sec id="sec4"><label>4</label><title>Conclusion</title><p>Endomicroscopy is a useful tool for point-of-care diagnosis of epithelial cancers. A low-cost fiber bundle-based-SIM endomicroscope can be built with a probe area much less than the area of a conventional biopsy, motivating the need for mosaicking. During the three-frame SIM acquisition sequence, motion between raw frames can generate artifacts, which limit mosaicking. The findings presented here show that by reorienting patterns to the direction of motion, and by registering and shifting the images, it is possible to greatly reduce these artifacts. This will further advance the clinical relevance of SIM-based endomicroscopy.</p></sec></body><back><ack><title>Acknowledgments</title><p>This work was supported by the Engineering and Physical Sciences Research Council (EPSRC); &#x0201c;Ultrathin fluorescence microscope in a needle&#x0201d; Award No.&#x000a0;EP/R019274/1. Andrew Thrapp was supported by the University of Kent Vice Chancellor&#x02019;s PhD Scholarship.</p></ack><notes notes-type="conflict-of-interest"><title>Disclosures</title><p>The authors have no relevant financial interests in this article and no potential conflicts of interest to disclose.</p></notes><ref-list><title>References</title><ref id="r1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J. T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Point-of-care pathology with miniature microscopes</article-title>,&#x0201d; <source>Anal. Cell. Pathol.</source>
<volume>34</volume>(<issue>3</issue>), <fpage>81</fpage>&#x02013;<lpage>98</lpage> (<year>2011</year>).<pub-id pub-id-type="doi">10.1155/2011/657403</pub-id></mixed-citation></ref><ref id="r2"><label>2.</label><mixed-citation publication-type="patent"><person-group person-group-type="author"><name><surname>Minsky</surname><given-names>M.</given-names></name></person-group>, &#x0201c;<article-title>Microscopy apparatus</article-title>,&#x0201d; US Patent <patent>3,013,467</patent> (<year>1957</year>).</mixed-citation></ref><ref id="r3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gmitro</surname><given-names>A. F.</given-names></name><name><surname>Aziz</surname><given-names>D.</given-names></name></person-group>, &#x0201c;<article-title>Confocal microscopy through a fiber-optic imaging bundle</article-title>,&#x0201d; <source>Opt. Lett.</source>
<volume>18</volume>(<issue>8</issue>), <fpage>565</fpage>&#x02013;<lpage>567</lpage> (<year>1993</year>).<pub-id pub-id-type="coden">OPLEDP</pub-id><issn>0146-9592</issn><pub-id pub-id-type="doi">10.1364/OL.18.000565</pub-id><pub-id pub-id-type="pmid">19802201</pub-id></mixed-citation></ref><ref id="r4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Goualher</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Towards optical biopsies with an integrated fibered confocal fluorescence microscope</article-title>,&#x0201d; <source>Lect. Notes Comput. Sci.</source>
<volume>3217</volume>, <fpage>761</fpage>&#x02013;<lpage>768</lpage> (<year>2004</year>).<pub-id pub-id-type="coden">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type="doi">10.1007/b100270</pub-id></mixed-citation></ref><ref id="r5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Confocal laser endomicroscopy: technical status and current indications</article-title>,&#x0201d; <source>Endoscopy</source>
<volume>38</volume>(<issue>12</issue>), <fpage>1275</fpage>&#x02013;<lpage>1283</lpage> (<year>2006</year>).<pub-id pub-id-type="coden">ENDCAM</pub-id><pub-id pub-id-type="doi">10.1055/s-2006-944813</pub-id><pub-id pub-id-type="pmid">17163333</pub-id></mixed-citation></ref><ref id="r6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laemmel</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Fibered confocal fluorescence microscopy (cell-vizio) facilitates extended imaging in the field of microcirculation. A comparison with intravital microscopy</article-title>,&#x0201d; <source>J. Vasc. Res.</source>
<volume>41</volume>(<issue>5</issue>), <fpage>400</fpage>&#x02013;<lpage>411</lpage> (<year>2004</year>).<pub-id pub-id-type="doi">10.1159/000081209</pub-id><pub-id pub-id-type="pmid">15467299</pub-id></mixed-citation></ref><ref id="r7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polglase</surname><given-names>A. L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>A fluorescence confocal endomicroscope for in vivo microscopy of the upper- and the lower-GI tract</article-title>,&#x0201d; <source>Gastrointest. Endosc.</source>
<volume>62</volume>(<issue>5</issue>), <fpage>686</fpage>&#x02013;<lpage>695</lpage> (<year>2005</year>).<pub-id pub-id-type="doi">10.1016/j.gie.2005.05.021</pub-id><pub-id pub-id-type="pmid">16246680</pub-id></mixed-citation></ref><ref id="r8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ford</surname><given-names>T. N.</given-names></name><name><surname>Lim</surname><given-names>D.</given-names></name><name><surname>Mertz</surname><given-names>J.</given-names></name></person-group>, &#x0201c;<article-title>Fast optically sectioned fluorescence Hilo endomicroscopy</article-title>,&#x0201d; <source>J. Biomed. Opt.</source>
<volume>17</volume>(<issue>2</issue>), <fpage>021105</fpage> (<year>2012</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.17.2.021105</pub-id><pub-id pub-id-type="pmid">22463023</pub-id></mixed-citation></ref><ref id="r9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muldoon</surname><given-names>T. J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Subcellular-resolution molecular imaging within living tissue by fiber microendoscopy</article-title>,&#x0201d; <source>Opt. Express</source>
<volume>15</volume>(<issue>25</issue>), <fpage>16413</fpage>&#x02013;<lpage>16423</lpage> (<year>2007</year>).<pub-id pub-id-type="coden">OPEXFF</pub-id><issn>1094-4087</issn><pub-id pub-id-type="doi">10.1364/OE.15.016413</pub-id><pub-id pub-id-type="pmid">19550931</pub-id></mixed-citation></ref><ref id="r10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>B. D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>High-resolution microendoscopy: a point-of-care diagnostic for cervical dysplasia in low-resource settings</article-title>,&#x0201d; <source>Eur. J. Cancer Prev.</source>
<volume>26</volume>(<issue>1</issue>), <fpage>63</fpage>&#x02013;<lpage>70</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">EJUPEK</pub-id><issn>0959-8278</issn><pub-id pub-id-type="doi">10.1097/CEJ.0000000000000219</pub-id><pub-id pub-id-type="pmid">26637074</pub-id></mixed-citation></ref><ref id="r11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidwai</surname><given-names>S. M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Optical imaging with a high-resolution microendoscope to identify sinonasal pathology</article-title>,&#x0201d; <source>Am. J. Otolaryngol.</source>
<volume>39</volume>(<issue>4</issue>), <fpage>383</fpage>&#x02013;<lpage>387</lpage> (<year>2018</year>).<pub-id pub-id-type="coden">AJOTDP</pub-id><issn>0196-0709</issn><pub-id pub-id-type="doi">10.1016/j.amjoto.2018.03.025</pub-id><pub-id pub-id-type="pmid">29622347</pub-id></mixed-citation></ref><ref id="r12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bozinovic</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Fluorescence endomicroscopy with structured illumination</article-title>,&#x0201d; <source>Opt. Express</source>
<volume>16</volume>(<issue>11</issue>), <fpage>8016</fpage>&#x02013;<lpage>8025</lpage> (<year>2008</year>).<pub-id pub-id-type="coden">OPEXFF</pub-id><issn>1094-4087</issn><pub-id pub-id-type="doi">10.1364/OE.16.008016</pub-id><pub-id pub-id-type="pmid">18545511</pub-id></mixed-citation></ref><ref id="r13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kyrish</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Needle-based fluorescence endomicroscopy via structured illumination with a plastic, achromatic objective</article-title>,&#x0201d; <source>J. Biomed. Opt.</source>
<volume>18</volume>(<issue>9</issue>), <fpage>096003</fpage> (<year>2013</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.18.9.096003</pub-id><pub-id pub-id-type="pmid">24002190</pub-id></mixed-citation></ref><ref id="r14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keahey</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Differential structured illumination microendoscopy for in vivo imaging of molecular contrast agents</article-title>,&#x0201d; <source>Proc. Natl. Acad. Sci. U. S. A.</source>
<volume>113</volume>(<issue>39</issue>), <fpage>10769</fpage>&#x02013;<lpage>10773</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">PNASA6</pub-id><issn>0027-8424</issn><pub-id pub-id-type="doi">10.1073/pnas.1613497113</pub-id><pub-id pub-id-type="pmid">27621464</pub-id></mixed-citation></ref><ref id="r15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagen</surname><given-names>N.</given-names></name><name><surname>Gao</surname><given-names>L.</given-names></name><name><surname>Tkaczyk</surname><given-names>T. S.</given-names></name></person-group>, &#x0201c;<article-title>Quantitative sectioning and noise analysis for structured illumination microscopy</article-title>,&#x0201d; <source>Opt. Express</source>
<volume>20</volume>(<issue>1</issue>), <fpage>403</fpage>&#x02013;<lpage>413</lpage> (<year>2012</year>).<pub-id pub-id-type="coden">OPEXFF</pub-id><issn>1094-4087</issn><pub-id pub-id-type="doi">10.1364/OE.20.000403</pub-id><pub-id pub-id-type="pmid">22274364</pub-id></mixed-citation></ref><ref id="r16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karadaglic</surname><given-names>D.</given-names></name><name><surname>Wilson</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>Image formation in structured illumination wide-field fluorescence microscopy</article-title>,&#x0201d; <source>Micron</source>
<volume>39</volume>(<issue>7</issue>), <fpage>808</fpage>&#x02013;<lpage>818</lpage> (<year>2008</year>).<pub-id pub-id-type="coden">MICNB2</pub-id><issn>0047-7206</issn><pub-id pub-id-type="doi">10.1016/j.micron.2008.01.017</pub-id><pub-id pub-id-type="pmid">18337108</pub-id></mixed-citation></ref><ref id="r17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>Optical sectioning in fluorescence microscopy</article-title>,&#x0201d; <source>J. Microsc.</source>
<volume>242</volume>(<issue>2</issue>), <fpage>111</fpage>&#x02013;<lpage>116</lpage> (<year>2011</year>).<pub-id pub-id-type="coden">JMICAR</pub-id><issn>0022-2720</issn><pub-id pub-id-type="doi">10.1111/jmi.2011.242.issue-2</pub-id><pub-id pub-id-type="pmid">21118248</pub-id></mixed-citation></ref><ref id="r18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neil</surname><given-names>M. A. A.</given-names></name><name><surname>Juskaitis</surname><given-names>R.</given-names></name><name><surname>Wilson</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>Method of obtaining optical sectioning by using structured light in a conventional microscope</article-title>,&#x0201d; <source>Opt. Lett.</source>
<volume>22</volume>(<issue>24</issue>), <fpage>1905</fpage>&#x02013;<lpage>1907</lpage> (<year>1997</year>).<pub-id pub-id-type="coden">OPLEDP</pub-id><issn>0146-9592</issn><pub-id pub-id-type="doi">10.1364/OL.22.001905</pub-id><pub-id pub-id-type="pmid">18188403</pub-id></mixed-citation></ref><ref id="r19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flusberg</surname><given-names>B. A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Fiber-optic fluorescence imaging</article-title>,&#x0201d; <source>Nat. Methods</source>
<volume>2</volume>(<issue>12</issue>), <fpage>941</fpage>&#x02013;<lpage>950</lpage> (<year>2005</year>).<issn>1548-7091</issn><pub-id pub-id-type="doi">10.1038/nmeth820</pub-id><pub-id pub-id-type="pmid">16299479</pub-id></mixed-citation></ref><ref id="r20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname><given-names>M. J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>En-face optical coherence tomography/fluorescence endomicroscopy for minimally invasive imaging using a robotic scanner</article-title>,&#x0201d; <source>J. Biomed. Opt.</source>
<volume>24</volume>(<issue>6</issue>), <fpage>066006</fpage> (<year>2019</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.24.6.066006</pub-id></mixed-citation></ref><ref id="r21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosa</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Online robust endomicroscopy video mosaicking using robot prior</article-title>,&#x0201d; <source>IEEE Rob. Autom. Lett.</source>
<volume>3</volume>(<issue>4</issue>), <fpage>4163</fpage>&#x02013;<lpage>4170</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1109/LRA.2018.2863372</pub-id></mixed-citation></ref><ref id="r22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>T. P.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Imaging breast cancer morphology using probe-based confocal laser endomicroscopy: towards a real-time intraoperative imaging tool for cavity scanning</article-title>,&#x0201d; <source>Breast Cancer Res. Treat.</source>
<volume>153</volume>(<issue>2</issue>), <fpage>299</fpage>&#x02013;<lpage>310</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">BCTRD6</pub-id><pub-id pub-id-type="doi">10.1007/s10549-015-3543-8</pub-id><pub-id pub-id-type="pmid">26283299</pub-id></mixed-citation></ref><ref id="r23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedard</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Real-time video mosaicing with a high-resolution microendoscope</article-title>,&#x0201d; <source>Biomed. Opt. Express</source>
<volume>3</volume>(<issue>10</issue>), <fpage>2428</fpage>&#x02013;<lpage>2435</lpage> (<year>2012</year>).<pub-id pub-id-type="coden">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type="doi">10.1364/BOE.3.002428</pub-id><pub-id pub-id-type="pmid">23082285</pub-id></mixed-citation></ref><ref id="r24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewke</surname><given-names>K. E.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Real-time image mosaicing for medical applications</article-title>,&#x0201d; <source>Stud. Health Technol. Inf.</source>
<volume>125</volume>, <fpage>304</fpage>&#x02013;<lpage>309</lpage> (<year>2007</year>).<pub-id pub-id-type="coden">SHTIEW</pub-id><issn>0926-9630</issn></mixed-citation></ref><ref id="r25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>M.</given-names></name><name><surname>Yang</surname><given-names>G. Z.</given-names></name></person-group>, &#x0201c;<article-title>High speed, line-scanning, fiber bundle fluorescence confocal endomicroscopy for improved mosaicking</article-title>,&#x0201d; <source>Biomed. Opt. Express</source>
<volume>6</volume>(<issue>4</issue>), <fpage>1241</fpage>&#x02013;<lpage>1252</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type="doi">10.1364/BOE.6.001241</pub-id><pub-id pub-id-type="pmid">25909008</pub-id></mixed-citation></ref><ref id="r26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vercauteren</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Real time autonomous video image registration for endomicroscopy: fighting the compromises</article-title>,&#x0201d; <source>Proc. SPIE</source>
<volume>6861</volume>, <fpage>68610C</fpage> (<year>2008</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.763089</pub-id></mixed-citation></ref><ref id="r27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewke</surname><given-names>K. E.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>In vivo micro-image mosaicing</article-title>,&#x0201d; <source>IEEE Trans. Biomed. Eng.</source>
<volume>58</volume>(<issue>1</issue>), <fpage>159</fpage>&#x02013;<lpage>171</lpage> (<year>2011</year>).<pub-id pub-id-type="coden">IEBEAX</pub-id><issn>0018-9294</issn><pub-id pub-id-type="doi">10.1109/TBME.2010.2085082</pub-id><pub-id pub-id-type="pmid">20934939</pub-id></mixed-citation></ref><ref id="r28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thrapp</surname><given-names>A. D.</given-names></name><name><surname>Hughes</surname><given-names>M. R.</given-names></name></person-group>, &#x0201c;<article-title>Motion compensation in structured illumination fluorescence endomicroscopy</article-title>,&#x0201d; <source>Proc. SPIE</source>
<volume>10854</volume>, <fpage>108541D</fpage> (<year>2019</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.2509590</pub-id></mixed-citation></ref><ref id="r29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dan</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>DMD-based led-illumination super-resolution and optical sectioning microscopy</article-title>,&#x0201d; <source>Sci. Rep.</source>
<volume>3</volume>, <fpage>1116</fpage> (<year>2013</year>).<pub-id pub-id-type="coden">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type="doi">10.1038/srep01116</pub-id><pub-id pub-id-type="pmid">23346373</pub-id></mixed-citation></ref><ref id="r30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Fast optical sectioning obtained by structured illumination microscopy using a digital mirror device</article-title>,&#x0201d; <source>J. Biomed. Opt.</source>
<volume>18</volume>(<issue>6</issue>), <fpage>060503</fpage> (<year>2013</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.18.6.060503</pub-id><pub-id pub-id-type="pmid">23757041</pub-id></mixed-citation></ref><ref id="r31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinde</surname><given-names>A.</given-names></name><name><surname>Matham</surname><given-names>M. V.</given-names></name></person-group>, &#x0201c;<article-title>Pixelate removal in an image fiber probe endoscope incorporating comb structure removal methods</article-title>,&#x0201d; <source>J. Med. Imaging Health Inf.</source>
<volume>4</volume>(<issue>2</issue>), <fpage>203</fpage>&#x02013;<lpage>211</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1166/jmihi.2014.1255</pub-id></mixed-citation></ref><ref id="r32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>J. P.</given-names></name></person-group>, &#x0201c;<article-title>Fast template matching</article-title>,&#x0201d; <source>Vision Interface</source>
<volume>95</volume>, <fpage>120</fpage>&#x02013;<lpage>123</lpage> (<year>1995</year>).</mixed-citation></ref><ref id="r33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keahey</surname><given-names>P. A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Optimizing modulation frequency for structured illumination in a fiber-optic microendoscope to image nuclear morphometry in columnar epithelium</article-title>,&#x0201d; <source>Biomed. Opt. Express</source>
<volume>6</volume>(<issue>3</issue>), <fpage>870</fpage>&#x02013;<lpage>880</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type="doi">10.1364/BOE.6.000870</pub-id><pub-id pub-id-type="pmid">25798311</pub-id></mixed-citation></ref><ref id="r34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perperidis</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Image computing for fibre-bundle endomicroscopy: a review</article-title>,&#x0201d; <source>Med. Image Anal.</source>
<fpage>101620</fpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1016/j.media.2019.101620</pub-id><pub-id pub-id-type="pmid">32279053</pub-id></mixed-citation></ref><ref id="r35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obstoy</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Safety and performance analysis of acriflavine and methylene blue for in vivo imaging of precancerous lesions using fibered confocal fluorescence microscopy (FCFM): an experimental study</article-title>,&#x0201d; <source>BMC Pulm. Med.</source>
<volume>15</volume>, <fpage>30</fpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1186/s12890-015-0020-4</pub-id><pub-id pub-id-type="pmid">25880748</pub-id></mixed-citation></ref></ref-list><bio id="b1"><p><bold>Andrew D. Thrapp</bold> received his bachelor&#x02019;s degree in physics with a minor in mathematics from the University of Colorado in 2017. He is a PhD student at the University of Kent. His current research interests include optical-sectioning in microscopy and endomicroscopy.</p></bio><bio id="b2"><p><bold>Michael R. Hughes</bold> is a lecturer in applied optics at the University of Kent. His research interests are in endoscopic and needle-based microscopes, and computational approaches to low-cost and point-of-care microscopy.</p></bio></back></article>