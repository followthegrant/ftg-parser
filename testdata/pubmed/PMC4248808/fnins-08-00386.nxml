<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="review-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1662-4548</issn><issn pub-type="epub">1662-453X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">25520611</article-id><article-id pub-id-type="pmc">4248808</article-id><article-id pub-id-type="doi">10.3389/fnins.2014.00386</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Review Article</subject></subj-group></subj-group></article-categories><title-group><article-title>Neural pathways for visual speech perception</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bernstein</surname><given-names>Lynne E.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://community.frontiersin.org/people/u/27731"/></contrib><contrib contrib-type="author"><name><surname>Liebenthal</surname><given-names>Einat</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://community.frontiersin.org/people/u/3213"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Speech and Hearing Sciences, George Washington University</institution><country>Washington, DC, USA</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Neurology, Medical College of Wisconsin</institution><country>Milwaukee, WI, USA</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Psychiatry, Brigham and Women's Hospital</institution><country>Boston, MA, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Josef P. Rauschecker, Georgetown University School of Medicine, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Ruth Campbell, University College London, UK; Josef P. Rauschecker, Georgetown University School of Medicine, USA; Kaisa Tiippana, University of Helsinki, Finland</p></fn><corresp id="fn001">*Correspondence: Lynne E. Bernstein, Communication Neuroscience Laboratory, Department of Speech and Hearing Science, George Washington University, 550 Rome Hall, 810 22nd Street, NW Washington, DC 20052, USA e-mail: <email xlink:type="simple">lbernste@gwu.edu</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to the journal Frontiers in Neuroscience.</p></fn></author-notes><pub-date pub-type="epub"><day>01</day><month>12</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>8</volume><elocation-id>386</elocation-id><history><date date-type="received"><day>25</day><month>7</month><year>2014</year></date><date date-type="accepted"><day>10</day><month>11</month><year>2014</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2014 Bernstein and Liebenthal.</copyright-statement><copyright-year>2014</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>This paper examines the questions, what levels of speech can be perceived visually, and how is visual speech represented by the brain? Review of the literature leads to the conclusions that every level of psycholinguistic speech structure (i.e., phonetic features, phonemes, syllables, words, and prosody) can be perceived visually, although individuals differ in their abilities to do so; and that there are visual modality-specific representations of speech <italic>qua</italic> speech in higher-level vision brain areas. That is, the visual system represents the modal patterns of visual speech. The suggestion that the auditory speech pathway receives and represents visual speech is examined in light of neuroimaging evidence on the auditory speech pathways. We outline the generally agreed-upon organization of the visual ventral and dorsal pathways and examine several types of visual processing that might be related to speech through those pathways, specifically, face and body, orthography, and sign language processing. In this context, we examine the visual speech processing literature, which reveals widespread diverse patterns of activity in posterior temporal cortices in response to visual speech stimuli. We outline a model of the visual and auditory speech pathways and make several suggestions: (1) The visual perception of speech relies on visual pathway representations of speech <italic>qua</italic> speech. (2) A proposed site of these representations, the temporal visual speech area (TVSA) has been demonstrated in posterior temporal cortex, ventral and posterior to multisensory posterior superior temporal sulcus (pSTS). (3) Given that visual speech has dynamic and configural features, its representations in feedforward visual pathways are expected to integrate these features, possibly in TVSA.</p></abstract><kwd-group><kwd>functional organization</kwd><kwd>audiovisual processing</kwd><kwd>speech perception</kwd><kwd>lipreading</kwd><kwd>visual processing</kwd></kwd-group><counts><fig-count count="1"/><table-count count="0"/><equation-count count="0"/><ref-count count="235"/><page-count count="18"/><word-count count="18479"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>This paper examines the questions, what levels of speech can be perceived visually, and how is visual speech represented by the brain? These questions would hardly have arisen 50 years ago. Mid-twentieth century speech perception theories were strongly influenced by the expectation that speech perception is an <italic>auditory</italic> function for processing <italic>acoustic</italic> speech stimuli (Klatt, <xref rid="B108" ref-type="bibr">1979</xref>; Stevens, <xref rid="B203" ref-type="bibr">1981</xref>), perhaps, in close coordination with the motor system (Liberman et al., <xref rid="B119" ref-type="bibr">1967</xref>; Liberman, <xref rid="B118" ref-type="bibr">1982</xref>). At the time, theorizing about speech perception was unrelated to evidence about visual speech perception (lipreading<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>), even though there were reports available in the literature showing that speech can be perceived visually. For example, there was extensive evidence during most of the twentieth century that lipreading can substitute for hearing in the education of deaf children (Jeffers and Barley, <xref rid="B94" ref-type="bibr">1971</xref>), and there was evidence about the important role of lipreading in combination with residual hearing for children and adults with hearing impairments (Erber, <xref rid="B58" ref-type="bibr">1971</xref>). The basic finding in normal-hearing adults that vision can compensate for hearing under noisy conditions was reported by mid-twentieth century (Sumby and Pollack, <xref rid="B208" ref-type="bibr">1954</xref>). Even the report by McGurk and MacDonald (<xref rid="B139" ref-type="bibr">1976</xref>) that a visual speech stimulus mismatched with an auditory stimulus can alter perception of an auditory speech stimulus, an effect that has come to be known as the McGurk effect, had few responses in the literature until a number of years following its publication.</p><p>Research efforts to explain the McGurk effect and understand its general implications for speech perception and multisensory processing began in the 1980s (e.g., Massaro and Cohen, <xref rid="B135" ref-type="bibr">1983</xref>; Liberman and Mattingly, <xref rid="B120" ref-type="bibr">1985</xref>; Campbell et al., <xref rid="B38" ref-type="bibr">1986</xref>; Green and Kuhl, <xref rid="B75" ref-type="bibr">1989</xref>), as did forays into theoretical explanations for how auditory and visual speech information combines perceptually (Liberman and Mattingly, <xref rid="B120" ref-type="bibr">1985</xref>; Massaro, <xref rid="B134" ref-type="bibr">1987</xref>; Summerfield, <xref rid="B209" ref-type="bibr">1987</xref>). In the following decade, in tandem with the development of new neuroimaging technologies, reports emerged that visual speech stimuli elicit auditory cortical responses (Sams et al., <xref rid="B183" ref-type="bibr">1991</xref>; Calvert et al., <xref rid="B33" ref-type="bibr">1997</xref>), results that seemed consistent with the phenomenal experience of the McGurk effect as a change in the auditory perception of speech. In the 1990s, breakthrough research on multisensory processing in cat superior colliculus was presented by Stein and Meredith (<xref rid="B200" ref-type="bibr">1993</xref>). Their evidence about multisensory neuronal integration provided a potential neural mechanism for explaining how auditory and visual speech information is processed (Calvert, <xref rid="B31" ref-type="bibr">2001</xref>), specifically, that auditory and visual speech information converges early in the stream of processing.</p><p>Evidence for multisensory inputs to classically defined unisensory cortical areas (e.g., Falchier et al., <xref rid="B59" ref-type="bibr">2002</xref>; Foxe et al., <xref rid="B69" ref-type="bibr">2002</xref>) helped to shift the view of the sensory pathways as modality-specific until the levels of association cortex (Mesulam, <xref rid="B142" ref-type="bibr">1998</xref>) toward the view that the brain is massively multisensory (Foxe and Schroeder, <xref rid="B68" ref-type="bibr">2005</xref>; Ghazanfar and Schroeder, <xref rid="B72" ref-type="bibr">2006</xref>). Findings suggesting the possibility that visual speech stimuli have special access to the early auditory speech processing pathway (Calvert et al., <xref rid="B33" ref-type="bibr">1997</xref>; Ludman et al., <xref rid="B127" ref-type="bibr">2000</xref>; Pekkola et al., <xref rid="B162" ref-type="bibr">2005</xref>) were consistent with the emerging multisensory view. More recently, reconsideration of the motor theory of speech perception (Liberman and Mattingly, <xref rid="B120" ref-type="bibr">1985</xref>) and mirror neuron system theory (Rizzolatti and Arbib, <xref rid="B176" ref-type="bibr">1998</xref>; Rizzolatti and Craighero, <xref rid="B177" ref-type="bibr">2004</xref>) have led inquiry into the role of somatomotor processing in speech perception, including visual speech perception (Hasson et al., <xref rid="B81" ref-type="bibr">2007</xref>; Skipper et al., <xref rid="B194" ref-type="bibr">2007a</xref>; Matchin et al., <xref rid="B137" ref-type="bibr">2014</xref>). In this context, a question has been the extent to which visual speech is represented in frontal cortex (Callan et al., <xref rid="B28" ref-type="bibr">2014</xref>). Thus, both the auditory and somatomotor systems have been studied for their roles in representing visual speech.</p><p>Curiously, the role of the visual system in representing speech has received less attention than the role of the auditory speech pathways. What is particularly curious is that the visual speech stimulus is psycholinguistically extremely rich, as shown below, yet there has been little research that has focused on how the visual system represents visible psycholinguistic structure (i.e., phonetic features, phonemes, syllables, prosody, and even words); although there have been, as we discuss below, multiple studies that show that speech activates areas in high-level visual pathways (for reviews, Campbell, <xref rid="B36" ref-type="bibr">2008</xref>, <xref rid="B37" ref-type="bibr">2011</xref>). The absence of pointed investigations of how visual speech is represented&#x02014;in contrast to the detailed knowledge about auditory speech representations&#x02014;is surprising, because sensory systems transduce specific types of energy such as light and sound, each affording its own form of evidence about the environment, including speech; and the current view of multisensory interactions does not overturn the classical hierarchical models of auditory and visual sensory pathways (e.g., Felleman and Van Essen, <xref rid="B61" ref-type="bibr">1991</xref>; Kaas and Hackett, <xref rid="B102" ref-type="bibr">2000</xref>; Rauschecker and Tian, <xref rid="B173" ref-type="bibr">2000</xref>) as much as it enriches them. Clearly, the diverse evidence for multisensory interactions needs to be reconciled with evidence pointing to modality-specific stimulus representations and processing (Hertz and Amedi, <xref rid="B84" ref-type="bibr">2014</xref>). This review explores the expectation that perception of visual speech stimuli requires visual representations of the stimuli through the visual pathways.</p><p>In this paper, we review the visual speech perception literature to support the view that every psycholinguistic level of speech organization is visible. That being the case, we consider the cortical representation of auditory speech as a possible model for the organization of visual speech processing. We suggest that research on the auditory organization of speech processing does not in fact encourage the notion that visual speech perception can be explained by multisensory connections alone. We propose a model that posits modality-specific as well as amodal speech processing pathways. Figure <xref ref-type="fig" rid="F1">1</xref> summarizes our model, which is discussed in detail further below.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Neuroanatomical working model of audiovisual speech perception in the left hemisphere based on models of dual visual (Wilson et al., <xref rid="B226" ref-type="bibr">1993</xref>; Haxby et al., <xref rid="B83" ref-type="bibr">1994</xref>; Ungerleider et al., <xref rid="B216" ref-type="bibr">1998</xref>; Weiner and Grill-Spector, <xref rid="B225" ref-type="bibr">2013</xref>) and auditory (Romanski et al., <xref rid="B178" ref-type="bibr">1999</xref>; Hickok and Poeppel, <xref rid="B86" ref-type="bibr">2007</xref>; Saur et al., <xref rid="B185" ref-type="bibr">2008</xref>; Rauschecker and Scott, <xref rid="B172" ref-type="bibr">2009</xref>; Liebenthal et al., <xref rid="B122" ref-type="bibr">2010</xref>) pathways and audiovisual integration (Beauchamp et al., <xref rid="B10" ref-type="bibr">2004</xref>) in humans</bold>. Audiovisual speech is processed in auditory (blue) and visual (pink) areas projecting to amodal (green) middle temporal cortex via auditory (light blue arrows) and visual (light red arrows) ventral pathways terminating in VLPFC, and to multimodal posterior temporal cortex via auditory (dark blue) and visual (dark red) dorsal pathways terminating in DLPFC. Specialization for phoneme processing is suggested to exist in both auditory and visual pathways, at the level of mSTG/S and TVSA, respectively, although the pattern of connectivity of TVSA (shown in red dotted arrows), and whether it is part of the ventral and/or dorsal visual streams is unknown. Multimodal or amodal areas in the ventral and dorsal streams connect bi-directionally via direct and indirect ventral (light green arrows) and dorsal (dark green arrows) pathways. (HG/STG, Heschl's gyrus/superior temporal gyrus; aSTG, anterior superior temporal gyrus; mSTG/S, middle superior temporal gyrus and sulcus; pSTG/S, posterior superior temporal gyrus and sulcus; MTG, middle temporal gyrus; OC, occipital cortex; FFA, fusiform face area; LOC, lateral occipital complex; MT, middle temporal area; TVSA, temporal visual speech area; SMG, supramarginal gyrus; SMC, somatomotor cortex; VLPFC, ventrolateral prefrontal cortex; DLPFC, dorsolateral prefrontal cortex).</p></caption><graphic xlink:href="fnins-08-00386-g0001"/></fig></sec><sec><title>Visual speech perception</title><sec><title>Implications of individual differences in lipreading ability</title><p>Any discussion of visual speech perception and its underlying neural mechanisms needs to acknowledge the fact of large inter-individual variation, both within and across normal-hearing and deaf populations (Bernstein et al., <xref rid="B17" ref-type="bibr">2000</xref>, <xref rid="B14" ref-type="bibr">2001</xref>; Auer and Bernstein, <xref rid="B5" ref-type="bibr">2007</xref>; Tye-Murray et al., <xref rid="B215" ref-type="bibr">2014</xref>). The differences are so large that findings on visual speech processing can probably not be accurately interpreted without knowing something about individual participants' lipreading ability and auditory experience.</p><p>For example, in a test of words correctly lipread in isolated sentences, the scores by deaf lipreaders ranged from zero to greater than 85% correct (Bernstein et al., <xref rid="B17" ref-type="bibr">2000</xref>). Deaf lipreaders were able to identify as many as 42% of isolated monosyllabic words from a list of highly confusable rhyming words (each test word rhymed with five other English words). Among adults with normal hearing, there was a narrower performance range for the same stimulus materials: There were individuals with scores as low as zero and ones with very good lipreading ability with scores as high as 75% correct words in sentences and 24% correct on the isolated rhyming words. Analyses of phoneme confusions in lipreading sentences suggested that the deaf participants were using more visual phonetic feature information than the hearing adults. But the individual variation in lipreading sentences accounted for by isolated word vs. isolated phoneme identification (using non-sense syllables) scores showed that isolated words accounted for more variance than phonemes: Word identification scores with isolated rhyme words accounted for between 66 and 71% of the variance in words-in-sentences scores for deaf lipreaders and between 44 and 64% of the variance for normal-hearing lipreaders, values commensurate with other reports (Conklin, <xref rid="B50" ref-type="bibr">1917</xref>; Utley, <xref rid="B219" ref-type="bibr">1946</xref>; Lyxell et al., <xref rid="B129" ref-type="bibr">1993</xref>). In Bernstein et al. (<xref rid="B17" ref-type="bibr">2000</xref>), phoneme identification in non-sense syllables accounted for between 21 and 43% of the variance in words-in-sentences scores for deaf lipreaders and between 6 and 18% of the variance for normal-hearing lipreaders. When regression was used to predict words-in-sentences scores, only participant group (deaf, normal-hearing) and isolated word scores were significant predictors (multiple <italic>R</italic> between 0.88 and 0.90). Additional studies confirm that the best lipreaders experienced profound congenital hearing loss, but that even among normal-hearing adults there are individuals with considerable lipreading expertise (Mohammed et al., <xref rid="B145" ref-type="bibr">2006</xref>; Auer and Bernstein, <xref rid="B5" ref-type="bibr">2007</xref>).</p><p>Individuals with hearing impairments may rely primarily on visual speech, even in the context of hearing aid and cochlear implant usage (Rouger et al., <xref rid="B181" ref-type="bibr">2007</xref>; Bernstein et al., <xref rid="B18" ref-type="bibr">2014</xref>; Bottari et al., <xref rid="B26" ref-type="bibr">2014</xref>; Song et al., <xref rid="B198" ref-type="bibr">2014</xref>). Lipreading ability in individuals with hearing loss, including those with congenital impairments is likely associated with a wide range of neuroplastic effects, including take-over of auditory processing areas by vision (Karns et al., <xref rid="B104" ref-type="bibr">2012</xref>; Bottari et al., <xref rid="B26" ref-type="bibr">2014</xref>) or somatosensation (Levanen et al., <xref rid="B116" ref-type="bibr">1998</xref>; Auer et al., <xref rid="B6" ref-type="bibr">2007</xref>; Karns et al., <xref rid="B104" ref-type="bibr">2012</xref>), and alterations of sub-cortical connections (Lyness et al., <xref rid="B128" ref-type="bibr">2014</xref>).</p></sec><sec><title>Visible levels of speech</title><p>From a psycholinguistic perspective, speech has a hierarchical structure comprising features, phonemes, syllables, words, phrases, and larger units such as utterances, sentences, and discourse. The questions here are which of these levels can be perceived visually, and whether any type of these speech patterns is represented in visual modality-specific areas. As with auditory speech perception, we expect that at a minimum visual speech perception extends to the physical properties of speech, that is, its <italic>phonetic</italic> feature properties, and that those properties express the vowels, consonants, and prosody of a language. The term <italic>phonemic</italic> refers to language-specific segmental (vowel and consonant) properties. Thus, for example, the term <italic>phonetic</italic> applies to speech features without necessarily specifying a particular language, and <italic>phonemic</italic> refers to segmental distinctions used by a particular language to distinguish among words (Catford, <xref rid="B42" ref-type="bibr">1977</xref>). Prosody comprises phonetic attributes that span words or phrases, such as lexical stress in English (e.g., the distinction between the verb in &#x0201c;to re<italic>cord</italic>&#x0201d; and the noun in &#x0201c;the <italic>re</italic>cord&#x0201d;), and intonation (e.g., pronunciation of the same phrase as an exclamation or a statement, &#x0201c;we won!/?&#x0201d;). Necessarily, physical acoustic phonetic speech signals are different than optical phonetic speech signals; and although they may convey the same linguistic content, they are expected to be represented initially by different peripheral, subcortical, and primary sensory areas that code different low-level basic sensory features (e.g., light intensities vs. sound intensities, spatio-temporal vs. temporal frequencies, etc.). As we suggest below, there is the possibility that modality-specific representations exist to the level of whole words. But we do not expect separate representations of the meanings of individual words or of whole visual multi-word utterances, although there may be highly frequent utterances that are represented as such.</p></sec><sec><title>Features, phonemes, and visemes</title><p>Speech production simultaneously produces the sounds and sights of speech, but the vocal tract shapes, glottal vibrations, and velar gestures that produce acoustic speech (Stevens, <xref rid="B204" ref-type="bibr">1998</xref>) are not all directly visible. Some of them are visible as correlated motions of the jaw and the cheeks (Yehia et al., <xref rid="B232" ref-type="bibr">1998</xref>; Jiang et al., <xref rid="B96" ref-type="bibr">2002</xref>, <xref rid="B97" ref-type="bibr">2007</xref>). An ongoing idea in the literature is that visual speech is too impoverished to convey much phonetic information (Kuhl and Meltzoff, <xref rid="B111" ref-type="bibr">1988</xref>). This idea is supported by examples of poor lipreading performance and by focusing on how acoustic signals are generated. For example, the voicing feature (i.e., the feature that distinguishes &#x0201c;b&#x0201d; from &#x0201c;p&#x0201d;) is typically expressed acoustically in pre-vocalic position in terms of glottal vibration characteristics such as onset time (Lisker et al., <xref rid="B125" ref-type="bibr">1977</xref>). But the glottis is not a visible structure, so a possible inference is that the voicing feature cannot be perceived visually. However, there are other phonetic attributes that contribute to voicing distinctions. For example, post-vocalic consonant voicing depends greatly on vowel duration (Raphael, <xref rid="B170" ref-type="bibr">1971</xref>), and vowel duration&#x02014;the duration of the open mouth gesture&#x02014;is visible. When visual consonant identification was compared across initial (C[=consonant]V[=vowel]), medial (VCV), and final (VC) position (Van Son et al., <xref rid="B220" ref-type="bibr">1994</xref>), identification of final consonants was 44% correct in contrast to 28% for consonants elsewhere. The point is that both optical and acoustic phonetic attributes instantiate speech features on the basis of diverse sensory information; so the visibility of speech features or phonemes cannot be inferred accurately from a simple one-to-one mapping between the visibility of speech production anatomy (e.g., lips, mouth, tongue, glottis) and speech features (e.g., voicing, place, manner, nasality).</p><p>At the same time, the reduction in visual vs. auditory speech information needs to be taken into account. The concept of the <italic>viseme</italic> was invented to describe and account for the somewhat stable patterns of lipreaders' phoneme confusions (Woodward and Barber, <xref rid="B230" ref-type="bibr">1960</xref>; Fisher, <xref rid="B64" ref-type="bibr">1968</xref>; Owens and Blazek, <xref rid="B159" ref-type="bibr">1985</xref>). Visemes are sets such as /p, b, m/ that are typically formed using some grouping principle such as hierarchical clustering of consonant confusions from phoneme identification paradigms (Walden et al., <xref rid="B223" ref-type="bibr">1977</xref>; Auer and Bernstein, <xref rid="B4" ref-type="bibr">1997</xref>; Iverson et al., <xref rid="B92" ref-type="bibr">1998</xref>). A typical rule is on the order of grouping together phonemes whose mutual confusions account for around 70% of responses. Massaro suggested that, &#x0201c;Because of the data-limited property of visible speech in comparison to audible speech, many phonemes are virtually indistinguishable by sight, even from a natural face, and so are expected to be easily confused&#x0201d; (p. 316); and that, &#x0201c;a difference between visemes is significant, informative, and categorical to the perceiver; a difference within a viseme class is not&#x0201d; (Massaro et al., <xref rid="B136" ref-type="bibr">2012</xref>, p. 316).</p><p>However, most research that has used the viseme concept has involved phoneme identification tasks, for which there is a need to account for identification errors. A difference within a viseme class could be significant and informative. It could also be categorical at the level of a feature. Indeed, when presented with pairs of spoken words that differed only in terms of phonemes from within putative viseme sets, participants (deaf and normal-hearing adults) were able to identify which of the spoken words corresponded to an orthographic target word (Bernstein, <xref rid="B12" ref-type="bibr">2012</xref>). That is, each word pair in the target identification paradigm was constructed so that in sequential order each of its phonemes was selected from within the same viseme. The visemes were defined along the standard lines of constructing viseme sets. An additional set of word pairs was constructed from within sets that comprised even higher levels of confusability than used to construct visemes (referred to as &#x0201c;phoneme equivalence classes&#x0201d;; Auer and Bernstein, <xref rid="B4" ref-type="bibr">1997</xref>). Normal-hearing lipreaders with above-average lipreading scored between 65 and 80% correct word identification with stimuli comprising the <italic>sub-visemic</italic> phoneme sets (i.e., the sets of very similar phonemes). Deaf participants scored between 80 and 100% correct on those word-pairs. This would not have been possible if the phonemes that comprise visemes were not significant or informative. Thus, while there is no doubt that visual speech stimuli afford reduced phonetic detail in support of phoneme categories, there is also evidence that perceivers are not limited to perceiving viseme categories.</p><p>Interestingly, not only are perceivers able to perceive speech stimuli based on fine visual phonetic distinctions, they are also able to make judgments of the reliability of their own perceptions, apparently in terms of perceived phoneme or feature stimulus-to-response discrepancies. In a study of sentence lipreading (Demorest and Bernstein, <xref rid="B52" ref-type="bibr">1997</xref>), deaf and normal-hearing adults were presented with isolated spoken sentences for open set identification of the words in the sentences. Participants were asked to type what they thought the talker had said and also to rate their confidence in their typed responses, and they received no feedback on their performance. Confidence ratings ranged from 0 = &#x0201c;no confidence&#x02014;I guessed&#x0201d; to 7 = &#x0201c;complete confidence&#x02014;I understood every word.&#x0201d; Scoring for how well sentences were lipread included a measure of the perceptual distance based on phoneme alignments between the stimulus and the response and was computed using a sequence comparison algorithm (Kruskal and Wish, <xref rid="B110" ref-type="bibr">1978</xref>; Bernstein et al., <xref rid="B16" ref-type="bibr">1994</xref>) that aligned stimulus and response phoneme sequences using visual perceptual phoneme dissimilarity weights. As an example, when the stimulus sentence was, &#x0201c;Why should I get up so early in the morning?&#x0201d; and the response was, &#x0201c;Watch what I'm doing in the morning,&#x0201d; casual inspection of the stimulus and response suggest that they have similar phoneme strings even when some of the words were incorrectly identified. The sequence comparator aligned the phonemes of these two sentences as follows (in Arpabet phonemic notation):</p><preformat>
Stimulus: wA SUd A gEt ^p so Rli In Dx morn|G
Response: wa C-- - wxt Am du |G- In Dx morn|G
</preformat><p>Perusal of the string alignment suggests that there were phoneme similarities even when whole words were incorrect. A visual distance score was computed for each stimulus-response pair based only on the distances between aligned <italic>incorrect</italic> phonemes (e.g., &#x0201c;S&#x0201d; vs. &#x0201c;C&#x0201d; in the example) normalized by stimulus length in phonemes. Correct phonemes did not contribute to distance scores. Correlations between stimulus-response distances and subjective confidence ratings showed that as stimulus-response distance (perceptual dissimilarity) increased, subjective confidence went down (reliable Pearson correlations of &#x02212;0.511 for normal-hearing and &#x02212;0.626 for deaf). These findings suggest that deaf and hearing adults have access to perceptual representations that preserve to some extent the phonetic information in the visual stimulus and thereby allow them to judge discrepancy between the stimulus and their own response. Thus, both this approach and the target identification approach described above reveal that sub-visemic speech information is significant and informative.</p><p>If lipreading relies on visual image processing, there should be direct relationships between the structure of the visual images and perception. A study (Jiang et al., <xref rid="B97" ref-type="bibr">2007</xref>) addressed the relationship between optical recordings and visual speech perception. Recordings were made of 3-dimensional movement of the face and simultaneous video while talkers produced many different CV syllables (i.e., all the initial English consonants, followed by one of three different vowels, and spoken by four different talkers). If visual stimuli drive visual speech perception, than there should be a second-order isomorphism (Shepard and Chipman, <xref rid="B193" ref-type="bibr">1970</xref>) between optical data and perception such that the dissimilarity of physical speech signals should map onto perceptual dissimilarity. The study showed that a linearly warped physical stimulus dissimilarity space was highly effective in accounting for the perceptual structure of phoneme identification for spoken CVs. Across talkers, the 3-dimensional face movement data accounted for between 46 and 66% of the variance in perceptual dissimilarities among CV stimuli.</p></sec><sec><title>Spoken words</title><p>Visual spoken word recognition has been studied in experiments that were designed to investigate the pattern of visual confusions among spoken words. These studies show that visual dissimilarities affect perception to the level of spoken word identification.</p><p>For example, Mattys et al. (<xref rid="B138" ref-type="bibr">2002</xref>) presented isolated mono- and disyllabic spoken word stimuli to normal-hearing and deaf lipreaders for open-set visual identification. The words were selected so that they varied in terms of the number of words in the lexicon with which each was potentially confusable based on visual phoneme confusability (Iverson et al., <xref rid="B92" ref-type="bibr">1998</xref>). The results showed that visual phoneme confusability predicted the relative accuracy levels for word identification by both participant groups, and phoneme errors tended to be from within groups of visually more confusable phonemes.</p><p>Auer (<xref rid="B3" ref-type="bibr">2002</xref>) visually presented isolated spoken monosyllabic words to deaf and normal-hearing lipreaders and modeled perception using auditory vs. visual phoneme confusion data. The visual confusions were better predictors of visual spoken word recognition than auditory confusions. Strand and Sommers (<xref rid="B207" ref-type="bibr">2011</xref>) followed up and tested monosyllabic words in visual-only and auditory-only (with noise background) conditions. They modeled lexical competition effects separately for visual vs. auditory phoneme similarity and showed that measures of similarity (i.e., lexical competition) that were based on one modality were not good predictors of word identification accuracy for the other modality.</p></sec><sec><title>Prosody</title><p>Prosody comprises stress and intonation (Risberg and Lubker, <xref rid="B175" ref-type="bibr">1978</xref>; Jesse and McQueen, <xref rid="B95" ref-type="bibr">2014</xref>). Several studies have investigated visual prosody perception in normal-hearing adults (Fisher, <xref rid="B65" ref-type="bibr">1969</xref>; Lansing and McConkie, <xref rid="B112" ref-type="bibr">1999</xref>; Scarborough et al., <xref rid="B186" ref-type="bibr">2007</xref>; Jesse and McQueen, <xref rid="B95" ref-type="bibr">2014</xref>). Results suggest that prosody is perceived visually.</p><p>For example, emphatic stress for specific words such as, &#x0201c;We OWE you a yoyo,&#x0201d; vs., &#x0201c;We owe YOU a yoyo,&#x0201d; was perceived quite accurately (70%, chance = 33.3%), while perception of whether those sentences were spoken as statements or questions was perceived somewhat less accurately (60%, chance = 50%) (Bernstein et al., <xref rid="B19" ref-type="bibr">1989</xref>; see also, Lansing and McConkie, <xref rid="B112" ref-type="bibr">1999</xref>). Lexical stress in bisyllabic words such as <italic>SUBject</italic> (the noun) and <italic>subJECT</italic> (the verb) can be visually discriminated (62%, chance = 50%), as can phrasal stress that distinguishes (in sentences with stress on one of the names in &#x0201c;So, [name1] gave/sang [name2] a song from/by [name3]&#x0201d;) (54% correct, chance = 25%) (Scarborough et al., <xref rid="B186" ref-type="bibr">2007</xref>). In the latter study, larger and faster face movements were associated with the perception of stress. For example, lower lip opening peak velocity and the size of lip opening were related to lexical stress perception.</p><p>Even whole head movement has been shown to be correlated with prosody (63% of variance accounted for between voice pitch and six components of head movement) (Munhall et al., <xref rid="B147" ref-type="bibr">2004</xref>), with head movement contributing to the accuracy of speech perception in noise. Visible head movement can be used by talkers for perceiving emphasis (Lansing and McConkie, <xref rid="B112" ref-type="bibr">1999</xref>).</p><p>Visual prosody perception has been studied in infants. Prosody is used in parsing connected speech and may thereby assist infants in acquiring their native language (Johnson et al., <xref rid="B101" ref-type="bibr">2014</xref>). Visible prosody is likely a contributor to infants' demonstrated sensitivity to language differences in visual speech stimuli (Weikum et al., <xref rid="B224" ref-type="bibr">2007</xref>).</p></sec><sec><title>Interim summary</title><p>In answer to our question, What levels of speech can be perceived visually? we conclude that all levels of speech patterns (from features to connected speech) that can be heard can also be visually perceived, at least by the more skilled of lipreaders. Visual phoneme categories have internal perceptual structure that is different from that of auditory phoneme categories. At least in the better lipreaders, there may be visual modality-specific syllable or word pattern representations. Research on visual prosody suggests that it can be perceived in multisyllabic words and in connected speech. Thus, the perceptual evidence is fully compatible with the possibility that the visual speech perception relies on extensive visual modality-specific neural representations.</p></sec></sec><sec><title>An auditory representation of visual speech?</title><p>The earliest human neuroimaging studies on lipreading revealed activity in the region of primary auditory cortex, leading to discussions about the role of the auditory pathway in processing visual speech, perhaps as early as the primary auditory cortex (Sams et al., <xref rid="B183" ref-type="bibr">1991</xref>; Calvert et al., <xref rid="B33" ref-type="bibr">1997</xref>). Interpretations of the observed activity pointed to a role for the auditory pathway akin to its role in processing auditory speech stimuli: For example, &#x0201c;results show that visual information from articulatory movements has an entry into the auditory cortex&#x0201d; (Sams et al., <xref rid="B183" ref-type="bibr">1991</xref>); &#x0201c;activation of primary auditory cortex during lipreading suggests that these visual cues may influence the perception of heard speech before speech sounds are categorized in auditory association cortex into distinct phonemes&#x0201d; (Calvert et al., <xref rid="B33" ref-type="bibr">1997</xref>); &#x0201c;Visual speech has access to auditory sensory memory&#x0201d; (M&#x000f6;tt&#x000f6;nen et al., <xref rid="B146" ref-type="bibr">2002</xref>); and &#x0201c;seen speech with normal time-varying characteristics appears to have preferential access to &#x02018;purely&#x02019; auditory processing regions specialized for language&#x0201d; (Calvert and Campbell, <xref rid="B34" ref-type="bibr">2003</xref>).</p><p>These statements were not accompanied by an explicit model or theory about how visual speech stimuli are represented by visual cortical areas upstream of auditory cortex. One reading of these statements is that rather than computing the patterns of visual speech <italic>qua</italic> speech within the visual system, there is a special route for visual speech to the auditory pathway where it is represented as though it were an auditory speech stimulus.</p><p>Alternatively, visual speech patterns are integrated somehow within the visual system and then projected to the primary auditory cortex where they are re-represented. However, the re-representation of information is considered to be a computationally untenable solution for the brain (von der Malsburg, <xref rid="B222" ref-type="bibr">1995</xref>).</p><p>Another possibility is that visual stimuli are analyzed by the visual system only to the level of features such as motion or edges that are not integrated specifically as speech, and those feature representations are projected to the auditory pathway. But then it would be necessary to explain at what point the unbound information specific to speech was recognized as speech and was prioritized for entry into the auditory pathway. This possibility clearly suggests a &#x0201c;chicken and egg&#x0201d; problem.</p><p>Whatever its implications, there have been various attempts to confirm with neuroimaging in the human that primary auditory cortex activation levels increase following visual speech stimuli, with mixed results (Ludman et al., <xref rid="B127" ref-type="bibr">2000</xref>; Bernstein et al., <xref rid="B13" ref-type="bibr">2002</xref>; Calvert and Campbell, <xref rid="B34" ref-type="bibr">2003</xref>; Besle et al., <xref rid="B22" ref-type="bibr">2004</xref>; Pekkola et al., <xref rid="B162" ref-type="bibr">2005</xref>; Okada et al., <xref rid="B157" ref-type="bibr">2013</xref>). However, were visual speech prioritized for entry to auditory cortex, we might expect to see its effects more consistently.</p><p>Even when obtained, higher activation levels measured in the region of primary auditory cortex are of course not unambiguous with regard to the underlying neural response. They could for example be due to auditory imagery (Hickok et al., <xref rid="B85" ref-type="bibr">2003</xref>). Or visual motion could drive the response (Okada et al., <xref rid="B157" ref-type="bibr">2013</xref>). The location of primary auditory cortex could be inaccurately identified, particularly with group averaging, as non-invasive methods are imprecise in delineating the auditory core vs. belt cortex (Desai et al., <xref rid="B53" ref-type="bibr">2005</xref>). Finally, a definite possibility is that activity measured with functional imaging in the region of the auditory cortex is attributable to feedback rather than visual stimulus pattern representation (Calvert et al., <xref rid="B35" ref-type="bibr">2000</xref>; Schroeder et al., <xref rid="B188" ref-type="bibr">2008</xref>).</p><p>There are relevant monkey data concerning the representation of input across modalities. Direct connections have been demonstrated from auditory core and parabelt to V1 in monkeys (Falchier et al., <xref rid="B59" ref-type="bibr">2002</xref>) and from V2 to caudal auditory cortex (Falchier et al., <xref rid="B60" ref-type="bibr">2010</xref>). These studies did not show connections from V1 to A1. The character of the connections is that of feedback through the dorsal visual pathway, commensurate with the function of representing extra-personal peripheral space and motion. &#x0201c;These results suggest a model in which putative unisensory visual and auditory cortices do not interact in a classical feedforward&#x02013;feedback relationship but rather by way of a feedback loop. A possible implication of this organization is that the dominant effects of these connections between early sensory areas are modulatory&#x0201d; (Falchier et al., <xref rid="B60" ref-type="bibr">2010</xref>). Importantly, monkey work has also shown that visual stimuli can modulate auditory responses in primary and secondary auditory fields <italic>independent of the visual stimulus categories</italic> (Kayser et al., <xref rid="B105" ref-type="bibr">2008</xref>), and similar findings have been generalized to modulation of auditory cortices by somatosensory stimuli (Lemus et al., <xref rid="B115" ref-type="bibr">2010</xref>). Thus, while there are functional connections, these connections between early sensory areas may serve primarily downstream modulatory functions and not upstream representation of perceptual detail needed for recognizing stimulus categories.</p><p>Overall, replication of primary auditory cortex activation by visual speech has not been completely successful, explanations invoking phonetic processing have been vague with regard to upstream visual input computations, and animal research has not been supportive of the possibility that visual speech perception is the result of representing the visual speech information through activation of auditory speech representations. The research on auditory speech processing, to which we now turn, also discourages notions about the representation of visual speech by the auditory pathway.</p></sec><sec><title>The auditory representation of speech</title><p>The research on auditory speech processing is fairly clear in establishing that phonetic and phonemic speech representations in superior temporal regions beyond auditory core are viewed as modal, that is, abstracted from low-level acoustic characteristics but preserving some of their attributes. These modality specific auditory representations are not predicted to also respond to visual speech stimulus phonetic features or phonemes. Thus, our neuroanatomical model in Figure <xref ref-type="fig" rid="F1">1</xref> posits distinct visual and auditory pathways to the level of pSTS.</p><p>Emerging work in the human suggests that neurons in the left superior temporal gyrus (STG) show selectivity to spectrotemporal acoustic cues that map to distinct phonetic features (e.g., manner of articulation) and not to distinct phonemes. Sensitivity to different phonetic features has been demonstrated in the middle and posterior STG using data-mining algorithms to identify patterns of activity in functional magnetic resonance imaging (fMRI) (Formisano et al., <xref rid="B66" ref-type="bibr">2008</xref>; Kilian-Hutten et al., <xref rid="B107" ref-type="bibr">2011</xref>; Humphries et al., <xref rid="B91" ref-type="bibr">2013</xref>) and in intracranial (Chang et al., <xref rid="B45" ref-type="bibr">2010</xref>; Steinschneider et al., <xref rid="B201" ref-type="bibr">2011</xref>; Chan et al., <xref rid="B44" ref-type="bibr">2014</xref>; Mesgarani et al., <xref rid="B141" ref-type="bibr">2014</xref>) responses. There is now also conclusive evidence that an area in the left middle and ventral portion of STG and adjacent superior temporal sulcus (mSTG/S) is specifically sensitive to highly-familiar, over-learned, speech categories, responding more strongly to native vowels and syllables relative to spectrotemporally matched non-speech sounds (Liebenthal et al., <xref rid="B121" ref-type="bibr">2005</xref>; Joanisse et al., <xref rid="B98" ref-type="bibr">2007</xref>; Obleser et al., <xref rid="B154" ref-type="bibr">2007</xref>; Leaver and Rauschecker, <xref rid="B113" ref-type="bibr">2010</xref>; Turkeltaub and Coslett, <xref rid="B213" ref-type="bibr">2010</xref>; DeWitt and Rauschecker, <xref rid="B55" ref-type="bibr">2012</xref>), or relative to non-native speech sounds (Jacquemot et al., <xref rid="B93" ref-type="bibr">2003</xref>; Golestani and Zatorre, <xref rid="B73" ref-type="bibr">2004</xref>). Importantly, there appears to be spatial segregation within the left STG, such that dorsal STG areas largely surrounding the auditory core demonstrate sensitivity to acoustic features relevant to phonetic perception (whether embedded within speech or non-speech sounds), and a comparatively small ventral STG area adjoining the upper bank of the middle superior temporal sulcus (mSTG/S) demonstrates specificity to phonemic processing (Humphries et al., <xref rid="B91" ref-type="bibr">2013</xref>). Thus, there is evidence for hierarchical organization of a ventral stream of processing in the left superior temporal cortex for the representation of phonemic information based on acoustic phonetic features.</p><p>These findings indicate at least two levels of processing for auditory phonemic information in the left lateral STG, generally consistent with the hierarchical processing of spectral and temporal sound structure during auditory object perception in belt and parabelt areas in the monkey (Rauschecker, <xref rid="B171" ref-type="bibr">1998</xref>; Kaas and Hackett, <xref rid="B102" ref-type="bibr">2000</xref>; Rauschecker and Tian, <xref rid="B173" ref-type="bibr">2000</xref>; Rauschecker and Scott, <xref rid="B172" ref-type="bibr">2009</xref>). In the monkey, selectivity for communication calls has been shown in the lateral belt (Rauschecker et al., <xref rid="B174" ref-type="bibr">1995</xref>) and especially in the anterolateral area feeding into the ventral stream (Tian et al., <xref rid="B211" ref-type="bibr">2001</xref>), already one synaptic level from the core, although it is possible that increased selectivity occurs along the ventral-stream hierarchy. In the human, it appears that selectivity for phoneme processing in the left mSTG/S is at least two synaptic levels downstream from the auditory core. An important implication of the foregoing findings for our discussion here is that neural representations of auditory speech features in the left STG are <italic>modal</italic> (and not a-modal or symbolic), as they preserve a form of the acoustic signal that is abstracted from low-level acoustic characteristics coded in hierarchically earlier auditory cortex. This intermediate level of sensory information representation (preserving the form of complex sensory features or patterns) is predicted by a computational model of categorical auditory speech perception (Harnad, <xref rid="B80" ref-type="bibr">1987</xref>). The findings are also consistent with models of speech perception based primarily on acoustic features (Stevens and Wickesberg, <xref rid="B202" ref-type="bibr">2002</xref>). An open question however, is how to correctly characterize neural representations in the phonemic left mSTG/S area. The anatomical proximity of this area to auditory cortex and strong specificity for speech perception over other language functions (Liebenthal et al., <xref rid="B123" ref-type="bibr">2014</xref>) may suggest retention of some acoustic form (though greatly abstracted) even at this higher level of the speech processing hierarchy. Activation in areas more anterior in the STG (relative to mSTG/S) has been associated with the processing of linguistic and paralinguistic features available in larger chunks of speech such as words and sentences, for example syntax, prosody, and voice (Belin et al., <xref rid="B11" ref-type="bibr">2000</xref>; Zatorre et al., <xref rid="B233" ref-type="bibr">2004</xref>; Humphries et al., <xref rid="B90" ref-type="bibr">2005</xref>, <xref rid="B89" ref-type="bibr">2006</xref>; Hoekert et al., <xref rid="B88" ref-type="bibr">2008</xref>; DeWitt and Rauschecker, <xref rid="B55" ref-type="bibr">2012</xref>), whereas activation in the more ventral middle temporal cortex is associated with speech comprehension (Binder, <xref rid="B23" ref-type="bibr">2000</xref>; Binder et al., <xref rid="B24" ref-type="bibr">2000</xref>; Scott et al., <xref rid="B190" ref-type="bibr">2000</xref>; Davis and Johnsrude, <xref rid="B51" ref-type="bibr">2003</xref>; Humphries et al., <xref rid="B90" ref-type="bibr">2005</xref>; DeWitt and Rauschecker, <xref rid="B55" ref-type="bibr">2012</xref>).</p><p>Other areas outside the left mSTG/S have also been implicated in the neural representation of auditory phonemic information, particularly during phonological processing (i.e., when phonemic perception involves phonological awareness and phonological working memory, for example during explicit phonemic category judgment). The areas implicated in phonological processing are primarily those associated with the auditory dorsal pathway, including the posterior superior temporal gyrus (pSTG), inferior parietal cortex and ventral aspect of the precentral gyrus (Wise et al., <xref rid="B229" ref-type="bibr">2001</xref>; Davis and Johnsrude, <xref rid="B51" ref-type="bibr">2003</xref>; Buchsbaum et al., <xref rid="B27" ref-type="bibr">2005</xref>; Hickok and Poeppel, <xref rid="B86" ref-type="bibr">2007</xref>; Rauschecker and Scott, <xref rid="B172" ref-type="bibr">2009</xref>; Liebenthal et al., <xref rid="B122" ref-type="bibr">2010</xref>, <xref rid="B124" ref-type="bibr">2013</xref>). Neurons in the supramarginal gyrus (SMG) (Caplan et al., <xref rid="B41" ref-type="bibr">1997</xref>; Celsis et al., <xref rid="B43" ref-type="bibr">1999</xref>; Jacquemot et al., <xref rid="B93" ref-type="bibr">2003</xref>; Guenther et al., <xref rid="B78" ref-type="bibr">2006</xref>; Raizada and Poldrack, <xref rid="B169" ref-type="bibr">2007</xref>; Desai et al., <xref rid="B54" ref-type="bibr">2008</xref>; Tourville et al., <xref rid="B212" ref-type="bibr">2008</xref>) and ventral precentral gyrus (Wilson and Iacoboni, <xref rid="B227" ref-type="bibr">2006</xref>; Meister et al., <xref rid="B140" ref-type="bibr">2007</xref>; Chang et al., <xref rid="B45" ref-type="bibr">2010</xref>; Osnes et al., <xref rid="B158" ref-type="bibr">2011</xref>; Chevillet et al., <xref rid="B46" ref-type="bibr">2013</xref>) may represent the somatosensory and motor properties of speech sounds, and these areas are thought to exert modulatory influences on phonemic processing. In the inferior frontal cortex (pars opercularis in particular), sensitivity to phoneme categories (Myers et al., <xref rid="B149" ref-type="bibr">2009</xref>; Lee et al., <xref rid="B114" ref-type="bibr">2012</xref>; Niziolek and Guenther, <xref rid="B153" ref-type="bibr">2013</xref>) may be related to the role of more anterior inferior frontal cortex areas (pars orbitalis, pars triangularis) in response selection during auditory and phoneme categorization tasks.</p><p>The evidence reviewed here is consistent with the idea that both ventral and dorsal auditory streams contribute to phonemic perception. Phonemic perception in the left ventral auditory stream is organized hierarchically from dorsal STG areas surrounding the auditory core and representing acoustic phonetic features to ventral mSTG/S areas representing phoneme categories. In the dorsal auditory pathway, phonemic perception is a result of the interaction of neurons in the left pSTG representing acoustic phonetic features of speech and neurons in inferior parietal and frontal regions representing somatosensory and motor properties of speech. With respect to visual speech, the strategic location of pSTG at the junction with inferior parietal and ventral motor cortex and the multifunctionality of this area (Liebenthal et al., <xref rid="B123" ref-type="bibr">2014</xref>) make it ideally suited to interact with visual speech areas and mediate the effects of visual speech input on auditory phonemic perception, an observation that has been extensively explored in the audiovisual speech processing literature, which we discuss below. However, visual speech may also exert its influence through interaction with frontal cortices, also discussed below.</p><sec><title>Interim summary</title><p>Research on auditory speech is producing a detailed understanding of the organization of auditory speech representations. Although far from complete, the present view is that auditory speech is processed hierarchically from basic acoustic feature representations, to phonetic features and phonemes, and then to higher-levels such as words. The evidence is strong that neural representations of auditory speech features in the left STG are modal (and not a-modal or symbolic), as they preserve an acoustic form of the signal that is abstracted from low-level acoustic characteristics coded in hierarchically earlier auditory cortex. This evidence has at least one very strong implication for visual speech perception: Visual speech is not expected to share representations with auditory speech at its early modal levels of representation.</p></sec></sec><sec><title>Multisensory speech processing research: its relevance to understanding visual speech representations</title><p>Evidence is abundant that the brain is remarkably multisensory (Foxe and Schroeder, <xref rid="B68" ref-type="bibr">2005</xref>; Schroeder and Foxe, <xref rid="B187" ref-type="bibr">2005</xref>; Ghazanfar and Schroeder, <xref rid="B72" ref-type="bibr">2006</xref>; Kayser et al., <xref rid="B106" ref-type="bibr">2012</xref>), in the sense that it affords diverse neural mechanisms for integration and/or interaction (Stein et al., <xref rid="B199" ref-type="bibr">2010</xref>) among different sensory inputs. Research on audiovisual speech processing has focused on discovering those mechanisms. But the approaches have mostly not been designed to answer questions about the organization of unisensory speech representations: It has focused on answering questions such as whether there are influences from visual speech in classically defined auditory cortical areas (e.g., Sams et al., <xref rid="B183" ref-type="bibr">1991</xref>; Calvert et al., <xref rid="B33" ref-type="bibr">1997</xref>, <xref rid="B32" ref-type="bibr">1999</xref>; Bernstein et al., <xref rid="B13" ref-type="bibr">2002</xref>; Pekkola et al., <xref rid="B162" ref-type="bibr">2005</xref>), whether relative information clarity in auditory vs. visual stimuli affects neural network activations (Nath and Beauchamp, <xref rid="B150" ref-type="bibr">2011</xref>; Stevenson et al., <xref rid="B205" ref-type="bibr">2012</xref>), and whether audiovisual integration demonstrates the principle of inverse effectiveness [(Stein and Meredith, <xref rid="B200" ref-type="bibr">1993</xref>) i.e., multisensory gain is inversely related to unisensory stimulus effectiveness] (e.g., Calvert, <xref rid="B31" ref-type="bibr">2001</xref>; Beauchamp, <xref rid="B9" ref-type="bibr">2005</xref>; Stevenson et al., <xref rid="B205" ref-type="bibr">2012</xref>). Studies of multisensensory speech interactions commonly depend on designs that use audiovisual, auditory-only, and visual-only speech stimuli without controls designed to test hypotheses about the detailed organization of unisensory processing. Unisensory stimuli are used in the research as controls and for defining multisensory sites. For example, a common control for visual-only speech is a still frame of the talker or a no-stimulus baseline (e.g., Sekiyama et al., <xref rid="B191" ref-type="bibr">2003</xref>; Stevenson and James, <xref rid="B206" ref-type="bibr">2009</xref>; Nath and Beauchamp, <xref rid="B150" ref-type="bibr">2011</xref>, <xref rid="B151" ref-type="bibr">2012</xref>; Barros-Loscertales et al., <xref rid="B7" ref-type="bibr">2013</xref>; Okada et al., <xref rid="B157" ref-type="bibr">2013</xref>).</p><p>Because of the interest in multisensory interactions, research has focused on putative integration sites such as the pSTS (Calvert et al., <xref rid="B35" ref-type="bibr">2000</xref>; Wright et al., <xref rid="B231" ref-type="bibr">2003</xref>; Callan et al., <xref rid="B30" ref-type="bibr">2004</xref>; Nath and Beauchamp, <xref rid="B151" ref-type="bibr">2012</xref>; Stevenson et al., <xref rid="B205" ref-type="bibr">2012</xref>), which is part of both the auditory and visual pathways (see Figure <xref ref-type="fig" rid="F1">1</xref>). The left pSTS is routinely activated during audiovisual phoneme perception (e.g., Calvert, <xref rid="B31" ref-type="bibr">2001</xref>; Sekiyama et al., <xref rid="B191" ref-type="bibr">2003</xref>; Miller and D'Esposito, <xref rid="B144" ref-type="bibr">2005</xref>; Stevenson and James, <xref rid="B206" ref-type="bibr">2009</xref>; Nath and Beauchamp, <xref rid="B150" ref-type="bibr">2011</xref>). However, high-resolution examination of pSTS demonstrates clusters of neurons in the dorsal and ventral bank of bilateral pSTS that respond to either auditory or visual input, with intervening clusters responding most strongly to audiovisual input (Beauchamp et al., <xref rid="B10" ref-type="bibr">2004</xref>). What speech pattern attributes may be coded by such multisensory vs. unisensory clusters has not to our knowledge been investigated. In monkey, the STS has been found to have stronger feedback, as well as feed forward, connections with auditory and visual association rather than core areas (Seltzer and Pandya, <xref rid="B192" ref-type="bibr">1994</xref>; Lewis and Van Essen, <xref rid="B117" ref-type="bibr">2000</xref>; Foxe et al., <xref rid="B69" ref-type="bibr">2002</xref>; Ghazanfar et al., <xref rid="B71" ref-type="bibr">2005</xref>; Smiley et al., <xref rid="B197" ref-type="bibr">2007</xref>).</p><sec><title>Interim summary</title><p>To this point, we have reviewed the evidence that demonstrates visual perception of every psycholinguistic level of speech stimuli. We have discussed the hypothesis that visual speech might be represented through the auditory speech pathway. But our review of the auditory speech pathways suggests that representations are considered to be modal to the level of phonetic and phonemic speech representations in superior temporal regions beyond auditory core. Our view of the audiovisual speech processing literature is that its focus on multisensory interactions has resulted in limited evidence about the organization of the unisensory speech pathways. However, the expectation from the study of pSTS is that visual speech representations are projected to pSTS, and the question then is what information is represented through the visual system.</p></sec></sec><sec><title>Organization of the bottom-up visual pathways and implications for speech representations</title><p>Since the 1980s, the visual system organization has been described in terms of a <italic>ventral</italic> stream associated with form and object perception, and a <italic>dorsal</italic> stream associated with movement, space perception, and visually guided actions (Ungerleider and Mishkin, <xref rid="B218" ref-type="bibr">1982</xref>; Goodale et al., <xref rid="B74" ref-type="bibr">1994</xref>; Ungerleider and Haxby, <xref rid="B217" ref-type="bibr">1994</xref>; Logothetis and Sheinberg, <xref rid="B126" ref-type="bibr">1996</xref>; Zeki, <xref rid="B234" ref-type="bibr">2005</xref>). Both streams effect hierarchical organization with each level of representations building on preceding ones, and higher levels are more invariant to surface characteristics of visual objects, such as orientation and size. But perception is not limited to higher level representations. That is, perceivers have access to multiple levels of the pathways (Hochstein and Ahissar, <xref rid="B87" ref-type="bibr">2002</xref>; Zeki, <xref rid="B234" ref-type="bibr">2005</xref>).</p><p>In its general outline, the visual ventral stream extends from V1 in the occipital lobe to V2, V3, and V4, and into ventral temporal cortex and frontal cortex. The dorsal stream extends from V1 into V2, V3, V5/MT, and dorsal temporal areas including STS, extending further to parietal and frontal areas. This organization has long been known to be not strictly hierarchical and to comprise cross-talk among areas (Felleman and Van Essen, <xref rid="B61" ref-type="bibr">1991</xref>; for a recent review, Perry and Fallah, <xref rid="B163" ref-type="bibr">2014</xref>). A recent proposal for a three-stream model (Weiner and Grill-Spector, <xref rid="B225" ref-type="bibr">2013</xref>) implicates communication between ventral and dorsal streams for language processing, to which we return below.</p><sec><title>Visual pathway organizations of faces, orthography, and sign language perception</title><p>The organization of visual speech pathways could possibly be in common with the organization of other types of input, including faces, orthography, and possibly sign language that share certain attributes with visual speech. Face processing obviously must to be considered in relationship to visual speech (Campbell et al., <xref rid="B38" ref-type="bibr">1986</xref>; Campbell, <xref rid="B37" ref-type="bibr">2011</xref>). Faces and visual speech are usually co-present, and faces are a rich source of many types of socially significant information (Allison et al., <xref rid="B1" ref-type="bibr">2000</xref>; Haxby et al., <xref rid="B82" ref-type="bibr">2002</xref>)&#x02014;such as person identity, emotion, affect, and gaze. The &#x0201c;core face processing network&#x0201d; is generally considered to include the right lateral portion of the fusiform gyrus (FG) referred to as the fusiform face area (FFA), the lateral surface of the inferior occipital gyrus referred to as the occipital face area (OFA), and an area of the pSTS (Kanwisher et al., <xref rid="B103" ref-type="bibr">1997</xref>; Fox et al., <xref rid="B67" ref-type="bibr">2009</xref>). There is ample evidence that face and body representations are distinct (Downing et al., <xref rid="B57" ref-type="bibr">2006</xref>; Weiner and Grill-Spector, <xref rid="B225" ref-type="bibr">2013</xref>), and that body and visual speech representations are distinct (Santi et al., <xref rid="B184" ref-type="bibr">2003</xref>). Face areas in cortex may be localized more reliably with moving than with still face stimuli (Fox et al., <xref rid="B67" ref-type="bibr">2009</xref>). In a comparison between static and dynamic non-speech face images, right FFA and OFA did not prefer dynamic images but right posterior and anterior STS did (Pitcher et al., <xref rid="B164" ref-type="bibr">2011</xref>). However, in a study with different frame rates and scrambled vs. ordered frames of non-speech facial motion stimuli, differential effects were observed in face processing areas (Schultz et al., <xref rid="B189" ref-type="bibr">2013</xref>): Bilaterally, STS was more responsive to dynamic and ordered frames, but FFA and OFA were not sensitive to the order of frames, only to the amount of image diversity in the scrambled frames.</p><p>Visual speech activations have also been recorded in the FG (Calvert and Campbell, <xref rid="B34" ref-type="bibr">2003</xref>; Capek et al., <xref rid="B40" ref-type="bibr">2008</xref>), leading to the suggestion that visual speech processing uses the FFA (Campbell, <xref rid="B37" ref-type="bibr">2011</xref>). However, as noted above, the moving face is likely to more effectively activate face representations in the FFA, and diverse static images activate FFA more effectively than a single image. An independent face localizer is needed to functionally define the FFA region of interest (ROI) (Kanwisher et al., <xref rid="B103" ref-type="bibr">1997</xref>), because it cannot be defined based on anatomy alone. But FFA localizers have not typically been used with visual speech. To determine whether FFA represents speech distinctions such as speech features or phonemes also requires methods that are sensitive to differences across speech features or phonemes within FFA ROIs. Below, we discuss results when an independent FFA localizer was used, and FFA was shown responsive to speech stimuli but less so than to non-speech face movements (Bernstein et al., <xref rid="B20" ref-type="bibr">2011</xref>).</p><p>Although orthography is visually different from visual speech, both stimulus types likely make contact with higher-level mechanisms of spoken language; and both may involve recognizing words through fairly automatized whole-word recognition and also phonological analyses. Dorsal and ventral pathways have been shown to represent orthographic stimuli (Pugh et al., <xref rid="B168" ref-type="bibr">2000</xref>; Jobard et al., <xref rid="B99" ref-type="bibr">2003</xref>; Borowsky et al., <xref rid="B25" ref-type="bibr">2006</xref>). With respect to language, as with the auditory ventral pathway, the visual ventral pathway organized from occipital through inferior temporal to frontal regions is characterized as having responsibility for relating orthographic forms to word meanings. The ventral stream could be viewed as representing specifically the forms of familiar words and exception words (e.g., letter strings with atypical spelling-to-sound correspondences, e.g., &#x0201c;pint&#x0201d;), and mapping them to word pronunciations.</p><p>We are not suggesting that lipreading is built on reading. If anything, the opposite would be more likely, given that speech is encountered earlier in development, and given that orthography is an evolutionarily recent form of visual input. But the dual stream organization observed in reading research could be related to the processing resources needed by lipreaders, inasmuch as a more skilled lipreader would be expected to have more automatized access to certain lexical items as well as need for phonological processing; and a less skilled lipreader might have greater reliance on dorsal stream processing to glean fragmentary phonetic or phonemic category information and construct possible lexical items in stimuli. Spoken words with few or no visually similar competitors (Auer and Bernstein, <xref rid="B4" ref-type="bibr">1997</xref>; Iverson et al., <xref rid="B92" ref-type="bibr">1998</xref>) might be particularly good candidates for skilled lipreading via whole-word representations. Likewise, the wide individual differences among lipreaders (Bernstein et al., <xref rid="B17" ref-type="bibr">2000</xref>; Auer and Bernstein, <xref rid="B5" ref-type="bibr">2007</xref>) could be the consequence of differential development of visual speech pathways.</p><p>Sign language perception is also visually distinct from visual speech but might have some commonality with lipreading. Classical language areas (inferior frontal and posterior temporal areas) within the left hemisphere were recruited by American Sign Language in deaf and hearing native signers (Bavelier et al., <xref rid="B8" ref-type="bibr">1998</xref>). However, lipreading, auditory speech perception, and reading are united by their basis in spoken language (MacSweeney et al., <xref rid="B132" ref-type="bibr">2008</xref>). In addition, deaf users of sign language likely have experienced extensive neuroplastic changes in cortical and sub-cortical organization (MacSweeney et al., <xref rid="B131" ref-type="bibr">2004</xref>; Fine et al., <xref rid="B63" ref-type="bibr">2005</xref>; Auer et al., <xref rid="B6" ref-type="bibr">2007</xref>; Kral and Eggermont, <xref rid="B109" ref-type="bibr">2007</xref>; Lyness et al., <xref rid="B128" ref-type="bibr">2014</xref>) such that there could be commonality in the visual pathway for representing the configurations and dynamics of visual speech and signs. Both types of stimuli are reliant on form and motion. But research on sign language processing emphasizes commonalities at higher psycholinguistic levels (MacSweeney et al., <xref rid="B133" ref-type="bibr">2002</xref>). However, consistent with reading, there is some evidence for dual-stream processing of sign language. Hearing native signers activated left inferior termporal gyrus (ITG) and STS more with British sign language than with Tic Tac, a manual system used by bookmakers at race tracks (MacSweeney et al., <xref rid="B131" ref-type="bibr">2004</xref>) in contrast with hearing non-signers. Hearing native signers more than non-native signers activated ITG and middle temporal gyrus (MTG) for word lists vs. a still baseline, supporting a general role for the ventral pathway in fluent word recognition regardless of the form of the stimuli (speech, sign, orthography).</p></sec><sec><title>Organization of visual speech processing</title><p>In our model of auditory and visual modality-specific processing (Figure <xref ref-type="fig" rid="F1">1</xref>), we assume the standard visual pathways labeled &#x0201c;dorsal&#x0201d; and &#x0201c;ventral,&#x0201d; because we expect that visual speech is subject to visual system organization. But the pathway labeled &#x0201c;dorsal&#x0201d; may actually correspond to the lateral pathway in Weiner and Grill-Spector (<xref rid="B225" ref-type="bibr">2013</xref>), which we discuss further below. The model is highly schematized, because in fact there are few results in the literature that speak directly to how the levels of speech that can be perceived by vision are neurally represented.</p><p>The literature on visual speech processing is fairly consistent in showing bilateral posterior activation in areas associated with ventral and dorsal visual pathways (Calvert et al., <xref rid="B33" ref-type="bibr">1997</xref>; Campbell et al., <xref rid="B39" ref-type="bibr">2001</xref>; Nishitani and Hari, <xref rid="B152" ref-type="bibr">2002</xref>; Skipper et al., <xref rid="B195" ref-type="bibr">2005</xref>; Bernstein et al., <xref rid="B15" ref-type="bibr">2008a</xref>, <xref rid="B20" ref-type="bibr">2011</xref>; Capek et al., <xref rid="B40" ref-type="bibr">2008</xref>; Murase et al., <xref rid="B148" ref-type="bibr">2008</xref>; Okada and Hickok, <xref rid="B156" ref-type="bibr">2009</xref>; Ponton et al., <xref rid="B165" ref-type="bibr">2009</xref>; Files et al., <xref rid="B62" ref-type="bibr">2013</xref>). When spoken digits were contrasted with gurning (Campbell et al., <xref rid="B39" ref-type="bibr">2001</xref>), bilateral FG, and right STG and MTG were more activated by speech; left IT areas were more active in the contrast between speech and a still face. When still images of speech gestures were contrasted against the baseline of a still face, bilateral FG, occipito-temporal junction, MTG, and left STS were activated (Calvert and Campbell, <xref rid="B34" ref-type="bibr">2003</xref>); and dynamic stimuli were more effective than still speech in those same areas, except the bilateral lingual gyri. In a study in which spoken words were contrasted with a still face image (Capek et al., <xref rid="B40" ref-type="bibr">2008</xref>), widespread bilateral activation was reported in ventral and lateral temporal areas. In a magnetoencephalography study (Nishitani and Hari, <xref rid="B152" ref-type="bibr">2002</xref>), still speech images evoked a progression of activation from occipital to lateral temporal cortex labeled as pSTS. In a study in which short sentences were contrasted with videos of gurning and also with static faces (Hall et al., <xref rid="B79" ref-type="bibr">2005</xref>), there was extensive bilateral but greater left-hemisphere activation in ventral and lateral middle temporal cortices. MTG activation extended to the pSTS. When lipreading syllables and gurning were contrasted (Okada and Hickok, <xref rid="B156" ref-type="bibr">2009</xref>), left posterior MTG/STS, and STG activation was obtained. When participants were imaged with positron emission tomography (PET) (Paulesu et al., <xref rid="B160" ref-type="bibr">2003</xref>) while watching a still face, a face saying words, and the backwards video of the same words (backwards and forwards speech contains segments that are not different, such as vowels and transitions into and out of consonants), activations were obtained bilaterally in STG, bilateral superior temporal cortex and V5/MT. Connected speech in a story was presented in a lipreading condition that did not require any attempt to understand the story (Skipper et al., <xref rid="B195" ref-type="bibr">2005</xref>), however significant activity was restricted to occipital gyri and right ITG. This result seems difficult to interpret in light of the possibility that participants were not paying attention to the speech information.</p><p>Several generalizations can be made about the above studies. A variety of stimuli was contrasted mostly against a fixed image or gurning. For the most part, visual speech stimuli reliably activated areas that can be identified within the classical ventral and dorsal visual streams. Activity was typically widespread. Activations were often bilateral although not in strictly homologous locations. Typically, results were reported as group averages and smoothed activations. Cortical surface renderings of individual activations on native anatomy were not presented. So the published results are not very helpful with regard to individual differences in anatomical location or extent of activation. Independent functional localizers for visual areas such as the FFA and V5/MT were not used, although activations generally consistent with their locations were discussed. As a group, these studies provide confirmation that the ventral and dorsal visual pathways can be activated by visual speech, but they were not designed to investigate in any detail how visual speech is represented through the pathways. To do so would have required using various controls for low-level features and higher-level objects such as faces, taking into account factors such as sensitivity to movement in FFA, using contrasts reflective of the organization of speech such as between different phonemes or speech levels, and taking into account individual variations in visual speech perception.</p><p>Bernstein et al. (<xref rid="B20" ref-type="bibr">2011</xref>) sought to begin to address several of the previous limitations in methodology that limit ability to determine the organization of visual speech representations in high-level vision. They used functional localizers, a variety of speech, non-speech, and moving control stimuli, and contrasted video vs. point-light images. Participants underwent independent localizer scans for the FFA, the lateral occipital complex (LOC) associated with image structure (Grill-Spector et al., <xref rid="B76" ref-type="bibr">2001</xref>), and the V5/MT motion processing areas. The experimental stimuli were nonsense syllables that were selected for their visual dissimilarity [&#x0201c;du,&#x0201d; &#x0201c;sha,&#x0201d; &#x0201c;zi,&#x0201d; &#x0201c;fa,&#x0201d; &#x0201c;ta,&#x0201d; &#x0201c;bi,&#x0201d; &#x0201c;wi,&#x0201d; &#x0201c;dhu&#x0201d; (i.e., the voiced &#x0201c;th&#x0201d;), &#x0201c;ku,&#x0201d; &#x0201c;li,&#x0201d; and &#x0201c;mu&#x0201d;]. In separate conditions, a variety of non-speech face gestures (&#x0201c;puff,&#x0201d; &#x0201c;kiss,&#x0201d; &#x0201c;raspberry,&#x0201d; &#x0201c;growl,&#x0201d; &#x0201c;yawn,&#x0201d; &#x0201c;smirk,&#x0201d; &#x0201c;fishface,&#x0201d; &#x0201c;chew,&#x0201d; &#x0201c;gurn,&#x0201d; &#x0201c;nose wiggle,&#x0201d; and &#x0201c;frown-to-smile&#x0201d;) was presented. A parallel set of stimuli and controls was created based on 3-dimensional optical recordings that were made simultaneously with the video recordings. The optical recordings were of the motion of retro-reflectors positioned at 17 locations with most positions around the mouth, jaw, and cheeks. The optical recordings were used to generate point-light videos (Johansson, <xref rid="B100" ref-type="bibr">1973</xref>). The point-light stimuli presented speech and non-speech motion patterns without other natural visual features such as the talker's eye gaze, shape of face components (mouth, etc.) and general appearance. Speech and non-speech stimuli were easy to discern in the point-light displays. The point-light stimulus patterns were hypothesized to represent the structure of the speech information in motion and to some extent also configuration in terms of the arrangement of the dots and shape from motion (Johansson, <xref rid="B100" ref-type="bibr">1973</xref>). Point-light speech stimuli enhance the intelligibility of acoustic speech in noise (Rosenblum et al., <xref rid="B179" ref-type="bibr">1996</xref>) and can interfere with audiovisual speech perception when they are incongruent (Rosenblum and Saldana, <xref rid="B180" ref-type="bibr">1996</xref>). Visual controls were created from the speech and non-speech stimuli by dividing the area of the mouth and jaw into 100 square tiles. The order of frames within each tile was scrambled across sequential temporal groups of three frames. Using this scheme, the stimulus energy/luminance of the original stimuli was maintained. The control stimuli had the appearance of a face with square patches of unrelated movement.</p><p>The results showed that <italic>non-speech</italic> face gestures significantly activated the FFA, LOC, and V5/MT ROIs more strongly than <italic>speech</italic> face-gestures, supporting the expectation that none of those visual areas are selective for speech patterns. Detailed analysis of the motion data from the optical image recordings suggested that the reduced activity to speech in FFA, LOC, and V5/MT ROIs was not due to different speed of motion across stimulus types. One surprise, given its ubiquity in the literature, was that the gurn stimulus had much higher motion speed than the speech or the other non-speech stimuli. However, removal of the results that were obtained when gurns were presented did not change the overall pattern of results in ROIs.</p><p>The main experimental results were used to search for areas selective for speech independent of media (that is across point-light and video stimuli). Because point-light stimuli present primarily motion information with very much reduced configural information and no face detail, activations in conjunctions were interpreted as areas most concerned with speech patterns. Although there were activations in the right temporal cortices, the left-hemisphere activations were viewed as candidates for visual speech representations in high-level vision areas feeding forward into left-lateralized language areas. Based on individual and group results, contiguous areas of posterior MTG and STS were shown to be selective for speech. The localized posterior temporal speech selective area was dubbed the temporal visual speech area (TVSA). Figure <xref ref-type="fig" rid="F1">1</xref> shows the approximate location of TVSA, with the caveat that precise locations varied with individual anatomy (see Supplementary Figure 7, Bernstein et al., <xref rid="B20" ref-type="bibr">2011</xref>, for individual ROIs). On an individual-participant basis, the speech activations in pSTS/pMTG were more anterior than adjacent cortex that preferred non-speech gestures. They demonstrated preliminary evidence for a positive correlation with individual lipreading scores. The finding of a visual speech area (i.e., TVSA) posterior and inferior to pSTS is consistent with the idea that TVSA is a modal area in high-level vision, possibly distinct from multisensory pSTS.</p><p>In order to examine sensitivity to phonemic speech dissimilarity in the putative TVSA, Files et al. (<xref rid="B62" ref-type="bibr">2013</xref>) used a visual mismatch negativity (vMMN) paradigm to present consonant-vowel stimuli. The vMMN is elicited by change in the regularity of a sequence of visual stimuli (Pazo-Alvarez et al., <xref rid="B161" ref-type="bibr">2003</xref>; Winkler and Czigler, <xref rid="B228" ref-type="bibr">2012</xref>). Visual speech stimuli were selected to be <italic>near</italic> (ambiguous yet phonemically discriminable) or <italic>far</italic> (clearly different phonemes) in physical and speech perceptual distance based on a quantitative model of visual speech dissimilarity (Jiang et al., <xref rid="B97" ref-type="bibr">2007</xref>). The hypothesis was tested that the left posterior temporal cortex (i.e., TVSA) has tuning for visual speech, but the right homologous cortex has tuning for discriminable speech stimuli regardless of whether they can be labeled reliably as different phonemes. Discrimination among speech stimuli that are phonemically ambiguous would be expected of cortical areas that process non-speech face movements that can vary continuously (Puce et al., <xref rid="B166" ref-type="bibr">2000</xref>, <xref rid="B167" ref-type="bibr">2003</xref>; Miki et al., <xref rid="B143" ref-type="bibr">2004</xref>; Thompson et al., <xref rid="B210" ref-type="bibr">2007</xref>; Bernstein et al., <xref rid="B20" ref-type="bibr">2011</xref>) such as with different extent of mouth opening or with different motion velocities. The prediction was that regardless of perceptual distance the right hemisphere would generate the vMMN across discriminable stimuli; but only <italic>far</italic> phonemic contrasts would generate the vMMN on the left. Larger, more discriminable phoneme differences would be expected to feed forward to the left-lateralized language cortex.</p><p>Several attempts had previously been made to obtain vMMNs for visual speech category differences (Sams et al., <xref rid="B183" ref-type="bibr">1991</xref>; Colin et al., <xref rid="B49" ref-type="bibr">2002</xref>, <xref rid="B48" ref-type="bibr">2004</xref>; Saint-Amour et al., <xref rid="B182" ref-type="bibr">2007</xref>; Ponton et al., <xref rid="B165" ref-type="bibr">2009</xref>; Winkler and Czigler, <xref rid="B228" ref-type="bibr">2012</xref>). In those studies, either the vMMN was not obtained, the mismatch response was at a very long latency suggesting that it was not related to input pattern processing <italic>per se</italic>, or the obtained vMMN could be attributed to non-speech visual stimulus attributes. In Files et al. (<xref rid="B62" ref-type="bibr">2013</xref>), the stimulus selection was designed to defend against mismatch responses due to stimulus differences other than phoneme membership (be it perceptually near or far). Two tokens were presented for each phoneme category so that the vMMN would not be attributable to individual stimulus token differences. Stimuli were shifted spatially from trial to trial to defend against low-level stimulus change such as slight head or eye position variation on the screen. Care was taken to identify the temporal points in each stimulus at which the moving speech images deviated from each other, and those points were used to measure the vMMN latencies.</p><p>Current density reconstructions (Fuchs et al., <xref rid="B70" ref-type="bibr">1999</xref>) and statistical analyses using clusters of posterior temporal electrodes showed reliable left-hemisphere responses to individual stimuli and vMMNs to <italic>far</italic> stimulus phonemic category change. On the right, vMMNs were obtained with both <italic>far</italic> and <italic>near</italic> changes. Responses were in the range of latencies observed with non-speech face gestures stimuli. Current density reconstructions demonstrated consistent patterns of posterior temporal responses in the region of pMTG to the visual speech stimuli (Figures 4&#x02013;6 in Files et al., <xref rid="B62" ref-type="bibr">2013</xref>), with the caveat that reconstructions are limited in their spatial resolution. The finding of hemispheric differences in the pattern of vMMN responses, with greater sensitivity to smaller difference on the right, was interpreted as evidence the left posterior temporal cortex (putative TVSA) processes phonemic patterns that feed forward into language processing areas, and that more analog processing is carried out on the right as would be required for perceiving non-categorical, non-speech face gestures.</p></sec><sec><title>Proposed model</title><p>Figure <xref ref-type="fig" rid="F1">1</xref> proposes a schematic model of the auditory and visual pathways and interactions between them. The primary prediction of the model is that modal representations of visual speech exist to the level of the TVSA, and that this area is posterior and ventral to the multisensory pSTS. We acknowledge that far too little experimental evidence currently exists to determine with any precision what the organization of visual speech representations is through the visual system.</p><p>Lipreading must rely on processing of both configural features and/or stimulus patterns, and dynamic stimulus features. Although the processing of configural features is typically associated with the ventral visual stream and that of dynamic features with the dorsal visual stream, both types of information may be represented along both ventral and dorsal streams to some extent. Form has long been known to be perceived from motion (Johansson, <xref rid="B100" ref-type="bibr">1973</xref>). Current research on interactions between dorsal and ventral stream processing in object and motion perception (for a review see Perry and Fallah, <xref rid="B163" ref-type="bibr">2014</xref>) supports the view that object segmentation and representation is assisted by motion features, and motion representations are affected by object form input. Perry and Fallah propose that these interactions may occur further downstream from the visual motion area (MT). The conjunction results in Bernstein et al. (<xref rid="B20" ref-type="bibr">2011</xref>) using point-light and video speech stimuli that localized TVSA in pMTG seems consistent with the suggestion that TVSA is responsive to both form and motion. Observations of speech activations in IT could be due to configural processing but likely are supported by motion processing, given cross-talk between ventral and dorsal streams.</p><p>It is an entirely open question whether the identified TVSA has an internal organization that could support processing in both the dorsal and ventral visual streams, for example, as an anterior area that is part of the ventral stream and a posterior area that is part of the dorsal stream, similar to the anterior-to-posterior differentiation in the left STG for auditory speech perception. It also remains an open question whether TVSA overlaps at least partially with other high-level visual areas, for example LOC in the ventral visual stream. We suggest that such questions can be answered only with careful mapping of the different functional areas within individuals and taking into account perceptual variability.</p><p>Recently, a three-stream model was proposed by Weiner and Grill-Spector (<xref rid="B225" ref-type="bibr">2013</xref>). In their model, the visual system is organized in terms of a dorsal vision-action stream, a ventral visual perception stream for recognition of forms such as objects and faces, and a lateral stream concerned with form, visual dynamics and language, among other functions. The lateral pathway comprises the lateral occipital sulcus, the middle occipital gyrus, the posterior inferior temporal sulcus, and the MTG extending into V5/MT. The lateral stream communicates with both the parietal cortex of the dorsal stream and the inferior temporal cortex of the ventral stream. This arrangement is compatible with what is known to date about visual speech processing. Weiner and Grill-Spector do not elaborate on the possible role of their proposed lateral stream, but research on visual speech processing could contribute to a better understanding of this proposed lateral pathway.</p></sec><sec><title>The role of frontal and parietal areas in visual speech perception</title><p>Our discussion of a neural model of visual speech perception has focused thus far on high-level vision areas. However, as for auditory speech perception, other motor and somatosensory areas in the frontal and parietal cortex have also been implicated in visual speech perception, particularly within the theoretical framework that posits a human frontal cortex mirror neuron system (Rizzolatti and Arbib, <xref rid="B176" ref-type="bibr">1998</xref>). This view is compatible with the longstanding motor theory of speech perception (Liberman and Mattingly, <xref rid="B120" ref-type="bibr">1985</xref>) and with the evidence for modulatory effects of the somatomotor system on auditory phonemic perception reviewed above (Wilson et al., <xref rid="B227a" ref-type="bibr">2004</xref>; Meister et al., <xref rid="B140" ref-type="bibr">2007</xref>; M&#x000f6;tt&#x000f6;nen and Watkins, <xref rid="B146a" ref-type="bibr">2009</xref>; Osnes et al., <xref rid="B158" ref-type="bibr">2011</xref>) in the context of a somatomotor role for both the auditory and visual dorsal streams (Rauschecker and Scott, <xref rid="B172" ref-type="bibr">2009</xref>).</p><p>Frontal cortex activation is commonly observed with audiovisual or visual speech perception (e.g., MacSweeney et al., <xref rid="B130" ref-type="bibr">2000</xref>; Bernstein et al., <xref rid="B13" ref-type="bibr">2002</xref>, <xref rid="B20" ref-type="bibr">2011</xref>; M&#x000f6;tt&#x000f6;nen et al., <xref rid="B146" ref-type="bibr">2002</xref>; Callan et al., <xref rid="B29" ref-type="bibr">2003</xref>; Calvert and Campbell, <xref rid="B34" ref-type="bibr">2003</xref>; Paulesu et al., <xref rid="B160" ref-type="bibr">2003</xref>; Sekiyama et al., <xref rid="B191" ref-type="bibr">2003</xref>; Miller and D'Esposito, <xref rid="B144" ref-type="bibr">2005</xref>; Ojanen et al., <xref rid="B155" ref-type="bibr">2005</xref>; Skipper et al., <xref rid="B195" ref-type="bibr">2005</xref>, <xref rid="B196" ref-type="bibr">2007b</xref>; Okada and Hickok, <xref rid="B156" ref-type="bibr">2009</xref>; Matchin et al., <xref rid="B137" ref-type="bibr">2014</xref>). Inferior frontal activations during overt categorization of speech stimuli have been attributed to a role of this area in cognitive control and domain-general category computation (Hasson et al., <xref rid="B81" ref-type="bibr">2007</xref>; Myers et al., <xref rid="B149" ref-type="bibr">2009</xref>). Somatomotor system engagement is often observed in the context of failure to integrate audiovisual stimuli. Because visual speech is typically less intelligible than acoustic speech, or is presented in the context of noisy acoustic speech, speech somatomotor activity observed during audiovisual speech perception could arise due to conflict resolution with degraded speech (Miller and D'Esposito, <xref rid="B144" ref-type="bibr">2005</xref>; Callan et al., <xref rid="B28" ref-type="bibr">2014</xref>) or due to response biases (Venezia et al., <xref rid="B221" ref-type="bibr">2012</xref>). However, unlike auditory and visual cortices, the frontal cortex does not appear to play a critical role in the perception of clear speech, that is, in the accurate representation of stimulus patterns.</p><p>A study (Hasson et al., <xref rid="B81" ref-type="bibr">2007</xref>) comparing rapid adaptation (Grill-Spector and Malach, <xref rid="B77" ref-type="bibr">2001</xref>) effects with veridical vs. perceptual speech stimulus repetition concluded that areas in inferior frontal gyrus (IFG) coded for perceptual rather than sensory physical stimulus properties. Thus, when a mismatched visual &#x0201c;ka&#x0201d; and auditory &#x0201c;pa&#x0201d; were preceded by an audiovisual &#x0201c;ta&#x0201d;&#x02014;the syllable typically heard with the mismatched stimuli&#x02014;adaptation in IFG was similar to that with a veridical audiovisual &#x0201c;ta.&#x0201d; Thus, the observed adaptation effects followed perceived category change and not sensory stimulus change.</p><p>Callan et al. (<xref rid="B28" ref-type="bibr">2014</xref>) presented CVC English words under audiovisual conditions with three levels of noise, auditory-only conditions with three levels of noise, visual-only speech, and a still face baseline. The task was forced-choice identification of the vowel. Visual-only and audiovisual stimuli activated left IFG and ventral premotor cortex. Visual-only activation was greater than audiovisual in a dorsal part of the premotor cortex, implying some modal effects even in frontal cortex. However, there was not an examination of categorization effects within the dorsal premotor cortex, so it is not at all clear what the modality-specific response is attributable to.</p><p>The SMG has also been a focus in research on audiovisual speech integration (Hasson et al., <xref rid="B81" ref-type="bibr">2007</xref>; Bernstein et al., <xref rid="B15" ref-type="bibr">2008a</xref>,<xref rid="B21" ref-type="bibr">b</xref>; Arnal et al., <xref rid="B2" ref-type="bibr">2009</xref>; Dick et al., <xref rid="B56" ref-type="bibr">2010</xref>). Activation in this area has been observed with visual-only speech (Chu et al., <xref rid="B47" ref-type="bibr">2013</xref>) and with auditory speech (Caplan et al., <xref rid="B41" ref-type="bibr">1997</xref>; Celsis et al., <xref rid="B43" ref-type="bibr">1999</xref>; Jacquemot et al., <xref rid="B93" ref-type="bibr">2003</xref>; Guenther et al., <xref rid="B78" ref-type="bibr">2006</xref>; Raizada and Poldrack, <xref rid="B169" ref-type="bibr">2007</xref>; Desai et al., <xref rid="B54" ref-type="bibr">2008</xref>; Tourville et al., <xref rid="B212" ref-type="bibr">2008</xref>; Liebenthal et al., <xref rid="B124" ref-type="bibr">2013</xref>). Left SMG is sensitive to individual differences in processing incongruity of visual speech (Hasson et al., <xref rid="B81" ref-type="bibr">2007</xref>). It is sensitive to the degree of stimulus incongruity measured independently across auditory and visual speech, which suggests also that some modal aspect of representation extends to the SMG (Bernstein et al., <xref rid="B21" ref-type="bibr">2008b</xref>).</p><p>Overall, common activation in parietal and frontal areas in response to auditory and visual speech is expected (see Figure <xref ref-type="fig" rid="F1">1</xref>), in light of the evidence that such areas participate in higher-level (amodal) aspects of language processing.</p></sec></sec><sec><title>Summary and conclusions</title><p>Our inquiry into the visual speech perception literature shows that all levels of speech patterns that can be heard can also be seen, with the proviso that perception is subject to large individual differences. The perceptual evidence is highly valuable, because it leads to a strong rationale for undertaking research to discover how the brain represents visual speech.</p><p>We discussed the implication from neuroimaging results that visual speech has special status in possibly being represented not by the visual system but by the auditory system. Our review of the literature, including the organization of the auditory pathways leads us to doubt the validity of that suggestion. Modal representations of auditory speech exist beyond the auditory core areas that have been observed to respond to visual speech. We are in accord with the view that those activations are related to feedback, modulatory effects (Calvert et al., <xref rid="B32" ref-type="bibr">1999</xref>) and not to the representation of visual speech patterns <italic>per se</italic>.</p><p>Neuroimaging literature on lipreading shows widespread and diverse activity in the classical ventral and dorsal visual pathways in response to visual speech. However, the literature has for the most part not addressed in sufficient detail the organization and specificity of visual pathways for visual speech perception. A main drawback has been the use of baseline stimuli such as a still face or gurns to contrast with visual speech. Our recent fMRI and EEG studies with more in-depth focus on visual speech attributes provide evidence for a left posterior temporal area, TVSA, in high-level vision, possibly the recipient of both ventral and dorsal stream input, and sensitive to phonetic and phonemic speech attributes.</p><p>While there is not at the moment sufficient evidence for making detailed neuroanatomical predictions regarding the organization of the visual cortex for visual speech processing, we make the following empirically testable predictions: (1) The visual perception of speech relies on visual pathway representations of speech <italic>qua</italic> speech. That is, visual speech perception relies on stimulus patterns represented through visual pathways. (2) A proposed site of these, the TVSA, has been demonstrated in posterior temporal cortex, ventral and posterior to multisensory posterior superior temporal sulcus (pSTS). TVSA may feed modal information to downstream multisensory integration sites in pSTS. (3) Given that visual speech has dynamic and configural features that together are important for visual speech perception, neural representation of visual speech in feed forward visual pathways are expected to integrate to some extent across these features, possibly at the level of TVSA. Thus, a rigid division of the visual system into a dorsal and a ventral stream likely is not an adequate description for visual speech. Rather, the expectation is that there is cross-talk between areas in these paths for the processing of visual speech. (4) Visual speech information is expected to be fed forward from the occipital cortex to both the inferior parietal cortex along a dorsal visual pathway, and to the middle temporal cortex along a ventral visual pathway. Given the implication of the occipital-parietal (dorsal) visual stream in visual control of motor actions and spatial short-term memory (amongst other functions), we expect that the neural representations of visual speech in high-level areas of this stream may maintain more of the veridical, dynamic, and sequential information of the visual input, similar to neural representations of speech in the dorsal auditory stream (Wise et al., <xref rid="B229" ref-type="bibr">2001</xref>; Buchsbaum et al., <xref rid="B27" ref-type="bibr">2005</xref>; Hickok and Poeppel, <xref rid="B86" ref-type="bibr">2007</xref>; Rauschecker and Scott, <xref rid="B172" ref-type="bibr">2009</xref>; Liebenthal et al., <xref rid="B122" ref-type="bibr">2010</xref>). Given the implication of the occipito-temporal (ventral) visual stream in visual object recognition and long-term memory, we expect that neural representations in high-level areas of this stream may be highly abstracted from the visual input, similar to the neural representations of speech phonemes in the ventral auditory pathway (Liebenthal et al., <xref rid="B121" ref-type="bibr">2005</xref>; Joanisse et al., <xref rid="B98" ref-type="bibr">2007</xref>; Obleser et al., <xref rid="B154" ref-type="bibr">2007</xref>; Leaver and Rauschecker, <xref rid="B113" ref-type="bibr">2010</xref>; Turkeltaub and Coslett, <xref rid="B213" ref-type="bibr">2010</xref>; DeWitt and Rauschecker, <xref rid="B55" ref-type="bibr">2012</xref>).</p><p>We make the following suggestions for future research: (1) Given individual differences in perception and functional location of TVSA, detailed examination is needed within individuals to understand the organization of visual speech representations; (2) To understand fully how neural processes underlying visual and auditory speech perception interact, examination is needed, again within individuals, of the organization of both visual and auditory pathways for speech perception. (3) The ability to visually perceive all the psycholinguistic levels of speech calls for research both within and across psycholinguistic levels (i.e., phonetic features, phonemes, syllables, words, and prosody) of organization. In principle, the organization of visual speech processing cannot be determined based only on unspecific contrasts such as speech stimuli vs. still face images.</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>We thank the reviewers and editor for their insightful comments. This paper was supported in part by grants from the US National Institutes of Health/National Institute on Deafness and Other Communication Disorders grants DC008583, DC008308 (Bernstein PI) and DC006287 (Liebenthal, PI).</p></ack><fn-group><fn id="fn0001"><p><sup>1</sup>The term <italic>lipreading</italic> is used in this paper to refer to perceiving speech by vision. An alternate term that appears in the literature is <italic>speechreading</italic>. This term is sometimes used to emphasize the point that visual speech perception is more than perception of lips, and sometimes it is used to refer to visual speech perception augmented by residual hearing in individuals with hearing impairments.</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>T.</given-names></name><name><surname>Puce</surname><given-names>A.</given-names></name><name><surname>McCarthy</surname><given-names>G.</given-names></name></person-group> (<year>2000</year>). <article-title>The neurobiology of social cognition</article-title>. <source>Trends Cogn. Sci. (Regul. Ed)</source>. <volume>4</volume>, <fpage>267</fpage>&#x02013;<lpage>279</lpage>
<pub-id pub-id-type="doi">10.1016/S1364-6613(00)01501-1</pub-id><pub-id pub-id-type="pmid">10859571</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>L. H.</given-names></name><name><surname>Morillon</surname><given-names>B.</given-names></name><name><surname>Kell</surname><given-names>C. A.</given-names></name><name><surname>Giraud</surname><given-names>A. L.</given-names></name></person-group> (<year>2009</year>). <article-title>Dual neural routing of visual facilitation in speech processing</article-title>. <source>J. Neurosci</source>. <volume>29</volume>, <fpage>13445</fpage>&#x02013;<lpage>13453</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3194-09.2009</pub-id><pub-id pub-id-type="pmid">19864557</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name></person-group> (<year>2002</year>). <article-title>The influence of the lexicon on speech read word recognition: contrasting segmental and lexical distinctiveness</article-title>. <source>Psychon. Bull. Rev</source>. <volume>9</volume>, <fpage>341</fpage>&#x02013;<lpage>347</lpage>. <pub-id pub-id-type="doi">10.3758/BF03196291</pub-id><pub-id pub-id-type="pmid">12120798</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>1997</year>). <article-title>Speechreading and the structure of the lexicon: computationally modeling the effects of reduced phonetic distinctiveness on lexical uniqueness</article-title>. <source>J. Acous. Soc. Am</source>. <volume>102</volume>, <fpage>3704</fpage>&#x02013;<lpage>3710</lpage>. <pub-id pub-id-type="doi">10.1121/1.420402</pub-id><pub-id pub-id-type="pmid">9407662</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>2007</year>). <article-title>Enhanced visual speech perception in individuals with early-onset hearing impairment</article-title>. <source>J. Speech Lang. Hear. Res</source>. <volume>50</volume>, <fpage>1157</fpage>&#x02013;<lpage>1165</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2007/080)</pub-id><pub-id pub-id-type="pmid">17905902</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Sungkarat</surname><given-names>W.</given-names></name><name><surname>Singh</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>Vibrotactile activation of the auditory cortices in deaf versus hearing adults</article-title>. <source>Neuroreport</source>
<volume>18</volume>, <fpage>645</fpage>&#x02013;<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1097/WNR.0b013e3280d943b9</pub-id><pub-id pub-id-type="pmid">17426591</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barros-Loscertales</surname><given-names>A.</given-names></name><name><surname>Ventura-Campos</surname><given-names>N.</given-names></name><name><surname>Visser</surname><given-names>M.</given-names></name><name><surname>Alsius</surname><given-names>A.</given-names></name><name><surname>Pallier</surname><given-names>C.</given-names></name><name><surname>Avila Rivera</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>Neural correlates of audiovisual speech processing in a second language</article-title>. <source>Brain Lang</source>. <volume>126</volume>, <fpage>253</fpage>&#x02013;<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2013.05.009</pub-id><pub-id pub-id-type="pmid">23872285</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bavelier</surname><given-names>D.</given-names></name><name><surname>Corina</surname><given-names>D.</given-names></name><name><surname>Jezzard</surname><given-names>P.</given-names></name><name><surname>Clark</surname><given-names>V.</given-names></name><name><surname>Karni</surname><given-names>A.</given-names></name><name><surname>Lalwani</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>1998</year>). <article-title>Hemispheric specialization for English and ASL: left invariance-right variability</article-title>. <source>Neuroreport</source>
<volume>9</volume>, <fpage>1537</fpage>&#x02013;<lpage>1542</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-199805110-00054</pub-id><pub-id pub-id-type="pmid">9631463</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>M. S.</given-names></name></person-group> (<year>2005</year>). <article-title>Statistical criteria in FMRI studies of multisensory integration</article-title>. <source>Neuroinformatics</source>
<volume>3</volume>, <fpage>93</fpage>&#x02013;<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1385/NI:3:2:093</pub-id><pub-id pub-id-type="pmid">15988040</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>M. S.</given-names></name><name><surname>Argall</surname><given-names>B. D.</given-names></name><name><surname>Bodurka</surname><given-names>J.</given-names></name><name><surname>Duyn</surname><given-names>J. H.</given-names></name><name><surname>Martin</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Unraveling multisensory integration: patchy organization within human STS multisensory cortex</article-title>. <source>Nat. Neurosci</source>. <volume>7</volume>, <fpage>1190</fpage>&#x02013;<lpage>1192</lpage>. <pub-id pub-id-type="doi">10.1038/nn1333</pub-id><pub-id pub-id-type="pmid">15475952</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P.</given-names></name><name><surname>Zatorre</surname><given-names>R. J.</given-names></name><name><surname>Lafaille</surname><given-names>P.</given-names></name><name><surname>Ahad</surname><given-names>P.</given-names></name><name><surname>Pike</surname><given-names>B.</given-names></name></person-group> (<year>2000</year>). <article-title>Voice-selective areas in human auditory cortex</article-title>. <source>Nature</source>
<volume>403</volume>, <fpage>309</fpage>&#x02013;<lpage>312</lpage>. <pub-id pub-id-type="doi">10.1038/35002078</pub-id><pub-id pub-id-type="pmid">10659849</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>2012</year>). <article-title>Visual speech perception</article-title>, in <source>AudioVisual Speech Processing</source>, eds <person-group person-group-type="editor"><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name><name><surname>Bailly</surname><given-names>G.</given-names></name><name><surname>Perrier</surname><given-names>P.</given-names></name></person-group> (<publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University</publisher-name>), <fpage>21</fpage>&#x02013;<lpage>39</lpage>.</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Moore</surname><given-names>J. K.</given-names></name><name><surname>Ponton</surname><given-names>C. W.</given-names></name><name><surname>Don</surname><given-names>M.</given-names></name><name><surname>Singh</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Visual speech perception without primary auditory cortex activation</article-title>. <source>Neuroreport</source>
<volume>13</volume>, <fpage>311</fpage>&#x02013;<lpage>315</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200203040-00013</pub-id><pub-id pub-id-type="pmid">11930129</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Tucker</surname><given-names>P. E.</given-names></name></person-group> (<year>2001</year>). <article-title>Enhanced speechreading in deaf adults: can short-term training/practice close the gap for hearing adults?</article-title>
<source>J. Speech Lang. Hear. Res</source>. <volume>44</volume>, <fpage>5</fpage>&#x02013;<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2001/001)</pub-id><pub-id pub-id-type="pmid">11218108</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Wagner</surname><given-names>M.</given-names></name><name><surname>Ponton</surname><given-names>C. W.</given-names></name></person-group> (<year>2008a</year>). <article-title>Spatiotemporal dynamics of audiovisual speech processing</article-title>. <source>Neuroimage</source>
<volume>39</volume>, <fpage>423</fpage>&#x02013;<lpage>435</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.08.035</pub-id><pub-id pub-id-type="pmid">17920933</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Demorest</surname><given-names>M. E.</given-names></name><name><surname>Eberhardt</surname><given-names>S. P.</given-names></name></person-group> (<year>1994</year>). <article-title>A computational approach to analyzing sentential speech perception: phoneme-to-phoneme stimulus-response alignment</article-title>. <source>J. Acous. Soc. Am</source>. <volume>95</volume>, <fpage>3617</fpage>&#x02013;<lpage>3622</lpage>. <pub-id pub-id-type="doi">10.1121/1.409930</pub-id><pub-id pub-id-type="pmid">8046151</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Demorest</surname><given-names>M. E.</given-names></name><name><surname>Tucker</surname><given-names>P. E.</given-names></name></person-group> (<year>2000</year>). <article-title>Speech perception without hearing</article-title>. <source>Percept. Psychophys</source>. <volume>62</volume>, <fpage>233</fpage>&#x02013;<lpage>252</lpage>. <pub-id pub-id-type="doi">10.3758/BF03205546</pub-id><pub-id pub-id-type="pmid">10723205</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Eberhardt</surname><given-names>S. P.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name></person-group> (<year>2014</year>). <article-title>Audiovisual spoken word training can promote or impede auditory-only perceptual learning: results from prelingually deafened adults with late-acquired cochlear implants versus normal-hearing adults</article-title>. <source>Front. Psychol</source>. <volume>5</volume>:<issue>934</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00934</pub-id><pub-id pub-id-type="pmid">25206344</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Eberhardt</surname><given-names>S. P.</given-names></name><name><surname>Demorest</surname><given-names>M. E.</given-names></name></person-group> (<year>1989</year>). <article-title>Single-channel vibrotactile supplements to visual perception of intonation and stress</article-title>. <source>J. Acous. Soc. Am</source>. <volume>85</volume>, <fpage>397</fpage>&#x02013;<lpage>405</lpage>. <pub-id pub-id-type="doi">10.1121/1.397690</pub-id><pub-id pub-id-type="pmid">2522107</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Jiang</surname><given-names>J.</given-names></name><name><surname>Pantazis</surname><given-names>D.</given-names></name><name><surname>Lu</surname><given-names>Z. L.</given-names></name><name><surname>Joshi</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Visual phonetic processing localized using speech and nonspeech face gestures in video and point-light displays</article-title>. <source>Hum. Brain Mapp</source>. <volume>32</volume>, <fpage>1660</fpage>&#x02013;<lpage>1676</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.21139</pub-id><pub-id pub-id-type="pmid">20853377</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Lu</surname><given-names>Z. L.</given-names></name><name><surname>Jiang</surname><given-names>J.</given-names></name></person-group> (<year>2008b</year>). <article-title>Quantified acoustic-optical speech signal incongruity identifies cortical sites of audiovisual speech processing</article-title>. <source>Brain Res</source>. <volume>1242</volume>, <fpage>172</fpage>&#x02013;<lpage>184</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2008.04.018</pub-id><pub-id pub-id-type="pmid">18495091</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besle</surname><given-names>J.</given-names></name><name><surname>Fort</surname><given-names>A.</given-names></name><name><surname>Delpuech</surname><given-names>C.</given-names></name><name><surname>Giard</surname><given-names>M.-H.</given-names></name></person-group> (<year>2004</year>). <article-title>Bimodal speech: early suppressive visual effects in human auditory cortex</article-title>. <source>Eur. J. Neurosci</source>. <volume>20</volume>, <fpage>2225</fpage>&#x02013;<lpage>2234</lpage>. <pub-id pub-id-type="doi">10.1111/j.1460-9568.2004.03670.x</pub-id><pub-id pub-id-type="pmid">15450102</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>J. R.</given-names></name></person-group> (<year>2000</year>). <article-title>The new neuroanatomy of speech perception</article-title>. <source>Brain</source>
<volume>123(Pt 12)</volume>, <fpage>2371</fpage>&#x02013;<lpage>2372</lpage>. <pub-id pub-id-type="doi">10.1093/brain/123.12.2371</pub-id><pub-id pub-id-type="pmid">11099441</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>J. R.</given-names></name><name><surname>Frost</surname><given-names>J. A.</given-names></name><name><surname>Hammeke</surname><given-names>T. A.</given-names></name><name><surname>Bellgowan</surname><given-names>P. S.</given-names></name><name><surname>Springer</surname><given-names>J. A.</given-names></name><name><surname>Kaufman</surname><given-names>J. N.</given-names></name><etal/></person-group>. (<year>2000</year>). <article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title>. <source>Cereb. Cortex</source>
<volume>10</volume>, <fpage>512</fpage>&#x02013;<lpage>528</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/10.5.512</pub-id><pub-id pub-id-type="pmid">10847601</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borowsky</surname><given-names>R.</given-names></name><name><surname>Cummine</surname><given-names>J.</given-names></name><name><surname>Owen</surname><given-names>W. J.</given-names></name><name><surname>Friesen</surname><given-names>C. K.</given-names></name><name><surname>Shih</surname><given-names>F.</given-names></name><name><surname>Sarty</surname><given-names>G. E.</given-names></name></person-group> (<year>2006</year>). <article-title>FMRI of ventral and dorsal processing streams in basic reading processes: insular sensitivity to phonology</article-title>. <source>Brain Topogr</source>. <volume>18</volume>, <fpage>233</fpage>&#x02013;<lpage>239</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-006-0001-2</pub-id><pub-id pub-id-type="pmid">16845597</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bottari</surname><given-names>D.</given-names></name><name><surname>Heimler</surname><given-names>B.</given-names></name><name><surname>Caclin</surname><given-names>A.</given-names></name><name><surname>Dalmolin</surname><given-names>A.</given-names></name><name><surname>Giard</surname><given-names>M. H.</given-names></name><name><surname>Pavani</surname><given-names>F.</given-names></name></person-group> (<year>2014</year>). <article-title>Visual change detection recruits auditory cortices in early deafness</article-title>. <source>Neuroimage</source>
<volume>94</volume>, <fpage>172</fpage>&#x02013;<lpage>184</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.02.031</pub-id><pub-id pub-id-type="pmid">24636881</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchsbaum</surname><given-names>B. R.</given-names></name><name><surname>Olsen</surname><given-names>R. K.</given-names></name><name><surname>Koch</surname><given-names>P.</given-names></name><name><surname>Berman</surname><given-names>K. F.</given-names></name></person-group> (<year>2005</year>). <article-title>Human dorsal and ventral auditory streams subserve rehearsal-based and echoic processes during verbal working memory</article-title>. <source>Neuron</source>
<volume>48</volume>, <fpage>687</fpage>&#x02013;<lpage>697</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2005.09.029</pub-id><pub-id pub-id-type="pmid">16301183</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>D. E.</given-names></name><name><surname>Jones</surname><given-names>J. A.</given-names></name><name><surname>Callan</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Multisensory and modality specific processing of visual speech in different regions of the premotor cortex</article-title>. <source>Front. Psychol</source>. <volume>5</volume>:<issue>389</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00389</pub-id><pub-id pub-id-type="pmid">24860526</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>D. E.</given-names></name><name><surname>Jones</surname><given-names>J. A.</given-names></name><name><surname>Munhall</surname><given-names>K.</given-names></name><name><surname>Callan</surname><given-names>A. M.</given-names></name><name><surname>Kroos</surname><given-names>C.</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>). <article-title>Neural processes underlying perceptual enhancement by visual speech gestures</article-title>. <source>Neuroreport</source>
<volume>14</volume>, <fpage>2213</fpage>&#x02013;<lpage>2218</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200312020-00016</pub-id><pub-id pub-id-type="pmid">14625450</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>D. E.</given-names></name><name><surname>Jones</surname><given-names>J. A.</given-names></name><name><surname>Munhall</surname><given-names>K.</given-names></name><name><surname>Kroos</surname><given-names>C.</given-names></name><name><surname>Callan</surname><given-names>A. M.</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name></person-group> (<year>2004</year>). <article-title>Multisensory integration sites identified by perception of spatial wavelet filtered visual speech gesture information</article-title>. <source>J. Cogn. Neurosci</source>. <volume>16</volume>, <fpage>805</fpage>&#x02013;<lpage>816</lpage>. <pub-id pub-id-type="doi">10.1162/089892904970771</pub-id><pub-id pub-id-type="pmid">15200708</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>G. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Crossmodal processing in the human brain: insights from functional neuroimaging studies</article-title>. <source>Cereb. Cortex</source>
<volume>11</volume>, <fpage>1110</fpage>&#x02013;<lpage>1123</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/11.12.1110</pub-id><pub-id pub-id-type="pmid">11709482</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>G. A.</given-names></name><name><surname>Brammer</surname><given-names>M. J.</given-names></name><name><surname>Bullmore</surname><given-names>E. T.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Iversen</surname><given-names>S. D.</given-names></name><name><surname>David</surname><given-names>A. S.</given-names></name></person-group> (<year>1999</year>). <article-title>Response amplification in sensory-specific cortices during crossmodal binding</article-title>. <source>Neuroreport</source>
<volume>10</volume>, <fpage>2619</fpage>&#x02013;<lpage>2623</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-199908200-00033</pub-id><pub-id pub-id-type="pmid">10574380</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>G. A.</given-names></name><name><surname>Bullmore</surname><given-names>E. T.</given-names></name><name><surname>Brammer</surname><given-names>M. J.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Williams</surname><given-names>S. C.</given-names></name><name><surname>McGuire</surname><given-names>P. K.</given-names></name><etal/></person-group>. (<year>1997</year>). <article-title>Activation of auditory cortex during silent lipreading</article-title>. <source>Science</source>
<volume>276</volume>, <fpage>593</fpage>&#x02013;<lpage>596</lpage>. <pub-id pub-id-type="doi">10.1126/science.276.5312.593</pub-id><pub-id pub-id-type="pmid">9110978</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>G. A.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name></person-group> (<year>2003</year>). <article-title>Reading speech from still and moving faces: the neural substrates of visible speech</article-title>. <source>J. Cogn. Neurosci</source>. <volume>15</volume>, <fpage>57</fpage>&#x02013;<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1162/089892903321107828</pub-id><pub-id pub-id-type="pmid">12590843</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>G. A.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Brammer</surname><given-names>M. J.</given-names></name></person-group> (<year>2000</year>). <article-title>Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex</article-title>. <source>Curr. Biol</source>. <volume>10</volume>, <fpage>649</fpage>&#x02013;<lpage>657</lpage>. <pub-id pub-id-type="doi">10.1016/S0960-9822(00)00513-3</pub-id><pub-id pub-id-type="pmid">10837246</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>The processing of audio-visual speech: empirical and neural bases</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source>. <volume>363</volume>, <fpage>1001</fpage>&#x02013;<lpage>1010</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2007.2155</pub-id><pub-id pub-id-type="pmid">17827105</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Speechreading and the Bruce-Young model of face recognition: early findings and recent developments</article-title>. <source>Br. J. Psychol</source>. <volume>102</volume>, <fpage>704</fpage>&#x02013;<lpage>710</lpage>. <pub-id pub-id-type="doi">10.1111/j.2044-8295.2011.02021.x</pub-id><pub-id pub-id-type="pmid">21988379</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Landis</surname><given-names>T.</given-names></name><name><surname>Regard</surname><given-names>M.</given-names></name></person-group> (<year>1986</year>). <article-title>Face recognition and lipreading. A neurological dissociation</article-title>. <source>Brain</source>
<volume>109(Pt 3)</volume>, <fpage>509</fpage>&#x02013;<lpage>521</lpage>. <pub-id pub-id-type="doi">10.1093/brain/109.3.509</pub-id><pub-id pub-id-type="pmid">3719288</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>MacSweeney</surname><given-names>M.</given-names></name><name><surname>Surguladze</surname><given-names>S.</given-names></name><name><surname>Calvert</surname><given-names>G.</given-names></name><name><surname>McGuire</surname><given-names>P.</given-names></name><name><surname>Suckling</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2001</year>). <article-title>Cortical substrates for the perception of face actions: an fMRI study of the specificity of activation for seen speech and for meaningless lower-face acts (gurning)</article-title>. <source>Cogn. Brain Res</source>. <volume>12</volume>, <fpage>233</fpage>&#x02013;<lpage>243</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(01)00054-4</pub-id><pub-id pub-id-type="pmid">11587893</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Capek</surname><given-names>C. M.</given-names></name><name><surname>Macsweeney</surname><given-names>M.</given-names></name><name><surname>Woll</surname><given-names>B.</given-names></name><name><surname>Waters</surname><given-names>D.</given-names></name><name><surname>McGuire</surname><given-names>P. K.</given-names></name><name><surname>David</surname><given-names>A. S.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Cortical circuits for silent speechreading in deaf and hearing people</article-title>. <source>Neuropsychologia</source>
<volume>46</volume>, <fpage>1233</fpage>&#x02013;<lpage>1241</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.11.026</pub-id><pub-id pub-id-type="pmid">18249420</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caplan</surname><given-names>D.</given-names></name><name><surname>Waters</surname><given-names>G. S.</given-names></name><name><surname>Hildebrandt</surname><given-names>N.</given-names></name></person-group> (<year>1997</year>). <article-title>Determinants of sentence comprehension in aphasic patients in sentence-picture matching tasks</article-title>. <source>J. Speech Lang. Hear. Res</source>. <volume>40</volume>, <fpage>542</fpage>&#x02013;<lpage>555</lpage>. <pub-id pub-id-type="doi">10.1044/jslhr.4003.542</pub-id><pub-id pub-id-type="pmid">9210113</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Catford</surname><given-names>J. C.</given-names></name></person-group> (<year>1977</year>). <source>Fundamental Problems in Phonetics</source>. <publisher-loc>Bloomington, IN</publisher-loc>: <publisher-name>Indiana University</publisher-name>.</mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celsis</surname><given-names>P.</given-names></name><name><surname>Boulanouar</surname><given-names>K.</given-names></name><name><surname>Doyon</surname><given-names>B.</given-names></name><name><surname>Ranjeva</surname><given-names>J. P.</given-names></name><name><surname>Berry</surname><given-names>I.</given-names></name><name><surname>Nespoulous</surname><given-names>J. L.</given-names></name><etal/></person-group>. (<year>1999</year>). <article-title>Differential fMRI responses in the left posterior superior temporal gyrus and left supramarginal gyrus to habituation and change detection in syllables and tones</article-title>. <source>Neuroimage</source>
<volume>9</volume>, <fpage>135</fpage>&#x02013;<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.1998.0389</pub-id><pub-id pub-id-type="pmid">9918735</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>A. M.</given-names></name><name><surname>Dykstra</surname><given-names>A. R.</given-names></name><name><surname>Jayaram</surname><given-names>V.</given-names></name><name><surname>Leonard</surname><given-names>M. K.</given-names></name><name><surname>Travis</surname><given-names>K. E.</given-names></name><name><surname>Gygi</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Speech-specific tuning of neurons in human superior temporal gyrus</article-title>. <source>Cereb Cortex</source>. <volume>24</volume>, <fpage>2679</fpage>&#x02013;<lpage>2693</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bht127</pub-id><pub-id pub-id-type="pmid">23680841</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>E. F.</given-names></name><name><surname>Rieger</surname><given-names>J. W.</given-names></name><name><surname>Johnson</surname><given-names>K.</given-names></name><name><surname>Berger</surname><given-names>M. S.</given-names></name><name><surname>Barbaro</surname><given-names>N. M.</given-names></name><name><surname>Knight</surname><given-names>R. T.</given-names></name></person-group> (<year>2010</year>). <article-title>Categorical speech representation in human superior temporal gyrus</article-title>. <source>Nat. Neurosci</source>. <volume>13</volume>, <fpage>1428</fpage>&#x02013;<lpage>1432</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2641</pub-id><pub-id pub-id-type="pmid">20890293</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chevillet</surname><given-names>M. A.</given-names></name><name><surname>Jiang</surname><given-names>X.</given-names></name><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name><name><surname>Riesenhuber</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>Automatic phoneme category selectivity in the dorsal auditory stream</article-title>. <source>J. Neurosci</source>. <volume>33</volume>, <fpage>5208</fpage>&#x02013;<lpage>5215</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1870-12.2013</pub-id><pub-id pub-id-type="pmid">23516286</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>Y.-H.</given-names></name><name><surname>Lin</surname><given-names>F.-H.</given-names></name><name><surname>Chou</surname><given-names>Y.-J.</given-names></name><name><surname>Tsai</surname><given-names>K. W.-K.</given-names></name><name><surname>Kuo</surname><given-names>W.-J.</given-names></name><name><surname>Jaaskelainen</surname><given-names>L. P.</given-names></name></person-group> (<year>2013</year>). <article-title>Effective cerebral connectivity during silent speech reading revealed by functional magnetic resonance imaging</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e80265</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0080265</pub-id><pub-id pub-id-type="pmid">24278266</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colin</surname><given-names>C.</given-names></name><name><surname>Radeau</surname><given-names>M.</given-names></name><name><surname>Soquet</surname><given-names>A.</given-names></name><name><surname>Deltenre</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>). <article-title>Generalization of the generation of an MMN by illusory McGurk percepts: voiceless consonants</article-title>. <source>Clin. Neurophysiol</source>. <volume>115</volume>, <fpage>1989</fpage>&#x02013;<lpage>2000</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2004.03.027</pub-id><pub-id pub-id-type="pmid">15294201</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colin</surname><given-names>C.</given-names></name><name><surname>Radeau</surname><given-names>M.</given-names></name><name><surname>Soquet</surname><given-names>A.</given-names></name><name><surname>Demolin</surname><given-names>D.</given-names></name><name><surname>Colin</surname><given-names>F.</given-names></name><name><surname>Deltenre</surname><given-names>P.</given-names></name></person-group> (<year>2002</year>). <article-title>Mismatch negativity evoked by the McGurk-MacDonald effect: a phonetic representation within short-term memory</article-title>. <source>Clin. Neurophysiol</source>. <volume>113</volume>, <fpage>495</fpage>&#x02013;<lpage>506</lpage>. <pub-id pub-id-type="doi">10.1016/S1388-2457(02)00024-X</pub-id><pub-id pub-id-type="pmid">11955994</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conklin</surname><given-names>E. S.</given-names></name></person-group> (<year>1917</year>). <article-title>A method for the determination of relative skill in lip-reading</article-title>. <source>Volta Rev</source>. <volume>19</volume>, <fpage>216</fpage>&#x02013;<lpage>219</lpage>.</mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>M. H.</given-names></name><name><surname>Johnsrude</surname><given-names>I. S.</given-names></name></person-group> (<year>2003</year>). <article-title>Hierarchical processing in spoken language comprehension</article-title>. <source>J. Neurosci</source>. <volume>23</volume>, <fpage>3423</fpage>&#x02013;<lpage>3431</lpage>. <pub-id pub-id-type="pmid">12716950</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demorest</surname><given-names>M. E.</given-names></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>1997</year>). <article-title>Relationships between subjective ratings and objective measures of performance in speechreading sentences</article-title>. <source>J. Speech Lang. Hear. Res</source>. <volume>40</volume>, <fpage>900</fpage>&#x02013;<lpage>911</lpage>. <pub-id pub-id-type="doi">10.1044/jslhr.4004.900</pub-id><pub-id pub-id-type="pmid">9263953</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desai</surname><given-names>R.</given-names></name><name><surname>Liebenthal</surname><given-names>E.</given-names></name><name><surname>Possing</surname><given-names>E. T.</given-names></name><name><surname>Waldron</surname><given-names>E.</given-names></name><name><surname>Binder</surname><given-names>J. R.</given-names></name></person-group> (<year>2005</year>). <article-title>Volumetric vs. surface-based alignment for localization of auditory cortex activation</article-title>. <source>Neuroimage</source>
<volume>26</volume>, <fpage>1019</fpage>&#x02013;<lpage>1029</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.03.024</pub-id><pub-id pub-id-type="pmid">15893476</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desai</surname><given-names>R.</given-names></name><name><surname>Liebenthal</surname><given-names>E.</given-names></name><name><surname>Waldron</surname><given-names>E.</given-names></name><name><surname>Binder</surname><given-names>J. R.</given-names></name></person-group> (<year>2008</year>). <article-title>Left posterior temporal regions are sensitive to auditory categorization</article-title>. <source>J. Cogn. Neurosci</source>. <volume>20</volume>, <fpage>1174</fpage>&#x02013;<lpage>1188</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2008.20081</pub-id><pub-id pub-id-type="pmid">18284339</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeWitt</surname><given-names>I.</given-names></name><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name></person-group> (<year>2012</year>). <article-title>Phoneme and word recognition in the auditory ventral stream</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>109</volume>, <fpage>E505</fpage>&#x02013;<lpage>E514</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1113427109</pub-id><pub-id pub-id-type="pmid">22308358</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dick</surname><given-names>A. S.</given-names></name><name><surname>Solodkin</surname><given-names>A.</given-names></name><name><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2010</year>). <article-title>Neural development of networks for audiovisual speech comprehension</article-title>. <source>Brain Lang</source>. <volume>114</volume>, <fpage>101</fpage>&#x02013;<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2009.08.005</pub-id><pub-id pub-id-type="pmid">19781755</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>P. E.</given-names></name><name><surname>Chan</surname><given-names>A. W.</given-names></name><name><surname>Peelen</surname><given-names>M. V.</given-names></name><name><surname>Dodds</surname><given-names>C. M.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2006</year>). <article-title>Domain specificity in visual cortex</article-title>. <source>Cereb. Cortex</source>
<volume>16</volume>, <fpage>1453</fpage>&#x02013;<lpage>1461</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhj086</pub-id><pub-id pub-id-type="pmid">16339084</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erber</surname><given-names>N. P.</given-names></name></person-group> (<year>1971</year>). <article-title>Auditory and audiovisual reception of words in low-frequency noise by children with normal hearing and by children with impaired hearing</article-title>. <source>J. Speech Hear. Res</source>. <volume>14</volume>, <fpage>496</fpage>&#x02013;<lpage>512</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.1403.496</pub-id><pub-id pub-id-type="pmid">5163883</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falchier</surname><given-names>A.</given-names></name><name><surname>Clavagnier</surname><given-names>S.</given-names></name><name><surname>Barone</surname><given-names>P.</given-names></name><name><surname>Kennedy</surname><given-names>H.</given-names></name></person-group> (<year>2002</year>). <article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title>. <source>J. Neurosci</source>. <volume>22</volume>, <fpage>5749</fpage>&#x02013;<lpage>5759</lpage>. <pub-id pub-id-type="pmid">12097528</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falchier</surname><given-names>A.</given-names></name><name><surname>Schroeder</surname><given-names>C. E.</given-names></name><name><surname>Hackett</surname><given-names>T. A.</given-names></name><name><surname>Lakatos</surname><given-names>P.</given-names></name><name><surname>Nascimento-Silva</surname><given-names>S.</given-names></name><name><surname>Ulbert</surname><given-names>I.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Projection from visual areas V2 and prostriata to caudal auditory cortex in the monkey</article-title>. <source>Cereb. Cortex</source>
<volume>20</volume>, <fpage>1529</fpage>&#x02013;<lpage>1538</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhp213</pub-id><pub-id pub-id-type="pmid">19875677</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>D. J.</given-names></name><name><surname>Van Essen</surname><given-names>D. C.</given-names></name></person-group> (<year>1991</year>). <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cereb. Cortex</source>
<volume>1</volume>, <fpage>1</fpage>&#x02013;<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Files</surname><given-names>B. T.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>2013</year>). <article-title>The visual mismatch negativity elicited with visual speech stimuli</article-title>. <source>Front. Hum. Neurosci</source>. <volume>7</volume>:<issue>371</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00371</pub-id><pub-id pub-id-type="pmid">23882205</pub-id></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fine</surname><given-names>I.</given-names></name><name><surname>Finney</surname><given-names>E. M.</given-names></name><name><surname>Boynton</surname><given-names>G. M.</given-names></name><name><surname>Dobkins</surname><given-names>K. R.</given-names></name></person-group> (<year>2005</year>). <article-title>Comparing the effects of auditory deprivation and sign language within the auditory and visual cortex</article-title>. <source>J. Cogn. Neurosci</source>. <volume>17</volume>, <fpage>1621</fpage>&#x02013;<lpage>1637</lpage>. <pub-id pub-id-type="doi">10.1162/089892905774597173</pub-id><pub-id pub-id-type="pmid">16269101</pub-id></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>C. G.</given-names></name></person-group> (<year>1968</year>). <article-title>Confusions among visually perceived consonants</article-title>. <source>J. Speech Hear. Res</source>. <volume>11</volume>, <fpage>796</fpage>&#x02013;<lpage>804</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.1104.796</pub-id><pub-id pub-id-type="pmid">5719234</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>C. G.</given-names></name></person-group> (<year>1969</year>). <article-title>The visibility of terminal pitch contour</article-title>. <source>J. Speech Hear. Res</source>. <volume>12</volume>, <fpage>379</fpage>&#x02013;<lpage>382</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.1202.379</pub-id><pub-id pub-id-type="pmid">5808865</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname><given-names>E.</given-names></name><name><surname>De Martino</surname><given-names>F.</given-names></name><name><surname>Bonte</surname><given-names>M.</given-names></name><name><surname>Goebel</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;Who&#x0201d; is saying &#x0201c;what&#x0201d;? Brain-based decoding of human voice and speech</article-title>. <source>Science</source>
<volume>322</volume>, <fpage>970</fpage>&#x02013;<lpage>973</lpage>. <pub-id pub-id-type="doi">10.1126/science.1164318</pub-id><pub-id pub-id-type="pmid">18988858</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>C. J.</given-names></name><name><surname>Iaria</surname><given-names>G.</given-names></name><name><surname>Barton</surname><given-names>J. J.</given-names></name></person-group> (<year>2009</year>). <article-title>Defining the face processing network: optimization of the functional localizer in fMRI</article-title>. <source>Hum. Brain Mapp</source>. <volume>30</volume>, <fpage>1637</fpage>&#x02013;<lpage>1651</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20630</pub-id><pub-id pub-id-type="pmid">18661501</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foxe</surname><given-names>J. J.</given-names></name><name><surname>Schroeder</surname><given-names>C. E.</given-names></name></person-group> (<year>2005</year>). <article-title>The case for feedforward multisensory convergence during early cortical processing</article-title>. <source>Neuroreport</source>
<volume>16</volume>, <fpage>419</fpage>&#x02013;<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200504040-00001</pub-id><pub-id pub-id-type="pmid">15770144</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foxe</surname><given-names>J. J.</given-names></name><name><surname>Wylie</surname><given-names>G. R.</given-names></name><name><surname>Martinez</surname><given-names>A. S.</given-names></name><name><surname>Schroeder</surname><given-names>C. E.</given-names></name><name><surname>Javitt</surname><given-names>D. C.</given-names></name><name><surname>Guilfoyle</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2002</year>). <article-title>Auditory-somatosensory multisensory processing in auditory association cortex: an fMRI study</article-title>. <source>J. Neurophysiol</source>. <volume>88</volume>, <fpage>540</fpage>&#x02013;<lpage>543</lpage>. <pub-id pub-id-type="pmid">12091578</pub-id></mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuchs</surname><given-names>M.</given-names></name><name><surname>Wagner</surname><given-names>M.</given-names></name><name><surname>K&#x000f6;hler</surname><given-names>T.</given-names></name><name><surname>Wischmann</surname><given-names>H. A.</given-names></name></person-group> (<year>1999</year>). <article-title>Linear and nonlinear current density reconstructions</article-title>. <source>J. Clin. Neurophysiol</source>. <volume>16</volume>, <fpage>267</fpage>&#x02013;<lpage>295</lpage>. <pub-id pub-id-type="doi">10.1097/00004691-199905000-00006</pub-id><pub-id pub-id-type="pmid">10426408</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>A. A.</given-names></name><name><surname>Maier</surname><given-names>J. X.</given-names></name><name><surname>Hoffman</surname><given-names>K. L.</given-names></name><name><surname>Logothetis</surname><given-names>N. K.</given-names></name></person-group> (<year>2005</year>). <article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title>. <source>J. Neurosci</source>. <volume>25</volume>, <fpage>5004</fpage>&#x02013;<lpage>5012</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0799-05.2005</pub-id><pub-id pub-id-type="pmid">15901781</pub-id></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>A. A.</given-names></name><name><surname>Schroeder</surname><given-names>C. E.</given-names></name></person-group> (<year>2006</year>). <article-title>Is neocortex essentially multisensory?</article-title>
<source>Trends Cogn. Sci. (Regul. Ed)</source>. <volume>10</volume>, <fpage>278</fpage>&#x02013;<lpage>285</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golestani</surname><given-names>N.</given-names></name><name><surname>Zatorre</surname><given-names>R. J.</given-names></name></person-group> (<year>2004</year>). <article-title>Learning new sounds of speech: reallocation of neural substrates</article-title>. <source>Neuroimage</source>
<volume>21</volume>, <fpage>494</fpage>&#x02013;<lpage>506</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.09.071</pub-id><pub-id pub-id-type="pmid">14980552</pub-id></mixed-citation></ref><ref id="B74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>M. A.</given-names></name><name><surname>Meenan</surname><given-names>J. P.</given-names></name><name><surname>Bulthoff</surname><given-names>H. H.</given-names></name><name><surname>Nicolle</surname><given-names>D. A.</given-names></name><name><surname>Murphy</surname><given-names>K. J.</given-names></name><name><surname>Racicot</surname><given-names>C. I.</given-names></name></person-group> (<year>1994</year>). <article-title>Separate neural pathways for the visual analysis of object shape in perception and prehension</article-title>. <source>Curr. Biol</source>. <volume>4</volume>, <fpage>604</fpage>&#x02013;<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1016/S0960-9822(00)00132-9</pub-id><pub-id pub-id-type="pmid">7953534</pub-id></mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>K. P.</given-names></name><name><surname>Kuhl</surname><given-names>P. K.</given-names></name></person-group> (<year>1989</year>). <article-title>The role of visual information in the processing of place and manner features in speech perception</article-title>. <source>Percept. Psychophys</source>. <volume>45</volume>, <fpage>34</fpage>&#x02013;<lpage>42</lpage>. <pub-id pub-id-type="doi">10.3758/BF03208030</pub-id><pub-id pub-id-type="pmid">2913568</pub-id></mixed-citation></ref><ref id="B76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name><name><surname>Kourtzi</surname><given-names>Z.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2001</year>). <article-title>The lateral occipital complex and its role in object recognition</article-title>. <source>Vision Res</source>. <volume>41</volume>, <fpage>1409</fpage>&#x02013;<lpage>1422</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name><name><surname>Malach</surname><given-names>R.</given-names></name></person-group> (<year>2001</year>). <article-title>fMR-adaptation: a tool for studying the functional properties of human cortical neurons</article-title>. <source>Acta Psychol</source>. <volume>107</volume>, <fpage>293</fpage>&#x02013;<lpage>321</lpage>. <pub-id pub-id-type="doi">10.1016/S0001-6918(01)00019-1</pub-id><pub-id pub-id-type="pmid">11388140</pub-id></mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guenther</surname><given-names>F. H.</given-names></name><name><surname>Ghosh</surname><given-names>S. S.</given-names></name><name><surname>Tourville</surname><given-names>J. A.</given-names></name></person-group> (<year>2006</year>). <article-title>Neural modeling and imaging of the cortical interactions underlying syllable production</article-title>. <source>Brain Lang</source>. <volume>96</volume>, <fpage>280</fpage>&#x02013;<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2005.06.001</pub-id><pub-id pub-id-type="pmid">16040108</pub-id></mixed-citation></ref><ref id="B79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>D. A.</given-names></name><name><surname>Fussell</surname><given-names>C.</given-names></name><name><surname>Summerfield</surname><given-names>A. Q.</given-names></name></person-group> (<year>2005</year>). <article-title>Reading fluent speech from talking faces: typical brain networks and individual differences</article-title>. <source>J. Cogn. Neurosci</source>. <volume>17</volume>, <fpage>939</fpage>&#x02013;<lpage>953</lpage>. <pub-id pub-id-type="doi">10.1162/0898929054021175</pub-id><pub-id pub-id-type="pmid">15969911</pub-id></mixed-citation></ref><ref id="B80"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harnad</surname><given-names>S.</given-names></name></person-group> (<year>1987</year>). <article-title>Category induction and representation</article-title>, in <source>Categorical Perception: The Groundwork of Cognition</source>, ed <person-group person-group-type="editor"><name><surname>Harnad</surname><given-names>S.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>), <fpage>535</fpage>&#x02013;<lpage>565</lpage>.</mixed-citation></ref><ref id="B81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U.</given-names></name><name><surname>Skipper</surname><given-names>J. I.</given-names></name><name><surname>Nusbaum</surname><given-names>H. C.</given-names></name><name><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2007</year>). <article-title>Abstract coding of audiovisual speech: beyond sensory representation</article-title>. <source>Neuron</source>
<volume>56</volume>, <fpage>1116</fpage>&#x02013;<lpage>1126</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.037</pub-id><pub-id pub-id-type="pmid">18093531</pub-id></mixed-citation></ref><ref id="B82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>J. V.</given-names></name><name><surname>Hoffman</surname><given-names>E. A.</given-names></name><name><surname>Gobbini</surname><given-names>M. I.</given-names></name></person-group> (<year>2002</year>). <article-title>Human neural systems for face recognition and social communication</article-title>. <source>Biol. Psychiatry</source>
<volume>51</volume>, <fpage>59</fpage>&#x02013;<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1016/S0006-3223(01)01330-0</pub-id><pub-id pub-id-type="pmid">11801231</pub-id></mixed-citation></ref><ref id="B83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>J. V.</given-names></name><name><surname>Horwitz</surname><given-names>B.</given-names></name><name><surname>Ungerleider</surname><given-names>L. G.</given-names></name><name><surname>Maisog</surname><given-names>J. M.</given-names></name><name><surname>Pietrini</surname><given-names>P.</given-names></name><name><surname>Grady</surname><given-names>C. L.</given-names></name></person-group> (<year>1994</year>). <article-title>The functional organization of human extrastriate cortex: a PET-rCBF study of selective attention to faces and locations</article-title>. <source>J. Neurosci</source>. <volume>14(11 Pt 1)</volume>, <fpage>6336</fpage>&#x02013;<lpage>6353</lpage>. <pub-id pub-id-type="pmid">7965040</pub-id></mixed-citation></ref><ref id="B84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>U.</given-names></name><name><surname>Amedi</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Flexibility and stability in sensory processing revealed using visual-to-auditory sensory substitution</article-title>. <source>Cereb. Cortex</source>. [Epub ahead of print]. <pub-id pub-id-type="doi">10.1093/cercor/bhu010</pub-id><pub-id pub-id-type="pmid">24518756</pub-id></mixed-citation></ref><ref id="B85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G.</given-names></name><name><surname>Buchsbaum</surname><given-names>B.</given-names></name><name><surname>Humphries</surname><given-names>C.</given-names></name><name><surname>Muftuler</surname><given-names>T.</given-names></name></person-group> (<year>2003</year>). <article-title>Auditory-motor interaction revealed by fMRI: speech, music, and working memory in area Spt</article-title>. <source>J. Cogn. Neurosci</source>. <volume>15</volume>, <fpage>673</fpage>&#x02013;<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1162/089892903322307393</pub-id><pub-id pub-id-type="pmid">12965041</pub-id></mixed-citation></ref><ref id="B86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G.</given-names></name><name><surname>Poeppel</surname><given-names>D.</given-names></name></person-group> (<year>2007</year>). <article-title>The cortical organization of speech processing</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>8</volume>, <fpage>393</fpage>&#x02013;<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></mixed-citation></ref><ref id="B87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname><given-names>S.</given-names></name><name><surname>Ahissar</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title>. <source>Neuron</source>
<volume>36</volume>, <fpage>791</fpage>&#x02013;<lpage>804</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></mixed-citation></ref><ref id="B88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekert</surname><given-names>M.</given-names></name><name><surname>Bais</surname><given-names>L.</given-names></name><name><surname>Kahn</surname><given-names>R. S.</given-names></name><name><surname>Aleman</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>Time course of the involvement of the right anterior superior temporal gyrus and the right fronto-parietal operculum in emotional prosody perception</article-title>. <source>PLoS ONE</source>
<volume>3</volume>:<fpage>e2244</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0002244</pub-id><pub-id pub-id-type="pmid">18493307</pub-id></mixed-citation></ref><ref id="B89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>C.</given-names></name><name><surname>Binder</surname><given-names>J. R.</given-names></name><name><surname>Medler</surname><given-names>D. A.</given-names></name><name><surname>Liebenthal</surname><given-names>E.</given-names></name></person-group> (<year>2006</year>). <article-title>Syntactic and semantic modulation of neural activity during auditory sentence comprehension</article-title>. <source>J. Cogn. Neurosci</source>. <volume>18</volume>, <fpage>665</fpage>&#x02013;<lpage>679</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2006.18.4.665</pub-id><pub-id pub-id-type="pmid">16768368</pub-id></mixed-citation></ref><ref id="B90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>C.</given-names></name><name><surname>Love</surname><given-names>T.</given-names></name><name><surname>Swinney</surname><given-names>D.</given-names></name><name><surname>Hickok</surname><given-names>G.</given-names></name></person-group> (<year>2005</year>). <article-title>Response of anterior temporal cortex to syntactic and prosodic manipulations during sentence processing</article-title>. <source>Hum. Brain Mapp</source>. <volume>26</volume>, <fpage>128</fpage>&#x02013;<lpage>138</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20148</pub-id><pub-id pub-id-type="pmid">15895428</pub-id></mixed-citation></ref><ref id="B91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>C.</given-names></name><name><surname>Sabri</surname><given-names>M.</given-names></name><name><surname>Heugel</surname><given-names>N.</given-names></name><name><surname>Lewis</surname><given-names>K.</given-names></name><name><surname>Liebenthal</surname><given-names>E.</given-names></name></person-group> (<year>2013</year>). <article-title>Pattern specific adaptation to speech and non-speech sounds in human auditory cortex (354.21/SS7)</article-title>. <source>Soc. Neurosci</source>. Abstract 354.21/SS7.</mixed-citation></ref><ref id="B92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iverson</surname><given-names>P.</given-names></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name></person-group> (<year>1998</year>). <article-title>Modeling the interaction of phonemic intelligibility and lexical structure in audiovisual word recognition</article-title>. <source>Speech Commun</source>. <volume>26</volume>, <fpage>45</fpage>&#x02013;<lpage>63</lpage>
<pub-id pub-id-type="doi">10.1016/S0167-6393(98)00049-1</pub-id></mixed-citation></ref><ref id="B93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacquemot</surname><given-names>C.</given-names></name><name><surname>Pallier</surname><given-names>C.</given-names></name><name><surname>LeBihan</surname><given-names>D.</given-names></name><name><surname>Dehaene</surname><given-names>S.</given-names></name><name><surname>Dupoux</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>). <article-title>Phonological grammar shapes the auditory cortex: a functional magnetic resonance imaging study</article-title>. <source>J. Neurosci</source>. <volume>23</volume>, <fpage>9541</fpage>&#x02013;<lpage>9546</lpage>. <pub-id pub-id-type="pmid">14573533</pub-id></mixed-citation></ref><ref id="B94"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffers</surname><given-names>J.</given-names></name><name><surname>Barley</surname><given-names>M.</given-names></name></person-group> (<year>1971</year>). <source>Speechreading (Lipreading)</source>. <publisher-loc>Springfield, IL</publisher-loc>: <publisher-name>Charles C. Thomas</publisher-name>.</mixed-citation></ref><ref id="B95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jesse</surname><given-names>A.</given-names></name><name><surname>McQueen</surname><given-names>J. M.</given-names></name></person-group> (<year>2014</year>). <article-title>Suprasegmental lexical stress cues in visual speech can guide spoken-word recognition</article-title>. <source>Q. J. Exp. Psychol. (Hove)</source>. <volume>67</volume>, <fpage>793</fpage>&#x02013;<lpage>808</lpage>. <pub-id pub-id-type="doi">10.1080/17470218.2013.834371</pub-id><pub-id pub-id-type="pmid">24134065</pub-id></mixed-citation></ref><ref id="B96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J.</given-names></name><name><surname>Alwan</surname><given-names>A.</given-names></name><name><surname>Keating</surname><given-names>P.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>2002</year>). <article-title>On the relationship between face movements, tongue movements, and speech acoustics</article-title>. <source>EURASIP J. Appl. Signal Process</source>. <volume>2002</volume>, <fpage>1174</fpage>&#x02013;<lpage>1188</lpage>
<pub-id pub-id-type="doi">10.1155/S1110865702206046</pub-id></mixed-citation></ref><ref id="B97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name><name><surname>Alwan</surname><given-names>A.</given-names></name><name><surname>Keating</surname><given-names>P. A.</given-names></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name></person-group> (<year>2007</year>). <article-title>Similarity structure in visual speech perception and optical phonetic signals</article-title>. <source>Percept. Psychophys</source>. <volume>69</volume>, <fpage>1070</fpage>&#x02013;<lpage>1083</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193945</pub-id><pub-id pub-id-type="pmid">18038946</pub-id></mixed-citation></ref><ref id="B98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joanisse</surname><given-names>M. F.</given-names></name><name><surname>Zevin</surname><given-names>J. D.</given-names></name><name><surname>McCandliss</surname><given-names>B. D.</given-names></name></person-group> (<year>2007</year>). <article-title>Brain mechanisms implicated in the preattentive categorization of speech sounds revealed using FMRI and a short-interval habituation trial paradigm</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>2084</fpage>&#x02013;<lpage>2093</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhl124</pub-id><pub-id pub-id-type="pmid">17138597</pub-id></mixed-citation></ref><ref id="B99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jobard</surname><given-names>G.</given-names></name><name><surname>Crivello</surname><given-names>F.</given-names></name><name><surname>Tzourio-Mazoyer</surname><given-names>N.</given-names></name></person-group> (<year>2003</year>). <article-title>Evaluation of the dual route theory of reading: a metanalysis of 35 neuroimaging studies</article-title>. <source>Neuroimage</source>
<volume>20</volume>, <fpage>693</fpage>&#x02013;<lpage>712</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00343-4</pub-id><pub-id pub-id-type="pmid">14568445</pub-id></mixed-citation></ref><ref id="B100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname><given-names>G.</given-names></name></person-group> (<year>1973</year>). <article-title>Visual perception of biological motion and a model for its analysis</article-title>. <source>Percept. Psychophys</source>. <volume>14</volume>, <fpage>201</fpage>&#x02013;<lpage>211</lpage>
<pub-id pub-id-type="doi">10.3758/BF03212378</pub-id></mixed-citation></ref><ref id="B101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>E. K.</given-names></name><name><surname>Seidl</surname><given-names>A.</given-names></name><name><surname>Tyler</surname><given-names>M. D.</given-names></name></person-group> (<year>2014</year>). <article-title>The edge factor in early word segmentation: utterance-level prosody enables word form extraction by 6-month-olds</article-title>. <source>PLoS ONE</source>
<volume>9</volume>:<fpage>e83546</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0083546</pub-id><pub-id pub-id-type="pmid">24421892</pub-id></mixed-citation></ref><ref id="B102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>J. H.</given-names></name><name><surname>Hackett</surname><given-names>T. A.</given-names></name></person-group> (<year>2000</year>). <article-title>Subdivisions of auditory cortex and processing streams in primates</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>97</volume>, <fpage>11793</fpage>&#x02013;<lpage>11799</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.97.22.11793</pub-id><pub-id pub-id-type="pmid">11050211</pub-id></mixed-citation></ref><ref id="B103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N.</given-names></name><name><surname>McDermott</surname><given-names>J.</given-names></name><name><surname>Chun</surname><given-names>M. M.</given-names></name></person-group> (<year>1997</year>). <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source>J. Neurosci</source>. <volume>17</volume>, <fpage>4302</fpage>&#x02013;<lpage>4311</lpage>. <pub-id pub-id-type="pmid">9151747</pub-id></mixed-citation></ref><ref id="B104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karns</surname><given-names>C. M.</given-names></name><name><surname>Dow</surname><given-names>M. W.</given-names></name><name><surname>Neville</surname><given-names>H. J.</given-names></name></person-group> (<year>2012</year>). <article-title>Altered cross-modal processing in the primary auditory cortex of congenitally deaf adults: a visual-somatosensory fMRI study with a double-flash illusion</article-title>. <source>J. Neurosci</source>. <volume>32</volume>, <fpage>9626</fpage>&#x02013;<lpage>9638</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.6488-11.2012</pub-id><pub-id pub-id-type="pmid">22787048</pub-id></mixed-citation></ref><ref id="B105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C.</given-names></name><name><surname>Petkov</surname><given-names>C. I.</given-names></name><name><surname>Logothetis</surname><given-names>N. K.</given-names></name></person-group> (<year>2008</year>). <article-title>Visual modulation of neurons in auditory cortex</article-title>. <source>Cereb. Cortex</source>
<volume>18</volume>, <fpage>1560</fpage>&#x02013;<lpage>1574</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhm187</pub-id><pub-id pub-id-type="pmid">18180245</pub-id></mixed-citation></ref><ref id="B106"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C.</given-names></name><name><surname>Petkov</surname><given-names>C. I.</given-names></name><name><surname>Remedios</surname><given-names>R.</given-names></name><name><surname>Logothetis</surname><given-names>N. K.</given-names></name></person-group> (<year>2012</year>). <article-title>Multisensory influences on auditory processing: perspectives from fMRI and electrophysiology</article-title>, in <source>The Neural Bases of Multisensory Processes</source>, eds <person-group person-group-type="editor"><name><surname>Murray</surname><given-names>M. M.</given-names></name><name><surname>Wallace</surname><given-names>M. T.</given-names></name></person-group> (<publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>CRC Press</publisher-name>). <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/books/NBK92843/">http://www.ncbi.nlm.nih.gov/books/NBK92843/</ext-link></mixed-citation></ref><ref id="B107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilian-Hutten</surname><given-names>N.</given-names></name><name><surname>Valente</surname><given-names>G.</given-names></name><name><surname>Vroomen</surname><given-names>J.</given-names></name><name><surname>Formisano</surname><given-names>E.</given-names></name></person-group> (<year>2011</year>). <article-title>Auditory cortex encodes the perceptual interpretation of ambiguous sound</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>1715</fpage>&#x02013;<lpage>1720</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4572-10.2011</pub-id><pub-id pub-id-type="pmid">21289180</pub-id></mixed-citation></ref><ref id="B108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klatt</surname><given-names>D.</given-names></name></person-group> (<year>1979</year>). <article-title>Speech perception: a model of acoustic-phonetic analysis and lexical access</article-title>. <source>J. Phon</source>. <volume>7</volume>, <fpage>279</fpage>&#x02013;<lpage>312</lpage>. <pub-id pub-id-type="pmid">21261450</pub-id></mixed-citation></ref><ref id="B109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kral</surname><given-names>A.</given-names></name><name><surname>Eggermont</surname><given-names>J. J.</given-names></name></person-group> (<year>2007</year>). <article-title>What's to lose and what's to learn: development under auditory deprivation, cochlear implants and limits of cortical plasticity</article-title>. <source>Brain Res. Rev</source>. <volume>56</volume>, <fpage>259</fpage>&#x02013;<lpage>269</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainresrev.2007.07.021</pub-id><pub-id pub-id-type="pmid">17950463</pub-id></mixed-citation></ref><ref id="B110"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kruskal</surname><given-names>J. B.</given-names></name><name><surname>Wish</surname><given-names>M.</given-names></name></person-group> (<year>1978</year>). <source>Multidimensional Scaling</source>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</mixed-citation></ref><ref id="B111"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>P. K.</given-names></name><name><surname>Meltzoff</surname><given-names>A. N.</given-names></name></person-group> (<year>1988</year>). <article-title>Speech as an intermodal object of perception</article-title>, in <source>Perceptual Development in Infancy (Vol. The Minnesota Symposia on Child Psychology, 20, pp. 235&#x02013;266)</source>, ed <person-group person-group-type="editor"><name><surname>Yonas</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates, Inc.</publisher-name>).</mixed-citation></ref><ref id="B112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lansing</surname><given-names>C. R.</given-names></name><name><surname>McConkie</surname><given-names>G. W.</given-names></name></person-group> (<year>1999</year>). <article-title>Attention to facial regions in segmental and prosodic visual speech perception tasks</article-title>. <source>J. Speech Lang. Hear. Res</source>. <volume>42</volume>, <fpage>526</fpage>&#x02013;<lpage>539</lpage>. <pub-id pub-id-type="doi">10.1044/jslhr.4203.526</pub-id><pub-id pub-id-type="pmid">10391620</pub-id></mixed-citation></ref><ref id="B113"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver</surname><given-names>A. M.</given-names></name><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name></person-group> (<year>2010</year>). <article-title>Cortical representation of natural complex sounds: effects of acoustic features and auditory object category</article-title>. <source>J. Neurosci</source>. <volume>30</volume>, <fpage>7604</fpage>&#x02013;<lpage>7612</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0296-10.2010</pub-id><pub-id pub-id-type="pmid">20519535</pub-id></mixed-citation></ref><ref id="B114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y. S.</given-names></name><name><surname>Turkeltaub</surname><given-names>P.</given-names></name><name><surname>Granger</surname><given-names>R.</given-names></name><name><surname>Raizada</surname><given-names>R. D.</given-names></name></person-group> (<year>2012</year>). <article-title>Categorical speech processing in Broca's area: an fMRI study using multivariate pattern-based analysis</article-title>. <source>J. Neurosci</source>. <volume>32</volume>, <fpage>3942</fpage>&#x02013;<lpage>3948</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3814-11.2012</pub-id><pub-id pub-id-type="pmid">22423114</pub-id></mixed-citation></ref><ref id="B115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemus</surname><given-names>L.</given-names></name><name><surname>Hernandez</surname><given-names>A.</given-names></name><name><surname>Luna</surname><given-names>R.</given-names></name><name><surname>Zainos</surname><given-names>A.</given-names></name><name><surname>Romo</surname><given-names>R.</given-names></name></person-group> (<year>2010</year>). <article-title>Do sensory cortices process more than one sensory modality during perceptual judgments?</article-title>
<source>Neuron</source>
<volume>67</volume>, <fpage>335</fpage>&#x02013;<lpage>348</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.06.015</pub-id><pub-id pub-id-type="pmid">20670839</pub-id></mixed-citation></ref><ref id="B116"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levanen</surname><given-names>S.</given-names></name><name><surname>Jousmaki</surname><given-names>V.</given-names></name><name><surname>Hari</surname><given-names>R.</given-names></name></person-group> (<year>1998</year>). <article-title>Vibration-induced auditory-cortex activation in a congenitally deaf adult</article-title>. <source>Curr. Biol</source>. <volume>8</volume>, <fpage>869</fpage>&#x02013;<lpage>872</lpage>. <pub-id pub-id-type="doi">10.1016/S0960-9822(07)00348-X</pub-id><pub-id pub-id-type="pmid">9705933</pub-id></mixed-citation></ref><ref id="B117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>J. W.</given-names></name><name><surname>Van Essen</surname><given-names>D. C.</given-names></name></person-group> (<year>2000</year>). <article-title>Corticocortical connections of visual, sensorimotor, and multimodal processing areas in the parietal lobe of the macaque monkey</article-title>. <source>J. Comp. Neurol</source>. <volume>428</volume>, <fpage>112</fpage>&#x02013;<lpage>137</lpage>. <pub-id pub-id-type="pmid">11058227</pub-id></mixed-citation></ref><ref id="B118"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>A. M.</given-names></name></person-group> (<year>1982</year>). <article-title>On finding that speech is special</article-title>. <source>Am. Psychol</source>. <volume>37</volume>, <fpage>148</fpage>&#x02013;<lpage>167</lpage>
<pub-id pub-id-type="doi">10.1037/0003-066X.37.2.148</pub-id></mixed-citation></ref><ref id="B119"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>A. M.</given-names></name><name><surname>Cooper</surname><given-names>F. S.</given-names></name><name><surname>Shankweiler</surname><given-names>D. P.</given-names></name><name><surname>Studdert-Kennedy</surname><given-names>M.</given-names></name></person-group> (<year>1967</year>). <article-title>Perception of the speech code</article-title>. <source>Psychol. Rev</source>. <volume>74</volume>, <fpage>431</fpage>&#x02013;<lpage>461</lpage>. <pub-id pub-id-type="doi">10.1037/h0020279</pub-id><pub-id pub-id-type="pmid">4170865</pub-id></mixed-citation></ref><ref id="B120"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>A. M.</given-names></name><name><surname>Mattingly</surname><given-names>I. G.</given-names></name></person-group> (<year>1985</year>). <article-title>The motor theory of speech perception revised</article-title>. <source>Cognition</source>
<volume>21</volume>, <fpage>1</fpage>&#x02013;<lpage>36</lpage>. <pub-id pub-id-type="doi">10.1016/0010-0277(85)90021-6</pub-id><pub-id pub-id-type="pmid">4075760</pub-id></mixed-citation></ref><ref id="B121"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebenthal</surname><given-names>E.</given-names></name><name><surname>Binder</surname><given-names>J. R.</given-names></name><name><surname>Spitzer</surname><given-names>S. M.</given-names></name><name><surname>Possing</surname><given-names>E. T.</given-names></name><name><surname>Medler</surname><given-names>D. A.</given-names></name></person-group> (<year>2005</year>). <article-title>Neural substrates of phonemic perception</article-title>. <source>Cereb. Cortex</source>
<volume>15</volume>, <fpage>1621</fpage>&#x02013;<lpage>1631</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhi040</pub-id><pub-id pub-id-type="pmid">15703256</pub-id></mixed-citation></ref><ref id="B122"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebenthal</surname><given-names>E.</given-names></name><name><surname>Desai</surname><given-names>R.</given-names></name><name><surname>Ellingson</surname><given-names>M. M.</given-names></name><name><surname>Ramachandran</surname><given-names>B.</given-names></name><name><surname>Desai</surname><given-names>A.</given-names></name><name><surname>Binder</surname><given-names>J. R.</given-names></name></person-group> (<year>2010</year>). <article-title>Specialization along the left superior temporal sulcus for auditory categorization</article-title>. <source>Cereb. Cortex</source>
<volume>20</volume>, <fpage>2958</fpage>&#x02013;<lpage>2970</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhq045</pub-id><pub-id pub-id-type="pmid">20382643</pub-id></mixed-citation></ref><ref id="B123"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebenthal</surname><given-names>E.</given-names></name><name><surname>Desai</surname><given-names>R. H.</given-names></name><name><surname>Humphries</surname><given-names>C.</given-names></name><name><surname>Sabri</surname><given-names>M.</given-names></name><name><surname>Desai</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>The functional organization of the left STS: a large scale meta-analysis of PET and fMRI studies of healthy adults</article-title>. <source>Front. Neurosci</source>. <volume>8</volume>:<issue>289</issue>. <pub-id pub-id-type="doi">10.3389/fnins.2014.00289</pub-id><pub-id pub-id-type="pmid">25309312</pub-id></mixed-citation></ref><ref id="B124"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebenthal</surname><given-names>E.</given-names></name><name><surname>Sabri</surname><given-names>M.</given-names></name><name><surname>Beardsley</surname><given-names>S. A.</given-names></name><name><surname>Mangalathu-Arumana</surname><given-names>J.</given-names></name><name><surname>Desai</surname><given-names>A.</given-names></name></person-group> (<year>2013</year>). <article-title>Neural dynamics of phonological processing in the dorsal auditory stream</article-title>. <source>J. Neurosci</source>. <volume>33</volume>, <fpage>15414</fpage>&#x02013;<lpage>15424</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1511-13.2013</pub-id><pub-id pub-id-type="pmid">24068810</pub-id></mixed-citation></ref><ref id="B125"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisker</surname><given-names>L.</given-names></name><name><surname>Liberman</surname><given-names>A. M.</given-names></name><name><surname>Erickson</surname><given-names>D. M.</given-names></name><name><surname>Dechovitz</surname><given-names>D.</given-names></name><name><surname>Mandler</surname><given-names>R.</given-names></name></person-group> (<year>1977</year>). <article-title>On pushing the voice onset-time (VOT) boundary about</article-title>. <source>Lang. Speech</source>
<volume>20</volume>, <fpage>209</fpage>&#x02013;<lpage>216</lpage>. <pub-id pub-id-type="pmid">613182</pub-id></mixed-citation></ref><ref id="B126"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>N. K.</given-names></name><name><surname>Sheinberg</surname><given-names>D. L.</given-names></name></person-group> (<year>1996</year>). <article-title>Visual object recognition</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>19</volume>, <fpage>577</fpage>&#x02013;<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.19.030196.003045</pub-id><pub-id pub-id-type="pmid">8833455</pub-id></mixed-citation></ref><ref id="B127"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludman</surname><given-names>C. N.</given-names></name><name><surname>Summerfield</surname><given-names>A. Q.</given-names></name><name><surname>Hall</surname><given-names>D.</given-names></name><name><surname>Elliott</surname><given-names>M.</given-names></name><name><surname>Foster</surname><given-names>J.</given-names></name><name><surname>Hykin</surname><given-names>J. L.</given-names></name><etal/></person-group>. (<year>2000</year>). <article-title>Lip-reading ability and patterns of cortical activation studied using fMRI</article-title>. <source>Br. J. Audiol</source>. <volume>34</volume>, <fpage>225</fpage>&#x02013;<lpage>230</lpage>. <pub-id pub-id-type="doi">10.3109/03005364000000132</pub-id><pub-id pub-id-type="pmid">10997451</pub-id></mixed-citation></ref><ref id="B128"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyness</surname><given-names>R. C.</given-names></name><name><surname>Alvarez</surname><given-names>I.</given-names></name><name><surname>Sereno</surname><given-names>M. I.</given-names></name><name><surname>MacSweeney</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Microstructural differences in the thalamus and thalamic radiations in the congenitally deaf</article-title>. <source>Neuroimage</source>
<volume>100</volume>, <fpage>347</fpage>&#x02013;<lpage>357</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.077</pub-id><pub-id pub-id-type="pmid">24907483</pub-id></mixed-citation></ref><ref id="B129"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyxell</surname><given-names>B.</given-names></name><name><surname>Ronnberg</surname><given-names>J.</given-names></name><name><surname>Andersson</surname><given-names>J.</given-names></name><name><surname>Linderoth</surname><given-names>E.</given-names></name></person-group> (<year>1993</year>). <article-title>Vibrotactile support: initial effects on visual speech perception</article-title>. <source>Scand. Audiol. Suppl</source>. <volume>22</volume>, <fpage>179</fpage>&#x02013;<lpage>183</lpage>. <pub-id pub-id-type="doi">10.3109/01050399309047465</pub-id><pub-id pub-id-type="pmid">8210957</pub-id></mixed-citation></ref><ref id="B130"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname><given-names>M.</given-names></name><name><surname>Amaro</surname><given-names>E.</given-names></name><name><surname>Calvert</surname><given-names>G. A.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>David</surname><given-names>A. S.</given-names></name><name><surname>McGuire</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2000</year>). <article-title>Silent speechreading in the absence of scanner noise: an event-related fMRI study</article-title>. <source>Neuroreport</source>
<volume>11</volume>, <fpage>1729</fpage>&#x02013;<lpage>1733</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200006050-00026</pub-id><pub-id pub-id-type="pmid">10852233</pub-id></mixed-citation></ref><ref id="B131"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname><given-names>M.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Woll</surname><given-names>B.</given-names></name><name><surname>Giampietro</surname><given-names>V.</given-names></name><name><surname>David</surname><given-names>A. S.</given-names></name><name><surname>McGuire</surname><given-names>P. K.</given-names></name><etal/></person-group>. (<year>2004</year>). <article-title>Dissociating linguistic and nonlinguistic gestural communication in the brain</article-title>. <source>Neuroimage</source>
<volume>22</volume>, <fpage>1605</fpage>&#x02013;<lpage>1618</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.015</pub-id><pub-id pub-id-type="pmid">15275917</pub-id></mixed-citation></ref><ref id="B132"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname><given-names>M.</given-names></name><name><surname>Capek</surname><given-names>C. M.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Woll</surname><given-names>B.</given-names></name></person-group> (<year>2008</year>). <article-title>The signing brain: the neurobiology of sign language</article-title>. <source>Trends Cogn. Sci. (Regul. Ed)</source>. <volume>12</volume>, <fpage>432</fpage>&#x02013;<lpage>440</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2008.07.010</pub-id><pub-id pub-id-type="pmid">18805728</pub-id></mixed-citation></ref><ref id="B133"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname><given-names>M.</given-names></name><name><surname>Woll</surname><given-names>B.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>McGuire</surname><given-names>P. K.</given-names></name><name><surname>David</surname><given-names>A. S.</given-names></name><name><surname>Williams</surname><given-names>S. C.</given-names></name><etal/></person-group>. (<year>2002</year>). <article-title>Neural systems underlying British Sign Language and audio-visual English processing in native users</article-title>. <source>Brain</source>
<volume>125</volume>, <fpage>1583</fpage>&#x02013;<lpage>1593</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awf153</pub-id><pub-id pub-id-type="pmid">12077007</pub-id></mixed-citation></ref><ref id="B134"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Massaro</surname><given-names>D. W.</given-names></name></person-group> (<year>1987</year>). <source>Speech Perception by Ear and Eye: A Paradigm for Psychological Inquiry</source>. <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates, Inc.</publisher-name></mixed-citation></ref><ref id="B135"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massaro</surname><given-names>D. W.</given-names></name><name><surname>Cohen</surname><given-names>M. M.</given-names></name></person-group> (<year>1983</year>). <article-title>Evaluation and integration of visual and auditory information in speech perception</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>9</volume>, <fpage>753</fpage>&#x02013;<lpage>771</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.9.5.753</pub-id><pub-id pub-id-type="pmid">6227688</pub-id></mixed-citation></ref><ref id="B136"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Massaro</surname><given-names>D. W.</given-names></name><name><surname>Cohen</surname><given-names>M. M.</given-names></name><name><surname>Tabain</surname><given-names>M.</given-names></name><name><surname>Beskow</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>Animated speech: research progress and applications</article-title>, in <source>Audiovisual Speech Processing</source>, eds <person-group person-group-type="editor"><name><surname>Clark</surname><given-names>R. B.</given-names></name><name><surname>Perrier</surname><given-names>J. P.</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name></person-group> (<publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University</publisher-name>), <fpage>246</fpage>&#x02013;<lpage>272</lpage>.</mixed-citation></ref><ref id="B137"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matchin</surname><given-names>W.</given-names></name><name><surname>Groulx</surname><given-names>K.</given-names></name><name><surname>Hickok</surname><given-names>G.</given-names></name></person-group> (<year>2014</year>). <article-title>Audiovisual speech integration does not rely on the motor system: evidence from articulatory suppression, the McGurk effect, and fMRI</article-title>. <source>J. Cog. Neurosci</source>. <volume>26</volume>, <fpage>606</fpage>&#x02013;<lpage>620</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00515</pub-id><pub-id pub-id-type="pmid">24236768</pub-id></mixed-citation></ref><ref id="B138"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattys</surname><given-names>S. L.</given-names></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name></person-group> (<year>2002</year>). <article-title>Stimulus-based lexical distinctiveness as a general word-recognition mechanism</article-title>. <source>Percept. Psychophys</source>. <volume>64</volume>, <fpage>667</fpage>&#x02013;<lpage>679</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194734</pub-id><pub-id pub-id-type="pmid">12132766</pub-id></mixed-citation></ref><ref id="B139"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGurk</surname><given-names>H.</given-names></name><name><surname>MacDonald</surname><given-names>J.</given-names></name></person-group> (<year>1976</year>). <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>
<volume>264</volume>, <fpage>746</fpage>&#x02013;<lpage>748</lpage>. <pub-id pub-id-type="doi">10.1038/264746a0</pub-id><pub-id pub-id-type="pmid">1012311</pub-id></mixed-citation></ref><ref id="B140"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>I. G.</given-names></name><name><surname>Wilson</surname><given-names>S. M.</given-names></name><name><surname>Deblieck</surname><given-names>C.</given-names></name><name><surname>Wu</surname><given-names>A. D.</given-names></name><name><surname>Iacoboni</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>The essential role of premotor cortex in speech perception</article-title>. <source>Curr. Biol</source>. <volume>17</volume>, <fpage>1692</fpage>&#x02013;<lpage>1696</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2007.08.064</pub-id><pub-id pub-id-type="pmid">17900904</pub-id></mixed-citation></ref><ref id="B141"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N.</given-names></name><name><surname>Cheung</surname><given-names>C.</given-names></name><name><surname>Johnson</surname><given-names>K.</given-names></name><name><surname>Chang</surname><given-names>E. F.</given-names></name></person-group> (<year>2014</year>). <article-title>Phonetic feature encoding in human superior temporal gyrus</article-title>. <source>Science</source>
<volume>343</volume>, <fpage>1006</fpage>&#x02013;<lpage>1010</lpage>. <pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></mixed-citation></ref><ref id="B142"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesulam</surname><given-names>M. M.</given-names></name></person-group> (<year>1998</year>). <article-title>From sensation to cognition</article-title>. <source>Brain</source>
<volume>121</volume>, <fpage>1013</fpage>&#x02013;<lpage>1052</lpage>. <pub-id pub-id-type="doi">10.1093/brain/121.6.1013</pub-id><pub-id pub-id-type="pmid">9648540</pub-id></mixed-citation></ref><ref id="B143"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miki</surname><given-names>K.</given-names></name><name><surname>Watanabe</surname><given-names>S.</given-names></name><name><surname>Kakigi</surname><given-names>R.</given-names></name><name><surname>Puce</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Magnetoencephalographic study of occipitotemporal activity elicited by viewing mouth movements</article-title>. <source>J. Clin. Neurophysiol</source>. <volume>115</volume>, <fpage>1559</fpage>&#x02013;<lpage>1574</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2004.02.013</pub-id><pub-id pub-id-type="pmid">15203057</pub-id></mixed-citation></ref><ref id="B144"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>L. M.</given-names></name><name><surname>D'Esposito</surname><given-names>M.</given-names></name></person-group> (<year>2005</year>). <article-title>Perceptual fusion and stimulus coincidence in the cross-modal integration of speech</article-title>. <source>J. Neurosci</source>. <volume>25</volume>, <fpage>5884</fpage>&#x02013;<lpage>5893</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0896-05.2005</pub-id><pub-id pub-id-type="pmid">15976077</pub-id></mixed-citation></ref><ref id="B145"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohammed</surname><given-names>T.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Macsweeney</surname><given-names>M.</given-names></name><name><surname>Barry</surname><given-names>F.</given-names></name><name><surname>Coleman</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Speechreading and its association with reading among deaf, hearing and dyslexic individuals</article-title>. <source>Clin. Linguist. Phon</source>. <volume>20</volume>, <fpage>621</fpage>&#x02013;<lpage>630</lpage>. <pub-id pub-id-type="doi">10.1080/02699200500266745</pub-id><pub-id pub-id-type="pmid">17056494</pub-id></mixed-citation></ref><ref id="B146"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000f6;tt&#x000f6;nen</surname><given-names>R.</given-names></name><name><surname>Krause</surname><given-names>C. M.</given-names></name><name><surname>Tiippana</surname><given-names>K.</given-names></name><name><surname>Sams</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Processing of changes in visual speech in the human auditory cortex</article-title>. <source>Cogn. Brain Res</source>. <volume>13</volume>, <fpage>417</fpage>&#x02013;<lpage>425</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(02)00053-8</pub-id><pub-id pub-id-type="pmid">11919005</pub-id></mixed-citation></ref><ref id="B146a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000f6;tt&#x000f6;nen</surname><given-names>R.</given-names></name><name><surname>Watkins</surname><given-names>K. E.</given-names></name></person-group> (<year>2009</year>). <article-title>Motor representations of articulators contribute to categorical perception of speech sounds</article-title>. <source>J. Neurosci</source>. <volume>29</volume>, <fpage>9819</fpage>&#x02013;<lpage>9825</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.6018-08.2009</pub-id><pub-id pub-id-type="pmid">19657034</pub-id></mixed-citation></ref><ref id="B147"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munhall</surname><given-names>K. G.</given-names></name><name><surname>Jones</surname><given-names>J. A.</given-names></name><name><surname>Callan</surname><given-names>D. E.</given-names></name><name><surname>Kuratate</surname><given-names>T.</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name></person-group> (<year>2004</year>). <article-title>Visual prosody and speech intelligibility: head movement improves auditory speech perception</article-title>. <source>Psychol. Sci</source>. <volume>15</volume>, <fpage>133</fpage>&#x02013;<lpage>137</lpage>. <pub-id pub-id-type="doi">10.1111/j.0963-7214.2004.01502010.x</pub-id><pub-id pub-id-type="pmid">14738521</pub-id></mixed-citation></ref><ref id="B148"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murase</surname><given-names>M.</given-names></name><name><surname>Saito</surname><given-names>D. N.</given-names></name><name><surname>Kochiyama</surname><given-names>T.</given-names></name><name><surname>Tanabe</surname><given-names>H. C.</given-names></name><name><surname>Tanaka</surname><given-names>S.</given-names></name><name><surname>Harada</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Cross-modal integration during vowel identification in audiovisual speech: a functional magnetic resonance imaging study</article-title>. <source>Neurosci. Lett</source>. <volume>434</volume>, <fpage>71</fpage>&#x02013;<lpage>76</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2008.01.044</pub-id><pub-id pub-id-type="pmid">18280656</pub-id></mixed-citation></ref><ref id="B149"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>E. B.</given-names></name><name><surname>Blumstein</surname><given-names>S. E.</given-names></name><name><surname>Walsh</surname><given-names>E.</given-names></name><name><surname>Eliassen</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <article-title>Inferior frontal regions underlie the perception of phonetic category invariance</article-title>. <source>Psychol. Sci</source>. <volume>20</volume>, <fpage>895</fpage>&#x02013;<lpage>903</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02380.x</pub-id><pub-id pub-id-type="pmid">19515116</pub-id></mixed-citation></ref><ref id="B150"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>A. R.</given-names></name><name><surname>Beauchamp</surname><given-names>M. S.</given-names></name></person-group> (<year>2011</year>). <article-title>Dynamic changes in superior temporal sulcus connectivity during perception of noisy audiovisual speech</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>1704</fpage>&#x02013;<lpage>1714</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4853-10.2011</pub-id><pub-id pub-id-type="pmid">21289179</pub-id></mixed-citation></ref><ref id="B151"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>A. R.</given-names></name><name><surname>Beauchamp</surname><given-names>M. S.</given-names></name></person-group> (<year>2012</year>). <article-title>A neural basis for interindividual differences in the McGurk effect, a multisensory speech illusion</article-title>. <source>Neuroimage</source>
<volume>59</volume>, <fpage>781</fpage>&#x02013;<lpage>787</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.07.024</pub-id><pub-id pub-id-type="pmid">21787869</pub-id></mixed-citation></ref><ref id="B152"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishitani</surname><given-names>N.</given-names></name><name><surname>Hari</surname><given-names>R.</given-names></name></person-group> (<year>2002</year>). <article-title>Viewing lip forms: cortical dynamics</article-title>. <source>Neuron</source>
<volume>36</volume>, <fpage>1211</fpage>&#x02013;<lpage>1220</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(02)01089-9</pub-id><pub-id pub-id-type="pmid">12495633</pub-id></mixed-citation></ref><ref id="B153"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niziolek</surname><given-names>C. A.</given-names></name><name><surname>Guenther</surname><given-names>F. H.</given-names></name></person-group> (<year>2013</year>). <article-title>Vowel category boundaries enhance cortical and behavioral responses to speech feedback alterations</article-title>. <source>J. Neurosci</source>. <volume>33</volume>, <fpage>12090</fpage>&#x02013;<lpage>12098</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1008-13.2013</pub-id><pub-id pub-id-type="pmid">23864694</pub-id></mixed-citation></ref><ref id="B154"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J.</given-names></name><name><surname>Zimmermann</surname><given-names>J.</given-names></name><name><surname>Van Meter</surname><given-names>J.</given-names></name><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name></person-group> (<year>2007</year>). <article-title>Multiple stages of auditory speech perception reflected in event-related FMRI</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>2251</fpage>&#x02013;<lpage>2257</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhl133</pub-id><pub-id pub-id-type="pmid">17150986</pub-id></mixed-citation></ref><ref id="B155"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojanen</surname><given-names>V.</given-names></name><name><surname>M&#x000f6;tt&#x000f6;nen</surname><given-names>R.</given-names></name><name><surname>Pekkola</surname><given-names>J.</given-names></name><name><surname>Jaaskelainen</surname><given-names>I. P.</given-names></name><name><surname>Joensuu</surname><given-names>R.</given-names></name><name><surname>Autti</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2005</year>). <article-title>Processing of audiovisual speech in Broca's area</article-title>. <source>Neuroimage</source>
<volume>25</volume>, <fpage>333</fpage>&#x02013;<lpage>338</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.001</pub-id><pub-id pub-id-type="pmid">15784412</pub-id></mixed-citation></ref><ref id="B156"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname><given-names>K.</given-names></name><name><surname>Hickok</surname><given-names>G.</given-names></name></person-group> (<year>2009</year>). <article-title>Two cortical mechanisms support the integration of visual and auditory speech: a hypothesis and preliminary data</article-title>. <source>Neurosci. Lett</source>. <volume>452</volume>, <fpage>219</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2009.01.060</pub-id><pub-id pub-id-type="pmid">19348727</pub-id></mixed-citation></ref><ref id="B157"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname><given-names>K.</given-names></name><name><surname>Venezia</surname><given-names>J. H.</given-names></name><name><surname>Matchin</surname><given-names>W.</given-names></name><name><surname>Saberi</surname><given-names>K.</given-names></name><name><surname>Hickok</surname><given-names>G.</given-names></name></person-group> (<year>2013</year>). <article-title>An fMRI study of audiovisual speech perception reveals multisensory interactions in auditory cortex</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e68959</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0068959</pub-id><pub-id pub-id-type="pmid">23805332</pub-id></mixed-citation></ref><ref id="B158"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osnes</surname><given-names>B.</given-names></name><name><surname>Hugdahl</surname><given-names>K.</given-names></name><name><surname>Specht</surname><given-names>K.</given-names></name></person-group> (<year>2011</year>). <article-title>Effective connectivity analysis demonstrates involvement of premotor cortex during speech perception</article-title>. <source>Neuroimage</source>
<volume>54</volume>, <fpage>2437</fpage>&#x02013;<lpage>2445</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.078</pub-id><pub-id pub-id-type="pmid">20932914</pub-id></mixed-citation></ref><ref id="B159"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owens</surname><given-names>E.</given-names></name><name><surname>Blazek</surname><given-names>B.</given-names></name></person-group> (<year>1985</year>). <article-title>Visemes observed by hearing-impaired and normal hearing adult viewers</article-title>. <source>J. Speech Hear. Res</source>. <volume>28</volume>, <fpage>381</fpage>&#x02013;<lpage>393</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.2803.381</pub-id><pub-id pub-id-type="pmid">4046579</pub-id></mixed-citation></ref><ref id="B160"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulesu</surname><given-names>E.</given-names></name><name><surname>Perani</surname><given-names>D.</given-names></name><name><surname>Blasi</surname><given-names>V.</given-names></name><name><surname>Silani</surname><given-names>G.</given-names></name><name><surname>Borghese</surname><given-names>N. A.</given-names></name><name><surname>De Giovanni</surname><given-names>U.</given-names></name><etal/></person-group>. (<year>2003</year>). <article-title>A functional-anatomical model for lipreading</article-title>. <source>J. Neurophysiol</source>. <volume>90</volume>, <fpage>2005</fpage>&#x02013;<lpage>2013</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00926.2002</pub-id><pub-id pub-id-type="pmid">12750414</pub-id></mixed-citation></ref><ref id="B161"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pazo-Alvarez</surname><given-names>P.</given-names></name><name><surname>Cadaveira</surname><given-names>F.</given-names></name><name><surname>Amenedo</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>). <article-title>MMN in the visual modality: a review</article-title>. <source>Biol. Psychol</source>. <volume>63</volume>, <fpage>199</fpage>&#x02013;<lpage>236</lpage>. <pub-id pub-id-type="doi">10.1016/S0301-0511(03)00049-8</pub-id><pub-id pub-id-type="pmid">12853168</pub-id></mixed-citation></ref><ref id="B162"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pekkola</surname><given-names>J.</given-names></name><name><surname>Ojanen</surname><given-names>V.</given-names></name><name><surname>Autti</surname><given-names>T.</given-names></name><name><surname>Jaaskelainen</surname><given-names>I. P.</given-names></name><name><surname>Mottonen</surname><given-names>R.</given-names></name><name><surname>Tarkiainen</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2005</year>). <article-title>Primary auditory cortex activation by visual speech: an fMRI study at 3 T</article-title>. <source>Neuroreport</source>
<volume>16</volume>, <fpage>125</fpage>&#x02013;<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200502080-00010</pub-id><pub-id pub-id-type="pmid">15671860</pub-id></mixed-citation></ref><ref id="B163"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>C. J.</given-names></name><name><surname>Fallah</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Feature integration and object representations along the dorsal stream visual hierarchy</article-title>. <source>Front. Comput. Neurosci</source>. <volume>8</volume>:<issue>84</issue>. <pub-id pub-id-type="doi">10.3389/fncom.2014.00084</pub-id><pub-id pub-id-type="pmid">25140147</pub-id></mixed-citation></ref><ref id="B164"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D.</given-names></name><name><surname>Dilks</surname><given-names>D. D.</given-names></name><name><surname>Saxe</surname><given-names>R. R.</given-names></name><name><surname>Triantafyllou</surname><given-names>C.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2011</year>). <article-title>Differential selectivity for dynamic versus static information in face-selective cortical regions</article-title>. <source>Neuroimage</source>
<volume>56</volume>, <fpage>2356</fpage>&#x02013;<lpage>2363</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.067</pub-id><pub-id pub-id-type="pmid">21473921</pub-id></mixed-citation></ref><ref id="B165"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponton</surname><given-names>C. W.</given-names></name><name><surname>Bernstein</surname><given-names>L. E.</given-names></name><name><surname>Auer</surname><given-names>E. T.</given-names><suffix>Jr.</suffix></name></person-group> (<year>2009</year>). <article-title>Mismatch negativity with visual-only and audiovisual speech</article-title>. <source>Brain Topogr</source>. <volume>21</volume>, <fpage>207</fpage>&#x02013;<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-009-0094-5</pub-id><pub-id pub-id-type="pmid">19404730</pub-id></mixed-citation></ref><ref id="B166"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>A.</given-names></name><name><surname>Allison</surname><given-names>T.</given-names></name></person-group> (<year>2000</year>). <article-title>ERPs evoked by viewing facial movements</article-title>. <source>Cogn. Neuropsychol</source>. <volume>17</volume>, <fpage>221</fpage>&#x02013;<lpage>239</lpage>. <pub-id pub-id-type="doi">10.1080/026432900380580</pub-id><pub-id pub-id-type="pmid">20945181</pub-id></mixed-citation></ref><ref id="B167"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A.</given-names></name><name><surname>Syngeniotis</surname><given-names>A.</given-names></name><name><surname>Thompson</surname><given-names>J. C.</given-names></name><name><surname>Abbott</surname><given-names>D. F.</given-names></name><name><surname>Wheaton</surname><given-names>K. J.</given-names></name><name><surname>Castiello</surname><given-names>U.</given-names></name></person-group> (<year>2003</year>). <article-title>The human temporal lobe integrates facial form and motion: evidence from fMRI and ERP studies</article-title>. <source>Neuroimage</source>
<volume>19</volume>, <fpage>861</fpage>&#x02013;<lpage>869</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00189-7</pub-id><pub-id pub-id-type="pmid">12880814</pub-id></mixed-citation></ref><ref id="B168"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pugh</surname><given-names>K. R.</given-names></name><name><surname>Mencl</surname><given-names>W. E.</given-names></name><name><surname>Jenner</surname><given-names>A. R.</given-names></name><name><surname>Katz</surname><given-names>L.</given-names></name><name><surname>Frost</surname><given-names>S. J.</given-names></name><name><surname>Lee</surname><given-names>J. R.</given-names></name><etal/></person-group>. (<year>2000</year>). <article-title>Functional neuroimaging studies of reading and reading disability (developmental dyslexia)</article-title>. <source>Ment. Retard. Dev. Disabil. Res. Rev</source>. <volume>6</volume>, <fpage>207</fpage>&#x02013;<lpage>213</lpage>. <pub-id pub-id-type="doi">10.1002/1098-2779(2000)6:3&#x0003c;207::aid-mrdd8&#x0003e;3.0.co;2-p</pub-id><pub-id pub-id-type="pmid">10982498</pub-id></mixed-citation></ref><ref id="B169"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raizada</surname><given-names>R. D.</given-names></name><name><surname>Poldrack</surname><given-names>R. A.</given-names></name></person-group> (<year>2007</year>). <article-title>Selective amplification of stimulus differences during categorical processing of speech</article-title>. <source>Neuron</source>
<volume>56</volume>, <fpage>726</fpage>&#x02013;<lpage>740</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.11.001</pub-id><pub-id pub-id-type="pmid">18031688</pub-id></mixed-citation></ref><ref id="B170"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raphael</surname><given-names>L. J.</given-names></name></person-group> (<year>1971</year>). <article-title>Preceding vowel duration as a cue to the perception of the voicing characteristic of word-final consonants in American English</article-title>. <source>J. Acous. Soc. Am</source>. <volume>51</volume>, <fpage>1296</fpage>&#x02013;<lpage>1303</lpage>. <pub-id pub-id-type="pmid">5032946</pub-id></mixed-citation></ref><ref id="B171"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name></person-group> (<year>1998</year>). <article-title>Cortical processing of complex sounds</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>8</volume>, <fpage>516</fpage>&#x02013;<lpage>521</lpage>. <pub-id pub-id-type="doi">10.1016/S0959-4388(98)80040-8</pub-id><pub-id pub-id-type="pmid">9751652</pub-id></mixed-citation></ref><ref id="B172"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name><name><surname>Scott</surname><given-names>S. K.</given-names></name></person-group> (<year>2009</year>). <article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title>. <source>Nat. Neurosci</source>. <volume>12</volume>, <fpage>718</fpage>&#x02013;<lpage>724</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2331</pub-id><pub-id pub-id-type="pmid">19471271</pub-id></mixed-citation></ref><ref id="B173"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name><name><surname>Tian</surname><given-names>B.</given-names></name></person-group> (<year>2000</year>). <article-title>Mechanisms and streams for processing of &#x0201c;what&#x0201d; and &#x0201c;where&#x0201d; in auditory cortex</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>97</volume>, <fpage>11800</fpage>&#x02013;<lpage>11806</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.97.22.11800</pub-id><pub-id pub-id-type="pmid">11050212</pub-id></mixed-citation></ref><ref id="B174"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name><name><surname>Tian</surname><given-names>B.</given-names></name><name><surname>Hauser</surname><given-names>M.</given-names></name></person-group> (<year>1995</year>). <article-title>Processing of complex sounds in the macaque nonprimary auditory cortex</article-title>. <source>Science</source>
<volume>268</volume>, <fpage>111</fpage>&#x02013;<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1126/science.7701330</pub-id><pub-id pub-id-type="pmid">7701330</pub-id></mixed-citation></ref><ref id="B175"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Risberg</surname><given-names>A.</given-names></name><name><surname>Lubker</surname><given-names>J. L.</given-names></name></person-group> (<year>1978</year>). <article-title>Prosody and speechreading</article-title>, in <source>Quarterly Progress and Status Report</source>, <volume>Vol. 4</volume> (<publisher-loc>Stockholm</publisher-loc>: <publisher-name>Speech Transmission Laboratory of the Royal Institute of Technology</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>16</lpage>.</mixed-citation></ref><ref id="B176"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G.</given-names></name><name><surname>Arbib</surname><given-names>M. A.</given-names></name></person-group> (<year>1998</year>). <article-title>Language within our grasp</article-title>. <source>Trends Neurosci</source>. <volume>21</volume>, <fpage>188</fpage>&#x02013;<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1016/S0166-2236(98)01260-0</pub-id><pub-id pub-id-type="pmid">9610880</pub-id></mixed-citation></ref><ref id="B177"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G.</given-names></name><name><surname>Craighero</surname><given-names>L.</given-names></name></person-group> (<year>2004</year>). <article-title>The mirror-neuron system</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>27</volume>, <fpage>169</fpage>&#x02013;<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144230</pub-id><pub-id pub-id-type="pmid">15217330</pub-id></mixed-citation></ref><ref id="B178"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romanski</surname><given-names>L. M.</given-names></name><name><surname>Tian</surname><given-names>B.</given-names></name><name><surname>Fritz</surname><given-names>J.</given-names></name><name><surname>Mishkin</surname><given-names>M.</given-names></name><name><surname>Goldman-Rakic</surname><given-names>P. S.</given-names></name><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name></person-group> (<year>1999</year>). <article-title>Dual streams of auditory afferents target multiple domains in the primate prefrontal cortex</article-title>. <source>Nat. Neurosci</source>. <volume>2</volume>, <fpage>1131</fpage>&#x02013;<lpage>1136</lpage>. <pub-id pub-id-type="doi">10.1038/16056</pub-id><pub-id pub-id-type="pmid">10570492</pub-id></mixed-citation></ref><ref id="B179"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>L. D.</given-names></name><name><surname>Johnson</surname><given-names>J. A.</given-names></name><name><surname>Saldana</surname><given-names>H. M.</given-names></name></person-group> (<year>1996</year>). <article-title>Point-light facial displays enhance comprehension of speech in noise</article-title>. <source>J. Speech Hear. Res</source>. <volume>39</volume>, <fpage>1159</fpage>&#x02013;<lpage>1170</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.3906.1159</pub-id><pub-id pub-id-type="pmid">8959601</pub-id></mixed-citation></ref><ref id="B180"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>L. D.</given-names></name><name><surname>Saldana</surname><given-names>H. M.</given-names></name></person-group> (<year>1996</year>). <article-title>An audiovisual test of kinematic primitives for visual speech perception</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>22</volume>, <fpage>318</fpage>&#x02013;<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.22.2.318</pub-id><pub-id pub-id-type="pmid">8934846</pub-id></mixed-citation></ref><ref id="B181"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouger</surname><given-names>J.</given-names></name><name><surname>Lagleyre</surname><given-names>S.</given-names></name><name><surname>Fraysse</surname><given-names>B.</given-names></name><name><surname>Deneve</surname><given-names>S.</given-names></name><name><surname>Deguine</surname><given-names>O.</given-names></name><name><surname>Barone</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>Evidence that cochlear-implanted deaf patients are better multisensory integrators</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>104</volume>, <fpage>7295</fpage>&#x02013;<lpage>7300</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0609419104</pub-id><pub-id pub-id-type="pmid">17404220</pub-id></mixed-citation></ref><ref id="B182"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saint-Amour</surname><given-names>D.</given-names></name><name><surname>Sanctis</surname><given-names>P. D.</given-names></name><name><surname>Molholm</surname><given-names>S.</given-names></name><name><surname>Ritter</surname><given-names>W.</given-names></name><name><surname>Foxe</surname><given-names>J. J.</given-names></name></person-group> (<year>2007</year>). <article-title>Seeing voices: high-density electrical mapping and source-analysis of the multisensory mismatch negativity evoked during the McGurk illusion</article-title>. <source>Neuropsychologia</source>
<volume>45</volume>, <fpage>587</fpage>&#x02013;<lpage>597</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.03.036</pub-id><pub-id pub-id-type="pmid">16757004</pub-id></mixed-citation></ref><ref id="B183"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sams</surname><given-names>M.</given-names></name><name><surname>Aulanko</surname><given-names>R.</given-names></name><name><surname>H&#x000e4;m&#x000e4;l&#x000e4;inen</surname><given-names>M.</given-names></name><name><surname>Hari</surname><given-names>R.</given-names></name><name><surname>Lounasmaa</surname><given-names>O. V.</given-names></name><name><surname>Lu</surname><given-names>S. T.</given-names></name><etal/></person-group>. (<year>1991</year>). <article-title>Seeing speech: visual information from lip movements modifies activity in the human auditory cortex</article-title>. <source>Neurosci. Lett</source>. <volume>127</volume>, <fpage>141</fpage>&#x02013;<lpage>145</lpage>. <pub-id pub-id-type="doi">10.1016/0304-3940(91)90914-F</pub-id><pub-id pub-id-type="pmid">1881611</pub-id></mixed-citation></ref><ref id="B184"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santi</surname><given-names>A.</given-names></name><name><surname>Servos</surname><given-names>P.</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name><name><surname>Kuratate</surname><given-names>T.</given-names></name><name><surname>Munhall</surname><given-names>K.</given-names></name></person-group> (<year>2003</year>). <article-title>Perceiving biological motion: dissociating visible speech from walking</article-title>. <source>J. Cogn. Neurosci</source>. <volume>15</volume>, <fpage>800</fpage>&#x02013;<lpage>809</lpage>. <pub-id pub-id-type="doi">10.1162/089892903322370726</pub-id><pub-id pub-id-type="pmid">14511533</pub-id></mixed-citation></ref><ref id="B185"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saur</surname><given-names>D.</given-names></name><name><surname>Kreher</surname><given-names>B. W.</given-names></name><name><surname>Schnell</surname><given-names>S.</given-names></name><name><surname>Kummerer</surname><given-names>D.</given-names></name><name><surname>Kellmeyer</surname><given-names>P.</given-names></name><name><surname>Vry</surname><given-names>M. S.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Ventral and dorsal pathways for language</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>105</volume>, <fpage>18035</fpage>&#x02013;<lpage>18040</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0805234105</pub-id><pub-id pub-id-type="pmid">19004769</pub-id></mixed-citation></ref><ref id="B186"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Scarborough</surname><given-names>R.</given-names></name><name><surname>Keating</surname><given-names>P.</given-names></name><name><surname>Baroni</surname><given-names>M.</given-names></name><name><surname>Cho</surname><given-names>T.</given-names></name><name><surname>Mattys</surname><given-names>S.</given-names></name><name><surname>Alwan</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2007</year>). <source>Optical Cues to the Visual Perception of Lexical and Phrasal Stress in English</source>. Working Papers in Phonetics, <publisher-name>University of California</publisher-name>, <publisher-loc>Los Angeles, CA</publisher-loc> Available online at: <ext-link ext-link-type="uri" xlink:href="http://escholarship.org/uc/item/4gk6008p">http://escholarship.org/uc/item/4gk6008p</ext-link>.</mixed-citation></ref><ref id="B187"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>C. E.</given-names></name><name><surname>Foxe</surname><given-names>J. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Multisensory contributions to low-level, &#x02018;unisensory&#x02019; processing</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>15</volume>, <fpage>454</fpage>&#x02013;<lpage>458</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2005.06.008</pub-id><pub-id pub-id-type="pmid">16019202</pub-id></mixed-citation></ref><ref id="B188"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>C. E.</given-names></name><name><surname>Lakatos</surname><given-names>P.</given-names></name><name><surname>Kajikawa</surname><given-names>Y.</given-names></name><name><surname>Partan</surname><given-names>S.</given-names></name><name><surname>Puce</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>Neuronal oscillations and visual amplification of speech</article-title>. <source>Trends Cogn. Sci. (Regul. Ed)</source>. <volume>12</volume>, <fpage>106</fpage>&#x02013;<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2008.01.002</pub-id><pub-id pub-id-type="pmid">18280772</pub-id></mixed-citation></ref><ref id="B189"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>J.</given-names></name><name><surname>Brockhaus</surname><given-names>M.</given-names></name><name><surname>Bulthoff</surname><given-names>H. H.</given-names></name><name><surname>Pilz</surname><given-names>K. S.</given-names></name></person-group> (<year>2013</year>). <article-title>What the human brain likes about facial motion</article-title>. <source>Cereb. Cortex</source>
<volume>23</volume>, <fpage>1167</fpage>&#x02013;<lpage>1178</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhs106</pub-id><pub-id pub-id-type="pmid">22535907</pub-id></mixed-citation></ref><ref id="B190"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>S. K.</given-names></name><name><surname>Blank</surname><given-names>C. C.</given-names></name><name><surname>Rosen</surname><given-names>S.</given-names></name><name><surname>Wise</surname><given-names>R. J.</given-names></name></person-group> (<year>2000</year>). <article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title>. <source>Brain</source>
<volume>123(Pt 12)</volume>, <fpage>2400</fpage>&#x02013;<lpage>2406</lpage>. <pub-id pub-id-type="doi">10.1093/brain/123.12.2400</pub-id><pub-id pub-id-type="pmid">11099443</pub-id></mixed-citation></ref><ref id="B191"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekiyama</surname><given-names>K.</given-names></name><name><surname>Kanno</surname><given-names>I.</given-names></name><name><surname>Miura</surname><given-names>S.</given-names></name><name><surname>Sugita</surname><given-names>Y.</given-names></name></person-group> (<year>2003</year>). <article-title>Auditory-visual speech perception examined by fMRI and PET</article-title>. <source>Neurosci. Res</source>. <volume>47</volume>, <fpage>277</fpage>&#x02013;<lpage>287</lpage>. <pub-id pub-id-type="doi">10.1016/S0168-0102(03)00214-1</pub-id><pub-id pub-id-type="pmid">14568109</pub-id></mixed-citation></ref><ref id="B192"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seltzer</surname><given-names>B.</given-names></name><name><surname>Pandya</surname><given-names>D. N.</given-names></name></person-group> (<year>1994</year>). <article-title>Parietal, temporal, and occipital projections to cortex of the superior temporal sulcus in the rhesus monkey: a retrograde tracer study</article-title>. <source>J. Comp. Neurol</source>. <volume>343</volume>, <fpage>445</fpage>&#x02013;<lpage>463</lpage>. <pub-id pub-id-type="doi">10.1002/cne.903430308</pub-id><pub-id pub-id-type="pmid">8027452</pub-id></mixed-citation></ref><ref id="B193"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>R. N.</given-names></name><name><surname>Chipman</surname><given-names>S.</given-names></name></person-group> (<year>1970</year>). <article-title>Second-order isomorphism of internal representations: shapes of states</article-title>. <source>Cogn. Psychol</source>. <volume>1</volume>, <fpage>1</fpage>&#x02013;<lpage>17</lpage>
<pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id></mixed-citation></ref><ref id="B194"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>J. I.</given-names></name><name><surname>Goldin-Meadow</surname><given-names>S.</given-names></name><name><surname>Nusbaum</surname><given-names>H. C.</given-names></name><name><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2007a</year>). <article-title>Speech-associated gestures, Broca's area, and the human mirror system</article-title>. <source>Brain Lang</source>. <volume>101</volume>, <fpage>260</fpage>&#x02013;<lpage>277</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2007.02.008</pub-id><pub-id pub-id-type="pmid">17533001</pub-id></mixed-citation></ref><ref id="B195"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>J. I.</given-names></name><name><surname>Nusbaum</surname><given-names>H. C.</given-names></name><name><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2005</year>). <article-title>Listening to talking faces: motor cortical activation during speech perception</article-title>. <source>Neuroimage</source>
<volume>25</volume>, <fpage>76</fpage>&#x02013;<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.11.006</pub-id><pub-id pub-id-type="pmid">15734345</pub-id></mixed-citation></ref><ref id="B196"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>J. I.</given-names></name><name><surname>van Wassenhove</surname><given-names>V.</given-names></name><name><surname>Nusbaum</surname><given-names>H. C.</given-names></name><name><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2007b</year>). <article-title>Hearing lips and seeing voices: how cortical areas supporting speech production mediate audiovisual speech perception</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>2387</fpage>&#x02013;<lpage>2399</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhl147</pub-id><pub-id pub-id-type="pmid">17218482</pub-id></mixed-citation></ref><ref id="B197"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smiley</surname><given-names>J. F.</given-names></name><name><surname>Hackett</surname><given-names>T. A.</given-names></name><name><surname>Ulbert</surname><given-names>I.</given-names></name><name><surname>Karmas</surname><given-names>G.</given-names></name><name><surname>Lakatos</surname><given-names>P.</given-names></name><name><surname>Javitt</surname><given-names>D. C.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Multisensory convergence in auditory cortex, I. Cortical connections of the caudal superior temporal plane in macaque monkeys</article-title>. <source>J. Comp. Neurol</source>. <volume>502</volume>, <fpage>894</fpage>&#x02013;<lpage>923</lpage>. <pub-id pub-id-type="doi">10.1002/cne.21325</pub-id><pub-id pub-id-type="pmid">17447261</pub-id></mixed-citation></ref><ref id="B198"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>J. J.</given-names></name><name><surname>Lee</surname><given-names>H. J.</given-names></name><name><surname>Kang</surname><given-names>H.</given-names></name><name><surname>Lee</surname><given-names>D. S.</given-names></name><name><surname>Chang</surname><given-names>S. O.</given-names></name><name><surname>Oh</surname><given-names>S. H.</given-names></name></person-group> (<year>2014</year>). <article-title>Effects of congruent and incongruent visual cues on speech perception and brain activity in cochlear implant users</article-title>. <source>Brain Struct. Funct</source>. [Epub ahead of print]. <pub-id pub-id-type="doi">10.1007/s00429-013-0704-6</pub-id><pub-id pub-id-type="pmid">24402676</pub-id></mixed-citation></ref><ref id="B199"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>B. E.</given-names></name><name><surname>Burr</surname><given-names>D.</given-names></name><name><surname>Constantinidis</surname><given-names>C.</given-names></name><name><surname>Laurienti</surname><given-names>P. J.</given-names></name><name><surname>Meredith</surname><given-names>A. M.</given-names></name><name><surname>Perrault</surname><given-names>T. J.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Semantic confusion regarding the development of multisensory integration: a practical solution</article-title>. <source>Eur. J. Neurosci</source>. <volume>31</volume>, <fpage>1713</fpage>&#x02013;<lpage>1720</lpage>. <pub-id pub-id-type="doi">10.1111/j.1460-9568.2010.07206.x</pub-id><pub-id pub-id-type="pmid">20584174</pub-id></mixed-citation></ref><ref id="B200"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>B. E.</given-names></name><name><surname>Meredith</surname><given-names>A.</given-names></name></person-group> (<year>1993</year>). <source>The Merging of the Senses</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT</publisher-name>.</mixed-citation></ref><ref id="B201"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname><given-names>M.</given-names></name><name><surname>Nourski</surname><given-names>K. V.</given-names></name><name><surname>Kawasaki</surname><given-names>H.</given-names></name><name><surname>Oya</surname><given-names>H.</given-names></name><name><surname>Brugge</surname><given-names>J. F.</given-names></name><name><surname>Howard</surname><given-names>M. A.</given-names><suffix>3rd.</suffix></name></person-group> (<year>2011</year>). <article-title>Intracranial study of speech-elicited activity on the human posterolateral superior temporal gyrus</article-title>. <source>Cereb. Cortex</source>
<volume>21</volume>, <fpage>2332</fpage>&#x02013;<lpage>2347</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr014</pub-id><pub-id pub-id-type="pmid">21368087</pub-id></mixed-citation></ref><ref id="B202"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>H. E.</given-names></name><name><surname>Wickesberg</surname><given-names>R. E.</given-names></name></person-group> (<year>2002</year>). <article-title>Representation of whispered word-final stop consonants in the auditory nerve</article-title>. <source>Hear. Res</source>. <volume>173</volume>, <fpage>119</fpage>&#x02013;<lpage>133</lpage>. <pub-id pub-id-type="doi">10.1016/S0378-5955(02)00608-1</pub-id><pub-id pub-id-type="pmid">12372641</pub-id></mixed-citation></ref><ref id="B203"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>K. N.</given-names></name></person-group> (<year>1981</year>). <article-title>Constraints imposed by the auditory system on the properties used to classify speech sounds: Data from phonology, acoustics, and psychoacoustics</article-title>, in <source>The Cognitive Representation of Speech</source>, eds <person-group person-group-type="editor"><name><surname>Myers</surname><given-names>T.</given-names></name><name><surname>Laver</surname><given-names>J.</given-names></name><name><surname>Anderson</surname><given-names>J.</given-names></name></person-group> (<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>North Holland; Elsevier Science Ltd.</publisher-name>), <fpage>61</fpage>&#x02013;<lpage>74</lpage>.</mixed-citation></ref><ref id="B204"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>K. N.</given-names></name></person-group> (<year>1998</year>). <source>Acoustic Phonetics</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref><ref id="B205"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>R. A.</given-names></name><name><surname>Bushmakin</surname><given-names>M.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>Wallace</surname><given-names>M. T.</given-names></name><name><surname>Puce</surname><given-names>A.</given-names></name><name><surname>James</surname><given-names>T. W.</given-names></name></person-group> (<year>2012</year>). <article-title>Inverse effectiveness and multisensory interactions in visual event-related potentials with audiovisual speech</article-title>. <source>Brain Topogr</source>. <volume>25</volume>, <fpage>308</fpage>&#x02013;<lpage>326</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-012-0220-7</pub-id><pub-id pub-id-type="pmid">22367585</pub-id></mixed-citation></ref><ref id="B206"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>R. A.</given-names></name><name><surname>James</surname><given-names>T. W.</given-names></name></person-group> (<year>2009</year>). <article-title>Audiovisual integration in human superior temporal sulcus: inverse effectiveness and the neural processing of speech and object recognition</article-title>. <source>Neuroimage</source>
<volume>44</volume>, <fpage>1210</fpage>&#x02013;<lpage>1223</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.034</pub-id><pub-id pub-id-type="pmid">18973818</pub-id></mixed-citation></ref><ref id="B207"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strand</surname><given-names>J. F.</given-names></name><name><surname>Sommers</surname><given-names>M. S.</given-names></name></person-group> (<year>2011</year>). <article-title>Sizing up the competition: quantifying the influence of the mental lexicon on auditory and visual spoken word recognition</article-title>. <source>J. Acous. Soc. Am</source>. <volume>130</volume>, <fpage>1663</fpage>&#x02013;<lpage>1672</lpage>. <pub-id pub-id-type="doi">10.1121/1.3613930</pub-id><pub-id pub-id-type="pmid">21895103</pub-id></mixed-citation></ref><ref id="B208"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumby</surname><given-names>W. H.</given-names></name><name><surname>Pollack</surname><given-names>I.</given-names></name></person-group> (<year>1954</year>). <article-title>Visual contribution to speech intelligibility in noise</article-title>. <source>J. Acous. Soc. Am</source>. <volume>26</volume>, <fpage>212</fpage>&#x02013;<lpage>215</lpage>
<pub-id pub-id-type="doi">10.1121/1.1907309</pub-id></mixed-citation></ref><ref id="B209"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>A. Q.</given-names></name></person-group> (<year>1987</year>). <article-title>Some preliminaries to a comprehensive account of audio-visual speech perception</article-title>, in <source>Hearing by Eye: The Psychology of Lip-Reading</source>, eds <person-group person-group-type="editor"><name><surname>Dodd</surname><given-names>B.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name></person-group> (<publisher-loc>London</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates, Inc.</publisher-name>), <fpage>3</fpage>&#x02013;<lpage>52</lpage>.</mixed-citation></ref><ref id="B210"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>J. C.</given-names></name><name><surname>Hardee</surname><given-names>J. E.</given-names></name><name><surname>Panayiotou</surname><given-names>A.</given-names></name><name><surname>Crewther</surname><given-names>D.</given-names></name><name><surname>Puce</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). <article-title>Common and distinct brain activation to viewing dynamic sequences of face and hand movements</article-title>. <source>Neuroimage</source>
<volume>37</volume>, <fpage>966</fpage>&#x02013;<lpage>973</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.05.058</pub-id><pub-id pub-id-type="pmid">17616403</pub-id></mixed-citation></ref><ref id="B211"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>B.</given-names></name><name><surname>Reser</surname><given-names>D.</given-names></name><name><surname>Durham</surname><given-names>A.</given-names></name><name><surname>Kustov</surname><given-names>A.</given-names></name><name><surname>Rauschecker</surname><given-names>J. P.</given-names></name></person-group> (<year>2001</year>). <article-title>Functional specialization in rhesus monkey auditory cortex</article-title>. <source>Science</source>
<volume>292</volume>, <fpage>290</fpage>&#x02013;<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1126/science.1058911</pub-id><pub-id pub-id-type="pmid">11303104</pub-id></mixed-citation></ref><ref id="B212"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tourville</surname><given-names>J. A.</given-names></name><name><surname>Reilly</surname><given-names>K. J.</given-names></name><name><surname>Guenther</surname><given-names>F. H.</given-names></name></person-group> (<year>2008</year>). <article-title>Neural mechanisms underlying auditory feedback control of speech</article-title>. <source>Neuroimage</source>
<volume>39</volume>, <fpage>1429</fpage>&#x02013;<lpage>1443</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.054</pub-id><pub-id pub-id-type="pmid">18035557</pub-id></mixed-citation></ref><ref id="B213"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turkeltaub</surname><given-names>P. E.</given-names></name><name><surname>Coslett</surname><given-names>H. B.</given-names></name></person-group> (<year>2010</year>). <article-title>Localization of sublexical speech perception components</article-title>. <source>Brain Lang</source>. <volume>114</volume>, <fpage>1</fpage>&#x02013;<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2010.03.008</pub-id><pub-id pub-id-type="pmid">20413149</pub-id></mixed-citation></ref><ref id="B215"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tye-Murray</surname><given-names>N.</given-names></name><name><surname>Hale</surname><given-names>S.</given-names></name><name><surname>Spehar</surname><given-names>B.</given-names></name><name><surname>Myerson</surname><given-names>J.</given-names></name><name><surname>Sommers</surname><given-names>M. S.</given-names></name></person-group> (<year>2014</year>). <article-title>Lipreading in school-age children: the roles of age, hearing status, and cognitive ability</article-title>. <source>J. Speech Lang. Hear. Res</source>. <volume>57</volume>, <fpage>556</fpage>&#x02013;<lpage>565</lpage>. <pub-id pub-id-type="doi">10.1044/2013_JSLHR-H-12-0273</pub-id><pub-id pub-id-type="pmid">24129010</pub-id></mixed-citation></ref><ref id="B216"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>L. G.</given-names></name><name><surname>Courtney</surname><given-names>S. M.</given-names></name><name><surname>Haxby</surname><given-names>J. V.</given-names></name></person-group> (<year>1998</year>). <article-title>A neural system for human visual working memory</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>95</volume>, <fpage>883</fpage>&#x02013;<lpage>890</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.95.3.883</pub-id><pub-id pub-id-type="pmid">9448255</pub-id></mixed-citation></ref><ref id="B217"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>L. G.</given-names></name><name><surname>Haxby</surname><given-names>J. V.</given-names></name></person-group> (<year>1994</year>). <article-title>&#x0201c;What&#x0201d; and &#x0201c;where&#x0201d; in the human brain</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>4</volume>, <fpage>157</fpage>&#x02013;<lpage>165</lpage>. <pub-id pub-id-type="doi">10.1016/0959-4388(94)90066-3</pub-id><pub-id pub-id-type="pmid">8038571</pub-id></mixed-citation></ref><ref id="B218"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>L. G.</given-names></name><name><surname>Mishkin</surname><given-names>M.</given-names></name></person-group> (<year>1982</year>). <article-title>Two cortical visual systems</article-title>, in <source>Analysis of Visual Behavior</source>, ed <person-group person-group-type="editor"><name><surname>Ingle</surname><given-names>D. J.</given-names></name></person-group> (<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>), <fpage>549</fpage>&#x02013;<lpage>586</lpage>.</mixed-citation></ref><ref id="B219"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Utley</surname><given-names>J.</given-names></name></person-group> (<year>1946</year>). <article-title>A test of lip reading ability</article-title>. <source>J. Speech Lang. Hear. Disord</source>. <volume>11</volume>, <fpage>109</fpage>&#x02013;<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1044/jshd.1102.109</pub-id><pub-id pub-id-type="pmid">20986556</pub-id></mixed-citation></ref><ref id="B220"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Son</surname><given-names>N.</given-names></name><name><surname>Huiskamp</surname><given-names>T. M. I.</given-names></name><name><surname>Bosman</surname><given-names>A. J.</given-names></name><name><surname>Smoorenburg</surname><given-names>G. F.</given-names></name></person-group> (<year>1994</year>). <article-title>Viseme classifications of Dutch consonants and vowels</article-title>. <source>J. Acous. Soc. Am</source>. <volume>96</volume>, <fpage>1341</fpage>&#x02013;<lpage>1355</lpage>
<pub-id pub-id-type="doi">10.1121/1.411324</pub-id></mixed-citation></ref><ref id="B221"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venezia</surname><given-names>J. H.</given-names></name><name><surname>Saberi</surname><given-names>K.</given-names></name><name><surname>Chubb</surname><given-names>C.</given-names></name><name><surname>Hickok</surname><given-names>G.</given-names></name></person-group> (<year>2012</year>). <article-title>Response bias modulates the speech motor system during syllable discrimination</article-title>. <source>Front. Psychol</source>. <volume>3</volume>:<issue>157</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2012.00157</pub-id><pub-id pub-id-type="pmid">22723787</pub-id></mixed-citation></ref><ref id="B222"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von der Malsburg</surname><given-names>C.</given-names></name></person-group> (<year>1995</year>). <article-title>Binding in models of perception and brain function</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>5</volume>, <fpage>520</fpage>&#x02013;<lpage>526</lpage>. <pub-id pub-id-type="doi">10.1016/0959-4388(95)80014-X</pub-id><pub-id pub-id-type="pmid">7488855</pub-id></mixed-citation></ref><ref id="B223"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walden</surname><given-names>B. E.</given-names></name><name><surname>Prosek</surname><given-names>R. A.</given-names></name><name><surname>Montgomery</surname><given-names>A. A.</given-names></name><name><surname>Scherr</surname><given-names>C. K.</given-names></name><name><surname>Jones</surname><given-names>C. J.</given-names></name></person-group> (<year>1977</year>). <article-title>Effects of training on the visual recognition of consonants</article-title>. <source>J. Speech Hear. Res</source>. <volume>20</volume>, <fpage>130</fpage>&#x02013;<lpage>145</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.2001.130</pub-id><pub-id pub-id-type="pmid">846196</pub-id></mixed-citation></ref><ref id="B224"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weikum</surname><given-names>W. M.</given-names></name><name><surname>Vouloumanos</surname><given-names>A.</given-names></name><name><surname>Navarra</surname><given-names>J.</given-names></name><name><surname>Soto-Faraco</surname><given-names>S.</given-names></name><name><surname>Sebastian-Galles</surname><given-names>N.</given-names></name><name><surname>Werker</surname><given-names>J. F.</given-names></name></person-group> (<year>2007</year>). <article-title>Visual language discrimination in infancy</article-title>. <source>Science</source>
<volume>316</volume>, <fpage>1159</fpage>. <pub-id pub-id-type="doi">10.1126/science.1137686</pub-id><pub-id pub-id-type="pmid">17525331</pub-id></mixed-citation></ref><ref id="B225"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>K. S.</given-names></name><name><surname>Grill-Spector</surname><given-names>K.</given-names></name></person-group> (<year>2013</year>). <article-title>Neural representations of faces and limbs neighbor in human high-level visual cortex: evidence for a new organization principle</article-title>. <source>Psychol. Res</source>. <volume>77</volume>, <fpage>74</fpage>&#x02013;<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1007/s00426-011-0392-x</pub-id><pub-id pub-id-type="pmid">22139022</pub-id></mixed-citation></ref><ref id="B226"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>F. A. W.</given-names></name><name><surname>Scalaidhe</surname><given-names>S. P. O.</given-names></name><name><surname>Goldman-Rakic</surname><given-names>P. S.</given-names></name></person-group> (<year>1993</year>). <article-title>Dissociation of object and spatial processing domains in primate prefrontal cortex</article-title>. <source>Science</source>
<volume>260</volume>, <fpage>1955</fpage>&#x02013;<lpage>1958</lpage>. <pub-id pub-id-type="doi">10.1126/science.8316836</pub-id><pub-id pub-id-type="pmid">8316836</pub-id></mixed-citation></ref><ref id="B227"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>S. M.</given-names></name><name><surname>Iacoboni</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Neural responses to non-native phonemes varying in producibility: evidence for the sensorimotor nature of speech perception</article-title>. <source>Neuroimage</source>
<volume>33</volume>, <fpage>316</fpage>&#x02013;<lpage>325</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.032</pub-id><pub-id pub-id-type="pmid">16919478</pub-id></mixed-citation></ref><ref id="B227a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>S. M.</given-names></name><name><surname>Saygin</surname><given-names>A. P.</given-names></name><name><surname>Sereno</surname><given-names>M. I.</given-names></name><name><surname>Iacoboni</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Listening to speech activates motor areas involved in speech production</article-title>. <source>Nat. Neurosci</source>. <volume>7</volume>, <fpage>701</fpage>&#x02013;<lpage>702</lpage>. <pub-id pub-id-type="doi">10.1038/nn1263</pub-id><pub-id pub-id-type="pmid">15184903</pub-id></mixed-citation></ref><ref id="B228"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>I.</given-names></name><name><surname>Czigler</surname><given-names>I.</given-names></name></person-group> (<year>2012</year>). <article-title>Evidence from auditory and visual event-related potential (ERP) studies of deviance detection (MMN and vMMN) linking predictive coding theories and perceptual object representations</article-title>. <source>Int. J. Psychophysiol</source>. <volume>83</volume>, <fpage>132</fpage>&#x02013;<lpage>143</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2011.10.001</pub-id><pub-id pub-id-type="pmid">22047947</pub-id></mixed-citation></ref><ref id="B229"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wise</surname><given-names>R. J.</given-names></name><name><surname>Scott</surname><given-names>S. K.</given-names></name><name><surname>Blank</surname><given-names>S. C.</given-names></name><name><surname>Mummery</surname><given-names>C. J.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Warburton</surname><given-names>E. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Separate neural subsystems within &#x02018;Wernicke&#x02019;s area'</article-title>. <source>Brain</source>
<volume>124(Pt 1)</volume>, <fpage>83</fpage>&#x02013;<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1093/brain/124.1.83</pub-id><pub-id pub-id-type="pmid">11133789</pub-id></mixed-citation></ref><ref id="B230"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodward</surname><given-names>M. F.</given-names></name><name><surname>Barber</surname><given-names>C. G.</given-names></name></person-group> (<year>1960</year>). <article-title>Phoneme perception in lipreading</article-title>. <source>J. Speech Hear. Res</source>. <volume>3</volume>, <fpage>212</fpage>&#x02013;<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.0303.212</pub-id><pub-id pub-id-type="pmid">13845910</pub-id></mixed-citation></ref><ref id="B231"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>T. M.</given-names></name><name><surname>Pelphrey</surname><given-names>K. A.</given-names></name><name><surname>Allison</surname><given-names>T.</given-names></name><name><surname>McKeowin</surname><given-names>M. J.</given-names></name><name><surname>McCarthy</surname><given-names>G.</given-names></name></person-group> (<year>2003</year>). <article-title>Polysensory interactions along lateral temporal regions evoked by audiovisual speech</article-title>. <source>Cereb. Cortex</source>. <volume>13</volume>, <fpage>1034</fpage>&#x02013;<lpage>1043</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/13.10.1034</pub-id><pub-id pub-id-type="pmid">12967920</pub-id></mixed-citation></ref><ref id="B232"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yehia</surname><given-names>H.</given-names></name><name><surname>Rubin</surname><given-names>P.</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name></person-group> (<year>1998</year>). <article-title>Quantitative association of vocal-tract and facial behavior</article-title>. <source>Speech Commun</source>. <volume>26</volume>, <fpage>23</fpage>&#x02013;<lpage>43</lpage>
<pub-id pub-id-type="doi">10.1016/S0167-6393(98)00048-X</pub-id></mixed-citation></ref><ref id="B233"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>R. J.</given-names></name><name><surname>Bouffard</surname><given-names>M.</given-names></name><name><surname>Belin</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>). <article-title>Sensitivity to auditory object features in human temporal neocortex</article-title>. <source>J. Neurosci</source>. <volume>24</volume>, <fpage>3637</fpage>&#x02013;<lpage>3642</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5458-03.2004</pub-id><pub-id pub-id-type="pmid">15071112</pub-id></mixed-citation></ref><ref id="B234"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeki</surname><given-names>S.</given-names></name></person-group> (<year>2005</year>). <article-title>The Ferrier lecture 1995: behind the seen: the functional specialization of the brain in space and time</article-title>. <source>Philos. Trans. Biol. Sci</source>. <volume>360</volume>, <fpage>1145</fpage>&#x02013;<lpage>1183</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2005.1666</pub-id><pub-id pub-id-type="pmid">16147515</pub-id></mixed-citation></ref></ref-list></back></article>