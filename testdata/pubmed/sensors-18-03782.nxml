<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30400668</article-id><article-id pub-id-type="pmc">6264130</article-id><article-id pub-id-type="doi">10.3390/s18113782</article-id><article-id pub-id-type="publisher-id">sensors-18-03782</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Road Scene Simulation Based on Vehicle Sensors: An Intelligent Framework Using Random Walk Detection and Scene Stage Reconstruction</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5741-5280</contrib-id><name><surname>Li</surname><given-names>Yaochen</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03782">1</xref><xref rid="c1-sensors-18-03782" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Cui</surname><given-names>Zhichao</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03782">2</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yuehu</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03782">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Jihua</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03782">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Danchen</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03782">2</xref></contrib><contrib contrib-type="author"><name><surname>Yuan</surname><given-names>Jian</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03782">1</xref></contrib></contrib-group><aff id="af1-sensors-18-03782"><label>1</label>School of Software Engineering, Xi&#x02019;an Jiaotong University, No. 28 Xianning West Road, Xi&#x02019;an 710049, Shaanxi, China; <email>zhujh@mail.xjtu.edu.cn</email> (J.Z.); <email>yuanjian8868343@stu.xjtu.edu.cn</email> (J.Y.)</aff><aff id="af2-sensors-18-03782"><label>2</label>Institute of Artificial Intelligence and Robotics, Xi&#x02019;an Jiaotong University, No. 28 Xianning West Road, Xi&#x02019;an 710049, Shaanxi, China; <email>cui.zhichao@stu.xjtu.edu.cn</email> (Z.C.); <email>liuyh@mail.xjtu.edu.cn</email> (Y.L.); <email>danchenzhao@foxmail.com</email> (D.Z.)</aff><author-notes><corresp id="c1-sensors-18-03782"><label>*</label>Correspondence: <email>yaochenli@mail.xjtu.edu.cn</email>; Tel.: +86-029-8266-3000 (ext. 8012)</corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>11</month><year>2018</year></pub-date><volume>18</volume><issue>11</issue><elocation-id>3782</elocation-id><history><date date-type="received"><day>28</day><month>9</month><year>2018</year></date><date date-type="accepted"><day>29</day><month>10</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 by the authors.</copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Road scene model construction is an important aspect of intelligent transportation system research. This paper proposes an intelligent framework that can automatically construct road scene models from image sequences. The road and foreground regions are detected at superpixel level via a new kind of random walk algorithm. The seeds for different regions are initialized by trapezoids that are propagated from adjacent frames using optical flow information. The superpixel level region detection is implemented by the random walk algorithm, which is then refined by a fast two-cycle level set method. After this, scene stages can be specified according to a graph model of traffic elements. These then form the basis of 3D road scene models. Each technical component of the framework was evaluated and the results confirmed the effectiveness of the proposed approach.</p></abstract><kwd-group><kwd>road scene model</kwd><kwd>region detection</kwd><kwd>random walk</kwd><kwd>scene stage</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-18-03782"><title>1. Introduction</title><p>Road scene simulation and modeling based on vehicle sensors are currently an important research topic in the field of intelligent transportation systems [<xref rid="B1-sensors-18-03782" ref-type="bibr">1</xref>,<xref rid="B2-sensors-18-03782" ref-type="bibr">2</xref>]. The data acquired from vehicle-mounted sensors mainly include images, laser scans, and GPS. Once acquired, road scene modeling results can be applied to the simulation of photo-realistic traffic scenes.</p><p>A key use of such simulation is for the purposes of road safety, especially with regard to unmanned vehicles. Extensive tests and evaluations need to be performed to ensure the safety and robustness of unmanned vehicles. These mainly involve the field tests and offline tests. Field tests are the most traditional way to proceed. Micky Town at Michigan State and Google&#x02019;s Castle Atwater in California have both been constructed to undertake unmanned vehicle evaluation. To be effective, field tests have to be implemented on different types of roads including urban roads, rural roads and highways. However, there are a number of drawbacks to using field tests, such as limited driving space, the time and effort involved in constructing them and their significant monetary cost. As a result, offline tests have become an increasingly important complement to field-based evaluation.</p><p>Early studies using offline testing were designed around computer graphics methods. Typical applications here included Prescan from TNO (Netherland) [<xref rid="B3-sensors-18-03782" ref-type="bibr">3</xref>] and CarMaker from IPG (United States) [<xref rid="B4-sensors-18-03782" ref-type="bibr">4</xref>]. Recently, road scene modeling based on road image sequences has become popular, for instance Google Street View [<xref rid="B5-sensors-18-03782" ref-type="bibr">5</xref>] or Microsoft Street Slide [<xref rid="B6-sensors-18-03782" ref-type="bibr">6</xref>]. These systems provide users with an immersive touring experience while browsing a map. The Waymo Team at Google (California, USA) have constructed a virtual scene platform from road image sequences that can be applied to the simulation of virtual roads for billions of miles.</p><p>In this paper, we propose a new framework for road scene simulation from road image sequences. This specific application we have in mind for this framework is the offline testing of unmanned vehicles. The framework consists of two successive steps: (1) the detection of road and foreground regions&#x02019; detection; and (2) the construction of scene models. For the first step, we have developed a region detection method based on a superpixel-level random walk (RW) algorithm, which is further refined by a fast two-cycle (FTC) level set approach. For the second step, foreground and background models are constructed using scene stages. &#x0201c;Floor-wall&#x0201d; structured traffic scenes are then established.</p><p>The main contributions of this work can be summarized as follows:
<list list-type="bullet"><list-item><p>The development of a novel region detection method using a superpixel RW algorithm. The superpixel features are computed using a combination of color, texture and location information. Region seeds for the RW algorithm are initialized through region-related trapezoids using an optical flow map. An FTC level set is then employed for region refinement.</p></list-item><list-item><p>The development of a new framework for constructing 3D scene models based on scene stages. The scene stages are specified according to the detected road and foreground regions. 3D scene models are then constructed on the basis of graph models of the scene stages.</p></list-item><list-item><p>The development of a new system to simulate traffic scenes. Two modes are designed for the simulation of traffic scenes: (1) an interactive touring mode, and (2) a bird&#x02019;s-eye view mode. This system can form the basis of the offline testing of unmanned vehicles.</p></list-item></list>
</p></sec><sec id="sec2-sensors-18-03782"><title>2. Related Works</title><p>The framework of this paper consists of several components, which mainly includes region detection and scene model construction. In this framework, the computation of the superpixel features is an important pre-processing step. A new RW algorithm is then proposed based on the superpixel features. The road scene models are further constructed based on the region detection results. The related works are summarized in the areas of region detection and scene model construction.</p><p>Superpixels are the mid-level processing units widely used in the computer vision community. Schick et al. [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>] convert the pixel-based segmentation into a probabilistic superpixel representation. A Markov random field (MRF) is then applied to exploit the structural information and similarities to improve the segmentation. Lu et al. [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>] specify the superpixel-level seeds using an unsupervised learning strategy. The appropriate superpixel neighbors are further localized by a GrowCut framework. However, the vanishing points in the images must be detected as a pre-processing step. Siogkas et al. [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>] propose a RW algorithm integrating spatial and temporal information for road detection. Nevertheless, this pixel-level RW algorithm has the drawback of low efficiency and slow speed. Shen et al. [<xref rid="B10-sensors-18-03782" ref-type="bibr">10</xref>] present an image segmentation approach using lazy random walk (LRW). The LRW algorithm with self-loop has the merits of segmenting the weak boundaries. However, this method cannot be applied to image sequences since the spatio-temporal superpixel features are not considered. The similar studies include the fixation point-based segmentation [<xref rid="B11-sensors-18-03782" ref-type="bibr">11</xref>], etc. Recently, the region detection methods based on convolutional neural networks (CNN) become popular. Teichmann et al. [<xref rid="B12-sensors-18-03782" ref-type="bibr">12</xref>] apply deep CNN to jointly reason about classification, detection and semantic segmentation based on the KITTI road dataset [<xref rid="B13-sensors-18-03782" ref-type="bibr">13</xref>]. The effectiveness of residual networks (ResNet) [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>] and VGG [<xref rid="B15-sensors-18-03782" ref-type="bibr">15</xref>] are also evaluated. However, the training of the networks is computationally expensive.</p><p>The 3D scene models can be constructed based on the region detection results. Saxena et al. [<xref rid="B16-sensors-18-03782" ref-type="bibr">16</xref>] propose a method to learn plane parameters using an MRF model. The location and orientation of each mesh facet can be judged to construct the 3D wireframe of the scene model. However, this method lacks a hypothesis about the scene layout, and the computation of the image features is time-consuming. Hoiem et al. [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>] apply superpixel segmentation to the input images, and then utilize a support vector machine (SVM) for superpixel clustering. Delage et al. [<xref rid="B18-sensors-18-03782" ref-type="bibr">18</xref>] apply a dynamic Bayesian network to automatically judge the edge pixels between the floor and wall regions for indoor images. The &#x0201c;floor-wall&#x0201d;-structured scene model is then constructed. Unfortunately, this scene model cannot be applied to the outdoor images. The Tour into the Picture (TIP) model of Horry et al. [<xref rid="B19-sensors-18-03782" ref-type="bibr">19</xref>] partitions an input image into regions of &#x0201c;left wall&#x0201d;, &#x0201c;right wall&#x0201d;, &#x0201c;back wall&#x0201d;, &#x0201c;ceiling&#x0201d; and &#x0201c;floor&#x0201d;. The foreground objects are assumed to stand perpendicularly to the floor plane. The main drawback of the TIP model lies in the fact that it is applicable to the curved floor conditions. Nedovic et al. [<xref rid="B20-sensors-18-03782" ref-type="bibr">20</xref>] introduce the typical 3D scene geometries called stages, each with a unique depth profile. The stage information serves as the first step to infer the global depth. Lou et al. [<xref rid="B21-sensors-18-03782" ref-type="bibr">21</xref>] propose an approach that first predicts the global image structure, and then extract the pixel-level 3D layout. However, the prediction of the scene stage label is complex and computationally expensive.</p><p>The comparison of the previous studies is shown in <xref rid="sensors-18-03782-t001" ref-type="table">Table 1</xref>, where region detection and scene construction are summarized, respectively.</p><p>The rest of the paper is organized as follows: The superpixel-level RW method and FTC refinement is presented in <xref ref-type="sec" rid="sec3-sensors-18-03782">Section 3</xref>. In <xref ref-type="sec" rid="sec4-sensors-18-03782">Section 4</xref>, the construction of road scene models is introduced. Experiments and comparisons are shown in <xref ref-type="sec" rid="sec5-sensors-18-03782">Section 5</xref>. Finally, we close this paper with conclusion and future works.</p></sec><sec id="sec3-sensors-18-03782"><title>3. Random Walk for Region Detection at a Superpixel Level</title><p>In order to reconstruct road scene models, the road and foreground regions in each frame have to be detected first of all. To do this, we use a region detection method based on an RW algorithm. The road and the foreground regions of the first frame are specified by means of user annotation. For the rest of the frames, corresponding regions are detected based on the seeds propagated from the previous frames. The detection flow diagram for an example frame <italic>t</italic> is shown in <xref ref-type="fig" rid="sensors-18-03782-f001">Figure 1</xref>.</p><sec id="sec3dot1-sensors-18-03782"><title>3.1. Superpixel Segmentation</title><p>For the region detection we use superpixel segmentation, specifically adopting the Ncut method [<xref rid="B22-sensors-18-03782" ref-type="bibr">22</xref>], with superpixel features being extracted as the basis of the algorithm (see, <xref rid="sensors-18-03782-t002" ref-type="table">Table 2</xref>). Color, texture and location descriptors are combined to denote the superpixel features. The color descriptor mainly used the HSV and CIElab color spaces. The texture descriptor is composed of a Gabor filter and a local binary pattern (LBP). The location descriptor consists of a convex hull, a clique number, etc. These descriptors are then concatenated into an overall vector.</p></sec><sec id="sec3dot2-sensors-18-03782"><title>3.2. Definition of Seed Trapezoid</title><p>Seed trapezoids for the road and foreground regions are depicted in <xref ref-type="fig" rid="sensors-18-03782-f002">Figure 2</xref>. If <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is taken to be the center of the detected road region of frame <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> can be denoted as the upper bound, with the same vertical coordinate as the centroid. The bottom line of the trapezoid (<inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is the same as frame <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x02019;s road base. The top of the trapezoid (<inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is equal to the perpendicular distance for <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The seed trapezoid seed for the road region can then be specified. The trapezoid for the foreground region is defined in a similar way, as shown with regard to the placement in <xref ref-type="fig" rid="sensors-18-03782-f002">Figure 2</xref>.</p></sec><sec id="sec3dot3-sensors-18-03782"><title>3.3. Seed Trapezoid Initialization by Optical Flow</title><p>The trapezoid of a certain frame <italic>t</italic> is initialized in frame <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> using an optical flow map. Optical flow map generation is typically classified into (1) sparse optical flow and (2) dense optical flow. Sparse optical flow is computed from the local feature points in images, e.g., by using the Lucas&#x02013;Kanade algorithm [<xref rid="B23-sensors-18-03782" ref-type="bibr">23</xref>]. Dense optical flow, however, attempts to compute the optical flow information for all of the pixels in an image, for instance, by using the Horn and Schunck [<xref rid="B24-sensors-18-03782" ref-type="bibr">24</xref>] approach. The Bruhn method [<xref rid="B25-sensors-18-03782" ref-type="bibr">25</xref>] brings together the advantages of both sparse and dense optical flows. A dense optical flow field is generated by using this method that is robust under conditions of environmental noise. A comparison of the results produced by the Lucas&#x02013;Kanade, Horn&#x02013;Schunck and Bruhn methods are shown in <xref ref-type="fig" rid="sensors-18-03782-f003">Figure 3</xref>.</p></sec><sec id="sec3dot4-sensors-18-03782"><title>3.4. Random Walk Detection at a Superpixel Level</title><p>The superpixel-level RW detection is shown in Algorithm 1. Its basic steps are as follows:
<list list-type="bullet"><list-item><p>The input image is transformed into the graph: <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the graph nodes denote the superpixels. The weight of the edges in the graph are defined as follows:
<disp-formula id="FD1-sensors-18-03782"><label>(1)</label><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm12"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the features of superpixels <italic>i</italic> and <italic>j</italic>, while <inline-formula><mml:math id="mm13"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the respective superpixel locations. The superpixel features are represented by a concatenation of the color and texture vectors:
<disp-formula id="FD2-sensors-18-03782"><label>(2)</label><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>B</mml:mi></mml:mrow><mml:mo>&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:munder><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the color feature of the current superpixel. <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the texture feature, which is extracted by a Gabor filter. The image block centered at the current superpixel is used to compute the output vector of the Gabor filter. The superpixel features match those described in <xref ref-type="sec" rid="sec3dot1-sensors-18-03782">Section 3.1</xref>.</p></list-item><list-item><p>A Laplacian matrix is now defined to denote the superpixel probability labels:
<disp-formula id="FD3-sensors-18-03782"><label>(3)</label><mml:math id="mm18"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the superpixel value indexed by <inline-formula><mml:math id="mm20"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm21"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the degree for <inline-formula><mml:math id="mm23"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on all edges <inline-formula><mml:math id="mm24"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Next, vertices of the graph <italic>G</italic> are partitioned into two groups: a seeded set <inline-formula><mml:math id="mm25"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and an unseeded set <inline-formula><mml:math id="mm26"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. These two sets satisfy the following requirements: <inline-formula><mml:math id="mm27"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm28"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02205;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The Laplacian matrix can now be redefined as:
<disp-formula id="FD4-sensors-18-03782"><label>(4)</label><mml:math id="mm29"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>B</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>B</mml:mi><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm31"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the seeded and unseeded pixels in <italic>L</italic> and <inline-formula><mml:math id="mm32"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the decomposition result.</p></list-item><list-item><p>The Dirichlet integral is:
<disp-formula id="FD5-sensors-18-03782"><label>(5)</label><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>L</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>x</italic> is a function that minimizes Equation (<xref ref-type="disp-formula" rid="FD5-sensors-18-03782">5</xref>) and <inline-formula><mml:math id="mm34"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is an edge of the graph connecting the vertices <inline-formula><mml:math id="mm35"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>The decomposition of Equation (<xref ref-type="disp-formula" rid="FD5-sensors-18-03782">5</xref>) can be defined as:
<disp-formula id="FD6-sensors-18-03782"><label>(6)</label><mml:math id="mm37"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>L</mml:mi><mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>x</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>x</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm38"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm39"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the sets of regional probabilities corresponding to the unseeded and seeded superpixels. Differentiating <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with respect to <inline-formula><mml:math id="mm41"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> yields
<disp-formula id="FD7-sensors-18-03782"><label>(7)</label><mml:math id="mm42"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
This equation can be solved using the method outlined in [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>], with it being considered to be a two-class segmentation problem.</p></list-item></list>
</p></sec><sec id="sec3dot5-sensors-18-03782"><title>3.5. Region Refinement Using an FTC Level Set</title><p>The region detection algorithm described above is effective for most conditions. However, as the computing units of the algorithm are superpixels, there is a risk that, if the superpixel features are similar in the foreground and background regions, it may fail to get accurate results. To solve this problem, a fast two-cycle (FTC) level set method can be applied that is based on pixels for region refinement. The main data structure for this method is composed of three parts: an integer array <inline-formula><mml:math id="mm43"><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:math></inline-formula> as the level set kernel matrix, an integer array <inline-formula><mml:math id="mm44"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for the speed function and two linked lists of boundary pixels to denote the foreground contours <inline-formula><mml:math id="mm45"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm46"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The foreground contours are initialized from the regions detected by the superpixel-level RW algorithm. In order to facilitate curve evolution of the FTC level set, the first cycle should be conducted to establish the data fidelity terms, then a second cycle should be conducted to establish the data smoothness terms. The pixels inside <inline-formula><mml:math id="mm47"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are called interior pixels, and the pixels outside <inline-formula><mml:math id="mm48"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are called exterior pixels. The kernel matrix can be defined as:
<disp-formula id="FD8-sensors-18-03782"><label>(8)</label><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>3</mml:mn><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>if</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>y</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>is</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>an</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>exterior</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>pixel</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>if</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>y</mml:mi><mml:mspace width="1.0pt"/><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>if</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>if</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>y</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>is</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>an</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>interior</mml:mi><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>pixel</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>The speed function <inline-formula><mml:math id="mm50"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined according to the competition terms between the foreground and background regions [<xref rid="B26-sensors-18-03782" ref-type="bibr">26</xref>]. The curve evolution can then be determined by using the signs of the speed function rather than by solving any partial differential equations. The speed function is as follows:
<disp-formula id="FD9-sensors-18-03782"><label>(9)</label><mml:math id="mm51"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo>&#x003a9;</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo>&#x003a9;</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo>&#x003a9;</mml:mo><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo>&#x003a9;</mml:mo><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm52"><mml:mrow><mml:msub><mml:mo>&#x003a9;</mml:mo><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm53"><mml:mrow><mml:msub><mml:mo>&#x003a9;</mml:mo><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the feature pools for the foreground and the background, respectively. <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the color and texture feature vectors extracted at pixel <italic>y</italic>. After definition of the kernel matrix and speed function, the curve evolution can be implemented across two cycles. The first cycle is for the boundary pixel evolution. The second cyle is for the boundary smoothness. The functions <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be used for the curve evolution [<xref rid="B27-sensors-18-03782" ref-type="bibr">27</xref>]. The region refinement examples based on the FTC level set are shown in <xref ref-type="fig" rid="sensors-18-03782-f004">Figure 4</xref>.</p><p>The region detection algorithm based on a superpixel-level RW with FTC refinement is summarized in Algorithm 1.</p><array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Region detection based on a superpixel-level RW with FTC refinement</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Require</mml:mi><mml:mo mathvariant="bold">:</mml:mo><mml:mfenced open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Input image sequence</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Superpixel set for each frame</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p>Initialize the regions and trapezoids for the frame <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p>(1) Region detection based on a RW at superpixel level:</p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p><bold>for</bold> t = 1 to M <bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>&#x02003;Compute the optical flow map between frame <italic>t</italic> and <inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; </p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>&#x02003;Initialize region seed of frame <italic>t</italic> using the trapezoid of frame <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on the basis of the optical flow map;</p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>&#x02003;Down-sample frame <italic>t</italic> to a lower resolution; </p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x02003;Implement the RW algorithm for frame <italic>t</italic>;</p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x02003;Implement the threshold for the probability matrix to get the detection results;</p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>&#x02003;Up-sample frame <italic>t</italic> to the original resolution;</p></list-item><list-item><label>&#x000a0;10:</label><p>
<bold>end for</bold>
</p></list-item><list-item><label>&#x000a0;11:</label><p>(2) Region refinement based on the FTC level set: </p></list-item><list-item><label>&#x000a0;12:</label><p><bold>for</bold> t = 1 to M <bold>do</bold></p></list-item><list-item><label>&#x000a0;13:</label><p>&#x02003;Initialize <inline-formula><mml:math id="mm62"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm63"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> based on the RW detection results; </p></list-item><list-item><label>&#x000a0;14:</label><p>&#x02003;Define the speed function <inline-formula><mml:math id="mm64"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> using Eq. (9);</p></list-item><list-item><label>&#x000a0;15:</label><p>&#x02003;Undertake the Fast Two-Cycle curve evolution;</p></list-item><list-item><label>&#x000a0;16:</label><p>&#x02003;Get the refined regions according to the contours;</p></list-item><list-item><label>&#x000a0;17:</label><p>
<bold>end for</bold>
</p></list-item></list>
<inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Ensure</mml:mi><mml:mo mathvariant="bold">:</mml:mo><mml:mfenced open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mtext>The region detection results for the image frames</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mtext>The seed trapezoid for each image frame</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></array></sec></sec><sec id="sec4-sensors-18-03782"><title>4. Road Scene Construction and Simulation</title><p>After detection of the road and foreground regions, the corresponding static and dynamic traffic elements can be extracted. The scene models are then constructed based on the scene stages. These form the basis of the road scene simulation process.</p><sec id="sec4dot1-sensors-18-03782"><title>4.1. Scene Model Construction</title><p>In order to construct the road scene models, a graph model can be used for each image to represent the connection between the foreground and background regions:
<disp-formula id="FD10-sensors-18-03782"><label>(10)</label><mml:math id="mm66"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>N</italic> and <italic>E</italic> represent the scene nodes and the corresponding relationships. For the <italic>i</italic>th image, <inline-formula><mml:math id="mm67"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm68"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are defined by:
<disp-formula id="FD11-sensors-18-03782"><label>(11)</label><mml:math id="mm69"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote &#x0201c;road plane&#x0201d;, &#x0201c;left wall&#x0201d;, &#x0201c;right wall&#x0201d; and &#x0201c;back wall&#x0201d;, respectively. <inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the control points of road boundaries on each side [<xref rid="B1-sensors-18-03782" ref-type="bibr">1</xref>]. <inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <italic>n</italic> foreground objects, while <inline-formula><mml:math id="mm77"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the positions of rht foreground polygons standing on the road plane.</p><p>A set of scene stages <inline-formula><mml:math id="mm78"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is defined in advance, which can represent the basic structures of typical road scenes. A related scene stage can be specified for each graph model to denote the scene&#x02019;s geometric structure. The scene stage is a traffic scene wireframe model that represents the scene layout for 3D traffic scene modeling. Scene stages can be classified into simple and complex types, as shown in <xref ref-type="fig" rid="sensors-18-03782-f005">Figure 5</xref>. A simple scene stage consists of just the background elements of a traffic scenes. A complex scene stage consists of both the background and foreground elements.</p><p>For the simple scene stages, the corresponding background models have a &#x0201c;floor-wall&#x0201d; structure, with the road region being located on the horizon plane. The rest of the background regions are assumed to be perpendicular to the road plane. For the complex scene stages, the foreground models are constructed from rectangular polygons, which are set vertically to the road plane. The foreground polygons have <inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> data structure, with <italic>A</italic> denoting the transparency ratio. The polygons in the region outside of the foreground are set to be transparent.</p><p>We determine the control points for the road boundaries according to the traffic scene wireframe model. The control points are distributed on both sides of the road. In each scene stage, the two farthest control points are taken from the far end of the road surface, while the two closest control points are specified by the near end of the road surface. The remaining control points are distributed between the farthest and the nearest control points, as shown in <xref ref-type="fig" rid="sensors-18-03782-f006">Figure 6</xref>a. A 3D corridor road scene model can then be constructed according to the control points (this principle is shown in <xref ref-type="fig" rid="sensors-18-03782-f006">Figure 6</xref>b). In addtion to traditional scene models, cartoon scene models can be constructed by applying non-photorealistic rendering to the input images [<xref rid="B28-sensors-18-03782" ref-type="bibr">28</xref>]. The road scene model construction process is summarized in Algorithm 2.</p><array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold> Road scene model construction.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold> The region detection results for the image frames <inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.
<list list-type="simple"><list-item><label>1:</label><p><bold>for</bold><inline-formula><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to M <bold>do</bold></p></list-item><list-item><label>2:</label><p>&#x02003;Specify the foreground regions <inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and road region <inline-formula><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:msup><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm84"><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><label>3:</label><p>&#x02003;Construct the scene graph model <inline-formula><mml:math id="mm85"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><label>4:</label><p>&#x02003;Specify the scene stage <inline-formula><mml:math id="mm86"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:msup><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> according to the scene graph model <italic>G</italic>;</p></list-item><list-item><label>5:</label><p>&#x02003;Perform the scene construction based on the geometric structure of <inline-formula><mml:math id="mm87"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:msup><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>6:</label><p><bold>end for</bold></p></list-item></list>
<bold>Ensure:</bold> Road scene models with the &#x0201c;floor-wall&#x0201d; structure.</td></tr></tbody></array></sec><sec id="sec4dot2-sensors-18-03782"><title>4.2. Road Scene Simulation</title><p>The 3D traffic scene simulation can be performed by assembling the &#x0201c;floor-wall&#x0201d;-structured scene model. The viewpoint can be freely changed during the simulation process by using the commands forward, back, up and down. With the assistance of GPS data, the trajectory of the viewpoint can be displayed on the map. Depending on how the viewpoint moves, the traffic scene simulation can be classified into two modes: (1) Roaming mode, where new viewpoint images are generated according to the movement of the viewpoint; (2) Bird&#x02019;s-eye view mode, where the viewpoint is static and virtual vehicles and obstacles can be added into the scene. The generation of new viewpoint images is shown in <xref ref-type="fig" rid="sensors-18-03782-f007">Figure 7</xref>a. The black grid represents the basic scene model. The green grid denotes the foreground vehicle model. Users can change the viewing angle through adjustments of the road scene model. The new viewpoint images are then generated, as shown in <xref ref-type="fig" rid="sensors-18-03782-f007">Figure 7</xref>b.</p><p>We have designed five metrics for the offline testing of unmanned vehicles, namely: pedestrian recognition; collision avoidance; traffic signal recognition; pavement identification; and fog recognition. Four levels of performance for each metric were also defined to evaluate vehicle behavior, (L0, L1, L2, L3), as shown in <xref rid="sensors-18-03782-t003" ref-type="table">Table 3</xref>. The complexity of the road scene can generally be divided into three categories, (R1, R2, R3), according to: road conditions; special areas; and special kinds of weather. On the basis of these categories, various scenes can be constructed easily for the offline testing of unmanned vehicles. The three categories of road complexity are as follows:
<list list-type="bullet"><list-item><p>R1: Different road conditions including rural roads, urban roads, highways and tunnels. The road parameters relate to the specific road width, the number of obstacles, traffic signs and traffic lights.</p></list-item><list-item><p>R2: Special areas including campuses, hospitals, crowded streets, etc. Vehicles in special areas need to react quickly and perform different operations.</p></list-item><list-item><p>R3: Special kinds of weather including rain, snow, fog, etc. The special kinds of weather may also be of varying degree. This needs to be accurately identified for the evaluations.</p></list-item></list>
</p><p>Finally, the performance of unmanned vehicles may be evaluated across a combination of different degrees of scene complexity and different evaluation metrics.</p></sec></sec><sec id="sec5-sensors-18-03782"><title>5. Experiments and Comparisons</title><p>Our experiments were undertaken on a computer with an Intel i5 processor @3.33 GHZ and with 16 GB Memory). The experimental data was mostly taken from the TSD-max dataset [<xref rid="B29-sensors-18-03782" ref-type="bibr">29</xref>], which was constructed by the Institute of Artificial Intelligence and Robotics at Xi&#x02019;an Jiaotong University in China. The dataset is composed of road images captured from urban roads, rural roads, highways, etc. Specifically, the experiment of road region detection is also based on the KITTI dataset [<xref rid="B12-sensors-18-03782" ref-type="bibr">12</xref>].</p><sec id="sec5dot1-sensors-18-03782"><title>5.1. Evaluation of the Region Detection</title><p>First of all, we perform the region detection experiments. Three image sequences are selected for the region detection experiments: White Car (512 &#x000d7; 512 pixel size, 200 frames), Gray Truck (512 &#x000d7; 512 pixel size, 300 frames), and Red Truck (512 &#x000d7; 512 pixel size, 250 frames). The KITTI dataset is utilized for the task of road region detection (1242 &#x000d7; 375 pixel size, 200 frames). <xref ref-type="fig" rid="sensors-18-03782-f008">Figure 8</xref> shows the superpixel segmentation at different scales, the optical flow map between adjacent frames, and the region detection results before and after refinement. The road detection results based on the superpixel-level RW method with refinement for the KITTI dataset is shown in <xref ref-type="fig" rid="sensors-18-03782-f009">Figure 9</xref>. The algorithms mainly work on the basis of a superpixel segmentation size of <inline-formula><mml:math id="mm92"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. For quantitative evaluation, we use three metrics: precision; recall; and F-measure to compare with the ground truth road and foreground regions. Precision denotes the ratio of correct pixels within the detected road region. Recall denotes the ratio of correct pixels in relation to the benchmark road region. In our experiments, precision and recall are defined as follows:
<disp-formula id="FD12-sensors-18-03782"><label>(12)</label><mml:math id="mm93"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced separators="" open="|" close="|"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>R</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mspace width="1.0pt"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced separators="" open="|" close="|"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic>R</italic> and <inline-formula><mml:math id="mm94"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the detected region and ground truth, respectively.</p><p>The <italic>F-Measure</italic> can be computed by combining <italic>Pre</italic> and <italic>Rec</italic>:
<disp-formula id="FD13-sensors-18-03782"><label>(13)</label><mml:math id="mm95"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
We set <inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to give precision and recall equal weight.</p><p>We employ the superpixel SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>], Markov random field (MRF) [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>], GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>] and Siogkas&#x02019;s RW [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>] as baseline methods for road region detection. Besides these methods, VGG16 [<xref rid="B15-sensors-18-03782" ref-type="bibr">15</xref>] and ResNet101 [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>] are implemented for the comparison of road detection results. The average values for all of the image sequences are shown in <xref rid="sensors-18-03782-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-18-03782-t005" ref-type="table">Table 5</xref>. With regard to the methods presented in this paper, the superpixel-level random walk (SRW) and the superpixel-level RW with level set refinement (SRW+Refine) are implemented. As the results of the comparisons show, the accuracy our proposed method is approximately similar to ResNet101, which is superior to the other baseline methods.</p></sec><sec id="sec5dot2-sensors-18-03782"><title>5.2. Evaluation of the Scene Construction</title><p>Next, we performed experiments relating to the scene construction and simulation. Input image sequences with a resolution of 1024 &#x000d7; 1024 pixels were once again selected from the TSD-max dataset, as shown in <xref ref-type="fig" rid="sensors-18-03782-f010">Figure 10</xref>. There were three main types of scene models: (1) simple scene models based on pure background images; (2) complex scene models based on both foreground and background images; and (3) cartoon scene models based on non-photorealistic images [<xref rid="B28-sensors-18-03782" ref-type="bibr">28</xref>].</p><p>The scene models generated by our own approach were compared to those generated by Make3D [<xref rid="B16-sensors-18-03782" ref-type="bibr">16</xref>] and Photo Pop-up [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>], as shown in <xref ref-type="fig" rid="sensors-18-03782-f011">Figure 11</xref>. The results demonstrate that distortions can occur in the Make3D and Photo Pop-up models. When compared to these baseline methods, our own approach produce the most realistic results. We also find that our proposed scene structure is more suitable for curved road conditions.</p><p>To qualitatively evaluate the scene models, we apply two metrics [<xref rid="B16-sensors-18-03782" ref-type="bibr">16</xref>]: (1) a plane correctness ratio, with a plane being defined as correct if more than 75% of the plane patches are correctly detected as semantic wall and road regions; (2) a model correctness ratio, with a model being defined as correct if more than 75% of the patches in the wall and the road planes had the correct relationship to their neighbors. The evaluation is performed by someone who is not associated with the project. One-thousand road images with a resolution of 1024 &#x000d7; 1024 are chosen overall. The comparative results according to these metrics are shown in <xref rid="sensors-18-03782-t006" ref-type="table">Table 6</xref>. As can be seen, the proposed scene models outperform the baseline methods in terms of both plane and model correctness.</p></sec></sec><sec id="sec6-sensors-18-03782"><title>6. Conclusions and Future Works</title><p>In this paper, we have proposed a framework for road scene model construction via superpixel-level RW region detection. The RW detection is able to locate road and foreground regions simultaneously. After segmentation of the superpixels, a region seed for the first frame is specified through user annotation. For each subsequent frame, the region seeds are located using trapezoids propagated from the previous frame. The RW algorithm is implemented through initialization of the region seeds, with the superpixels being utilized as graph nodes. In order to achieve more accurate region detection results, an FTC level set algorithm was also implemented for region refinement. After detection of the road and foreground regions, 3D corridor-style scene models can be constructed, depending on the type of scene stage. 3D traffic scene simulations can then be developed that are based on the scene models.</p><p>In the future research, we intend to use depth maps to construct more detailed scene models. A multi-view registration of point clouds can then be implemented to reconstruct 3D foreground structures. Cuboid models of the traffic elements can also be constructed on the basis of this for an even more sophisticated simulation of traffic scenes.</p></sec></body><back><notes><title>Author Contributions</title><p>Y.L. designed the algorithms and wrote the manuscript. Z.C. conducted the experiments and took part in the manuscript revision. Y.L. managed the project. J.Z. performed the discussion of the results. D.Z. analyzed the data. J.Y. took part in the manuscript revision.</p></notes><notes><title>Funding</title><p>This research was supported by the National Natural Science Foundation of China under Grant No. 61803298, and it was also supported by the Natural Science Foundation of Jiangsu Province under Grant BK20180236.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-18-03782"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Su</surname><given-names>Y.</given-names></name><name><surname>Hua</surname><given-names>G.</given-names></name><name><surname>Zheng</surname><given-names>N.</given-names></name></person-group><article-title>Three-dimensional traffic scenes simulation from road image sequences</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2016</year><volume>17</volume><fpage>1121</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1109/TITS.2015.2497408</pub-id></element-citation></ref><ref id="B2-sensors-18-03782"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Kim</surname><given-names>Z.</given-names></name><name><surname>Xiong</surname><given-names>Z.</given-names></name></person-group><article-title>Spatio-temporal traffic scene modeling for object motion detection</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2014</year><volume>14</volume><fpage>1662</fpage><lpage>1668</lpage><pub-id pub-id-type="doi">10.1109/TITS.2012.2212432</pub-id></element-citation></ref><ref id="B3-sensors-18-03782"><label>3.</label><element-citation publication-type="web"><article-title>Simulation of ADAS and Active Safety</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.tassinternational.com/prescan">http://www.tassinternational.com/prescan</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2018-11-05">(accessed on 5 November 2018)</date-in-citation></element-citation></ref><ref id="B4-sensors-18-03782"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasagasioglu</surname><given-names>S.</given-names></name><name><surname>Kilicaslan</surname><given-names>K.</given-names></name><name><surname>Atabay</surname><given-names>O.</given-names></name></person-group><article-title>Vehicle dynamics analysis of a heavy-duty commercial vehicle by using multibody simulation methods</article-title><source>Int. J. Adv. Manuf. Technol.</source><year>2012</year><volume>60</volume><fpage>825</fpage><lpage>839</lpage><pub-id pub-id-type="doi">10.1007/s00170-011-3588-8</pub-id></element-citation></ref><ref id="B5-sensors-18-03782"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anguelov</surname><given-names>D.</given-names></name><name><surname>Dulong</surname><given-names>C.</given-names></name><name><surname>Filip</surname><given-names>D.</given-names></name><name><surname>Frueh</surname><given-names>C.</given-names></name><name><surname>Lafon</surname><given-names>S.</given-names></name><name><surname>Lyon</surname><given-names>R.</given-names></name><name><surname>Ogale</surname><given-names>A.</given-names></name><name><surname>Vincent</surname><given-names>L.</given-names></name><name><surname>Weaver</surname><given-names>J.</given-names></name></person-group><article-title>Google street view: Capturing the world at street level</article-title><source>Computer</source><year>2010</year><volume>42</volume><fpage>32</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1109/MC.2010.170</pub-id></element-citation></ref><ref id="B6-sensors-18-03782"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopf</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Szeliski</surname><given-names>R.</given-names></name><name><surname>Cohen</surname><given-names>M.</given-names></name></person-group><article-title>Street slide: Browsing street level imagery</article-title><source>ACM Trans. Graph.</source><year>2010</year><volume>29</volume><fpage>102</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1145/1778765.1778833</pub-id></element-citation></ref><ref id="B7-sensors-18-03782"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schick</surname><given-names>A.</given-names></name><name><surname>Bauml</surname><given-names>M.</given-names></name><name><surname>Stiefelhagen</surname><given-names>R.</given-names></name></person-group><article-title>Improving foreground segmentations with probabilistic superpixel Markov random fields</article-title><source>Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Providence, RI, USA</conf-loc><conf-date>16&#x02013;21 June 2012</conf-date><fpage>27</fpage><lpage>31</lpage></element-citation></ref><ref id="B8-sensors-18-03782"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>K.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>An</surname><given-names>X.</given-names></name><name><surname>He</surname><given-names>H.</given-names></name></person-group><article-title>Vision sensor-based road detection for field robot navigation</article-title><source>Sensors</source><year>2015</year><volume>15</volume><fpage>29594</fpage><lpage>29617</lpage><pub-id pub-id-type="doi">10.3390/s151129594</pub-id><pub-id pub-id-type="pmid">26610514</pub-id></element-citation></ref><ref id="B9-sensors-18-03782"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siogkas</surname><given-names>G.K.</given-names></name><name><surname>Dermatas</surname><given-names>E.S.</given-names></name></person-group><article-title>Random-walker monocular road detection in adverse conditions using automated spatiotemporal seed selection</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2013</year><volume>14</volume><fpage>527</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1109/TITS.2012.2223686</pub-id></element-citation></ref><ref id="B10-sensors-18-03782"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>J.</given-names></name><name><surname>Du</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Lazy random walks for superpixel segmentation</article-title><source>IEEE Trans. Image Process.</source><year>2014</year><volume>23</volume><fpage>1451</fpage><lpage>1462</lpage><pub-id pub-id-type="doi">10.1109/TIP.2014.2302892</pub-id><pub-id pub-id-type="pmid">24565788</pub-id></element-citation></ref><ref id="B11-sensors-18-03782"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>X.</given-names></name><name><surname>Jung</surname><given-names>C.</given-names></name></person-group><article-title>Point-cut: Fixation point-based image segmentation using random walk model</article-title><source>Proceedings of the 2015 IEEE International Conference on Image Processing</source><conf-loc>Quebec City, QC, Canada</conf-loc><conf-date>27&#x02013;30 September 2015</conf-date><fpage>2125</fpage><lpage>2129</lpage></element-citation></ref><ref id="B12-sensors-18-03782"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Teichmann</surname><given-names>M.</given-names></name><name><surname>Weber</surname><given-names>M.</given-names></name><name><surname>Zoellner</surname><given-names>M.</given-names></name><name><surname>Cipolla</surname><given-names>R.</given-names></name><name><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>MultiNet: Real-time joing semantic reasoning for autonomous driving</article-title><source>Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Changshu, China</conf-loc><conf-date>26&#x02013;30 June 2018</conf-date><fpage>1013</fpage><lpage>1020</lpage></element-citation></ref><ref id="B13-sensors-18-03782"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Geiger</surname><given-names>A.</given-names></name><name><surname>Lenz</surname><given-names>P.</given-names></name><name><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Are we ready for autonomous driving: The KITTI vision benchmark suite</article-title><source>Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Providence, RI, USA</conf-loc><conf-date>16&#x02013;21 June 2012</conf-date><fpage>3354</fpage><lpage>3361</lpage></element-citation></ref><ref id="B14-sensors-18-03782"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B15-sensors-18-03782"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>M.D.</given-names></name><name><surname>Fergus</surname><given-names>R.</given-names></name></person-group><article-title>Visualizing and understanding convolutional networks</article-title><source>Proceedings of the 2014 European Conference on Computer Vision</source><conf-loc>Zurich, Switzerland</conf-loc><conf-date>6&#x02013;12 September 2014</conf-date><fpage>818</fpage><lpage>833</lpage></element-citation></ref><ref id="B16-sensors-18-03782"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>A.</given-names></name><name><surname>Sun</surname><given-names>M.</given-names></name><name><surname>Ng</surname><given-names>A.Y.</given-names></name></person-group><article-title>Make3D: Learning 3D scene structure from a single still image</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2009</year><volume>31</volume><fpage>824</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2008.132</pub-id><pub-id pub-id-type="pmid">19299858</pub-id></element-citation></ref><ref id="B17-sensors-18-03782"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoiem</surname><given-names>D.</given-names></name><name><surname>Efros</surname><given-names>A.</given-names></name><name><surname>Hebert</surname><given-names>M.</given-names></name></person-group><article-title>Recovering surface layout from an image</article-title><source>Int. J. Comput. Vis.</source><year>2007</year><volume>75</volume><fpage>151</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1007/s11263-006-0031-y</pub-id></element-citation></ref><ref id="B18-sensors-18-03782"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Delage</surname><given-names>E.</given-names></name><name><surname>Lee</surname><given-names>H.</given-names></name><name><surname>Ng</surname><given-names>A.Y.</given-names></name></person-group><article-title>A dynamic Bayesian model for autonomous 3D reconstruction from a single indoor image</article-title><source>Proceedings of the 2006 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>New York, NY, USA</conf-loc><conf-date>17&#x02013;22 June 2006</conf-date><fpage>2418</fpage><lpage>2428</lpage></element-citation></ref><ref id="B19-sensors-18-03782"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Horry</surname><given-names>Y.</given-names></name><name><surname>Anjyo</surname><given-names>K.</given-names></name><name><surname>Arai</surname><given-names>K.</given-names></name></person-group><article-title>Tour into the picture: Using a spidery interface to make animation from a single image</article-title><source>Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>3&#x02013;8 August 1997</conf-date><fpage>225</fpage><lpage>232</lpage></element-citation></ref><ref id="B20-sensors-18-03782"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nedovic</surname><given-names>V.</given-names></name><name><surname>Smeulders</surname><given-names>A.W.</given-names></name><name><surname>Redert</surname><given-names>A.</given-names></name><name><surname>Geusebroek</surname><given-names>J.M.</given-names></name></person-group><article-title>Stages as models of scene geometry</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2010</year><volume>32</volume><fpage>1673</fpage><lpage>1687</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2009.174</pub-id><pub-id pub-id-type="pmid">20634560</pub-id></element-citation></ref><ref id="B21-sensors-18-03782"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lou</surname><given-names>Z.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Hu</surname><given-names>N.</given-names></name></person-group><article-title>Extracting 3D layout from a single image using global image structures</article-title><source>IEEE Trans. Image Process.</source><year>2015</year><volume>24</volume><fpage>3098</fpage><lpage>3108</lpage><pub-id pub-id-type="pmid">25966478</pub-id></element-citation></ref><ref id="B22-sensors-18-03782"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>J.B.</given-names></name><name><surname>Malik</surname><given-names>J.</given-names></name></person-group><article-title>Normalized cuts and image segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2000</year><volume>22</volume><fpage>888</fpage><lpage>905</lpage></element-citation></ref><ref id="B23-sensors-18-03782"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bouguet</surname><given-names>J.Y.</given-names></name></person-group><source>Pyramidal Implementation of the Lucas-Kanade Feature Tracker</source><publisher-name>Intel Corporation, Microprocessor Research Labs</publisher-name><publisher-loc>Santa Clara, CA, USA</publisher-loc><year>1999</year></element-citation></ref><ref id="B24-sensors-18-03782"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horn</surname><given-names>B.K.</given-names></name><name><surname>Schunck</surname><given-names>B.G.</given-names></name></person-group><article-title>Determining optical flow</article-title><source>Artif. Intell.</source><year>1981</year><volume>17</volume><fpage>185</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(81)90024-2</pub-id></element-citation></ref><ref id="B25-sensors-18-03782"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruhn</surname><given-names>A.</given-names></name><name><surname>Weickert</surname><given-names>J.</given-names></name><name><surname>Schn</surname><given-names>C.</given-names></name></person-group><article-title>Lucas/Kanade meets Horn/Schunck: Combining local and global optical flow methods</article-title><source>Int. J. Comput. Vis.</source><year>2005</year><volume>61</volume><fpage>211</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000045324.43199.43</pub-id></element-citation></ref><ref id="B26-sensors-18-03782"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>S.C.</given-names></name><name><surname>Yuille</surname><given-names>A.</given-names></name></person-group><article-title>Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1996</year><volume>18</volume><fpage>884</fpage><lpage>900</lpage></element-citation></ref><ref id="B27-sensors-18-03782"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Y.</given-names></name><name><surname>Karl</surname><given-names>W.C.</given-names></name></person-group><article-title>A real-time algorithm for the approximationof level-set-based curve evolution</article-title><source>IEEE Trans. Image Process.</source><year>2008</year><volume>17</volume><fpage>645</fpage><lpage>656</lpage><pub-id pub-id-type="pmid">18390371</pub-id></element-citation></ref><ref id="B28-sensors-18-03782"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A.</given-names></name></person-group><article-title>Non-photorealistic rendering: Unleashing the artist&#x02019;s imagination</article-title><source>IEEE Comput. Graph. Appl.</source><year>2009</year><volume>29</volume><fpage>81</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1109/MCG.2009.61</pub-id><pub-id pub-id-type="pmid">19798865</pub-id></element-citation></ref><ref id="B29-sensors-18-03782"><label>29.</label><element-citation publication-type="web"><article-title>Institute of Artificial Intelligence and Robotics at Xi&#x02019;an Jiaotong University in China</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://trafficdata.xjtu.edu.cn/index.do">http://trafficdata.xjtu.edu.cn/index.do</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2018-11-05">(accessed on 5 November 2018)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig id="sensors-18-03782-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Flow diagram of the superpixel-level random walk algorithm at frame <italic>t</italic>.</p></caption><graphic xlink:href="sensors-18-03782-g001"/></fig><fig id="sensors-18-03782-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Definition of seed trapezoid for road and foreground regions. The green and blue colors denote the respective regions.</p></caption><graphic xlink:href="sensors-18-03782-g002"/></fig><fig id="sensors-18-03782-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Optical flow map computed by different methods. (<bold>a</bold>) Lucas&#x02013;Kanade; (<bold>b</bold>) Horn&#x02013;Schunck; (<bold>c</bold>) Bruhn.</p></caption><graphic xlink:href="sensors-18-03782-g003"/></fig><fig id="sensors-18-03782-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Region refinement based on the FTC level set. (<bold>a</bold>) the superpixel-level RW results; (<bold>b</bold>) the original region contours; (<bold>c</bold>) the refined region contours.</p></caption><graphic xlink:href="sensors-18-03782-g004"/></fig><fig id="sensors-18-03782-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Scene stages of traffic scenes. (<bold>a</bold>) through lane; (<bold>b</bold>) left-turn lane; (<bold>c</bold>) right-turn lane; (<bold>d</bold>) intersection; (<bold>e</bold>) tunnel.</p></caption><graphic xlink:href="sensors-18-03782-g005"/></fig><fig id="sensors-18-03782-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Generation of control points. <bold>(a)</bold> labeling of control points; (<bold>b</bold>) scene modeling results.</p></caption><graphic xlink:href="sensors-18-03782-g006"/></fig><fig id="sensors-18-03782-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>New viewpoint images rendering. (<bold>a</bold>) the 3D traffic scene model. The black mesh denotes the background scene, while the green mesh denotes the foreground vehicle; (<bold>b</bold>) the control of view angles; (<bold>c</bold>) the new viewpoint images.</p></caption><graphic xlink:href="sensors-18-03782-g007"/></fig><fig id="sensors-18-03782-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Region detection results based on the RW method (TSD-max dataset). (<bold>a</bold>) input image sequence; (<bold>b</bold>&#x02013;<bold>d</bold>): superpixel segmentation with superpixel numbers of <italic>N</italic> = 40, <italic>N</italic> = 200, and <italic>N</italic> = 500, respectively. (<bold>e</bold>) optical flow map between adjacent frames; (<bold>f</bold>) detection of foreground regions; (<bold>g</bold>) detection of both foreground and road regions; (<bold>h</bold>) region refinement by FTC level set.</p></caption><graphic xlink:href="sensors-18-03782-g008"/></fig><fig id="sensors-18-03782-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Road region detection results based on an RW method with refinement (KITTI dataset).</p></caption><graphic xlink:href="sensors-18-03782-g009"/></fig><fig id="sensors-18-03782-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Construction of scene models. (<bold>a</bold>) traditional scene models; (<bold>b</bold>) cartoon scene models.</p></caption><graphic xlink:href="sensors-18-03782-g010"/></fig><fig id="sensors-18-03782-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Comparison of 3D traffic scene construction methods. (<bold>a</bold>) input images; (<bold>b</bold>) proposed method; (<bold>c</bold>) Make3D; (<bold>d</bold>) photo pop-up.</p></caption><graphic xlink:href="sensors-18-03782-g011"/></fig><table-wrap id="sensors-18-03782-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03782-t001_Table 1</object-id><label>Table 1</label><caption><p>Overview of the previous studies.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
<bold>Region Detection</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"/><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Lu&#x02019;s [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Siogkas&#x02019; [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Teichmann&#x02019;s [<xref rid="B12-sensors-18-03782" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ResNet101 [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ours</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Superpixel pre-processing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spatio-temporal features</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For image Sequence</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>Scene Construction</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"/><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Horry&#x02019;s [<xref rid="B19-sensors-18-03782" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Saxena&#x02019;s [<xref rid="B16-sensors-18-03782" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hoiem&#x02019;s [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lou&#x02019;s [<xref rid="B21-sensors-18-03782" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0201c;floor-wall&#x0201d; geometry</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scene stage</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For image sequence</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03782-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03782-t002_Table 2</object-id><label>Table 2</label><caption><p>Feature descriptors of superpixels (superpixel cliques).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature Descriptors</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature Numbers</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Color</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>9</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;C1 RGB color</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;C2 HSV color</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;C3 CIELAB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Texture</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>62</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;T1 Gabor filters: 4 scales, 6 orientations</td><td align="center" valign="middle" rowspan="1" colspan="1">48</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;T2 Local binary pattern: 3 &#x000d7; 3 template</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;T3 Edge histogram descriptors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Locations and Shapes</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>6</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;L1 Location: Normalized <italic>x</italic> and <italic>y</italic> coordinates</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;L2 Shapes: Superpixel number in the clique</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;L3 Shapes: Edge number within convex hull</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;L4 Shapes: Ratio of the pixels to the convex hull</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;L5 Shapes: Whether the clique is continuous</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03782-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03782-t003_Table 3</object-id><label>Table 3</label><caption><p>Evaluation metrics of unmanned vehicles.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"/><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">L0</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">L1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">L2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">L3</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pedestrian detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Partially detect</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mostly detect</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully detect</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perfectly detect</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">collision avoidance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully collide</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Always collide</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Partially collide</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perfectly avoid</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Traffic signal recognition</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Partially identify</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mostly identify</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully identify</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perfectly identify</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pavement identification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not at all</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Misidentify</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recognize</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perfectly recognize</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain and fog identification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not at all</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Misidentify</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recognize</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perfectly recognize</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03782-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03782-t004_Table 4</object-id><label>Table 4</label><caption><p>Evaluation of road detection results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasets</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F-Score</th></tr></thead><tbody><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">White Car</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8650</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8720</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8673</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9040</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9150</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9076</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9280</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9320</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9232</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9410</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9370</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG16 [<xref rid="B15-sensors-18-03782" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9425</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9500</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9450</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet101 [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9480</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9560</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9507</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9400</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9420</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9407</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9520</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9550</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9530</bold>
</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Gray Truck</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8550</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8680</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8593</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9180</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9100</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9153</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9220</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9280</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9240</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9300</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9330</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9310</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG16 [<xref rid="B15-sensors-18-03782" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9355</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9500</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9403</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet101 [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9480</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9400</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9453</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9400</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9367</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9460</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9420</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9447</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Red Truck</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8700</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8750</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8717</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9120</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9050</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9097</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9300</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9360</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9320</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9380</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9420</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9393</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG16 [<xref rid="B15-sensors-18-03782" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9290</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9310</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet101 [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9450</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9420</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9440</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9400</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9367</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9440</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9480</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9453</bold>
</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">KITTI</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7900</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7520</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7769</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8250</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8330</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8276</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8580</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8930</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8694</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8895</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9050</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8946</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG16 [<xref rid="B15-sensors-18-03782" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9110</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9050</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9090</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet101 [<xref rid="B14-sensors-18-03782" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9225</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9308</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9220</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9155</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9198</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9360</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9350</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9357</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03782-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03782-t005_Table 5</object-id><label>Table 5</label><caption><p>Evaluation of foreground detection results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasets</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F-Score</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">White Car</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.825</td><td align="center" valign="middle" rowspan="1" colspan="1">0.865</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8379</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.890</td><td align="center" valign="middle" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8940</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9125</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9290</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9179</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9315</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9327</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9400</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9280</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9360</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9480</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9575</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9511</bold>
</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Gray Truck</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8570</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8785</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8640</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8580</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8690</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8616</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9155</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9225</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9178</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9280</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9310</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9290</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9305</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9320</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9310</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9385</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9490</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9420</bold>
</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Red Truck</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8660</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8725</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8682</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MRF [<xref rid="B7-sensors-18-03782" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8610</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8650</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8623</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GrowCut [<xref rid="B8-sensors-18-03782" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9235</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9320</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9263</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Siogkas [<xref rid="B9-sensors-18-03782" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9345</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9380</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9357</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SRW</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9390</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9410</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9397</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRW+Refine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9455</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9520</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9477</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03782-t006" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03782-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparisons of scene models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"/><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Make3D [<xref rid="B16-sensors-18-03782" ref-type="bibr">16</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Photo Pop-Up [<xref rid="B17-sensors-18-03782" ref-type="bibr">17</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Proposed</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Correct Planes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>94%</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Correct Models</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96%</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>