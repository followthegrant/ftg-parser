<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30353143</article-id><article-id pub-id-type="pmc">6199340</article-id><article-id pub-id-type="publisher-id">33819</article-id><article-id pub-id-type="doi">10.1038/s41598-018-33819-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Detecting long-lived autodependency changes in a multivariate system via change point detection and regime switching models</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Cabrieto</surname><given-names>Jedelyn</given-names></name><address><email>Jed.Cabrieto@kuleuven.be</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Adolf</surname><given-names>Janne</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Tuerlinckx</surname><given-names>Francis</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Kuppens</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Ceulemans</surname><given-names>Eva</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0668 7884</institution-id><institution-id institution-id-type="GRID">grid.5596.f</institution-id><institution>Research Group of Quantitative Psychology and Individual Differences, </institution><institution>KU Leuven &#x02013; University of Leuven, </institution></institution-wrap>Leuven, Belgium </aff></contrib-group><pub-date pub-type="epub"><day>23</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>8</volume><elocation-id>15637</elocation-id><history><date date-type="received"><day>27</day><month>4</month><year>2018</year></date><date date-type="accepted"><day>7</day><month>10</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Long-lived simultaneous changes in the autodependency of dynamic system variables characterize crucial events as epileptic seizures and volcanic eruptions and are expected to precede psychiatric conditions. To understand and predict such phenomena, methods are needed that detect such changes in multivariate time series. We put forward two methods: First, we propose KCP-AR, a novel adaptation of the general-purpose KCP (Kernel Change Point) method. Whereas KCP is implemented on the raw data and does not shed light on which parameter changed, KCP-AR is applied to the running autocorrelations, allowing to focus on changes in this parameter. Second, we revisit the regime switching AR(1) approach and propose to fit models wherein only the parameters capturing autodependency differ across the regimes. We perform a simulation study comparing both methods: KCP-AR outperforms regime switching AR(1) when variables are uncorrelated, while the latter is more reliable when multicolinearity is severe. Regime switching AR(1), however, may yield recurrent switches even when the change is long-lived. We discuss an application to psychopathology data where we investigate whether emotional inertia -the autodependency of affective states- changes before a relapse into depression.</p></abstract><funding-group><award-group><funding-source><institution>Fund for Scientific Research-Flanders (FWO, Project No. G.0582.14) Research Council of KU Leuven (GOA/15/003)</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">In many scientific fields &#x02013;ranging from financial economics and signal processing over ecology<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref></sup> to emotion and abnormal psychology<sup><xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>&#x02013;multivariate time series are collected to study how a system changes over time. Of the many features of a system that can be studied (e.g., mean, variance, correlation), a crucial one is its temporal dependency, reflecting how well system scores at specific time points can be predicted by scores at earlier time points. Temporal dependency is important as it provides insight into the system&#x02019;s memory and predictability<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref></sup> and even resilience<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>. For instance, emotional inertia, the tendency of feelings to carry over form one moment to the next, captures self-predictability of emotion and resistance to change, and has been found to be higher for individuals with low emotion-regulation skills<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> and predictive for the development of depression<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>.</p><p id="Par3">The simplest form of temporal dependency is autodependency, measuring the extent to which a variable can be predicted by itself at the previous moment. If the time intervals between observations are (almost) equally large, two statistical approaches can be employed to estimate autodependency. First, one can simply compute the autocorrelation, by calculating the correlation between a variable and its lagged version. The second approach is to fit an autoregressive lag one (AR(1)) model<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, yielding a regression coefficient that quantifies autodependency (possibly on top of other model parameters). This coefficient will usually be almost equal to the autocorrelation.</p><p id="Par4">Although most applications assume that the autocorrelation remains constant across time, there is ample evidence that this parameter may change as well across time. For instance, emotional inertia was demonstrated to change in response to triggers such as social stress<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> or isolation<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, and to increase before the onset of depression<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. During sleep, the autocorrelation of EEG signals dramatically drops at the start of the REM phase<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. The autocorrelation of seismic signals decreases abruptly during a volcanic eruption<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. The onset of epileptic seizures is characterized by a strong increase in the autocorrelation of EEG scores<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. So far, these changes have mostly been documented using knowledge about their timing (when did the volcano erupt or did a stressful event happen). However, this timing is often unknown. To predict crucial events and advance knowledge on the underpinnings of the phenomena of interest, methods that can reliably capture unknown autocorrelation changes are called for.</p><p id="Par5">In this paper, we therefore aim to propose and evaluate two methods for screening multivariate time series for the presence of long-lived changes (i.e., changes that last for multiple successive time points) in the autocorrelation of at least one of the monitored variables: a non-parametric change point detection method and a regime switching AR(1) approach. Regarding the first, we will tailor KCP (Kernel Change Point)<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, a general-purpose change point detection method that can signal changes in &#x02013;in principle- any parameter, so that it focuses on autocorrelation changes. KCP locates change points by pooling subsequent time points that are characterized by similar distributions into a phase. To target autocorrelation changes, we propose to implement KCP to the running autocorrelations instead of the raw data. These running autocorrelations are derived by sliding a window across the time series and computing the autocorrelation in each window. The regime switching AR(1) approach<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, allows the time series to recurrently switch between <italic>R</italic> different autodependency regimes. This switching is governed by a hidden Markov process that specifies the switching probabilities between any two regimes. Next to recurrent short-lived switches, the method can also capture longer-lasting changes, which is of interest in this paper.</p><p id="Par6">While the performance of KCP and regime switching methods in detecting mean changes is well studied<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>, their performance in capturing changes in autocorrelations was not evaluated so far. In the next section, we will first demonstrate the key ideas of both methods through a toy example. Next, we report the results of an extensive simulation study, where the autocorrelation change may involve all or only a subset of the variables. In the simulations, we manipulated the size of the autocorrelation changes, the number of change points and the number of (noise) variables. We focus on evaluating the performance in terms of sensitivity (i.e., the power to detect autocorrelation changes) and deal with specificity (i.e., whether and how they are affected by changes in other parameters) in the discussion. We end by reanalyzing psychopathological data from Wichers <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Plotting running autocorrelations, these authors found an increase in emotional inertia before the patient relapses to depression. We will confirm whether the data contain a significant autocorrelation change indeed, and if so, whether this change precedes depression relapse.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Toy Example</title><p id="Par7">We simulated a trivariate time series from a multivariate normal distribution with zero means, variances of one, and all correlations equal to zero. The time series consists of three phases of 100 time points each (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). In the first phase the variables have zero autocorrelations, in the second phase, an autocorrelation of 0.50 was introduced to all the variables, and in the third phase all autocorrelations become zero again. Because of the autocorrelation change in the second phase, we expect the variance to be somewhat higher in this phase.<fig id="Fig1"><label>Figure 1</label><caption><p>The raw scores (<bold>a</bold>) and running autocorrelations (<bold>b</bold>) for the toy example. A trivariate time series comprised of 300 time points was simulated from a multivariate normal distribution with zero means, unit variances and zero cross-correlations. Three phases are simulated with 100 time points each. In Phase 1, all autocorrelations are equal to zero. In Phase 2, a change was introduced such that all autocorrelations shifted to 0.50. In Phase 3, all autocorrelations reverted to zero. The different phases are indicated by the varying background shading.</p></caption><graphic xlink:href="41598_2018_33819_Fig1_HTML" id="d29e344"/></fig></p><sec id="Sec4"><title>KCP-AR (Kernel change point detection on the running autocorrelations)</title><p id="Par8">KCP-AR detects autocorrelation changes, by implementing KCP, a non-parametric technique proposed by Arlot <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, on the running autocorrelations. Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref> shows the running autocorrelations for the toy example, computed within a window of size 25 that is slid across the time series. The time point that we associate with the autocorrelation computed in a window is the median of the time points included. Other possible choices can be made but this choice is optimal in case a change point is present as it would correspond to a window containing an equal number of time points from both phases, maximizing the chance of detecting it. To locate <italic>K</italic> change points, these running autocorrelations are pooled in <italic>K</italic>&#x02009;+&#x02009;1 phases of subsequent values. These phases are as homogeneous as possible, as is quantified by a within-phase variance measure <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq1.gif"/></alternatives></inline-formula>. This measure is based on pairwise similarities between the running autocorrelations in different windows, computed using the Gaussion Kernel function. In Section 4, a detailed description of the computation and optimization of <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq2.gif"/></alternatives></inline-formula>&#x000a0;is provided. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref> displays the <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq3.gif"/></alternatives></inline-formula>-values obtained for the toy example letting <italic>K</italic> range from 0&#x000a0;to 10, and the corresponding optimal change point locations.<fig id="Fig2"><label>Figure 2</label><caption><p>KCP-AR analysis for the toy example. In (<bold>a</bold>), the optimal change point locations and the within-phase variance, <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq4.gif"/></alternatives></inline-formula>, for a given K are displayed. In (<bold>b</bold>), the grid search to find the most optimal K is exhibited.</p></caption><graphic xlink:href="41598_2018_33819_Fig2_HTML" id="d29e491"/></fig></p><p id="Par9">To decide on the adequate number of change points, KCP-AR employs two steps: In the first step, a significance test is conducted on the <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq5.gif"/></alternatives></inline-formula>&#x02019;s to decide whether the time series contains at least one significant autocorrelation change point. If this test is significant, we move on to step 2 and perform a grid search in which we linearly increment the penalty coefficient, <italic>C</italic>, proposed by Arlot <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and retain the <italic>K</italic>-value that is most often selected across the considered <italic>C</italic>-values. Also, these steps will be elaborated in Section 4. For the toy example, a highly significant result was obtained in step 1 (p-value&#x02009;=&#x02009;0), thus, we proceeded with the grid search (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>) showing that the most frequently returned and therefore selected <italic>K</italic>-value is 2. Going back to the optimal change point locations in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>, the corresponding change point locations are <italic>T</italic>&#x02009;=&#x02009;103 and <italic>T</italic>&#x02009;=&#x02009;203. KCP-AR, therefore, successfully located the two autocorrelation change points at <italic>T</italic>&#x02009;=&#x02009;101 and <italic>T</italic>&#x02009;=&#x02009;201, with a minimal delay of 2 time points.</p></sec><sec id="Sec5"><title>Regime Switching AR(1) Model</title><p id="Par10">Next, we fitted a regime switching AR(1) model to the toy data, using the dynr package<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and estimated 1 up to 3 regimes. The only model parameters that were allowed to change across the estimated regimes are the regression coefficients of the three variables. Thus, each observation, <bold><italic>X</italic></bold><sub><italic>i</italic></sub>, in the toy example was modelled as,<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i}={\boldsymbol{\alpha }}+{{\boldsymbol{\phi }}}^{r}{{\boldsymbol{X}}}_{i-1}+{{\boldsymbol{\varepsilon }}}_{i}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">&#x003b1;</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003d5;</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b5;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <bold><italic>&#x003b1;</italic></bold> is the vector of intercepts, which we fixed to 0, <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{\phi }}}^{r}$$\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003d5;</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq6.gif"/></alternatives></inline-formula> is the vector of regime dependent regression parameters, and <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{\varepsilon }}}_{{\boldsymbol{i}}}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b5;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq7.gif"/></alternatives></inline-formula> is the vector of innovations which are assumed to be independently sampled from a multivariate normal distribution with means equal to zero and covariance matrix, <bold>&#x02211;</bold>.</p><p id="Par11">The optimal number of regimes can be decided on using the AIC and BIC values. As could be expected on how the data were generated, the model with two regimes yields the lowest AIC and BIC values (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>) and should therefore be retained. To determine how well the method captures the location of the autocorrelation changes, we examined to which regimes the different time points are assigned. This assignment can be obtained by applying Bayes&#x02019; rule to the posterior probabilities with which a time point belongs to the different regimes (i.e., the time point is assigned to the regime with the highest posterior probability). Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref> exhibits these assignments for the toy example, showing that the majority of the time points at the beginning (until <italic>T</italic>&#x02009;=&#x02009;98) and at the end of the time series (from <italic>T</italic>&#x02009;=&#x02009;200) are classified into Regime 1, while those in the middle are assigned to Regime 2. Both changes are very proximal to the two real change points at <italic>T</italic>&#x02009;=&#x02009;101 and <italic>T</italic>&#x02009;=&#x02009;201. The regime switching AR(1) model with two regimes, therefore successfully recovered the abrupt autocorrelation changes we induced in the data. Note that it may happen, however, that the regime assignments exhibit very short-lived switches, which would suggest that no important longer-lasting changes occur.<fig id="Fig3"><label>Figure 3</label><caption><p>Regime switching AR(1) analysis for the toy example. In (<bold>a</bold>), the AIC and BIC values for the fitted models are tabulated, and in (<bold>b</bold>), the regime assignment for each time point&#x000a0;based on the best model is displayed.</p></caption><graphic xlink:href="41598_2018_33819_Fig3_HTML" id="d29e694"/></fig></p></sec></sec><sec id="Sec6"><title>Simulation Study</title><p id="Par12">The toy example demonstrated that both methods can detect autocorrelation change points. We now report the results of an extensive simulation study considering cases with one or two change points, to further compare their sensitivity. When monitoring a single system, variables are often dependent<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, and when this collinearity is high, estimates are usually unstable (higher standard errors), resulting to lower power<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Thus, we also simulated both uncorrelated and correlated settings to investigate the robustness of the methods against this. At the end, we examined the false alarm rate by analyzing data in which the autocorrelation remains constant.</p><p id="Par13">For the single change point case, two phases comprised of 100 time points each were simulated from a multivariate normal distribution. In the first phase, all autocorrelations are equal to zero, and in the second phase, a subset of the variables become autocorrelated. In the two change points case, three phases are simulated with 100 time points each. In the first and third phases all variable have zero autocorrelation, but in the second phase a subset of them become autocorrelated. We crossed the number of change points with the following factors and generated 100 replicates for each unique combination.<list list-type="order"><list-item><p id="Par14">
<italic>Correlation between variables (fixed across all time points), r: 0, 0.60</italic>
</p></list-item><list-item><p id="Par15"><italic>Number of variables in the system, V</italic>: 1, 2, 3, 5 and 7</p></list-item><list-item><p id="Par16"><italic>Number of changing variables S</italic>: ranges from 1 (uncorrelated settings) or 2 (correlated settings) until <italic>V</italic> (the number of noise variables is V-S)</p></list-item><list-item><p id="Par17"><italic>Change in autocorrelation</italic>, <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{\Delta }}\phi $$\end{document}</tex-math><mml:math id="M18"><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>&#x003d5;</mml:mi></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq8.gif"/></alternatives></inline-formula>: <italic>0.20, 0.40, 0.60</italic></p></list-item></list></p><p id="Par18">Each simulated data set was analyzed with the regime switching AR(1) method, only allowing the regression parameters to switch, while the intercepts were set to zero and the covariance matrix was freely estimated but fixed across the time series. We always fitted 1, 2 and 3 regime models and employed AIC and BIC to choose the optimal number of regimes.</p><p id="Par19">We also analyzed each data set with KCP-AR, using a window size of 25 to derive the running correlations. This choice was based on previous studies which revealed that this window size leads to more power and yields less biased KCP correlation change points in comparison to larger windows<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. The number of change points was determined through the significance test and the grid search described above. These two steps require setting the maximum number of change points to be considered; we used 10.</p><p id="Par20">Using the number of regimes indicated by AIC/BIC and the number of KCP change points suggested by the grid search, we computed the RI (Rand Index<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>) between the regime assignments (derived from applying Bayes&#x02019; rule on the posterior regime probabilities), the estimated KCP phases and the true phase memberships. The RI lies between 0 and 1, and the closer it is to 1, the more similar are the recovered and the underlying phases. Higher RI values, therefore, imply better recovery. Note that also in case the regime switches occur frequently, the RI score can be computed.</p><p id="Par21">We focus first on the case where there is a single change point and the variables are uncorrelated (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4a</xref>). In this case, the performance of both methods is largely determined by the magnitude of the autocorrelation change. As we move from the left to the right panels, the change in autocorrelation becomes larger, and the RI&#x02019;s obtained are higher. The RI values for a shift of 0.20 are very low, implying that detecting this minimal change is difficult, regardless of the method. As expected, the presence of noise variables strongly affects detection performance. KCP-AR was generally better than regime switching AR(1), especially yielding higher RI&#x02019;s in difficult cases, where the autocorrelation change is minimal and/or noise variables outnumber the changing ones. For regime switching AR(1), AIC (in orange) proves better than BIC (in green) as it consistently yielded higher RI&#x02019;s. Regarding the number of changes extracted, both methods often indicate that no change is present when the size of the autocorrelation change is small (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>). When the size of the change increases, however, AIC retained three instead of two regimes in a considerable percentage of cases (i.e., across all conditions, the solution with three regimes was selected for 14% of the data sets on average, whereas this percentage went up to 42% in specific conditions). BIC tends to favor models with no change (one regime) for settings with many noise variables, yet it also yields too many regimes for a considerable number of data sets (i.e., up to 17% in specific conditions). KCP-AR, on the other hand, only seldom extracted too many change points (i.e., up to 7% in some settings). Most of the results in the one change point case extend to the two change points scenario (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref>), but the latter proved to be more difficult as the RI&#x02019;s were generally lower.<fig id="Fig4"><label>Figure 4</label><caption><p>Rand Indices (RI&#x02019;s) for the one change point case. RI&#x02019;s for the uncorrelated and correlated settings are displayed in (<bold>a</bold>) and (<bold>b</bold>), respectively. KCP-AR is indicated in black, and the regime switching AR (1) method is in orange for AIC and in green for BIC. The settings in the x-axis are denoted as (no. of changing variables)/(total number of variables).</p></caption><graphic xlink:href="41598_2018_33819_Fig4_HTML" id="d29e803"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Rand Indices (RI&#x02019;s) for the two change points case. RI&#x02019;s for the uncorrelated and correlated settings are displayed in (<bold>a</bold>) and (<bold>b</bold>), respectively. KCP-AR is indicated in black, and the regime switching AR (1) method is in orange for AIC and in green for BIC. The settings in the x-axis are denoted as (no. of changing variables)/(total number of variables).</p></caption><graphic xlink:href="41598_2018_33819_Fig5_HTML" id="d29e818"/></fig></p><p id="Par22">When the variables are strongly correlated (corr&#x02009;=&#x02009;0.60) throughout, an autocorrelation change of 0.20 remains difficult to detect (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4b</xref>). Except for the settings with 2 and 3 variables only, regime switching AR(1) outperforms KCP-AR, for both AIC and BIC. This result implies that KCP-AR&#x02019;s detection performance is hampered by the correlation introduced between the variables. Specifically, KCP-AR has lower power than regime switching AR(1) since often no change points are extracted (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S3</xref>). As in the uncorrelated setting, the regime switching method (AIC) yields an extra third regime for a considerable number of data sets (up to 50% in some conditions), while BIC still performs worse, often choosing one regime in difficult settings yet also prone to overextraction (up to10%) when it declares a change. For KCP-AR, overextraction occurs only seldom (up to 4%). Results for settings with two change points are very similar (see Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref> and Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S4</xref>).</p><p id="Par23">The simulation settings above were replicated, but this time, the phase sizes were reduced to 50 time points, to study how this reduction in information affects power. Results are very similar for the one change point case, where KCP-AR outperforms the regime switching method in the uncorrelated settings (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S5a</xref>), while the latter is more reliable when there is high collinearity between variables (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S5b</xref>). For the two change points case, the regime switching method (AIC) was clearly the better method, both in the uncorrelated (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S6a</xref>) and correlated (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S6b</xref>) settings. However, it should be emphasized that the power of both methods is only acceptable (&#x02265;0.80) for a very large autocorrelation change (0.60) or, for KCP-AR, if seven variables exhibited a 0.40 change (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S5a</xref>).</p><p id="Par24">Finally, we consider the results for data sets in which no autocorrelation change point is present to check how prone the methods are to false alarms. Type 1 error percentages were computed by counting the proportion of data sets for which the methods declared at least one change to be present (i.e., at least one change point for the case of KCP-AR and more than one regime for regime switching AR(1)). The same numbers of variables were considered, but this time, 500 replicates were simulated to get more reliable type 1 error percentages. Four autocorrelation levels were introduced to the time series and were kept constant all throughout (i.e., 0 and the autocorrelation levels tested above). In Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, it is evident that using BIC for regime switching AR(1) ensures a very conservative result when it comes to judging the evidence in favor of a change point. With KCP-AR, type 1 error rates are approximately around 0.05, which is the alpha value used in the KCP-AR significance test. Using AIC for the regime switching AR(1), on the other hand,&#x000a0;leads to a higher chance of yielding false alarms with type 1 error rate reaching almost 15% in some settings.<fig id="Fig6"><label>Figure 6</label><caption><p>Type I error rate. The percentage of data sets for which false alarms were obtained are plotted for KCP-AR (in black) and the regime switching method (AIC &#x02013; in orange; BIC in green). The x-axis indicates the settings which are written as (no. of changing variables)/(total number of variables).</p></caption><graphic xlink:href="41598_2018_33819_Fig6_HTML" id="d29e863"/></fig></p></sec><sec id="Sec7"><title>Illustrative example</title><p id="Par25">We now turn to empirical data regarding relapse into depression. Examining a system of depression relevant mood states that are repeatedly measured for a single patient, we aim to confirm whether the onset of depression is preceded by a change in autocorrelation, as is predicted by theories on early warning signs for depression<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>. Wichers <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> analyzed the same time series, and indeed found evidence for elevated autocorrelations. This analysis was univariate and rather descriptive, however, in that all variables were aggregated into a single sum score, and the running autocorrelation of this sum score was monitored graphically to investigate whether it exhibited a rising trend. No further testing was done to determine the significance of the observed changes. The expected autocorrelation change in this data is rather gradual<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>. However, if this change is large enough and long-lived, the methods can in principle, detect this by segmenting the data into phases with different autocorrelation levels. The timing of the detected change, though, may come later relative to the start of the increasing trend, especially for KCP-AR, which needs a considerable number of time points in a phase to declare a change point. We are interested, therefore, to test whether a significant autocorrelation change can be detected by the methods before the depression relapse, thus predicting this critical event.</p><p id="Par26">The data is obtained from an experience sampling study of 239 days involving a participant treated for major depression, who agreed to undergo a dose reduction scheme of his anti-depressant. Each day of the study, the participant was beeped 10 times to report on his momentary experiences, generating multivariate data with1474 time points. Following Wichers <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> we will focus on five of these momentary experiences: negative affect, positive affect, mental unrest, worry and suspicion. The detrended time series is displayed in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7a</xref>, where the different experimental phases (i.e., baseline, double blind, post assessment and follow-up) are indicated by the varying background shading. To evaluate the depression status, depressive symptoms were monitored on a weekly basis using the SCL-90-R depression subscale. Using E-divisive<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, another non-parametric and general-purpose change point detection method, Wichers <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> detected a critical transition point in the sum score of this subscale around Day 127 when the patient relapsed into depression. This critical transition point is indicated by the black vertical line in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7a</xref>.<fig id="Fig7"><label>Figure 7</label><caption><p>Momentary states (<bold>a</bold>) and their running autocorrelations (<bold>b</bold>). The experimental phases are indicated by the varying background shading, and the critical transition to the depressive state (Day 127) is marked by the black vertical line. Horizontal lines indicate zero autocorrelation for the corresponding variable. Range of running autocorrelations per variable: PA (&#x02212;0.21, 0.88), NA (&#x02212;0.64, 0.91), MU (&#x02212;0.35, 0.77), Worry (&#x02212;0.61, 0.99) and Suspicion (&#x02212;0.46, 0.99).</p></caption><graphic xlink:href="41598_2018_33819_Fig7_HTML" id="d29e931"/></fig></p><p id="Par27">Before implementing both methods, the data was pre-processed to ensure that the observations satisfy the AR(1) process. This was done by including only the time points for which lag 1 counterparts are available. Overnight lags were removed since the time intervals between these occasions are considerably larger. Moreover, we removed excessive outliers by replacing all scores exceeding the 3sd threshold with the maximum of the scores that stay within the threshold. This was done for each variable, separately.</p><sec id="Sec8"><title>KCP-AR Results</title><p id="Par28">The running autocorrelations were obtained using a moving window of 25 time points (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7b</xref>). For <italic>K</italic> ranging from zero&#x000a0;to ten, the location of the change points and the associated <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq9.gif"/></alternatives></inline-formula> &#x02013;values are shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8a</xref>. Next, we tested whether there is at least one significant autocorrelation change point present in the data. Since the test yields a p-value of 0.01, we conclude that there is at least one autocorrelation change point present in the data.<fig id="Fig8"><label>Figure 8</label><caption><p>KCP-AR analysis: depression data. In (<bold>a</bold>), the optimal change point locations from zero&#x000a0;up to ten change points and the corresponding within-phase variance, <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq10.gif"/></alternatives></inline-formula>, are tabulated. In (<bold>b</bold>), the grid search is shown, suggesting that extracting one change point yields an adequate description of the data.</p></caption><graphic xlink:href="41598_2018_33819_Fig8_HTML" id="d29e1014"/></fig></p><p id="Par29">Proceeding to the grid search, Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8b</xref> shows that the most stable solution is <italic>K</italic>&#x02009;=&#x02009;1, suggesting to retain one change point. The location of this single change point is at Day 86 (see Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8a</xref>), which indeed precedes the known transition at Day 127. The dominant trend in the autocorrelation changes is that the autodependence becomes stronger after the change point. The most dramatic change was exhibited by suspicion (&#x02212;0.04 to 0.38), followed by negative affect (0.14 to 0.42), positive affect (0.36 to 0.56) and worry (0.42 to 0.59). Mental unrest, on the other hand, barely changed in autocorrelation (0.39 to 0.36).</p></sec><sec id="Sec9"><title>Regime Switching AR(1) Results</title><p id="Par30">Regime switching AR(1) models were fitted using one up to five regimes, allowing only the autodependency parameters to vary per regime. In Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9a</xref>, the model with four regimes yields the lowest AIC and BIC values. However, we retained the model with two regimes as both models lead to the same overall conclusions and the only significant difference between the parameters of the four- and two-regime models pertains to the regression parameter of Suspicion. Moreover, our simulation study shows that regime switching AR(1) models can be prone to overextraction.<fig id="Fig9"><label>Figure 9</label><caption><p>Regime switching AR(1) analysis: depression data. In (<bold>a</bold>), AIC and BIC values of the fitted regime switching AR(1) models are displayed. In (<bold>b</bold>), the assigned regimes under the 2 regime solution is exhibited. Each time point is classified into either the first (blue) or the second (brown) regime based on the posterior probabilities. The black vertical line indicates the critical transition to Depression at Day 127.</p></caption><graphic xlink:href="41598_2018_33819_Fig9_HTML" id="d29e1048"/></fig></p><p id="Par31">In Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9b</xref>, we display the regime to which each time point is assigned. The majority of the time points were classified into the first regime, but some short-lived switches to the second regime can be observed, especially after Day 153 which is already several days after the critical transition at Day 127. The first regime is characterized by absence of autodependence, as the regression parameters range from &#x02212;0.06 to 0.12. In the second regime, regression parameters range from 0.65 to 0.93, implying strong autodependence. Since the switches to the second regime remain short-lived throughout, inferring a clear autocorrelation change point is not straightforward.</p></sec><sec id="Sec10"><title>Summary</title><p id="Par32">The results of both approaches do only partly concur. Both methods yield evidence that autodependency increases across time in this data. However, the regime switching AR(1) model reveals mostly short-lived changes in autocorrelation, which become more rampant only after the critical transition point, when the patient has already relapsed to depression. Using KCP-AR the conclusion aligns more with the running autocorrelations chart of Wichers <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, which suggest that the running autocorrelations rise before the known transition point.</p></sec></sec></sec><sec id="Sec11"><title>Discussion and Conclusion</title><p id="Par33">In various application fields, crucial events are conjectured to be preceded or accompanied by long-lived autocorrelation changes. To verify these conjectures and understand the underpinnings of the phenomena under study, methods are needed that allow to screen time series for such changes. Yet, the available state-of-the-art methods, regime switching AR(1) models and KCP, are general-purpose, implying that they are also sensitive to other parameter changes including the mean, variance and correlations, and were mainly tested to detect such changes. This paper adds to the literature by showing how these methods can be tailored to focus on autocorrelation changes and by evaluating their performance in this regard.</p><p id="Par34">Through the toy example, we have shown that regime switching AR(1) and KCP-AR can indeed detect abrupt autocorrelation changes. Our simulation results revealed that detection performance is influenced by the magnitude of the autocorrelation change, the number of change points, the number of noise variables, collinearity of the variables and the phase size. As expected, power increases for larger autocorrelation changes. It is worth noting that a small shift in autocorrelation (0.20) is extremely difficult to signal for both methods. When the variables are uncorrelated, KCP-AR is the more powerful method and also the most robust against noise variables. Collinearity of the variables, however, deteriorates KCP-AR&#x02019;s performance, while that of regime switching AR(1) is enhanced. For KCP-AR, the power is probably lower when the variables are correlated because there is less independent information regarding the change in autocorrelation, obfuscating the permutation test. Regime switching AR(1), in contrast, includes covariance parameters dedicated to capture relationships between the variables. In the uncorrelated settings, estimating these parameters probably led to overfitting and yielded larger information criteria values. This led to a preference for the simpler yet incorrect model, including a single regime only. Lastly, our simulation settings consistently revealed that using AIC rather than BIC yielded better regime switching AR(1) results in terms of power. However, we stress that these results may depend on the length of the time series and the event. We expect BIC to perform better for longer time series.</p><p id="Par35">For a shorter time series (phase size&#x02009;=&#x02009;50), regime switching AR(1) is generally more sensitive than KCP-AR. This can be expected since KCP-AR employs a moving window technique, thus if the sample size is very small, the number of windows is even smaller, leading to less power. We, however, remind users that for both methods, acceptable power (&#x02265;0.80) can only be achieved if the autocorrelation change is very large (0.60), or in case of a smaller effect size (e.g., 0.40), when the number of variables exhibiting the autocorrelation change is at least 7. Such dramatic changes are rather unrealistic, thus we recommend to collect more than 50 time points per a priori expected phase. We have shown that, on top of the size of the change and the sample size, many factors can influence power (e.g., number of change points, collinearity between variables, number of variables). If prior knowledge on their expected values is available, we recommend to employ this to conduct a simulation study to assess and also optimize power for the considered experiment.</p><p id="Par36">Turning to the illustrative example, both methods provided evidence for changes in autodependency, but there were also striking differences in the results, in that regime switching AR(1) did not yield a clear longer-lasting change but rather revealed some short-lived switches, whereas KCP-AR did yield one autocorrelation change. We remark that although the recovered changes for most of the variables are not very drastic (mean&#x02009;=&#x02009;0.21, max&#x02009;=&#x02009;0.43), the change point obtained was highly significant, which can be attributed to the abundance of time points for this specific data set (about 4 times longer than our one change point settings in the simulations). Going back to the differences in the obtained results, these naturally follow from the differences between the methods. As holds for all change point detection methods, KCP-AR can only locate longer-lasting change points that demarcate events occurring for a certain period<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. The regime switching AR(1) method, on the other hand, can unravel both longer-lasting switches bounded by change points and short-lived switches. This means that we cannot rule out the possibility that the true pattern of autocorrelation changes might indeed be recurrent abrupt switching. At the end, this illustrative application nicely demonstrates how findings can differ when different statistical approaches are employed, and that users should be aware of this. We also recapitulate that in employing KCP-AR or regime switching AR(1), we do not impose that the change should be strictly abrupt and the goal is mainly to capture a long-lived autocorrelation change, which can predict relapse to depression. As a future direction, we also suggest that for this data, it certainly makes sense to also consider methods that can test for gradual autocorrelation changes<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> as well as look for changes in other system features (e.g., correlation, as done by Cabrieto <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>).</p><p id="Par37">In this paper, our main goal was to evaluate the performance of the two approaches in terms of power, that is, their sensitivity to autocorrelation change. Future studies should investigate their specificity and inspect whether they can yield false detections that are driven by changes in other parameters such as the mean, variance and correlations. If this is the case, then these changes should be filtered out beforehand if the goal is to focus on locating autocorrelation change points. In our simulations, specificity is not an issue since only the autocorrelation parameter changes. In the illustrative data, mean changes were filtered out but variances and correlations can still change, and such changes might have influenced our results. For KCP-AR, we have initial evidence that such changes would not be picked up<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, however. For regime switching AR(1), this remains to be tested. A solution would be to fit regime switching models in which also the variances and/or correlations are allowed to change across regimes, but further evaluation is needed to solve the associated model selection problem (which parameters should be free to vary) and to verify whether the correct timing and the correct parameters involved in the change can be effectively recovered. One plausible issue that may arise is that when mean changes co-occur with another, differently timed parameter change, change detection may be dominated by the mean change, leaving the other changes undetected<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>.</p><p id="Par38">Because of the differences between both methods under study, we deem it necessary to enumerate both their strengths and limitations to guide users on deciding which method will better fit their data analytical needs. First, in terms of general detection performance, the two methods seem to perform comparably when the variables are uncorrelated. This is a very promising result since it implies that when the goal is detecting autocorrelation change points, two competitive methods coming from different statistical frameworks can be implemented. When variables are correlated, though, KCP-AR&#x02019;s detection performance may be hampered. One way to address this is to implement dimension reduction techniques beforehand (PCA<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>) that can transform the variables into a few uncorrelated components. Future studies can investigate whether this will improve KCP-AR&#x02019;s performance. We remark that this data-driven dimension reduction step is not implemented in the dynr package that we used to fit the regime switching AR(1) model. Extensions to state-space models, wherein the variables are mapped to latent variables, is possible, but the mapping should be known to the user a priori. We also note that both methods can be applied to each single variable separately as autodependency is essentially a univariate parameter. The advantage of such an approach would be that it allows the timing of the change point to differ across the variables. However, such an approach would be more prone to yield false alarms due to multiple testing and will also be less sensitive because the information provided by different variables is not integrated in the detection process.</p><p id="Par39">Second, the methods differ in how easy it is to infer the location of the change point marking the long-lived change(s). KCP-AR automatically yields the change point estimate. When the change is large as in the toy example, inspecting the regime assignments&#x000a0;derived from the regime switching AR(1) model can also yield straightforward change point estimates. However, as in the illustrative application, the method can also yield recurrent switches, complicating matters. One way to infer the change point in such cases is to slide a window across the time series and check at which window the shift to the next regime started to stabilize. With regards to quantifying the change occurring at a change point, in case of regime switching AR(1), one can look at the maximum likelihood estimates that are automatically generated. However, users should first scrutinize the number of regimes retained, as our simulation results revealed that even if information criteria are used to choose the best fitting model, the method can still be prone to overfitting, retaining too many regimes, and consequently, yielding too many parameters. For KCP-AR, on the other hand, auxiliary analysis is needed to compute and compare the autocorrelations per phase. This is reasonable since change point detection methods segment the series into phases with distinct parameter levels as in a piecewise constant function<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. What sets the regime switching parameter estimates and the KCP-AR phase-specific autocorrelations apart is that they are based on different subsets of the data. Regime switching AR(1) considers all time points, while the estimates for KCP-AR are always computed within a phase. If there is an obvious change point, then we would expect these estimates to be similar, and those obtained with regime switching AR(1) can be preferred since they are estimated simultaneously (given that the AR(1) assumptions hold). However, if the regime switching method yields many short-lived switches rather than regimes that are clearly separated in time, it makes sense to look at the autocorrelations based on the KCP-AR change points.</p><p id="Par40">Third, KCP-AR looks at the running autocorrelations whereas the regime switching model fits the original data. While computing these running scores KCP-AR should only take observations into account for which the preceding time point is available and the gaps between them are equal. In the illustrative example, we addressed this by choosing only the time points with lag 1 counterparts and by deleting the overnight lags. One may also consider imputation techniques<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup> to fill in the time series with plausible values. Regime switching AR(1), on the other hand, can naturally handle this unequal time interval issue by fitting a continuous-time AR(1) model<sup><xref ref-type="bibr" rid="CR44">44</xref>&#x02013;<xref ref-type="bibr" rid="CR46">46</xref></sup>.</p><p id="Par41">Finally, in terms of flexibility, both methods can be extended in many ways. Regime switching AR(1) models can be extended to handle state space models<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, non-linear dynamics<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, missing data and multiple subjects. In fact, the dynr package can accommodate all these extensions already<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. KCP-AR, on the other hand, can be implemented in data sets for which prior information on the distribution is unknown. Next to autocorrelations, KCP can be adapted to signal changes in other statistics for which no change point detection techniques are developed yet such as the distance correlation (non-linear association between vectors<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>) or Jaccard index (similarity coefficient between two binary time series<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>).</p><p id="Par42">We end by reminding readers that they can also consider alternative parametric approaches to detect autocorrelation change points, such as Auto-PARM<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. The advantages of Auto-PARM are that, similar to KCP-AR, it automatically yields change points, and similar to regime switching AR(1), it immediately returns parameter estimates. Moreover, the method offers more flexibility as it does not impose a common lag order across the phases. It is therefore possible to obtain phases with different AR orders. However, Auto-PARM would generally require a way larger sample size to achieve an acceptable power. For instance, for an autocorrelation change of 0.40, the method would require almost 5 times more time points in comparison to KCP-AR and regime switching AR(1) to achieve sufficient power. We suggest that in applications where ample time points are available, or the expected autocorrelation change is large, this method be considered because of the mentioned advantages. We, however, emphasize that this method is also sensitive to other parameter changes, and currently, unlike KCP-AR and regime switching AR(1), the software is not designed to allow users to solely focus on detecting autocorrelation changes.</p><p id="Par43">In conclusion, both KCP-AR and regime switching AR(1) can effectively capture long-lived autocorrelation changes. Our simulation results revealed that their overall performance in detecting such change points is largely comparable, but the latter may yield short-lived switches. KCP-AR performs better when variables are uncorrelated, while regime switching AR(1) is more reliable in case of strong collinearity. The regime switching method, being a full-blown modeling approach, comes with all the associated advantages such as maximum likelihood estimates for the model parameters and extensions to more complex models. KCP-AR, on the other hand, is an attractive alternative which does not impose many stringent distributional assumptions and the underlying ideas can in principle be used to detect change points in any type of summary statistic.</p></sec><sec id="Sec12"><title>Methods</title><sec id="Sec13"><title>KCP-AR</title><sec id="Sec14"><title>Locating the change points</title><p id="Par44">KCP locates change points by segmenting the time series into phases, such that the observations within each phase are as homogeneous as possible. This is carried out by examining the similarities between time points through a kernel function. In this paper, we employed the Gaussian kernel function, which is the most used kernel in the literature<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. Furthermore, our previous studies<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup> revealed that it performs reliably in change point detection. Given a time series, <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{X}}=\{{{\boldsymbol{X}}}_{1},{{\boldsymbol{X}}}_{2},\mathrm{...}{{\boldsymbol{X}}}_{n}\}$$\end{document}</tex-math><mml:math id="M24"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq11.gif"/></alternatives></inline-formula>, where each observation <bold><italic>X</italic></bold><sub><italic>i</italic></sub> represents a vector of scores on <italic>V</italic> variables, the Gaussian kernel based similarity between two time points, <bold><italic>X</italic></bold><sub><italic>i</italic></sub> and <bold><italic>X</italic></bold><sub><italic>j</italic></sub>, is given by<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Gk({{\boldsymbol{X}}}_{i},{{\boldsymbol{X}}}_{j}\,)=\exp \,(\frac{-\parallel {{\boldsymbol{X}}}_{{\boldsymbol{i}}}-{{\boldsymbol{X}}}_{j}{\parallel }^{2}}{2{h}^{2}}),$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mi>G</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width=".25em"/></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mspace width=".10em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">&#x02225;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where the bandwidth, <italic>h</italic>, is obtained by taking the median Euclidean distance between all <bold><italic>X</italic></bold><sub><italic>i</italic></sub>&#x02019;s and <bold><italic>X</italic></bold><sub><italic>j</italic></sub>&#x02019;s. The similarity takes on values close to 1 if the scores are similar, while it approaches zero when they are distant. For KCP-AR, where the goal is to focus the detection on the autocorrelation, the raw data is first pre-processed to obtain a derived time series that reflect only the fluctuations in the statistic of interest. Specifically, we move a window, <bold><italic>w</italic></bold><sub><italic>i</italic></sub>, across the time series and compute the corresponding <bold><italic>P</italic></bold><sub><italic>i</italic></sub>, the vector of autocorrelations for the <italic>V</italic> variables in the system. KCP-AR thus looks at the similarity,<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Gk({{\boldsymbol{P}}}_{i},{{\boldsymbol{P}}}_{j}\,)=\exp (\frac{-\parallel {{\boldsymbol{P}}}_{{\boldsymbol{i}}}-{{\boldsymbol{P}}}_{j}{\parallel }^{2}}{2{{h}_{P}}^{2}}),$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mi>G</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width=".25em"/></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">&#x02225;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>and then computes the intra-phase scatter<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>,<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{V}}_{{\tau }_{1},{\tau }_{2},{\tau }_{3}\mathrm{...},{\tau }_{{\rm{K}}},m}=({\tau }_{m}-{\tau }_{m-1})-\frac{1}{{\tau }_{m}-{\tau }_{m-1}}\sum _{i={\tau }_{m-1}+1}^{{\tau }_{m}}\sum _{j={\tau }_{m-1}+1}^{{{\rm{\tau }}}_{m}}\,Gk({{\boldsymbol{P}}}_{i},{{\boldsymbol{P}}}_{j}\,),$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mspace width=".25em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mspace width=".25em"/><mml:mi>G</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width=".25em"/></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where the indices, <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{1},{\tau }_{2},\mathrm{...},{\tau }_{K}$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq12.gif"/></alternatives></inline-formula>, denote the locations of the phase boundaries, <italic>m</italic> is the current phase number, and <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{m-1}+1$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{m}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq14.gif"/></alternatives></inline-formula> are the first and last time points of this phase. When the running autocorrelations included in the phase are more similar, the rightmost term of <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{V}}_{m}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq15.gif"/></alternatives></inline-formula> becomes more negative, yielding a smaller intra-phase scatter.</p><p id="Par45">To estimate the location of <italic>K</italic> change points, KCP-AR optimizes a variance criterion based on the sum of all <italic>K</italic>&#x02009;+&#x02009;1 intra-phase scatters,<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{R}({\tau }_{1},{\tau }_{2},\mathrm{...},{\tau }_{K})=\frac{1}{w}\sum _{m=1}^{K+1}\,{\hat{V}}_{{\tau }_{1},{\tau }_{2},{\tau }_{3}\mathrm{...},{\tau }_{{\rm{K}}},m},$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>w</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mspace width=".25em"/></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <italic>w</italic> denotes the total number of windows. For instance, in the simplest case where a single change point is being estimated, the criterion <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{R}({\tau }_{1})$$\end{document}</tex-math><mml:math id="M42"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq16.gif"/></alternatives></inline-formula> is given by <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{w}({\hat{V}}_{{\tau }_{1},1}+{\hat{V}}_{{\tau }_{1},2})$$\end{document}</tex-math><mml:math id="M44"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>w</mml:mi></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq17.gif"/></alternatives></inline-formula>, where the two intra-phase scatters are measured given the phases delineated by <inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{1}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq18.gif"/></alternatives></inline-formula>. KCP-AR tries all possible locations <inline-formula id="IEq19"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{1}\in \{2,\,3,\mathrm{...},w-1\}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq19.gif"/></alternatives></inline-formula>, and the time point <inline-formula id="IEq20"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\tau }}_{1}$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq20.gif"/></alternatives></inline-formula> yielding the minimum <inline-formula id="IEq21"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{R}({\tau }_{1})$$\end{document}</tex-math><mml:math id="M52"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq21.gif"/></alternatives></inline-formula> determines the change point, <inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\tau }}_{1}+1$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq22.gif"/></alternatives></inline-formula>. To locate multiple change points, the same procedure of minimizing the variance criterion is carried out such that the optimal change point locations, <inline-formula id="IEq23"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\tau }}_{1}+1$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq23.gif"/></alternatives></inline-formula>, <inline-formula id="IEq24"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\tau }}_{2}+1$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq24.gif"/></alternatives></inline-formula>, &#x02026;<inline-formula id="IEq25"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\tau }}_{K}+1$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq25.gif"/></alternatives></inline-formula>, are determined by<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\tau }}_{1},{\hat{\tau }}_{2},\ldots ,{\hat{\tau }}_{K}={\rm{\arg }}\,{\rm{\min }}\,\hat{R}({\tau }_{1},\,{\tau }_{2},\mathrm{...},{\tau }_{K})$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">min</mml:mi><mml:mspace width=".25em"/><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par46">To simplify the notations as this will prove useful in describing the next KCP steps, we denoted the minimized variance criterion for every <italic>K</italic> as <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq26.gif"/></alternatives></inline-formula>.</p></sec><sec id="Sec15"><title>Choosing the number of change points</title><p id="Par47">KCP can yield change points for any <italic>K</italic> specified by the user. However, it is not often the case that the number of change points is known a priori, implying that this number should be inferred from the data as well. Since the fit measure, <inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq27.gif"/></alternatives></inline-formula>, continues to decrease as more change points are introduced, a penalization strategy was proposed by Arlot <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> to choose a solution that balances the fit and complexity caused by adding more change points. The penalized term is given by,<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{K}={\rm{\arg }}\,{\rm{\min }}\,{\hat{R}}_{min,K}+pe{n}_{K},$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">min</mml:mi><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$pe{n}_{K}=C\frac{{V}_{max}(K+1)}{w}[1+\,{\rm{l}}{\rm{o}}{\rm{g}}(\frac{w}{K+1})]$$\end{document}</tex-math><mml:math id="M70"><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>w</mml:mi></mml:mfrac><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mi>w</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">]</mml:mo></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq28.gif"/></alternatives></inline-formula>. The penalty coefficient, <italic>C</italic>, determines the influence of the penalty term such that a larger <italic>C</italic>-value imposes a stronger penalty and would lead to less change points. The constant, <italic>v</italic><sub><italic>max</italic></sub>, is estimated by taking the empirical covariance matrix of the first (and also the last) 5% of the time series and choosing whichever is the larger. This penalization scheme, though, strongly relies on the independence assumption, yielding excessive false alarms when KCP is applied to a highly dependent time series such as the running autocorrelations. To address this, Cabrieto <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> proposed to first conduct a test that confirms whether there is at least one change point in the running statistic, and if this test is significant, the most stable solution from the penalization scheme is determined through a grid search.</p><p id="Par48">Significance test: The significance test we will employ is the variance drop test, which examines the improvement in the variance criterion caused by inducing an extra change point. The variance drop, <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}-\,{\hat{R}}_{min,K-1}$$\end{document}</tex-math><mml:math id="M72"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq29.gif"/></alternatives></inline-formula>, is computed for all <italic>K</italic>&#x02019;s, and the maximum drop is compared to that of the permuted data. If there is no change point in the data, introducing additional change points will not yield significant improvement in the variance. However, if there is strong evidence for a change point then the fit improvement for the original data will be significantly larger than that of reshuffled data. Thus, we compare the maximal <inline-formula id="IEq30"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}-{\hat{R}}_{min,K-1}$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq30.gif"/></alternatives></inline-formula> to the distribution obtained from reshuffling the data for a large number of times. In this paper, we always set the significance level to 0.05, implying that the permutation test is significant once the maximal <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{R}}_{min,K}-{\hat{R}}_{min,K-1}$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq31.gif"/></alternatives></inline-formula> exceeds the 95th percentile cut-off of the permuted distribution. We note that Cabrieto <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> proposed two sub-tests (i.e., the variance and the variance drop tests) for testing the presence of change points in the running statistics but we only employed the variance drop test since the variance test on running autocorrelations would lead to an inflated Type 1 error in cases when there is no change point but the autocorrelation is high.</p><p id="Par49">Grid Search: Once the permutation based test above is significant, the grid search is performed. Going back to the penalized term in Equation (<xref rid="Equ7" ref-type="">7</xref>), the choice of the penalty coefficient, <italic>C</italic>, directly influences the retained number of change points, <italic>K</italic>. Thus, the grid search aims to determine which <italic>K</italic> is optimal without relying too much on the choice of <italic>C</italic>. Hence, a range of <italic>C</italic>-values is plugged into the algorithm and the most stable <italic>K</italic> is identified. It proceeds by first setting <italic>C</italic> equal to 1, which will always lead to <inline-formula id="IEq32"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K={K}_{max}$$\end{document}</tex-math><mml:math id="M78"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq32.gif"/></alternatives></inline-formula>. Then, <italic>C</italic> is increased linearly by a certain increment, strengthening the influence of the penalty term. The algorithm, thus, returns decreasing <italic>K</italic>-values as <italic>C</italic> is increased. The grid search terminates when <italic>C</italic> becomes too large, yielding <italic>K</italic>&#x02009;=&#x02009;0. The retained <italic>K</italic> is the most stable solution, which is the most frequently returned <italic>K</italic> during the grid search.</p></sec></sec><sec id="Sec16"><title>Regime Switching AR(1) Model</title><p id="Par50">The regime switching AR(1) model for a time series, <inline-formula id="IEq33"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{X}}=\{{{\boldsymbol{X}}}_{1},{{\boldsymbol{X}}}_{2},\mathrm{...}{{\boldsymbol{X}}}_{n}\}$$\end{document}</tex-math><mml:math id="M80"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq33.gif"/></alternatives></inline-formula>, where each observation <bold><italic>X</italic></bold><sub><italic>i</italic></sub> represents a vector of scores on <italic>V</italic> variables, is written as<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i}={{\boldsymbol{\alpha }}}^{r}+{{\boldsymbol{\phi }}}^{r}{{\boldsymbol{X}}}_{i-1}+{{\boldsymbol{\varepsilon }}}_{i},$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b1;</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003d5;</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b5;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where the regime dependent parameters include, <bold><italic>&#x003b1;</italic></bold><sup><italic>r</italic></sup>, the vector of intercepts, <inline-formula id="IEq34"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{\phi }}}^{r}$$\end{document}</tex-math><mml:math id="M84"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003d5;</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq34.gif"/></alternatives></inline-formula>, the vector of regression parameters describing the dependence between two successive time points, and <inline-formula id="IEq35"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{\varepsilon }}}_{i}$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b5;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq35.gif"/></alternatives></inline-formula>, the vector of independent and identically distributed residuals drawn from a multivariate normal distribution with means equal to zero and also a regime dependent covariance matrix, <inline-formula id="IEq36"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{\Sigma }}}^{r}$$\end{document}</tex-math><mml:math id="M88"><mml:msup><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq36.gif"/></alternatives></inline-formula>. Given <italic>R</italic> regimes, the hidden Markov process that governs the switching between any two regimes is characterized by transition probabilities,<disp-formula id="Equa"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[\begin{array}{cccc}{p}_{11} &#x00026; {p}_{12} &#x00026; \cdots  &#x00026; {p}_{1R}\\ {p}_{21} &#x00026; {p}_{22} &#x00026; \cdots  &#x00026; {p}_{2R}\\ \vdots  &#x00026; \vdots  &#x00026; \ddots  &#x00026; \vdots \\ {p}_{R1} &#x00026; {p}_{R2} &#x00026; \cdots  &#x00026; {p}_{RR}\end{array}]$$\end{document}</tex-math><mml:math id="M90" display="block"><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>where <italic>p</italic><sub><italic>st</italic></sub> is the probability of switching from regime <italic>s</italic> to regime <italic>t</italic>, and <italic>p</italic><sub><italic>ss</italic></sub> is the probability of staying in regime <italic>s</italic>. In this paper we used the dynr package to fit the regime switching AR(1) model. This package employs the algorithm proposed by Kim and Nelson<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, which consists of five general steps<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>. The first three resemble the Kalman filter steps that yield a predicted value for every <bold><italic>X</italic></bold><sub><italic>i</italic></sub>. The last two steps are the approximation and collapsing steps which are necessary for the regime switching extension. Denoting the observation <bold><italic>X</italic></bold><sub><italic>i</italic></sub> which implies a switch from regime <italic>s</italic> to <italic>t</italic> as <inline-formula id="IEq37"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i}^{st}$$\end{document}</tex-math><mml:math id="M92"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq37.gif"/></alternatives></inline-formula>, we enumerate these steps below:<list list-type="order"><list-item><p id="Par51"><italic>Prediction step</italic>. First, the predicted value of <inline-formula id="IEq38"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i}^{st}$$\end{document}</tex-math><mml:math id="M94"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq38.gif"/></alternatives></inline-formula> is estimated based on the information from all preceding time points, <inline-formula id="IEq39"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i-1|i-1}^{s}$$\end{document}</tex-math><mml:math id="M96"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq39.gif"/></alternatives></inline-formula>. This prediction is denoted as <inline-formula id="IEq40"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i|i-1}^{st}$$\end{document}</tex-math><mml:math id="M98"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq40.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p id="Par52"><italic>One-step-ahead prediction error</italic>. Then, the actual observation <bold><italic>X</italic></bold><sub><italic>i</italic></sub> is compared to the prediction, <inline-formula id="IEq41"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i|i-1}^{st}\,$$\end{document}</tex-math><mml:math id="M100"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mspace width=".25em"/></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq41.gif"/></alternatives></inline-formula>resulting from step 1. Specifically, the one-step-ahead-prediction-error, <inline-formula id="IEq42"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{d}}}_{i}^{(s,t)}$$\end{document}</tex-math><mml:math id="M102"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq42.gif"/></alternatives></inline-formula>, is the difference between these two values.</p></list-item><list-item><p id="Par53"><italic>Update step</italic>. The prediction is updated obtaining <inline-formula id="IEq43"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i|i}$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq43.gif"/></alternatives></inline-formula>, which takes into account the actual observation, <bold><italic>X</italic></bold><sub><italic>i</italic></sub>, and the one-step-ahead-prediction error, <inline-formula id="IEq44"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{d}}}_{i}^{(s,t)}$$\end{document}</tex-math><mml:math id="M106"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq44.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p id="Par54"><italic>Approximation step</italic>. For steps 1 to 3, it is assumed that both <italic>s</italic> and <italic>t</italic> are known, but to yield the predictions, one would have to consider <italic>R</italic> switching possibilities per time point, yielding <italic>R</italic><sup><italic>n</italic></sup> trajectories for a time series with <italic>n</italic> time points. This can quickly become computationally intractable; therefore Kim<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> proposed an approximation, which requires the quantity, <inline-formula id="IEq45"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{P}}[{r}_{i-1}=s|{r}_{i}=t,{{\boldsymbol{X}}}_{1:i}]$$\end{document}</tex-math><mml:math id="M108"><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq45.gif"/></alternatives></inline-formula>, denoting the probability that the system previously belonged to regime <italic>s</italic> while it is now in regime <italic>t</italic> given all observed data until time point <italic>i</italic>, <bold><italic>X</italic></bold><sub>1:</sub><sub><italic>i</italic></sub>. This probability is estimated using the Hamilton filter<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup> for all combinations of <italic>s</italic> and <italic>t</italic>.</p></list-item><list-item><p id="Par55"><italic>Collapsing step</italic>. Finally, a collapsing step is carried out on the probability estimates from step 4 to estimate <inline-formula id="IEq46"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\boldsymbol{X}}}_{i|i}^{s}$$\end{document}</tex-math><mml:math id="M110"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq46.gif"/></alternatives></inline-formula>, which will be used in step 1 when predicting the next time point.</p></list-item></list></p><p id="Par56">To estimate the parameters of this model, the following log likelihood function is optimized,<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$log\,L=\sum _{i=1}^{n}\,log\,f({{\boldsymbol{X}}}_{i}|{{\boldsymbol{X}}}_{1:i-1})$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width=".25em"/><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mspace width=".25em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width=".25em"/><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq47"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f({{\boldsymbol{X}}}_{i}|{{\boldsymbol{X}}}_{1:i-1})$$\end{document}</tex-math><mml:math id="M114"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq47.gif"/></alternatives></inline-formula> is the likelihood of <bold><italic>X</italic></bold><sub><italic>i</italic></sub>, given all observed preceding data. This likelihood can be written as,<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f({{\boldsymbol{X}}}_{i}|{{\boldsymbol{X}}}_{1:i-1})=\sum _{s=1}^{R}\sum _{t=1}^{R}\,f({{\boldsymbol{X}}}_{i}|{r}_{i-1}=s,\,{r}_{i}=t,{{\boldsymbol{X}}}_{1:i-1})\,{\rm{P}}[{r}_{i-1}=s,\,{r}_{i}=t,|{{\boldsymbol{X}}}_{1:i-1}]$$\end{document}</tex-math><mml:math id="M116" display="block"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:mspace width=".25em"/><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width=".25em"/><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>with<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f({{\boldsymbol{X}}}_{i}|{r}_{i-1}=s,\,{r}_{i}=t,{{\boldsymbol{X}}}_{1:i-1})={(2\pi )}^{V/2}|{{\boldsymbol{\Sigma }}}^{(t)}{|}^{-1/2}exp\{-\frac{1}{2}{{\boldsymbol{d}}}_{i}^{(s,t)\text{'}}{{\boldsymbol{\Sigma }}}^{({\boldsymbol{t}})-1}\,{{\boldsymbol{d}}}_{i}^{(s,t)}\}.$$\end{document}</tex-math><mml:math id="M118" display="block"><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>'</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width=".25em"/><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41598_2018_33819_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par57">In this paper, we focused on evaluating the power to capture pure autocorrelation changes. Thus, we fitted models that only allowed for changes in the parameters in <inline-formula id="IEq48"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{\phi }}$$\end{document}</tex-math><mml:math id="M120"><mml:mi mathvariant="bold-italic">&#x003d5;</mml:mi></mml:math><inline-graphic xlink:href="41598_2018_33819_Article_IEq48.gif"/></alternatives></inline-formula>.</p></sec></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec17"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2018_33819_MOESM1_ESM.docx"><caption><p>Supplementary Figures</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Electronic supplementary material</title><p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-018-33819-8.</p></sec><ack><title>Acknowledgements</title><p>The research leading to the results reported in this paper was sponsored in part by a research grant from the Fund for Scientific Research-Flanders (FWO, Project No. G.0582.14 awarded to Eva Ceulemans, Peter Kuppens and Francis Tuerlinckx) and by the Research Council of KU Leuven (GOA/15/003). The computational resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Hercules Foundation and the Flemish Government department EWI. The authors would like to thank Kristof Meers for his help in using this supercomputer. The authors also acknowledge Marieke Wichers who shared R scripts of their analyses.</p></ack><notes notes-type="author-contribution"><title>Author Contributions</title><p>J.C. and E.C. wrote the manuscript. J.C., E.C. and F.T. proposed the KCP-AR approach and designed the simulation studies. J.A. developed the regime switching AR(1) model implementation. J.C. conducted the simulation studies and analyzed the empirical data. P.K. helped with the interpretation of the depression data. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability</title><p>All codes used are available upon request from the corresponding author. The depression data is publicly available at <ext-link ext-link-type="uri" xlink:href="https://openpsychologydata.metajnl.com/articles/10.5334/jopd.29/">https://openpsychologydata.metajnl.com/articles/10.5334/jopd.29/</ext-link>.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Competing Interests</title><p>The authors declare no competing interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Box, G. E., Jenkins, G. M., Reinsel, G. C. &#x00026; Ljung, G. M. <italic>Time series analysis: forecasting and control</italic> (John Wiley &#x00026; Sons, 2015).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Hamilton, J. <italic>Time Series Analysis</italic> (Princton University Press, 1994).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Shumway, R. H. &#x00026; Stoffer, D. S. <italic>Time series analysis and its applications</italic> (Springer Nature, 2000).</mixed-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>RA</given-names></name><name><surname>Lee</surname><given-names>T</given-names></name><name><surname>Rodriguez-Yam</surname><given-names>GA</given-names></name></person-group><article-title>Structural break estimation for nonstationary time series models</article-title><source>Journal of the American Statistical Association</source><year>2006</year><volume>101</volume><issue>473</issue><fpage>223</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1198/016214505000000745</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Basseville, M. &#x00026; Nikiforov, I. <italic>Detection of abrupt changes:Theory and application</italic> (Prentice-Hall Inc., 1993).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Darkhovsky, B. S. &#x00026; Brodsky, E. <italic>Non-Parametric Statistical Diagnosis: Problems and Methods</italic> (Springer Science &#x00026; Business Media, 2000).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Michael, S. <italic>Applied nonlinear time series analysis: applications in physics, physiology and finance</italic> (World Scientific, 2005).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Andersen, T. G., Davis, R. A., Krei&#x000df;, J. P. &#x00026; Mikosch, T. V. <italic>Handbook of financial time series</italic> (Springer Science &#x00026; Business Media, 2009).</mixed-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Small</surname><given-names>M</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name></person-group><article-title>Complex network analysis of time series</article-title><source>EPL (Europhysics Letters)</source><year>2016</year><volume>116</volume><fpage>50001</fpage><pub-id pub-id-type="doi">10.1209/0295-5075/116/50001</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pe</surname><given-names>ML</given-names></name><etal/></person-group><article-title>Emotion-network density in major depressive disorder</article-title><source>Clinical Psychological Science</source><year>2015</year><volume>3</volume><fpage>292</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1177/2167702614540645</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Leemput</surname><given-names>I</given-names></name><etal/></person-group><article-title>Critical slowing down as early warning for the onset and termination of depression</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2014</year><volume>111</volume><fpage>87</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312114110</pub-id><pub-id pub-id-type="pmid">24324144</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wichers</surname><given-names>M</given-names></name><etal/></person-group><article-title>Critical slowing down as a personalized early warning signal for depression</article-title><source>Psychotherapy and Psychosomatics</source><year>2016</year><volume>85</volume><fpage>114</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1159/000441458</pub-id><pub-id pub-id-type="pmid">26821231</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamaker</surname><given-names>EL</given-names></name><name><surname>Ceulemans</surname><given-names>E</given-names></name><name><surname>Grasman</surname><given-names>RP</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name></person-group><article-title>Modeling affect dynamics: State-of-the-art and future challenges</article-title><source>Emotion Review (Special issue: Affect Dynamics)</source><year>2015</year><volume>7</volume><fpage>316</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1177/1754073915590619</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvo</surname><given-names>RA</given-names></name><name><surname>D&#x02019;Mello</surname><given-names>S</given-names></name></person-group><article-title>Affect detection: An interdisciplinary review of models, methods, and their applications</article-title><source>IEEE Transactions on affective computing</source><year>2010</year><volume>1</volume><issue>1</issue><fpage>18</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2010.1</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dakos</surname><given-names>V</given-names></name><name><surname>Van Nes</surname><given-names>E</given-names></name><name><surname>D&#x02019;Odorico</surname><given-names>P</given-names></name><name><surname>Scheffer</surname><given-names>M</given-names></name></person-group><article-title>Robustness of variance and autocorrelation as indicators of critical slowing down</article-title><source>Ecology</source><year>2012</year><volume>93</volume><issue>2</issue><fpage>264</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1890/11-0889.1</pub-id><pub-id pub-id-type="pmid">22624308</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ives</surname><given-names>A</given-names></name></person-group><article-title>Measuring resilience in stochastic systems</article-title><source>Ecological Monographs</source><year>1995</year><volume>65</volume><issue>2</issue><fpage>217</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.2307/2937138</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuppens</surname><given-names>P</given-names></name><name><surname>Allen</surname><given-names>NB</given-names></name><name><surname>Sheeber</surname><given-names>L</given-names></name></person-group><article-title>Emotional inertia and psychological maladjustment</article-title><source>Psychological Science</source><year>2010</year><volume>21</volume><fpage>984</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1177/0956797610372634</pub-id><pub-id pub-id-type="pmid">20501521</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koval</surname><given-names>P</given-names></name><name><surname>Kuppens</surname><given-names>P</given-names></name></person-group><article-title>Changing emotion dynamics: Individual differences in the effect of anticipatory social stress on emotional inertia</article-title><source>Emotion</source><year>2012</year><volume>12</volume><fpage>256</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1037/a0024756</pub-id><pub-id pub-id-type="pmid">21787072</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bringmann</surname><given-names>LF</given-names></name><etal/></person-group><article-title>Changing dynamics: Time-varying autoregressive models using generalized additive modeling</article-title><source>Psychological methods</source><year>2017</year><volume>22</volume><issue>3</issue><fpage>409</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1037/met0000085</pub-id><pub-id pub-id-type="pmid">27668421</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steyn-Ross</surname><given-names>DA</given-names></name><etal/></person-group><article-title>The sleep cycle modelled as a cortical phase transition</article-title><source>Journal of Biological Physics</source><year>2005</year><volume>31</volume><issue>3-4</issue><fpage>547</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1007/s10867-005-1285-2</pub-id><pub-id pub-id-type="pmid">23345918</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>T&#x000e1;rraga</surname><given-names>M</given-names></name><name><surname>Mart&#x000ed;</surname><given-names>J</given-names></name><name><surname>Abella</surname><given-names>R</given-names></name><name><surname>Carniel</surname><given-names>R</given-names></name><name><surname>L&#x000f3;pez</surname><given-names>C</given-names></name></person-group><article-title>Volcanic tremors: Good indicators of change in plumbing systems during volcanic eruptions</article-title><source>Journal of Volcanology and Geothermal Research</source><year>2014</year><volume>273</volume><fpage>33</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/j.jvolgeores.2014.01.003</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Efficient unsupervised algorithms for the detection of seizures in continuous EEG recordings from rats after brain injury</article-title><source>Journal of neuroscience methods</source><year>2006</year><volume>152</volume><issue>1&#x02013;2</issue><fpage>255</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2005.09.014</pub-id><pub-id pub-id-type="pmid">16337006</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Hahn</surname><given-names>JS</given-names></name><name><surname>Heldt</surname><given-names>GP</given-names></name><name><surname>Coen</surname><given-names>RW</given-names></name></person-group><article-title>Detection of neonatal seizures through computerized EEG analysis</article-title><source>Electroencephalography and clinical neurophysiology</source><year>1992</year><volume>82</volume><issue>1</issue><fpage>30</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(92)90179-L</pub-id><pub-id pub-id-type="pmid">1370141</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Arlot, S., Celisse, A. &#x00026; Harchaoui, Z. Kernel change-point detection. <italic>Cornell University Library</italic>, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1202.3878v1">http://arxiv.org/abs/1202.3878v1</ext-link> (2012).</mixed-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>J</given-names></name></person-group><article-title>A new approach to the economic analysis of nonstationary time series and the business cycle</article-title><source>Econometrica: Journal of the Econometric Society</source><year>1989</year><volume>57</volume><issue>2</issue><fpage>357</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.2307/1912559</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Kim, C. J. &#x00026; Nelson, C. R. State-space models with regime switching: classical and Gibbs-sampling approaches with applications. In <italic>MIT Press Books</italic> (1 ed., Vol. 1). (The MIT Press, 1999).</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabrieto</surname><given-names>J</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name><name><surname>Kuppens</surname><given-names>P</given-names></name><name><surname>Grassmann</surname><given-names>M</given-names></name><name><surname>Ceulemans</surname><given-names>E</given-names></name></person-group><article-title>Detecting correlation changes in multivariate time series: A comparison of four non-parametric change point detection methods</article-title><source>Behavior Research Methods</source><year>2017</year><volume>49</volume><issue>3</issue><fpage>988</fpage><lpage>1005</lpage><pub-id pub-id-type="doi">10.3758/s13428-016-0754-9</pub-id><pub-id pub-id-type="pmid">27383753</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamaker</surname><given-names>EL</given-names></name><name><surname>Grasman</surname><given-names>RP</given-names></name></person-group><article-title>Regime switching state-space models applied to psychological processes: Handling missing data and making inferences</article-title><source>Psychometrika</source><year>2012</year><volume>77</volume><issue>2</issue><fpage>400</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1007/s11336-012-9254-8</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Ou, L., Hunter, M. &#x00026; Chow, S. What&#x02019;s for dynr: A Package for Linear and Nonlinear Dynamic Modeling in R. Journal of Statistical Software (under review).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Bulteel, K., Mestdagh, M., Tuerlinckx, F. &#x00026; Ceulemans, E. VAR(1) based models do not always outpredict AR(1) models in typical psychological applications. <italic>Psychological Methods</italic> (in press).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farrar</surname><given-names>D</given-names></name><name><surname>Glauber</surname><given-names>R</given-names></name></person-group><article-title>Multicollinearity in regression analysis: The problem revisited</article-title><source>The Review of Economics and Statistics</source><year>1967</year><volume>49</volume><issue>1</issue><fpage>92</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.2307/1937887</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabrieto</surname><given-names>J</given-names></name><etal/></person-group><article-title>Capturing correlation changes by applying kernel change point detection on the running correlations</article-title><source>Information Sciences</source><year>2018</year><volume>447</volume><fpage>117</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2018.03.010</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rand</surname><given-names>W</given-names></name></person-group><article-title>Objective criteria for the evaluation of clustering</article-title><source>Journal of the American Statistical Association</source><year>1971</year><volume>66</volume><issue>336</issue><fpage>846</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1080/01621459.1971.10482356</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>M</given-names></name><etal/></person-group><article-title>Early-warning signals for critical transitions</article-title><source>Nature</source><year>2009</year><volume>461</volume><fpage>53</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1038/nature08227</pub-id><pub-id pub-id-type="pmid">19727193</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matteson</surname><given-names>D</given-names></name><name><surname>James</surname><given-names>N</given-names></name></person-group><article-title>A nonparametric approach for multiple change point analysis</article-title><source>Journal of the American Statistical Association</source><year>2014</year><volume>109</volume><issue>505</issue><fpage>334</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1080/01621459.2013.849605</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabrieto</surname><given-names>J</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name><name><surname>Kuppens</surname><given-names>P</given-names></name><name><surname>Borb&#x000e1;la</surname><given-names>H</given-names></name><name><surname>Ceulemans</surname><given-names>E</given-names></name></person-group><article-title>Testing for the presence of correlation changes in a multivariate time series:: A permutation based approach</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><fpage>769</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-19067-2</pub-id><pub-id pub-id-type="pmid">29335504</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molenaar</surname><given-names>P</given-names></name><name><surname>Sinclair</surname><given-names>KO</given-names></name><name><surname>Rovine</surname><given-names>MJ</given-names></name><name><surname>Ram</surname><given-names>N</given-names></name><name><surname>Corneal</surname><given-names>SE</given-names></name></person-group><article-title>Analyzing developmental processes on an individual level using nonstationary time series modeling</article-title><source>Developmental psychology</source><year>2009</year><volume>45</volume><issue>1</issue><fpage>260</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1037/a0014170</pub-id><pub-id pub-id-type="pmid">19210007</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Cabrieto, J., Tuerlinckx, F., Kuppens, P. &#x00026; Ceulemans, E. The sensitivity and specificity of parameter-specific multivariate kernel change point detection. <italic>Submitted</italic>.</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Jolliffe, I. T. Principal component analysis. In <italic>Principal component analysis and factor analysis</italic>, 115&#x02013;128 (Springer, 1986).</mixed-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bulteel</surname><given-names>K</given-names></name><etal/></person-group><article-title>DeCon: A tool to detect emotional concordance in multivariate time series data of emotional responding</article-title><source>Biological Psychology</source><year>2014</year><volume>98</volume><issue>1</issue><fpage>29</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2013.10.011</pub-id><pub-id pub-id-type="pmid">24220647</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebarbier</surname><given-names>E</given-names></name></person-group><article-title>Detecting multiple change-points in the mean of Gaussian process by model selection</article-title><source>Signal Processing</source><year>2005</year><volume>85</volume><issue>4</issue><fpage>717</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2004.11.012</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Rubin, D. <italic>Multiple Imputation for Nonresponse in Surveys</italic> (John Wiley &#x00026; Sons, 1987).</mixed-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schafer</surname><given-names>JL</given-names></name><name><surname>Olsen</surname><given-names>MK</given-names></name></person-group><article-title>Multiple imputation for multivariate missing-data problems: A data analyst&#x02019;s perspective</article-title><source>Multivariate behavioral research</source><year>1998</year><volume>33</volume><issue>4</issue><fpage>545</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1207/s15327906mbr3304_5</pub-id><pub-id pub-id-type="pmid">26753828</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Bar-Shalom, Y., Li, X. R. &#x00026; Kirubarajan, T. <italic>Estimation with Applications to Tracking and Navigation: Theory Algorithms and Software</italic>, (John Wiley &#x00026; Sons, 2001).</mixed-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulikov</surname><given-names>G</given-names></name><name><surname>Kulikova</surname><given-names>M</given-names></name></person-group><article-title>Accurate Numerical Implementation of the Continuous-Discrete Extended Kalman Filter</article-title><source>IEEE Transactions on Automatic Control</source><year>2014</year><volume>59</volume><issue>1</issue><fpage>273</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1109/TAC.2013.2272136</pub-id></element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulikova</surname><given-names>M</given-names></name><name><surname>Kulikov</surname><given-names>G</given-names></name></person-group><article-title>Adaptive ODE Solvers in Extended Kalman Filtering</article-title><source>Journal of Computational and Applied Mathematics</source><year>2014</year><volume>262</volume><fpage>205</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.cam.2013.09.064</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chow</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>G</given-names></name></person-group><article-title>Nonlinear Regime-Switching State-Space (RSSS) Models</article-title><source>Psychometrika</source><year>2013</year><volume>78</volume><issue>4</issue><fpage>740</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1007/s11336-013-9330-8</pub-id><pub-id pub-id-type="pmid">24092487</pub-id></element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szekely</surname><given-names>G</given-names></name><name><surname>Rizzo</surname><given-names>M</given-names></name><name><surname>Bakirov</surname><given-names>N</given-names></name></person-group><article-title>Measuring and testing dependence by correlation of distances</article-title><source>The Annals of Statistics</source><year>2007</year><volume>35</volume><issue>6</issue><fpage>2769</fpage><lpage>2794</lpage><pub-id pub-id-type="doi">10.1214/009053607000000505</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaccard</surname><given-names>P</given-names></name></person-group><article-title>The Distribution of the Flora in the Alpine Zone</article-title><source>New Phytologist</source><year>1912</year><volume>11</volume><issue>2</issue><fpage>37</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8137.1912.tb05611.x</pub-id></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sriperumbudur</surname><given-names>B</given-names></name><name><surname>Gretton</surname><given-names>A</given-names></name><name><surname>Fukumizu</surname><given-names>K</given-names></name><name><surname>Lanckriet</surname><given-names>G</given-names></name><name><surname>Scholkopf</surname><given-names>B</given-names></name></person-group><article-title>Hilbert Space embeddings and metrics on probability measures</article-title><source>Journal of Machine Learning Research</source><year>2010</year><volume>11</volume><fpage>1517</fpage><lpage>1561</lpage></element-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Harchaoui, Z. &#x00026; Cappe, O. Retrospective multiple change-point estimation with kernels. <italic>Statistical Signal Processing</italic>, IEEE/SP 14th Workshop, 768&#x02013;772 (2007).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Hamaker, E. L., Grasman, R. P. &#x00026; Kamphuis, J. H. Regime-switching models to study psychological processes. In Molenaar, P. C. &#x00026; Newell, K. M. (Eds), <italic>Individual pathways of change: Statistical models for analyzing learning and development</italic>, 155&#x02013;168 (American Psychological Association, 2010).</mixed-citation></ref><ref id="CR53"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C</given-names></name></person-group><article-title>Dynamic linear models with Markov-switching</article-title><source>Journal of Econometrics</source><year>1994</year><volume>60</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/0304-4076(94)90036-1</pub-id></element-citation></ref></ref-list></back></article>