<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychology</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">21738516</article-id><article-id pub-id-type="pmc">3125532</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2011.00041</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>How Different is Different? Criterion and Sensitivity in Face-Space</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hill</surname><given-names>Harold</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib><contrib contrib-type="author"><name><surname>Claes</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Corcoran</surname><given-names>Michelle</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Walters</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Johnston</surname><given-names>Alan</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib><contrib contrib-type="author"><name><surname>Clement</surname><given-names>John Gerald</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>School of Psychology, University of Wollongong</institution><country>Wollongong, NSW, Australia</country></aff><aff id="aff2"><sup>2</sup><institution>Melbourne Dental School, University of Melbourne</institution><country>Melbourne, VIC, Australia</country></aff><aff id="aff3"><sup>3</sup><institution>Cranio-Maxillo-Facial Unit, Princess Margaret Hospital for Children</institution><country>Perth, WA, Australia</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Cognitive, Perceptual and Brain Sciences, University College London</institution><country>London, UK</country></aff><aff id="aff5"><sup>5</sup><institution>CoMPLEX, University College London</institution><country>London, UK</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Frans Verstraten, Universiteit Utrecht, Netherlands</p></fn><fn fn-type="edited-by"><p>Reviewed by: Alice O'Toole, University of Texas at Dallas, USA; Nicholas Costen, Manchester Metropolitan University, UK</p></fn><corresp id="fn001">*Correspondence: Harold Hill, School of Psychology, University of Wollongong, Wollongong, NSW 2522, Australia. e-mail: <email>harry@uow.edu.au</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Frontiers in Perception Science, a specialty of Frontiers in Psychology.</p></fn></author-notes><pub-date pub-type="epreprint"><day>25</day><month>1</month><year>2011</year></pub-date><pub-date pub-type="epub"><day>23</day><month>3</month><year>2011</year></pub-date><pub-date pub-type="collection"><year>2011</year></pub-date><volume>2</volume><elocation-id>41</elocation-id><history><date date-type="received"><day>21</day><month>12</month><year>2010</year></date><date date-type="accepted"><day>28</day><month>2</month><year>2011</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2011 Hill, Claes, Corcoran, Walters, Johnston and Clement.</copyright-statement><copyright-year>2011</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><license-p>This is an open-access article subject to an exclusive license agreement between the authors and Frontiers Media SA, which permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.</license-p></license></permissions><abstract><p>Not all detectable differences between face images correspond to a change in identity. Here we measure both sensitivity to change and the criterion difference that is perceived as a change in identity. Both measures are used to test between possible similarity metrics. Using a same/different task and the method of constant stimuli criterion is specified as the 50% &#x0201c;different&#x0201d; point (P50) and sensitivity as the difference limen (DL). Stimuli and differences are defined within a &#x0201c;face-space&#x0201d; based on principal components analysis of measured differences in three-dimensional shape. In Experiment 1 we varied views available. Criterion (P50) was lowest for identical full-face view comparisons that can be based on image differences. When comparing across views P50, was the same for a static 45&#x000b0; change as for multiple animated views, although sensitivity (DL) was higher for the animated case, where it was as high as for identical views. Experiments 2 and 3 tested possible similarity metrics. Experiment 2 contrasted Euclidean and Mahalanobis distance by setting PC1 or PC2 to zero. DL did not differ between conditions consistent with Mahalanobis. P50 was lower when PC2 changed emphasizing that perceived changes in identity are not determined by the magnitude of Euclidean physical differences. Experiment 3 contrasted a distance with an angle based similarity measure. We varied the distinctiveness of the faces being compared by varying distance from the origin, a manipulation that affects distances but not angles between faces. Angular P50 and DL were both constant for faces from 1 to 2 SD from the mean, consistent with an angular measure. We conclude that both criterion and sensitivity need to be considered and that an angular similarity metric based on standardized PC values provides the best metric for specifying what physical differences will be perceived to change in identity.</p></abstract><kwd-group><kwd>face recognition</kwd><kwd>criterion</kwd><kwd>sensitivity</kwd><kwd>face-space</kwd><kwd>three-dimensional shape</kwd><kwd>viewpoint</kwd><kwd>distinctiveness</kwd></kwd-group><counts><fig-count count="6"/><table-count count="3"/><equation-count count="1"/><ref-count count="69"/><page-count count="14"/><word-count count="10980"/></counts></article-meta></front><body><sec sec-type="introduction"><title>Introduction</title><p>Being able to detect differences between different face images is critical for recognition. However not all detectable changes correspond to a change in identity. In this paper we measure the criterion difference people use when making &#x0201c;same or different identity?&#x0201d; judgments, with criterion specified in terms of physical differences in three-dimensional (3D) shape of the face itself. The criterion corresponds to the physical difference above which people will tend to respond &#x0201c;different&#x0201d; and below which they will tend to respond &#x0201c;same.&#x0201d; We also measure sensitivity to determine the extent to which these measures co-vary. For example in Experiment 1 we vary viewpoint, a manipulation well-known to affect sensitivity but one which does not affect underlying differences in shape. Both criterion and sensitivity are tested and defined within a &#x0201c;face-space&#x0201d; based on a principal components analysis (PCA) of measured variations in 3D face shape and, in Experiments 2 and 3, differences between conditions allow us to test between alternative distance metrics for relating physical to perceived differences.</p><p>Many previous studies have addressed our ability to detect changes between faces. For example Freire et al. (<xref ref-type="bibr" rid="B19">2000</xref>) examined detection of changes in facial features or the spacing between those features. Some of the motivation for the current study comes from the observation that examples of the stimuli in this and many other studies can often look like the same <italic>person</italic> even when there are clearly detectable <italic>image</italic> differences (e.g., Figure <xref ref-type="fig" rid="F1">1</xref>; Freire et al., <xref ref-type="bibr" rid="B19">2000</xref>). The instructions given in these studies encourage participants to respond on the basis of <italic>any</italic> difference by emphasizing that &#x0201c;faces would be quite similar in appearance&#x0201d; (Freire et al., <xref ref-type="bibr" rid="B19">2000</xref>) or &#x0201c;kind of like twins&#x0201d; (Mondloch et al., <xref ref-type="bibr" rid="B41">2002</xref>) and &#x0201c;Correct&#x0201d; responses are defined in terms of the presence or absence of <italic>any</italic> physical difference. Additionally, comparisons are made between identical views &#x02013; examples are often modified images &#x02013; allowing image, rather than face, based. Detecting differences between images is necessary for recognition and the studies cited have made critical contributions to understanding featural and configural information. Here we measure the magnitude of differences that are perceived as a change in identity as well as measuring sensitivity to differences.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>(A)</bold> Examples of the stimuli shown in the FF condition of Experiment 1. From left to right, the average reference face and 0.3, 0.6, and 0.9 SD comparison faces. The P50 observed in this condition was 0.33 SD, just to the right of the second face from the left. <bold>(B)</bold> Two-dimensional illustration of face-space used for Experiment 1. Black points show locations of the 54 faces used to define the face-space. The locations of the average reference face and the 0.3, 0.6, and 0.9 SD comparison faces illustrated in <bold>(A)</bold> are also indicated. The blue diagonals and points represent possible locations of the comparison faces used and for which Euclidean and Mahalanobis distances are equivalent. Please note that in the 22 dimensional space used mean Mahalanobis distance from the average of the 54 actual faces was 4.6 SD&#x02009;&#x02248;&#x02009;&#x0221a;(22) cf (Burton and Vokey, <xref ref-type="bibr" rid="B9">1998</xref>).</p></caption><graphic xlink:href="fpsyg-02-00041-g001"/></fig><p>This distinction mirrors that made between sensitivity and bias in signal detection theory. Same/different and other decisions are co-determined by both sensitivity, the ability to discriminate, and criterion, the point on the decision axis where responses change (Macmillan and Creelman, <xref ref-type="bibr" rid="B33">2005</xref>). It is criterion that determines bias, the tendency to prefer one response over the other. In low level psychophysics sensitivity is normally the focus and the same/different task avoided precisely because it is subject to bias. Instead other tasks including two-alternative forced choice (2-AFC) and odd-one-out are used that are designed to minimize bias. Correct responses can be made on the basis of any difference and bias reflects an arbitrary tendency to prefer one response over another. These tasks have been applied to measuring sensitivity to differences between faces (Rhodes et al., <xref ref-type="bibr" rid="B52">2007</xref>; Dakin and Omigie, <xref ref-type="bibr" rid="B13">2009</xref>; Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>) but, again, stimuli often look like the same person even when the images can be discriminated. As Rhodes et al. (<xref ref-type="bibr" rid="B52">2007</xref>) point out &#x0201c;only in the case of identical twins, do we need to determine whether two faces are truly identical or not.&#x0201d; While those authors go on to use suprathreshold measures of sensitivity, we use the criterion difference inherent to a same/different decision to estimate the magnitude of physical different that is perceived as a change in identity.</p><p>Bias has been reported in previous experiments on face matching and recognition. For example there is a bias to respond &#x0201c;same&#x0201d; when matching a photograph to a live individual (Kemp et al., <xref ref-type="bibr" rid="B26">1997</xref>; Megreya and Burton, <xref ref-type="bibr" rid="B37">2008</xref>; Davis and Valentine, <xref ref-type="bibr" rid="B14">2009</xref>), when matching other as opposed to own race faces (Slone et al., <xref ref-type="bibr" rid="B57">2000</xref>; Meissner and Brigham, <xref ref-type="bibr" rid="B38">2001</xref>), if instructions are given emphasizing that appearance may have changed (Chapman and Wells, <xref ref-type="bibr" rid="B10">2007</xref>), if a photographic line-up is presented simultaneously as opposed to sequentially (Meissner et al., <xref ref-type="bibr" rid="B39">2005</xref>), or if the faces being matched have similar external features (Fletcher et al., <xref ref-type="bibr" rid="B18">2008</xref>). While all of these results can be interpreted in terms of criteria, measuring this has not previously been the explicit aim as it is here.</p><p>We record the proportion of &#x0201c;different&#x0201d; responses as a function of the physical differences between faces and use these proportions to estimate the physical difference that would result in people responding &#x0201c;different&#x0201d; 50% of the time (P50). P50 specifies the abscissa location of the psychometric function that links physical differences to observer responses and thus is determined by criterion. Specifically, P50 corresponds to the point where the mean difference between the faces in a pair would be equal to the criterion. We also estimate difference limen (DL), half the difference between the 75 and 25% points on the psychometric function. This corresponds to the steepness of the psychometric function, and index of sensitivity: a smaller DL indicates a steeper function and thus higher sensitivity. Previous studies using same/different tasks have only reported proportion correct providing an indication of sensitivity but not criterion (Freire et al., <xref ref-type="bibr" rid="B19">2000</xref>; Mondloch et al., <xref ref-type="bibr" rid="B41">2002</xref>). Estimating both P50 and DL allows us to focus on criterion but also relate our results to previous findings on sensitivity. It also allows us to test whether criterion and sensitivity co-vary.</p><p>In Experiment 1 we vary the views available, a manipulation well-known to affect sensitivity (Bruce et al., <xref ref-type="bibr" rid="B7">1987</xref>; Hill et al., <xref ref-type="bibr" rid="B24">1997</xref>; Troje and Bulthoff, <xref ref-type="bibr" rid="B59">1998</xref>; Favelle et al., <xref ref-type="bibr" rid="B16">2007</xref>), especially for unfamiliar faces (Hancock et al., <xref ref-type="bibr" rid="B20">2000</xref>). In many of the studies cited above the faces being compared were presented in the same view (Freire et al., <xref ref-type="bibr" rid="B19">2000</xref>; Mondloch et al., <xref ref-type="bibr" rid="B41">2002</xref>; Rhodes et al., <xref ref-type="bibr" rid="B52">2007</xref>; Dakin and Omigie, <xref ref-type="bibr" rid="B13">2009</xref>; Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>) and this allows decisions to be based on image differences alone: all differences in the images presented result from differences between the faces. Changing view introduces image changes that are much larger than those associated with a change in identity (Moses et al., <xref ref-type="bibr" rid="B42">1996</xref>), but which themselves do not provide information about whether the face is the same or different person. As well as the usual identical full-face (0&#x000b0;) view comparison, we also include two conditions where the faces are compared across changes in viewpoint. One condition involved matching between static full-face and a three-quarter (TQ; 45&#x000b0;) views, a condition well-known to reduce sensitivity (Liu and Chaudhuri, <xref ref-type="bibr" rid="B32">2002</xref>). The other involved matching between an animated sequence of seven views (&#x02212;45&#x000b0; to 45&#x000b0; in steps of 15&#x000b0;), temporally offset to avoid image based comparisons (see Movie <xref ref-type="supplementary-material" rid="SM1">S1</xref>). Similar animations have been shown to increase subsequent sensitivity compared to static or jumbled views in an old/new recognition task (Hill et al., <xref ref-type="bibr" rid="B24">1997</xref>). If criterion is independent of sensitivity and is determined by differences between faces that are independent of viewing conditions it, unlike sensitivity, will not vary the two conditions requiring face based comparisons.</p><p>The differences between reference and comparison faces in Experiment 1 and the experimental manipulations used in Experiments 2 and 3 are all defined in terms of &#x0201c;face-space,&#x0201d; a widely used metaphor defined and elaborated elsewhere (Valentine, <xref ref-type="bibr" rid="B61">1991</xref>, <xref ref-type="bibr" rid="B62">1995</xref>, <xref ref-type="bibr" rid="B63">2001</xref>; O'Toole et al., <xref ref-type="bibr" rid="B46">2001b</xref>). Briefly, face-space models propose that faces are encoded in terms of their location in a multi-dimensional space where the dimensions correspond to &#x0201c;features&#x0201d; that can be used to discriminate between them. Distances within the space correspond to similarity and are fundamental to categorization decisions including recognition (Aeria et al., <xref ref-type="bibr" rid="B1">2010</xref>). In all the experiments reported here we use a physical face-space (O'Toole et al., <xref ref-type="bibr" rid="B45">2001a</xref>) based on PCA of differences in 3D shape to define the stimuli tested (Vetter and Blanz, <xref ref-type="bibr" rid="B65">1999</xref>; Claes, <xref ref-type="bibr" rid="B11">2007</xref>). We also compare conditions designed to test between different possible distance metrics within this space to see which best predict human perception and decisions.</p><p>We focus on shape because it is a property of the face itself and as such genuinely represents a <italic>face</italic>- rather than an image-space. In practical terms shape is also relatively difficult to alter artificially. It is the target of many forms of surgical interventions, and critical for craniofacial reconstruction (Claes et al., <xref ref-type="bibr" rid="B12">2006</xref>) and understanding how physical changes in shape are perceived is critical for these applications. Surface reflectance is the other key property in determining the appearance of faces, but the reflectance information currently provided by scanners is itself image based and affected by factors extrinsic to the face itself, particularly lighting. Also, while relatively few principal components (PCs) can explain the majority of the data in the case of shape, modeling detailed facial reflectance including unique facial markings is much more challenging and requires many more components to explain an equal proportion of variation. Lastly reflectance can and is readily and easily altered by the application of makeup. Thus, while we do not wish to deny the importance of reflectance information (Hill et al., <xref ref-type="bibr" rid="B23">1995</xref>; O'Toole et al., <xref ref-type="bibr" rid="B48">1999</xref>; Russell et al., <xref ref-type="bibr" rid="B55">2007</xref>), here we focus on shape, and keep reflectance constant.</p><p>Principal components analysis of shape or any other property defines a space with orthogonal axes that correspond to the principal sources of variation between faces. The PCs are the dimensions of the space and are ordered in terms of the proportion of the total variation that they account for. PCA based physical face-spaces are widely used, although most are based on PCA of face images (Turk and Pentland, <xref ref-type="bibr" rid="B60">1991</xref>). The performance of automatic systems based on these analyses corresponds well with that of humans (Burton et al., <xref ref-type="bibr" rid="B8">2001</xref>). The degree of correspondence depends on the similarity measure used and, for example, Mahalanobis distance (Mahalanobis, <xref ref-type="bibr" rid="B34">1936</xref>) has been found to produce a better correspondence than Euclidean distance (Burton et al., <xref ref-type="bibr" rid="B8">2001</xref>). Mahalanobis distance weights each variable according to its variance and covariances:</p><disp-formula id="E1"><mml:math id="M1"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula><p>where <italic>T</italic> indicates transpose, and <inline-formula><mml:math id="M2"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:math></inline-formula> and are <inline-formula><mml:math id="M3"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:math></inline-formula> two vectors from a distribution with variance&#x02013;covariance matrix S. In the case of PCA, covariance between dimensions is zero by definition and, in that case, Mahalanobis distance equates to standardized Euclidean distance, the square root of the sum of the squares of all PC values standardized by their associated SD (De Maesschalck et al., <xref ref-type="bibr" rid="B15">2000</xref>). Raw Euclidean distance is simple the sum of the squares of unstandardized PC values. As identity specific information is necessarily shared by only a few if any faces, it is encoded in lower PC dimensions (O'Toole et al., <xref ref-type="bibr" rid="B43">1993</xref>). These have smaller variance and thus values by definition as so contribute little to Euclidean or other distance measures when PC values are not standardized. While Experiment 1 was designed so that Euclidean and Mahalanobis distances are equivalent, Experiment 2 provides a test between these metrics.</p><p>An alternative similarity metric is based on differences between angles subtended by &#x0201c;identity vectors&#x0201d; at the origin of face-space rather than distances (Leopold et al., <xref ref-type="bibr" rid="B28">2001</xref>; Romdhani et al., <xref ref-type="bibr" rid="B53">2005</xref>; Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>). In a PCA space the origin is the average face and this may play the role of a norm or prototype with identity vectors defining faces with respect to that point. The direction of the identity vector is more important than its magnitude in determining identity, probably because direction captures the relations between different sources of variation (Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>). Automatic caricatures, which exaggerate differences from the mean while preserving or even enhancing perceived identity, inspired such models, and emphasize the importance of direction over magnitude (Brennan, <xref ref-type="bibr" rid="B3">1985</xref>; Rhodes, <xref ref-type="bibr" rid="B50">1996</xref>).</p><p>In Experiment 3 we test between angle and distance based measures by varying the distinctiveness of the faces being compared. Distinctiveness is a central concept to face-space models often characterized in terms of &#x0201c;how easy would it be to pick out this person in a crowd&#x0201d; (Light et al., <xref ref-type="bibr" rid="B31">1979</xref>; Valentine, <xref ref-type="bibr" rid="B61">1991</xref>). Subjective ratings show good inter-rater agreement and predict memorability (Light et al., <xref ref-type="bibr" rid="B31">1979</xref>; Valentine and Bruce, <xref ref-type="bibr" rid="B64">1986</xref>; Vokey and Read, <xref ref-type="bibr" rid="B66">1992</xref>). Distinctiveness also correlates with distance from the average in both physical (Bruce et al., <xref ref-type="bibr" rid="B5">1994</xref>; Hancock et al., <xref ref-type="bibr" rid="B21">1998</xref>) and psychological face-spaces (Johnston et al., <xref ref-type="bibr" rid="B25">1997</xref>). Here we varied distinctiveness by varying the distance of the faces being compared from the origin in physical space, i.e., the magnitude of the identity vectors. Comparison faces were &#x0201c;lateral caricatures&#x0201d; (Rhodes et al., <xref ref-type="bibr" rid="B51">1998</xref>; Lewis and Johnston, <xref ref-type="bibr" rid="B30">1999</xref>) of the reference faces, having identity vectors with the same magnitude but in a different direction. Increasing distinctiveness by increasing magnitude leaves the angles between identity vectors unchanged while increasing the distance between faces proportionally (by similar triangles).</p></sec><sec><title>Conclusion</title><p>In all experiments we estimate criterion P50 and sensitivity DL in terms of physical differences in facial shape. In Experiment 1 we use the average face as a reference and test whether P50 and DL are affected by the face views available. We expect sensitivity to be affected but, if criterion is a function of the differences between faces and independent of sensitivity, it should remain constant. Experiment 2 provides a test between two possible physical distance metrics, Mahalanobis and Euclidean, to see which best predicts observed P50 and DL. Lastly, in Experiment 3 we use 10 references faces and vary the distinctiveness of both reference and comparison faces in order to test between angle and distance based similarity measures. The general methods are described next followed by the individual experiments.</p></sec><sec id="s1"><title>General Methods and Materials</title><sec><title>Stimuli</title><p>All Stimuli were rendered images of 3D solid body computer models of synthetic faces based on measurements of real faces. For Experiments 1 and 2, the analysis was based on 54 faces recorded using a NEC &#x0201c;Fiore&#x0201d; 3D facial surface scanner (Yoshino et al., <xref ref-type="bibr" rid="B69">2000</xref>). Ethics approval was given by the University of Melbourne, Human Research Ethics Committee (HESC 050550.1), VIC, Australia. For Experiment 3 the analysis was based on 224 real faces recorded using a 3dMD scanner (Aeria et al., <xref ref-type="bibr" rid="B1">2010</xref>). Ethics approval was granted by the Princess Margaret Hospital for Children ethics committee (PMHEC 1443/EP), Perth, WA, Australia. In each case equal numbers of males and females randomly selected from those available were included. The Melbourne database consists of two distinct populations, Japanese and &#x0201c;Caucasian&#x0201d;<sup>1</sup>, of which only ones labeled Caucasian were used. The Perth database contains people from a variety of origins and ethnicity was not used as a criterion for inclusion/exclusion (which was randomized). Self-reported ethnicities of the 224 faces used were: 155 Caucasian, 29 Chinese, 10 Indian, 8 Eurasian, 5 Italian, 3 Anglo-Indian, 2 Filipino Australian, and 1 each of African (Black), African-Italian, Anglo-Saxon, Chilean, Chinese-Indian, Malaysian, Mixed, Portuguese, Somalian, Spanish, Vietnamese, and one face with no self-reported ethnicity. Ethnicity was not the focus of this research but this information is included as ethnicity is known to affect facial processing (Meissner and Brigham, <xref ref-type="bibr" rid="B38">2001</xref>). All faces were shown with the same modeled surface reflectance, &#x0201c;skin color,&#x0201d; which was based on the average of the faces used to define the face-space (Figures <xref ref-type="fig" rid="F1">1</xref>, <xref ref-type="fig" rid="F2">2</xref>, <xref ref-type="fig" rid="F4">4</xref>, and <xref ref-type="fig" rid="F6">6</xref> show examples).</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Proportion &#x0201c;different&#x0201d; responses as a function of physical difference in SD units for the three conditions of Experiment 1</bold>. Dotted lines indicate individual psychometric functions and solid red lines combined data.</p></caption><graphic xlink:href="fpsyg-02-00041-g002"/></fig><p>Scanned shape data for each individual real face is first fit to a generic face mesh with well specified topological properties (Claes, <xref ref-type="bibr" rid="B11">2007</xref>). Each face is represented as a vector with <italic>x</italic>, <italic>y</italic>, <italic>z</italic> position values for each vertex. The mesh with 9,327 vertices was used for Experiments 1 and 2 and one with 12,016 vertices for Experiment 3. The dimensionality is considerably reduced by using PCA while still retaining 98% of the original variance, the rest being largely noise. Twenty-two dimensions were needed to satisfy this criterion in Experiments 1 and 2 and 38 in Experiment 3. This dimensionality is of the same order as that suggested appropriate for recognition (Lewis, <xref ref-type="bibr" rid="B29">2004</xref>). The synthetic face models used as stimuli were defined within these PCA spaces assuming normal distributions averaged around the mean face and with SD for each dimension based on the real faces. All differences between faces are specified in units of these SD, <italic>z</italic>-scores.</p><p>Details of the synthetic faces generated were determined by experimental design and are specified in the corresponding methods section. Both types of scanner record information about surface color as well as shape, and this information was used to define an average surface color which was applied to all stimuli. Synthetic face models were imported to Blender v2.45 (<uri xlink:type="simple" xlink:href="http://www.blender.com">www.blender.com</uri>), an open source 3D modeling software package. A single directional white light (intensity 1.5) from 30&#x000b0; above the line of sight and a 60-mm camera 0.5&#x02009;m in front of the face were used to render the scene. Seven views were generated of each face by rotating it about its vertical axis in 15&#x000b0; steps from left TQ (&#x02212;45&#x000b0;) to right TQ (45&#x000b0;). Each view was rendered using Blender's internal raytracing engine and output as 480&#x02009;&#x000d7;&#x02009;480 pixel bitmap on the default Blender blue background (RGB 15, 56, 102). Modeled surface reflectance was based on average values and kept constant for all views of all face models. Other Blender settings were left at default values. Copies of the stimuli are available from the corresponding author on request.</p></sec><sec><title>Participants</title><p>Ethical approval for all experiments was granted by the University of Wollongong Human Research Ethics Committee, application HE09/358, in accordance with Australian National guidelines. All participants were students at the university, were over 18, and were assumed to have normal or corrected to normal vision and face processing abilities. Participation was irrespective of ethnicity and this was not recorded.</p></sec><sec><title>Procedure</title><p>For Experiments 1 and 3 testing took place in groups of up to 20 but at individual computers. For Experiment 2 participants were tested individually. Presentation of stimuli and recording of responses was controlled by individual Dell PCs using software written in Runtime Revolution. Viewing distance was unconstrained but approximately 50&#x02009;cm. Participants entered an individual identifier and on screen and instructions informed them that they would be shown pairs of faces presented simultaneously and that their task was to decide whether &#x0201c;the two faces are the same person or not.&#x0201d; The 480&#x02009;&#x000d7;&#x02009;480 pixel images measured 14&#x02009;cm&#x02009;&#x000d7;&#x02009;14&#x02009;cm (16&#x000b0;&#x02009;&#x000d7;&#x02009;16&#x000b0;) on the screen. Individual faces varied in size but were approximately 7&#x02009;cm&#x02009;&#x000d7;&#x02009;10&#x02009;cm (8&#x000b0;&#x02009;&#x000d7;&#x02009;11&#x000b0;). The centers of each image were separated by 18&#x02009;cm (20&#x000b0;) horizontally and offset 5&#x02009;cm (6&#x000b0;) vertically to prevent direct comparisons. Left/right positions and order of trials was fully randomized for each participant. Responses were made by moving a scroll bar. Although this enabled participants to indicate a level of confidence in their decision results were binarized as &#x0201c;same&#x0201d; or &#x0201c;different&#x0201d; for analysis. The slider started each trial positioned on the center tick mark but this was not a valid response and participants had to move the pointer to one side or the other. Images remained on the screen until a response was made. Ten practice trials preceded the experiment proper during or after which participants were able to ask questions regarding the task. No feedback about accuracy was given at any stage. There were a total of 110 trials in Experiments 1 and 3 and 220 in Experiment 2.</p></sec><sec><title>Design and treatment of results</title><p>All experiments used the method of constant stimuli with 11 levels of difference including identical face pairs. There were 10 repetitions at each level and the percentage of &#x0201c;different&#x0201d; responses recorded. Observed response probabilities were used to estimate the median location (P50) and 25th (P25) and 75th (P75) percentiles for each observer using the Spearman&#x02013;Karber method, a distribution free approach to estimating psychometric functions (Miller and Ulrich, <xref ref-type="bibr" rid="B40">2001</xref>). Data were first monotonized (Klein, <xref ref-type="bibr" rid="B27">2001</xref>) as this gives the maximum likelihood estimate of the true response probability (Miller and Ulrich, <xref ref-type="bibr" rid="B40">2001</xref>). The median was used as a distribution free estimate of criterion location (P50), as there is a 50% probability the criterion falls below this point. Similarly half the inter quartile range (IQR), (P75&#x02212;P25)/2, was used to estimate the DL, a measure inversely related to sensitivity. Between condition differences in P50 and DL were tested using non-parametric test and effect size, <italic>r</italic>, calculated on the basis of corresponding <italic>z</italic> and <italic>N</italic> values (Field, <xref ref-type="bibr" rid="B17">2009</xref>).</p></sec></sec><sec><title>Experiment 1: Criterion as a Function of Views Presented</title><sec id="s2"><title>Introduction</title><p>The primary aim of this experiment was to estimate the criterion adopted by observers when deciding whether two faces presented are of the same person or not. We also measure sensitivity and test how both vary as a function of the face views presented.</p><p>The average face was used as the reference throughout. We expect criterion will vary as a function of both location and direction in face-space and the average represents a neutral starting point in both respects. Direction was randomized with the constraint that all comparison faces were at the corners of concentric multi-dimensional hypercubes (Wilson et al., <xref ref-type="bibr" rid="B68">2002</xref>). That is they were equal distances away on all PC dimensions when distance is specified in terms of the SD associated with each dimension. This corresponds to Mahalanobis distance but constraining the comparison faces in this way ensures that they were equivalent with respect to other possible distance metrics including Euclidean. Examples of the stimuli are shown in Figure <xref ref-type="fig" rid="F1">1</xref>A) and their corresponding locations on the first 2D of face-space in Figure <xref ref-type="fig" rid="F1">1</xref>B).</p><p>We varied the face views shown as a between participants manipulation. In one condition (FF) identical full-faces views were shown as for many previous studies (Freire et al., <xref ref-type="bibr" rid="B19">2000</xref>; Mondloch et al., <xref ref-type="bibr" rid="B41">2002</xref>; Rhodes et al., <xref ref-type="bibr" rid="B52">2007</xref>; Dakin and Omigie, <xref ref-type="bibr" rid="B13">2009</xref>; Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>). In this condition all image differences reflect differences between the faces and decisions may reflect image based rather than face based comparisons (Bruce, <xref ref-type="bibr" rid="B4">1982</xref>). In a second condition (TQ) comparison faces were shown in TQ view, preventing direct image based comparisons. This view change is known to result in large image differences and lower sensitivity (Bruce et al., <xref ref-type="bibr" rid="B7">1987</xref>; Moses et al., <xref ref-type="bibr" rid="B42">1996</xref>; Troje and Bulthoff, <xref ref-type="bibr" rid="B58">1996</xref>, <xref ref-type="bibr" rid="B59">1998</xref>; Hill et al., <xref ref-type="bibr" rid="B24">1997</xref>; O'Toole et al., <xref ref-type="bibr" rid="B44">1998</xref>). Thirdly we included an animated condition (GIF) where seven views (&#x02212;45&#x000b0; to 45&#x000b0; in 15&#x000b0; increments) of each face were presented in an animated sequence giving the impression of a head oscillating from side to side. This condition was designed to maximize information about face shape available by providing both multiple views and motion based cues to structure. Similar presentation conditions lead to better recognition memory (Hill et al., <xref ref-type="bibr" rid="B24">1997</xref>) and were expected to increase sensitivity compared to a single static view. The animations were presented offset in time to prevent direct image based comparisons.</p><p>We expected sensitivity to be high for FF and GIF but lower for TQ. Criterion may not vary if it is independent of sensitivity and reflects underlying difference in shape between faces as intended.</p></sec><sec><title>Method and materials</title><sec><title>Participants</title><p>Forty-seven undergraduate students took part in this experiment as part of third year laboratory classes.</p></sec><sec><title>Design</title><p>Participants were randomly assigned to one of the three conditions: paired full-face (FF), full face, and TQ or paired GIF animations (GIF) as outlined in the Section &#x0201c;<xref ref-type="sec" rid="s2">Introduction</xref> and <xref ref-type="sec" rid="s1">General Methods and Materials</xref>.&#x0201d; The method of constant stimuli was used with 10 repetitions at each of 11 levels of difference between the faces. The levels used varied from 0, identical faces, to 1 in steps of 0.1 defined units of SD for each of the 22 PCs used. Sign was randomized for each dimension independently giving 2<sup>22</sup> possible faces for each level. There were a total of 110 trials with order and left/right positions fully randomized for each participant.</p><p>Other details of the method and materials were as described in the Section <xref ref-type="sec" rid="s1">&#x0201c;General Methods and Materials.&#x0201d;</xref></p></sec></sec><sec><title>Results</title><p>Figure <xref ref-type="fig" rid="F2">2</xref> shows the overall proportion of &#x0201c;different&#x0201d; responses as a function of distance from the reference face for each of the three conditions of viewpoint. Table <xref ref-type="table" rid="T1">1</xref> gives medians and interquartile ranges for P50 and DL. Overall the number of &#x0201c;different&#x0201d; responses increased with greater physical differences between stimuli, measured in SD units, as intended. The rank order correlation between distance and proportion &#x0201c;different&#x0201d; was Spearman's rho <italic>r</italic><sub>s</sub>&#x02009;=&#x02009;0.511, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001 (<italic>N</italic>&#x02009;=&#x02009;5,170 trials; unmonotonized data). One participant in the FF condition was not included in the analysis because P50 could not be determined as the maximum difference between stimuli, 1 SD, elicited less than 50% &#x0201c;different&#x0201d; responses. Similarly P75 could not be estimated for four other participants (three in TQ and one in FF) and DL was set to the maximum possible value of 0.5 SD.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Median (IQR) of P50 and DL for all participants</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Condition measure</th><th align="left" rowspan="1" colspan="1">FF</th><th align="left" rowspan="1" colspan="1">TQ</th><th align="left" rowspan="1" colspan="1">GIF</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">P50</td><td align="left" rowspan="1" colspan="1">0.33 (0.16)</td><td align="left" rowspan="1" colspan="1">0.46 (0.13)</td><td align="left" rowspan="1" colspan="1">0.47 (0.19)</td></tr><tr><td align="left" rowspan="1" colspan="1">DL</td><td align="left" rowspan="1" colspan="1">0.14 (0.14)</td><td align="left" rowspan="1" colspan="1">0.31 (0.14)</td><td align="left" rowspan="1" colspan="1">0.18 (0.11)</td></tr></tbody></table><table-wrap-foot><p><italic>All values are in the SD units used to construct the stimuli (see <xref ref-type="sec" rid="s1">General Methods and Materials</xref>)</italic>.</p></table-wrap-foot></table-wrap><p>A Kruskal&#x02013;Wallis test showed a main effect of Views presented on P50: <italic>H</italic>(2)&#x02009;=&#x02009;9.23, <italic>p</italic>&#x02009;=&#x02009;0.01. Follow-up Mann&#x02013;Whitney tests with Bonferroni corrected alpha&#x02009;=&#x02009;0.05/3 confirmed that P50 was significantly lower in the FF condition than TQ, <italic>U</italic>&#x02009;=&#x02009;54.5, <italic>p</italic>&#x02009;=&#x02009;0.008, effect size <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.47, or GIF, <italic>U</italic>&#x02009;=&#x02009;49.5, <italic>p</italic>&#x02009;=&#x02009;0.008, effect size <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.48. There was no significant difference between TQ and GIF conditions, <italic>U</italic>&#x02009;=&#x02009;110, <italic>p</italic>&#x02009;=&#x02009;0.711, <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.07.</p><p>There was also a main effect of View on DL: <italic>H</italic>(2)&#x02009;=&#x02009;13.59, <italic>p</italic>&#x02009;=&#x02009;0.001. Follow-up Mann&#x02013;Whitney tests with Bonferroni corrected alpha&#x02009;=&#x02009;0.05/3 showed that DL was significantly higher, and thus sensitivity lower, for TQ than FF, <italic>U</italic>&#x02009;=&#x02009;38.5, <italic>p</italic>&#x02009;=&#x02009;0.001, effect size <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.58, or GIF, <italic>U</italic>&#x02009;=&#x02009;48.5, <italic>p</italic>&#x02009;=&#x02009;0.004, effect size <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.51. There was no significant difference in DL between FF and GIF, <italic>U</italic>&#x02009;=&#x02009;78.5, <italic>p</italic>&#x02009;=&#x02009;0.161, <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.26.</p><p>Median false alarm rates (&#x0201c;different&#x0201d; responses at the 0 SD level) for the three conditions were: FF 0.15 TQ 0.35 GIF 0.10. The high rate in TQ shows how changes in viewing conditions can be misinterpreted as a change in identity even when there is no change in the shape of the face.</p></sec><sec><title>Discussion</title><p>The proportion of &#x0201c;different identity&#x0201d; decisions increased with distance from the average reference face as expected. Criterion was significantly lower for FF than TQ or GIF, which did not differ from each other. Sensitivity was higher for FF and GIF than TQ as expected.</p><p>Criterion was constant for both conditions that involved comparing between views, despite the significant difference in sensitivity. This is consistent with people making their judgments on the basis of differences between faces that are independent of viewing conditions that significantly affect sensitivity. Criterion was significantly lower in the identical view condition, but as argued in the introduction, this is likely to reflect image rather than face based comparisons. Based on this data, the best estimate of the criterion people used is 0.47 SD when the average face is the reference. While it would clearly be necessary to test a variety of other view combinations and other changes in presentation conditions to determine if this remains constant over a wider range of conditions, that is not the focus of this work.</p><p>Sensitivity was higher for FF and GIF comparisons than for TQ comparisons as expected. The GIF condition was associated with both high sensitivity and low false alarm rates while still ensuring that comparisons have to be face rather than image based and was used for the remaining experiments.</p></sec></sec><sec><title>Experiment 2: Contrasting Euclidean and Mahalanobis Distance Metrics</title><sec><title>Introduction</title><p>This experiment was designed as a test between Euclidean and Mahalanobis distance metrics again using the average face as a reference. While in Experiment 1 comparison faces were designed to be equivalent with respect to these two metrics, here we compared two conditions that would be expected to differ. Here stimuli were generated in the same manner as before except that either the first (PC1) or second (PC2) PCs was set to zero (Please see Figure <xref ref-type="fig" rid="F3">3</xref>). This meant that Euclidean but not Mahalanobis distances will differ between the conditions.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>(A)</bold> Examples stimuli used in Experiment 2. The average reference face is shown on the left and all other faces are 1 SD away. The top row shows PC2 zero faces and the bottom row PC1 zero faces. The average faces is shown here at 45&#x000b0; and other faces at 30&#x000b0; but in the experiment all were shown as temporally offset GIF animations rotating between 45&#x000b0; and &#x02212;45&#x000b0;. <bold>(B)</bold> Two-dimensional illustration of the face-space used for Experiment 2. Black points show locations of the 54 faces used to define the face-space. The locations of the average reference face and the PC1 zero (0, &#x000b1;1 SD) and PC2 zero (&#x000b1;1, 0 SD) comparison faces shown in <bold>(A)</bold> are also indicated. Red/blue points indicate possible locations of comparison faces for PC1 zero and PC2 zero conditions respectively.</p></caption><graphic xlink:href="fpsyg-02-00041-g003"/></fig><p>By definition PC1 is associated with more physical variation and a larger SD than PC2. This means that Euclidean distances will be larger when PC2 is zero and PC1 varies than when the reverse is true. For the particular PCA space used Euclidean distances will be &#x0223c;1.5&#x000d7; greater in the PC2 zero condition &#x02013; a function of the proportions of total variance accounted for by each dimension in the PCA space. In contrast Mahalanobis distances are standardized by SD and will not vary between conditions.</p><p>If criterion corresponds to a particular Mahalanobis distances, P50 will not vary between conditions. In contrast, if criterion corresponds to particular Euclidean distance, the P50 SD value will be 1.5&#x000d7; larger in the PC1 zero condition.</p><p>Viewing conditions were constant in this experiment ruling out one possible source of variation in sensitivity. If sensitivity is constant, the same differences between conditions would be expected for DL as for P50. However sensitivity can vary as a function of direction in face-space (Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>), and this could also produce a difference between the conditions tested here.</p></sec><sec><title>Method and materials</title><sec><title>Participants</title><p>Twenty-four undergraduate students took part in this experiment for course credit and were tested individually.</p></sec><sec><title>Design</title><p>This experiment was a within subjects design with two conditions, PC1 zero or PC2 zero. Comparison faces were constructed in the same way as for Experiment 1, except that either PC1 or PC2 was zero for all comparison faces in the condition. Sign was randomized for other dimensions as before. There were a total of 220 trials, 110 for each condition. Order was fully randomized for each participant and PC1 zero and PC2 zero trials were not distinguished in any way.</p><p>The reference face was the average face for both conditions and all stimuli were presented as animated GIFs. Other details of the method and materials were as described in the Section <xref ref-type="sec" rid="s1">&#x0201c;General Methods and Materials.&#x0201d;</xref></p></sec></sec><sec><title>Results</title><p>Figure <xref ref-type="fig" rid="F4">4</xref> shows the overall proportion of &#x0201c;different&#x0201d; responses as a function of distance from the reference face for PC1 zero and PC2 zero conditions. Table <xref ref-type="table" rid="T2">2</xref> gives group medians and interquartile ranges for P50 and DL. Overall number of &#x0201c;different&#x0201d; responses again correlated positively with physical differences between stimuli: Spearman's rho <italic>r</italic><sub>s</sub>&#x02009;=&#x02009;0.541, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001 (<italic>N</italic>&#x02009;=&#x02009;5,280 trials).</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Proportion &#x0201c;different&#x0201d; responses as a function of physical difference in SD units for the two conditions of Experiment 2</bold>. Dotted lines indicate individual psychometric functions and solid red lines combined data.</p></caption><graphic xlink:href="fpsyg-02-00041-g004"/></fig><table-wrap id="T2" position="float"><label>Table 2</label><caption><p><bold>Median (IQR) of P50 and DL for all participants</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Condition measure</th><th align="left" rowspan="1" colspan="1">PC1 zero</th><th align="left" rowspan="1" colspan="1">PC2 zero</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">P50</td><td align="left" rowspan="1" colspan="1">0.46 (0.13)</td><td align="left" rowspan="1" colspan="1">0.55 (0.33)</td></tr><tr><td align="left" rowspan="1" colspan="1">DL</td><td align="left" rowspan="1" colspan="1">0.17 (0.18)</td><td align="left" rowspan="1" colspan="1">0.18 (0.18)</td></tr></tbody></table><table-wrap-foot><p><italic>All values are in the SD units used to construct the stimuli (see <xref ref-type="sec" rid="s1">General Methods and Materials</xref>)</italic>.</p></table-wrap-foot></table-wrap><p>Wilcoxon signed ranks dependent samples test showed that P50 was significantly lower for PC1 zero faces, <italic>N</italic>&#x02009;=&#x02009;24, <italic>T</italic>&#x02009;=&#x02009;49, <italic>p</italic>&#x02009;=&#x02009;0.02, <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.48. This is the opposite direction to that predicted by Euclidean distance.</p><p>There was no significant difference in DL, <italic>N</italic>&#x02009;=&#x02009;24, <italic>T</italic>&#x02009;=&#x02009;118, 0&#x02009;=&#x02009;0.54, <italic>r</italic>&#x02009;=&#x02009;0.13, consistent with sensitivity reflecting Mahalanobis distance.</p><p>Median false alarms rate (&#x0201c;different&#x0201d; responses at 0 SD level) was 0.15 based on the 20 identical 0 SD pairs.</p></sec><sec><title>Discussion</title><p>Participants adopted a significantly lower criterion in the PC1 zero condition but sensitivity did not differ. The null effect on sensitivity is consistent with Mahalanobis distance and the difference in criterion is in the opposite direction to the predicted by Euclidean distance. While there is considerable overlap between individual functions, P50 appears to be somewhat more affected by changes in PC2, despite the fact that this dimension accounts for less physical variation than PC1. Standardizing PC values in terms of the associated SD, as is the case for Mahalanobis distance, provides a principled way of ensuring that distances are not dominated by the values of early PCs.</p><p>As can be seen from Figure <xref ref-type="fig" rid="F3">3</xref>A, size differences are a major source of variation captured by PC1. Previous work has shown that size differences do not affect human categorizations even when potentially discriminating (Bruce et al., <xref ref-type="bibr" rid="B6">1993</xref>), and the relative unimportance of PC1 found here may reflect this. P50 for the PC1 but not PC2 zero condition was similar to that found in the GIF condition of Experiment 1 where all PC dimensions varied under the same viewing conditions, suggesting that fixing PC2 but not PC1 affects the proportion of &#x0201c;different&#x0201d; decisions. If PC1 is unique in its lack of influence, we would expect P50 to remain around 0.46 SD if other PC dimensions were set at zero but this remains to be tested.</p><p>In Experiment 3 we test whether an angle based similarity measure in general better accounts for perceived changes in identity than distance based measures.</p></sec></sec><sec><title>Experiment 3: Comparing Angle and Distance Based Measures</title><sec><title>Introduction</title><p>In this experiment we test whether changes in angle capture changes in perceived identity better than changes in distance. Angle here refers to is the differences in direction between the &#x0201c;identity vectors&#x0201d; that define reference and comparison faces relative to the mean in face-space (Leopold et al., <xref ref-type="bibr" rid="B28">2001</xref>). Angle is contrasted as a measure to distance between the end points of the same vectors. Varying the magnitude of the vectors affects distance between endpoints but not angle. Vector magnitude also corresponds to perceived distinctiveness.</p><p>Ten reference faces were used, all one SD from the mean and generated in the same way as the 1 SD comparison faces in Experiment 1. One SD is the mean value expected for a face drawn from a multinormally distributed population. For each of these reference faces comparison faces were &#x0201c;lateral caricatures&#x0201d; constructed by moving in a direction orthogonal to the reference face identity vector (Rhodes et al., <xref ref-type="bibr" rid="B51">1998</xref>; Lewis and Johnston, <xref ref-type="bibr" rid="B30">1999</xref>; Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>). Comparison faces were the same distance from the average but in a different direction. All faces can be thought of as located on the same multi-dimensional annulus in face-space. Angular differences in direction between pairs of faces were systematically varied and P50 and DL calculated in these terms.</p><p>As a between participants manipulation we varied the distance of both comparison and reference faces from the mean, in effect moving all faces to a different annulus. This has the effect of varying their distinctiveness and the distance between them in face-space while keeping angular differences constant. Distances between vector endpoints increase in proportion to distance from the mean (by similar triangles). We ran four conditions with SD 0.5, 1.0, 1.5, or 2.0. Reference faces for other conditions were generated by automatically caricaturing the 1 SD reference faces (Brennan, <xref ref-type="bibr" rid="B3">1985</xref>; Rhodes, <xref ref-type="bibr" rid="B50">1996</xref>; O'Toole et al., <xref ref-type="bibr" rid="B49">1997</xref>). 0.5 SD reference faces constitute 50% &#x0201c;anti-caricatures&#x0201d; while the 1.5 and 2 SD versions represent 150 and 200% caricatures respectively. New sets of comparison faces were generated for each condition to avoid possible confounds associated with the particular faces used. Please see Figure <xref ref-type="fig" rid="F5">5</xref> for examples of stimuli.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>(A)</bold> Example stimuli from Experiment 3. The rows show 0.5, 1.0, and 2.0 SD conditions respectively. The first column shows one of the reference faces used and corresponds to 50, 100, and 200% caricatures of that face. The following three columns show random examples of 27&#x000b0;, 54&#x000b0;, and 81&#x000b0; comparison faces used in each condition. P50 was found to be slightly to the right of the third face in the first row (59.4&#x000b0;) and to the left of the third face in rows two (48.9&#x000b0;) and three (51.0&#x000b0;). All faces shown here at an angle of 30&#x000b0; but in the experiment all faces were shown as animation appearing to rotate between &#x02212;45&#x000b0; to 45&#x000b0;. <bold>(B)</bold> Two-dimensional illustration of the relationship between reference and comparison faces in Experiment 3. Concentric circles show the four levels of distinctiveness used, 0.5, 1, 1.5, and 2 SD. Reference faces for the 1 and 2 SD conditions are indicated in red as R1SD and R2SD respectively. Corresponding comparison faces are shown in blue. C1SD27 and C1SD54 are possible 27&#x000b0; and 54&#x000b0; comparison faces for the R1SD reference face. C2SD27 and C2SD54 are the equivalent comparison faces for the R2SD reference face. The angle subtended at the origin remains constant between conditions but the distance between equivalent reference and comparison faces is greater for the 2 SD faces.</p></caption><graphic xlink:href="fpsyg-02-00041-g005"/></fig><p>If perceived differences in identity are determined by differences in angle, distinctiveness should have little or no effect on P50 and/or DL. Alternatively, if distance between faces is critical, P50 and/or DL expressed as an angle should decrease with increasing distinctiveness as a constant distance between faces will correspond to a smaller angle when the faces are at a greater distance form the mean.</p></sec><sec><title>Method and materials</title><sec id="s3"><title>Stimuli</title><p>All stimuli were generated using a PCA space based on a different set of faces and measurements than Experiments 1 and 2 (please see <xref ref-type="sec" rid="s1">General Methods and Materials</xref>). Ten reference faces were used generated in the same way as the 1 SD comparison faces in Experiment 1 (i.e., &#x000b1;1 SD from the mean on all dimensions). This ensured that reference faces were all equivalent Euclidean and Mahalanobis distances away from the average. As with Experiment 1, there were 11 levels of difference between the reference and the comparison face with 10 repetitions at each. Here differences were defined in terms of angle with levels from 0&#x000b0; to 90&#x000b0; in steps of 9&#x000b0;. All angles were calculated using Mahalanobis distances. Direction relative to the reference face was randomized with the constraint that direction was always orthogonal to the reference face's identity vector. Orthogonalization was achieved using the Gram&#x02013;Schmidt procedure and the magnitude of resulting vectors scaled so that comparison face vectors had the same magnitude as the corresponding reference faces. Euclidean values will have varied somewhat depending on random direction. In order to avoid possible confounds, one of 10 possible comparison faces at each angle and for each reference face was randomly selected for each participant. For the between subjects conditions the reference faces were &#x0201c;caricatured&#x0201d; by scaling all PC values equally and new comparison faces generated. Resulting levels of distinctiveness were 0.5, 1.0, 1.5, and 2.0 SD.</p></sec><sec><title>Participants</title><p>One hundred eleven undergraduate students took part in this experiment as part of third year laboratory classes.</p></sec><sec><title>Design</title><p>Participants were randomly assigned to different groups which varied according to the distinctiveness of the faces shown: 0.5, 1.0, 1.5, or 2.0 SD. P50 and DL were both specified in terms of angle.</p><p>Other details of the method were as described in the Section <xref ref-type="sec" rid="s1">&#x0201c;General Methods and Materials.&#x0201d;</xref></p></sec></sec><sec><title>Results</title><p>Figure <xref ref-type="fig" rid="F6">6</xref> show the overall proportion of &#x0201c;different&#x0201d; responses as a function of distance from the reference face for each of the three conditions of viewpoint. Table <xref ref-type="table" rid="T3">3</xref> gives medians and interquartile ranges for P50 and DL. Overall number of &#x0201c;different&#x0201d; responses increased with increasing angle between identity vectors: Spearman's rho <italic>r</italic><sub>s</sub>&#x02009;=&#x02009;0.621, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. Increasing the angle between identity vectors increased the proportion of &#x0201c;different&#x0201d; responses (<italic>N</italic>&#x02009;=&#x02009;12,210 total trials). Individual data for participants in the distinctiveness 0.5 condition was truncated in a number of cases. No P50 location could be calculated for four participants, three because their &#x0201c;different&#x0201d; response rate was &#x0003c;50% for the largest difference used (P50 set to 1, an underestimate) and one because false alarm rate was greater than 50 (P50 set to 0). Similarly, no P75 could be obtained for 10 participants in this condition because their &#x0201c;different&#x0201d; response rate never reached that level. In these cases P75 was set to 1 and DL calculated accordingly (DL underestimated). Two other participants, one in distinctiveness condition 1.0 and one in 2.0, also did not reach a 75% response rate and were also assigned P75&#x02009;=&#x02009;1. Re-analysis of data with these participants omitted did not change the effects reported so they are included here.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Proportion &#x0201c;different&#x0201d; responses as a function of physical difference expressed as the angle subtended at the origin of face-space for the four conditions of Experiment 3</bold>. Dotted lines indicate individual psychometric functions and solid red lines combined data.</p></caption><graphic xlink:href="fpsyg-02-00041-g006"/></fig><table-wrap id="T3" position="float"><label>Table 3</label><caption><p><bold>Median (IQR) of P50 and DL for all participants</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Distinctiveness (SD) measure</th><th align="left" rowspan="1" colspan="1">0.5</th><th align="left" rowspan="1" colspan="1">1.0</th><th align="left" rowspan="1" colspan="1">1.5</th><th align="left" rowspan="1" colspan="1">2.0</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">P50</td><td align="left" rowspan="1" colspan="1">59.4 (27.5)</td><td align="left" rowspan="1" colspan="1">52.7 (11.9)</td><td align="left" rowspan="1" colspan="1">48.9 (18.0)</td><td align="left" rowspan="1" colspan="1">51.0 (18.3)</td></tr><tr><td align="left" rowspan="1" colspan="1">DL</td><td align="left" rowspan="1" colspan="1">23.3 (19.2)</td><td align="left" rowspan="1" colspan="1">13.3 (5.5)</td><td align="left" rowspan="1" colspan="1">11.4 (5.6)</td><td align="left" rowspan="1" colspan="1">12.4 (7.9)</td></tr></tbody></table><table-wrap-foot><p><italic>All values are in the degrees and correspond to the angle subtended at the origin (see <xref ref-type="sec" rid="s3">Stimuli</xref>)</italic>.</p></table-wrap-foot></table-wrap><p>A Kruskal&#x02013;Wallis test showed an effect of Distinctiveness on P50, with sensitivity appearing to be higher for the &#x0201c;anti-caricatured&#x0201d; 0.5 SD condition: <italic>H</italic>(3)&#x02009;=&#x02009;9.09, <italic>p</italic>&#x02009;=&#x02009;0.03. Follow-up Mann&#x02013;Whitney tests with Bonferroni corrected alpha&#x02009;=&#x02009;0.05/6 showed no significant pairwise differences in location between levels (all <italic>p's</italic>&#x02009;&#x0003e;&#x02009;0.1/6).</p><p>There was also an effect of distinctiveness on sensitivity with DL higher, sensitivity lower, for the 0.5 SD condition: <italic>H</italic>(3)&#x02009;=&#x02009;33.9, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. Follow-up Mann&#x02013;Whitney tests with Bonferroni corrected alpha&#x02009;=&#x02009;0.05/6 showed that DL was significantly higher in the 0.5 SD condition than 1.0 SD, <italic>U</italic>&#x02009;=&#x02009;156.5, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.55; 1.5 SD, <italic>U</italic>&#x02009;=&#x02009;95, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.66, and 2.0 SD, <italic>U</italic>&#x02009;=&#x02009;125, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>r</italic>&#x02009;=&#x02009;&#x02212;0.71, conditions. Median false alarms rates (&#x0201c;different&#x0201d; responses at 0&#x000b0; level) were all 0 except for the 0.5 SD condition which had a median rate of 0.1.</p></sec><sec><title>Discussion</title><p>Both criterion and sensitivity expressed as an angle were constant over the SD1 to SD2 range: median P50 was 51.5&#x000b0; and median sensitivity DL 12.3&#x000b0;. This is consistent with an angle based similarity metric predicting perceived differences in identity. The corresponding distances between faces will have doubled over this range and results are clearly not consistent with either P50 or DL reflecting a constant distance as this would have corresponded to a decreasing angle.</p><p>Difference limen was significantly larger for 0.5 SD faces and there was a trend for a higher P50 in this condition. Previous studies have found that anti-caricatures are more poorly recognized than lateral caricatures, contrary to an angle based account (Rhodes et al., <xref ref-type="bibr" rid="B51">1998</xref>; Lewis and Johnston, <xref ref-type="bibr" rid="B30">1999</xref>). The results of Experiment 1 suggest that 0.5 SD faces are perceived as the same as the average face much of the time. The higher P50 and DL found in this experiment suggests that we have difficulty discriminating between them and are less likely to judge them as &#x0201c;different&#x0201d; (see Figure <xref ref-type="fig" rid="F5">5</xref>A, row 1). This maybe why anti-caricatures (0.5 SD) behaved differently from the &#x0201c;normal&#x0201d; (1 SD) or caricatured (1.5 and 2 SD) faces here.</p><p>A criterion angle of 51.5&#x000b0; was found for judgments of faces in the normal (1 SD) to distinctive (2 SD) range. Angular differences account for both criterion and sensitivity over this range. In future it would be important to test if this generalizes to cases where the magnitudes of comparison and reference face identity vectors are not the same, for example when the reference face is distinctive but the comparison face is not.</p></sec></sec><sec><title>General Discussion</title><p>We measured criteria (P50) and sensitivity (DL) for same or different identity decisions in terms of differences in 3D face shape. Increasing physical differences between faces increased the proportion of &#x0201c;different&#x0201d; identity responses throughout. The relationship between physical differences and response proportions provided the basis for estimates of the magnitude of differences perceived as a change in identity.</p><p>In Experiment 1 criterion was the same for a static or an animated change in view, despite the expected difference in sensitivity. This is consistent with criterion being determined by underlying differences between faces and not particular properties of the views shown. Sensitivity for the animated condition was equivalent to identical full-face view comparisons. The latter condition was associated with a significantly smaller criterion difference but both this and the high sensitivity may be accounted for by image rather than face based comparisons. The overall dissociation between sensitivity and criterion emphasizes the need to consider both when seeking to understand and predict both laboratory and real world identity matching.</p><p>Experiment 2 showed that neither criterion nor sensitivity corresponds to Euclidean distance: the major sources of physical variation are not necessarily the most important for decisions about identity. Sensitivity corresponded well with Mahalanobis distance, where each PC is weighted in terms of its variance. However criterion was still disproportionately affected by PC2, although this is associated with less physical variation than PC1 with which it was compared. This may reflect the particular dimensions tested and it was argued that Mahalanobis distance provides a principled way to weight the contribution of different sources of variation that allows dimensions more likely to be associated with individual variation to influence physical distance measures.</p><p>Experiment 3 provided evidence that an angle based difference measure better predicts perceived differences in identity than a distance based measure. Angular criterion and sensitivity were constant over a range where corresponding distances doubled. Observers were less sensitive to differences between &#x0201c;anti-caricatured&#x0201d; faces 0.5 SD from the mean, and the associated criterion was higher.</p><p>In the introduction we argued that perceiving a change in identity involves more than just being able to detect differences between images. Our aim was to measure criterion difference that would correspond to a perceived change in identity at least 50% of the time. The human face recognition system remains one of the best available (although see O'Toole et al., <xref ref-type="bibr" rid="B47">2007</xref>) and an assumption of this work was that the criterion adopted by humans would provide a valuable estimate in the context of automatic or objective comparisons. Criterion difference may also be critical to the problem of knowing who you do not know, the &#x0201c;unfamiliar&#x0201d; response in recognition. Any face seen for the first time will always have a known face that is the closest match and has the potential to lead to a false identification. A criterion difference above which a novel face is correctly categorized as unfamiliar is one possible solution to this issue (Shin and Nosofsky, <xref ref-type="bibr" rid="B56">1992</xref>). While equivalent or very similar decision rules can be formulated based on the ratio of the closest match to one or more other matches (Valentine, <xref ref-type="bibr" rid="B61">1991</xref>; Wills et al., <xref ref-type="bibr" rid="B67">2000</xref>; Ross et al., <xref ref-type="bibr" rid="B54">2010</xref>), understanding how different a face has to be before it is perceived as a different person is the central problem.</p><p>There were considerable individual differences, as evidenced here by IQR for P50 and reported previously (Kemp et al., <xref ref-type="bibr" rid="B26">1997</xref>; Megreya and Burton, <xref ref-type="bibr" rid="B37">2008</xref>). Understanding these is necessary for establishing objective and general criteria but medians provide an initial estimate. It would also be valuable to test if the estimates of criterion do predict which faces are perceived as familiar in recognition tests. It may also be important to try and deliberately manipulate criterion through, for example, instructions. Here we asked people to respond on the basis of whether &#x0201c;the two faces are the same person or not&#x0201d; but it may be necessary to demonstrate that this produces different results than if we ask people to respond on the basis of &#x0201c;any difference.&#x0201d; Sensitivity is estimated by the latter task and we have shown that criterion can change independently of this. Animating views in Experiment 1 increased sensitivity without changing the physical difference that was perceived as a change in identity. It is also potentially interesting to see whether the reverse change in instructions and analysis in terms of criterion and sensitivity would inform interpretation of previously reported experiments (Freire et al., <xref ref-type="bibr" rid="B19">2000</xref>; Maurer et al., <xref ref-type="bibr" rid="B35">2002</xref>).</p><p>The criterion observed did not correspond to step change but both individual and pooled psychometric functions had sigmoid or similar non-linear functions with response rates often constant at the ends of the scale. The comparison between animated and static views in Experiment 1 showed that the underlying psychometric function can be made steeper without shifting criterion. In a sense criterion corresponds to a category boundary between &#x0201c;same&#x0201d; and &#x0201c;different&#x0201d; person although we did not seek to test for categorical perception as such (Harnard, <xref ref-type="bibr" rid="B22">1987</xref>). This would be possible in future, especially measuring discrimination at the category boundary although categorical perception is though to be associated with familiar rather than unfamiliar faces (Mckone et al., <xref ref-type="bibr" rid="B36">2001</xref>; Angeli et al., <xref ref-type="bibr" rid="B2">2008</xref>).</p><p>The broader aim of this work is to link physical and psychological face-spaces. Experiments 2 and 3 provided evidence for Mahalanobis over Euclidean distance as a metric, and for differences in angle over distance as determinants of identity distinctions. Clearly there are many other issues to explore, particularly the optimal weighting of physical dimensions and we are currently doing this by seeking to establish a psychological face-space on the basis of perceived similarity that can be specified in terms of the physical face-space. It will then be necessary to test whether the psychological face-space provides a better account of perceived changes in identity using the methods developed here.</p><p>While the use of 3D shape models has considerable advantages, keeping surface reflectance constant inevitably affects the task and generalization of results. Surface scanning technology is advancing and can now produce near photorealistic results. If accurate surface reflectance information that was independent of lighting could be recorded and modeled, this would provide an even more powerful tool for generating controlled stimuli. The ability to scan faces at increasing frame rates will also allow within face variation to be addressed &#x02013; faces are constantly changing quite dramatically in shape when, for example, we speak, eat, or express emotion although there is no corresponding change in identity. Incorporating such variation in face-space models is a critical challenge for future work especially as within face changes epitomize types of large detectable changes that should not be interpreted as a change in identity.</p></sec><sec><title>Summary</title><p>The criteria adopted when people make same or different identity decisions determine whether two example faces are seen as being the same person or not. The experiments reported provided estimates of the physical differences in 3D face shape that correspond to these criteria. Criterion was also found to dissociate from sensitivity with, for example, P50 but not DL the same for an animated and a static change in view (Experiment 1). Raw Euclidean physical differences in shape did not characterize when faces were seen as different but Mahalanobis distances predicted sensitivity and provide a principled compromise for weighting different sources of variation (Experiment 2). The angles between identity vectors were found to characterize perceived changes in identity better than distances for faces one or more SD from the average (Experiment 3). Taken together, the results demonstrate the importance of considering criterion in addition to sensitivity and suggest that angular differences based on a Mahalanobis metric may provide a good way to link physical to perceived differences.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><p>The Movie <xref ref-type="supplementary-material" rid="SM1">S1</xref> for this article can be found online at <uri xlink:type="simple" xlink:href="http://www.frontiersin.org/Perception_Science/10.3389/fpsyg.2011.00041/abstract">http://www.frontiersin.org/Perception_Science/10.3389/fpsyg.2011.00041/abstract</uri></p><supplementary-material content-type="local-data" id="SM1"><label>Supplementary Movie S1</label><caption><p>Illustration of matching across animated views, the animated GIF condition used in all experiments. The stimuli shown here are taken from Experiment 3 and show a reference face (left) and comparison face (right) where the &#x0201c;identity vectors&#x0201d; subtend an angle of 54&#x000b0; at the origin of the face-space used. Both faces are 1 SD from the average. Please see Experiment 3 and Figure <xref ref-type="fig" rid="F5">5</xref> for details.</p></caption><media xlink:href="Movie1.AVI" mimetype="video" mime-subtype="x-msvideo"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>This work was supported by Australian Research Council Discovery Project 0986898 and Engineering and Physical Sciences Research Council grant EP/F037503/1. Thanks to Harry Matthews for careful and insightful comments on earlier drafts.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aeria</surname><given-names>G.</given-names></name><name><surname>Claes</surname><given-names>P.</given-names></name><name><surname>Vandermeulen</surname><given-names>D.</given-names></name><name><surname>Clement</surname><given-names>J. G.</given-names></name></person-group> (<year>2010</year>). <article-title>Targeting specific facial variation for different identification tasks</article-title>. <source>Forensic Sci. Int.</source> <volume>201</volume>, <fpage>118</fpage>&#x02013;<lpage>124</lpage><pub-id pub-id-type="pmid">20359838</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angeli</surname><given-names>A.</given-names></name><name><surname>Davidoff</surname><given-names>J.</given-names></name><name><surname>Valentine</surname><given-names>T.</given-names></name></person-group> (<year>2008</year>). <article-title>Face familiarity, distinctiveness, and categorical perception</article-title>. <source>Q. J. Exp. Psychol.</source> <volume>61</volume>, <fpage>690</fpage>&#x02013;<lpage>707</lpage></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>S. E.</given-names></name></person-group> (<year>1985</year>). <article-title>The caricature generator</article-title>. <source>Leonardo</source> <volume>18</volume>, <fpage>170</fpage>&#x02013;<lpage>178</lpage><pub-id pub-id-type="doi">10.2307/1578048</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>V.</given-names></name></person-group> (<year>1982</year>). <article-title>Changing faces: visual and non-visual coding processes in face recognition</article-title>. <source>Br. J. Psychol.</source> <volume>73</volume>, <fpage>105</fpage>&#x02013;<lpage>116</lpage><pub-id pub-id-type="pmid">7059746</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Burton</surname><given-names>A. M.</given-names></name><name><surname>Dench</surname><given-names>N.</given-names></name></person-group> (<year>1994</year>). <article-title>What's distinctive about a distinctive face?</article-title> <source>Q. J. Exp. Psychol. A</source> <volume>47</volume>, <fpage>119</fpage>&#x02013;<lpage>141</lpage><pub-id pub-id-type="pmid">8177958</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Burton</surname><given-names>A. M.</given-names></name><name><surname>Hanna</surname><given-names>E.</given-names></name><name><surname>Healey</surname><given-names>P.</given-names></name><name><surname>Mason</surname><given-names>O.</given-names></name><name><surname>Coombes</surname><given-names>A.</given-names></name><name><surname>Fright</surname><given-names>R.</given-names></name><name><surname>Linney</surname><given-names>A.</given-names></name></person-group> (<year>1993</year>). <article-title>Sex discrimination: how do we tell the difference between male and female faces?</article-title> <source>Perception</source> <volume>22</volume>, <fpage>131</fpage>&#x02013;<lpage>152</lpage><pub-id pub-id-type="doi">10.1068/p220131</pub-id><pub-id pub-id-type="pmid">8474840</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Valentine</surname><given-names>T.</given-names></name><name><surname>Baddeley</surname><given-names>A.</given-names></name></person-group> (<year>1987</year>). <article-title>The basis of the 3/4 view advantage in face recognition</article-title>. <source>Appl. Cogn. Psychol.</source> <volume>1</volume>, <fpage>109</fpage>&#x02013;<lpage>120</lpage></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>A. M.</given-names></name><name><surname>Miller</surname><given-names>P.</given-names></name><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Hancock</surname><given-names>P. J. B.</given-names></name><name><surname>Henderson</surname><given-names>Z.</given-names></name></person-group> (<year>2001</year>). <article-title>Human and automatic face recognition: a comparison across image formats</article-title>. <source>Vision Res.</source> <volume>41</volume>, <fpage>3185</fpage>&#x02013;<lpage>3195</lpage><pub-id pub-id-type="pmid">11711142</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>A. M.</given-names></name><name><surname>Vokey</surname><given-names>J. R.</given-names></name></person-group> (<year>1998</year>). <article-title>The face-space typicality paradox: understanding the face-space metaphor</article-title>. <source>Q. J. Exp. Psychol. A</source> <volume>51</volume>, <fpage>475</fpage>&#x02013;<lpage>583</lpage></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapman</surname><given-names>S.</given-names></name><name><surname>Wells</surname><given-names>G.</given-names></name></person-group> (<year>2007</year>). <article-title>Eyewitness lineups: is the appearance-change instruction a good idea?</article-title> <source>Law Hum. Behav.</source> <volume>31</volume>, <fpage>3</fpage>&#x02013;<lpage>22</lpage><pub-id pub-id-type="pmid">16612580</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Claes</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <source>A Robust Statistical Surface Registration Framework using Implicit Function Representations: Applications in Craniofacial Reconstruction</source>. Ph.D. thesis, <publisher-name>Katholieke Universiteit Leuven</publisher-name>, <publisher-loc>Leuven</publisher-loc></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Claes</surname><given-names>P.</given-names></name><name><surname>Vandermeulen</surname><given-names>D.</given-names></name><name><surname>De Greef</surname><given-names>S.</given-names></name><name><surname>Willems</surname><given-names>G.</given-names></name><name><surname>Suetens</surname><given-names>P.</given-names></name></person-group> (<year>2006</year>). <article-title>Craniofacial reconstruction using a combined statistical model of face shape and soft tissue depths: methodology and validation</article-title>. <source>Forensic Sci. Int.</source> <volume>159</volume>, <fpage>S147</fpage>&#x02013;<lpage>S158</lpage><pub-id pub-id-type="pmid">16540276</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dakin</surname><given-names>S. C.</given-names></name><name><surname>Omigie</surname><given-names>D.</given-names></name></person-group> (<year>2009</year>). <article-title>Psychophysical evidence for a non-linear representation of facial identity</article-title>. <source>Vision Res.</source> <volume>49</volume>, <fpage>2285</fpage>&#x02013;<lpage>2296</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.06.016</pub-id><pub-id pub-id-type="pmid">19555705</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>J. P.</given-names></name><name><surname>Valentine</surname><given-names>T.</given-names></name></person-group> (<year>2009</year>). <article-title>CCTV on trial: matching video images with the defendant in the dock</article-title>. <source>Appl. Cogn. Psychol.</source> <volume>23</volume>, <fpage>482</fpage>&#x02013;<lpage>505</lpage></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Maesschalck</surname><given-names>R.</given-names></name><name><surname>Jouan-Rimbaud</surname><given-names>D.</given-names></name><name><surname>Massart</surname><given-names>D. L.</given-names></name></person-group> (<year>2000</year>). <article-title>The Mahalanobis distance</article-title>. <source>Chemom. Intell. Lab. Syst.</source> <volume>50</volume>, <fpage>1</fpage>&#x02013;<lpage>18</lpage></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favelle</surname><given-names>S.</given-names></name><name><surname>Palmisano</surname><given-names>S.</given-names></name><name><surname>Maloney</surname><given-names>R.</given-names></name></person-group> (<year>2007</year>). <article-title>Things are looking up: differential decline in face recognition following pitch and yaw rotation</article-title>. <source>Perception</source> <volume>36</volume>, <fpage>1334</fpage>&#x02013;<lpage>1352</lpage><pub-id pub-id-type="doi">10.1068/p5637</pub-id><pub-id pub-id-type="pmid">18196700</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Field</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <source>Discovering Statistics Using SPSS</source>. <publisher-loc>London</publisher-loc>: <publisher-name>SAGE Publications Ltd</publisher-name></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>K.</given-names></name><name><surname>Butavicius</surname><given-names>M. A.</given-names></name><name><surname>Lee</surname><given-names>M. D.</given-names></name></person-group> (<year>2008</year>). <article-title>Attention to internal face features in unfamiliar face matching</article-title>. <source>Br. J. Psychol.</source> <volume>99</volume>, <fpage>379</fpage>&#x02013;<lpage>394</lpage><pub-id pub-id-type="pmid">17706001</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freire</surname><given-names>A.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Symons</surname><given-names>L. A.</given-names></name></person-group> (<year>2000</year>). <article-title>The face-inversion effect as a deficit in the encoding of configural information: direct evidence</article-title>. <source>Perception</source> <volume>29</volume>, <fpage>159</fpage>&#x02013;<lpage>170</lpage><pub-id pub-id-type="doi">10.1068/p3012</pub-id><pub-id pub-id-type="pmid">10820599</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname><given-names>P. J. B.</given-names></name><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Burton</surname><given-names>A. M.</given-names></name></person-group> (<year>2000</year>). <article-title>Recognition of unfamiliar faces</article-title>. <source>Trends Cogn. Sci.</source> <volume>4</volume>, <fpage>330</fpage>&#x02013;<lpage>337</lpage><pub-id pub-id-type="pmid">10962614</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname><given-names>P. J. B.</given-names></name><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Burton</surname><given-names>M. A.</given-names></name></person-group> (<year>1998</year>). <article-title>A comparison of two computer-based face identification systems with human perceptions of faces</article-title>. <source>Vision Res.</source> <volume>38</volume>, <fpage>2277</fpage>&#x02013;<lpage>2288</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00439-2</pub-id><pub-id pub-id-type="pmid">9797999</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harnard</surname><given-names>S.</given-names></name></person-group> (<year>1987</year>). <source>Categorical Perception: The Ground-work of Cognition</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>H.</given-names></name><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Akamatsu</surname><given-names>S.</given-names></name></person-group> (<year>1995</year>). <article-title>Perceiving the sex and race of faces: the role of shape and colour</article-title>. <source>Proc. R. Soc. Lond., B, Biol. Sci.</source> <volume>261</volume>, <fpage>367</fpage>&#x02013;<lpage>373</lpage></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>H.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Akamatsu</surname><given-names>S.</given-names></name></person-group> (<year>1997</year>). <article-title>Information and viewpoint dependence in face recognition</article-title>. <source>Cognition</source> <volume>62</volume>, <fpage>201</fpage>&#x02013;<lpage>222</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(96)00785-8</pub-id><pub-id pub-id-type="pmid">9141907</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>R. A.</given-names></name><name><surname>Milne</surname><given-names>A. B.</given-names></name><name><surname>Williams</surname><given-names>C.</given-names></name><name><surname>Hosie</surname><given-names>J.</given-names></name></person-group> (<year>1997</year>). <article-title>Do distinctive faces come from outer space</article-title>. <source>Vis. Cogn.</source> <volume>4</volume>, <fpage>59</fpage>&#x02013;<lpage>67</lpage></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemp</surname><given-names>R.</given-names></name><name><surname>Towell</surname><given-names>N.</given-names></name><name><surname>Pike</surname><given-names>G.</given-names></name></person-group> (<year>1997</year>). <article-title>When seeing should not be believing: photographs, credit cards and fraud</article-title>. <source>Appl. Cogn. Psychol.</source> <volume>11</volume>, <fpage>211</fpage>&#x02013;<lpage>222</lpage></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>S. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Measuring, estimating, and understanding the psychometric function: a commentary</article-title>. <source>Percept. Psychophys.</source> <volume>63</volume>, <fpage>1421</fpage>&#x02013;<lpage>1455</lpage><pub-id pub-id-type="pmid">11800466</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>D. A.</given-names></name><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Blanz</surname><given-names>V.</given-names></name></person-group> (<year>2001</year>). <article-title>Prototype-referenced shape encoding revealed by high-level aftereffects</article-title>. <source>Nat. Neurosci.</source> <volume>4</volume>, <fpage>89</fpage>&#x02013;<lpage>94</lpage><pub-id pub-id-type="pmid">11135650</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Face-space-R: towards a unified account of face recognition</article-title>. <source>Vis. Cogn.</source> <volume>11</volume>, <fpage>29</fpage>&#x02013;<lpage>69</lpage></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>M. B.</given-names></name><name><surname>Johnston</surname><given-names>R. A.</given-names></name></person-group> (<year>1999</year>). <article-title>A unified account of the effects of caricaturing faces</article-title>. <source>Vis. Cogn.</source> <volume>6</volume>, <fpage>1</fpage>&#x02013;<lpage>41</lpage></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Light</surname><given-names>L. L.</given-names></name><name><surname>Kayra-Stuart</surname><given-names>F.</given-names></name><name><surname>Hollander</surname><given-names>S.</given-names></name></person-group> (<year>1979</year>). <article-title>Recognition memory for typical and unusual faces</article-title>. <source>J. Exp. Psychol. Hum. Learn. Mem.</source> <volume>5</volume>, <fpage>212</fpage>&#x02013;<lpage>228</lpage></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C. H.</given-names></name><name><surname>Chaudhuri</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>Reassessing the 3/4 view effect in face recognition</article-title>. <source>Cognition</source> <volume>83</volume>, <fpage>31</fpage>&#x02013;<lpage>48</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(01)00164-0</pub-id><pub-id pub-id-type="pmid">11814485</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Macmillan</surname><given-names>N.</given-names></name><name><surname>Creelman</surname><given-names>C.</given-names></name></person-group> (<year>2005</year>). <conf-name>Detection Theory: A User's Guide</conf-name> <conf-loc>Mahwah, NJ</conf-loc>: <conf-sponsor>Lawrence Erlbaum Associates</conf-sponsor></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mahalanobis</surname><given-names>P. C.</given-names></name></person-group> (<year>1936</year>). <article-title>&#x0201c;On the generalised distance in statistics,&#x0201d;</article-title> in <conf-name>Proceedings National Institute of Science</conf-name>, <conf-loc>Calcutta</conf-loc>, <fpage>49</fpage>&#x02013;<lpage>55</lpage></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname><given-names>D.</given-names></name><name><surname>Grand</surname><given-names>R. L.</given-names></name><name><surname>Mondloch</surname><given-names>C. J.</given-names></name></person-group> (<year>2002</year>). <article-title>The many faces of configural processing</article-title>. <source>Trends Cogn. Sci.</source> <volume>6</volume>, <fpage>255</fpage>&#x02013;<lpage>260</lpage><pub-id pub-id-type="pmid">12039607</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mckone</surname><given-names>E.</given-names></name><name><surname>Martini</surname><given-names>P.</given-names></name><name><surname>Nakayama</surname><given-names>K.</given-names></name></person-group> (<year>2001</year>). <article-title>Categorical perception of face identity in noise isolates configural processing</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source> <volume>27</volume>, <fpage>573</fpage>&#x02013;<lpage>599</lpage><pub-id pub-id-type="pmid">11424647</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Megreya</surname><given-names>A. M.</given-names></name><name><surname>Burton</surname><given-names>A. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Matching faces to photographs: poor performance in eyewitness memory (without the memory)</article-title>. <source>J. Exp. Psychol. Appl.</source> <volume>14</volume>, <fpage>364</fpage>&#x02013;<lpage>372</lpage><pub-id pub-id-type="pmid">19102619</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meissner</surname><given-names>C. A.</given-names></name><name><surname>Brigham</surname><given-names>J. C.</given-names></name></person-group> (<year>2001</year>). <article-title>Thirty years of investigating the own-race bias in memory for faces: a meta-analytic review</article-title>. <source>Psychol. Public Policy Law</source> <volume>7</volume>, <fpage>3</fpage>&#x02013;<lpage>35</lpage><pub-id pub-id-type="doi">10.1037/1076-8971.7.1.3</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meissner</surname><given-names>C. A.</given-names></name><name><surname>Tredoux</surname><given-names>C. G.</given-names></name><name><surname>Parker</surname><given-names>J. F.</given-names></name><name><surname>Maclin</surname><given-names>O. H.</given-names></name></person-group> (<year>2005</year>). <article-title>Eyewitness decisions in simultaneous and sequential lineups: a dual-process signal detection theory analysis</article-title>. <source>Mem. Cognit.</source> <volume>33</volume>, <fpage>783</fpage>&#x02013;<lpage>792</lpage><pub-id pub-id-type="pmid">16383167</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>J.</given-names></name><name><surname>Ulrich</surname><given-names>R.</given-names></name></person-group> (<year>2001</year>). <article-title>On the analysis of psychometric functions: the Spearman-K&#x000c3;&#x000a4;rber method</article-title>. <source>Percept. Psychophys.</source> <volume>63</volume>, <fpage>1399</fpage>&#x02013;<lpage>1420</lpage><pub-id pub-id-type="pmid">11800465</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mondloch</surname><given-names>C. J.</given-names></name><name><surname>Grand</surname><given-names>R. L.</given-names></name><name><surname>Maurer</surname><given-names>D.</given-names></name></person-group> (<year>2002</year>). <article-title>Configural face processing develops more slowly than featural face processing</article-title>. <source>Perception</source> <volume>31</volume>, <fpage>553</fpage>&#x02013;<lpage>566</lpage><pub-id pub-id-type="doi">10.1068/p3339</pub-id><pub-id pub-id-type="pmid">12044096</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moses</surname><given-names>Y.</given-names></name><name><surname>Ullman</surname><given-names>S.</given-names></name><name><surname>Edelman</surname><given-names>S.</given-names></name></person-group> (<year>1996</year>). <article-title>Generalization to novel images in upright and inverted images</article-title>. <source>Perception</source> <volume>25</volume>, <fpage>443</fpage>&#x02013;<lpage>461</lpage><pub-id pub-id-type="doi">10.1068/p250443</pub-id><pub-id pub-id-type="pmid">8817621</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Abdi</surname><given-names>H.</given-names></name><name><surname>Deffenbacher</surname><given-names>K. A.</given-names></name><name><surname>Valentin</surname><given-names>D.</given-names></name></person-group> (<year>1993</year>). <article-title>Low-dimensional representation of faces in higher dimensions of the face space</article-title>. <source>J. Opt. Soc. Am. A</source> <volume>10</volume>, <fpage>405</fpage>&#x02013;<lpage>411</lpage></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Edelman</surname><given-names>S.</given-names></name><name><surname>Bulthoff</surname><given-names>H. H.</given-names></name></person-group> (<year>1998</year>). <article-title>Stimulus-specific effects in face recognition over changes in viewpoint</article-title>. <source>Vision Res.</source> <volume>38</volume>, <fpage>2351</fpage>&#x02013;<lpage>2363</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(98)00042-X</pub-id><pub-id pub-id-type="pmid">9798004</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Leopold</surname><given-names>D. A.</given-names></name><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Blanz</surname><given-names>V.</given-names></name></person-group> (<year>2001a</year>). <article-title>Prototype-referenced shape perception: adaptation and aftereffects in a multidimensional face space</article-title>. <source>J. Vis.</source> <volume>1</volume>, <fpage>332</fpage><pub-id pub-id-type="doi">10.1167/1.3.332</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Wenger</surname><given-names>M. J.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name></person-group> (<year>2001b</year>). <article-title>&#x0201c;Quantitative models of perceiving and remembering faces: precedents and possibilities,&#x0201d;</article-title> in <source>Computational, Geometric, and Process Perspectives on Facial Cognition</source>, eds <person-group person-group-type="editor"><name><surname>Wenger</surname><given-names>M. J.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name></person-group> (<publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>38</lpage></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Phillips</surname><given-names>P. J.</given-names></name><name><surname>Jiang</surname><given-names>F.</given-names></name><name><surname>Ayyad</surname><given-names>J.</given-names></name><name><surname>Penard</surname><given-names>N.</given-names></name><name><surname>Parent</surname><given-names>M. A.</given-names></name></person-group> (<year>2007</year>). <article-title>Face recogntion algorithms surpass humans matching acros changes in illumination</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source> <volume>29</volume>, <fpage>1642</fpage>&#x02013;<lpage>1646</lpage><pub-id pub-id-type="pmid">17627050</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Price</surname><given-names>T.</given-names></name><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Bartlett</surname><given-names>J. C.</given-names></name><name><surname>Blanz</surname><given-names>V.</given-names></name></person-group> (<year>1999</year>). <article-title>3D shape and 2D surface textures of human faces: the role of &#x0201c;averages&#x0201d; in attractiveness and age</article-title>. <source>Image Vis. Comput.</source> <volume>18</volume>, <fpage>9</fpage>&#x02013;<lpage>19</lpage></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Volz</surname><given-names>H.</given-names></name><name><surname>Salter</surname><given-names>E. M.</given-names></name></person-group> (<year>1997</year>). <article-title>Three-dimensional caricatures of human heads: distinctiveness and the perception of facial age</article-title>. <source>Perception</source> <volume>26</volume>, <fpage>719</fpage>&#x02013;<lpage>732</lpage><pub-id pub-id-type="pmid">9474342</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G.</given-names></name></person-group> (<year>1996</year>). <source>Superportaits</source>. <publisher-loc>Hove</publisher-loc>: <publisher-name>Psychology Press</publisher-name></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G.</given-names></name><name><surname>Carey</surname><given-names>S.</given-names></name><name><surname>Byatt</surname><given-names>G.</given-names></name><name><surname>Proffitt</surname><given-names>F.</given-names></name></person-group> (<year>1998</year>). <article-title>Coding spatial variations in faces and simple shapes: a test of two models</article-title>. <source>Vision Res.</source> <volume>38</volume>, <fpage>2307</fpage>&#x02013;<lpage>2321</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00470-7</pub-id><pub-id pub-id-type="pmid">9798001</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G.</given-names></name><name><surname>Maloney</surname><given-names>L. T.</given-names></name><name><surname>Turner</surname><given-names>J.</given-names></name><name><surname>Ewing</surname><given-names>L.</given-names></name></person-group> (<year>2007</year>). <article-title>Adaptive face coding and discrimination around the average face</article-title>. <source>Vision Res.</source> <volume>47</volume>, <fpage>974</fpage>&#x02013;<lpage>989</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.12.010</pub-id><pub-id pub-id-type="pmid">17316740</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Romdhani</surname><given-names>S.</given-names></name><name><surname>Blanz</surname><given-names>V.</given-names></name><name><surname>Basso</surname><given-names>C.</given-names></name><name><surname>Vetter</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Morphable models of faces,&#x0201d;</article-title> in <source>Handbook of Face Recognition</source>, eds <person-group person-group-type="editor"><name><surname>Li</surname><given-names>S. Z.</given-names></name><name><surname>Jain</surname><given-names>A. K.</given-names></name></person-group> (<publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>217</fpage>&#x02013;<lpage>245</lpage></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>D. A.</given-names></name><name><surname>Hancock</surname><given-names>P. J. B.</given-names></name><name><surname>Lewis</surname><given-names>M. B.</given-names></name></person-group> (<year>2010</year>). <article-title>Changing faces: direction is important</article-title>. <source>Vis. Cogn.</source> <volume>18</volume>, <fpage>67</fpage>&#x02013;<lpage>81</lpage></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>R.</given-names></name><name><surname>Biederman</surname><given-names>I.</given-names></name><name><surname>Nederhouser</surname><given-names>M.</given-names></name><name><surname>Sinha</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>The utility of surface reflectance for the recognition of upright and inverted faces</article-title>. <source>Vision Res.</source> <volume>47</volume>, <fpage>157</fpage>&#x02013;<lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.11.002</pub-id><pub-id pub-id-type="pmid">17174375</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>H. J.</given-names></name><name><surname>Nosofsky</surname><given-names>R. M.</given-names></name></person-group> (<year>1992</year>). <article-title>Similarity-scaling studies of dot-pattern classification and recognition</article-title>. <source>J. Exp. Psychol. Gen.</source> <volume>121</volume>, <fpage>278</fpage>&#x02013;<lpage>304</lpage><pub-id pub-id-type="pmid">1402702</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slone</surname><given-names>A. E.</given-names></name><name><surname>Brigham</surname><given-names>J. C.</given-names></name><name><surname>Meissner</surname><given-names>C. A.</given-names></name></person-group> (<year>2000</year>). <article-title>Social and cognitive factors affecting the own-race bias in Whites</article-title>. <source>Basic Appl. Soc. Psych.</source> <volume>22</volume>, <fpage>71</fpage>&#x02013;<lpage>84</lpage></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troje</surname><given-names>N. F.</given-names></name><name><surname>Bulthoff</surname><given-names>H. H.</given-names></name></person-group> (<year>1996</year>). <article-title>Face recognition under varying poses: the role of texture and shape</article-title>. <source>Vision Res.</source> <volume>36</volume>, <fpage>1761</fpage>&#x02013;<lpage>1771</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00230-8</pub-id><pub-id pub-id-type="pmid">8759445</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troje</surname><given-names>N. F.</given-names></name><name><surname>Bulthoff</surname><given-names>H. H.</given-names></name></person-group> (<year>1998</year>). <article-title>How is bilateral symmetry of human faces used for recognition of novel views?</article-title> <source>Vision Res.</source> <volume>38</volume>, <fpage>79</fpage>&#x02013;<lpage>89</lpage><pub-id pub-id-type="pmid">9474378</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk</surname><given-names>M.</given-names></name><name><surname>Pentland</surname><given-names>A.</given-names></name></person-group> (<year>1991</year>). <article-title>Eigenfaces for recognition</article-title>. <source>J. Cogn. Neurosci.</source> <volume>3</volume>, <fpage>71</fpage>&#x02013;<lpage>86</lpage></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T.</given-names></name></person-group> (<year>1991</year>). <article-title>A unified account of the effects of distinctiveness, inversion and race in face recognition</article-title>. <source>Q. J. Exp. Psychol.</source> <volume>44A</volume>, <fpage>161</fpage>&#x02013;<lpage>204</lpage></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Valentine</surname><given-names>T.</given-names></name></person-group> (ed.). (<year>1995</year>). <source>Cognitive and Computational Aspects of Face Recognition: Explorations in Face Space</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T.</given-names></name></person-group> (<year>2001</year>). <article-title>&#x0201c;Face-space models of face recognition,&#x0201d;</article-title> in <source>Computational, Geometric and Process Perspectives on Facial Cognition: Contexts and Challenges</source>, eds <person-group person-group-type="editor"><name><surname>Wenger</surname><given-names>M. J.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name></person-group> (<publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates Inc</publisher-name>), <fpage>83</fpage>&#x02013;<lpage>114</lpage></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T.</given-names></name><name><surname>Bruce</surname><given-names>V.</given-names></name></person-group> (<year>1986</year>). <article-title>The effects of distinctiveness in recognising and classifying faces</article-title>. <source>Perception</source> <volume>15</volume>, <fpage>525</fpage>&#x02013;<lpage>535</lpage><pub-id pub-id-type="doi">10.1068/p150525</pub-id><pub-id pub-id-type="pmid">3588212</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Blanz</surname><given-names>V.</given-names></name></person-group> (<year>1999</year>). <article-title>&#x0201c;A morphable model for the synthesis of 3D faces,&#x0201d;</article-title> in <source>SIGGRAPH</source>, ed. <person-group person-group-type="editor"><name><surname>Rockwood</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Addison Wesley</publisher-name>), <fpage>187</fpage>&#x02013;<lpage>194</lpage></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vokey</surname><given-names>J. R.</given-names></name><name><surname>Read</surname><given-names>J. D.</given-names></name></person-group> (<year>1992</year>). <article-title>Familiarity, memorability, and the effect of typicality on the recogntion of faces</article-title>. <source>Mem. Cognit.</source> <volume>20</volume>, <fpage>291</fpage>&#x02013;<lpage>302</lpage><pub-id pub-id-type="pmid">1508054</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wills</surname><given-names>A. J.</given-names></name><name><surname>Reimers</surname><given-names>S.</given-names></name><name><surname>Stewart</surname><given-names>N.</given-names></name><name><surname>Suret</surname><given-names>M.</given-names></name><name><surname>Mclaren</surname><given-names>I. P. L.</given-names></name></person-group> (<year>2000</year>). <article-title>Tests of the ratio rule in categorization</article-title>. <source>Q. J. Exp. Psychol. A</source> <volume>53</volume>, <fpage>983</fpage>&#x02013;<lpage>1011</lpage><pub-id pub-id-type="pmid">11131824</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>H. R.</given-names></name><name><surname>Loffler</surname><given-names>G.</given-names></name><name><surname>Wilkinson</surname><given-names>F.</given-names></name></person-group> (<year>2002</year>). <article-title>Synthetic faces, face cubes, and the geometry of face space</article-title>. <source>Vision Res.</source> <volume>42</volume>, <fpage>2909</fpage>&#x02013;<lpage>2923</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(02)00362-0</pub-id><pub-id pub-id-type="pmid">12450502</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshino</surname><given-names>M.</given-names></name><name><surname>Matsuda</surname><given-names>H.</given-names></name><name><surname>Kubota</surname><given-names>S.</given-names></name><name><surname>Imaizumi</surname><given-names>K.</given-names></name><name><surname>Miyasaka</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>Computer-assisted facial image identification system using a 3-D physiognomic range finder</article-title>. <source>Forensic Sci. Int.</source> <volume>109</volume>, <fpage>225</fpage>&#x02013;<lpage>237</lpage><pub-id pub-id-type="pmid">10725658</pub-id></mixed-citation></ref></ref-list></back></article>