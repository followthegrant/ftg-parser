<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30286094</article-id><article-id pub-id-type="pmc">6171860</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0204638</article-id><article-id pub-id-type="publisher-id">PONE-D-17-17983</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Electromagnetic Radiation</subject><subj-group><subject>Light</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Age Groups</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Age Groups</subject><subj-group><subject>Elderly</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Assessing the pedestrian response to urban outdoor lighting: A full-scale laboratory study</article-title><alt-title alt-title-type="running-head">Pedestrian response to urban outdoor lighting</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7331-1286</contrib-id><name><surname>Rahm</surname><given-names>Johan</given-names></name><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="corresp" rid="cor001">*</xref><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Johansson</surname><given-names>Maria</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Project administration</role><role content-type="http://credit.casrai.org/">Resources</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"/></contrib></contrib-group><aff id="aff001"><addr-line>Environmental Psychology, Department of Architecture and Built Environment, Lund University, Lund, Sweden</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Chen</surname><given-names>Peng</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Beihang University, CHINA</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>johan.rahm@arkitektur.lth.se</email></corresp></author-notes><pub-date pub-type="epub"><day>4</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>13</volume><issue>10</issue><elocation-id>e0204638</elocation-id><history><date date-type="received"><day>10</day><month>5</month><year>2017</year></date><date date-type="accepted"><day>12</day><month>9</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 Rahm, Johansson</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Rahm, Johansson</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0204638.pdf"/><abstract><p>This study identifies and applies methods for evaluating the human response to pedestrian lighting applications intended for future use by the municipality of Malm&#x000f6;, Sweden. The methods employed provide a supplementary perspective to that given by the photometric properties of the lighting applications. The study involved 89 participants from two age groups (Young: N: 43, 19&#x02013;31 yrs.; Elderly: N: 46, 62&#x02013;77 yrs.). Data were collected in a full-scale laboratory using a mock-up pedestrian pathway. Three lighting applications (one ceramic metal halide and two LED) were presented and the participants&#x02019; behavior (walking speed), perception (ability to perform visual tasks&#x02013;recognize facial expressions, detect obstacles, read street signpost), affective response, and evaluation of the lighting quality were assessed.</p><p>The three lighting applications significantly differed with regard to the human response. The facial expression recognition distance, sign reading distance and the obstacle detection task, along with the evaluation of lighting quality and level of arousal, distinguished one of the LEDs (Correlated Color Temperature: 3810, Color Rendering Index: 75, Scotopic/Photopic ratio: 1.48) from the other two lighting applications&#x02013;the participants performed better on the visual tasks, and the lighting was perceived as brighter, more arousing and less pleasant. Methods to capture human perception, evaluation and behavior in relation to outdoor lighting, provide a valuable perspective that should be systematically applied when municipalities consider different pedestrian lighting applications.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004527</institution-id><institution>Energimyndigheten</institution></institution-wrap></funding-source><award-id>2012-00-3180</award-id><principal-award-recipient><name><surname>Johansson</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>European Regional Development Fund</institution></funding-source><award-id>NYPS 20200430</award-id><principal-award-recipient>Maria Johansson</principal-award-recipient></award-group><funding-statement>This research was funded by the Swedish energy agency, dnr: 2012-00-3180 (<ext-link ext-link-type="uri" xlink:href="http://www.energimyndigheten.se">www.energimyndigheten.se</ext-link>), and by the INTERREG-project Lighting Metropolis, funded by the European Regional Development Fund, NYPS 20200430 (<ext-link ext-link-type="uri" xlink:href="http://www.lightingmetropolis.com">www.lightingmetropolis.com</ext-link>).</funding-statement></funding-group><counts><fig-count count="7"/><table-count count="4"/><page-count count="20"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All data files are available from the Swedish National Data Service (accession number doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5878/cy2y-wj90">10.5878/cy2y-wj90</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All data files are available from the Swedish National Data Service (accession number doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5878/cy2y-wj90">10.5878/cy2y-wj90</ext-link>).</p></notes></front><body><sec id="sec001"><title>1. Introduction</title><p>In the Nordic countries, daylight hours are very limited in winter. This makes pedestrians dependent on outdoor lighting for ensuring functional levels of visual accessibility and perceived safety when getting to and from work and while going about their everyday activities. Several studies support the importance of outdoor lighting for the walkability of a neighborhood [<xref rid="pone.0204638.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0204638.ref004" ref-type="bibr">4</xref>] and it has been found to increase the level of walking after dark among all age groups [<xref rid="pone.0204638.ref005" ref-type="bibr">5</xref>&#x02013;<xref rid="pone.0204638.ref011" ref-type="bibr">11</xref>].</p><p>However, the advantages of artificial lighting come at a cost, both environmental and financial. There is potential for reducing this cost through saving between 30&#x02013;50% of the total annual lighting energy use [<xref rid="pone.0204638.ref012" ref-type="bibr">12</xref>] by updating existing outdoor lighting installations in terms of design and more energy-efficient light sources [<xref rid="pone.0204638.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0204638.ref014" ref-type="bibr">14</xref>]. When updating existing lighting applications, both technical and human aspects should be considered, to find pedestrian-friendly lighting solutions that are adapted to user needs while minimizing energy use. Special consideration must be taken to users from vulnerable groups, such as the elderly and the visually impaired [<xref rid="pone.0204638.ref015" ref-type="bibr">15</xref>].</p><p>In Sweden, many municipalities are on the verge of updating their outdoor lighting infrastructure, due to the age and state of existing installations, as well as for energy-conservation reasons [<xref rid="pone.0204638.ref016" ref-type="bibr">16</xref>]. In view of the sheer scale of the investments, much is won if the human perspective is applied early in the decision-making process.</p><p>The technical aspects of lighting have a set of standard measures stipulated in national and international standards [<xref rid="pone.0204638.ref017" ref-type="bibr">17</xref>&#x02013;<xref rid="pone.0204638.ref019" ref-type="bibr">19</xref>]. However, there is to date no consensus on measures to capture pedestrians&#x02019; response to outdoor lighting. This study, part of the EU project Lighting Metropolis [<xref rid="pone.0204638.ref020" ref-type="bibr">20</xref>] and a collaboration with the City of Malm&#x000f6;, evaluates the human response to three different outdoor lighting applications proposed by the city authority for future use in pedestrian environments in Malm&#x000f6;. In order to employ a within-subjects design with a wide variety of measures and in order to reduce confounding factors (such as traffic, other pedestrians and weather), a mock-up in a laboratory was used.</p><sec id="sec002"><title>1.1 Previous research</title><p>Using a systematic literature review of the research on the human response to outdoor lighting [<xref rid="pone.0204638.ref021" ref-type="bibr">21</xref>] as a starting point, the study set out to assess differences between lighting applications in terms of the human response.</p><p>The systematic review suggested that three major themes of human responses were relevant for the design of new energy-efficient pedestrian-friendly lighting installations. The first theme, &#x02018;Perception of the lit environment&#x02019;, concerns how light is perceived differently depending on individual factors, such as age and eyesight, and on the various characteristics of the light source, especially at mesopic vision. The second and third themes concern how the individual responds to the perceived environment, either psychologically, &#x02018;Evaluation of the lit environment&#x02019;, or physically, &#x02018;Behavior in the lit environment&#x02019; [<xref rid="pone.0204638.ref021" ref-type="bibr">21</xref>] (<xref ref-type="fig" rid="pone.0204638.g001">Fig 1</xref>). Methods associated with each theme proposed in the literature and considered potentially feasible for evaluating outdoor lighting are described below.</p><fig id="pone.0204638.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g001</object-id><label>Fig 1</label><caption><title>Overarching themes and corresponding categories identified through the systematic literature review.</title><p>Categories included in the study are marked in bold.</p></caption><graphic xlink:href="pone.0204638.g001"/></fig><p>Within the theme human perception of the lit environment the proposed tasks related to facial recognition and obstacle detection were deemed feasible for use in a practical setting, so were included in the laboratory study. Facial recognition is considered to be an important visual task for pedestrians after dark, since judging the intent of oncoming pedestrians from a safe distance is deemed important for the perception of safety [<xref rid="pone.0204638.ref022" ref-type="bibr">22</xref>]. This has been investigated both in laboratory settings [<xref rid="pone.0204638.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pone.0204638.ref028" ref-type="bibr">28</xref>] and in the field [<xref rid="pone.0204638.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0204638.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0204638.ref029" ref-type="bibr">29</xref>&#x02013;<xref rid="pone.0204638.ref033" ref-type="bibr">33</xref>], using photographs, dummies or assistants as targets that should be recognized by the respondents. A common technique has been to measure the distance at which the respondents are certain of the gender, can guess the identity, or make out facial features. It is suggested, however, that recognition of facial <italic>expressions</italic> may be a more relevant task for evaluating lighting quality, and more ecologically valid due to its greater relevance for judging the intent of oncoming strangers [<xref rid="pone.0204638.ref034" ref-type="bibr">34</xref>].</p><p>The second task, obstacle detection, has been explored in the laboratory using apparatus with adjustable obstacle heights [<xref rid="pone.0204638.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0204638.ref036" ref-type="bibr">36</xref>] and by using a mock-up path with a false floor displaying obstacles at various heights [<xref rid="pone.0204638.ref037" ref-type="bibr">37</xref>, <xref rid="pone.0204638.ref038" ref-type="bibr">38</xref>]. An inverse relationship has been found between illuminance level and the obstacle height needed for it to be detected, and at low light levels (0.2 lx) younger participants detect significantly lower obstacles than older participants [<xref rid="pone.0204638.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0204638.ref037" ref-type="bibr">37</xref>]. Also, at equally low light levels, lighting applications with higher scotopic/photopic (S/P) ratios consistently assisted detection of lower obstacles. At higher illuminance levels (2 &#x00026; 20 lx) the differences due to age and Spectral Power Distribution (SPD) were non-significant [<xref rid="pone.0204638.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0204638.ref037" ref-type="bibr">37</xref>].</p><p>Within the second overarching theme, the evaluation of the lit environment, perceived lighting quality and the impact of lighting on the emotional state were selected for use in the laboratory. Perceived lighting quality adds the human evaluation component to the assessment of lighting quality and complements the technical environmental assessment measures. It has been conceptualized in different ways [<xref rid="pone.0204638.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0204638.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0204638.ref039" ref-type="bibr">39</xref>&#x02013;<xref rid="pone.0204638.ref043" ref-type="bibr">43</xref>]. In a recent effort to assess the perceived outdoor lighting quality, Johansson et al. [<xref rid="pone.0204638.ref015" ref-type="bibr">15</xref>] and Kuhn et al. [<xref rid="pone.0204638.ref014" ref-type="bibr">14</xref>] used the dimensions brightness and hedonic tone, previously used by K&#x000fc;ller and Wetterberg [<xref rid="pone.0204638.ref044" ref-type="bibr">44</xref>, <xref rid="pone.0204638.ref045" ref-type="bibr">45</xref>] to describe subjective impressions. Their concept has since been further developed by the development of the Perceived Outdoor Lighting Quality scale (POLQ) [<xref rid="pone.0204638.ref039" ref-type="bibr">39</xref>], which distinguishes between lighting applications on two indices, the Perceived Strength Quality (PSQ) and the Perceived Comfort Quality (PCQ) index. In addition to the more readily accessible qualitative evaluation of lighting properties, it has also been found that lighting may affect the emotional state of the individual, regarding the level of arousal and pleasure [<xref rid="pone.0204638.ref043" ref-type="bibr">43</xref>, <xref rid="pone.0204638.ref046" ref-type="bibr">46</xref>, <xref rid="pone.0204638.ref047" ref-type="bibr">47</xref>].</p><p>Within the third overarching theme, behavior in the lit environment, walking speed was deemed to be a measure with potential to differentiate between the three lighting applications. Studies indicate that illuminance level may have tangible effects on pedestrian behavior. Insufficient lighting is suggested to decrease walking speed [<xref rid="pone.0204638.ref048" ref-type="bibr">48</xref>, <xref rid="pone.0204638.ref049" ref-type="bibr">49</xref>] and make people look at the ground more, compared to brighter conditions [<xref rid="pone.0204638.ref050" ref-type="bibr">50</xref>].</p></sec><sec id="sec003"><title>1.2 Aim and hypotheses</title><p>The aim of this study was to assess differences between three lighting applications fulfilling requirements for current Swedish standards on illuminance levels at pedestrian paths but differing in photometric qualities with regard to light distribution, Correlated Color Temperature (CCT), SPD and glare. The lighting applications were chosen on the basis of being considered for future use at pedestrian paths in the city of Malm&#x000f6;, Sweden.</p><p>The three overarching themes of human perception, evaluation and behavior in response to artificial outdoor lighting were addressed, focusing on the previously researched aspects that were feasible to use in an outdoor setting and that would be relevant and of practical use in the decision-making processes of municipalities. Perception involved obstacle detection, facial recognition, sign reading; Evaluation involved perceived quality of lighting, emotional state; and for Behavior, walking speed was measured.</p><p>To identify lighting applications suitable for all age groups, a further aim was to test whether people from different age groups responded differently to the three lighting applications.</p></sec></sec><sec id="sec004"><title>2. Method</title><sec id="sec005"><title>2.1 Participants</title><p>The study comprised 89 participants divided into two age groups: one group of young people (N: 43, aged 19&#x02013;31 years, mean age: 22, 47% female) and one group of elderly (N: 46, aged 62&#x02013;77, mean age: 69, 54% female). The participants were recruited through information meetings at organizations for the elderly, in public places, on the university campus and through personal networks. The participants&#x02019; visual acuity and contrast vision were tested using Sloan optotypes on a visual acuity board (by Precision Vision) at a distance of 3 m. When applicable, participants wore the same glasses as normally worn outdoors. Color vision was tested with the Ishihara Color Vision Test (Young: Acuity both eyes: min = 0.77 max = 1.25, mean = 1.21; Contrast: min = 0.0 max = 0.5, mean = 0.46; Full color vision 98%; Elderly: Acuity both eyes min = 0.46, max = 1.25, mean = 1.00; Contrast min = 0.0 max = 0.5, mean = 0.30, Full color vision 85%).</p></sec><sec id="sec006"><title>2.2 Ethics statement</title><p>This study was carried out in accordance with the rules and regulations laid down by the Ethics Committee for the Swedish Research Council [<xref rid="pone.0204638.ref051" ref-type="bibr">51</xref>] after consultation with the Regional Ethical Review Board. The Board concluded that approval according to the Swedish Ethical Review Act was not needed for this study. Information about the aim of the study was given and written informed consent was obtained from all participants in accordance with the Declaration of Helsinki. The participants were informed of their right to withdraw at any time without giving an explanation. Personal information was anonymized to retain the privacy of the participants, who received approximately 52 EUR after participation as remuneration.</p></sec><sec id="sec007"><title>2.3 Setting and lighting applications</title><p>The study was carried out in a laboratory (14.5 x 18.5 x 4.5 m) where a 23-meter long and 2.5-meter wide pedestrian path was constructed along the diagonal axis, with two lampposts with the height of 4 meters placed 16 meters apart (1.5 and 17.5 meters from the starting point) (<xref ref-type="fig" rid="pone.0204638.g002">Fig 2</xref>). The lampposts were of the same height as the standard for Malm&#x000f6; municipality, but were placed closer together than the normal 21 meters. The walls were covered in black cloth and the floor had a graphite-grey carpet with a reflection factor of 5%. Adjacent to the laboratory was a waiting room with a lighting design resembling that of an apartment in Sweden (<inline-formula id="pone.0204638.e001"><alternatives><graphic xlink:href="pone.0204638.e001.jpg" id="pone.0204638.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mn>292</mml:mn><mml:mspace width="0.25em"/><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>). The participants were thus not dark-adapted when they entered the laboratory. The scenario was designed to imitate the process of going outside from a brightly lit home in the evening.</p><fig id="pone.0204638.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g002</object-id><label>Fig 2</label><caption><title>Photograph of the mock-up pathway (placed diagonally in the full-scale laboratory) under lit conditions.</title></caption><graphic xlink:href="pone.0204638.g002"/></fig><p>The three lamps were one ceramic metal halide (Lighting application A) and two LEDs (Lighting applications B &#x00026; C) (<xref rid="pone.0204638.t001" ref-type="table">Table 1</xref>, for SPD see <xref ref-type="fig" rid="pone.0204638.g003">Fig 3</xref>). The differences in light distribution are manifested in the horizontal illuminance gradient maps (<xref ref-type="fig" rid="pone.0204638.g004">Fig 4</xref>). The horizontal illuminance (E<sub>H</sub>) on the path varied between 7 and 86 lx (Lighting application A, E<sub>H</sub>: 7&#x02013;45 lx, <inline-formula id="pone.0204638.e002"><alternatives><graphic xlink:href="pone.0204638.e002.jpg" id="pone.0204638.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 24 lx; Lighting application B, E<sub>H</sub>: 17&#x02013;86 lx, <inline-formula id="pone.0204638.e003"><alternatives><graphic xlink:href="pone.0204638.e003.jpg" id="pone.0204638.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 46 lx; Lighting application C, E<sub>H</sub>: 14&#x02013;67 lx, <inline-formula id="pone.0204638.e004"><alternatives><graphic xlink:href="pone.0204638.e004.jpg" id="pone.0204638.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 41 lx). The discomfort glare de Boer rating was calculated for each lighting application based upon the vertical illuminance at the eye provided by the glare source (E<sub>g</sub>), the vertical illuminance at the eye provided by ambient light (E<sub>b</sub>),and the horizontal viewing angle between the viewing direction and the luminaire (&#x0019f;), following the model proposed by Lin et. al. [<xref rid="pone.0204638.ref052" ref-type="bibr">52</xref>] (<xref ref-type="disp-formula" rid="pone.0204638.e005">Eq 1</xref>, <xref ref-type="fig" rid="pone.0204638.g005">Fig 5</xref>). E<sub>g</sub> was measured every meter, in the middle of the path, at a height of 1.5 meters. E<sub>b</sub> was estimated by calculating the average of three measures of the vertical illuminance reflected from the walls of the laboratory.</p><disp-formula id="pone.0204638.e005"><alternatives><graphic xlink:href="pone.0204638.e005.jpg" id="pone.0204638.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>7.09</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2.21</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1.02</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="0.25em"/><mml:mi>x</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x0019f;</mml:mi></mml:mrow><mml:mrow><mml:mn>1.62</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></alternatives><label>(1)</label></disp-formula><fig id="pone.0204638.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g003</object-id><label>Fig 3</label><caption><title>Normalized SPD for the three lighting applications.</title><p>Relative power is depicted on the y-axis and wavelength on the x-axis.</p></caption><graphic xlink:href="pone.0204638.g003"/></fig><fig id="pone.0204638.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g004</object-id><label>Fig 4</label><caption><p>Gradient maps illustrating the horizontal illuminance for lighting applications A, B and C, border of the path, location of lampposts, starting point, obstacle detection area and locations of the photograph and street signpost. Illuminance was measured using a grid of 1x1 m. The grid followed the contours of the irregularly shaped laboratory.</p></caption><graphic xlink:href="pone.0204638.g004"/></fig><fig id="pone.0204638.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g005</object-id><label>Fig 5</label><caption><title>Discomfort glare de Boer rating calculations based on vertical illuminance by the light source, vertical illuminance by ambient light and the horizontal viewing angle.</title><p>(de Boer scale: 1 = unbearable, 3 = disturbing, 5 = just permissible, 7 = satisfactory, 9 = just noticeable).</p></caption><graphic xlink:href="pone.0204638.g005"/></fig><table-wrap id="pone.0204638.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.t001</object-id><label>Table 1</label><caption><title>Technical specifications for the lighting applications.</title></caption><alternatives><graphic id="pone.0204638.t001g" xlink:href="pone.0204638.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="right" rowspan="1" colspan="1">Lighting application</th><th align="right" rowspan="1" colspan="1">A</th><th align="right" rowspan="1" colspan="1">B</th><th align="right" rowspan="1" colspan="1">C</th></tr></thead><tbody><tr><td align="right" rowspan="1" colspan="1">Light source</td><td align="right" rowspan="1" colspan="1">CMH</td><td align="right" rowspan="1" colspan="1">LED</td><td align="right" rowspan="1" colspan="1">LED</td></tr><tr><td align="right" rowspan="1" colspan="1">Power (W)</td><td align="right" rowspan="1" colspan="1">68</td><td align="right" rowspan="1" colspan="1">36</td><td align="right" rowspan="1" colspan="1">93</td></tr><tr><td align="right" rowspan="1" colspan="1">Luminous efficacy (lm/W)</td><td align="right" rowspan="1" colspan="1">50</td><td align="right" rowspan="1" colspan="1">94</td><td align="right" rowspan="1" colspan="1">64</td></tr><tr><td align="right" rowspan="1" colspan="1">Path mean horizontal illuminance (lx)</td><td align="right" rowspan="1" colspan="1">24</td><td align="right" rowspan="1" colspan="1">46</td><td align="right" rowspan="1" colspan="1">41</td></tr><tr><td align="right" rowspan="1" colspan="1">Laboratory mean horizontal illuminance (lx)</td><td align="right" rowspan="1" colspan="1">18</td><td align="right" rowspan="1" colspan="1">28</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">32</td></tr><tr><td align="right" rowspan="1" colspan="1">S/P</td><td align="right" rowspan="1" colspan="1">1.25</td><td align="right" rowspan="1" colspan="1">1.22</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">1.48</td></tr><tr><td align="right" rowspan="1" colspan="1">CCT (K)</td><td align="right" rowspan="1" colspan="1">2890</td><td align="right" rowspan="1" colspan="1">2912</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">3810</td></tr><tr><td align="right" rowspan="1" colspan="1">Color rendering index, CRI</td><td align="right" rowspan="1" colspan="1">81</td><td align="right" rowspan="1" colspan="1">75</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">75</td></tr><tr><td align="right" rowspan="1" colspan="1">Face luminance (cd/m<sup>2</sup>)</td><td align="right" rowspan="1" colspan="1">0.28</td><td align="right" rowspan="1" colspan="1">0.36</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.55</td></tr><tr><td align="right" rowspan="1" colspan="1">Sign luminance (cd/m<sup>2</sup>)</td><td align="right" rowspan="1" colspan="1">0.13</td><td align="right" rowspan="1" colspan="1">0.13</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.35</td></tr><tr><td align="right" rowspan="1" colspan="1">Obstacle illuminance range (lx)</td><td align="right" rowspan="1" colspan="1">4&#x02013;8</td><td align="right" rowspan="1" colspan="1">2&#x02013;15</td><td align="right" style="background-color:#FFFFFF" rowspan="1" colspan="1">10&#x02013;21</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec008"><title>2.4 Measurements</title><sec id="sec009"><title>2.4.1 Perception</title><p>Three tasks were used to evaluate visual accessibility: obstacle detection, facial expression recognition, and sign reading.</p><p>The obstacle detection task was inspired by the laboratory studies by Fotios and Cheal [<xref rid="pone.0204638.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0204638.ref036" ref-type="bibr">36</xref>]. The participants were placed at a specific viewing point and instructed to focus straight ahead while simultaneously stating the number of obstacles (10 x 10 x 2.5 cm, made of the same carpet as was covering the floor) they could discern on the ground on their right-hand side. The size and height of the obstacles were chosen to try to emulate a raised cobblestone [<xref rid="pone.0204638.ref036" ref-type="bibr">36</xref>]. The obstacle detection area was shielded by the researcher&#x02019;s body while the participants walked towards the viewing point. To help prevent participants from glancing at the obstacles, the researcher maintained eye contact with the participants while giving instructions for the task.</p><p>For each presentation the number of obstacles was chosen randomly and varied between 4 and 7. The position of the obstacles was also chosen randomly from a pool of 15 positions created by using a 25, 35 and 45 degree angle from the viewing direction and at distances representing 2, 4, 6, 8 and 10 paces from the participants (corresponding to 1.2, 2.4, 3.6, 4.8 &#x00026; 6 meters [<xref rid="pone.0204638.ref036" ref-type="bibr">36</xref>], see <xref ref-type="fig" rid="pone.0204638.g006">Fig 6</xref>). The size of the obstacles, given as visual angle subtended at the eye, ranged from 4.77&#x000b0; at 1.2 meters to 0.95&#x000b0; at 6 meters distance.</p><fig id="pone.0204638.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g006</object-id><label>Fig 6</label><caption><title>The 15 possible positions for the obstacles in the obstacle detection task.</title></caption><graphic xlink:href="pone.0204638.g006"/></fig><p>The illumination measurements of the obstacle area (4 x 5 meters, located on the right-hand side of the path, centered on the midpoint of the distance between the lampposts, see <xref ref-type="fig" rid="pone.0204638.g004">Fig 4</xref>.) were conducted as part of the overall illumination measurement, using a 1x1-meter grid. The horizontal illuminance measurements for the obstacle detection area varied between 2 and 33 lx (Lighting application A, E<sub>H</sub>: 3&#x02013;10 lx, <inline-formula id="pone.0204638.e006"><alternatives><graphic xlink:href="pone.0204638.e006.jpg" id="pone.0204638.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 6 lx; Lighting application B, E<sub>H</sub>: 2&#x02013;17 lx, <inline-formula id="pone.0204638.e007"><alternatives><graphic xlink:href="pone.0204638.e007.jpg" id="pone.0204638.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 5 lx; Lighting application C, E<sub>H</sub>: 11&#x02013;33 lx, <inline-formula id="pone.0204638.e008"><alternatives><graphic xlink:href="pone.0204638.e008.jpg" id="pone.0204638.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 17 lx) with the corresponding uniformities U<sub>A</sub>: .59, U<sub>B</sub>: .28 and U<sub>C</sub>: .63. Horizontal illuminance was also measured at the center of each obstacle, with measurements ranging from 2 to 15 lx (Lighting application A, E<sub>H</sub>: 4&#x02013;8 lx, <inline-formula id="pone.0204638.e009"><alternatives><graphic xlink:href="pone.0204638.e009.jpg" id="pone.0204638.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 5 lx; Lighting application B, E<sub>H</sub>: 2&#x02013;15 lx, <inline-formula id="pone.0204638.e010"><alternatives><graphic xlink:href="pone.0204638.e010.jpg" id="pone.0204638.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 6 lx; Lighting application C, E<sub>H</sub>: 10&#x02013;21 lx, <inline-formula id="pone.0204638.e011"><alternatives><graphic xlink:href="pone.0204638.e011.jpg" id="pone.0204638.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 15 lx).</p><p>In the facial expression recognition task, the participants were instructed to walk towards a photograph of a woman&#x000b4;s face (175x200 mm; positioned at a height of 1.65 m; printed on non-glossy paper), placed on the right-hand side of the path 12 meters from the first lamppost (13.5 meters from the starting point). The participants were instructed to stop when they could discern the facial expression of the woman&#x02019;s face. The facial photographs belong to the standardized P. Ekman&#x02019;s Emotions Revealed photo set, and the photos used were fear (p. 165), surprise (p. 166) and anger (p. 127) [<xref rid="pone.0204638.ref053" ref-type="bibr">53</xref>].The distance was measured, and the correctness of the evaluation was assessed on 11 items (Interest, Enjoyment, Surprise, Anger, Disgust, Contempt, Fear, Shyness, Shame, Guilt &#x00026; Hostility) from the Differential Emotions Scale, DES [<xref rid="pone.0204638.ref054" ref-type="bibr">54</xref>], graded using a 5-point Likert scale. Since the presentation order of the lighting applications was counterbalanced, not all participants experienced the same facial expression/lighting application combination. To overcome this difference the mean value for the three DES items that corresponded with the facial expressions of the photographs (fear, surprise and anger) was calculated and subtracted with the overall mean rating for all DES items. This was done for each lighting application, and the resulting variable was then used as a proxy for the ability to identify the relevant facial expressions.</p><p>In the sign reading task, the participants were asked to walk along the path towards a street signpost placed 4 meters to the right of the path and 4.5 meters before the second lamppost (13.6 meters from the starting point), positioned at a height of 2.10 meters. Three versions of the street signpost were used, each showing street names of similar type and equivalent number of syllables (Gullbackegatan, Almedalsgatan &#x00026; Hagalundsgatan; Tratex font; Size: 212). The task of the participants was to stop when the text on the street signpost became legible. The distance was measured and the participants were asked to read the street name aloud in order to validate the legibility.</p></sec><sec id="sec010"><title>2.4.2 Evaluation</title><p>In order to assess the emotional state of the participants when they had just finished walking down the path, the affect grid [<xref rid="pone.0204638.ref055" ref-type="bibr">55</xref>, <xref rid="pone.0204638.ref056" ref-type="bibr">56</xref>], an instrument in which the participants rate their degree of arousal and valence in a two-dimensional grid (consisting of 5 x 5 cells with the labels Active placed above, Passive below, Negative to the left and Positive to the right), was used. Two other scales, asking the participants to rate their valence (1 = sad, depressed, displeased to 5 = glad, happy, pleased) and arousal-level (1 = dull, passive, sleepy to 5 = peppy, active, awake), were administered. The mean score for the arousal measures (&#x003b1;<sub>A</sub> = .874; &#x003b1;<sub>B</sub> = .900; &#x003b1;<sub>C</sub> = .859) and valence measures (&#x003b1;<sub>A</sub> = .904; &#x003b1;<sub>B</sub> = .846; &#x003b1;<sub>C</sub> = .690) was then calculated.</p><p>To capture how the participants experienced the lighting and the lit environment, the POLQ scale [<xref rid="pone.0204638.ref039" ref-type="bibr">39</xref>] was used, which consists of ten items (subdued-brilliant; strong-weak; dark-light; unfocused-focused; clear-drab; hard-soft; warm-cool; natural-unnatural, glaring- shaded; mild-sharp) rated on a 7-point scale, constituting two indices, PSQ and PCQ. The participants were also asked to rate how well they could see under the present lighting application using a 7-point Likert scale (1 = very poorly, to 7 = very well).</p></sec><sec id="sec011"><title>2.4.3 Behavior</title><p>Behavior was, in line with Pedersen and Johansson [<xref rid="pone.0204638.ref048" ref-type="bibr">48</xref>], assessed in terms of walking speed. Before entering the laboratory, the baseline walking speed of the participants was measured on a 33.3-meter distance in a corridor under well-lit conditions (<inline-formula id="pone.0204638.e012"><alternatives><graphic xlink:href="pone.0204638.e012.jpg" id="pone.0204638.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>195</mml:mn><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">lx</mml:mi></mml:math></alternatives></inline-formula>). Then, in the laboratory, at the start and end of the path, motion sensors connected to a stopwatch measured the time it took the participants to walk the path, and walking speed was calculated. The impact of lighting on walking speed was then calculated for each lighting application by subtracting the walking speeds from the laboratory from the baseline walking speed.</p></sec></sec><sec id="sec012"><title>2.5 Design and procedure</title><p>The study employed a mixed design, with a within-subjects repeated measures design for evaluating differences due to the different lighting applications, and a between-groups design for exploring differences due to age. Each participant performed the test procedure three times, once for each lighting application (<xref rid="pone.0204638.t002" ref-type="table">Table 2</xref>). The luminaires were shifted by rotating the top of the lamppost and the presentation order was counterbalanced. The participants arrived in groups of five and, before entering the lab, they gathered in a waiting room, where they were informed about the procedure and asked to fill in questionnaires surveying background data and individual characteristics. In an adjacent room their vision was tested individually and then they walked a 33-meter distance in a corridor where their baseline walking speed was measured.</p><table-wrap id="pone.0204638.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.t002</object-id><label>Table 2</label><caption><title>Flowchart for the test procedure.</title></caption><alternatives><graphic id="pone.0204638.t002g" xlink:href="pone.0204638.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Process</th><th align="left" rowspan="1" colspan="1">Location</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Introductory meeting</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Measurement of baseline walking speed</td><td align="left" rowspan="2" colspan="1">Corridor</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Vision tests</td></tr><tr><td align="left" rowspan="12" colspan="1">Lighting presentation #1</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="6" colspan="1">Laboratory</td></tr><tr><td align="left" rowspan="1" colspan="1">Measurement of walking speed</td></tr><tr><td align="left" rowspan="1" colspan="1">Affect grid</td></tr><tr><td align="left" rowspan="1" colspan="1">Valence &#x00026; arousal scales</td></tr><tr><td align="left" rowspan="1" colspan="1">POLQ</td></tr><tr><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" style="background-color:#D9D9D9" rowspan="1" colspan="1">Waiting / self-report of background data</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="5" colspan="1">Laboratory</td></tr><tr><td align="left" rowspan="1" colspan="1">Obstacle detection</td></tr><tr><td align="left" rowspan="1" colspan="1">Facial recognition</td></tr><tr><td align="left" rowspan="1" colspan="1">Sign reading</td></tr><tr><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/><td align="left" style="background-color:#D9D9D9" rowspan="1" colspan="1">Waiting</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr><tr><td align="left" rowspan="12" colspan="1">Lighting presentation #2</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="6" colspan="1">Laboratory</td></tr><tr><td align="left" rowspan="1" colspan="1">Measurement of walking speed</td></tr><tr><td align="left" rowspan="1" colspan="1">Affect grid</td></tr><tr><td align="left" rowspan="1" colspan="1">Valence &#x00026; arousal scales</td></tr><tr><td align="left" rowspan="1" colspan="1">POLQ</td></tr><tr><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" style="background-color:#D9D9D9" rowspan="1" colspan="1">Waiting</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="5" colspan="1">Laboratory</td></tr><tr><td align="left" rowspan="1" colspan="1">Obstacle detection</td></tr><tr><td align="left" rowspan="1" colspan="1">Facial recognition</td></tr><tr><td align="left" rowspan="1" colspan="1">Sign reading</td></tr><tr><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/><td align="left" style="background-color:#D9D9D9" rowspan="1" colspan="1">Waiting</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr><tr><td align="left" rowspan="12" colspan="1">Lighting presentation #3</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="6" colspan="1">Laboratory</td></tr><tr><td align="left" rowspan="1" colspan="1">Measurement of walking speed</td></tr><tr><td align="left" rowspan="1" colspan="1">Affect grid</td></tr><tr><td align="left" rowspan="1" colspan="1">Valence &#x00026; arousal scales</td></tr><tr><td align="left" rowspan="1" colspan="1">POLQ</td></tr><tr><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" style="background-color:#D9D9D9" rowspan="1" colspan="1">Waiting</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="5" colspan="1">Laboratory</td></tr><tr><td align="left" rowspan="1" colspan="1">Obstacle detection</td></tr><tr><td align="left" rowspan="1" colspan="1">Facial recognition</td></tr><tr><td align="left" rowspan="1" colspan="1">Sign reading</td></tr><tr><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Debriefing and conclusion</td><td align="left" rowspan="1" colspan="1">Waiting room</td></tr></tbody></table></alternatives></table-wrap><p>The participants entered the laboratory one at a time, while the rest of the group remained in the waiting room. The participants were asked to walk down the path past the second lamppost, after which a podium was placed where the participants rated their emotional state. The participants then returned to the first lamppost, looked out over the path and rated the perceived lighting quality [<xref rid="pone.0204638.ref039" ref-type="bibr">39</xref>].</p><p>When all participants had completed the task, they waited in the adjoining waiting room while the visual performance tasks were prepared. Next, the participants individually entered the laboratory a second time and walked to the obstacle detection point 5.5 meters down the path from the first lamppost and performed the obstacle detection task. The participants returned to the starting point before starting the facial expression recognition task, after which they returned to the starting point. Lastly, the participants performed the sign reading task. Data collection took approximately two hours for each group of participants.</p><p>The data was analyzed with IBM SPSS 22, using Repeated Measures ANOVA with age as between-subjects factor. To avoid potential problems with violations of assumptions underlying the use of ANOVA, parallel analyses were conducted using a two-way between-within subjects ANOVA on the trimmed means (20%) using the bwtrim function of R. The results from the trimmed means ANOVA corroborated the findings from the Repeated Measures ANOVA.</p></sec></sec><sec id="sec013"><title>3. Results</title><sec id="sec014"><title>3.1 Perception</title><p>There were statistically significant differences between the different lighting applications for obstacle detection, facial expression distance, and sign reading distance (<xref rid="pone.0204638.t003" ref-type="table">Table 3</xref>, <xref ref-type="fig" rid="pone.0204638.g007">Fig 7</xref>). For obstacle detection, participants were most successful in detecting obstacles under lighting application C, on average detecting 42% of the obstacles, followed by lighting application A (26%), and then lighting application B (19%) (F (2, 174) = 38.021, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .304). Similar results were found for the distances needed to recognize a facial expression (F (2, 174) = 8.115, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .085) and the distance to read the street signpost (F (2, 174) = 31.906, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .268). Lighting application C enabled the participants to identify facial expressions and read a street signpost at significantly greater distances (mean distance, <inline-formula id="pone.0204638.e013"><alternatives><graphic xlink:href="pone.0204638.e013.jpg" id="pone.0204638.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula>: 4.6 &#x00026; 11.3 meters) than lighting applications A (<inline-formula id="pone.0204638.e014"><alternatives><graphic xlink:href="pone.0204638.e014.jpg" id="pone.0204638.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>:</mml:mo></mml:math></alternatives></inline-formula> 4.0 &#x00026; 10.1 meters) and B (<inline-formula id="pone.0204638.e015"><alternatives><graphic xlink:href="pone.0204638.e015.jpg" id="pone.0204638.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>:</mml:mo></mml:math></alternatives></inline-formula> 4.2 &#x00026; 9.9 meters) (<xref rid="pone.0204638.t004" ref-type="table">Table 4</xref>).</p><fig id="pone.0204638.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.g007</object-id><label>Fig 7</label><caption><title>Overview of significant results in <xref rid="pone.0204638.t003" ref-type="table">Table 3</xref> (each measure rescaled by division by its greatest value).</title></caption><graphic xlink:href="pone.0204638.g007"/></fig><table-wrap id="pone.0204638.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.t003</object-id><label>Table 3</label><caption><title>Results from the repeated measures ANOVA.</title></caption><alternatives><graphic id="pone.0204638.t003g" xlink:href="pone.0204638.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Response</th><th align="left" rowspan="1" colspan="1">Within subject</th><th align="left" rowspan="1" colspan="1">Between groups</th><th align="left" rowspan="1" colspan="1">Interaction</th><th align="left" rowspan="1" colspan="1">Post hoc<xref ref-type="table-fn" rid="t003fn001">*</xref></th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"><underline>Perception</underline></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Obstacle detection</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 38.021,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 10.234,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 1.164,</td><td align="left" rowspan="2" colspan="1">C &#x0003e; A &#x0003e; B</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .304</td><td align="left" rowspan="1" colspan="1"><bold>p = .002</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .105</td><td align="left" rowspan="1" colspan="1">p = .315</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Facial expression distance</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 8.115,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 7.291,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 3.257,</td><td align="left" rowspan="2" colspan="1">C &#x0003e; A &#x00026; B</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .085</td><td align="left" rowspan="1" colspan="1"><bold>p = .008</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .077</td><td align="left" rowspan="1" colspan="1"><bold>p = .041</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .036</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Facial expression recognition</italic></td><td align="left" rowspan="1" colspan="1">F(2,174) = .791,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 7.437,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = .788,</td><td align="left" rowspan="2" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">p = .455</td><td align="left" rowspan="1" colspan="1"><bold>p = .008</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .079</td><td align="left" rowspan="1" colspan="1">p = .456</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Sign reading distance</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 31.906,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 74.308,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 1.292,</td><td align="left" rowspan="2" colspan="1">C &#x0003e; A &#x00026; B</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .268</td><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .461</td><td align="left" rowspan="1" colspan="1">p = .277</td></tr><tr><td align="left" rowspan="1" colspan="1"><underline>Evaluation</underline></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Arousal</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 10.084,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 12.793,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 2.437,</td><td align="left" rowspan="2" colspan="1">C &#x0003e; A</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .104</td><td align="left" rowspan="1" colspan="1"><bold>p = .001</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .128</td><td align="left" rowspan="1" colspan="1">p = .090</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Valence</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 1.903,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 27.695,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = .927,</td><td align="left" rowspan="2" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">p = .152</td><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .241</td><td align="left" rowspan="1" colspan="1">p = .398</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>PSQ</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 44.148,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = .089,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 4.827,</td><td align="left" rowspan="2" colspan="1">C &#x0003e; A &#x00026; B</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .337</td><td align="left" rowspan="1" colspan="1">p = .766</td><td align="left" rowspan="1" colspan="1"><bold>p = .009</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .053</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>PCQ</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 33.617,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 20.693,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 4.074,</td><td align="left" rowspan="2" colspan="1">A &#x00026; B &#x0003e; C</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .279</td><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .192</td><td align="left" rowspan="1" colspan="1"><bold>p = .019</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .045</td></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Seeing condition</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 12.892,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = .330,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = 1.656,</td><td align="left" rowspan="2" colspan="1">C &#x0003e; A &#x00026; B</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>p = .000</bold>, &#x003b7;<sub>p</sub><sup>2</sup> = .129</td><td align="left" rowspan="1" colspan="1">p = .567</td><td align="left" rowspan="1" colspan="1">p = .194</td></tr><tr><td align="left" rowspan="1" colspan="1"><underline>Behavior</underline></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="2" colspan="1"><italic>Walking speed difference</italic></td><td align="left" rowspan="1" colspan="1">F(2, 174) = 1.427,</td><td align="left" rowspan="1" colspan="1">F(1, 87) = 1.606,</td><td align="left" rowspan="1" colspan="1">F(2, 174) = .131,</td><td align="left" rowspan="2" colspan="1">&#x02014;<break/></td></tr><tr><td align="left" rowspan="1" colspan="1">p = .243</td><td align="left" rowspan="1" colspan="1">p = .208</td><td align="left" rowspan="1" colspan="1">p = .877</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t003fn001"><p>* Bonferroni, p &#x0003c; .017</p></fn></table-wrap-foot></table-wrap><table-wrap id="pone.0204638.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0204638.t004</object-id><label>Table 4</label><caption><title>Mean scores and standard deviations for all measures, by lighting application and age group.</title></caption><alternatives><graphic id="pone.0204638.t004g" xlink:href="pone.0204638.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" colspan="3" rowspan="1"/><th align="center" colspan="4" rowspan="1">Mean (SD)</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="center" colspan="2" rowspan="1"><underline>Lighting A</underline></th><th align="center" colspan="2" rowspan="1"><underline>Lighting B</underline></th><th align="center" colspan="2" rowspan="1"><underline>Lighting C</underline></th></tr><tr><th align="left" rowspan="1" colspan="1">Response</th><th align="left" rowspan="1" colspan="1">Young</th><th align="left" rowspan="1" colspan="1">Old</th><th align="left" rowspan="1" colspan="1">Young</th><th align="left" rowspan="1" colspan="1">Old</th><th align="left" rowspan="1" colspan="1">Young</th><th align="left" rowspan="1" colspan="1">Old</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"><underline>Perception</underline></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Obstacle detection</italic></td><td align="left" rowspan="1" colspan="1">.33 (.23)</td><td align="left" rowspan="1" colspan="1">.20 (.23)</td><td align="left" rowspan="1" colspan="1">.24 (.22)</td><td align="left" rowspan="1" colspan="1">.16 (.21)</td><td align="left" rowspan="1" colspan="1">.51 (.21)</td><td align="left" rowspan="1" colspan="1">.35 (.30)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Facial expression distance</italic></td><td align="left" rowspan="1" colspan="1">4.28 (1.40)</td><td align="left" rowspan="1" colspan="1">3.73 (2.10)</td><td align="left" rowspan="1" colspan="1">4.81 (1.75)</td><td align="left" rowspan="1" colspan="1">3.53 (2.04)</td><td align="left" rowspan="1" colspan="1">5.08 (1.72)</td><td align="left" rowspan="1" colspan="1">4.07 (1.87)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Facial expression recognition</italic></td><td align="left" rowspan="1" colspan="1">.37 (.50)</td><td align="left" rowspan="1" colspan="1">.06 (.48)</td><td align="left" rowspan="1" colspan="1">.38 (.62)</td><td align="left" rowspan="1" colspan="1">.20 (.33)</td><td align="left" rowspan="1" colspan="1">.33 (.63)</td><td align="left" rowspan="1" colspan="1">.16 (.39)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Sign reading distance</italic></td><td align="left" rowspan="1" colspan="1">11.83 (1.90)</td><td align="left" rowspan="1" colspan="1">8.47 (2.24)</td><td align="left" rowspan="1" colspan="1">11.66 (1.91)</td><td align="left" rowspan="1" colspan="1">8.29 (2.16)</td><td align="left" rowspan="1" colspan="1">12.80 (1.40)</td><td align="left" rowspan="1" colspan="1">9.08 (2.37)</td></tr><tr><td align="left" rowspan="1" colspan="1"><underline>Evaluation</underline></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Arousal</italic></td><td align="left" rowspan="1" colspan="1">3.40 (.94)</td><td align="left" rowspan="1" colspan="1">4.08 (.91)</td><td align="left" rowspan="1" colspan="1">3.73 (.92)</td><td align="left" rowspan="1" colspan="1">4.16 (.72)</td><td align="left" rowspan="1" colspan="1">4.05 (.73)</td><td align="left" rowspan="1" colspan="1">4.31 (.57)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Valence</italic></td><td align="left" rowspan="1" colspan="1">3.52 (.84)</td><td align="left" rowspan="1" colspan="1">4.10 (.83)</td><td align="left" rowspan="1" colspan="1">3.70 (.98)</td><td align="left" rowspan="1" colspan="1">4.28 (.59)</td><td align="left" rowspan="1" colspan="1">3.53 (.80)</td><td align="left" rowspan="1" colspan="1">4.34 (.59)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>PSQ</italic></td><td align="left" rowspan="1" colspan="1">4.28 (1.08)</td><td align="left" rowspan="1" colspan="1">4.50 (1.27)</td><td align="left" rowspan="1" colspan="1">4.66 (1.01)</td><td align="left" rowspan="1" colspan="1">4.80 (1.19)</td><td align="left" rowspan="1" colspan="1">5.86 (.70)</td><td align="left" rowspan="1" colspan="1">5.33 (.97)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>PCQ</italic></td><td align="left" rowspan="1" colspan="1">4.09 (1.25)</td><td align="left" rowspan="1" colspan="1">4.77 (1.09)</td><td align="left" rowspan="1" colspan="1">4.13 (1.35)</td><td align="left" rowspan="1" colspan="1">4.37 (1.25)</td><td align="left" rowspan="1" colspan="1">2.55 (.81)</td><td align="left" rowspan="1" colspan="1">3.75 (1.23)</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Seeing condition</italic></td><td align="left" rowspan="1" colspan="1">5.07 (1.44)</td><td align="left" rowspan="1" colspan="1">5.51 (1.38)</td><td align="left" rowspan="1" colspan="1">5.58 (1.12)</td><td align="left" rowspan="1" colspan="1">5.46 (1.24)</td><td align="left" rowspan="1" colspan="1">6.10 (.95)</td><td align="left" rowspan="1" colspan="1">6.09 (1.09)</td></tr><tr><td align="left" rowspan="1" colspan="1"><underline>Behavior</underline></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Walking speed difference</italic></td><td align="left" rowspan="1" colspan="1">.17 (.13)</td><td align="left" rowspan="1" colspan="1">.20 (.15)</td><td align="left" rowspan="1" colspan="1">.15 (12)</td><td align="left" rowspan="1" colspan="1">.19 (.15)</td><td align="left" rowspan="1" colspan="1">.15 (.12)</td><td align="left" rowspan="1" colspan="1">.19 (.18)</td></tr></tbody></table></alternatives></table-wrap><p>For obstacle detection, there were significant differences between the two age groups, where the younger group performed significantly better on average (36%) than the older group (24%) (F (1, 87) = 10.234, p = .002, &#x003b7;<sub>p</sub><sup>2</sup> = .105). Also, the younger group could both identify facial expressions (<inline-formula id="pone.0204638.e016"><alternatives><graphic xlink:href="pone.0204638.e016.jpg" id="pone.0204638.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>:</mml:mo></mml:math></alternatives></inline-formula> 4.7 meters) and read the street signpost (<inline-formula id="pone.0204638.e017"><alternatives><graphic xlink:href="pone.0204638.e017.jpg" id="pone.0204638.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>:</mml:mo></mml:math></alternatives></inline-formula> 12.1 meters) at greater distances than the older group (<xref rid="pone.0204638.t003" ref-type="table">Table 3</xref>). For facial expression distance there was also a weak interaction effect, indicating that the older group needed to be closest for lighting application B (F(2, 174) = 3.257, p = .041, &#x003b7;<sub>p</sub><sup>2</sup> = .036), whereas the younger group had their shortest distance for lighting application A. The younger participants were also more accurate in their assessment of facial expressions (F (1, 87) = 7.437, p = .008, &#x003b7;<sub>p</sub><sup>2</sup> = .079).</p></sec><sec id="sec015"><title>3.2 Evaluation</title><p>Instruments used for evaluating the lighting and the lit environment also showed significant differences. For the perceived lighting quality indices, PSQ and PCQ, there were significant differences between the three lighting applications (PSQ, F (2, 174) = 44.148, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .337; PCQ, F (2, 174) = 33.617, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .279). Lighting application C was rated highest on perceived strength quality (mean PSQ: 5.6) and lowest on perceived comfort quality (mean PCQ: 3.2). For the emotional state measurement, the different lighting applications differed on level of arousal (F (2, 174) = 10.084, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .104), where lighting application C was experienced as more arousing than lighting application A, but not significantly different from B. When it came to judging how well they could see under the different lighting applications, lighting application C was deemed to give significantly better seeing conditions (F(2, 174) = 12.892, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .129) (<xref rid="pone.0204638.t003" ref-type="table">Table 3</xref>).</p><p>There were significant differences between the age groups for PCQ (F (1, 87) = 20.693, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .192), where the younger group had lower average ratings (3.59 compared to 4.9), but not for PSQ F (1, 87) = .089, p = .766). However, for both indices there were interaction effects (PSQ, F (2, 174) = 4.827, p = .009, &#x003b7;<sub>p</sub><sup>2</sup> = .053; PCQ, F (2, 174) = 4.074, p = .019, &#x003b7;<sub>p</sub><sup>2</sup> = .045). For PSQ, lighting application C was rated higher by the younger group. For PCQ, lighting application B got higher ratings in the younger group, but still less than for the older group. Differences between the age groups were also found for emotional state. The older group was both more aroused (F (1, 87) = 12.793, p = .001, &#x003b7;<sub>p</sub><sup>2</sup> = .128) and felt more positive (F (1, 87) = 27.695, p = .000, &#x003b7;<sub>p</sub><sup>2</sup> = .241) than the younger group.</p></sec><sec id="sec016"><title>3.3 Behavior</title><p>Relative walking speed did not differ between the three lighting applications (F (2, 174) = 1.427, p = .243), and there were no significant differences between the different age groups (F (1, 87) = 1.606, p = .208).</p></sec></sec><sec id="sec017"><title>4. Discussion</title><p>This study employed a set of methods for evaluating the human response to outdoor lighting and assessing differences between three lighting applications selected by the city of Malm&#x000f6;. The main finding is that the set of methods could differentiate between the three lighting applications with regard to the human response and thereby provide a complementary perspective to photometric properties. Most methods capturing perception and evaluation differentiated between lighting application C and the other two lighting applications (obstacle detection, facial recognition distance, sign reading distance, arousal, PSQ, PCQ and seeing condition). However, the behavior measure, walking speed, failed to capture any differences between the lighting applications.</p><p>The lighting applications differed in terms of the underlying technology and consequently also on several photometric measures, including light distribution, illuminance levels, SPD, CCT, CRI, and glare.</p><p>There were also differences in power and luminous efficacy between the lighting applications. Lighting application C used more than 2.5 times the power than lighting application B, which used the least power (P<sub>A</sub>:68 W P<sub>B</sub>: 36 W; P<sub>C</sub>:93 W). Lighting application B was also most efficient, having a luminous efficacy well above the other lighting applications (A: 50 lm/W; B: 94 lm/W; C: 64 lm/W).</p><p>This complexity regarding photometry, energy efficiency and, as result, the cost of operating the lighting applications is characteristic for the situation municipalities will face in any given procurement process. In addition to energy efficiency and road lighting standards, it is relevant to consider the human response in the choice of outdoor lighting applications for pedestrians. The methods used in this study could be applied during such a process as a way to differentiate between different lighting applications based on the human response to the lighting.</p><p>Most differences concerned lighting application C in relation to lighting applications A and B. The question remains why the measures did not substantially differentiate between lighting application A and B, except for on the obstacle detection task. A few parameters clearly set lighting application C apart: Spectral Power Distribution (S/P) (A: 1.25; B: 1.22; C: 1.48), CCT (A: 2890; B: 2912; C: 3810) and the light distribution. These parameters will be discussed in relation to the results.</p><p>For the perceptual tasks there were significant differences between the lighting applications in the distance needed to identify a facial expression, for reading a street signpost, and the probability of detecting obstacles on the ground. Lighting application C created superior conditions for the perceptual tasks. For obstacle detection, participants were most successful in detecting obstacles under lighting application C, followed by lighting application A, and then lighting application B. For lighting application C, this was in line with both the higher horizontal illuminance level of the targets (E<sub>H A</sub>: 4&#x02013;8 lx; E<sub>H B</sub>: 2&#x02013;15; E<sub>H C</sub>: 10&#x02013;21) stemming from lighting application C&#x02019;s wider light distribution and with its greater S/P ratio (S/P<sub>A</sub>: 1.25; S/P<sub>B</sub>: 1.22; S/P<sub>C</sub>: 1.48). The peripheral vision used for the task is dependent on the rods, which are most sensitive to light with shorter wavelengths. A light source with a higher S/P-ratio emits a relatively greater amount of light with short wavelengths compared to light sources with a lower S/P-ratio. Light sources with greater S/P-ratio would thus be expected to produce better performance in peripheral tasks, particularly at lower light levels. However, even though lighting applications A and B had similar S/P ratios and similar average horizontal illuminance levels in the obstacle detection area (<inline-formula id="pone.0204638.e018"><alternatives><graphic xlink:href="pone.0204638.e018.jpg" id="pone.0204638.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 6 lx;, <inline-formula id="pone.0204638.e019"><alternatives><graphic xlink:href="pone.0204638.e019.jpg" id="pone.0204638.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: 5 lx), the participants were more successful in detecting obstacles under lighting application A. A possible reason for this is the markedly lower illumination values at some of the obstacles for lighting application B, as reflected in uniformity in the obstacle area (U<sub>A</sub>: .59; U<sub>B</sub>: .28). There was also a difference between the two age groups, where the younger group performed significantly better. This is to be expected due to the decline in night vision associated with increased age [<xref rid="pone.0204638.ref057" ref-type="bibr">57</xref>].</p><p>Lighting application C enabled the participants to perform their tasks at a significantly greater distance than lighting applications A and B. This could be an effect of the unequal light distribution, which resulted in differences in how much light was reflected off the photograph/sign respectively, with higher luminance levels for lighting application C (Face luminance: <italic>L</italic><sub><italic>V A</italic></sub>: .28; <italic>L</italic><sub><italic>V B</italic></sub>: .36; <italic>L</italic><sub><italic>V C</italic></sub>: .55; Sign luminance: <italic>L</italic><sub><italic>V A</italic></sub>: .13; <italic>L</italic><sub><italic>V B</italic></sub>: .13; <italic>L</italic><sub><italic>V C</italic></sub>: .35). A limitation to this study is that the facial recognition task and sign reading task were conducted in only one location for each task. Had several locations been used, the impact of the different light distributions on the luminance levels of the photographs and the signs would most likely have been clearer.</p><p>Once again, the younger group had better results than the older group. For facial expression distance, however, there was also a weak interaction effect indicating that the older group needed to be closest for lighting application B, whereas the shortest distance for the younger group was for lighting application A. This might be due to the group of elderly being more sensitive to glare [<xref rid="pone.0204638.ref058" ref-type="bibr">58</xref>], since lighting application B was more glaring than A, for the distances where most participants recognized facial expressions (approximately 3&#x02013;10 meters, see <xref ref-type="fig" rid="pone.0204638.g005">Fig 5</xref>). For the sign reading test this was not the case. Possibly the location of the task made the participants direct their gaze away from the path, thereby reducing the effect of glare. It is noteworthy, that even though the horizontal illuminance levels for all three lighting applications were well above the recommended levels for pedestrian paths according to the national standards [<xref rid="pone.0204638.ref019" ref-type="bibr">19</xref>], the distances for recognizing facial expressions were much shorter than what is suggested to be the desirable fixation distance (15 meters) [<xref rid="pone.0204638.ref059" ref-type="bibr">59</xref>]. This may be due to the difficulty associated with identifying the facial expressions of black and white photographs, in comparison to real faces.</p><p>Lighting application C was rated highest on PSQ and lowest on PCQ for the evaluation aspect. The results on PSQ are not surprising, since lighting application C had the highest mean illuminance (taking the entire laboratory into account) and the greatest S/P ratio. The results for PCQ might stem from the differences in correlated color temperature, since two of the five items constituting the index relate to color temperature (hard-soft and warm-cool). There were also significant differences between the age groups for PCQ, but not for PSQ. The lower ratings of the younger group may represent a greater sensitivity, or stricter standards in the assessment of outdoor lighting. The older group may be more indulgent towards the lighting applications due to their experience of older technologies, such as low-pressure sodium lighting.</p><p>The emotional state differed between the different lighting applications on level of arousal, with lighting application C rated highest. This is in line with results from previous research suggesting that lighting applications with a CCT around 5000 K will bring about a greater level of arousal than those with a CCT of around 3000 K, while also being perceived as less pleasurable [<xref rid="pone.0204638.ref047" ref-type="bibr">47</xref>]. In this study, however, there were no significant differences on the valence ratings.</p><p>The participants appeared to be able to judge how well they could see under the different lighting applications. Both age groups agreed on lighting application C as the one providing the best seeing conditions. This was in line with their results on the visual tasks, which they performed at a later stage of the experiment. However, the superior seeing conditions did not result in a greater walking speed. It is plausible that the illuminance levels were sufficient for all lighting applications to deliver the visual cues needed for walking at normal pace. The walking speed may also have been influenced by the study being conducted in a laboratory, where participants walked towards the opposite wall and did not have to scan the ground for obstacles. However, another study using a similar setting but with only one lighting application at different levels of dimming, did find significant differences, with walking speed declining with lower illuminances [<xref rid="pone.0204638.ref048" ref-type="bibr">48</xref>].</p><p>In this study, lighting application C created superior conditions for the perceptual tasks and was perceived to give the best seeing conditions, indicating that it would be a good choice for pedestrian paths in Malm&#x000f6;. However, at the same time, it was rated as less pleasant on the PCQ subscale of POLQ, and its luminous efficacy is about 68 percent of that of lighting application B. This highlights the fact that there may be no lighting application that is preferable in every given situation. Decision makers therefore need to take the context, as well as the purpose of the lighting, into account and weigh the advantages and disadvantages of lighting applications in order to find optimal solutions.</p><p>The study was designed with the intent to assess the human response to outdoor lighting for actual lighting applications used by local municipalities. The aspiration to maximize ecological validity made it difficult to systematically control variables related to the lighting, such as CCT and illuminance level. Naturally, not keeping any variables equal makes it difficult to discriminate which factors were most significant for the difference in perception and evaluation for lighting application C compared to A and B. However, this was never the intention of the study, as the focus was on evaluating three luminaires considered for future use and to collect a set of methods with practical relevance for municipalities in their evaluation of outdoor lighting solutions.</p><p>In order to address the three overarching themes of human perception, evaluation and behavior in response to artificial outdoor lighting, this study adopted a broad approach. The methods used were inspired by methods used in previous research and, in some cases, the original method was utilized (POLQ and emotional state). The methods were chosen on the basis of appearing to have high face validity and of being relevant for practice. Both the POLQ scale and the emotional state measure have been shown to be important for the perception of safety, which is a factor that has been frequently discussed in relation to artificial lighting in urban environments.</p><p>That outdoor lighting is an important factor, contributing to the walkability of neighborhoods [<xref rid="pone.0204638.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0204638.ref004" ref-type="bibr">4</xref>] and supporting walking for young, adults and elderly [<xref rid="pone.0204638.ref005" ref-type="bibr">5</xref>&#x02013;<xref rid="pone.0204638.ref011" ref-type="bibr">11</xref>], is well supported. However, pedestrian behavior in the lit environment of real-world settings is less researched. Existing studies have mainly focused on pedestrian flow and walking speed. Walking speed has previously been shown to vary due to different levels of illuminance [<xref rid="pone.0204638.ref048" ref-type="bibr">48</xref>], but in this study the three lighting conditions did not affect walking speed.</p><p>However, the use of pedestrian flow and walking speed in real-world settings might be questioned, since the significance of, for example, an increase in walking speed is ambiguous. Do people walk faster due to better seeing conditions or because that the environment is less inviting and perceived as unsafe? Also, pedestrian flow, i.e. the number of pedestrians during a set amount of time, is a crude measure that does not capture how pedestrians move in the lit environment. A supplementary approach could be to simulate pedestrian dynamics based on trajectory data collected from video recordings. This has, for example, been done with regard to pedestrian crossings at signalized intersections [<xref rid="pone.0204638.ref060" ref-type="bibr">60</xref>, <xref rid="pone.0204638.ref061" ref-type="bibr">61</xref>], but studies modelling the impact of lighting applications on pedestrian behavior are still lacking.</p><p>Another approach taken in the study of behavior in the lit environment is to use eye-tracking equipment to capture where pedestrians fix their gaze while walking [<xref rid="pone.0204638.ref024" ref-type="bibr">24</xref>, <xref rid="pone.0204638.ref062" ref-type="bibr">62</xref>&#x02013;<xref rid="pone.0204638.ref066" ref-type="bibr">66</xref>]. This might give important insights regarding the relative importance of different visual tasks, as well as indications on how to design light distributions in a better way, but would still need further development to be feasible for large-scale use by municipalities.</p><p>There is great potential to cut lighting energy use by upgrading present outdoor lighting installations to more energy-efficient luminaires. Pedestrian experience can be improved and mobility facilitated by upgrading to technologies more adapted to user needs. This study employed methods that differentiated between lighting applications and that could be used to guide the decisions of municipalities before undertaking major upgrades of the outdoor lighting on urban pedestrian paths. We suggest that pilot studies assessing how pedestrians experience the lighting in a real setting can help avoid installing lighting applications with unwanted characteristics, thereby finding a better solution early in the procurement process. We propose that facial expression recognition distance, sign reading distance, the POLQ scale and emotional state measure have potential for use by municipalities in a real-world context. However, validation of these tests and identification of minimum performance thresholds would be needed before considering them to be used as standardized tests.</p><p>The obstacle detection task and the motion sensors might be difficult to use outdoors and would need further development, and other methods would be needed to assess pedestrian behavior. One option might be video technology currently used for surveillance purposes. The use of video technology, possibly in combination with eye-tracking, could cast light on where pedestrians direct their gaze and which strategies are employed with regard to glare. Further research is needed to evaluate the methods in the field and to discern the implications of perception, evaluation and behavior for the perceived accessibility and safety, as well as for the choice of walking to different destinations along urban pedestrian paths.</p></sec></body><back><ack><p>We wish to express our gratitude to the City of Malm&#x000f6; for providing and installing lighting applications in the full-scale laboratory.</p></ack><ref-list><title>References</title><ref id="pone.0204638.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Ewing</surname><given-names>R</given-names></name>, <name><surname>Cervero</surname><given-names>R</given-names></name>. <article-title>Travel and the built environment</article-title>. <source>J Am Plann Assoc</source>. <year>2010</year>;<volume>76</volume>(<issue>3</issue>):<fpage>265</fpage>&#x02013;<lpage>94</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>S</given-names></name>, <name><surname>Choi</surname><given-names>K</given-names></name>, <name><surname>Lee</surname><given-names>JS</given-names></name>. <article-title>To walk or not to walk: Testing the effect of path walkability on transit users' access mode choices to the station</article-title>. <source>Int J Sustain Transp</source>. <year>2015</year>;<volume>9</volume>(<issue>8</issue>):<fpage>529</fpage>&#x02013;<lpage>41</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Park</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>JS</given-names></name>. <article-title>Meso- or micro-scale? Environmental factors influencing pedestrian satisfaction</article-title>. <source>Transport Res D-Tr E</source>. <year>2014</year>;<volume>30</volume>:<fpage>10</fpage>&#x02013;<lpage>20</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>S</given-names></name>, <name><surname>Deakin</surname><given-names>E</given-names></name>, <name><surname>Lee</surname><given-names>SL</given-names></name>. <article-title>Perception-based walkability index to test impact of microlevel walkability on sustainable mode choice decisions</article-title>. <source>Trans Res Record: J Trans Res Board</source>. <year>2014</year>(<issue>2464</issue>):<fpage>126</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Jago</surname><given-names>R</given-names></name>, <name><surname>Baranowski</surname><given-names>T</given-names></name>, <name><surname>Zakeri</surname><given-names>I</given-names></name>, <name><surname>Harris</surname><given-names>M</given-names></name>. <article-title>Observed environmental features and the physical activity of adolescent males</article-title>. <source>Am J Prev Med</source>. <year>2005</year>;<volume>29</volume>(<issue>2</issue>):<fpage>98</fpage>&#x02013;<lpage>104</lpage>. <pub-id pub-id-type="doi">10.1016/j.amepre.2005.04.002</pub-id>
<pub-id pub-id-type="pmid">16005805</pub-id></mixed-citation></ref><ref id="pone.0204638.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>C</given-names></name>, <name><surname>Moudon</surname><given-names>AV</given-names></name>. <article-title>Neighbourhood design and physical activity</article-title>. <source>Build Res Inf</source>. <year>2008</year>;<volume>36</volume>(<issue>5</issue>):<fpage>395</fpage>&#x02013;<lpage>411</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Eyler</surname><given-names>AA</given-names></name>, <name><surname>Matson-Koffman</surname><given-names>D</given-names></name>, <name><surname>Vest</surname><given-names>JR</given-names></name>, <name><surname>Evenson</surname><given-names>KR</given-names></name>, <name><surname>Sanderson</surname><given-names>B</given-names></name>, <name><surname>Thompson</surname><given-names>JL</given-names></name>, <etal>et al</etal>
<article-title>Environmental, policy, and cultural factors related to physical activity in a diverse sample of women: The women's cardiovascular health network project&#x02014;Introduction and methodology</article-title>. <source>Women Health</source>. <year>2002</year>;<volume>36</volume>(<issue>2</issue>):<fpage>1</fpage>&#x02013;<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1300/J013v36n02_01</pub-id>
<pub-id pub-id-type="pmid">12487137</pub-id></mixed-citation></ref><ref id="pone.0204638.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Addy</surname><given-names>CL</given-names></name>, <name><surname>Wilson</surname><given-names>DK</given-names></name>, <name><surname>Kirtland</surname><given-names>KA</given-names></name>, <name><surname>Ainsworth</surname><given-names>BE</given-names></name>, <name><surname>Sharpe</surname><given-names>P</given-names></name>, <name><surname>Kimsey</surname><given-names>D</given-names></name>. <article-title>Associations of perceived social and physical environmental supports with physical activity and walking behavior</article-title>. <source>Am J Public Health</source>. <year>2004</year>;<volume>94</volume>(<issue>3</issue>):<fpage>440</fpage>&#x02013;<lpage>3</lpage>. <pub-id pub-id-type="pmid">14998810</pub-id></mixed-citation></ref><ref id="pone.0204638.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Corseuil</surname><given-names>MW</given-names></name>, <name><surname>Schneider</surname><given-names>IJC</given-names></name>, <name><surname>Silva</surname><given-names>DAS</given-names></name>, <name><surname>Costa</surname><given-names>FF</given-names></name>, <name><surname>Silva</surname><given-names>KS</given-names></name>, <name><surname>Borges</surname><given-names>LJ</given-names></name>, <etal>et al</etal>
<article-title>Perception of environmental obstacles to commuting physical activity in Brazilian elderly</article-title>. <source>Prev Med</source>. <year>2011</year>;<volume>53</volume>(<issue>4&#x02013;5</issue>):<fpage>289</fpage>&#x02013;<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1016/j.ypmed.2011.07.016</pub-id>
<pub-id pub-id-type="pmid">21820007</pub-id></mixed-citation></ref><ref id="pone.0204638.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Corseuil</surname><given-names>MW</given-names></name>, <name><surname>Hallal</surname><given-names>PC</given-names></name>, <name><surname>Corseuil</surname><given-names>HX</given-names></name>, <name><surname>Schneider</surname><given-names>IJC</given-names></name>, <name><surname>D'Orsi</surname><given-names>E</given-names></name>. <article-title>Safety from crime and physical activity among older adults: A population-based study in Brazil</article-title>. <source>J Env Public Health</source>. <year>2012</year>;<volume>2012</volume>(Article ID 641010).</mixed-citation></ref><ref id="pone.0204638.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Rosenberg</surname><given-names>DE</given-names></name>, <name><surname>Huang</surname><given-names>DL</given-names></name>, <name><surname>Simonovich</surname><given-names>SD</given-names></name>, <name><surname>Belza</surname><given-names>B</given-names></name>. <article-title>Outdoor built environment barriers and facilitators to activity among midlife and older adults with mobility disabilities</article-title>. <source>Gerontologist</source>. <year>2013</year>;<volume>53</volume>(<issue>2</issue>):<fpage>268</fpage>&#x02013;<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1093/geront/gns119</pub-id>
<pub-id pub-id-type="pmid">23010096</pub-id></mixed-citation></ref><ref id="pone.0204638.ref012"><label>12</label><mixed-citation publication-type="other">International Energy Agency. Light's labour's lost: Policies for energy-efficient lighting. International Energy Agency; 2006.</mixed-citation></ref><ref id="pone.0204638.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Boyce</surname><given-names>PR</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Richards</surname><given-names>M</given-names></name>. <article-title>Road lighting and energy saving</article-title>. <source>Lighting Res Technol</source>. <year>2009</year>;<volume>41</volume>(<issue>3</issue>):<fpage>245</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Kuhn</surname><given-names>L</given-names></name>, <name><surname>Johansson</surname><given-names>M</given-names></name>, <name><surname>Laike</surname><given-names>T</given-names></name>, <name><surname>Gov&#x000e9;n</surname><given-names>T</given-names></name>. <article-title>Residents&#x02019; perceptions following retrofitting of residential area outdoor lighting with LEDs</article-title>. <source>Lighting Res Technol</source>. <year>2013</year>;<volume>45</volume>(<issue>5</issue>):<fpage>568</fpage>&#x02013;<lpage>84</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Johansson</surname><given-names>M</given-names></name>, <name><surname>Ros&#x000e9;n</surname><given-names>M</given-names></name>, <name><surname>K&#x000fc;ller</surname><given-names>R</given-names></name>. <article-title>Individual factors influencing the assessment of the outdoor lighting of an urban footpath</article-title>. <source>Lighting Res Technol</source>. <year>2011</year>;<volume>43</volume>(<issue>1</issue>):<fpage>31</fpage>&#x02013;<lpage>43</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref016"><label>16</label><mixed-citation publication-type="book"><name><surname>J&#x000e4;gerbrand</surname><given-names>A</given-names></name>, <name><surname>Robertson</surname><given-names>K</given-names></name>, <name><surname>Andersson</surname><given-names>HB</given-names></name>, <name><surname>Folkeson</surname><given-names>L</given-names></name>. <chapter-title>Planning and decision-making for more energy efficient road and street lighting in Swedish municipalities</chapter-title>
<publisher-loc>Link&#x000f6;ping, Sweden</publisher-loc>: <publisher-name>The Swedish National Road and Transport Research Institute (VTI)</publisher-name>; <year>2013</year>
<source>Report No.</source>: <fpage>786</fpage>.</mixed-citation></ref><ref id="pone.0204638.ref017"><label>17</label><mixed-citation publication-type="other">CIE. Lighting of roads for motor and pedestrian traffic. Commission Internationale de l&#x02019;Eclairage; 2010. Report No.: 115:2010.</mixed-citation></ref><ref id="pone.0204638.ref018"><label>18</label><mixed-citation publication-type="book"><collab>BSI</collab>. <chapter-title>Code of practice for the design of road lighting</chapter-title>
<source>Lighting of roads and public amenity areas</source>. <publisher-name>British Standards Institution</publisher-name>; <year>2013</year>. Report No.: BS 5489&#x02013;1:2013.</mixed-citation></ref><ref id="pone.0204638.ref019"><label>19</label><mixed-citation publication-type="book"><collab>Swedish Transport Administration</collab>. <source>Krav f&#x000f6;r v&#x000e4;gars och gators utformning, Version 2</source>. <publisher-loc>Stockholm, Sweden</publisher-loc>: <publisher-name>Swedish Transport Administration</publisher-name>; <year>2015</year>. Report No.: 2015:086.</mixed-citation></ref><ref id="pone.0204638.ref020"><label>20</label><mixed-citation publication-type="other">Lighting Metropolis. <ext-link ext-link-type="uri" xlink:href="http://www.lightingmetropolis.com">www.lightingmetropolis.com</ext-link> [170405].</mixed-citation></ref><ref id="pone.0204638.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Rahm</surname><given-names>J</given-names></name>, <name><surname>Johansson</surname><given-names>M</given-names></name>. <article-title>Walking after dark&#x02013;A systematic literature review of pedestrians&#x02019; response to outdoor lighting</article-title>. <source>Lund: Dept. of Architecture and Built Environment</source>; <year>2016</year>. Report No.: 26.</mixed-citation></ref><ref id="pone.0204638.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Caminada</surname><given-names>JF</given-names></name>, <name><surname>Van Bommel</surname><given-names>WJM</given-names></name>. <article-title>New lighting considerations for residential areas</article-title>. <source>International Lighting Review</source>. <year>1980</year>(<issue>3</issue>):<fpage>69</fpage>&#x02013;<lpage>75</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Dong</surname><given-names>M</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Lin</surname><given-names>Y</given-names></name>. <article-title>The influence of luminance, observation duration and procedure on the recognition of pedestrians' faces</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>6</issue>):<fpage>693</fpage>&#x02013;<lpage>704</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>B</given-names></name>, <name><surname>Cheal</surname><given-names>C</given-names></name>. <article-title>Effects of outdoor lighting on judgements of emotion and gaze direction</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>3</issue>):<fpage>301</fpage>&#x02013;<lpage>15</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Iwata</surname><given-names>M</given-names></name>, <name><surname>Uchida</surname><given-names>S</given-names></name>. <article-title>Experiment to evaluate visibility with street luminaires with different upward light output ratios and the use of calculated veiling luminance to determine contrast performance</article-title>. <source>J Light Vis Env</source>. <year>2011</year>;<volume>35</volume>(<issue>1</issue>):<fpage>42</fpage>&#x02013;<lpage>54</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Kohko</surname><given-names>S</given-names></name>, <name><surname>Kawakami</surname><given-names>K</given-names></name>, <name><surname>Nakamura</surname><given-names>Y</given-names></name>. <article-title>A study on affects of veiling luminance on pedestrian visibility</article-title>. <source>J Light Vis Env</source>. <year>2008</year>;<volume>32</volume>(<issue>3</issue>):<fpage>315</fpage>&#x02013;<lpage>21</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Castleton</surname><given-names>H</given-names></name>, <name><surname>Cheal</surname><given-names>C</given-names></name>, <name><surname>Yang</surname><given-names>B</given-names></name>. <article-title>Investigating the chromatic contribution to recognition of facial expression</article-title>. <source>Lighting Res Technol</source>. <year>2017</year>;<volume>49</volume>(<issue>2</issue>):<fpage>243</fpage>&#x02013;<lpage>58</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>B</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>. <article-title>Lighting and recognition of emotion conveyed by facial expressions</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>8</issue>):<fpage>964</fpage>&#x02013;<lpage>75</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Knight</surname><given-names>C</given-names></name>. <article-title>Field surveys of the effect of lamp spectrum on the perception of safety and comfort at night</article-title>. <source>Lighting Res Technol</source>. <year>2010</year>;<volume>42</volume>(<issue>3</issue>):<fpage>313</fpage>&#x02013;<lpage>29</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>Y</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>. <article-title>Investigating methods for measuring face recognition under lamps of different spectral power distribution</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>2</issue>):<fpage>221</fpage>&#x02013;<lpage>35</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Rea</surname><given-names>MS</given-names></name>, <name><surname>Bullough</surname><given-names>J</given-names></name>, <name><surname>Akashi</surname><given-names>Y</given-names></name>. <article-title>Several views of metal halide and high-pressure sodium lighting for outdoor applications</article-title>. <source>Lighting Res Technol</source>. <year>2009</year>;<volume>41</volume>(<issue>4</issue>):<fpage>297</fpage>&#x02013;<lpage>320</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>Q</given-names></name>, <name><surname>Sun</surname><given-names>Y</given-names></name>, <name><surname>Lin</surname><given-names>Y</given-names></name>. <article-title>Research on facial recognition and color identification under CMH and HPS lamps for road lighting</article-title>. <source>Leukos</source>. <year>2009</year>;<volume>6</volume>(<issue>2</issue>):<fpage>169</fpage>&#x02013;<lpage>78</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Uttley</surname><given-names>J</given-names></name>, <name><surname>Fox</surname><given-names>S</given-names></name>. <article-title>Exploring the nature of visual fixations on other pedestrians</article-title>. <source>Lighting Res Technol</source>. <year>2016</year>(OnlineFirst).</mixed-citation></ref><ref id="pone.0204638.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Raynham</surname><given-names>P</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>. <article-title>Is facial recognition what matters</article-title>. <source>Lighting Res Technol</source>. <year>2011</year>;<volume>43</volume>:<fpage>129</fpage>&#x02013;<lpage>30</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Cheal</surname><given-names>C</given-names></name>. <article-title>Obstacle detection: A pilot study investigating the effects of lamp type, illuminance and age</article-title>. <source>Lighting Res Technol</source>. <year>2009</year>;<volume>41</volume>(<issue>4</issue>):<fpage>321</fpage>&#x02013;<lpage>42</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Cheal</surname><given-names>C</given-names></name>. <article-title>Using obstacle detection to identify appropriate illuminances for lighting in residential roads</article-title>. <source>Lighting Res Technol</source>. <year>2013</year>;<volume>45</volume>(<issue>3</issue>):<fpage>362</fpage>&#x02013;<lpage>76</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Uttley</surname><given-names>J</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Cheal</surname><given-names>C</given-names></name>. <article-title>Effect of illuminance and spectrum on peripheral obstacle detection by pedestrians</article-title>. <source>Lighting Res Technol</source>. <year>2017</year>;<volume>49</volume>(<issue>2</issue>):<fpage>211</fpage>&#x02013;<lpage>27</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Uttley</surname><given-names>J</given-names></name>. <article-title>Illuminance required to detect a pavement obstacle of critical size</article-title>. <source>Lighting Res Technol</source>. <year>2018</year>;<volume>50</volume>(<issue>3</issue>):<fpage>390</fpage>&#x02013;<lpage>404</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Johansson</surname><given-names>M</given-names></name>, <name><surname>Pedersen</surname><given-names>E</given-names></name>, <name><surname>Maleetipwan-Mattsson</surname><given-names>P</given-names></name>, <name><surname>Kuhn</surname><given-names>L</given-names></name>, <name><surname>Laike</surname><given-names>T</given-names></name>. <article-title>Perceived outdoor lighting quality (POLQ): A lighting assessment tool</article-title>. <source>J Environ Psychol</source>. <year>2014</year>;<volume>39</volume>:<fpage>14</fpage>&#x02013;<lpage>21</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Shikakura</surname><given-names>T</given-names></name>, <name><surname>Kikuchi</surname><given-names>S</given-names></name>, <name><surname>Tanaka</surname><given-names>T</given-names></name>, <name><surname>Furuta</surname><given-names>Y</given-names></name>. <article-title>Psychological evaluation of outdoor pedestrian lighting based on rendered images by computer graphics</article-title>. <source>J Illum Eng Inst Japan</source>. <year>1992</year>;<volume>74</volume>(<issue>10</issue>):<fpage>648</fpage>&#x02013;<lpage>53</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Boyce</surname><given-names>PR</given-names></name>, <name><surname>Eklund</surname><given-names>NH</given-names></name>, <name><surname>Hamilton</surname><given-names>BJ</given-names></name>, <name><surname>Bruno</surname><given-names>LD</given-names></name>. <article-title>Perceptions of safety at night in different lighting conditions</article-title>. <source>Lighting Res Technol</source>. <year>2000</year>;<volume>32</volume>(<issue>2</issue>):<fpage>79</fpage>&#x02013;<lpage>91</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Vili&#x0016b;nas</surname><given-names>V</given-names></name>, <name><surname>Vaitkevi&#x0010d;ius</surname><given-names>H</given-names></name>, <name><surname>Stanik&#x0016b;nas</surname><given-names>R</given-names></name>, <name><surname>Vitta</surname><given-names>P</given-names></name>, <name><surname>Bliumas</surname><given-names>R</given-names></name>, <name><surname>Au&#x00161;kalnyt&#x00117;</surname><given-names>A</given-names></name>, <etal>et al</etal>
<article-title>Subjective evaluation of luminance distribution for intelligent outdoor lighting</article-title>. <source>Lighting Res Technol</source>. <year>2014</year>;<volume>46</volume>(<issue>4</issue>):<fpage>421</fpage>&#x02013;<lpage>33</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Hanyu</surname><given-names>K</given-names></name>. <article-title>Visual properties and affective appraisals in residential areas after dark</article-title>. <source>J Environ Psychol</source>. <year>1997</year>;<volume>17</volume>(<issue>4</issue>):<fpage>301</fpage>&#x02013;<lpage>15</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>K&#x000fc;ller</surname><given-names>R</given-names></name>, <name><surname>Wetterberg</surname><given-names>L</given-names></name>. <article-title>Melatonin, cortisol, EEG, ECG and subjective comfort in healthy humans: Impact of two fluorescent lamp types at two light intensities</article-title>. <source>Lighting Res Technol</source>. <year>1993</year>;<volume>25</volume>(<issue>2</issue>):<fpage>71</fpage>.</mixed-citation></ref><ref id="pone.0204638.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>K&#x000fc;ller</surname><given-names>R</given-names></name>, <name><surname>Wetterberg</surname><given-names>L</given-names></name>. <article-title>The subterranean work environment: Impact on well-being and health</article-title>. <source>Env Int</source>. <year>1996</year>;<volume>22</volume>(<issue>1</issue>):<fpage>33</fpage>&#x02013;<lpage>52</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Quartier</surname><given-names>K</given-names></name>, <name><surname>Vanrie</surname><given-names>J</given-names></name>, <name><surname>Van Cleempoel</surname><given-names>K</given-names></name>. <article-title>As real as it gets: What role does lighting have on consumer's perception of atmosphere, emotions and behaviour?</article-title>
<source>J Environ Psychol</source>. <year>2014</year>;<volume>39</volume>:<fpage>32</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>N</given-names></name>, <name><surname>Farr</surname><given-names>CA</given-names></name>. <article-title>The effects of lighting on consumers' emotions and behavioral intentions in a retail environment: A cross-cultural comparison</article-title>. <source>J Interior Design</source>. <year>2007</year>;<volume>33</volume>(<issue>1</issue>):<fpage>17</fpage>&#x02013;<lpage>32</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>Pedersen</surname><given-names>E</given-names></name>, <name><surname>Johansson</surname><given-names>M</given-names></name>. <article-title>Dynamic pedestrian lighting: Effects on walking speed, legibility and environmental perception</article-title>. <source>Lighting Res Technol</source>. <year>2018</year>;<volume>50</volume>(<issue>4</issue>):<fpage>522</fpage>&#x02013;<lpage>36</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Choi</surname><given-names>J-S</given-names></name>, <name><surname>Kang</surname><given-names>D-W</given-names></name>, <name><surname>Shin</surname><given-names>Y-H</given-names></name>, <name><surname>Tack</surname><given-names>G-R</given-names></name>. <article-title>Differences in gait pattern between the elderly and the young during level walking under low illumination</article-title>. <source>Acta Bioeng Biomech</source>. <year>2014</year>;<volume>16</volume>(<issue>1</issue>):<fpage>3</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="pmid">24707805</pub-id></mixed-citation></ref><ref id="pone.0204638.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Itoh</surname><given-names>N</given-names></name>. <article-title>Visual guidance of walking: Effects of illumination level and edge emphasis</article-title>. <source>Gerontechnology</source>. <year>2006</year>;<volume>5</volume>(<issue>4</issue>):<fpage>246</fpage>&#x02013;<lpage>52</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref051"><label>51</label><mixed-citation publication-type="book"><name><surname>Gustafsson</surname><given-names>B</given-names></name>, <name><surname>Hermer&#x000e9;n</surname><given-names>G</given-names></name>, <name><surname>Petterson</surname><given-names>B</given-names></name>. <chapter-title>Good research practice</chapter-title>
<publisher-loc>Stockholm</publisher-loc>: <publisher-name>Swedish Research Council</publisher-name>; <year>2011</year>. Report No.: 3:2011.</mixed-citation></ref><ref id="pone.0204638.ref052"><label>52</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>Y</given-names></name>, <name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Wei</surname><given-names>M</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Guo</surname><given-names>W</given-names></name>, <name><surname>Sun</surname><given-names>Y</given-names></name>. <article-title>Eye movement and pupil size constriction under discomfort glare</article-title>. <source>Invest Ophthalmol Vis Sci</source>. <year>2015</year>;<volume>56</volume>:<fpage>1649</fpage>&#x02013;<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1167/iovs.14-15963</pub-id>
<pub-id pub-id-type="pmid">25634984</pub-id></mixed-citation></ref><ref id="pone.0204638.ref053"><label>53</label><mixed-citation publication-type="book"><name><surname>Ekman</surname><given-names>P</given-names></name>. <chapter-title>Emotions revealed: Recognizing faces and feelings to improve communication and emotional life</chapter-title>
<edition>2 ed</edition>: <publisher-loc>New York</publisher-loc>: <publisher-name>Henry Holt</publisher-name>; <year>2007</year>.</mixed-citation></ref><ref id="pone.0204638.ref054"><label>54</label><mixed-citation publication-type="journal"><name><surname>Izard</surname><given-names>CE</given-names></name>, <name><surname>Libero</surname><given-names>DZ</given-names></name>, <name><surname>Putnam</surname><given-names>P</given-names></name>, <name><surname>Haynes</surname><given-names>OM</given-names></name>. <article-title>Stability of emotion experiences and their relations to traits of personality</article-title>. <source>J Pers Soc Psychol</source>. <year>1993</year>;<volume>64</volume>(<issue>5</issue>):<fpage>847</fpage>&#x02013;<lpage>60</lpage>. <pub-id pub-id-type="pmid">8505713</pub-id></mixed-citation></ref><ref id="pone.0204638.ref055"><label>55</label><mixed-citation publication-type="journal"><name><surname>Russell</surname><given-names>JA</given-names></name>, <name><surname>Weiss</surname><given-names>A</given-names></name>, <name><surname>Mendelsohn</surname><given-names>GA</given-names></name>. <article-title>Affect grid: A single-item scale of pleasure and arousal</article-title>. <source>J Pers Soc Psychol</source>. <year>1989</year>;<volume>57</volume>(<issue>3</issue>):<fpage>493</fpage>&#x02013;<lpage>502</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref056"><label>56</label><mixed-citation publication-type="journal"><name><surname>Johansson</surname><given-names>M</given-names></name>, <name><surname>Sternudd</surname><given-names>C</given-names></name>, <name><surname>K&#x000e4;rrholm</surname><given-names>M</given-names></name>. <article-title>Perceived urban design qualities and affective experiences of walking</article-title>. <source>Journal of Urban Design</source>. <year>2016</year>;<volume>21</volume>(<issue>2</issue>):<fpage>256</fpage>&#x02013;<lpage>75</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref057"><label>57</label><mixed-citation publication-type="journal"><name><surname>Jackson</surname><given-names>GR</given-names></name>, <name><surname>Owsley</surname><given-names>C</given-names></name>, <name><surname>McGwin</surname><given-names>G</given-names><suffix>Jr.</suffix></name>, <article-title>Aging and dark adaptation</article-title>. <source>Vision Res</source>. <year>1999</year>;<volume>39</volume>(<issue>23</issue>):<fpage>3975</fpage>&#x02013;<lpage>82</lpage>. <pub-id pub-id-type="pmid">10748929</pub-id></mixed-citation></ref><ref id="pone.0204638.ref058"><label>58</label><mixed-citation publication-type="journal"><name><surname>van den Berg</surname><given-names>TJTP</given-names></name>, <name><surname>van Rijn</surname><given-names>LJ</given-names></name>, <name><surname>Kaper-Bongers</surname><given-names>R</given-names></name>, <name><surname>Vonhoff</surname><given-names>DJ</given-names></name>, <name><surname>V&#x000f6;lker-Dieben</surname><given-names>HJ</given-names></name>, <name><surname>Grabner</surname><given-names>G</given-names></name>, <etal>et al</etal>
<article-title>Disability glare in the aging eye. Assessment and impact on driving</article-title>. <source>Journal of Optometry</source>, Vol <volume>2</volume>, Iss <issue>3</issue>, Pp <fpage>112</fpage>&#x02013;<lpage>118</lpage> (<year>2009</year>). 2009(3):112.</mixed-citation></ref><ref id="pone.0204638.ref059"><label>59</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Johansson</surname><given-names>M</given-names></name>. <article-title>Appraising the intention of other people: Ecological validity and procedures for investigating effects of lighting for pedestrians</article-title>. <source>Lighting Res Technol</source>. <year>2017</year>.</mixed-citation></ref><ref id="pone.0204638.ref060"><label>60</label><mixed-citation publication-type="journal"><name><surname>Zeng</surname><given-names>W</given-names></name>, <name><surname>Nakamura</surname><given-names>H</given-names></name>, <name><surname>Chen</surname><given-names>P</given-names></name>. <article-title>A modified social force model for pedestrian behavior simulation at signalized crosswalks</article-title>. <source>Procedia&#x02014;Social and Behavioral Sciences</source>. <year>2014</year>;<volume>138</volume>:<fpage>521</fpage>&#x02013;<lpage>30</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref061"><label>61</label><mixed-citation publication-type="journal"><name><surname>Zeng</surname><given-names>W</given-names></name>, <name><surname>Chen</surname><given-names>P</given-names></name>, <name><surname>Yu</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>. <article-title>Specification and calibration of a microscopic model for pedestrian dynamic simulation at signalized intersections: A hybrid approach</article-title>. <source>Transportation Research Part C: Emerging Technologies</source>. <year>2017</year>;<volume>80</volume>:<fpage>37</fpage>&#x02013;<lpage>70</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref062"><label>62</label><mixed-citation publication-type="journal"><name><surname>Davoudian</surname><given-names>N</given-names></name>, <name><surname>Raynham</surname><given-names>P</given-names></name>. <article-title>What do pedestrians look at at night?</article-title>
<source>Lighting Res Technol</source>. <year>2012</year>;<volume>44</volume>(<issue>4</issue>):<fpage>438</fpage>&#x02013;<lpage>48</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref063"><label>63</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Uttley</surname><given-names>J</given-names></name>, <name><surname>Cheal</surname><given-names>C</given-names></name>, <name><surname>Hara</surname><given-names>N</given-names></name>. <article-title>Using eye-tracking to identify pedestrians' critical visual tasks, Part 1. Dual task approach</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>2</issue>):<fpage>133</fpage>&#x02013;<lpage>48</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref064"><label>64</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Uttley</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>B</given-names></name>. <article-title>Using eye-tracking to identify pedestrians&#x02019; critical visual tasks. Part 2. Fixation on pedestrians</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>2</issue>):<fpage>149</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref065"><label>65</label><mixed-citation publication-type="journal"><name><surname>Fotios</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>B</given-names></name>, <name><surname>Uttley</surname><given-names>J</given-names></name>. <article-title>Observing other pedestrians: Investigating the typical distance and duration of fixation</article-title>. <source>Lighting Res Technol</source>. <year>2015</year>;<volume>47</volume>(<issue>5</issue>):<fpage>548</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="pone.0204638.ref066"><label>66</label><mixed-citation publication-type="journal"><name><surname>Luo</surname><given-names>W</given-names></name>, <name><surname>Puolakka</surname><given-names>M</given-names></name>, <name><surname>Zhang</surname><given-names>Q</given-names></name>, <name><surname>Yang</surname><given-names>C</given-names></name>, <name><surname>Halonen</surname><given-names>L</given-names></name>. <article-title>Pedestrian way lighting: User preferences and eyefixation measurements</article-title>. <source>J Lighting Eng</source>. <year>2013</year>;<volume>15</volume>(<issue>1</issue>):<fpage>19</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref></ref-list></back></article>