<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27548175</article-id><article-id pub-id-type="pmc">5017480</article-id><article-id pub-id-type="doi">10.3390/s16081315</article-id><article-id pub-id-type="publisher-id">sensors-16-01315</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Accurate Mobile Urban Mapping via Digital Map-Based SLAM <xref ref-type="author-notes" rid="fn1-sensors-16-01315">&#x02020;</xref></article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Roh</surname><given-names>Hyunchul</given-names></name><xref ref-type="aff" rid="af1-sensors-16-01315">1</xref></contrib><contrib contrib-type="author"><name><surname>Jeong</surname><given-names>Jinyong</given-names></name><xref ref-type="aff" rid="af2-sensors-16-01315">2</xref></contrib><contrib contrib-type="author"><name><surname>Cho</surname><given-names>Younggun</given-names></name><xref ref-type="aff" rid="af2-sensors-16-01315">2</xref></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Ayoung</given-names></name><xref ref-type="aff" rid="af2-sensors-16-01315">2</xref><xref rid="c1-sensors-16-01315" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Quattrochi</surname><given-names>Dale A.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-16-01315"><label>1</label>Robotics Program, Korea Advanced Institute of Science and Technology, Daejeon 34141, Korea; <email>rohs_@kaist.ac.kr</email></aff><aff id="af2-sensors-16-01315"><label>2</label>Department of Civil and Environmental Engineering, Korea Advanced Institute of Science and Technology, Daejeon 34141, Korea; <email>jjy0923@kaist.ac.kr</email> (J.J.), <email>yg.cho@kaist.ac.kr</email> (Y.C.)</aff><author-notes><corresp id="c1-sensors-16-01315"><label>*</label>Correspondence: <email>ayoungk@kaist.ac.kr</email>; Tel.: +82-42-350-3632</corresp><fn id="fn1-sensors-16-01315"><label>&#x02020;</label><p>This paper is an expanded version of Jeong, J.; Kim, A. Adaptive Inverse Perspective Mapping for Lane Map Generation with SLAM. In Proceedings of the IEEE Ubiquitous Robots and Ambient Intelligence, Xi&#x02019;an, China, 19&#x02013;22 August 2016.</p></fn></author-notes><pub-date pub-type="epub"><day>18</day><month>8</month><year>2016</year></pub-date><pub-date pub-type="collection"><month>8</month><year>2016</year></pub-date><volume>16</volume><issue>8</issue><elocation-id>1315</elocation-id><history><date date-type="received"><day>07</day><month>6</month><year>2016</year></date><date date-type="accepted"><day>11</day><month>8</month><year>2016</year></date></history><permissions><copyright-statement>&#x000a9; 2016 by the authors; licensee MDPI, Basel, Switzerland.</copyright-statement><copyright-year>2016</copyright-year><license><license-p><!--CREATIVE COMMONS-->This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC-BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This paper presents accurate urban map generation using digital map-based Simultaneous Localization and Mapping (SLAM). Throughout this work, our main objective is generating a 3D and lane map aiming for sub-meter accuracy. In conventional mapping approaches, achieving extremely high accuracy was performed by either (<italic>i</italic>) exploiting costly airborne sensors or (<italic>ii</italic>) surveying with a static mapping system in a stationary platform. Mobile scanning systems recently have gathered popularity but are mostly limited by the availability of the Global Positioning System (GPS). We focus on the fact that the availability of GPS and urban structures are both sporadic but complementary. By modeling both GPS and digital map data as measurements and integrating them with other sensor measurements, we leverage SLAM for an accurate mobile mapping system. Our proposed algorithm generates an efficient graph SLAM and achieves a framework running in real-time and targeting sub-meter accuracy with a mobile platform. Integrated with the SLAM framework, we implement a motion-adaptive model for the Inverse Perspective Mapping (IPM). Using motion estimation derived from SLAM, the experimental results show that the proposed approaches provide stable bird&#x02019;s-eye view images, even with significant motion during the drive. Our real-time map generation framework is validated via a long-distance urban test and evaluated at randomly sampled points using Real-Time Kinematic (RTK)-GPS.</p></abstract><kwd-group><kwd>3D mapping</kwd><kwd>SLAM</kwd><kwd>digital map</kwd><kwd>urban mapping system</kwd><kwd>IPM</kwd><kwd>lane map</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-16-01315"><title>1. Introduction</title><p>The recent development of autonomous vehicles encompasses many key issues in robotics problems, including perception, planning, control, localization and mapping. Among these problems, this paper focuses on solutions for an accurate urban map generation, specifically targeting 3D and lane maps for autonomous cars. Having an accurate map is an important issue for self-driving cars [<xref rid="B1-sensors-16-01315" ref-type="bibr">1</xref>,<xref rid="B2-sensors-16-01315" ref-type="bibr">2</xref>,<xref rid="B3-sensors-16-01315" ref-type="bibr">3</xref>] and many other areas such as virtual reality [<xref rid="B4-sensors-16-01315" ref-type="bibr">4</xref>], visualization [<xref rid="B5-sensors-16-01315" ref-type="bibr">5</xref>], recognition [<xref rid="B6-sensors-16-01315" ref-type="bibr">6</xref>], localization and navigation [<xref rid="B7-sensors-16-01315" ref-type="bibr">7</xref>,<xref rid="B8-sensors-16-01315" ref-type="bibr">8</xref>].</p><p>In conventional mapping approaches, aerial sensing has been heavily investigated. In these approaches, the fusion of aerial images with aerial Light Detection and Ranging (LiDAR) and/or radar is usually applied for an accurate digital map at the cm-level, including urban structures. This line of studies focuses on undistorting building shapes from aerial sensors, which is mostly too costly (ranging from $0.5 M to $1.4 M). To obtain cost-effective solutions for mapping, recent advances have appeared in mobile mapping systems. Using a car-like system with sensors mounted, 3D maps are often generated from a set of point clouds, using LiDAR sensors to actively acquire accurate and dense 3D point clouds of building fa&#x000e7;ades or surfaces of objects. Mobile scanning is widely used for modeling in architecture [<xref rid="B4-sensors-16-01315" ref-type="bibr">4</xref>] and agriculture [<xref rid="B9-sensors-16-01315" ref-type="bibr">9</xref>], as well as for urban and regional planning [<xref rid="B10-sensors-16-01315" ref-type="bibr">10</xref>]. Many studies combine a highly accurate GPS, a high precision Inertial Measurement Unit (IMU) and the wheel/visual odometry of the vehicle to compute fully timestamped trajectories. The absence of the GPS in constricted spaces is an exceptionally limiting factor for mobile scanning. It is more critical, in a complex urban environment, and thus it is difficult to calculate accurate positions using a GPS sensor due to blackouts or multipath problems. Generally, using a variety of sensors has became a popular strategy to complement the drawbacks of a single sensor.</p><p>We propose a SLAM-based approach to localize the mobile sensor system mounted on a car-like platform and generate an accurate urban map consisting of a large amount of 3D points. For accurate localization, we use publicly available digital maps together with cameras, Light Detection and Ranging (LiDAR)s, and inertial sensors. Using this accurate localization, motion compensated Inverse Perspective Mapping (IPM) is applied for accurate lane-map generation. Given accurate localization, projections of front-looking cameras follow the construction of the lane map. For the lane map description, we propose an adaptive IPM algorithm to obtain accurate bird&#x02019;s-eye view images from the sequential images of forward-looking cameras. These images are often distorted by the motion of the vehicle; even a small motion can cause a substantial effect on bird&#x02019;s-eye view images.</p><p>To use a digital map in Simultaneous Localization and Mapping (SLAM), we incorporate a shape file to extract structural and elevation information. Although digital maps with 1:1000 and 1:5000 scales offer sub-meter global average accuracy, only a subset of map data (e.g., buildings and road boundaries which are globally static) are available for vehicle localization. In this paper, we introduce a measurement model for both GPS and structural information, and leverage the digital map data as observations to actively fuse digital maps in SLAM when GPS signals are sporadic.</p><p>The contribution of the proposed method can be summarized as follows. <list list-type="bullet"><list-item><p>Digital map-based SLAM: Digital map information was incorporated with a building model, road network and elevation model. This paper introduces a measurement model to add global measurements for full 3D SLAM. Instead of addressing this problem as localization to a prior map, our modeling handles the structural information in the digital map as sporadic spatial measurements.</p></list-item><list-item><p>Development of online mobile mapping system with sub-meter accuracy: Typical LiDAR-based mapping system known to be slow, suffering from association problems. Despite a large number of nodes and a long trajectory, the entire method runs in real-time using only 40% of the total mission time. Not only the computational time, our algorithm provides memory-efficient implementation. Wall information obtained from point cloud turns into an urban signature, which then can effectively be used in the matching phase. Fully integrated single step implementation allows us to build a 3D map over 9 km while driving.</p></list-item><list-item><p>Thorough analysis for resulting 3D and lane maps accuracy: We perform a thorough analysis on the resulting maps to evaluate the effects of digital maps. The addition of elevation and buildings from digital maps enables accurate 3D urban mapping. We performed quantitative evaluation on random sample points. Using RTK-GPS with 10-mm-accuracy, we analyzed the accuracy of the points from buildings and lane maps.</p></list-item></list></p></sec><sec id="sec2-sensors-16-01315"><title>2. Related Works</title><p>Conventional approaches in digital map generation rely on aerial sensors, including high-resolution aerial images, Light Detection and Ranging (LiDAR)s, and radars. Toward accurate building detection, point cloud data from aerial LiDAR has been widely used for a Digital Building Model (DBM) [<xref rid="B11-sensors-16-01315" ref-type="bibr">11</xref>,<xref rid="B12-sensors-16-01315" ref-type="bibr">12</xref>,<xref rid="B13-sensors-16-01315" ref-type="bibr">13</xref>,<xref rid="B14-sensors-16-01315" ref-type="bibr">14</xref>]. One line of study used only aerial imagery to extract building information [<xref rid="B15-sensors-16-01315" ref-type="bibr">15</xref>,<xref rid="B16-sensors-16-01315" ref-type="bibr">16</xref>], while merging the high-resolution aerial imagery and aerial LiDAR [<xref rid="B17-sensors-16-01315" ref-type="bibr">17</xref>] has been introduced.</p><p>The aforementioned approaches use costly aerial sensors and have limitations in generating maps inside tunnels and multi-layered roads. For a cost-effective mobile mapping system, urban mapping systems similar to ours have been introduced in the literature [<xref rid="B18-sensors-16-01315" ref-type="bibr">18</xref>,<xref rid="B19-sensors-16-01315" ref-type="bibr">19</xref>,<xref rid="B20-sensors-16-01315" ref-type="bibr">20</xref>,<xref rid="B21-sensors-16-01315" ref-type="bibr">21</xref>]. Blanco et al. [<xref rid="B18-sensors-16-01315" ref-type="bibr">18</xref>] presented a collection of outdoor datasets using a large and heterogeneous set of sensors comprising color cameras, several laser scanners, precise GPSs, and an IMU. Their posterior research [<xref rid="B19-sensors-16-01315" ref-type="bibr">19</xref>] introduced a dataset gathered entirely in urban scenarios using a car equipped with one stereo camera and five laser scanners, among other sensors. Elseberg et al. [<xref rid="B20-sensors-16-01315" ref-type="bibr">20</xref>] used a commercial data logging system and collected data with an experimental platform constructed by the RIEGL scanner [<xref rid="B22-sensors-16-01315" ref-type="bibr">22</xref>]. Bok et al. [<xref rid="B21-sensors-16-01315" ref-type="bibr">21</xref>] used a mobile mapping sensor system mounted on a ground vehicle. A vertical 2D LiDAR scans structures and the reconstruction is done by accumulating scanned data. All of the conventional approaches cover a small region, assuming a 2D environment without taking altitude information into account.</p><p>For these mobile mapping systems, cameras are the most popular sensors in detecting lanes. For visual road-mark detection, IPM is often used in the vision-based perception of roads and lanes. IPM produces bird&#x02019;s eye-view images that remove the perspective effect by using information about camera parameters and the relationship between the camera and the ground. The results of IPM can provide post-processing algorithms with more efficient information, such as lane perception, mapping, localization, and pattern recognition. Many researchers have studied IPM in many applications, such as distance detection [<xref rid="B23-sensors-16-01315" ref-type="bibr">23</xref>], production of bird&#x02019;s eye-view images of a spacious area using a mosaic method [<xref rid="B24-sensors-16-01315" ref-type="bibr">24</xref>], provision of appropriate bird&#x02019;s eye-view images for parking assistance [<xref rid="B25-sensors-16-01315" ref-type="bibr">25</xref>], and lane-level map generation [<xref rid="B26-sensors-16-01315" ref-type="bibr">26</xref>].</p><p>The challenge for mobile mapping systems is to achieve a consistent map. Researchers investigated the SLAM algorithm for accurate vehicle localization [<xref rid="B27-sensors-16-01315" ref-type="bibr">27</xref>] and 3D mapping [<xref rid="B28-sensors-16-01315" ref-type="bibr">28</xref>] using perceptual sensors mounted on a vehicle. Using a digital map has also been recently investigated by many researchers [<xref rid="B29-sensors-16-01315" ref-type="bibr">29</xref>,<xref rid="B30-sensors-16-01315" ref-type="bibr">30</xref>,<xref rid="B31-sensors-16-01315" ref-type="bibr">31</xref>]. Schindler et al. [<xref rid="B29-sensors-16-01315" ref-type="bibr">29</xref>] first proposed lane map generation and representation methods using RTK GPS-based localization and showed high-precision digital map-based self-localization in [<xref rid="B30-sensors-16-01315" ref-type="bibr">30</xref>]. They showed an efficient Monte Carlo localization approach using a map model based on smooth arc splines. Floros et al. [<xref rid="B31-sensors-16-01315" ref-type="bibr">31</xref>] proposed an approach for global vehicle pose estimation that combines visual odometry with map information from OpenStreetMaps [<xref rid="B32-sensors-16-01315" ref-type="bibr">32</xref>] to provide accurate estimates of the vehicle&#x02019;s pose. Similarly, Pink and Stiller [<xref rid="B33-sensors-16-01315" ref-type="bibr">33</xref>] described landmark (lane-based) generation from aerial images with image classification geometric representation. This method used aerial images for prior maps and focused on the way to match orthographic lane images to a global lane map from aerial images. Guo et al. [<xref rid="B26-sensors-16-01315" ref-type="bibr">26</xref>] presented a system for lane-level map generation from local IPM images and the global OpenStreetMap (OSM) database. They locally constructed an orthographic lane map, matched it with a segmented aerial map, and finally generated a global lane graph of real-world roads.</p><p>As an accurate localization aspect, some previous works [<xref rid="B34-sensors-16-01315" ref-type="bibr">34</xref>,<xref rid="B35-sensors-16-01315" ref-type="bibr">35</xref>,<xref rid="B36-sensors-16-01315" ref-type="bibr">36</xref>] used a lane map as a local vehicle position estimation by matching pre-built local lane maps from IPM images and current incoming raw images. Napier and Newman [<xref rid="B34-sensors-16-01315" ref-type="bibr">34</xref>] proposed a vehicle localization method based on synthesized local orthographic lane maps. They generated local orthographic images from a first run and localized the vehicle by Mutual Information (MI) between live-stream images and synthesized images. Schreiber et al. [<xref rid="B35-sensors-16-01315" ref-type="bibr">35</xref>] constructed prior lane-based maps with extended sensors such as GPS/INS and matched road markings and curbs for lane-level localization. Rose et al. [<xref rid="B36-sensors-16-01315" ref-type="bibr">36</xref>] used lane maps for lateral position estimations of a vehicle. These studies only applied localization or pose estimation algorithms; no cases have generated 3D maps using the SLAM algorithm with a digital map. In this paper, we are interested in using publicly available digital maps in a SLAM framework to leverage for accurate lane map and 3D map generation.</p><p>A similar approach to ours was reported in [<xref rid="B37-sensors-16-01315" ref-type="bibr">37</xref>] which incorporated bundle adjustment using monocular vision. Although extracting buildings from digital maps and using them as constraints were introduced and conceptually similar to ours, their implementation relied mostly on vision, covering a relatively small urban area. Unlike their approach, proposed algorithm focuses on using LiDAR accomplishing real-time performance with a single-step mapping algorithm that can build an urban map while driving.</p></sec><sec id="sec3-sensors-16-01315"><title>3. System Overview</title><p>For accurate urban mapping, we developed a sensor suite called Urban Mapping System (UMS), which is mountable to a car-like platform. As shown in <xref ref-type="fig" rid="sensors-16-01315-f001">Figure 1</xref>a, the platform used for mapping was equipped with three Light Detection and Ranging (LiDAR) sensors, four cameras, a Global Positioning System (GPS), an Inertial Measurement Unit (IMU), an altimeter, and wheel encoders. The detailed specifications are summarized in <xref ref-type="table" rid="sensors-16-01315-t001">Table 1</xref>. As in <xref ref-type="fig" rid="sensors-16-01315-f001">Figure 1</xref>, coordinates of each sensor were represented with respect to the vehicle center coordinates (red, green and blue arrows).</p><p>Using a geometrical relationship between the vehicle center and LiDAR, the position of the camera is computed using extrinsic calibration results of LiDAR and cameras. For more details, refer to [<xref rid="B38-sensors-16-01315" ref-type="bibr">38</xref>] the system configuration.</p><p>In the proposed configuration, LiDAR sensors are facing each side of the vehicle, while the sweeping direction is orthogonal to the ground. This configuration allows us to capture surrounding data line-by-line. Motion induces accumulation of lines, and a 3D point cloud is obtained by stacking a sequence of point cloud lines. Researchers using similar configurations recommended a vehicle speed of approximately 15&#x02013;30 km/h [<xref rid="B39-sensors-16-01315" ref-type="bibr">39</xref>], which we also adopted in the experiments.</p><p>The objective of the proposed platform is to build an accurate 3D and lane map given various sensors and digital maps. The process of generating urban lane maps with 3D point clouds involves four main stepsas shown in <xref ref-type="fig" rid="sensors-16-01315-f002">Figure 2</xref>: (1) sensor data logging; (2) generating a node and the factor; (3) optimizing using an Incremental Smoothing and Mapping (iSAM) SLAM back-end; and (4) generating the final lane map based on the optimized trajectory from the SLAM results and IPM images. Measurements from odometry, digital map information, elevation, and GPS are all used.</p></sec><sec id="sec4-sensors-16-01315"><title>4. Pose-Graph Simultaneous Localization and Mapping (SLAM)</title><sec id="sec4dot1-sensors-16-01315"><title>4.1. State Definition</title><p>We estimate the vehicle&#x02019;s full 6-degree of freedom (DOF) pose, <inline-formula><mml:math id="mm1"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The augmented state representation is expressed as follows for <italic>n</italic> keyframes,
<disp-formula id="FD1-sensors-16-01315"><label>(1)</label><mml:math id="mm2"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn><mml:mi>&#x022a4;</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi><mml:mi>&#x022a4;</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x022a4;</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> while each of the pose samples (<inline-formula><mml:math id="mm3"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>) correspond to the time instance <inline-formula><mml:math id="mm4"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of a keyframe. For the SLAM back-end, we use iSAM [<xref rid="B40-sensors-16-01315" ref-type="bibr">40</xref>] to find an optimized solution of the trajectory, from which we build the urban map. Factors from each sensor measurement are collected as in <xref ref-type="fig" rid="sensors-16-01315-f003">Figure 3</xref>. The attitude measurements and altitude are given as absolute factors per node, with all nodes sequentially connected by odometry measurements. Loop-closing factors are mainly generated by walls that we generate from LiDAR when we add building measurement from digital maps. GPS availability is quite sporadic in urban area, and measurements are intermittently applied to a node using Dynamic Covariance Scaling (DCS).</p></sec><sec id="sec4dot2-sensors-16-01315"><title>4.2. Odometry Modeling</title><p>The system has two rotary encoders, as shown in <xref ref-type="fig" rid="sensors-16-01315-f001">Figure 1</xref>a, which are mounted to record wheel revolution counts and the resulting rotation angles (<inline-formula><mml:math id="mm5"><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="mm6"><mml:msub><mml:mi>C</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>) of each wheel. We define vehicle pose in 2D space, <inline-formula><mml:math id="mm7"><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> for odometry measurements. We calculate the relative pose difference as a function of left/right wheel diameter (<inline-formula><mml:math id="mm8"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), left and right wheel rotation angle (<inline-formula><mml:math id="mm9"><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="mm10"><mml:msub><mml:mi>C</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>), wheel diameter (<inline-formula><mml:math id="mm11"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and wheel base (<inline-formula><mml:math id="mm12"><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula>).
<disp-formula id="FD2-sensors-16-01315"><label>(2)</label><mml:math id="mm13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="normal">z</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> Here, <inline-formula><mml:math id="mm14"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the average and <inline-formula><mml:math id="mm15"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the difference of two travel distances from the left and right wheel.</p></sec><sec id="sec4dot3-sensors-16-01315"><title>4.3. Altitude and IMU Modeling</title><p>We also provide z-directional measurements from the altitude sensor. When mapping hilly terrain, this leverages the localization performance by providing both absolute and relative measurements on height. We used theWITHROBOT MyPressure (WITHROBOT, Seoul, Korea) altitude sensor [<xref rid="B41-sensors-16-01315" ref-type="bibr">41</xref>], which has accuracy within 1 cm. Attitude information was obtained via IMU. A MTI Xsens [<xref rid="B42-sensors-16-01315" ref-type="bibr">42</xref>] was mounted into the system to provide roll, pitch, and yaw in 100 Hz. Altitude and attitude information is used as absolute measurement factors and fed into the SLAM back-end.</p><p>The proposed implementation produces loop-closure on height term based on wall information <xref ref-type="sec" rid="sec5dot3-sensors-16-01315">Section 5.3</xref>. When a loop-closure happens in wall-to-wall matching (i.e., when a wall is revisited), we apply a relative constraint on two associated nodes so as for them to be the same height.</p></sec><sec id="sec4dot4-sensors-16-01315"><title>4.4. GPS Modeling</title><p>GPS becomes extremely unreliable in highly complex urban environments. Signals are lost or deteriorate in urban canyons. Despite these limitations, GPS still provides valuable information to ground vehicles. For our GPS measurement error modeling, we mainly used the GPS Pseudorange Noise Statistics (GPGST), which include the standard deviation of longitude/latitude errors. The error measurement is the most sensitive, depending on the complexity of the environment, showing high error values in complex urban area.</p><p>Outlier handling in SLAM determines the robustness of the entire framework, since a single wrong measurement may critically deteriorate the results. To tackle this issue, S&#x000fc;nderhauf and Protzel [<xref rid="B43-sensors-16-01315" ref-type="bibr">43</xref>] introduced Switchable Constraints (SC) and switching variables (<inline-formula><mml:math id="mm16"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) for the robust SLAM back-end. This research area was further developed by Agarwal et al. [<xref rid="B44-sensors-16-01315" ref-type="bibr">44</xref>], who solve for a closed form solution to determine these switching variables <inline-formula><mml:math id="mm17"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mo>.</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x003a6;</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003c7;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:math></inline-formula> As can be seen, the choice depends on <inline-formula><mml:math id="mm18"><mml:msubsup><mml:mi>&#x003c7;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, which represents the error for each loop closing constraint. By considering the induced error by a factor, the associated covariance is dynamically scaled. This substantially improves the results when a certain measurement may be unreliable during the mission, such as GPS. In this implementation, we adopted DCS to cope with intermittent and unreliable GPS signals.</p></sec></sec><sec id="sec5-sensors-16-01315"><title>5. Digital Map-Based SLAM</title><p>We introduce using a digital map with Light Detection and Ranging (LiDAR) sensor to correct the navigation error even under unreliable Global Positioning System (GPS). We first create walls from a 3D point cloud generated by LiDAR. Using this point cloud as a prior reference, the building edges are automatically detected to correct navigation error. In this section, we introduce the generation of walls, matching with a digital map, and matching between walls for loop-closure.</p><sec id="sec5dot1-sensors-16-01315"><title>5.1. Digital Map</title><p>We leverage publicly available digital maps for SLAM application. Specifically, we focus on vectorized digital maps widely used in a geospatial information system. Studies that automatically generate the digital map have been continued in recent years [<xref rid="B11-sensors-16-01315" ref-type="bibr">11</xref>,<xref rid="B12-sensors-16-01315" ref-type="bibr">12</xref>,<xref rid="B13-sensors-16-01315" ref-type="bibr">13</xref>,<xref rid="B14-sensors-16-01315" ref-type="bibr">14</xref>,<xref rid="B15-sensors-16-01315" ref-type="bibr">15</xref>,<xref rid="B16-sensors-16-01315" ref-type="bibr">16</xref>,<xref rid="B17-sensors-16-01315" ref-type="bibr">17</xref>,<xref rid="B45-sensors-16-01315" ref-type="bibr">45</xref>,<xref rid="B46-sensors-16-01315" ref-type="bibr">46</xref>,<xref rid="B47-sensors-16-01315" ref-type="bibr">47</xref>,<xref rid="B48-sensors-16-01315" ref-type="bibr">48</xref>] but it has mostly relied on aerial sensors. For the digital map data format, we chose to use a shapefile format developed by ESRI [<xref rid="B49-sensors-16-01315" ref-type="bibr">49</xref>]. The shapefile stores non-topological geometry and attribute information for the spatial features as a set of vector coordinates. Vector features including points, lines, and polygons are described in the shapefile format. The format also represents area features via closed loop and double-digitized polygons. Each attribute record has a one-to-one relationship with the associated shape record. Among many types of records, we focus on polygons that contain information about structural buildings.</p><p>As illustrated in <xref ref-type="fig" rid="sensors-16-01315-f004">Figure 4</xref>a, a polygon consists of one or points that form a closed, non-self-intersecting loop. A polygon may contain more closed curves. A closed curve is a connected sequence of four or more multiple outer closed curves. The order of vertices or orientation for a closed curve indicates which side of the closed curve is the interior of the polygon. Recording contents of this polygon data type are shown in <xref ref-type="table" rid="sensors-16-01315-t002">Table 2</xref>. <xref ref-type="fig" rid="sensors-16-01315-f005">Figure 5</xref>b presents an example of a building extracted from polygon data in a shapefile. In this paper, shapefiles from National Geographic Informaion Institue (NGII) are used for building extraction.</p><p>We also incorporate DEM for elevation information. For certain topographies, regions contain substantial elevation change. In general, the height of the terrain is represented by the contours. As presented in <xref ref-type="fig" rid="sensors-16-01315-f004">Figure 4</xref>d, points are connected by contour lines indicating the same height. The absolute height value of the contour is described in the attribute information of the shape-file. When the trajectory of the vehicle passes a contour, SLAM algorithm optimize position using absolute measurement of Z-direction from this absolute height.</p></sec><sec id="sec5dot2-sensors-16-01315"><title>5.2. Wall Segmentation</title><p>Given building information parsed from the digital maps, we now extract walls from the mobile sensor system. Building fa&#x000e7;ades and other structures&#x02019; surfaces are major features of the urban area. As digital maps are widely available these days, we propose a matching algorithm that uses a digital map and an accumulated local point cloud. In general, use of the point-to-point Iterated Closest Point (ICP) algorithm is popular but it is time-consuming and gives rise to issues when point clouds are not sufficiently dense.</p><p>We define a point cloud <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> as consisting of LiDAR points, while each point indicates a 3D position <inline-formula><mml:math id="mm20"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the global coordinate frame. From the LiDAR 3D point cloud in the sensor frame, (<inline-formula><mml:math id="mm21"><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>) is obtained by converting each beam ranging from <inline-formula><mml:math id="mm22"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to 360. The angular resolution of SICK LMS291 is <inline-formula><mml:math id="mm23"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>5</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, covering a <inline-formula><mml:math id="mm24"><mml:msup><mml:mn>180</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math></inline-formula> field of view for each size. Sensor frame point cloud <inline-formula><mml:math id="mm25"><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is then converted to vehicle frame point cloud <inline-formula><mml:math id="mm26"><mml:msub><mml:mi>P</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> using a coordinate transformation.</p><p>After the transformation, we obtain a vertical line of the point cloud as described in <xref ref-type="fig" rid="sensors-16-01315-f005">Figure 5</xref>a. As the vehicle proceeds, these lines are accumulated and form a local point cloud set. Given a single scan line of LiDAR and odometry information from the wheel encoder and IMU, the set of the line forms a local point cloud-based on the estimated pose. During the local point cloud forming, we evaluate the wall criteria to see if the set can be segmented to a wall. When a wall is registered, the algorithm creates a node in the SLAM graph as a keyframe. Dropping the node clears the accumulated local point cloud and starts a new accumulation of incoming scan lines.</p><p>We propose a fast wall segmentation algorithm-based on RANSAC when segmenting a wall from a point cloud (summarized in <xref ref-type="fig" rid="sensors-16-01315-f006">Figure 6</xref>). For the incoming line of the point cloud, we fit a plane using a 5-point RANSAC. Our wall initialization criteria involve checking the wall width (initialization criteria in <xref ref-type="fig" rid="sensors-16-01315-f006">Figure 6</xref>). If the width of the created wall reaches our threshold (minimum 3 m), the algorithm initializes a wall and stores normal vector and error statistics for the wall. Once the initial wall is established, a newly incoming scan line is evaluated to be merged to the initial wall (merging criteria in <xref ref-type="fig" rid="sensors-16-01315-f006">Figure 6</xref>).</p></sec><sec id="sec5dot3-sensors-16-01315"><title>5.3. Wall-to-wall Loop Closing</title><p>For loop-closure, we use building wall patches by introducing a wall-to-wall measurement model. This method provides an explicit relationship between the walls in the factor graph and produces efficient maps using a data-rich 3D point cloud.</p><p>We parameterize the wall <inline-formula><mml:math id="mm27"><mml:mrow><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> using normal vector <inline-formula><mml:math id="mm28"><mml:mrow><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and the center point of the wall <inline-formula><mml:math id="mm29"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Following wall coordinate transform operators <inline-formula><mml:math id="mm125"><mml:mo>&#x0229e;</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="mm128"><mml:mo>&#x0229f;</mml:mo></mml:math></inline-formula> introduced by Paul Ozog [<xref rid="B50-sensors-16-01315" ref-type="bibr">50</xref>] we derive our wall-to-wall measurement model. The <inline-formula><mml:math id="mm126"><mml:mo>&#x0229e;</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="mm129"><mml:mo>&#x0229f;</mml:mo></mml:math></inline-formula> operations change the base coordinate frame that a plane is described in, which can be written as <inline-formula><mml:math id="mm30"><mml:mrow><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0229f;</mml:mo><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm31"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0229e;</mml:mo><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02296;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0229f;</mml:mo><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>For two walls to be recognized as the same wall, they need to have the same normal vectors and the distance between a point in one plane to another plane should be zero. We compute them by evaluating the dot-product of two normals (<inline-formula><mml:math id="mm32"><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>) and the distance from a center point on one wall to another wall (<inline-formula><mml:math id="mm33"><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>).
<disp-formula id="FD3-sensors-16-01315"><label>(3)</label><mml:math id="mm34"><mml:mrow><mml:msub><mml:mi mathvariant="normal">z</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> Let a wall <italic>k</italic> seen at frame <italic>i</italic> to be <inline-formula><mml:math id="mm35"><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the same wall revisited at frame <italic>j</italic> to be <inline-formula><mml:math id="mm36"><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. To compare normal vectors, <inline-formula><mml:math id="mm37"><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> needs to be transformed to frame <italic>j</italic> using <inline-formula><mml:math id="mm130"><mml:mo>&#x0229f;</mml:mo></mml:math></inline-formula> operation. For simple notation we denote this as <inline-formula><mml:math id="mm38"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0229f;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with associated normal <inline-formula><mml:math id="mm39"><mml:msubsup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math></inline-formula> and the center point <inline-formula><mml:math id="mm40"><mml:msubsup><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math></inline-formula>. Then the error between two normal vectors can be written as <inline-formula><mml:math id="mm41"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Similarly, the plane to point distance (<inline-formula><mml:math id="mm42"><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>) is computed by measuring distance from <inline-formula><mml:math id="mm43"><mml:msubsup><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math></inline-formula> to a wall center point in <italic>j</italic> frame (<inline-formula><mml:math id="mm44"><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p><p>It should be noted that we intentionally excluded the center point&#x02019;s coincident constraints. This is because our point cloud density strongly depends on the vehicle&#x02019;s motion as we accumulate line scans. When a vehicle takes turns the density may increase or decrease, and the computed center point may be varying with respect to the accumulated point cloud density.</p></sec><sec id="sec5dot4-sensors-16-01315"><title>5.4. Wall-Based Digital Map Localization</title><p>With odometry having a moderate accuracy, structural information from buildings can be used as a feature to correct the accumulated navigation error. However, buildings usually deteriorate the GPS signal reception. For a complex urban area, therefore, GPS and buildings can be exploited complementary. The digital map contains a variety of local information and there are vectorized building data among them.</p><p>An extracted wall from a point cloud represented by <inline-formula><mml:math id="mm45"><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> consists of normal vector <inline-formula><mml:math id="mm46"><mml:mrow><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and the center point of the wall <inline-formula><mml:math id="mm47"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>Similarly, we can calculate wall information that consists of normal vector and a center position from digital map <inline-formula><mml:math id="mm48"><mml:msub><mml:mi mathvariant="bold">d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> in global coordinates. Using <inline-formula><mml:math id="mm127"><mml:mo>&#x0229e;</mml:mo></mml:math></inline-formula> operation and current node pose, we write extracted wall information from point cloud as <inline-formula><mml:math id="mm49"><mml:mrow><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0229e;</mml:mo><mml:msub><mml:mi mathvariant="bold">&#x000df;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>We obtain the measurement which consists of the dot-product of two normals (<inline-formula><mml:math id="mm50"><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>) and the distance from a center point on one wall to another wall (<inline-formula><mml:math id="mm51"><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>) in the same manner as in the <xref ref-type="sec" rid="sec5dot3-sensors-16-01315">Section 5.3</xref>.
<disp-formula id="FD4-sensors-16-01315"><label>(4)</label><mml:math id="mm52"><mml:mrow><mml:msub><mml:mi mathvariant="normal">z</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec5dot5-sensors-16-01315"><title>5.5. Elevation-based Full 3D Mapping</title><p>For full 3D mapping, we use DEM that consists of contour lines by introducing a node-to-contour measurement model. When the vehicle traverses a contour, the algorithm detects an intersection between vehicle motion vector and a line segment from the contour.</p><p>Let the vehicle pose be <inline-formula><mml:math id="mm53"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The DEM contour line, <inline-formula><mml:math id="mm54"><mml:mi mathvariant="bold">c</mml:mi></mml:math></inline-formula>, consists of vertices <inline-formula><mml:math id="mm55"><mml:mrow><mml:msub><mml:mi mathvariant="bold">c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>&#x022a4;</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Then we can extract two line segments, vehicle motion induced line (<inline-formula><mml:math id="mm56"><mml:mrow><mml:msub><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) that connects vehicle current node and previous node and contour line segment (<inline-formula><mml:math id="mm57"><mml:mrow><mml:msub><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>z</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) from a nearby contour. Using these two segments, we calculate the intersection point of these two line segments to determine the case when vehicle traverses contour line. Given vehicle pose that meets the DEM, our proposed method produces absolute measurement, which is described in the attribute information of the shape-file (<xref ref-type="fig" rid="sensors-16-01315-f007">Figure 7</xref>c).</p></sec></sec><sec id="sec6-sensors-16-01315"><title>6. Motion-Compensated Adaptive Lane Map Generation</title><p>Once we achieved an accurate localization by using a digital map-based SLAM, we apply IPM to back project road images onto the SLAM-induced map. This section briefly presents the basic IPM model [<xref rid="B51-sensors-16-01315" ref-type="bibr">51</xref>] by using the physical parameters of a camera before illustrating an adaptive IPM model. IPM is a mathematical technique that relates to coordinate systems with different perspectives. More detailed derivation and explanation can be found in [<xref rid="B52-sensors-16-01315" ref-type="bibr">52</xref>]. For lane map generation, we relate undisturbed bird&#x02019;s-eye view images and forward-facing distorted images (<xref ref-type="fig" rid="sensors-16-01315-f008">Figure 8</xref>).</p><sec id="sec6dot1-sensors-16-01315"><title>6.1. Basic Inverse Perspective Mapping (IPM) Model</title><p>The objective is to map pixel points <inline-formula><mml:math id="mm63"><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> to the world coordinate points <inline-formula><mml:math id="mm64"><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. The notation <inline-formula><mml:math id="mm65"><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> indicates a vectored version of the variables. We first define a unit vector <inline-formula><mml:math id="mm66"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> to set the camera&#x02019;s viewing direction. Being orthogonal to <inline-formula><mml:math id="mm67"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, we define another unit vector, <inline-formula><mml:math id="mm68"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, that is orthogonal to the camera-viewing direction on the ground. The IPM is to find the relation between the world coordinate (<inline-formula><mml:math id="mm69"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) and image coordinate (<inline-formula><mml:math id="mm70"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) in order to map image pixels to the world coordinate points. Note that two types of coordinates on an image are set as (<inline-formula><mml:math id="mm71"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm72"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="mm73"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>), depending on the unit. The relation between an image point in a pixel space (<inline-formula><mml:math id="mm74"><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="mm75"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>) and the same point in a meter space (<inline-formula><mml:math id="mm76"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="mm77"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>) is defined as follows <disp-formula id="FD5-sensors-16-01315"><label>(5)</label><mml:math id="mm78"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x027f7;</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="bold">K</mml:mi></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="FD6-sensors-16-01315"><label>(6)</label><mml:math id="mm79"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x027f7;</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="bold">K</mml:mi></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> with a scale factor between pixel and meter (px / m) <inline-formula><mml:math id="mm80"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, the image width <inline-formula><mml:math id="mm81"><mml:mi mathvariant="bold">m</mml:mi></mml:math></inline-formula>, and the images height <inline-formula><mml:math id="mm82"><mml:mi mathvariant="bold">n</mml:mi></mml:math></inline-formula>. The location of the camera&#x02019;s optical center (<inline-formula><mml:math id="mm83"><mml:mi mathvariant="bold">P</mml:mi></mml:math></inline-formula>) is <inline-formula><mml:math id="mm84"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> in the world coordinate system. The unit vector of the optical axis <inline-formula><mml:math id="mm85"><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is orthogonal to the image plane.</p><p>Using the top and side view in <xref ref-type="fig" rid="sensors-16-01315-f009">Figure 9</xref> we can derive Equations (<xref ref-type="disp-formula" rid="FD7-sensors-16-01315">7</xref>) and (<xref ref-type="disp-formula" rid="FD8-sensors-16-01315">8</xref>) for a fixed camera case.
<disp-formula id="FD7-sensors-16-01315"><label>(7)</label><mml:math id="mm86"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>h</mml:mi><mml:mfrac><mml:mrow><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="FD8-sensors-16-01315"><label>(8)</label><mml:math id="mm87"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> The location of <inline-formula><mml:math id="mm88"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> in the world coordinate is dependent on <inline-formula><mml:math id="mm89"><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> because <inline-formula><mml:math id="mm90"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> includes <inline-formula><mml:math id="mm91"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec6dot2-sensors-16-01315"><title>6.2. Adaptive IPM Model</title><p>When images are obtained from a moving vehicle, it is difficult for them to be transformed to the accurate bird&#x02019;s-eye view images because of the motion of the vehicle, especially its pitch direction. To resolve this problem, oscillatory pitch angle (<inline-formula><mml:math id="mm97"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>) of the camera is also added to original pitch angle (<inline-formula><mml:math id="mm98"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>) in this model. This oscillatory pitch angle can be obtained either from sensors or via visual odometry. In this work, we exploited IMU sensor for oscillatory pitch angle. <disp-formula id="FD9-sensors-16-01315"><label>(9)</label><mml:math id="mm99"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mfrac><mml:mrow><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="FD10-sensors-16-01315"><label>(10)</label><mml:math id="mm100"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced><mml:mi>tan</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Finally, the adaptive Inverse Perspective Mapping (IPM) modeling Equations (<xref ref-type="disp-formula" rid="FD9-sensors-16-01315">9</xref>) and (<xref ref-type="disp-formula" rid="FD10-sensors-16-01315">10</xref>) can be derived by adding <inline-formula><mml:math id="mm101"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>. <inline-formula><mml:math id="mm102"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is dependent on the pitch angle of the camera (<inline-formula><mml:math id="mm103"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>) and <inline-formula><mml:math id="mm104"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is also dependent on it, which means that bird&#x02019;s-eye view images are properly compensated against the pitch angle.</p></sec><sec id="sec6dot3-sensors-16-01315"><title>6.3. Consistent Lane Map Processing</title><p>As described in <xref ref-type="sec" rid="sec5-sensors-16-01315">Section 5</xref>, the algorithm drops nodes based upon the wall extraction events. Depending on the circumstances, structure occurrence may be sparse and thus produce a large gap between nodes. When the distance between two consecutive nodes are usually a couple of meters, inter-node inconsistency for measurements becomes substantial. This is because the SLAM algorithm only corrects navigation error for the nodes. Since only nodes are optimized through SLAM update, uncorrected measurements needs post-processing for self-consistent maps. This discrepancy statistics is shown in <xref ref-type="table" rid="sensors-16-01315-t003">Table 3</xref> that describes pose error prior to compensation. The error is computed as the distance and angle between the node position that is generated by the proposed method and is the expected pose by using the vehicle encoder. The cause of this discrepancy is encoder error from the wheel slip and modeling error, and this large discrepancy clearly indicates the need for the compensation. Smooth vehicle poses corrected with SLAM data are obtained after this compensation process.</p><p>The effect of compensation is presented in <xref ref-type="fig" rid="sensors-16-01315-f010">Figure 10</xref>. In our algorithm, valid SLAM nodes are sparse, and thus are not compact enough to create a lane map via IPM. Interpolation is required for a dense map by updating the navigation correction between the nodes using vehicle encoder data. <xref ref-type="fig" rid="sensors-16-01315-f010">Figure 10</xref>a shows interpolated positions between nodes without any compensation. There are lots of errors between expected positions using encoder data and nodes because all nodes are corrected when wall extraction events occur. To overcome this problem, we apply a compensation rule, as described below.
<disp-formula id="FD11-sensors-16-01315"><label>(11)</label><mml:math id="mm115"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">e</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mi>k</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>As shown in <xref ref-type="fig" rid="sensors-16-01315-f011">Figure 11</xref>, we compute the compensation value <inline-formula><mml:math id="mm116"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> per section between nodes. For a section with two nodes on each end, we compared the Dead-Reckoned (DR) position <inline-formula><mml:math id="mm117"><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> and the SLAM-corrected node position <inline-formula><mml:math id="mm118"><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>. When there are <italic>n</italic> number of images in the section we apply compensation value <inline-formula><mml:math id="mm119"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for the position associated with <inline-formula><mml:math id="mm120"><mml:msup><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> image. The result of this interpolation is shown in <xref ref-type="fig" rid="sensors-16-01315-f010">Figure 10</xref>c,d. Note that the compensated projection demonstrates a more smooth and consistent lane map.</p></sec></sec><sec id="sec7-sensors-16-01315"><title>7. Experiments and Results</title><p>To validate the proposed method in the real world, we conduct an experiment covering a campus area with 9.32 km path length. The dataset is the route around a campus environment including four loop-closures by passing the center building repeatedly. The campus area is usually composed of low-rise buildings and wide roads that offer sufficient but sporadic GPS signals. As can be seen in <xref ref-type="table" rid="sensors-16-01315-t004">Table 4</xref>, we have a 41.4% of GPS signal reception.</p><p>The overview of the entire campus area is as in <xref ref-type="fig" rid="sensors-16-01315-f012">Figure 12</xref> depicted in the top-down and perspective view with a scale marked. In total 9.32 km of path is covered in 32 min. Overall the campus shows an altitude difference of 29.5 m between the northern and southern part. The final SLAM graph consists of 1165 nodes, and the average distance between the nodes is 8.06 m.</p><sec id="sec7dot1-sensors-16-01315"><title>7.1. SLAM Results</title><p>We first evaluate the SLAM results. Compared to Dead-Reckoned (DR), SLAM provides a more consistent map as shown in <xref ref-type="fig" rid="sensors-16-01315-f013">Figure 13</xref>b. The position is a value converted to a standard reference point (38<inline-formula><mml:math id="mm121"><mml:msup><mml:mrow/><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math></inline-formula>00&#x02019;00&#x0201d;N/129<inline-formula><mml:math id="mm122"><mml:msup><mml:mrow/><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math></inline-formula>00&#x02019;00&#x0201d;E), which follows the method of transverse Mercator coordinates. The algorithm was capable of making four times of loop closure successfully (<xref ref-type="fig" rid="sensors-16-01315-f013">Figure 13</xref>c) using a wall-to-wall loop-closing measurements. The node uncertainty propagation plot shows the SLAM effectively bounds the uncertainty (<xref ref-type="fig" rid="sensors-16-01315-f013">Figure 13</xref>a). We plot both the entire covariance determinant (<inline-formula><mml:math id="mm123"><mml:mroot><mml:mi mathvariant="normal">&#x003a3;</mml:mi><mml:mn>6</mml:mn></mml:mroot></mml:math></inline-formula> in m&#x000b7;rad) and the positional covariance determinant (<inline-formula><mml:math id="mm124"><mml:mroot><mml:msub><mml:mi mathvariant="normal">&#x003a3;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mn>6</mml:mn></mml:mroot></mml:math></inline-formula> in m). DR shows approximately 10% of the positional uncertainty, while SLAM results are mutually bounded by wall-to-wall loop closure, GPS and digital map correction algorithm. In <xref ref-type="table" rid="sensors-16-01315-t004">Table 4</xref>, the computation time with respect to the total mission time is summarized. The entire method runs in real time using only 40% of the total mission time. This is due to the fact that the low-rise building allowed more GPS signal reception , because the number of the GPS and the wall node was the critical factor in terms of process time.</p><p><xref ref-type="fig" rid="sensors-16-01315-f014">Figure 14</xref> visualizes the nodes by the sensor types for the entire trajectory. As the environment is with low level buildings, GPS signals (blue) are available for many nodes (41.4%). However, the signal is sporadic and vulnerable to the surrounding structures. When the GPS signal becomes unreliable we use complementarity available structural information and correct against the digital map (green). DEM is also effective when altitude change is substantial within a region.</p></sec><sec id="sec7dot2-sensors-16-01315"><title>7.2. Qualitative Urban Mapping Results</title><p>For further validation of the proposed method, we back-project 3D point clouds and images using the accurate localization and mapping results. By doing so, we can generate a 3D urban map with lanes potentially for autonomous car driving. By confirming the consistency in the back-projected dataset, we validate the accuracy of the proposed method and application to urban map generation.</p><sec id="sec7dot2dot1-sensors-16-01315"><title>7.2.1. 3D Point Cloud Map</title><p>Collected LiDAR points are locally accurate with respect to the vehicle pose under exact extrinsic calibration. However, the entire map consistency may not be guaranteed if the estimated trajectory is with errors. Using the SLAM refined trajectory, we use point cloud information in order to create a 3D map of the urban environment. <xref ref-type="fig" rid="sensors-16-01315-f015">Figure 15</xref> presents a qualitative result on two sample buildings created from the back-projection. As can be seen in the sample buildings of <xref ref-type="fig" rid="sensors-16-01315-f015">Figure 15</xref>, the proposed method is capable of detecting even highly complex walls The accuracy of the 3D modeling is significantly affected by the trajectory estimation accuracy. The SLAM based method enables accurate estimation even under a complex motion with many turns (<xref ref-type="fig" rid="sensors-16-01315-f015">Figure 15</xref>c).</p></sec><sec id="sec7dot2dot2-sensors-16-01315"><title>7.2.2. Lane Map via IPM</title><p>We also generate a lane map for urban mapping. Using the Simultaneous Localization and Mapping (SLAM) results and the previously introduced adaptive Inverse Perspective Mapping (IPM) model, we produce precise bird&#x02019;s eye view images compensating for vehicle motion. To generate dense and precise lane map, additional vehicle positions are required because the average distance between nodes of SLAM result is 8.06 m. Additional vehicle position and heading angle are interpolated by using vehicle encoder data from nodes that generated by SLAM. <xref ref-type="fig" rid="sensors-16-01315-f016">Figure 16</xref> shows the entire campus lane map with four representative samples (shown in a zoomed view). Note the consistency of the generated map by the IPM images back-projection.</p></sec></sec><sec id="sec7dot3-sensors-16-01315"><title>7.3. Accuracy Analysis</title><p>In the previous section, we have demonstrated consistency and qualitative evaluation. We also performed quantitative evaluation on the mapping results. Using RTK-GPS with 10 mm-accuracy, we analyzed the accuracy of the points from both sample buildings and lane maps. We use GRX2 by Topcon [<xref rid="B56-sensors-16-01315" ref-type="bibr">56</xref>] in the RTK mode to measure ground truth point sampling. As shown in <xref ref-type="fig" rid="sensors-16-01315-f017">Figure 17</xref>, we measure sample points from both buildings and lanes.</p><p>We analyze the effect of each measurement model for the entire map generation. Our evaluation process is described in <xref ref-type="fig" rid="sensors-16-01315-f018">Figure 18</xref>. Four building corners measured by RTK-GPS are connected to complete the boundary of a building (red square in <xref ref-type="fig" rid="sensors-16-01315-f018">Figure 18</xref>a. By measuring a perpendicular distance from LiDAR-extracted wall to the ground truth, we compute the Root Mean Square Error (RMSE) mapping error for 3D structures. The error analysis is summarized in <xref ref-type="table" rid="sensors-16-01315-t005">Table 5</xref>. GPS-based method shows about a meter accuracy due to the sensor accuracy and unreliable availability. Compared to GPS-based mapping, the digital map-based SLAM shows substantial improvement over the conventional approach.</p><p>Similarly, we examine accuracy on a lane map from a set of sample points on road markings. In total, 16 points from five different sets are measured. Among them, <xref ref-type="fig" rid="sensors-16-01315-f019">Figure 19</xref> presents validation process for a sample set with six sample points. Six RTK-GPS positions are obtained and used as ground truth (<xref ref-type="fig" rid="sensors-16-01315-f019">Figure 19</xref>a). In this analysis, we compare the proposed lane map with aerial image and digital map. Overlaying ground truth on aerial images (<xref ref-type="fig" rid="sensors-16-01315-f019">Figure 19</xref>c) reveals substantial discrepancy due to distortion without compensation from other sensors. In digital maps, road boundaries are accurate but road marking and lanes are missing. Compared to digital and aerial maps, the proposed method (<xref ref-type="fig" rid="sensors-16-01315-f019">Figure 19</xref>b) demonstrates substantial accuracy improvement.</p><p><xref ref-type="table" rid="sensors-16-01315-t006">Table 6</xref> lists accuracy analysis for the proposed method for all five test sets. As in the 3D map analysis, we compare the RTK-GPS measured ground truth with aerial image and proposed method. Unlike the 3D structural map error, the SLAM error is not the only error source for this road marking error. When manually selecting a pixel from distorted and stitched road images, other factors related to image processing influences (e.g., image resolution, image quality, image distortion, manual pixel selection accuracy, and interpolated motion error). Lane parameterization and online conversion will surely improve overall lane results from fine refinement. In this work, however, we mainly aim to validate the SLAM result accuracy by examining error from structural and lane mapping results.</p></sec></sec><sec id="sec8-sensors-16-01315"><title>8. Conclusions</title><p>This paper proposes a seamless way to incorporate publically available digital maps and sensors mounted on a mobile platform. For accurate urban mapping, we focus on 3D structural and lane mapping. The proposed method tackles the accurate map building problem by integrating global measurements with local sensor measurements via Simultaneous Localization and Mapping (SLAM). The results validate the accuracy of the created map. As the ground truth is not available we have conducted accuracy analysis by measuring a set of sample points using RTK-GPS. The proposed method has proven itself capable of balancing unreliable GPS and digital map information to create an accurate map.</p></sec></body><back><ack><title>Acknowledgments</title><p>This material is based upon work supported from &#x02018;Development of real-time localization and 3-D mapping system with cm-level accuracy based on digital maps and vision data for autonomous driving&#x02019; project by the Ministry of Trade, Industry &#x00026; Energy (MOTIE, Korea) under Industrial Technology Innovation Program (No. 10051867) and &#x02018;3D Mapping for Intelligent Vehicle via Digital Map-Based SLAM&#x02019; project funded by Naver Corporation.</p></ack><notes><title>Author Contributions</title><p>Hyunchul Roh designed mobile mapping sensor system, developed digital map-based SLAM and performed data logging experiment. Jinyong Jeong modeled adaptive IPM and generated lane map using SLAM result. Younggun Cho analyzed initial lane map research and helped logging datasets. Ayoung Kim designed experiment and wrote the paper.</p></notes><notes><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-16-01315"><label>1.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y.</given-names></name><name><surname>Wada</surname><given-names>Y.</given-names></name><name><surname>Hsu</surname><given-names>L.</given-names></name><name><surname>Kamijo</surname><given-names>S.</given-names></name></person-group>
<article-title>Vehicle self-localization in urban canyon using 3D map based GPS positioning and vehicle sensors</article-title>
<source>Proceedings of the IEEEE International Conference on Conference on Connected Vehicle and Expo</source>
<conf-loc>Vienna, Austria</conf-loc>
<conf-date>3 November 2014</conf-date>
<fpage>792</fpage>
<lpage>798</lpage>
</element-citation></ref><ref id="B2-sensors-16-01315"><label>2.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Stiller</surname><given-names>C.</given-names></name><name><surname>Ziegler</surname><given-names>J.</given-names></name></person-group>
<article-title>3D perception and planning for self-driving and cooperative automobiles</article-title>
<source>Proceedings of the IEEE International Multi-Conference on Systems, Signals and Devices</source>
<conf-loc>Chemnitz, Germany</conf-loc>
<conf-date>20 March 2012</conf-date>
<fpage>1</fpage>
<lpage>7</lpage>
</element-citation></ref><ref id="B3-sensors-16-01315"><label>3.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Maddern</surname><given-names>W.</given-names></name><name><surname>Stewart</surname><given-names>A.D.</given-names></name><name><surname>Newman</surname><given-names>P.</given-names></name></person-group>
<article-title>LAPS-II: 6-DoF day and night visual localisation with prior 3D structure for autonomous road vehicles</article-title>
<source>Proceedings of the IEEE Intelligent Vehicle Symposium</source>
<conf-loc>Dearborn, MI, USA</conf-loc>
<conf-date>8 June 2014</conf-date>
<fpage>330</fpage>
<lpage>337</lpage>
</element-citation></ref><ref id="B4-sensors-16-01315"><label>4.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Pylv&#x000e4;n&#x000e4;inen</surname><given-names>T.</given-names></name><name><surname>Berclaz</surname><given-names>J.</given-names></name><name><surname>Korah</surname><given-names>T.</given-names></name><name><surname>Hedau</surname><given-names>V.</given-names></name><name><surname>Aanjaneya</surname><given-names>M.</given-names></name><name><surname>Grzeszczuk</surname><given-names>R.</given-names></name></person-group>
<article-title>3D city modeling from street-level data for augmented reality applications</article-title>
<source>Proceedings of the IEEE International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</source>
<conf-loc>Zurich, Switzerland</conf-loc>
<conf-date>13 October 2012</conf-date>
<fpage>238</fpage>
<lpage>245</lpage>
</element-citation></ref><ref id="B5-sensors-16-01315"><label>5.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Rusu</surname><given-names>R.B.</given-names></name><name><surname>Cousins</surname><given-names>S.</given-names></name></person-group>
<article-title>3D is here: Point cloud library (PCL)</article-title>
<source>Proceedings of the IEEE International Conference on Robotics and Automation</source>
<conf-loc>Shanghai, China</conf-loc>
<conf-date>9 May 2011</conf-date>
<fpage>1</fpage>
<lpage>4</lpage>
</element-citation></ref><ref id="B6-sensors-16-01315"><label>6.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Golovinskiy</surname><given-names>A.</given-names></name><name><surname>Kim</surname><given-names>V.G.</given-names></name><name><surname>Funkhouser</surname><given-names>T.</given-names></name></person-group>
<article-title>Shape-based recognition of 3D point clouds in urban environments</article-title>
<source>Proceedings of the IEEE International Conference on Computer Vision</source>
<conf-loc>Tokyo, Japan</conf-loc>
<conf-date>29 September 2009</conf-date>
<fpage>2154</fpage>
<lpage>2161</lpage>
</element-citation></ref><ref id="B7-sensors-16-01315"><label>7.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Sheehan</surname><given-names>M.</given-names></name><name><surname>Harrison</surname><given-names>A.</given-names></name><name><surname>Newman</surname><given-names>P.</given-names></name></person-group>
<article-title>Continuous vehicle localisation using sparse 3D sensing, kernelised R&#x000e9;nyi distance and fast Gauss transforms</article-title>
<source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</source>
<conf-loc>Tokyo, Japan</conf-loc>
<conf-date>3 November 2013</conf-date>
<fpage>398</fpage>
<lpage>405</lpage>
</element-citation></ref><ref id="B8-sensors-16-01315"><label>8.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Pascoe</surname><given-names>G.</given-names></name><name><surname>Maddern</surname><given-names>W.</given-names></name><name><surname>Newman</surname><given-names>P.</given-names></name></person-group>
<article-title>Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments</article-title>
<source>Proceedings of the IEEE International Conference on Computer Vision</source>
<conf-loc>Santiago, Chile</conf-loc>
<conf-date>11 December 2015</conf-date>
<fpage>9</fpage>
<lpage>16</lpage>
</element-citation></ref><ref id="B9-sensors-16-01315"><label>9.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Candiago</surname><given-names>S.</given-names></name><name><surname>Remondino</surname><given-names>F.</given-names></name><name><surname>De Giglio</surname><given-names>M.</given-names></name><name><surname>Dubbini</surname><given-names>M.</given-names></name><name><surname>Gattelli</surname><given-names>M.</given-names></name></person-group>
<article-title>Evaluating Multispectral Images and Vegetation Indices for Precision Farming Applications from UAV Images</article-title>
<source>Remote Sens.</source>
<year>2015</year>
<volume>7</volume>
<fpage>4026</fpage>
<lpage>4047</lpage>
<pub-id pub-id-type="doi">10.3390/rs70404026</pub-id>
</element-citation></ref><ref id="B10-sensors-16-01315"><label>10.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>M.</given-names></name></person-group>
<article-title>Robotic Online Path Planning on Point Cloud</article-title>
<source>IEEE Syst. Man Cybern. Magn.</source>
<year>2015</year>
<volume>46</volume>
<fpage>1217</fpage>
<lpage>1228</lpage>
<pub-id pub-id-type="doi">10.1109/TCYB.2015.2430526</pub-id>
<pub-id pub-id-type="pmid">26011876</pub-id></element-citation></ref><ref id="B11-sensors-16-01315"><label>11.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sampath</surname><given-names>A.</given-names></name><name><surname>Shan</surname><given-names>J.</given-names></name></person-group>
<article-title>Segmentation and reconstruction of polyhedral building roofs from aerial LiDAR point clouds</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2010</year>
<volume>48</volume>
<fpage>1554</fpage>
<lpage>1567</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2009.2030180</pub-id>
</element-citation></ref><ref id="B12-sensors-16-01315"><label>12.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Galvanin</surname><given-names>E.A.d.S.</given-names></name><name><surname>Poz</surname><given-names>A.P.D.</given-names></name></person-group>
<article-title>Extraction of building roof contours from LiDAR data using a Markov-Random-Field-based approach</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2012</year>
<volume>50</volume>
<fpage>981</fpage>
<lpage>987</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2011.2163823</pub-id>
</element-citation></ref><ref id="B13-sensors-16-01315"><label>13.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>You</surname><given-names>R.J.</given-names></name><name><surname>Lin</surname><given-names>B.C.</given-names></name></person-group>
<article-title>A quality prediction method for building model reconstruction using LiDAR data and topographic maps</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2011</year>
<volume>49</volume>
<fpage>3471</fpage>
<lpage>3480</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2011.2128326</pub-id>
</element-citation></ref><ref id="B14-sensors-16-01315"><label>14.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gonzalez-Aguilera</surname><given-names>D.</given-names></name><name><surname>Crespo-Matellan</surname><given-names>E.</given-names></name><name><surname>Hernandez-Lopez</surname><given-names>D.</given-names></name><name><surname>Rodriguez-Gonzalvez</surname><given-names>P.</given-names></name></person-group>
<article-title>Automated urban analysis based on LiDAR-derived building models</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2013</year>
<volume>51</volume>
<fpage>1844</fpage>
<lpage>1851</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2012.2205931</pub-id>
</element-citation></ref><ref id="B15-sensors-16-01315"><label>15.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Izadi</surname><given-names>M.</given-names></name><name><surname>Saeedi</surname><given-names>P.</given-names></name></person-group>
<article-title>Three-dimensional polygonal building model estimation from single satellite images</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2012</year>
<volume>50</volume>
<fpage>2254</fpage>
<lpage>2272</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2011.2172995</pub-id>
</element-citation></ref><ref id="B16-sensors-16-01315"><label>16.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Koc-San</surname><given-names>D.</given-names></name><name><surname>Turker</surname><given-names>M.</given-names></name></person-group>
<article-title>A model-based approach for automatic building database updating from high-resolution space imagery</article-title>
<source>Int. J. Remote Sens.</source>
<year>2012</year>
<volume>33</volume>
<fpage>4193</fpage>
<lpage>4218</lpage>
<pub-id pub-id-type="doi">10.1080/01431161.2011.640963</pub-id>
</element-citation></ref><ref id="B17-sensors-16-01315"><label>17.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhou</surname><given-names>G.</given-names></name><name><surname>Zhou</surname><given-names>X.</given-names></name></person-group>
<article-title>Seamless fusion of LiDAR and aerial imagery for building extraction</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2014</year>
<volume>52</volume>
<fpage>7393</fpage>
<lpage>7407</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2014.2311991</pub-id>
</element-citation></ref><ref id="B18-sensors-16-01315"><label>18.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Blanco</surname><given-names>J.L.</given-names></name><name><surname>Moreno</surname><given-names>F.A.</given-names></name><name><surname>Gonz&#x000e1;lez-Jim&#x000e9;nez</surname><given-names>J.</given-names></name></person-group>
<article-title>A Collection of Outdoor Robotic Datasets with centimeter-accuracy Ground Truth</article-title>
<source>Autom. Rob.</source>
<year>2009</year>
<volume>27</volume>
<fpage>327</fpage>
<lpage>351</lpage>
<pub-id pub-id-type="doi">10.1007/s10514-009-9138-7</pub-id>
</element-citation></ref><ref id="B19-sensors-16-01315"><label>19.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Blanco</surname><given-names>J.L.</given-names></name><name><surname>Moreno</surname><given-names>F.A.</given-names></name><name><surname>Gonz&#x000e1;lez-Jim&#x000e9;nez</surname><given-names>J.</given-names></name></person-group>
<article-title>The M&#x000e1;laga Urban Dataset: High-rate stereo and LiDARs in a realistic urban scenario</article-title>
<source>Int. J. Rob. Res.</source>
<year>2014</year>
<volume>33</volume>
<fpage>207</fpage>
<lpage>214</lpage>
<pub-id pub-id-type="doi">10.1177/0278364913507326</pub-id>
</element-citation></ref><ref id="B20-sensors-16-01315"><label>20.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Elseberg</surname><given-names>J.</given-names></name><name><surname>Borrmann</surname><given-names>D.</given-names></name><name><surname>N&#x000fc;chter</surname><given-names>A.</given-names></name></person-group>
<article-title>Algorithmic Solutions for Computing Precise Maximum Likelihood 3D Point Clouds from Mobile Laser Scanning Platforms</article-title>
<source>Remote Sens.</source>
<year>2013</year>
<volume>5</volume>
<fpage>5871</fpage>
<lpage>5906</lpage>
<pub-id pub-id-type="doi">10.3390/rs5115871</pub-id>
</element-citation></ref><ref id="B21-sensors-16-01315"><label>21.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Bok</surname><given-names>Y.</given-names></name><name><surname>Choi</surname><given-names>D.G.</given-names></name><name><surname>Jeong</surname><given-names>Y.</given-names></name><name><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group>
<article-title>Capturing city-level scenes with a synchronized camera-laser fusion sensor</article-title>
<source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</source>
<conf-loc>San Francisco, CA, USA</conf-loc>
<conf-date>25 September 2011</conf-date>
<fpage>4436</fpage>
<lpage>4441</lpage>
</element-citation></ref><ref id="B22-sensors-16-01315"><label>22.</label><element-citation publication-type="webpage">
<article-title>Riegl VMX-450</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.riegl.com">http://www.riegl.com</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref><ref id="B23-sensors-16-01315"><label>23.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Tuohy</surname><given-names>S.</given-names></name><name><surname>O&#x02019;Cualain</surname><given-names>D.</given-names></name><name><surname>Jones</surname><given-names>E.</given-names></name><name><surname>Glavin</surname><given-names>M.</given-names></name></person-group>
<article-title>Distance determination for an automobile environment using inverse perspective mapping in OpenCV</article-title>
<source>Proceedings of Irish Signals and Systems Conference</source>
<conf-loc>Cork, Ireland</conf-loc>
<conf-date>23 June 2010</conf-date>
</element-citation></ref><ref id="B24-sensors-16-01315"><label>24.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Laganiere</surname><given-names>R.</given-names></name></person-group>
<article-title>Compositing a bird&#x02019;s eye view mosaic</article-title>
<source>Proceedings of the Vision Interface Conference</source>
<conf-loc>Montreal, Canada</conf-loc>
<conf-date>14 May 2000</conf-date>
<fpage>382</fpage>
<lpage>387</lpage>
</element-citation></ref><ref id="B25-sensors-16-01315"><label>25.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lin</surname><given-names>C.C.</given-names></name><name><surname>Wang</surname><given-names>M.S.</given-names></name></person-group>
<article-title>A vision based top-view transformation model for a vehicle parking assistant</article-title>
<source>Sensors</source>
<year>2012</year>
<volume>12</volume>
<fpage>4431</fpage>
<lpage>4446</lpage>
<pub-id pub-id-type="doi">10.3390/s120404431</pub-id>
<pub-id pub-id-type="pmid">22666038</pub-id></element-citation></ref><ref id="B26-sensors-16-01315"><label>26.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Meguro</surname><given-names>J.i.</given-names></name><name><surname>Kojima</surname><given-names>Y.</given-names></name><name><surname>Naito</surname><given-names>T.</given-names></name></person-group>
<article-title>Automatic lane-level map generation for advanced driver assistance systems using low-cost sensors</article-title>
<source>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</source>
<conf-loc>Hong Kong, China</conf-loc>
<conf-date>31 May 2014</conf-date>
<fpage>3975</fpage>
<lpage>3982</lpage>
</element-citation></ref><ref id="B27-sensors-16-01315"><label>27.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wolcott</surname><given-names>R.W.</given-names></name><name><surname>Eustice</surname><given-names>R.M.</given-names></name></person-group>
<article-title>Visual localization within LiDAR maps for automated urban driving</article-title>
<source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</source>
<conf-loc>Chicago, IL, USA</conf-loc>
<conf-date>14 September 2014</conf-date>
<fpage>176</fpage>
<lpage>183</lpage>
</element-citation></ref><ref id="B28-sensors-16-01315"><label>28.</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Tanner</surname><given-names>M.</given-names></name><name><surname>Pinies</surname><given-names>P.</given-names></name><name><surname>Paz</surname><given-names>L.M.</given-names></name><name><surname>Newman</surname><given-names>P.</given-names></name></person-group>
<article-title>DENSER Cities: A System for Dense Efficient Reconstructions of Cities</article-title>
<year>2016</year>
<pub-id pub-id-type="other">arXiv:1604.03734</pub-id>
</element-citation></ref><ref id="B29-sensors-16-01315"><label>29.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Schindler</surname><given-names>A.</given-names></name><name><surname>Maier</surname><given-names>G.</given-names></name><name><surname>Janda</surname><given-names>F.</given-names></name></person-group>
<article-title>Generation of high precision digital maps using circular arc splines</article-title>
<source>Proceedings of the IEEE Intelligent Vehicle Symposium</source>
<conf-loc>Madrid, Spain</conf-loc>
<conf-date>3 June 2012</conf-date>
<fpage>246</fpage>
<lpage>251</lpage>
</element-citation></ref><ref id="B30-sensors-16-01315"><label>30.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Schindler</surname><given-names>A.</given-names></name></person-group>
<article-title>Vehicle self-localization with high-precision digital maps</article-title>
<source>Proceedings of the IEEE Intelligent Vehicle Symposium</source>
<conf-loc>Gold Coast City, Australia</conf-loc>
<conf-date>23 June 2013</conf-date>
<fpage>141</fpage>
<lpage>146</lpage>
</element-citation></ref><ref id="B31-sensors-16-01315"><label>31.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Floros</surname><given-names>G.</given-names></name><name><surname>van der Zander</surname><given-names>B.</given-names></name><name><surname>Leibe</surname><given-names>B.</given-names></name></person-group>
<article-title>OpenStreetSLAM: Global vehicle localization using OpenStreetMaps</article-title>
<source>Proceedings of the IEEE International conference on Robotics and Automation</source>
<conf-loc>Karlsruhe, German</conf-loc>
<conf-date>6 May 2013</conf-date>
<fpage>1054</fpage>
<lpage>1059</lpage>
</element-citation></ref><ref id="B32-sensors-16-01315"><label>32.</label><element-citation publication-type="webpage">
<article-title>OpenStreetMap</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.openstreetmap.org">http://www.openstreetmap.org</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref><ref id="B33-sensors-16-01315"><label>33.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Pink</surname><given-names>O.</given-names></name><name><surname>Stiller</surname><given-names>C.</given-names></name></person-group>
<article-title>Automated map generation from aerial images for precise vehicle localization</article-title>
<source>Proceedings of the 13th International IEEE Conference on Intelligent Transportation Systems (ITSC)</source>
<conf-loc>Madeira, Portugal</conf-loc>
<conf-date>19 September 2010</conf-date>
<fpage>1517</fpage>
<lpage>1522</lpage>
</element-citation></ref><ref id="B34-sensors-16-01315"><label>34.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Napier</surname><given-names>A.</given-names></name><name><surname>Newman</surname><given-names>P.</given-names></name></person-group>
<article-title>Generation and exploitation of local orthographic imagery for road vehicle localisation</article-title>
<source>Proceedings of the IEEE Intelligent Vehicle Symposium</source>
<conf-loc>Madrid, Spain</conf-loc>
<conf-date>3 June 2012</conf-date>
<fpage>590</fpage>
<lpage>596</lpage>
</element-citation></ref><ref id="B35-sensors-16-01315"><label>35.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Schreiber</surname><given-names>M.</given-names></name><name><surname>Knoppel</surname><given-names>C.</given-names></name><name><surname>Franke</surname><given-names>U.</given-names></name></person-group>
<article-title>Laneloc: Lane marking based localization using highly accurate maps</article-title>
<source>Proceedings of the IEEE Intelligent Vehicle Symposium</source>
<conf-loc>Gold Coast City, Australia</conf-loc>
<conf-date>23 June 2013</conf-date>
<fpage>449</fpage>
<lpage>454</lpage>
</element-citation></ref><ref id="B36-sensors-16-01315"><label>36.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Rose</surname><given-names>C.</given-names></name><name><surname>Britt</surname><given-names>J.</given-names></name><name><surname>Allen</surname><given-names>J.</given-names></name><name><surname>Bevly</surname><given-names>D.</given-names></name></person-group>
<article-title>An integrated vehicle navigation system utilizing lane-detection and lateral position estimation systems in difficult environments for GPS</article-title>
<source>IEEE Trans. Intell. Trans. Syst.</source>
<year>2014</year>
<volume>15</volume>
<fpage>2615</fpage>
<lpage>2629</lpage>
<pub-id pub-id-type="doi">10.1109/TITS.2014.2321108</pub-id>
</element-citation></ref><ref id="B37-sensors-16-01315"><label>37.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Larnaout</surname><given-names>D.</given-names></name><name><surname>Gay-Bellile</surname><given-names>V.</given-names></name><name><surname>Bourgeois</surname><given-names>S.</given-names></name><name><surname>Dhome</surname><given-names>M.</given-names></name></person-group>
<article-title>Fast and automatic city-scale environment modelling using hard and/or weak constrained bundle adjustments</article-title>
<source>Mach. Vision App.</source>
<year>2016</year>
<volume>27</volume>
<fpage>943</fpage>
<lpage>962</lpage>
<pub-id pub-id-type="doi">10.1007/s00138-016-0766-6</pub-id>
</element-citation></ref><ref id="B38-sensors-16-01315"><label>38.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Roh</surname><given-names>H.C.</given-names></name><name><surname>Oh</surname><given-names>T.J.</given-names></name><name><surname>Choe</surname><given-names>Y.</given-names></name><name><surname>Chung</surname><given-names>M.J.</given-names></name></person-group>
<article-title>Satellite map based quantitative analysis for 3D world modeling of urban environment</article-title>
<source>Proceedings of the 9th Asian IEEE Control Conference (ASCC)</source>
<conf-loc>Istanbul, Turkey</conf-loc>
<conf-date>23 June 2013</conf-date>
<fpage>1</fpage>
<lpage>6</lpage>
</element-citation></ref><ref id="B39-sensors-16-01315"><label>39.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Borenstein</surname><given-names>J.</given-names></name><name><surname>Feng</surname><given-names>L.</given-names></name></person-group>
<article-title>Measurement and correction of systematic odometry errors in mobile robots</article-title>
<source>IEEE Trans. Robot. Automat.</source>
<year>1996</year>
<volume>12</volume>
<fpage>869</fpage>
<lpage>880</lpage>
<pub-id pub-id-type="doi">10.1109/70.544770</pub-id>
</element-citation></ref><ref id="B40-sensors-16-01315"><label>40.</label><element-citation publication-type="webpage">
<person-group person-group-type="author"><name><surname>Kaess</surname><given-names>M.</given-names></name><name><surname>Johannsson</surname><given-names>H.</given-names></name><name><surname>Leonard</surname><given-names>J.</given-names></name></person-group>
<article-title>Open Source Implementation of iSAM</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://people.csail.mit.edu/kaess/isam">http://people.csail.mit.edu/kaess/isam</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref><ref id="B41-sensors-16-01315"><label>41.</label><element-citation publication-type="webpage">
<article-title>Withrobot altimeter mypressure</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.withrobot.com">http://www.withrobot.com</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref><ref id="B42-sensors-16-01315"><label>42.</label><element-citation publication-type="webpage">
<article-title>IMU XsensMTI</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.xsens.com">http://www.xsens.com</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref><ref id="B43-sensors-16-01315"><label>43.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>S&#x000fc;nderhauf</surname><given-names>N.</given-names></name><name><surname>Protzel</surname><given-names>P.</given-names></name></person-group>
<article-title>Switchable constraints for robust pose graph SLAM</article-title>
<source>Proceedings of the IEEE/RSJ Internaltional Conference on lIntelligent Robots and Systems</source>
<conf-loc>Vilamoura-Algarve, Portugal</conf-loc>
<conf-date>7 October 2012</conf-date>
<fpage>1879</fpage>
<lpage>1884</lpage>
</element-citation></ref><ref id="B44-sensors-16-01315"><label>44.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Agarwal</surname><given-names>P.</given-names></name><name><surname>Tipaldi</surname><given-names>G.D.</given-names></name><name><surname>Spinello</surname><given-names>L.</given-names></name><name><surname>Stachniss</surname><given-names>C.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name></person-group>
<article-title>Robust map optimization using dynamic covariance scaling</article-title>
<source>Proceedings of the IEEE International Conference on Robotics and Automation</source>
<conf-loc>Karlsruhe, German</conf-loc>
<conf-date>6 May 2013</conf-date>
<fpage>62</fpage>
<lpage>69</lpage>
</element-citation></ref><ref id="B45-sensors-16-01315"><label>45.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sportouche</surname><given-names>H.</given-names></name><name><surname>Tupin</surname><given-names>F.</given-names></name><name><surname>Denise</surname><given-names>L.</given-names></name></person-group>
<article-title>Extraction and three-dimensional reconstruction of isolated buildings in urban scenes from high-resolution optical and SAR spaceborne images</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2011</year>
<volume>49</volume>
<fpage>3932</fpage>
<lpage>3946</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2011.2132727</pub-id>
</element-citation></ref><ref id="B46-sensors-16-01315"><label>46.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Jia</surname><given-names>X.</given-names></name><name><surname>Benediktsson</surname><given-names>J.A.</given-names></name></person-group>
<article-title>A novel MKL model of integrating LIDAR data and MSI for urban area classification</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2015</year>
<volume>53</volume>
<fpage>5312</fpage>
<lpage>5326</lpage>
</element-citation></ref><ref id="B47-sensors-16-01315"><label>47.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X.X.</given-names></name><name><surname>Montazeri</surname><given-names>S.</given-names></name><name><surname>Gisinger</surname><given-names>C.</given-names></name><name><surname>Hanssen</surname><given-names>R.F.</given-names></name><name><surname>Bamler</surname><given-names>R.</given-names></name></person-group>
<article-title>Geodetic SAR tomography</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2016</year>
<volume>54</volume>
<fpage>18</fpage>
<lpage>35</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2015.2448686</pub-id>
</element-citation></ref><ref id="B48-sensors-16-01315"><label>48.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Brell</surname><given-names>M.</given-names></name><name><surname>Rogass</surname><given-names>C.</given-names></name><name><surname>Segl</surname><given-names>K.</given-names></name><name><surname>Bookhagen</surname><given-names>B.</given-names></name><name><surname>Guanter</surname><given-names>L.</given-names></name></person-group>
<article-title>Improving Sensor Fusion: A Parametric Method for the Geometric Coalignment of Airborne Hyperspectral and Lidar Data</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2016</year>
<volume>54</volume>
<fpage>3460</fpage>
<lpage>3474</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2016.2518930</pub-id>
</element-citation></ref><ref id="B49-sensors-16-01315"><label>49.</label><element-citation publication-type="webpage">
<article-title>Environmental Systems Research Institute</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.esri.com/">http://www.esri.com/</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref><ref id="B50-sensors-16-01315"><label>50.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ozog</surname><given-names>P.</given-names></name><name><surname>Carlevaris-Bianco</surname><given-names>N.</given-names></name><name><surname>Kim</surname><given-names>A.</given-names></name><name><surname>Eustice</surname><given-names>R.M.</given-names></name></person-group>
<article-title>Long-term Mapping Techniques for Ship Hull Inspection and Surveillance using an Autonomous Underwater Vehicle</article-title>
<source>J. Field Robot.</source>
<year>2015</year>
<volume>33</volume>
<fpage>265</fpage>
<lpage>289</lpage>
<pub-id pub-id-type="doi">10.1002/rob.21582</pub-id>
</element-citation></ref><ref id="B51-sensors-16-01315"><label>51.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mallot</surname><given-names>H.A.</given-names></name><name><surname>B&#x000fc;lthoff</surname><given-names>H.H.</given-names></name><name><surname>Little</surname><given-names>J.</given-names></name><name><surname>Bohrer</surname><given-names>S.</given-names></name></person-group>
<article-title>Inverse perspective mapping simplifies optical flow computation and obstacle detection</article-title>
<source>Biol. Cybern.</source>
<year>1991</year>
<volume>64</volume>
<fpage>177</fpage>
<lpage>185</lpage>
<pub-id pub-id-type="doi">10.1007/BF00201978</pub-id>
<pub-id pub-id-type="pmid">2004128</pub-id></element-citation></ref><ref id="B52-sensors-16-01315"><label>52.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Jeong</surname><given-names>J.</given-names></name><name><surname>Kim</surname><given-names>A.</given-names></name></person-group>
<article-title>Adaptive Inverse Perspective Mapping for Lane Map Generation with SLAM</article-title>
<source>Proceedings of the International Conference on Ubiquitous Robots and Ambient Intell</source>
<conf-loc>Xi&#x02019;an, China</conf-loc>
<conf-date>9 August 2016</conf-date>
</element-citation></ref><ref id="B53-sensors-16-01315"><label>53.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Carrillo</surname><given-names>H.</given-names></name><name><surname>Reid</surname><given-names>I.</given-names></name><name><surname>Castellanos</surname><given-names>J.A.</given-names></name></person-group>
<article-title>On the comparison of uncertainty criteria for active SLAM</article-title>
<source>Proceeding of IEEE International Conference on Robotics and Automation</source>
<conf-loc>Saint Paul, MN, USA</conf-loc>
<conf-date>14 May 2012</conf-date>
<fpage>2080</fpage>
<lpage>2087</lpage>
</element-citation></ref><ref id="B54-sensors-16-01315"><label>54.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>J.</given-names></name></person-group>
<article-title>General Equivalence Theory for Optimum Designs (Approximate Theory)</article-title>
<source>Ann. Stat.</source>
<year>1974</year>
<volume>2</volume>
<fpage>849</fpage>
<lpage>879</lpage>
<pub-id pub-id-type="doi">10.1214/aos/1176342810</pub-id>
</element-citation></ref><ref id="B55-sensors-16-01315"><label>55.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kim</surname><given-names>A.</given-names></name><name><surname>Eustice</surname><given-names>R.M.</given-names></name></person-group>
<article-title>Active visual SLAM for robotic area coverage: Theory and experiment</article-title>
<source>Int. J. Robot. Res.</source>
<year>2015</year>
<volume>34</volume>
<fpage>457</fpage>
<lpage>475</lpage>
<pub-id pub-id-type="doi">10.1177/0278364914547893</pub-id>
</element-citation></ref><ref id="B56-sensors-16-01315"><label>56.</label><element-citation publication-type="webpage">
<article-title>Topcon GRX2</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.topcon.co.jp/">http://www.topcon.co.jp/</ext-link></comment>
<date-in-citation>(accessed on 17 August 2016)</date-in-citation>
</element-citation></ref></ref-list></back><floats-group><fig id="sensors-16-01315-f001" position="float"><label>Figure 1</label><caption><p>Sensor system description. Our sensor suite includes two 2D LiDARs, four cameras, a GPS, two wheel encoders, an IMU, and an altimeter. Note that the front LiDAR in (<bold>b</bold>) is only for moving object detection, and thus excluded in Simultaneous Localization and Mapping (SLAM) framework.</p></caption><graphic xlink:href="sensors-16-01315-g001"/></fig><fig id="sensors-16-01315-f002" position="float"><label>Figure 2</label><caption><p>Software design diagram. The 3D point cloud map generation involves four main steps.</p></caption><graphic xlink:href="sensors-16-01315-g002"/></fig><fig id="sensors-16-01315-f003" position="float"><label>Figure 3</label><caption><p>Depiction of the pose-graph SLAM factors. Odometry constraints (odo) are sequential whereas wall loop constraints (wall) can be non-sequential. Absolute constraints (abs) includes digital map correction and altimeter. GPS factors may occur irregularly. When wall is created, we also provide a digital map correction factor.</p></caption><graphic xlink:href="sensors-16-01315-g003"/></fig><fig id="sensors-16-01315-f004" position="float"><label>Figure 4</label><caption><p>Shape file representations for building and contour. An example of polygon data type structure is in (<bold>a</bold>), and an example of contour map is in (<bold>b</bold>). (<bold>c</bold>) is sample shape-map (building) in the campus data. (<bold>d</bold>) is sample shape-map (DEM) in the campus data.</p></caption><graphic xlink:href="sensors-16-01315-g004"/></fig><fig id="sensors-16-01315-f005" position="float"><label>Figure 5</label><caption><p>(<bold>a</bold>) Illustration of point cloud accumulation from vertically installed side scanning LiDAR. We only collect a single scan line per vehicle pose, and accumulate lines into a local point cloud set via motion. (<bold>b</bold>) An example of wall represented by digital map and point cloud map (white points in green box). For wall segmentation, we use 5-point Random Sample Consensus (RANSAC)-based plane fitting.</p></caption><graphic xlink:href="sensors-16-01315-g005"/></fig><fig id="sensors-16-01315-f006" position="float"><label>Figure 6</label><caption><p>Block diagram for wall segmentation. The overall wall extraction algorithm can be divided into two parts. The first part is to obtain the initial normal, the second is a part of the segmentation wall to error distance test of the new incoming scan line by using initial normal information.</p></caption><graphic xlink:href="sensors-16-01315-g006"/></fig><fig id="sensors-16-01315-f007" position="float"><label>Figure 7</label><caption><p>Result of digital map-based SLAM (<bold>a</bold>) Magenta line represents the correspondence of the digital map and building wall (green) extracted by point cloud. (<bold>b</bold>) Loop closure of trajectory generated using wall to wall matching showed by cyan line. (<bold>c</bold>) Correcting in the Z-axis direction, we use matching of Digital Elevation Model (DEM) and SLAM node. the yellow line represents this correspondence. (<bold>d</bold>) We can confirm the z-directional trajectory over the change of color.</p></caption><graphic xlink:href="sensors-16-01315-g007a"/><graphic xlink:href="sensors-16-01315-g007b"/></fig><fig id="sensors-16-01315-f008" position="float"><label>Figure 8</label><caption><p>Coordinate the relationship between the camera, image and world. The unit of (<inline-formula><mml:math id="mm58"><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="mm59"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>) is the pixel measurement, and that of (<inline-formula><mml:math id="mm60"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="mm61"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm62"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) is the meter measurement. Reprinted, with permission, from Jeong et al. URAI 2016; &#x000a9;2016 IEEE.</p></caption><graphic xlink:href="sensors-16-01315-g008"/></fig><fig id="sensors-16-01315-f009" position="float"><label>Figure 9</label><caption><p>Side and top view of IPM model. In the illustration, <inline-formula><mml:math id="mm92"><mml:msub><mml:mi>f</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="mm93"><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> are the focal length of a camera; <inline-formula><mml:math id="mm94"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is the pitch angle; <inline-formula><mml:math id="mm95"><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="mm96"><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> are the half angle of the vertical and horizontal field of view (FOV) respectively. Reprinted, with permission, from Jeong et al. URAI 2016; &#x000a9;2016 IEEE.</p></caption><graphic xlink:href="sensors-16-01315-g009"/></fig><fig id="sensors-16-01315-f010" position="float"><label>Figure 10</label><caption><p>Result of self-consistent lane map process. Left (<bold>a</bold>,<bold>c</bold>) and right (<bold>b</bold>,<bold>d</bold>) images show the result of lane map.</p></caption><graphic xlink:href="sensors-16-01315-g010"/></fig><fig id="sensors-16-01315-f011" position="float"><label>Figure 11</label><caption><p>Compensation process of interpolated pose between DR computed node and iSAM node.</p></caption><graphic xlink:href="sensors-16-01315-g011"/></fig><fig id="sensors-16-01315-f012" position="float"><label>Figure 12</label><caption><p>3D mapping result and data logging path. (<bold>a</bold>) 3D map result represented by pseudo-colored point cloud in digital map background. Nodes are color-coded by height varying from red (low) to purple (high). (<bold>b</bold>) Illustration of data logging path performed in the experiments. Connection of the trajectory represented by color shift from red (start) to purple (end). There are three circular paths covering the eastern, the western, and the northern part of the campus.</p></caption><graphic xlink:href="sensors-16-01315-g012"/></fig><fig id="sensors-16-01315-f013" position="float"><label>Figure 13</label><caption><p>Proposed digital map-based SLAM results. (<bold>a</bold>) Uncertainty versus path length . Whereas the uncertainty of DR navigation (blue, dot) shows unbounded growth, the uncertainty of SLAM (red solid) is bounded from wall-to-wall loop closures. The graph at the top of the pose uncertainty with log-scale of y axis (unit: m&#x000b7;rad. The following [<xref rid="B53-sensors-16-01315" ref-type="bibr">53</xref>,<xref rid="B54-sensors-16-01315" ref-type="bibr">54</xref>,<xref rid="B55-sensors-16-01315" ref-type="bibr">55</xref>], we use m&#x000b7;rad to show pose uncertainty), the bottom graph shows the position uncertainty (unit: m). (<bold>b</bold>) Top-down view of the SLAM estimate (red line) versus dead-reckoning trajectory (gray line). (<bold>c</bold>) The xy component of the SLAM trajectory estimate is plotted versus time, where the vertical axis represents mission time. Green lines show wall-to-wall matching for loop closure.</p></caption><graphic xlink:href="sensors-16-01315-g013"/></fig><fig id="sensors-16-01315-f014" position="float"><label>Figure 14</label><caption><p>Sensor availability graph. The nodes are colored by the sensor type associated with the node. Blue dots are the GPS nodes and are in a smaller size for clear visualization for digital map nodes (green and red). Cyan nodes are loop-closure nodes via LiDAR comparison.</p></caption><graphic xlink:href="sensors-16-01315-g014"/></fig><fig id="sensors-16-01315-f015" position="float"><label>Figure 15</label><caption><p>3D mapping result for sample buildings. Accumulated and refined point cloud using SLAM trajectory is given. (<bold>a</bold>) and (<bold>c</bold>) are the 3D mapping of two samples. Points are colored by the height, and green squares indicate the classified building walls. (<bold>b</bold>) and (<bold>d</bold>) are the aerial view of each sample building.</p></caption><graphic xlink:href="sensors-16-01315-g015"/></fig><fig id="sensors-16-01315-f016" position="float"><label>Figure 16</label><caption><p>Generated lane map. (<bold>a</bold>) and (<bold>b</bold>) is a narrow straight section without and with parked car respectively. (<bold>c</bold>) is intersection and (<bold>d</bold>) includes speed bump and crosswalk.</p></caption><graphic xlink:href="sensors-16-01315-g016"/></fig><fig id="sensors-16-01315-f017" position="float"><label>Figure 17</label><caption><p>Accuracy analysis on sample points using VRS-GPS. (<bold>a</bold>) Four corners of the building rooftop are measured with building walls compensation. (<bold>b</bold>) Sample points on road marks are measured.</p></caption><graphic xlink:href="sensors-16-01315-g017"/></fig><fig id="sensors-16-01315-f018" position="float"><label>Figure 18</label><caption><p>(<bold>a</bold>) Red square drawn from four sample ground truth corners. (<bold>b</bold>) By measuring perpendicular distance (purple line) from 3D point cloud to ground truth (red line) error is measured. (<bold>c</bold>) shows the topview for a clear illustration.</p></caption><graphic xlink:href="sensors-16-01315-g018"/></fig><fig id="sensors-16-01315-f019" position="float"><label>Figure 19</label><caption><p>Six sample points having accurate global position are measured by RTK-GPS, and they are plotted on (<bold>a</bold>) street view, (<bold>b</bold>) lane map, (<bold>c</bold>) aerial map and (<bold>d</bold>) digital map. (<bold>c</bold>) Substantial position error with ground truth in aerial map, (<bold>d</bold>) Small position error with ground truth occurs, but lacking in detail such as road mark and cross walk. (<bold>b</bold>) Proposed method has position accuracy with global ground truth and detail information.</p></caption><graphic xlink:href="sensors-16-01315-g019"/></fig><table-wrap id="sensors-16-01315-t001" position="float"><object-id pub-id-type="pii">sensors-16-01315-t001_Table 1</object-id><label>Table 1</label><caption><p>Specifications of Urban Mapping System (UMS).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specification</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dimensions</td><td align="center" valign="middle" rowspan="1" colspan="1">1.67 m &#x000d7; 1.36 m &#x000d7; 0.31 m (L &#x000d7; W &#x000d7; H )</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dry weight</td><td align="center" valign="middle" rowspan="1" colspan="1">35.8 kg</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LiDAR</td><td align="center" valign="middle" rowspan="1" colspan="1">SICK LMS291,200 (35 Hz)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Imaging sensor</td><td align="center" valign="middle" rowspan="1" colspan="1">Point Grey Flea3, 1380 &#x000d7; 1024 pixel, 12-bit CCD (30 Hz)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPS</td><td align="center" valign="middle" rowspan="1" colspan="1">HUACE B20 (1 Hz)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IMU sensor</td><td align="center" valign="middle" rowspan="1" colspan="1">Xsens MTi (100 Hz)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Altimeter</td><td align="center" valign="middle" rowspan="1" colspan="1">WITHROBOT myPressure (1 Hz)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wheel encoder</td><td align="center" valign="middle" rowspan="1" colspan="1">Autonics E68S, rotary encoder type (100 Hz)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Processor</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel(R) Core(TM) i7-3790 CPU@3.4 GHz</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Battery</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Delkor 80 Ah, 12 V , lead&#x02013;acid type</td></tr></tbody></table></table-wrap><table-wrap id="sensors-16-01315-t002" position="float"><object-id pub-id-type="pii">sensors-16-01315-t002_Table 2</object-id><label>Table 2</label><caption><p>Description on polygon record contents. The fields for a polygon type are box, numParts, numPoints, parts, and points. Box means the bounding box for the polygon stored in the order Xmin, Ymin, Xmax, Ymax. The number of closed curves in the polygon is described by NumParts. NumPoints is the total number of points for all closed curves. parts means an array of length numParts. For each closed curve, the index of its first point stored in the points array. Points are array of length numPoints. The points for each closed curve in the polygon are stored end to end.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Position</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Field</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Byte Order</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Byte 0</td><td align="center" valign="middle" rowspan="1" colspan="1">Shape Type</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">Integer</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Little</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Byte 4</td><td align="center" valign="middle" rowspan="1" colspan="1">Box</td><td align="center" valign="middle" rowspan="1" colspan="1">Box</td><td align="center" valign="middle" rowspan="1" colspan="1">Double</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">Little</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Byte 36</td><td align="center" valign="middle" rowspan="1" colspan="1">NumParts</td><td align="center" valign="middle" rowspan="1" colspan="1">NumParts</td><td align="center" valign="middle" rowspan="1" colspan="1">Integer</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Little</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Byte 40</td><td align="center" valign="middle" rowspan="1" colspan="1">NumPoints</td><td align="center" valign="middle" rowspan="1" colspan="1">NumPoints</td><td align="center" valign="middle" rowspan="1" colspan="1">Integer</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Little</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Byte 44</td><td align="center" valign="middle" rowspan="1" colspan="1">Parts</td><td align="center" valign="middle" rowspan="1" colspan="1">Parts</td><td align="center" valign="middle" rowspan="1" colspan="1">Integer</td><td align="center" valign="middle" rowspan="1" colspan="1">NumParts</td><td align="center" valign="middle" rowspan="1" colspan="1">Little</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Byte X&#x02020;</td><td align="center" valign="middle" rowspan="1" colspan="1">Points</td><td align="center" valign="middle" rowspan="1" colspan="1">Points</td><td align="center" valign="middle" rowspan="1" colspan="1">Point</td><td align="center" valign="middle" rowspan="1" colspan="1">NumPoints</td><td align="center" valign="middle" rowspan="1" colspan="1">Little</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02020;X = 44 + 4 &#x000d7; NumParts</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-16-01315-t003" position="float"><object-id pub-id-type="pii">sensors-16-01315-t003_Table 3</object-id><label>Table 3</label><caption><p>Discrepancy prior to the compensation (per unit meter).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Min</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Max</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">x (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm105"><mml:mrow><mml:mn>7</mml:mn><mml:mo>.</mml:mo><mml:mn>98</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm106"><mml:mrow><mml:mn>9</mml:mn><mml:mo>.</mml:mo><mml:mn>37</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm107"><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>99</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">y (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm108"><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>64</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm109"><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>49</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm110"><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>13</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">yaw (<inline-formula><mml:math id="mm111"><mml:msup><mml:mrow/><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm112"><mml:mrow><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>08</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm113"><mml:mrow><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>47</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm114"><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>58</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-16-01315-t004" position="float"><object-id pub-id-type="pii">sensors-16-01315-t004_Table 4</object-id><label>Table 4</label><caption><p>Summary of Digital-Based SLAM results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Path Length</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Logging Time</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Computation Time</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GPS Node</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Wall Node</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Digital Map Node</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total Nodes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">No. of Point</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9322.35m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1952.06s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">792.04s (40.57%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">482 (41.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">249 (20.09%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">136 (11.67%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1165</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23,017,120</td></tr></tbody></table></table-wrap><table-wrap id="sensors-16-01315-t005" position="float"><object-id pub-id-type="pii">sensors-16-01315-t005_Table 5</object-id><label>Table 5</label><caption><p>Positional error measurement between the 3D building point cloud and RTK-GPS measured building corners. The map error is computed from average of RMSE between the ground truth wall generated from RTK measured sample points and LiDAR 3D points in the mapped wall. Note that this error is over 9.32 km of travel distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" colspan="1">Sample</th><th rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" colspan="1">No. of 3D Map Points</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Average RMSE</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPS Only [m]</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Digital Map-Based [m]</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Set 1</td><td align="center" valign="middle" rowspan="1" colspan="1">3352</td><td align="center" valign="middle" rowspan="1" colspan="1">0.437</td><td align="center" valign="middle" rowspan="1" colspan="1">0.190</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Set 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1967</td><td align="center" valign="middle" rowspan="1" colspan="1">1.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.193</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Set 3</td><td align="center" valign="middle" rowspan="1" colspan="1">227</td><td align="center" valign="middle" rowspan="1" colspan="1">2.070</td><td align="center" valign="middle" rowspan="1" colspan="1">0.347</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set 4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3314</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.407</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.136</td></tr></tbody></table></table-wrap><table-wrap id="sensors-16-01315-t006" position="float"><object-id pub-id-type="pii">sensors-16-01315-t006_Table 6</object-id><label>Table 6</label><caption><p>Error analysis for the proposed method on road marking. Each dataset has two to six sample points respectively. For set 2, no road marking in the aerial map and excluded in the comparison. The proposed method&#x02019;s error is also written as a ratio to the aerial image for clear comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" colspan="1">Sample</th><th rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" colspan="1">No. of Lane Points</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Average RMSE</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Aerial Image [m]</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method [m]</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">set 1</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">7.180</td><td align="center" valign="middle" rowspan="1" colspan="1">1.000 (13.927%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">set 2</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">1.055 ( - %)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">set 3</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">8.948</td><td align="center" valign="middle" rowspan="1" colspan="1">1.699 (18.989%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">set 4</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">8.044</td><td align="center" valign="middle" rowspan="1" colspan="1">0.622 (7.727%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">set 5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.261</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.610 (5.418%)</td></tr></tbody></table></table-wrap></floats-group></article>