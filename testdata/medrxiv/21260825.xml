<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.07.22.21260825</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Dentistry and Oral Medicine</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Fully automatic segmentation of craniomaxillofacial CT scans for computer-assisted orthognathic surgery planning using the nnU-Net framework</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2014-2623</contrib-id>
<name><surname>Dot</surname><given-names>Gauthier</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Schouman</surname><given-names>Thomas</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Dubois</surname><given-names>Guillaume</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Rouch</surname><given-names>Philippe</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2775-0106</contrib-id>
<name><surname>Gajny</surname><given-names>Laurent</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institut de Biomecanique Humaine Georges Charpak, Arts et Metiers Paristech</institution>, Paris, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>Universite de Paris, AP-HP, Hopital Pitie-Salpetriere, Service d&#x2019;Odontologie</institution>, Paris, <country>France</country></aff>
<aff id="a3"><label>3</label><institution>Sorbonne Universite, AP-HP, Hopital Pitie-Salpetriere, Service de Chirurgie Maxillo-Faciale</institution>, Paris, <country>France</country></aff>
<aff id="a4"><label>4</label><institution>Materialise</institution>, Malakoff, <country>France</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author: <email>gauthier.dot@ensam.eu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.07.22.21260825</elocation-id>
<history>
<date date-type="received">
<day>22</day>
<month>7</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>22</day>
<month>7</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>7</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="21260825.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<sec>
<title>Objectives</title>
<p>To evaluate the performance of the nnU-Net open-source deep learning framework for automatic multi-task segmentation of craniomaxillofacial (CMF) structures in CT scans obtained for computer-assisted orthognathic surgery.</p></sec>
<sec>
<title>Methods</title>
<p>Four hundred and fifty-three consecutive patients having undergone high-definition CT scans before orthognathic surgery were randomly distributed among a training/validation cohort (<italic>n</italic> = 300) and a testing cohort (<italic>n</italic> = 153). The ground truth segmentations were generated by 2 operators following an industry-certified procedure for use in computer-assisted surgical planning and personalized implant manufacturing. Model performance was assessed by comparing model predictions with ground truth segmentations. Examination of 45 CT scans by an industry expert provided additional evaluation. The model&#x2019;s generalizability was tested on a publicly available dataset of 10 CT scans with ground truth segmentations of the mandible.</p></sec>
<sec>
<title>Results</title>
<p>In the test cohort, mean volumetric Dice Similarity Coefficient (vDSC) &#x0026; surface Dice Similarity Coefficient at 1mm (sDSC) were 0.96 &#x0026; 0.97 for the upper skull, 0.94 &#x0026; 0.98 for the mandible, 0.95 &#x0026; 0.99 for the upper teeth, 0.94 &#x0026; 0.99 for the lower teeth and 0.82 &#x0026; 0.98 for the mandibular canal. Industry expert segmentation approval rates were 93&#x0025; for the mandible, 89&#x0025; for the mandibular canal, 82&#x0025; for the upper skull, 69&#x0025; for the upper teeth and 58&#x0025; for the lower teeth.</p></sec>
<sec>
<title>Conclusion</title>
<p>While additional efforts are required for the segmentation of dental apices, our results demonstrated the model&#x2019;s reliability in terms of fully automatic segmentation of preoperative orthognathic CT scans.</p></sec>
<sec>
<title>Key points</title>
<list list-type="simple">
<list-item><label>&#x2013;</label><p>The nnU-Net deep learning framework can be used out-of-the-box to provide robust fully automatic multi-task segmentation of CT scans performed for computer-assisted orthognathic surgery planning.</p></list-item>
<list-item><label>&#x2013;</label><p>Commonly used biomedical segmentation evaluation metrics (volumetric and surface Dice Similarity Coefficient) do not always match industry expert evaluation in the case of more demanding clinical applications.</p></list-item></list>
</sec>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Deep Learning</kwd>
<kwd>Orthognathic Surgery</kwd>
<kwd>Surgery</kwd>
<kwd>Computer-Assisted</kwd>
<kwd>Tomography</kwd>
<kwd>X-ray computed</kwd>
</kwd-group>
<counts>
<page-count count="16"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>All authors have completed the ICMJE uniform disclosure form and declare: no support from any organisation for the submitted work; T. Schouman is a consultant for Materialise (Malakoff, France), G. Dubois is the CEO of Materialise (Malakoff, France); no other relationships or activities that could appear to have influenced the submitted work.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>This study has received funding by the "Fondation des Gueules Cassees" (grant number 28-2020).</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>The IRB Comite d'Ethique pour la Recherche en Imagerie Medicale (CERIM) gave ethical approval for this research (number CRM-2001-051).</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Orthognathic surgery addresses congenital and acquired conditions of the facial skeleton by repositioning the jaws into a functional relationship in subjects presenting dentofacial deformities. It has been reported that up to 5&#x0025; of the USA and UK populations could require orthognathic surgery [<xref ref-type="bibr" rid="c1">1</xref>]. Surgical correction of craniomaxillofacial (CMF) deformities requires defining a specific surgical treatment plan for every single patient [<xref ref-type="bibr" rid="c2">2</xref>]. In recent years, several teams have shown the reliability of computerized methods to analyze dentofacial deformities or to elaborate surgical treatment plans [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. Virtual planning is usually performed using a CT scan or cone-beam CT (CBCT) scan of the patient&#x2019;s head [<xref ref-type="bibr" rid="c3">3</xref>]. The first step in the planning pipeline involves the extraction of structures of interest from the CT data by semantic segmentation. The choice of (CB)CT scan resolution and anatomical structures to be segmented depend on what the planning is intended for. For instance, high-definition CT scans can be used to perform the following segmentations for orthognathic surgery planning and personalized implant manufacturing: upper skull and mandible, including the mandibular canals, upper and lower teeth (crowns and roots). Proper segmentation and delineation of these structures is known to be challenging due to factors such as large interindividual morphological variations, tight connections between the structures, lack of contrast in joints and teeth apices, frequent presence of artifacts (orthodontic materials, fixation implants, dental fillings or crowns) [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>].</p>
<p>Semi-automatic algorithms may be used to make segmentation less time-consuming [<xref ref-type="bibr" rid="c6">6</xref>]. Some of these methods provide high segmentation accuracy, yet are not fully automatic and therefore still require time-consuming manipulations by trained operators. In recent years, segmentations of medical images using deep learning algorithms have outperformed previously used algorithms. Deep learning algorithms might indeed be able to perform fully automatic segmentations [<xref ref-type="bibr" rid="c7">7</xref>]. Several authors have developed specific deep learning-based models for automatic segmentation of the upper skull, mandibular bone, teeth or mandibular canal [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c8">8</xref>&#x2013;<xref ref-type="bibr" rid="c18">18</xref>]. Most of these approaches relied on a U-Net convolutional architecture [<xref ref-type="bibr" rid="c19">19</xref>], and yielded promising results, with reported volumetric Dice Similarity Coefficient (vDSC) between 90&#x0025; and 95&#x0025;. However, some limitations restrict their clinical applicability. None of them specified whether the dataset used for training and testing the algorithm included routine clinical cases, nor how the scans were selected. When the evaluation of the model on a hold-out test dataset was provided, the number of test scans was always less than 30. Moreover, none of the algorithms in the studies were used to segment all the structures of interest for computer-assisted planning of orthognathic surgery and personalized implant manufacturing, and only one work segmented bone and teeth separately [<xref ref-type="bibr" rid="c18">18</xref>]. This calls into question the reproducibility and generalizability of previously published results, which are crucial factors for clinical validity [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c20">20</xref>].</p>
<p>Recently, the nnU-Net framework was proposed as an out-of-the-box tool which automatically configures itself in order to perform deep learning-based biomedical image segmentation [<xref ref-type="bibr" rid="c21">21</xref>]. This tool is publicly available and was shown to surpass most existing models on 23 public datasets used in international biomedical segmentation competitions. nnU-Net could be helpful for direct clinical applications, as it is open source and does not necessarily require expert knowledge to obtain competitive results. To our knowledge, the performance of this tool has not yet been evaluated for the segmentation of craniofacial hard tissue in a clinical context.</p>
<p>In this work, our main objective was to evaluate the performance of the nnU-Net framework for automatic segmentation of CMF structures in routine CT scans performed for computer-assisted orthognathic surgery. We followed best practice in the quantitative evaluation of our results and provided industry expert evaluation for 45 subjects from our test dataset.</p>
</sec>
<sec id="s2">
<title>Materials and methods</title>
<sec id="s2a">
<title>Patient selection</title>
<p>Data were selected from a retrospective cohort of all consecutive patients having undergone orthognathic surgery in a single maxillofacial surgery department between January 2017 and December 2019. Patients referred to this center presented a wide variety of dentofacial deformities, came from various socioeconomic backgrounds and were ethnically diverse. Patients were considered for inclusion whatever dental deformity they presented. Exclusion criteria were refusal to participate in the research (all patients were contacted by mail) and lack of validated CT scan segmentations. Of the 473 subjects who underwent orthognathic surgery within the study timeframe, 4 refused to participate and 16 lacked validated CT scan segmentations. 453 subjects (453 CT scans) were eventually included in our dataset. Most CT scans (<italic>n</italic> = 417) were obtained using a GE Healthcare Discovery (GEHC) CT750HD scanner with a tube current of 50mA, an exposure time of 730s and a tube voltage of 100kV. Scans were randomly distributed among a train/validation set (<italic>n</italic> = 300) and test set (<italic>n</italic> = 153). CT scans characteristics and CT machines are detailed in <xref rid="tbl1" ref-type="table">Table 1</xref>. This study was ethically approved by an Institutional Review Board (IRB No. CRM-2001-051).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>CT scan characteristics and CT machines in the train/validation, test and public mandible test datasets.</p></caption>
<graphic xlink:href="21260825v1_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s2b">
<title>Ground truth segmentation process</title>
<p>All CT scans had been segmented prior to our study during patient treatment. Ground truth segmentations were used for diagnosis, computer-aided surgical planning and manufacturing of 3D-printed personalized surgical guides and fixation implants according to a certified internal procedure (Materialise). This industry procedure is confidential and cannot be fully described here. It implies semi-automatic segmentations, manually refined by a first operator [Step 1] before slice-by-slice verification for validation by a senior operator [Step 2] focusing mainly on the regions of interest: external surface of the bones, teeth and mandibular canals. Steps 1 &#x0026; 2 are repeated until the segmentations are approved and certified for clinical use. This process results in 5 segmentation masks: (1) upper skull, (2) mandible, (3) upper teeth, (4) lower teeth and (5) both mandibular canals.</p>
</sec>
<sec id="s2c">
<title>Public mandible test dataset</title>
<p>In order to assess the generalizability of our trained model, we tested it on a public dataset of 10 high-definition CT scans from clinical practice [<xref ref-type="bibr" rid="c22">22</xref>]. These scans had the particularity of presenting complete mandibular bone structures entirely devoid of teeth. Ground truth manual segmentations of the mandible by two experts (A and B) were provided, which differed from our segmentations in that they were filled. <xref rid="tbl1" ref-type="table">Table 1</xref> provides more details about this dataset. To the best of our knowledge, this is the only publicly available dataset of high-definition Head CT scans with ground truth segmentation of one of the structures of interest for orthognathic surgery planning.</p>
</sec>
<sec id="s2d">
<title>nnU-Net framework</title>
<p>The nnU-Net deep learning framework was used as an out-of-the-box tool, following instructions given by Isensee <italic>et al</italic>. [<xref ref-type="bibr" rid="c21">21</xref>]. Our raw train/validation dataset was used to automatically configure preprocessing, network architecture, training and post-processing pipelines. No modifications were made in setting the nnU-Net hyperparameters and data augmentation strategy. The training of 3D full resolution U-Net was performed on our train/validation set according to a 5-fold cross-validation strategy. After the end of the training pipeline, cross-validation result analysis showed that the model incorrectly labeled a few voxels as teeth in some scans displaying no upper and/or lower teeth. As a result, we implemented additional post-processing for teeth masks, consisting in removing all components smaller than an empirically-determined threshold. Finally, inference was performed on our test dataset as well as on the public mandible dataset. More details on the implementation of the deep learning framework are provided in Supplementary Information.</p>
</sec>
<sec id="s2e">
<title>Evaluation metrics</title>
<p>Quantitative evaluation of the model performance was performed on our test set by comparing ground truth masks with predictions for each of the 5 segmentation masks. We followed best practice in evaluating model results, using both volume-based and surface-based metrics. Our main volume-based metric was the commonly used vDSC. Our main surface-based metric was surface DSC at 1mm (sDSC). Compared with classical metrics such as vDSC, sDSC, which was introduced recently, has been shown to be more strongly correlated with the amount of time needed to correct a segmentation for clinical use [<xref ref-type="bibr" rid="c7">7</xref>]. We set our acceptable tolerance of sDSC at 1mm, as was done in recent international challenges in biomedical imaging. As in previous studies, an sDSC score of 95&#x0025; was chosen as threshold value to consider variations between two segmentations as clinically non-significant. Additional quantitative metrics were computed after the common biomedical segmentation evaluations: Jaccard Coefficient, Volumetric Similarity, Average Surface Distance and Hausdorff distance.</p>
</sec>
<sec id="s2f">
<title>Industry expert validation procedure</title>
<p>A random sample of our predicted masks for 45 subjects from our test dataset was sent to industry experts (Materialise) to be evaluated according to the 2-step validation process described above. Each segmentation mask was labeled &#x201C;validated for clinical use&#x201D; or &#x201C;not validated for clinical use&#x201D;.</p>
</sec>
<sec id="s2g">
<title>Statistical analysis</title>
<p>Continuous variables are presented as mean &#x00B1; standard deviation and categorical variables are expressed as numbers and percentages. vDSC and sDSC results are presented as percentages (&#x0025;). The Wilcoxon test was used to compare vDSC and sDSC in scans obtained with GEHC CT750HD and scans obtained with other machines; p-values &#x003C; 0.05 were considered significant. All data were analyzed with Python (v.3.7) and RStudio software (v.1.3).</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Patient characteristics</title>
<p>Patient mean age was 27 &#x00B1; 11 years. The patients presented diverse anatomical deformities. 237 subjects (52.3&#x0025;) exhibited skeletal class II (prognathic maxilla and/or retrognathic mandible), 163 subjects (36.0&#x0025;) exhibited skeletal class III (retrognathic maxilla and/or prognathic mandible) and 165 subjects (36.4&#x0025;) displayed asymmetry (clear asymmetry of the maxilla and/or the mandible evaluated on the 3D models). 89.6&#x0025; of the CT scans (<italic>n =</italic> 406) showed metallic artefacts, in the form of orthodontic materials for 354 subjects (77.9&#x0025;), metallic dental fillings or crowns for 193 subjects (42.6&#x0025;) and fixation implants from previous orthognathic surgeries for 30 subjects (6.6&#x0025;). Some subjects had no upper teeth (<italic>n =</italic> 4), no lower teeth (<italic>n =</italic> 1) or no teeth at all (<italic>n =</italic> 2). The public mandible test dataset included senior patients (63 &#x00B1; 9 years) with no teeth at all. <xref rid="tbl2" ref-type="table">Table 2</xref> summarizes patient characteristics in our datasets.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Descriptive characteristics of the patients in the train/validation, test and public mandible test datasets.</p></caption>
<graphic xlink:href="21260825v1_tbl2.tif"/>
</table-wrap>
</sec>
<sec id="s3b">
<title>Model performance</title>
<sec id="s3b1">
<title>Quantitative evaluation on our test dataset</title>
<p>The mean results of vDSC and sDSC for each segmentation label of our test set are shown in <xref rid="tbl3" ref-type="table">Table 3</xref>, while <xref rid="fig1" ref-type="fig">Figure 1</xref> shows the distribution of the results. The mean vDSC for all masks was 92.24 &#x00B1; 6.19&#x0025;. Without the mandibular canal results, the mean vDSC was 94.90 &#x00B1; 0.91&#x0025;. The mean sDSC for all masks was 98.03 &#x00B1; 2.48&#x0025;. Out of the 153 scans, 148 presented a mean sDSC for all masks which cleared the 95&#x0025; limit for clinical significance. There were no statistically significant differences in both vDSC and sDSC when comparing scans obtained with GEHC CT750HD and scans obtained with other machines. Additional quantitative evaluation results for cross-validation and test datasets are provided in Supplementary Materials.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><p>Mean vDSC and sDSC results on our test dataset (n = 153).</p></caption>
<graphic xlink:href="21260825v1_tbl3.tif"/>
</table-wrap>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Left side: 3D model reconstructed from predicted segmentation masks (Upper Skull and Mandible with transparent overlay). Right side: distribution of vDSC and sDSC results in our test dataset (153 CT scans), for each segmentation mask (no result below 50&#x0025;).</p></caption>
<graphic xlink:href="21260825v1_fig1.tif"/>
</fig>
<p>Four subjects, representative of our test dataset and of the anatomic diversity of the database, were chosen to illustrate our results (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, <xref rid="fig3" ref-type="fig">Fig. 3</xref>, Supplementary videoclips 1 to 4). The most notable segmentation error made by the model on these subjects was the incorrect labeling of an upper deciduous tooth as a lower tooth in a patient suffering from craniofacial syndrome with several impacted teeth (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>, <xref rid="fig3" ref-type="fig">Fig. 3b</xref> and Supplementary videoclip 2).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Four representative cases (a to d) showing sagittal slices of original data, ground truth segmentations, nnU-Net network-predicted segmentations and quantitative evaluation results. Red, upper skull; green, mandible; blue, upper teeth; yellow, lower teeth; cyan, mandibular canal.</p></caption>
<graphic xlink:href="21260825v1_fig2.tif"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>3D surface models of segmentation results for 4 subjects representative of the anatomical diversity and of the challenges arising from our test dataset: (a) prognathic and asymmetric mandible; (b) craniofacial syndrome, with included and missing teeth; (c) retrognathic mandible; (d) no teeth and maxillary fixation implants from previous surgery (not segmented by the network).</p></caption>
<graphic xlink:href="21260825v1_fig3.tif"/>
</fig>
</sec>
<sec id="s3b2">
<title>Expert evaluation on our test dataset</title>
<p>The results of the industry expert validation process are shown in <xref rid="tbl4" ref-type="table">Table 4</xref>. Validation rates were about 90&#x0025; for mandible and mandibular canals, 80&#x0025; for upper skull and 60&#x0025; for teeth. Comments appended to non-validated cases for bones mentioned small holes on the bony surface or under-segmentation of the anterior nasal spine. Reasons for rejection of mandibular canal masks were the inclusion of a few outlier voxels or the omission of an auxiliary canal during segmentation. As to teeth, reasons for non-validation were under-or over-segmentation of a few voxels of the apex (<xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><p>Results of industry expert evaluation on 45 random CT scans from our test set.</p></caption>
<graphic xlink:href="21260825v1_tbl4.tif"/>
</table-wrap>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Representative lower teeth mask which was not validated by industry expert. Red line: predicted mask contour. Blue line: ground truth mask contour.</p></caption>
<graphic xlink:href="21260825v1_fig4.tif"/>
</fig>
</sec>
<sec id="s3b3">
<title>Quantitative evaluation on public mandible dataset</title>
<p>vDSC and sDSC results of the inference on the public mandible dataset are provided in <xref rid="tbl5" ref-type="table">Table 5</xref>. Most of the model&#x2019;s errors were located in the anterior part of the mandible, where the edentulous patients&#x2019; alveolar bone was atrophic and extremely thin (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). One scan (mandible #10), performed on a subject with an endotracheal tube, produced low-quality results. Additional quantitative evaluation results for this dataset are presented in Supplementary Table 3.</p>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5.</label>
<caption><p>vDSC and sDSC results on public mandible dataset.</p></caption>
<graphic xlink:href="21260825v1_tbl5.tif"/>
</table-wrap>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>3D surface models of ground truth and automatic segmentation for mandible #2 of public mandible test dataset: black wireframe, ground truth (operator A); solid green, automatic segmentation result. (a) right lateral view; (b) upper view.</p></caption>
<graphic xlink:href="21260825v1_fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="s3c">
<title>Training and automatic segmentation times</title>
<p>Training time for one fold on one GPU was about 48 hours (1,000 epochs). The trained model provided automatic segmentation of 1 CT scan in approximately 10 minutes.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>The main goal of this study was to evaluate the performance of the nnU-Net framework for semantic segmentation of CMF CT scans obtained for the planning of orthognathic surgeries. We chose this deep learning framework because it is open source and well-maintained, and has the ability to automatically configure itself without manual intervention. It has delivered state-of-the art segmentation results on a large diversity of biomedical datasets, surpassing most highly-specialized algorithms [<xref ref-type="bibr" rid="c21">21</xref>]. Our quantitative results demonstrated the model&#x2019;s relevance for CT scan segmentation, with 97&#x0025; of our test dataset showing a mean sDSC above 95&#x0025;. However, this study also illustrated the challenges arising from the evaluation of deep learning-based algorithms in the case of demanding, specific clinical applications which forbid blind trust in quantitative metrics [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c23">23</xref>]. For example, our sDSC results for upper skull masks showed a relatively large dispersion, which is not reflected, however, in expert evaluation results. Indeed, discrepancies between ground truth and prediction masks were mainly found in small bony structures located inside the skull, which are not relevant for most clinical applications (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>). Conversely, the number of non-validated teeth segmentation masks cannot be directly correlated to quantitative results, most masks obtaining excellent vDSC and sDSC results (&#x003E;95&#x0025;). No teeth labels were rejected based on segmentation errors localized at crown level, despite the difficulty in delineating the upper and lower teeth when they are in contact [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. Instead, they were rejected because of a few mislabeled voxels located in zones (root apices) clinically relevant for personalized implant manufacturing. However, many other clinical applications, such as computer-assisted diagnosis or planning not involving personalized implant manufacturing, would not require such precision in the segmentation of dental apices. Finally, the clinical value of sDSC metrics in the context of small object segmentation was demonstrated by our mandibular canal segmentation results, for which vDSC did not seem like an appropriate metric [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c23">23</xref>].</p>
<p>This study, which followed best practice guidelines [<xref ref-type="bibr" rid="c20">20</xref>], is the first to train and test a deep learning segmentation model on such a large number of high-definition CMF CT scans. Our results are comparable or superior to those of previously published studies, despite the more challenging task we faced: all previous results were based on smaller test datasets (between 0 and 30 scans) and fewer segmentation masks [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c8">8</xref>&#x2013;<xref ref-type="bibr" rid="c18">18</xref>]. No previous work had included a cohort of consecutive patients, and only one previous publication had clearly stated that its database included patients with syndromic conditions [<xref ref-type="bibr" rid="c16">16</xref>]. Our results for the segmentation of scans from patients with marked syndromic deformities show the versatility of the model (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>), and are comparable to those of Wang <italic>et al</italic>. who recently reported on a deep learning-based model for multi-task segmentation of bone and teeth separately [<xref ref-type="bibr" rid="c18">18</xref>]. However, the latter study lacked a hold-out test dataset, included only scans devoid of metal artifacts and did not differentiate between maxillary and mandibular structures. Our results for the segmentation of mandibular canals (vDSC of 81.59&#x0025; &#x00B1; 5.79&#x0025;, sDSC of 97.9&#x0025; &#x00B1; 3.51&#x0025;) are significantly superior to those reported by Jaskari <italic>et al</italic>. (vDSC of 57&#x0025;) and Kwak <italic>et al</italic>. (average Jaccard coefficient of canal and background of 57.72&#x0025;) [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>]. However, most existing studies used CBCTs, which are more difficult to segment than CT scans [<xref ref-type="bibr" rid="c16">16</xref>]. In addition, the absence of a public dataset of full high-definition CMF (CB)CT scans with ground truth segmentations prevents direct comparison of our model with previously described ones.</p>
<p>This research has several limitations. It is a single-center study, and thus cannot assess the reliability of the results in other cohorts. Generalization of our results to other CT machines or deformities was partially evaluated on 10 public CT scans obtained using a different CT machine and presenting very different anatomies from those in our training dataset. No subjects from our training database had such edentulous mandibles or endotracheal tube. Setting aside mandible #10 on the basis of endotracheal tube interference, our vDSC results nonetheless outperformed semi-automatic open source segmentation algorithms tested on the same dataset [<xref ref-type="bibr" rid="c6">6</xref>]. This suggests that our model could be used in other settings and for other clinical applications in the fields of CMF or dentistry, although proper multicenter and prospective evaluations are still needed. Our study focused on CT scans because it is the imaging modality we use for computer-assisted planning of orthognathic surgery at this time. In future works we plan to finetune this model in order to evaluate its performance with CBCT scans, another widespread and challenging imaging modality. Finally, we will attempt to use our segmentation results for automatic localization of anatomic landmarks, in order to provide cephalometric measurements for clinical diagnosis and treatment planning [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>].</p>
<p>To conclude, this study showed that nnU-Net could be used out-of-the-box (along with a simple post-processing volume filter) to provide robust segmentation of routine preoperative CMF CT scans. While the successful segmentation of dental apices will require additional efforts, quantitative results and industry expert evaluation demonstrated the clinical validity of our trained model for the segmentation phase of computer-assisted orthognathic surgery planning. Our results suggest that the nnU-Net framework could be trained from scratch easily, using databases from other departments, to answer the specific needs of many clinical setting.</p>
</sec>
<sec sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material>
<label>Supplementary Information</label>
<media xlink:href="supplements/260825_file03.pdf" />
</supplementary-material>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>Some study subjects (10 mandibular CT scans) were shared in a previous publication: Wallner J, Mischak I, Jan Egger (2019) Computed tomography data collection of the complete human mandible and valid clinical ground truth models. Sci Data 6:190003. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sdata.2019.3">https://doi.org/10.1038/sdata.2019.3</ext-link></p>
</sec>
<ack>
<title>Acknowledgment</title>
<p>The authors would like to thank the &#x201C;Association Les Chirurgiens Maxillo-Faciaux&#x201D; for its technical support.</p>
</ack>
<sec id="s5">
<title>Funding</title>
<p>This study has received funding by the &#x201C;Fondation des Gueules Cass&#x00E9;es&#x201D; (grant number 28-2020).</p>
</sec>
<sec id="s6">
<title>Compliance with Ethical Standards</title>
<p><bold><italic>Guarantor:</italic></bold> The scientific guarantor of this publication is Laurent Gajny.</p>
<p><bold><italic>Conflict of Interest:</italic></bold> The authors of this manuscript declare relationships with the following company: Materialise.</p>
<p><bold><italic>Statistics and Biometry:</italic></bold> One of the authors has significant statistical expertise.</p>
<p><bold><italic>Informed Consent:</italic></bold> Written informed consent was waived by the Institutional Review Board. All patients were contacted by mail and were offered to refuse to participate in the research.</p>
<p><bold><italic>Ethical Approval</italic></bold>: The IRB &#x201C;Comit&#x00E9; d&#x2019;Ethique pour la Recherche en Imagerie M&#x00E9;dicale&#x201D; (CERIM) gave ethical approval for this research (number CRM-2001-051).</p>
<p><bold><italic>Study subjects or cohorts overlap:</italic></bold> Some study subjects (10 mandibular CT scans) were shared in a previous publication: Wallner J, Mischak I, Jan Egger (2019) Computed tomography data collection of the complete human mandible and valid clinical ground truth models. Sci Data 6:190003. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sdata.2019.3">https://doi.org/10.1038/sdata.2019.3</ext-link></p>
<sec id="s6a">
<title>Methodology</title>
<p>Methodology</p>
<list list-type="bullet">
<list-item><p>retrospective</p></list-item>
<list-item><p>diagnostic or prognostic study</p></list-item>
<list-item><p>performed at one institution</p></list-item>
</list>
</sec>
</sec>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item>
<term>CMF</term>
<def><p>Craniomaxillofacial</p></def>
</def-item>
<def-item>
<term>vDSC</term>
<def><p>Volumetric Dice Similarity Coefficient</p></def>
</def-item>
<def-item>
<term>sDSC</term>
<def><p>Surface Dice Similarity Coefficient at 1mm</p></def>
</def-item>
<def-item>
<term>CBCT</term>
<def><p>Cone Beam CT</p></def>
</def-item></def-list></glossary>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Borzabadi-Farahani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Eslamipour</surname> <given-names>F</given-names></string-name>, <string-name><surname>Shahmoradi</surname> <given-names>M</given-names></string-name> (<year>2016</year>) <article-title>Functional needs of subjects with dentofacial deformities: A study using the index of orthognathic functional treatment need (IOFTN)</article-title>. <source>J Plast Reconstr Aesthet Surg</source> <volume>69</volume>:<fpage>796</fpage>&#x2013;<lpage>801</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bjps.2016.03.008">https://doi.org/10.1016/j.bjps.2016.03.008</ext-link></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Xia</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Gateno</surname> <given-names>J</given-names></string-name>, <string-name><surname>Teichgraeber</surname> <given-names>JF</given-names></string-name> (<year>2009</year>) <article-title>New Clinical Protocol to Evaluate Craniomaxillofacial Deformity and Plan Surgical Correction</article-title>. <source>J Oral Maxillofac Surg</source> <volume>67</volume>:<fpage>2093</fpage>&#x2013;<lpage>2106</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.joms.2009.04.057">https://doi.org/10.1016/j.joms.2009.04.057</ext-link></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Alkhayer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Piffk&#x00F3;</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lippold</surname> <given-names>C</given-names></string-name>, <string-name><surname>Segatto</surname> <given-names>E</given-names></string-name> (<year>2020</year>) <article-title>Accuracy of virtual planning in orthognathic surgery: a systematic review</article-title>. <source>Head Face Med</source> <volume>16</volume>:<fpage>34</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13005-020-00250-2">https://doi.org/10.1186/s13005-020-00250-2</ext-link></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="other"><string-name><surname>Torosdagli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Liberton</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Verma</surname> <given-names>P</given-names></string-name>, <etal>et al</etal> (<year>2017</year>) <article-title>Robust and fully automated segmentation of mandible from CT scans</article-title>. <source>In: 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). IEEE, Melbourne, Australia</source>, pp <fpage>1209</fpage>&#x2013;<lpage>1212</lpage></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="website"><string-name><surname>Murabito</surname> <given-names>F</given-names></string-name>, <string-name><surname>Palazzo</surname> <given-names>S</given-names></string-name>, <string-name><surname>Salanitri</surname> <given-names>FP</given-names></string-name>, <etal>et al</etal> (<year>2021</year>) <article-title>Deep Recurrent-Convolutional Model for Automated Segmentation of Craniomaxillofacial CT Scans</article-title>. <source>In: 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, Milan, Italy</source>, pp <fpage>9062</fpage>&#x2013;<lpage>9067</lpage></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="website"><string-name><surname>Wallner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schwaiger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hochegger</surname> <given-names>K</given-names></string-name>, <etal>et al</etal> (<year>2019</year>) <article-title>A review on multiplatform evaluations of semi-automatic open-source based image segmentation for cranio-maxillofacial surgery</article-title>. <source>Comput Methods Programs Biomed</source> <volume>182</volume>:<fpage>105102</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cmpb.2019.105102">https://doi.org/10.1016/j.cmpb.2019.105102</ext-link></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="other"><string-name><surname>Nikolov</surname> <given-names>S</given-names></string-name>, <string-name><surname>Blackwell</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zverovitch</surname> <given-names>A</given-names></string-name>, <etal>et al</etal> (<year>2021</year>) <article-title>Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy</article-title>. <pub-id pub-id-type="arxiv">180904430</pub-id> <source>Phys Stat</source></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Chung</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>J</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <article-title>Pose-aware instance segmentation framework from cone beam CT images for tooth segmentation</article-title>. <source>Comput Biol Med</source> <volume>120</volume>:<fpage>103720</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.compbiomed.2020.103720">https://doi.org/10.1016/j.compbiomed.2020.103720</ext-link></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="other"><string-name><surname>Cui</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Li</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>W</given-names></string-name> (<year>2019</year>) <article-title>ToothNet: Automatic Tooth Instance Segmentation and Identification From Cone Beam CT Images</article-title>. <source>In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, Long Beach, CA, USA</source>, pp <fpage>6361</fpage>&#x2013;<lpage>6370</lpage></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Jaskari</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sahlsten</surname> <given-names>J</given-names></string-name>, <string-name><surname>J&#x00E4;rnstedt</surname> <given-names>J</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <article-title>Deep Learning Method for Mandibular Canal Segmentation in Dental Cone Beam Computed Tomography Volumes</article-title>. <source>Sci Rep</source> <volume>10</volume>:<fpage>5842</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-020-62321-3">https://doi.org/10.1038/s41598-020-62321-3</ext-link></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Kwak</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Kwak</surname> <given-names>E-J</given-names></string-name>, <string-name><surname>Song</surname> <given-names>JM</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <article-title>Automatic mandibular canal detection using a deep convolutional neural network</article-title>. <source>Sci Rep</source> <volume>10</volume>:<fpage>5711</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-020-62586-8">https://doi.org/10.1038/s41598-020-62586-8</ext-link></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="book"><string-name><surname>Lian</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>F</given-names></string-name>, <string-name><surname>Deng</surname> <given-names>HH</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <chapter-title>Multi-task Dynamic Transformer Network for Concurrent Bone Segmentation and Large-Scale Landmark Localization with Dental CBCT</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Martel</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Abolmaesumi</surname> <given-names>P</given-names></string-name>, <string-name><surname>Stoyanov</surname> <given-names>D</given-names></string-name>, <etal>et al</etal></person-group> (eds) <source>Medical Image Computing and Computer Assisted Intervention &#x2013; MICCAI 2020</source>. <publisher-name>Springer International Publishing, Cham</publisher-name>, pp <fpage>807</fpage>&#x2013;<lpage>816</lpage></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Minnema</surname> <given-names>J</given-names></string-name>, <string-name><surname>Eijnatten</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hendriksen</surname> <given-names>AA</given-names></string-name>, <etal>et al</etal> (<year>2019</year>) <article-title>Segmentation of dental cone-beam CT scans affected by metal artifacts using a mixed-scale dense convolutional neural network</article-title>. <source>Med Phys</source> <volume>46</volume>:<fpage>5027</fpage>&#x2013;<lpage>5035</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/mp.13793">https://doi.org/10.1002/mp.13793</ext-link></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Qiu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kraeima</surname> <given-names>J</given-names></string-name>, <etal>et al</etal> (<year>2019</year>) <article-title>Automatic segmentation of the mandible from computed tomography scans for 3D virtual surgical planning using the convolutional neural network</article-title>. <source>Phys Med Biol</source> <volume>64</volume>:<fpage>175020</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1361-6560/ab2c95">https://doi.org/10.1088/1361-6560/ab2c95</ext-link></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="other"><string-name><surname>Qiu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kraeima</surname> <given-names>J</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <article-title>Recurrent convolutional neural networks for mandible segmentation from computed tomography</article-title>. <pub-id pub-id-type="arxiv">200306486</pub-id> <source>Cs Eess</source></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Torosdagli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Liberton</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Verma</surname> <given-names>P</given-names></string-name>, <etal>et al</etal> (<year>2019</year>) <article-title>Deep Geodesic Learning for Segmentation and Anatomical Landmarking</article-title>. <source>IEEE Trans Med Imaging</source> <volume>38</volume>:<fpage>919</fpage>&#x2013;<lpage>931</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TMI.2018.2875814">https://doi.org/10.1109/TMI.2018.2875814</ext-link></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <article-title>Context-guided fully convolutional networks for joint craniomaxillofacial bone segmentation and landmark digitization</article-title>. <source>Med Image Anal</source> <volume>60</volume>:<fpage>101621</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.media.2019.101621">https://doi.org/10.1016/j.media.2019.101621</ext-link></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="website"><string-name><surname>Wang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Minnema</surname> <given-names>J</given-names></string-name>, <string-name><surname>Batenburg</surname> <given-names>KJ</given-names></string-name>, <etal>et al</etal> (<year>2021</year>) <article-title>Multiclass CBCT Image Segmentation for Orthodontics with Deep Learning</article-title>. <source>J Dent Res</source> 002203452110053. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/00220345211005338">https://doi.org/10.1177/00220345211005338</ext-link></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="other"><string-name><surname>Ronneberger</surname> <given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Brox</surname> <given-names>T</given-names></string-name> (<year>2015</year>) <article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title>. <pub-id pub-id-type="arxiv">150504597</pub-id> <source>Cs</source></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Schwendicke</surname> <given-names>F</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>J-H</given-names></string-name>, <etal>et al</etal> (<year>2021</year>) <article-title>Artificial intelligence in dental research: Checklist for authors, reviewers, readers</article-title>. <source>J Dent</source> <volume>107</volume>:<fpage>103610</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jdent.2021.103610">https://doi.org/10.1016/j.jdent.2021.103610</ext-link></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Isensee</surname> <given-names>F</given-names></string-name>, <string-name><surname>Jaeger</surname> <given-names>PF</given-names></string-name>, <string-name><surname>Kohl</surname> <given-names>SAA</given-names></string-name>, <etal>et al</etal> (<year>2021</year>) <article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>. <source>Nat Methods</source> <volume>18</volume>:<fpage>203</fpage>&#x2013;<lpage>211</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41592-020-01008-z">https://doi.org/10.1038/s41592-020-01008-z</ext-link></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Wallner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mischak</surname> <given-names>I</given-names></string-name>, <string-name><given-names>Jan</given-names> <surname>Egger</surname></string-name> (<year>2019</year>) <article-title>Computed tomography data collection of the complete human mandible and valid clinical ground truth models</article-title>. <source>Sci Data</source> <volume>6</volume>:<fpage>190003</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sdata.2019.3">https://doi.org/10.1038/sdata.2019.3</ext-link></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="other"><string-name><surname>Reinke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Eisenmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tizabi</surname> <given-names>MD</given-names></string-name>, <etal>et al</etal> (<year>2021</year>) <source>Common Limitations of Image Processing Metrics: A Picture Story</source>. <pub-id pub-id-type="arxiv">210405642</pub-id> Cs Eess</mixed-citation></ref>
</ref-list>
</back>
</article>