<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.09.01.21262086</article-id>
<article-version>1.2</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Oncology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Cell graph neural networks enable the digital staging of tumor microenvironment and precise prediction of patient survival in gastric cancer</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Wang</surname><given-names>Yanan</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002B;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Wang</surname><given-names>Yu Guang</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">&#x002B;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Hu</surname><given-names>Changyuan</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Li</surname><given-names>Ming</given-names></name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Fan</surname><given-names>Yanan</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Otter</surname><given-names>Nina</given-names></name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Sam</surname><given-names>Ikuan</given-names></name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gou</surname><given-names>Hongquan</given-names></name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Hu</surname><given-names>Yiqun</given-names></name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kwok</surname><given-names>Terry</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zalcberg</surname><given-names>John</given-names></name>
<xref ref-type="aff" rid="a9">9</xref>
<xref ref-type="aff" rid="a10">10</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Boussioutas</surname><given-names>Alex</given-names></name>
<xref ref-type="aff" rid="a11">11</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Daly</surname><given-names>Roger J.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Mont&#x00FA;far</surname><given-names>Guido</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Li&#x00F2;</surname><given-names>Pietro</given-names></name>
<xref ref-type="aff" rid="a12">12</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Xu</surname><given-names>Dakang</given-names></name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Webb</surname><given-names>Geoffrey I.</given-names></name>
<xref ref-type="aff" rid="a13">13</xref>
<xref ref-type="aff" rid="a14">14</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8031-9086</contrib-id>
<name><surname>Song</surname><given-names>Jiangning</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a14">14</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Biomedicine Discovery Institute and Department of Biochemistry and Molecular Biology, Monash University</institution>, Melbourne, 3800, <country>Australia</country></aff>
<aff id="a2"><label>2</label><institution>Institute of Natural Sciences, School of Mathematical Sciences, Key Laboratory of Scientific and Engineering Computing of Ministry of Education (MOE-LSC), and Center for Mathematics of Artificial Intelligence Institute, Shanghai Jiao Tong University</institution>, Shanghai, 200240, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>Max Planck Institute for Mathematics in Sciences</institution>, Leipzig, 04103, <country>Germany</country></aff>
<aff id="a4"><label>4</label><institution>School of Mathematics and Statistics, The University of New South Wales</institution>, Sydney, 2052, <country>Australia</country></aff>
<aff id="a5"><label>5</label><institution>Key Laboratory of Intelligent Education Technology and Application of Zhejiang Province, Zhejiang Normal University</institution>, Jinhua, 321004, <country>China</country></aff>
<aff id="a6"><label>6</label><institution>Department of Mathematics, Department of Statistics, University of California</institution>, Los Angeles, 90095, <country>USA</country></aff>
<aff id="a7"><label>7</label><institution>Department of Laboratory Medicine, Ruijin Hospital, and School of Medicine, Shanghai Jiao Tong University</institution>, Shanghai, 200025, <country>China</country></aff>
<aff id="a8"><label>8</label><institution>Biomedicine Discovery Institute and Department of Microbiology, Monash University</institution>, Melbourne, 3800, <country>Australia</country></aff>
<aff id="a9"><label>9</label><institution>School of Public Health and Preventive Medicine, Monash University</institution>, Melbourne, 3004, <country>Australia</country></aff>
<aff id="a10"><label>10</label><institution>Department of Medical Oncology, The Alfred Hospital</institution>, Melbourne, 3004, <country>Australia</country></aff>
<aff id="a11"><label>11</label><institution>The Alfred Hospital and Central Clinical School, Monash University</institution>, Melbourne, VIC 3004, <country>Australia</country></aff>
<aff id="a12"><label>12</label><institution>Cambridge Centre for AI in Medicine, Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country>United Kingdom</country></aff>
<aff id="a13"><label>13</label><institution>Department of Data Science and Artificial Intelligence, Monash University</institution>, Melbourne, 3800, <country>Australia</country></aff>
<aff id="a14"><label>14</label><institution>Monash Data Futures Institute, Monash University</institution>, Melbourne, 3800, <country>Australia</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>To whom correspondence should be addressed: <email>Jiangning.Song@monash.edu</email>. Correspondence may also be addressed to: <email>pl219@cam.ac.uk</email>, <email>dakang_xu@163.com</email>, <email>Geoff.Webb@monash.edu</email>.</corresp>
<fn id="n1" fn-type="equal"><label>&#x002B;</label><p>These authors contributed equally to this work.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.09.01.21262086</elocation-id>
<history>
<date date-type="received">
<day>01</day>
<month>9</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>11</day>
<month>9</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>9</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license><license-p>The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</license-p></license>
</permissions>
<self-uri xlink:href="21262086.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<p>Gastric cancer is one of the deadliest cancers worldwide. Accurate prognosis is essential for effective clinical assessment and treatment. Spatial patterns in the tumor microenvironment (TME) are conceptually indicative of the staging and progression of gastric cancer patients. Using spatial patterns of the TME by integrating and transforming the multiplexed immunohistochemistry (mIHC) images as Cell-Graphs, we propose a novel graph neural network-based approach, termed <italic>Cell-Graph Signature or CG</italic><sub><italic>Signature</italic></sub>, powered by artificial intelligence, for digital staging of TME and precise prediction of patient survival in gastric cancer. In this study, patient survival prediction is formulated as either a binary (<italic>short-term</italic> and <italic>long-term</italic>) or ternary (<italic>short-term, medium-term</italic>, and <italic>long-term</italic>) classification task. Extensive benchmarking experiments demonstrate that the <italic>CG</italic><sub><italic>Signature</italic></sub> achieves outstanding model performance, with Area Under the Receiver-Operating Characteristic curve (AUROC) of 0.960&#x00B1;0.01, and 0.771&#x00B1;0.024 to 0.904&#x00B1;0.012 for the binary- and ternary-classification, respectively. Moreover, Kaplan-Meier survival analysis indicates that the &#x2018;digital-grade&#x2019; cancer staging produced by <italic>CG</italic><sub><italic>Signature</italic></sub> provides a remarkable capability in discriminating both binary and ternary classes with statistical significance (<italic>p</italic>-value &#x003C; 0.0001), significantly outperforming the AJCC 8th edition Tumor-Node-Metastasis staging system. Using Cell-Graphs extracted from mIHC images, <italic>CG</italic><sub><italic>Signature</italic></sub> improves the assessment of the link between the TME spatial patterns and patient prognosis. Our study suggests the feasibility and benefits of such artificial intelligence-powered digital staging system in diagnostic pathology and precision oncology.</p>
</abstract>
<counts>
<page-count count="14"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>This work was supported by the Major Inter-Disciplinary Research (IDR) Grant awarded by Monash University</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>This study was approved by the Shanghai Ruijin Hospital under protocol 2021SQ015. All researchers were blinded to the patient private data during the experimental analysis.</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<p>Gastric cancer (GC) accounted for 768,793 deaths in 2020, representing the fourth deadliest cancer globally<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. The 5-year survival rate of GC is around 20&#x0025;<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. More accurate prognosis can greatly assist clinical decision-making, especially regarding which patients would benefit from aggressive treatment. The Tumor-Node-Metastasis (TNM) staging system<sup><xref ref-type="bibr" rid="c3">3</xref></sup> is the most prevalent cancer staging system primarily used in hospitals and medical centers worldwide, which reflects the information of the primary tumor, affected lymph nodes, and metastasis. Many current treatment recommendations and guidelines are based on the TNM stages. However, significant differences in clinical outcomes have been observed in GC patients with the same TNM stage and similar treatment regimens<sup><xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c6">6</xref></sup>. These findings indicate the TNM staging system has limitations and accordingly, cannot be used to accurately predict prognosis of cancer patients. As such, new strategies that can provide more tailored staging information and improve prognosis predictions are highly desirable.</p>
<p>Recent years have seen numerous data-driven, machine learning-based studies of cancer prognosis. For instance, Yu <italic>et al</italic>. introduced prognosis prediction of lung adenocarcinoma and squamous cell carcinoma of stage I, and their model can distinguish the shorter-term survivors from longer-term survivors (<italic>p &#x003C;</italic> 0.003 and <italic>p</italic> = 0.023)<sup><xref ref-type="bibr" rid="c7">7</xref></sup>. Mobadersany <italic>et al</italic>. presented survival convolutional neural network (SCNN), and their developed histology image-based SCNN reached comparable performance on astrocytomas of grade III and IV with histology grading or molecular subtyping<sup><xref ref-type="bibr" rid="c8">8</xref></sup>. In another study, Jiang <italic>et al</italic>. proposed the GC-SVM classifier as a powerful survival predictor using the data of immunomarkers and could predict the adjuvant chemotherapy benefit of gastric cancer patients with stages II and III<sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Wulczyn <italic>et al</italic>. conducted a survival prediction study involving multiple cancers based on deep learning, and as a result, their model was capable of making significant survival predictions for five out of ten cancers and could effectively stratify cancer patients of stages II and III<sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Jiang <italic>et al</italic>. developed a convolutional neural network-based classifier from H&#x0026;E images to predict the prognosis of stage III colon cancer patients<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. Dimitriou <italic>et al</italic>. introduced a K-nearest neighbor-based method to predict the mortality of stage II colorectal cancer patients using immunofluorescence images<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. Although these prognosis prediction studies achieved promising performance using H&#x0026;E staining histology or immunohistochemistry staining images, they were often restricted to specific subtypes or stages of the corresponding cancers. Moreover, these studies also did not consider any spatial information from the tumor microenvironment (TME).</p>
<p>Cell distribution in TME is not random but rather it is associated with the underlying functional state<sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Therefore, the exploration of the TME of cancer samples would offer critical insights into the key spatial patterns associated with the growth, cancer progression, and thus patient prognosis<sup><xref ref-type="bibr" rid="c14">14</xref></sup>. Recent advent multiplexed immunohistochemistry (mIHC) staining technique enables systematic investigation of the TME<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> and supports extraction of enriched spatial information from the TME, including the cell location, cell types, cell and nucleus morphological information, and related optical information<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. Researchers have applied the mIHC technique to analyze the TME of pancreatic cancer and found that spatial distribution of cytotoxic T cells in proximity to cancer cells correlates with increased overall patient survival<sup><xref ref-type="bibr" rid="c14">14</xref></sup>. Barua <italic>et al</italic>. applied a statistical scoring based method, G-cross function, to measure the patterns of two different cell types, such as T-reg and CD8, and found that high infiltration of T-reg in the core tumor area is an independent predictor of worse overall survival (OS) in patients of non-small cell lung cancer<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. However, these studies only considered the spatial features of limited cell types and only used handcrafted features. Therefore, comprehensive and quantitative methods that assess the relationships between spatial features descriptive of cell distribution and prognosis are currently lacking.</p>
<p>Inspired by the concept of the Cell-Graph<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup> and the success of graph neural networks (GNN)<sup><xref ref-type="bibr" rid="c19">19</xref>&#x2013;<xref ref-type="bibr" rid="c21">21</xref></sup>, especially their applications to the analysis of biology data<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>, we hypothesize that intricate spatial distribution information of the TME is informative for the prediction of the OS of GC patients and a GNN model can effectively capitalize on the useful patterns generated by Cell-Graphs. To validate this hypothesis, we have developed a novel GNN-based approach for predicting the prognosis of GC patients using Cell-Graph data, which we call the <italic>Cell-Graph Signature</italic> or <italic>CG</italic><sub><italic>Signature</italic></sub>. The overall workflow is illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref> and Figure S1. In this study, we formulate prognosis prediction as a classification problem by predicting the patient&#x2019;s survival time interval rather than a continuous time frame or a risk score and develop a workflow to perform the following three-fold tasks. Firstly, it extracts the comprehensive spatial and morphological information from mIHC images. Secondly, it further uses the extracted spatial information to stratify patients into either binary (<italic>short-term</italic> and <italic>long-term</italic>) or ternary (<italic>short-term, medium-term</italic>, and <italic>long-term</italic>) classes. Finally, it conducts the Kaplan-Meier survival analysis to verify the clinical significance of the <italic>CG</italic><sub><italic>Signature</italic></sub>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>An overall workflow of graph neural network-based prognosis prediction using Cell-Graphs. <bold>(a)</bold> Specimen processing: The tumor tissues were extracted from gastric cancer, and stained with seven different biomarkers including DAPI, Pan-CK, CD8, CD68, CD163, Foxp3, and PD-L1. <bold>(b)</bold> Image pre-processing: sub-sampling and cell-graph construction were conducted for image pre-processing. <bold>(c)</bold> An illustration for the cohort, 172 gastric cancer patients were collected. <bold>(d)</bold> Data split. The training, validation and testing datasets were split with the percentages of 64&#x0025;, 16&#x0025;, and 20&#x0025;, respectively. <bold>(e)</bold> Model construction: four different GNN model architectures, including GCNSag, GCNTopK, GINSag, and GINTopK, were constructed and compared. Multi-run model training, five-fold cross-validation, and independent test were conducted to evaluate the performance of the constructed GNN models. <bold>(f)</bold> Data binning: overall survival time ranged from 0 to 88 months, and two data binning strategies were applied to generate binary- and ternary-class datasets. <bold>(g)</bold> Model architecture: The four models shared the same architecture but employed different types of convolutional unit and pooling layer, which consists of four consecutive convolutional layer and pooling layer blocks, followed by a summary layer and three fully-connected layers, prior to the generation of the final classification outcome. Architecture of the best-performing GINTopK model is illustrated herein, which outperformed the other three model architectures and also achieved the best performance on the test dataset. The corresponding number of hidden layers or feature dimensions are indicated at the bottom of each box. Here, FC stands for &#x201C;fully connected layer&#x201D;.</p></caption>
<graphic xlink:href="21262086v2_fig1.tif"/>
</fig>
<p><italic>CG</italic><sub><italic>Signature</italic></sub> represents a powerful survival predictor under comprehensive and extensive benchmarking tests of gastric cancer across all subtypes and stages. Specifically, <italic>CG</italic><sub><italic>Signature</italic></sub> can effectively stratify short-term, medium-term, and long-term GC survivors at the early diagnosis stage, and achieved the area under the receiver-operating characteristic curve (AU-ROC) values of 0.960 &#x00B1; 0.01 in terms of binary classification, and 0.771 &#x00B1; 0.024 to 0.904 &#x00B1; 0.012 in terms of ternary classification, respectively. In the follow-up survival analysis, <italic>CG</italic><sub><italic>Signature</italic></sub> outperformed the AJCC 8th edition TNM staging system on the testing cohort in terms of the Harrell&#x2019;s Concordance-Index<sup><xref ref-type="bibr" rid="c24">24</xref></sup>, Hazard Ratio (HR), and <italic>p</italic>-value.</p>
<sec id="s1">
<title>Results</title>
<sec id="s1a">
<title>Clinical characteristics and data-binning of the patient cohort</title>
<p>We collected the data of 172 gastric cancer patients from Shanghai Ruijin Hospital, affiliated with the School of Medicine, Shanghai Jiao Tong University. The clinical characteristics of this cohort are illustrated in Table S1. This cohort contains 124 males, 47 females, and one case without gender information. With respect to the survival status, 113 cases were recorded as &#x201C;death&#x201D; while 59 patients as &#x201C;live&#x201D;. The statistical summary of their TNM (the AJCC 8th edition) stages are provided in Table S1. In particular, the patient numbers of the TNM stages of I, II, III, and IV are 14, 52, 95, and 3, respectively. The OS time of the cohort ranges from 0 to 88 months. Two data-binning strategies were applied to segment the patient OS into binary- or ternary-class datasets. More specifically, patients with OS time shorter than 24 months and longer than 48 months were categorized as short-term, and long-term in the binary-class dataset. The patients whose OS time is between 24 months and 48 months were removed from the training dataset but used in subsequent survival analyses. More details can be found in the section of <bold>Survival analysis and performance comparison with the TNM staging system</bold>. In the ternary-class dataset, patients were classified into short-term, medium-term, and long-term classes, using the thresholds of 12 and 60 months. Here, the data-binning thresholds were chosen to take into account the relative class balance and clinical importance. We did not optimize the data binning threshold, which, however, can be conducted when more data becomes available. Model training and subsequent analysis were performed using these two datasets.</p>
</sec>
<sec id="s1b">
<title>Workflow overview</title>
<p><xref rid="fig1" ref-type="fig">Figure 1</xref> illustrates an overall workflow and the model architecture of the proposed <italic>CG</italic><sub><italic>Signature</italic></sub> approach. As shown in <xref rid="fig1" ref-type="fig">Figure 1a</xref>, the mIHC technique was used to stain the GC tissue samples. Specifically, the nuclear counterstain, DAPI, was used for cell nuclei staining, and six antibodies of Pan-CK, CD8, CD68, CD163, Foxp3, and PD-L1 were used as annotation indicators for six different types of cells. After digitalization, cell locations, types, and related optical and morphological features were extracted using the digital pathology software. After this procedure, we obtained the CSV files in which each row corresponds to each cell with the node features shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. Based on these CSV files as the input, we developed a workflow (details can be seen in Algorithm 1) to process the raw data and build the GNN-based model to predict the patient OS interval using the features extracted from mIHC images.</p>
<p>The key steps of the workflow are as follows: <bold>(1) Image pre-processing</bold>: Sub-sampling and Cell-Graph generation were performed at this step. Specifically, each mIHC image was firstly segmented into multiple non-overlapping regions with no more than 100 cells. For each region, we built a graph where each cell was represented as a node and the reciprocal of the Euclidean distance of each cell-cell pair was used to establish edges between them with the distance of less than 20 <italic>&#x00B5;m</italic>. Detailed information can be found in the section &#x201C;Cell-Graph construction&#x201D; of Methods. Then, we extracted a total of 35 features (as shown in <xref rid="tbl3" ref-type="table">Table 3</xref>) for each cell as the node attributes, including 5 optical features for each biomarker and 5 morphological features for each cell. Such generated cell-based graph is referred to as Cell-Graph<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>. There are approximately 90 Cell-Graphs constructed for each mIHC image (for each patient). Cell-Graphs originated from the same mIHC image share the same label with the corresponding patient. <bold>(2) Data split</bold>: After Cell-Graph construction, the whole dataset was partitioned into the training, validation, and test sets with the ratio of 0.64 : 0.16 : 0.20 at the patient level. In addition, we also generated the files for performing five-fold cross-validation by generating five non-overlapping training-validation subsets and evaluating the model performance on these 5-fold subsets. <bold>(3) Hyper-parameter optimization</bold>: We utilized the Hyperopt toolkit<sup><xref ref-type="bibr" rid="c25">25</xref></sup>from the Ray software package<sup><xref ref-type="bibr" rid="c26">26</xref></sup> to tune the hyperparameters of GNN models. The optimized hyperparameters were then used for the follow-up model training and performance evaluation. <bold>(4) Model performance evaluation and data visualization</bold>: To comprehensively assess the capability and reliability of our GNN model, we evaluate model performance using multi-run model training, five-fold cross-validation, and independent test. The test results were visualized by generating the receiver-operating characteristic (ROC) curves, confusion matrix, and boxplots of Accuracy, F1-Score, and Matthews Correlation Coefficient (MCC). Performance metrics are defined in Section &#x201C;Metrics of model performance evaluation&#x201D; in the Supplementary material.</p>
</sec>
<sec id="s1c">
<title>Performance benchmarking of different GNN models for prognosis prediction</title>
<p>We constructed four different types of GNN models and examined their performance for predicting the OS of gastric cancer patients, including GINTopK, GINSAG, GCNTopK and GC-NSAG. Here, GIN<sup><xref ref-type="bibr" rid="c21">21</xref></sup> and GCN<sup><xref ref-type="bibr" rid="c27">27</xref></sup> are two graph convolution computational units (differences can be seen in Figure S2), whereas TopKPooling<sup><xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup> and SAGPooling<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup> are two graph pooling computational units. The graph convolutional and pooling layers are the core components of the GNN architecture. Five-fold cross-validation was conducted to assess the model of each GNN model on both binary- and ternary-classification tasks. The results are averaged on ten repetitions of five-fold cross-validation for GINTopK on binary classification (as shown in Figure S3) to circumvent the randomness of the model during training. In this procedure, Accuracy, F1-Score, MCC, and AUROC were calculated to evaluate the performance. <xref rid="fig2" ref-type="fig">Figure 2a</xref> illustrates the performance results of binary classification on five-fold cross-validation. As we observe, the median values of both Accuracy and F1-score for the four GNNs ranged from 0.83 to 0.92, while the median values of MCC ranged from 0.66 to 0.84, respectively. <xref rid="fig2" ref-type="fig">Figure 2b</xref> shows the performance results of ternary classification on five-fold cross-validation. We can see that the ternary-class classification models achieved the median values of Accuracy ranging from 0.76 to 0.82, F1-score from 0.64 to 0.72, and MCC from 0.46 to 0.5, respectively. According to the results shown in <xref rid="fig2" ref-type="fig">Figures 2a</xref> and <xref ref-type="fig" rid="fig2">2b</xref>, GINTopK slightly out-performed the other three GNN models on both binary- and ternary-classifications. Therefore, GINTopK was selected as the best-performing GNN model and employed for subsequent performance benchmarking and survival analysis.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Model performance of four GNNs on five-fold cross-validation. (a) and (b) show the Boxplots of performance metrics of Accuracy, F1-score, and MCC on five-fold cross-validation. (c) and (d) illustrate the ROCs of GINTopK binary- and ternary-models on five-fold cross-validation.</p></caption>
<graphic xlink:href="21262086v2_fig2.tif"/>
</fig>
<p>ROC curves of GINTopK on the binary- and ternary-classification tasks are illustrated in <xref rid="fig2" ref-type="fig">Figure 2c</xref> and <xref rid="fig2" ref-type="fig">Figure 2d</xref>, respectively. The binary-class GINTopK model achieved the AUROC value of 0.96 &#x00B1; 0.01 on five-fold cross-validation. In contrast, the ternary-class GINTopK classifier reached the AU-ROC values of 0.834 &#x00B1; 0.015, 0.771 &#x00B1; 0.024, and 0.904 &#x00B1; 0.012 for <italic>short-term</italic> (&#x003C;12 months), <italic>medium-term</italic> (&#x003E;12 and &#x003C;60 months), and <italic>long-term</italic> (&#x003E;60 months) on five-fold cross-validation, respectively (<xref rid="fig2" ref-type="fig">Figure 2d</xref>). Moreover, the performance results of binary-class GINTopK model on ten repetitions of five-fold cross-validation are displayed in Figure S3. We can see that the median values of both Accuracy and F1-score were within the range of 0.90-0.93 (MCC values ranged from 0.80 to 0.86), thereby suggesting the stability of our proposed GINTopK model.</p>
<p>In <xref rid="fig3" ref-type="fig">Figure 3</xref>, the performance results of the GINTopK model on the independent test are visualized using ROC curves and confusion matrix. It can be seen that the model achieved similar performance with that on five-fold cross-validation in terms of AUROC values on both binary- and ternary-classification tasks. In terms of the confusion matrix, 96&#x0025; and 89&#x0025; of the short-term and long-term patients could be accurately predicted using the binary-classification model. The true positive percentages of ternary-class model were 81&#x0025;, 59&#x0025;, and 85&#x0025;, corresponding to the short-term, medium-term, and long-term classes (<xref rid="fig3" ref-type="fig">Figure 3</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Performance assessment of the GINTopK model in terms of ROC curves and confusion matrix on the independent test. The left column shows the ROC curves of binary- and ternary-classification, while the right column displays the confusion matrix of the model predictions on the binary- and ternary-classification tasks.</p></caption>
<graphic xlink:href="21262086v2_fig3.tif"/>
</fig>
<p>Taken together, outstanding performance of the GINTopK model on both cross-validation and independent test indicate that our proposed GNN approach is capable of effectively capturing the underlying prognostic patterns from the well-constructed Cell-Graphs. The captured prognostic patterns by GNN model are characteristic of the spatial information of cell locations and types of the TME, which incorporates more potentially informative features than the TNM staging system.</p>
</sec>
<sec id="s1d">
<title>Ablation studies and prognostic value of different types of cell features</title>
<p>To examine the effect of node features of different cell types on model performance, we further performed ablation studies to assess the contribution of features to the binary- and ternary-classification performance by removing each type of features in an iterative manner. Thirty-five node features of seven types were used in this study, including DAPI, Pan-CK, CD8, CD68, Foxp3, PD-L1, and morphological features. We first evaluated the performance of the GNN model trained using all these features, and then, evaluated the performance of the models trained using the remaining features after removing each type of features from the all-feature set in turn. For each iteration, we trained the models five times with random initialization of the weights using the same dataset and calculated the mean and standard deviation of Accuracy. The results are shown in <xref rid="tbl2" ref-type="table">Table 2</xref>, where the feature contribution was measured by the accuracy change compared with that of the all-feature model. Note that when a type of feature is removed, an accuracy increase means that including the feature type reduced accuracy, and an accuracy decrease means that the feature type played an important role in attaining the all feature accuracy.</p>
<p>According to <xref rid="tbl2" ref-type="table">Table 2</xref>, the variant models trained using these feature subsets and all-feature set achieved comparable Accuracy values in both binary and ternary classifications. In the binary classification, the DAPI features and morphological features made more important contributions to the model performance compared with other types of features (e.g. the Accuracy dropped by 0.035 and 0.025, respectively), which reflect the nucleus differences of optical and morphology of the TME. Thus, inclusion of these two types of features helped to better distinguish the long-term from short-term patients. In the case of ternary classification, we can see that the GNN models trained without the of DAPI and morphology features achieved the lowest Accuracy, which is consistent with the observation in the binary classification.</p>
</sec>
<sec id="s1e">
<title>Survival analysis and performance comparison with the TNM staging system</title>
<p>To further investigate the prognostic values and clinical importance of the predictions produced by <italic>CG</italic><sub><italic>Signature</italic></sub>, we conducted the Kaplan-Meier survival analysis using the patient-level results of both binary- and ternary-classifications. For each patient, we first collected the predicted results of all the subsampled Cell-Graphs. Next, we calculated the class percentages of these predictions, and took the class with the maximum percentage as the final patient-level prediction of the corresponding patient. Using these patient-level predicted results (&#x2018;digital-grade&#x2019;) of binary-classification (with predicted class labels of <italic>CG</italic><sub><italic>Signature</italic></sub> = 0 and <italic>CG</italic><sub><italic>Signature</italic></sub> = 1) and ternary-classification (with predicted class labels of <italic>CG</italic><sub><italic>Signature</italic></sub> = 0, <italic>CG</italic><sub><italic>Signature</italic></sub> = 1, and <italic>CG</italic><sub><italic>Signature</italic></sub> = 2), we conducted the survival analysis and plotted their Kaplan-Meier curves, shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>. More specifically, when using the binary-class predictions, the median survival time of patient test cohorts predicted as <italic>CG</italic><sub><italic>Signature</italic></sub> = 0 and <italic>CG</italic><sub><italic>Signature</italic></sub> = 1 were about 18 months and 42 months, respectively. The hazard ratio was 0.217 (95&#x0025; CI: 0.108 &#x2013; 0.438), the C-Index was 0.699 (95&#x0025; CI: 0.637 &#x2013; 0.762), and the <italic>p</italic>-value was less than 0.0001, indicating that <italic>CG</italic><sub><italic>Signature</italic></sub> has statistically significant prognostic power in separating the two groups of patient cohorts. When using the ternary-class predictions, the median survival time of patient cohorts predicted as <italic>CG</italic><sub><italic>Signature</italic></sub> = 0 and <italic>CG</italic><sub><italic>Signature</italic></sub> = 1 were around 7 months and 28 months, respectively. The endpoint survival rate of <italic>CG</italic><sub><italic>Signature</italic></sub> = 2 was approximately 92.3&#x0025; (<xref rid="fig4" ref-type="fig">Figure 4b</xref>). The hazard ratio and C-Index were 0.204 (95&#x0025; CI: 0.107 &#x2013; 0.389) and 0.823 (95&#x0025; CI: 0.748 &#x2013; 0.899), respectively, with the <italic>p</italic>-value less than 0.0001.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Kaplan-Meier survival analysis of patient overall survival based on the &#x2018;digital grade&#x2019; (patient-level predictions) producted by <italic>CG</italic><sub><italic>Signature</italic></sub> in terms of <bold>(a)</bold> binary- and <bold>(b)</bold> ternary-classification. As can be seen from <xref ref-type="fig" rid="fig4">Figure 4</xref>, Kaplan-Meier survival analysis demonstrates that the &#x2018;digital grade&#x2019; cancer staging produced by <italic>CG</italic><sub><italic>Signature</italic></sub> provides a remarkable capability in discriminating both binary (short-term, long-term) and ternary (short-term, medium-term, and long-term) classes with C-Index (Binary: 0.699 (95&#x0025; CI: 0.637 &#x2013; 0.762), Ternary: 0.823 (95&#x0025; CI: 0.748 &#x2013; 0.899)), Hazard Ratio (Binary: 0.217 (95&#x0025; CI: 0.108 &#x2013; 0.438), Ternary: 0.204 (95&#x0025; CI: 0.107 &#x2013; 0.389)), and the <italic>p</italic>-values &#x003C; 0.0001.</p></caption>
<graphic xlink:href="21262086v2_fig4.tif"/>
</fig>
<p>We further compared the patient survival analysis based on predictions of <italic>CG</italic><sub><italic>Signature</italic></sub> with the AJCC 8th edition TNM staging system and showed the results in <xref rid="tbl1" ref-type="table">Table 1</xref>. In the TNM staging system, there were eight groups of <italic>I</italic><sub><italic>A</italic></sub>, <italic>I</italic><sub><italic>B</italic></sub>, <italic>II</italic><sub><italic>A</italic></sub>, <italic>II</italic><sub><italic>B</italic></sub>, <italic>III</italic><sub><italic>A</italic></sub>, <italic>III</italic><sub><italic>B</italic></sub>, <italic>III</italic><sub><italic>C</italic></sub>, <italic>IV</italic><sub><italic>A</italic></sub>, and <italic>IV</italic><sub><italic>B</italic></sub>. As no patients of stage <italic>IV</italic> were included in the binary-class test cohort and only one patient of stage <italic>IV</italic> was included in ternary-class test cohort, we excluded the patients of stage <italic>IV</italic> and those without OS information. Finally, 51 patients (including 20 uncategorized patients) and 35 patients were retained for binary- and ternary-class survival analysis, respectively. The detailed statistical information of the testing cohorts can be found in Table S2.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Overall Kaplan-Meier survival analysis based on the predictions of binary- and ternary-classification by <italic>CG</italic><sub><italic>Signature</italic></sub>. The classification results were compared with Harrell&#x2019;s Concordance-Index (C-Index), Hazard Ratio (HR), and <italic>p</italic>-value. For the convenience of survival analysis comparison, the variables of TNM stages were regrouped into TNM-2 (I&#x002B;II vs. III), TNM-3 (I vs. II vs. III), and TNM-6 (I, IIA, IIB, IIIA, IIIB, IIIC), while &#x201C;<italic>CG</italic><sub><italic>Signature</italic></sub>&#x002B;TNM-2&#x201D; denotes a four-class variable by combining the classes of TNM-2 and binary-class <italic>CG</italic><sub><italic>Signature</italic></sub>.</p></caption>
<graphic xlink:href="21262086v2_tbl1.tif"/>
</table-wrap>
<p>To make a fair comparison, three specific criteria were adopted to aggregate the TNM stages into TNM-2 (<italic>I, II</italic> vs. <italic>III</italic>), TNM-3 (<italic>I</italic> vs. <italic>II</italic> vs. <italic>III</italic>), and TNM-6 (<italic>I</italic> vs. <italic>II</italic><sub><italic>A</italic></sub> vs. <italic>II</italic><sub><italic>B</italic></sub>vs. <italic>III</italic><sub><italic>A</italic></sub> vs. <italic>III</italic><sub><italic>B</italic></sub> vs. <italic>III</italic><sub><italic>C</italic></sub>). The survival analysis results are provided in <xref rid="tbl1" ref-type="table">Table 1</xref> and Figures S4-S9. According to <xref rid="tbl1" ref-type="table">Table 1</xref> and Figure S4, the C-Index of the binary-class <italic>CG</italic><sub><italic>Signature</italic></sub> was 0.699 (<italic>p</italic>-value <italic>&#x003C;</italic> 0.0001), outperforming TNM-2 with an increase of 0.04. We further combined the TNM-2 with binary- class <italic>CG</italic><sub><italic>Signature</italic></sub> for survival analysis (Figure S6), which achieved the highest C-Index of 0.748 (<italic>p</italic>-value <italic>&#x003C;</italic> 0.0001), which was higher than TNM-6 by 0.034 (Figure S5). In ternary-class survival analysis, we compared the results of TNM-3, TNM-6, and the ternary-class <italic>CG</italic><sub><italic>Signature</italic></sub>. More specifically, C-Index of the ternary <italic>CG</italic><sub><italic>Signature</italic></sub> was 0.823 (<italic>p</italic>-value <italic>&#x003C;</italic> 0.0001, <xref rid="fig4" ref-type="fig">Figure 4b</xref>), which was superior to the TNM-3 (Figure S8) and TNM-6 (Figure S9) with an increase of 0.191 and 0.142, respectively. These results demonstrate the <italic>CG</italic><sub><italic>Signature</italic></sub> is capable of discriminating and stratifying gastric cancer patients into groups of different prognosis better than the TNM staging system. Moreover, we note that the prognostic power can be even further enhanced by integrating &#x2212; the <italic>CG</italic><sub><italic>Signature</italic></sub> predictions and the TNM stages for survival analysis, such as the <italic>CG</italic><sub><italic>Signature</italic></sub> &#x002B; TNM &#x2212; 2 in <xref rid="tbl1" ref-type="table">Table 1</xref> and Figure S6.</p>
<p>To summarize, by combining the spatial information from the mIHC images, <italic>CG</italic><sub><italic>Signature</italic></sub> has demonstrated outstanding performance in survival analysis, and achieved a better or at least comparable performance when comparing with the TNM staging system. The results suggest that effective prognostic features can indeed be captured by <italic>CG</italic><sub><italic>Signature</italic></sub>, which suggests a powerful method complementary to the current TNM staging system.</p>
</sec>
<sec id="s1f">
<title>Framelet decomposition for cell-graph</title>
<p>To examine the capacity of Cell-Graph to capture useful spatial features from mIHC images, we conducted a framelet decomposition on the whole mIHC images. The framelet transforms (including framelet decomposition and reconstruction) have proved an important tool for distilling multi-resolution information in low-pass and high-passes from the graph data<sup><xref ref-type="bibr" rid="c31">31</xref>&#x2013;<xref ref-type="bibr" rid="c35">35</xref></sup>.</p>
<p>We extracted low-pass and high-pass information of six types of features, corresponding to six different biomarkers DAPI, PAN-CK, CD8, CD68, FOXP3, and PD-L1. Tables S3-S11 show the low-pass and high-pass coefficients of the framelet decomposition on mIHC images of short-term, medium-term and long-term survivors on the entire mIHC images. For the selected samples, no significant differences were observed from the low-pass channel. However, major differences can be observed from the high-pass channel on the selected samples. More specifically, remarkable signal differences can be seen from the high-pass channel-1 and channel-2 in terms of the features of Cell Area and Nucleus Perimeter (summarized in Table S6-S11). These differences highlight the important prognostic value of cell morphological information of the TME, which is consistent with the prognostic value of different types of cell features.</p>
</sec>
</sec>
<sec id="s2">
<title>Discussion</title>
<p>In this study, we developed the first GNN-based approach, Cell-Graph Signature (<italic>CG</italic><sub><italic>Signature</italic></sub>), which is capable of predicting the prognosis of gastric cancer patients from Cell-Graphs extracted from mIHC images. Extensive benchmarking tests on multi-run model training, 5-fold cross-validation, and independent test demonstrate that <italic>CG</italic><sub><italic>Signature</italic></sub> can accurately predict the prognosis on both binary- and ternary-class classification tasks. We designed and compared the performance of four different GNN architectures, including GINSag, GCNTopK, GCNSag, and GINTopK. As a result, GINTopK achieved the best performance when compared with the other three GNN architectures (GINSag, GCNTopK, and GCNSag) on the same datasets. Feature ablation studies showed that the nucleus optical feature (DAPI) and cell morphological features are essential node features for and contributed most to the prognosis prediction, which indicate the potential pivotal roles of nuclear and cell morphology in gastric cancer progression. In survival analysis, <italic>CG</italic><sub><italic>Signature</italic></sub> clearly outperformed the AJCC 8th TNM staging system in terms of C-Index (0.823, 95&#x0025; CI: 0.748-0.899) using the ternary-classification model. In particular, we notice that <italic>CG</italic><sub><italic>Signature</italic></sub> achieved better or comparable performance with the TNM staging system when using the binary-classification model. These results of survival analysis indicate that <italic>CG</italic><sub><italic>Signature</italic></sub> provides more prognostic power than the existing TNM staging system and can help pinpoint patients who may benefit from more tailored and personalized therapy. Moreover, wavelet decomposition results suggest that Cell-Graphs can indeed capture certain important spatial features informative for classifying patient survival. Although many previous studies of prognosis prediction also achieved promising results, the majority of such studies were only limited to a specific subtype or stage of cancers. Nevertheless, in this study, we show that the proposed <italic>CG</italic><sub><italic>Signature</italic></sub> method is applicable to gastric cancer patients of all subtypes across all TNM stages. Moreover, <italic>CG</italic><sub><italic>Signature</italic></sub> achieved a better performance when stratifying test patient cohorts into different groups of prognosis, which has proven a powerful prognostic predictor for gastric cancer.</p>
<p>One caveat of the current study is that we could only obtain a limited size of the mIHC image data, and accordingly, the performance of the <italic>CG</italic><sub><italic>Signature</italic></sub> was only benchmarked on the limited size of the gastric cancer patient dataset. Thus, in future studies, it would be important to evaluate the performance of graph neural networks based on Cell-Graph data from mIHC images in much larger and/or multi-centre patient cohorts, as well as additional tumor types (in addition to gastric cancer), when more data become available. Exploration of the prognostic value of the <italic>CG</italic><sub><italic>Signature</italic></sub> method on datasets of other cancer types would surely be needed to verify its utility and capability. Additionally, future extension of the capability of <italic>CG</italic><sub><italic>Signature</italic></sub> by using whole-slide images and other biomarkers in mIHC/mIF staining, for example, holds great potential for a more comprehensive analysis of the tumor microenvironment<sup><xref ref-type="bibr" rid="c15">15</xref></sup>; this will in turn serve to better inform the training of more accurate GNN models. The continuing development of cutting-edge, robust, and broadly-applicable Cell Graph-based biomarker discovery algorithms is valuable and desirable to better inform and transform the medical care of cancer patients.</p>
</sec>
<sec id="s3">
<title>Methods</title>
<sec id="s3a">
<title>Dataset</title>
<p>The gastric cancer samples were collected and stained with mIHC technique and prepared as two batches of tissue microarray<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, in which all the samples were arranged in the matrix configuration. Then the two tissue microarrays were scanned by digital microscope (brand: Vectra Polaris) under the magnification of 40X with each pixel represents 0.5 <italic>&#x00B5;m</italic>. Totally, 181 mIHC images of cancer tissues were curated as the initial datasets. After excluding patients whose follow-up data were not available, 172 mIHC images were retained and used for model training and benchmarking. The overall survival time of the patients ranges from 0 to 88 months, as shown in <xref rid="fig1" ref-type="fig">Figure 1f</xref>. Detailed clinical characteristics and statistical summary of the cohort are provided in Table S1. Fifty-nine patients were still alive at the time of the last follow-up. All the images were stained using multiplexed immunohistochemistry of seven colors and reagents to identify the specific cell types. In this study, cells were stained with antibodies of Pan-CK, Foxp3, CD8, PD-L1, CD68, CD163, and DAPI. The dataset was randomly partitioned into the training, validation, and test subsets with the ratios of 0.64, 0.16, and 0.20 at the patient level. In addition, datasets for five-fold cross-validation were also prepared.</p>
</sec>
<sec id="s3b">
<title>Label generation</title>
<p>In this study, the survival prediction was formulated as a classification problem in the form of either binary- or ternary-classification. To explore the prognostic value of the Cell-Graphs extracted from the gastric cancer TME, the survival time of the cohort was categorized into two and three classes, and used as labels for training binary- and ternary-classification models based on graph neural networks. In binary-classification, 82 patients with survival time of less than 24 months were annotated as short-term while 70 patients with survival time of longer than 48 months were annotated as long-term. 20 patients with survival time between 24 and 48 months were removed from the training dataset, and denoted as uncategorized patients. For the ternary-classification, 12 months and 50 months were respectively used as the thresholds to divide patients into short-, medium-, and long-term, with the corresponding patient numbers of 51, 60, and 61, respectively.</p>
</sec>
<sec id="s3c">
<title>Cell segmentation</title>
<p>After digitization, the mIHC images were pre-processed using the pathology software HALO (Indica Labs) for cell segmentation and feature extraction. The extracted information was subsequently saved as a CSV file in which each row represents the features of a cell (as shown in <xref rid="tbl3" ref-type="table">Table 3</xref>), including the cell locations, optical features of stained cells, and morphology features. Thirty-five of such features were selected as the node features for each cell. Detailed information can be found in the <bold>Node attributes</bold> section.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Ablation studies of the major types of features used by the GNN models in both binary and ternary-classification. The relative importance and contribution of the features was measured by the accuracy change compared with that of the all-feature model.</p></caption>
<graphic xlink:href="21262086v2_tbl2.tif"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><p>The list of node attributes and their variable types. Each type of features are comprised of three Boolean variables and two float variables. These Boolean variables were identified by the pathology software based on the float values of Nucleus Intensity and Cytoplasm Intensity of each biomarker. Moreover, five different morphology features were extracted as the node attributes.</p></caption>
<graphic xlink:href="21262086v2_tbl3.tif"/>
</table-wrap>
</sec>
<sec id="s3d">
<title>Sub-sampling</title>
<p>Each mIHC staining image contains around 7, 000 &#x223C; 13, 000 cells. In particular, we conducted the sub-sampling when generating the Cell-Graphs. By treating each cell as a node in the Cell-Graph, we limited the graph size with no more than 100 nodes. A non-overlap sliding window was then applied to extract the local regions that contained approximately 100 cells from the mIHC images. As a result, we obtained 16951 Cell-Graphs, which would be used for GNN model training and testing. The extracted Cell-Graphs from one mIHC image were annotated with the same label as that of the corresponding mIHC image. The performance of the GNN models was firstly assessed at the Cell-Graph level; After that, the prediction outputs of all Cell-Graphs were aggregated to generate the votes for the final prediction outcome at the patient level.</p>
</sec>
<sec id="s3e">
<title>Cell-Graph construction</title>
<p>According to the previous study on the TME<sup><xref ref-type="bibr" rid="c17">17</xref></sup>, we assumed that the maximum effective distance was 20 <italic>&#x00B5;</italic>m between immune and tumor cells, which is equivalent to 40 pixels in the magnification of this study. We calculated the Euclidean distance between any pair of cells, and used this distance to define the edge weight between them according to the <xref ref-type="disp-formula" rid="eqn1">equations (1)</xref> and <xref ref-type="disp-formula" rid="eqn2">(2)</xref> shown below.</p>
<p>For the <italic>i</italic>th and <italic>j</italic>th cells with Cartesian coordinates (<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>) and (<italic>x</italic> <sub><italic>j</italic></sub>, <italic>y</italic> <sub><italic>j</italic></sub>) (which use pixel as the unit) in the same mIHC image, their Euclidean distance can be calculated as follows:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="21262086v2_eqn1.gif"/></alternatives>
</disp-formula></p>
<p>The weight between the <italic>i</italic>th and <italic>j</italic>th cells is assigned as follows:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="21262086v2_eqn2.gif"/></alternatives>
</disp-formula>
where 0 denotes that there is no interaction between the cell <italic>i</italic> and <italic>j</italic>. After sub-sampling, a number of Cell-Graphs (up to 100 nodes) were extracted and annotated, with the weight (2) of the edge between a given pair of cells.</p>
</sec>
<sec id="s3f">
<title>Node attributes</title>
<p>Graph neural network (GNN) is a powerful deep learning approach which can efficiently extract features from graph-structured data. In the present study, we focused on distilling five morphology features and 30 optical features generated by six staining biomarkers as the attributes of the node for each cell, including DAPI, PAN-CK, CD8, CD68, FOXP3, and PD-L1. The five morphology features include cell area, cytoplasm area, nucleus area, nucleus perimeter, and nucleus roundness. The optical features of each biomarker are comprised of positive, positive nucleus, positive cytoplasm, nucleus intensity, and cytoplasm intensity. As a result, a total of 35 features were extracted for each cell. These features indicates the area, shape, location and healthiness of the underlying cell. The detailed list of the features and their data types are listed in <xref rid="tbl3" ref-type="table">Table 3</xref>. All the features were linearly normalized to the range of [0, 1] prior to training the GNN models.</p>
</sec>
<sec id="s3g">
<title>Architecture of the designed graph neural networks</title>
<p>Graph-structured data are usually represented in the form of (<italic>x</italic><sub><italic>i</italic></sub>, <italic>A</italic><sub><italic>i</italic></sub>), where <italic>x</italic><sub><italic>i</italic></sub> denotes the feature of the node for the <italic>i</italic>th graph sample while <italic>A</italic><sub><italic>i</italic></sub> represents its adjacency matrix. A GNN has the similar network architecture to that of the traditional convolutional neural network. To address the classification task in this study, we designed the GNN model architecture of <italic>CG</italic><sub><italic>Signature</italic></sub>, which includes four computational units, each with two-layer <italic>graph convolution</italic> plus one-layer <italic>graph pooling</italic> followed by three-layer fully connected layers (MLP), before generating the prediction output (<xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<p>The graph convolutional layer is responsible for extracting an array of features from the last output array, which mimics the role of CNN convolution. It changes the dimension <italic>d</italic> of the feature array but does not change the number of nodes <italic>N</italic><sub><italic>i</italic></sub>. The output of graph convolutional layers is passed on to the graph pooling which compresses the node number by a fractional proportion while in this process usually the key structural information and node features are preserved. The MLP readout will then output the label class.</p>
<p>Graph convolution communicates the structural information of the data to the deep network model via the message passing between the neighborhood nodes, which contributes as the key to successfully capturing the geometric feature of the data. In this work, we adopted the GINConv<sup><xref ref-type="bibr" rid="c21">21</xref></sup> as the graph convolution and TopKPool<sup><xref ref-type="bibr" rid="c20">20</xref></sup> as the graph pooling method, respectively. The convolutional layer for GIN can be aggregated by
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="21262086v2_ueqn1.gif"/></alternatives>
</disp-formula>
where <bold>X</bold><sup>in</sup> &#x2208; &#x211D;<sup><italic>N</italic>&#x00D7;<italic>d</italic></sup> is the <italic>d</italic>-feature matrix on the nodes of the graph with <italic>N</italic> nodes for the input layer, and <italic>A</italic> &#x2208;&#x211D;<sup><italic>N</italic>&#x00D7;<italic>N</italic></sup> is the adjacency matrix of the graph. <italic>W</italic> is the filter weight parameter matrix with the size of <italic>m</italic> &#x00D7; <italic>n</italic> to be learned by the GNNs, where <italic>n</italic> is the number of hidden neurons. GINConv is a special neural message passing operator for GNN aggregation.</p>
<p>Our GNN model was trained by connecting multiple layers of graph convolution activated by a ReLU (Rectifier Linear Unit)<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. The graph pooling, which is used between two consecutive layers, serves to reduce the dimensionality of the feature map so that the network has appropriate amounts of parameters to circumvent over-fitting<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. Here we used <italic>Top-KPooling</italic><sup><xref ref-type="bibr" rid="c20">20</xref></sup> for graph pooling.</p>
<p>There exist different types of GNN models in the ma-. chine learning literature<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. Specifically, we tested the performance of the GINConv&#x002B;TopKPool model with the other three popular GNN models, i.e. GINConv&#x002B;SAGPool, GCNConv&#x002B;TopKPool, and GCNConv&#x002B;SAGPool. The results showed that the chosen model (GINConv&#x002B;TopKPool) achieved the highest AUROC value and stable training performance. Refer to <xref rid="fig2" ref-type="fig">Figures 2a</xref> and <xref ref-type="fig" rid="fig2">2b</xref> for a detailed illustration of the results.</p>
</sec>
<sec id="s3h">
<title>Hyperparameter optimization</title>
<p>We fine tuned the hyperparameters for the GNN models with the assistance of HyperOPT<sup><xref ref-type="bibr" rid="c25">25</xref></sup> and Ray<sup><xref ref-type="bibr" rid="c26">26</xref></sup>, where the network architecture and batch size were fixed. The hyperparameters were searched within the range as shown in <xref rid="tbl4" ref-type="table">Table 4</xref>. More specifically, the best-performing model used the following hyperparameters: learning rate 5 &#x00D7; 10<sup>&#x2212;4</sup>, weight decay rate 10<sup>&#x2212;4</sup>, number of hidden neurons 512, pooling ratio 0.5, number of hidden layers 4, batch size 256, and maximal number of epochs 200 with the early stopping strategy.</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><p>Search space for hyperparameters of GNN models.</p></caption>
<graphic xlink:href="21262086v2_tbl4.tif"/>
</table-wrap>
</sec>
<sec id="s3i">
<title>Prediction aggregation to assess the patient-level performance</title>
<p>The model performance was evaluated at the Cell-Graph level. After the model was optimized, the patient-level performance of the model was calculated by aggregating the prediction results produced by the optimized model. In particular, we fed Cell-Graphs of the test dataset to the optimal model to predict the label for each of them. Since hundreds of Cell-Graphs were sub-sampled from the mIHC images of the patient, hundreds of the predictions were also made for a given patient. To generate the patient-level prediction for a patient, we calculated the proportion of Cell-Graphs belonging to a specific class, and then classified the patient as the group that received the largest proportion of the Cell-Graphs.</p>
</sec>
<sec id="s3j">
<title>Framelet analysis to facilitate interpretation of the model prediction</title>
<p>From the mathematical perspective, the <italic>framelet system</italic><sup><xref ref-type="bibr" rid="c31">31</xref>&#x2013;<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup> refers to a set of functions that provide a multi-scale representation of graph structured data, which has a similar property to the traditional wavelet in the Euclidean space. Using the framelet transforms, we can decompose the graph features into low-pass and high-pass frequencies as the extracted features to train network models, via the framelet-based graph convolution.</p>
<p>Suppose <inline-formula><alternatives><inline-graphic xlink:href="21262086v2_inline1.gif"/></alternatives></inline-formula> are the pairs of the eigenvalue and eigenvector for the graph Laplacian <italic>&#x2112;</italic> of a graph <italic>&#x1D4A2;</italic> with <italic>N</italic> nodes. The (undecimated) framelets at the <italic>scale level j</italic> = 1, &#x2026;, <italic>J</italic> for graph <italic>&#x1D4A2;</italic> with the above scaling functions can be defined, for <italic>n</italic> = 1, &#x2026;, <italic>r</italic>, as follows:
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="21262086v2_eqn3.gif"/></alternatives>
</disp-formula>
where <italic>&#x03C6;</italic> <sub><italic>j,p</italic></sub> and <inline-formula><alternatives><inline-graphic xlink:href="21262086v2_inline2.gif"/></alternatives></inline-formula> are the low-pass and high-pass framelets translated at the graph node <italic>p</italic>. In the framelet analysis above, we have shown the low-pass and high-pass <italic>framelet coefficients v</italic> <sub><italic>j,p</italic></sub> and <inline-formula><alternatives><inline-graphic xlink:href="21262086v2_inline3.gif"/></alternatives></inline-formula> for a signal <italic>f</italic> on graph <italic>&#x1D4A2;</italic>. They are the projections &#x27E8;<italic>&#x03C6;</italic> <sub><italic>j,p</italic></sub> <italic>f</italic> &#x27E9; and <inline-formula><alternatives><inline-graphic xlink:href="21262086v2_inline4.gif"/></alternatives></inline-formula> of the graph signal onto framelets at the scale <italic>j</italic> and node <italic>p</italic>. The construction of framelet system and the framelet transforms rely on the filter bank (a collection of filters) to calculate framelet coefficients. Here we used the filter bank of the Haar-type filters for the experiments.<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>. The dilation factor is 2 <sup><italic>j</italic></sup> with the <italic>dilation</italic> (base) 2 for a natural number <italic>j</italic>, where <italic>j</italic> indicates the scale level and 2 <sup><italic>j</italic></sup> is the scale of the framelet. A bigger value of <italic>j</italic> indicates that the corresponding framelet coefficient carries more detailed information of the graph signal.</p>
<p>The above framelet system is a <italic>tight frame</italic>, which provides an exact representation of any <italic>L</italic><sub>2</sub> function on the graph. This guarantees that the framelet coefficients have a unique representation of a graph signal. Accordingly, the framelet coefficients can fully reflect the feature of the signal. Moreover, the coefficients decompose the signal at multi scales and can be used to observe whether a particular scale, or the high-pass or low-pass frequencies contain a more important feature of the data.</p>
</sec>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>The data used for the main analyses presented here is available for non-commercial use and can be accessible by request.</p>
</sec>
<sec id="s4">
<title>Ethics declaration</title>
<p>This study was approved by the Shanghai Ruijin Hospital under protocol 2021SQ015. All researchers were blinded to the patient private data during the experimental analysis.</p>
</sec>
<sec id="s5">
<title>Data availability</title>
<p>The data used for the main analyses presented here is available for non-commercial use and can be accessible by request.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>All the related scripts and code are publicly available and can be download at <ext-link ext-link-type="uri" xlink:href="https://github.com/docurdt/Cell-Graph_Signature.git">https://github.com/docurdt/Cell-Graph_Signature.git</ext-link>.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Sung</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title>Global cancer statistics 2020: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>. <source>CA: A Cancer J. for Clin</source>. <volume>71</volume>, <fpage>209</fpage>&#x2013;<lpage>249</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Etemadi</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>The global, regional, and national burden of stomach cancer in 195 countries, 1990&#x2013;2017: a systematic analysis for the global burden of disease study 2017</article-title>. <source>The Lancet Gastroenterol. &#x0026; Hepatol</source>. <volume>5</volume>, <fpage>42</fpage>&#x2013;<lpage>54</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Lababede</surname>, <given-names>O.</given-names></string-name> &#x0026; <string-name><surname>Meziane</surname>, <given-names>M. A.</given-names></string-name> <article-title>The eighth edition of TNM staging of lung cancer: reference chart and dia-grams</article-title>. <source>The Oncol</source>. <volume>23</volume>, <fpage>844</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Bang</surname>, <given-names>Y.-J.</given-names></string-name> <etal>et al.</etal> <article-title>Adjuvant capecitabine and oxaliplatin for gastric cancer after d2 gastrectomy (classic): a phase 3 open-label, randomised controlled trial</article-title>. <source>The Lancet</source> <volume>379</volume>, <fpage>315</fpage>&#x2013;<lpage>321</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Noh</surname>, <given-names>S. H.</given-names></string-name> <etal>et al.</etal> <article-title>Adjuvant capecitabine plus oxaliplatin for gastric cancer after d2 gastrectomy (classic): 5-year follow-up of an open-label, randomised phase 3 trial</article-title>. <source>The Lancet Oncol</source>. <volume>15</volume>, <fpage>1389</fpage>&#x2013;<lpage>1396</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Sasako</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Gastric cancer working group report</article-title>. <source>Jpn. J. Clin. Oncol</source>. <volume>40</volume>, <fpage>i28</fpage>&#x2013;<lpage>i37</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Yu</surname>, <given-names>K. H.</given-names></string-name> <etal>et al.</etal> <article-title>Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features</article-title>. <source>Nat. Commun</source>. <volume>7</volume>, <fpage>1</fpage>&#x2013;<lpage>10</lpage>, DOI: <pub-id pub-id-type="doi">10.1038/ncomms12474</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Mobadersany</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Predicting cancer outcomes from histology and genomics using convolutional networks</article-title>. <source>Proc. Natl. Acad. Sci. United States Am</source>. <volume>115</volume>, <fpage>E2970</fpage>&#x2013;<lpage>E2979</lpage>, DOI: <pub-id pub-id-type="doi">10.1073/pnas.1717139115</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>Immunomarker support vector machine classifier for prediction of gastric cancer survival and adjuvant chemotherapeutic benefit</article-title>. <source>Clin. Cancer Res</source>. <volume>24</volume>, <fpage>5574</fpage>&#x2013;<lpage>5584</lpage>, DOI: <pub-id pub-id-type="doi">10.1158/1078-0432.CCR-18-0848</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Wulczyn</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <article-title>Deep learning-based survival prediction for multiple cancer types using histopathology images</article-title>. <source>PLoS ONE</source> <volume>15</volume>, <fpage>1</fpage>&#x2013;<lpage>18</lpage>, DOI: <pub-id pub-id-type="doi">10.1371/journal.pone.0233678</pub-id> (<year>2020</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Jiang</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title>A machine learning-based prognostic predictor for stage III colon cancer</article-title>. <source>Sci. Reports</source> <volume>10</volume>, <fpage>1</fpage>&#x2013;<lpage>9</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Dimitriou</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Arandjelovi&#x0107;</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Harrison</surname>, <given-names>D. J.</given-names></string-name> &#x0026; <string-name><surname>Caie</surname>, <given-names>P. D.</given-names></string-name> <article-title>A principled machine learning framework improves accuracy of stage II colorectal cancer prognosis</article-title>. <source>NPJ Digit. Medicine</source> <volume>1</volume>, <fpage>1</fpage>&#x2013;<lpage>9</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Yener</surname>, <given-names>B.</given-names></string-name> <article-title>Cell-graphs: image-driven modeling of structure-function relationship</article-title>. <source>Commun. ACM</source> <volume>60</volume>, <fpage>74</fpage>&#x2013;<lpage>84</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Carstens</surname>, <given-names>J. L.</given-names></string-name> <etal>et al.</etal> <article-title>Spatial computation of intratumoral T cells correlates with survival of patients with pancreatic cancer</article-title>. <source>Nat. Commun</source>. <volume>8</volume>, <fpage>1</fpage>&#x2013;<lpage>13</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Berry</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Analysis of multispectral imaging with the AstroPath platform informs efficacy of PD-1 blockade</article-title>. <source>Science</source> <volume>372</volume>, DOI: <pub-id pub-id-type="doi">10.1126/science.aba2609</pub-id> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://science.sciencemag.org/content/372/6547/eaba2609.full.pdf">https://science.sciencemag.org/content/372/6547/eaba2609.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Lu</surname>, <given-names>M. Y.</given-names></string-name>, <string-name><surname>Sater</surname>, <given-names>H. A.</given-names></string-name> &#x0026; <string-name><surname>Mahmood</surname>, <given-names>F.</given-names></string-name> <article-title>Multiplex computational pathology for treatment response predication</article-title>. <source>Cancer Cell</source> <volume>39</volume>, <fpage>1053</fpage>&#x2013;<lpage>1055</lpage>, DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ccell.2021.07.014">https://doi.org/10.1016/j.ccell.2021.07.014</ext-link> (<year>2021</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Barua</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Spatial interaction of tumor cells and regulatory T cells correlates with survival in non-small cell lung cancer</article-title>. <source>Lung Cancer</source> <volume>117</volume>, <fpage>73</fpage>&#x2013;<lpage>79</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Gunduz</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Yener</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Gultekin</surname>, <given-names>S. H.</given-names></string-name> <article-title>The cell graphs of cancer</article-title>. <source>Bioinformatics</source> <volume>20</volume>, <fpage>i145</fpage>&#x2013;<lpage>i151</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Scarselli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gori</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tsoi</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Hagenbuchner</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Monfardini</surname>, <given-names>G.</given-names></string-name> <article-title>The graph neural network model</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>20</volume>, <fpage>61</fpage>&#x2013;<lpage>80</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="other"><string-name><surname>Gao</surname>, <given-names>H.</given-names></string-name> &#x0026; <string-name><surname>Ji</surname>, <given-names>S.</given-names></string-name> <source>Graph U-Nets. In ICML</source>, <fpage>2083</fpage>&#x2013;<lpage>2092</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="other"><string-name><surname>Xu</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Leskovec</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Jegelka</surname>, <given-names>S.</given-names></string-name> <article-title>How powerful are graph neural networks?</article-title> In <source>ICLR</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>scGNN is a novel graph neural network framework for single-cell RNA-Seq analyses</article-title>. <source>Nat. Commun</source>. <volume>12</volume>, <fpage>1</fpage>&#x2013;<lpage>11</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Zhao</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Valsdottir</surname>, <given-names>L. R.</given-names></string-name>, <string-name><surname>Zang</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Peng</surname>, <given-names>J.</given-names></string-name> <article-title>Identifying drug&#x2013;target interactions based on graph convolutional network and deep neural network</article-title>. <source>Briefings Bioinforma</source>. <volume>22</volume>, <fpage>2141</fpage>&#x2013;<lpage>2150</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="other"><string-name><surname>Harrell</surname>, <given-names>F. E.</given-names></string-name>, <string-name><surname>Califf</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pryor</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K. L.</given-names></string-name> &#x0026; <string-name><surname>Rosati</surname>, <given-names>R. A.</given-names></string-name> <article-title>Evaluating the Yield of Medical Tests</article-title>. <source>JAMA: The J. Am. Med. Assoc</source>. DOI: <pub-id pub-id-type="doi">10.1001/jama.1982.03320430047030</pub-id> (<year>1982</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Bergstra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yamins</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Cox</surname>, <given-names>D. D.</given-names></string-name> <article-title>Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms</article-title>. In <source>Proceedings of the 12th Python in Science Conference</source>, vol. <volume>13</volume>, <fpage>20</fpage> (Citeseer, <year>2013</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="other"><string-name><surname>Nishihara</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> <article-title>Real-time machine learning: The missing pieces</article-title>. In <source>Workshop on Relational Representation Learning (R2L) at NIPS</source>, <fpage>106</fpage>&#x2013;<lpage>110</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="other"><string-name><surname>Kipf</surname>, <given-names>T. N.</given-names></string-name> &#x0026; <string-name><surname>Welling</surname>, <given-names>M.</given-names></string-name> <article-title>Semi-supervised classification with graph convolutional networks</article-title>. In <source>ICLR</source> (<year>2017</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="other"><string-name><surname>Cangea</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Veli&#x010D;kovi&#x0107;</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jovanovi&#x0107;</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Kipf</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Li&#x00F2;</surname>, <given-names>P.</given-names></string-name> <article-title>Towards sparse hierarchical graph classifiers</article-title>. In <source>Workshop on Relational Representation Learning (R2L) at NIPS</source> (<year>2018</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="other"><string-name><surname>Knyazev</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>G. W.</given-names></string-name> &#x0026; <string-name><surname>Amer</surname>, <given-names>M. R.</given-names></string-name> <article-title>Understanding attention in graph neural networks</article-title>. In <source>NeurIPS</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="other"><string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Kang</surname>, <given-names>J.</given-names></string-name> <article-title>Self-attention graph pooling</article-title>. In <source>ICML</source>, <fpage>3734</fpage>&#x2013;<lpage>3743</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Dong</surname>, <given-names>B.</given-names></string-name> <article-title>Sparse representation on graphs by tight wavelet frames and applications</article-title>. <source>Appl. Comput. Harmon. Analysis</source> <volume>42</volume>, <fpage>452</fpage>&#x2013;<lpage>479</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="other"><string-name><surname>Zheng</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y. G.</given-names></string-name> &#x0026; <string-name><surname>Zhuang</surname>, <given-names>X.</given-names></string-name> <article-title>Decimated framelet system on graphs and fast G-framelet transforms</article-title>. <source>J. Mach. Leanring Res</source>. (<year>2021</year>, to appear).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="book"><string-name><surname>Wang</surname>, <given-names>Y. G.</given-names></string-name> &#x0026; <string-name><surname>Zhuang</surname>, <given-names>X.</given-names></string-name> <chapter-title>Tight framelets on graphs for multiscale data analysis</chapter-title>. In <source>Wavelets and Sparsity XVIII</source>, vol. <volume>11138</volume>, <fpage>111380B</fpage> (<publisher-name>International Society for Optics and Photonics</publisher-name>, <year>2019</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Y. G.</given-names></string-name> &#x0026; <string-name><surname>Zhuang</surname>, <given-names>X.</given-names></string-name> <article-title>Tight framelets and fast framelet filter bank transforms on manifolds</article-title>. <source>Appl. Comput. Harmon. Analysis</source> <volume>48</volume>, <fpage>64</fpage>&#x2013;<lpage>95</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="other"><string-name><surname>Zheng</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal> <article-title>How framelets enhance graph neural networks</article-title>. In <source>ICML</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="book"><string-name><surname>Voduc</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kenney</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Nielsen</surname>, <given-names>T. O.</given-names></string-name> <chapter-title>Tissue microarrays in clinical oncology</chapter-title>. In <source>Seminars in radiation oncology</source>, vol. <volume>18</volume>, <fpage>89</fpage>&#x2013;<lpage>97</lpage> (<publisher-name>Elsevier</publisher-name>, <year>2008</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="other"><string-name><surname>Agarap</surname>, <given-names>A. F.</given-names></string-name> <source>Deep learning using rectified linear units (ReLU)</source> (<year>2019</year>). 1803.08375.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="other"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> <article-title>ImageNet classification with deep convolutional neural networks</article-title>. In <source>NeurIPS</source>, <fpage>1097</fpage>&#x2013;<lpage>1105</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="other"><string-name><surname>Wu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal> <article-title>A comprehensive survey on graph neural networks</article-title>. <source>IEEE Transactions on Neural Networks Learn. Syst</source>. (<year>2020</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="other"><string-name><surname>Fey</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Lenssen</surname>, <given-names>J. E.</given-names></string-name> <article-title>Fast graph representation learning with PyTorch Geometric</article-title>. In <source>ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds</source> (<year>2019</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements</title>
<p>The authors would like to thank the Ruijin Hospital affiliated with Shanghai Jiao Tong University School of Medicine for providing the support of this project. We would also like to thank all the collaborators and colleagues for the enlightening discussions and feedback. This work was supported by the Major Inter-Disciplinary Research (IDR) Grant awarded by Monash University.</p>
</ack>
<sec id="s7">
<title>Author contributions</title>
<p>Y.W. and Y.G.W. conceived and conducted the experiments, analysed the results, and wrote the first draft. Y.W., Y.G.W., C.H., M.L., P.L., G.I.W., and J.S. were responsible for the methodology and experiment design. Y.F., N.O., T.K., R.J.D., J.Z., A.B. and G.M. helped to analyse the results. I.S., Q.G., Y.H., and D.X. processed and curated the mIHC data. P.L., D.X., G.I.W., and J.S. supervised this study. All authors reviewed or revised the manuscript.</p>
</sec>
<sec id="s8">
<title>Competing interests</title>
<p>The authors declare no competing financial interests.</p>
</sec>
<sec id="s9">
<title>Additional information</title>
<sec id="s9a">
<title>Hardware</title>
<p>All the experiments were performed using Py-Torch Geometric<sup><xref ref-type="bibr" rid="c40">40</xref></sup> on a server with Intel(R) Core(TM) i9-9820X 230 CPU 3.30GHz, NVIDIA GeForce RTX 2080Ti and NVIDIA TITAN V GV100.</p>
</sec>
</sec>
</back>
</article>