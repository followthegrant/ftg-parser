<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.08.17.21262189</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neurology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Detecting orientation of Brain MR scans using deep learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Singhal</surname><given-names>Chinmay</given-names></name>
<degrees>MD</degrees>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gupta</surname><given-names>Nihit</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Stein</surname><given-names>Anouk</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zhou</surname><given-names>Quan</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<degrees>MD</degrees>
</contrib>
<contrib contrib-type="author">
<name><surname>Chen</surname><given-names>Leon</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<degrees>MD</degrees>
</contrib>
<contrib contrib-type="author">
<name><surname>Shih</surname><given-names>George</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label>MD.ai</aff>
<aff id="a2"><label>2</label><institution>University of Waterloo</institution></aff>
<aff id="a3"><label>3</label>geo.rge.ai</aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label><email>chinmay@md.ai</email></corresp>
<fn fn-type="others"><p><email>nihitgupta.ng@gmail.com</email></p></fn>
<fn fn-type="others"><p><email>anouk.stein@gmail.com</email></p></fn>
<fn fn-type="others"><p><email>quan@md.ai</email></p></fn>
<fn fn-type="others"><p><email>leon@md.ai</email></p></fn>
<fn fn-type="others"><p><email>geo@rge.ai</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.08.17.21262189</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>8</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>17</day>
<month>8</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>24</day>
<month>8</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="21262189.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>A<sc>bstract</sc></title>
<p>There has been a steady escalation in the impact of Artificial Intelligence (AI) on Healthcare along with an increasing amount of progress being made in this field. While many entities are working on the development of significant deep learning models for the diagnosis of brain-related diseases, identifying precise images needed for model training and inference tasks is limited due to variation in DICOM fields which use free text to define things like series description, sequence and orientation [<xref ref-type="bibr" rid="c1">1</xref>]. Detecting the orientation of brain MR scans (Axial/Sagittal/Coronal) remains a challenge due to these variations caused by linguistic barriers, human errors and de-identification - essentially rendering the tags unreliable [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>]. In this work, we propose a deep learning model that identifies the orientation of brain MR scans with near perfect accuracy.</p>
</abstract>
<counts>
<page-count count="5"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>None</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>None</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>The utility of AI in the healthcare sector has been globally increasing. It&#x2019;s current market size value in 2021 is USD 10.4 billion and is expected to expand to USD 120.2 billion by the end of 2028 [<xref ref-type="bibr" rid="c5">5</xref>]. Further the growth in the field of medical imaging and diagnostics can be attributed to the existence of a large amount of imaging data, availability of many AI systems to radiologists for managing diagnosis and treatment and the influx of a large number of startups in this segment [<xref ref-type="bibr" rid="c6">6</xref>]. In our work, we develop a solution for detecting the orientation of brain MR scans. Our solution aims to find its use in the management of large datasets, data annotations, and model testing to be used by various entities working in the area of developing healthcare AI.</p>
<p>The current works in this area suggest methods relying on the metadata of the DICOM files but the real question is how reliable is the metadata? The values of various attributes in the DICOM metadata are unreliable and inconsistent [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>]. Automating the classification of the orientation plane of the brain MR scans thus becomes challenging. Given this context, we aim to develop a reliable solution that utilizes the image pixels for determining the orientation plane. In this article, we describe the method of using a deep learning model and includes the following parts:</p>
<list list-type="bullet">
<list-item><p>Annotating the series containing the DICOM files using the computation of Image Orientation Plane attribute&#x2019;s value method and verifying the annotations.</p></list-item>
<list-item><p>Pre-processing the pixel data of DICOM files for feature extraction.</p></list-item>
<list-item><p>Using a pre-trained neural network to train and fine-tune the classifier.</p></list-item>
</list>
<p>Foremost, we present the usual and conventional methods of detecting the orientation of the Brain MR scans. After that, we describe the various aspects of the proposed solution, and then we finally present our results.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Background Information</title>
<p>Various works suggests retrieving the orientation plane from the &#x201C;SeriesDescription&#x201D; DICOM attribute as it is often specified in a series description. For example, &#x201C;AX&#x201D; in &#x201C;AX T1 pre gd&#x201D; means axial. However, orientation is not always included in the description. For example, retrieving the orientation plane from the series description,&#x201C;T2-weighted trace&#x201D; is not possible using this method. Current practice also suggests using the DICOM attribute Image Orientation Patient (0020,0037). The attribute values give the direction cosines of the first row and first column of an image with respect to the patient which could be further used to compute the orientation plane of the scan by computing the main direction of the normal to the slices [<xref ref-type="bibr" rid="c2">2</xref>]. But even in the aforementioned method missing slices or extra slices, unidentified attributes and unexpected errors may cause difficulty in detecting the orientation plane of the brain MR scan.There is very little work done to detect the orientation plane of brain MR scans especially using pixel data and deep learning. Hence, with the use of the pixel data and deep learning methods we achieved to train 3 different models to predict the orientation of the brain MR scans.</p>
</sec>
<sec id="s3">
<label>3</label>
<title>Data Preparation</title>
<p>The dataset used in this work comes from The Cancer Imaging Archive (TCGA - LGG) [<xref ref-type="bibr" rid="c7">7</xref>]. It was filtered to obtain the subset of images having Image Modality being MR. Two subsets (A and B) of the dataset were downloaded. Subset A consisted of 40 out of the 199 total subjects from the TCGA-LGG dataset which in turn had 551 series. Subset B consisted of 20 out of the remaining 159 subjects, which further gave 299 series. Both subsets A and B were then filtered so that they only consisted of series with sequence T1, T2 or FLAIR. This filtering was done with the help of the series description attribute of the DICOM files, however, due to variation and multiple sequence tagging in the values of series description, 19 out 551 series in subset A and 42 out of 299 series in subset B were manually verified and annotated. Finally, subset A contained 266 series and subset B consisted 125 series. We used subset A for the purposes of training and validation, in the ratio 75:25 respectively. Subset B was used as the test set for model evaluation.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Extracting ground truth labels</title>
<p>The series were classified into three different labels - Axial, Sagittal and Coronal, which are the standard and most common orientation planes for brain MR scans. Instead of annotating all the data manually, most labels the labels were derived from the Image Orientation Patient (0020,0037) DICOM attribute. The first three values are considered as vector <inline-formula><alternatives><inline-graphic xlink:href="21262189v1_inline1.gif"/></alternatives></inline-formula> and the other three values are considered as vector <inline-formula><alternatives><inline-graphic xlink:href="21262189v1_inline2.gif"/></alternatives></inline-formula>, after which the cross product of vectors <inline-formula><alternatives><inline-graphic xlink:href="21262189v1_inline3.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="21262189v1_inline4.gif"/></alternatives></inline-formula> is calculated. If the resultant vector&#x2019;s <italic>&#x00EE;</italic> component value is 1 then the series is labeled as &#x2018;Axial&#x2019;, if not then the <italic>&#x0135;</italic> component is considered. Then, if the <italic>&#x0135;</italic> component has the value 1 then the series is labeled as &#x2018;Coronal&#x2019;, if not then we move on to checking the <inline-formula><alternatives><inline-graphic xlink:href="21262189v1_inline5.gif"/></alternatives></inline-formula> component. Finally, if the <inline-formula><alternatives><inline-graphic xlink:href="21262189v1_inline6.gif"/></alternatives></inline-formula> component value is 1 then the series is labeled as &#x2018;Sagittal&#x2019; [<xref ref-type="bibr" rid="c8">8</xref>]. For each series, the SeriesInstanceUID and its derived orientation plane (ground truth) are stored in a csv file for training and evaluation purposes.</p>
</sec>
<sec id="s5">
<label>5</label>
<title>Methodology</title>
<p>The preprocessing happens at the series level. Each series contains multiple DICOM images (instances). Each DICOM image (instance) is converted to an 8-bit NumPy array of the given size and is processed 4 times, with four different windows each time. Windowing can be understood as a means of manipulating the high amount of voxel values in the DICOM images in order to change the appearance of the image so particular features an image look highlighted [<xref ref-type="bibr" rid="c9">9</xref>]. The pixel data of each of these four different windows with different feature extraction are appended in four different lists. Each list containing pixel data of all the DICOM images (instances) with a particular window is called a stack. Hence, we created only four unique stacks with four unique windows, which were then used with different approaches.</p>
<sec id="s5a">
<label>5.1</label>
<title>The Greyscale MPSM Approach</title>
<p>In MPSM both the M stands for mean, P stands for peak-to-peak which is the range of values (maximum - minimum) along an axis and S stands for standard deviation which is the square root of the average of the squared deviations from the mean. In this approach we use the four stacks. Each stack was processed to find the characteristic pixel data values as NumPy arrays. We create two NumPy arrays using the mean values of pixel data along axis 0 of Stack 1 and 4 respectively, similarly, then we find two other NumPy arrays having peak-to-peak values and standard deviation values of Stack 2 and 3 respectively. These four NumPy arrays were then used to form four new images of size 128&#x00D7;128 pixels each which were then pasted on top of each other on a newly created blank image of size 256&#x00D7;256 pixels. The final image was then stored with it&#x2019;s filename being SeriesInstanceUID. This is done for each series and the resultant image looks like the following figures -</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Axial Image</p></caption>
<graphic xlink:href="21262189v1_fig1.tif"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Coronal Image</p></caption>
<graphic xlink:href="21262189v1_fig2.tif"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Sagittal Image</p></caption>
<graphic xlink:href="21262189v1_fig3.tif"/>
</fig>
</sec>
<sec id="s5b">
<label>5.2</label>
<title>The Greyscale 4M Approach</title>
<p>Here the 4M stands for mean, maximum, minimum and mean. Similar to the first approach, we use the above-mentioned four stacks. Each stack was processed to find the characteristic pixel data values as NumPy arrays. We create two NumPy array using the mean values of pixel data along axis 0 of Stack 1 and 4 respectively, similarly, then we create two other NumPy arrays having maximum values and minimum values of Stack 2 and 3 respectively. These four NumPy arrays were then used to form four new images of size 128&#x00D7;128 pixels each which were then pasted on a newly created blank image of size 256&#x00D7;256 pixels. Similar to previous approach the final image was then stored with its filename being SeriesInstanceUID. This is again done for each series and the resultant images look like the following figures -</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Axial Image</p></caption>
<graphic xlink:href="21262189v1_fig4.tif"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Coronal Image</p></caption>
<graphic xlink:href="21262189v1_fig5.tif"/>
</fig>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Sagittal Image</p></caption>
<graphic xlink:href="21262189v1_fig6.tif"/>
</fig>
</sec>
<sec id="s5c">
<label>5.3</label>
<title>The MPSM Approach with both Greyscale and RGB</title>
<p>This approach is similar to Greyscale MPSM approach. In addition to using the greyscale pixel data, we also utilize each channel of the RGB scale. Each stack was again used to find the characteristic pixel data values as NumPy arrays. We created two NumPy arrays having mean values, peak-to-peak values, standard deviation values and again mean values of Stack 1, 2, 3 and 4 respectively in greyscale. Then four new NumPy arrays having the same values as im1, im2, im3 and im4 respectively were created but were converted to the R channel of RGB scale and hence were no longer greyscale. Similarly another four arrays having the same values were converted to the G channel of the RGB scale and NumPy arrays and finally another four NumPy arrays were converted to the B channel of the RGB scale. These sixteen NumPy arrays were used to form sixteen new images of size 64&#x00D7;64 pixels each which were then pasted on a newly created blank image of size 256&#x00D7;256 pixels. Similar to the previous approaches the final image was then stored with its filename being SeriesInstanceUID. Thus, for each series with the help of these 16 resultant NumPy arrays, we create 16 images pasted on a single image, as shown in the following figures -</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><p>Axial Image</p></caption>
<graphic xlink:href="21262189v1_fig7.tif"/>
</fig>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><p>Coronal Image</p></caption>
<graphic xlink:href="21262189v1_fig8.tif"/>
</fig>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9:</label>
<caption><p>Sagittal Image</p></caption>
<graphic xlink:href="21262189v1_fig9.tif"/>
</fig>
</sec>
</sec>
<sec id="s6">
<label>6</label>
<title>Model</title>
<p>We built a single-label classifier using the FastAI library [<xref ref-type="bibr" rid="c10">10</xref>], keeping in mind the ease of use and reproducibility aspects. We also maintain a csv file consisting of all the SeriesInstanceUIDs and their corresponding orientation. We then create a FastAI object that combines the data and a pre-trained convolutional neural network model, in our case squeezenet1_0 [<xref ref-type="bibr" rid="c11">11</xref>], and also used transfer learning to fine-tune the pretrained model. We chose squeezenet as it is a small CNN with fewer parameters, hence provides quick inference results with minimal degradation in performance and does not have extravagant computational cost. Squeezenet provides similar accuracy as AlexNet, which is 3 times slower, 500 times bigger and has 50X more parameters than squeezenet [<xref ref-type="bibr" rid="c11">11</xref>]. The model was then exported and saved.</p>
</sec>
<sec id="s7">
<label>7</label>
<title>Result</title>
<p>The models were tested on subset B and the following results were obtained -</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Results</p></caption>
<graphic xlink:href="21262189v1_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s8">
<label>8</label>
<title>Future Work</title>
<p>By using pixel data, we were able to develop a deep learning model that precisely identifies the orientation (Axial/Sagittal/Coronal) of brain MR scans. The next step is to develop a deep learning model that will predict the orientation of brain MR scans (T1/T2/FLAIR) which we can then integrate with the orientation detection model. We will be open-sourcing our method and will be updating this manuscript with further details.</p>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>We used publicly available data from the TCGA website.</p>
<p>
<ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/display/Public/TCGA-LGG">https://wiki.cancerimagingarchive.net/display/Public/TCGA-LGG</ext-link>
</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>Martin J.</given-names> <surname>Willemink</surname></string-name>, <string-name><given-names>Wojciech A.</given-names> <surname>Koszek</surname></string-name>, <string-name><given-names>Cailin</given-names> <surname>Hardell</surname></string-name>, <string-name><given-names>Jie</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Dominik</given-names> <surname>Fleischmann</surname></string-name>, <string-name><given-names>Hugh</given-names> <surname>Harvey</surname></string-name>, <string-name><given-names>Les R.</given-names> <surname>Folio</surname></string-name>, <string-name><given-names>Ronald M.</given-names> <surname>Summers</surname></string-name>, <string-name><given-names>Daniel L.</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Matthew P.</given-names> <surname>Lungren</surname></string-name>. <article-title>Preparing medical imaging data for machine learning</article-title>. <source>Radiology</source>, <volume>295</volume>(<issue>1</issue>):<fpage>4</fpage>&#x2013;<lpage>15</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>Romane</given-names> <surname>Gauriau</surname></string-name>, <string-name><surname>Christopher</surname> <given-names>Bridge</given-names></string-name>, <string-name><given-names>Lina</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Felipe</given-names> <surname>Kitamura</surname></string-name>, <string-name><given-names>Neil A.</given-names> <surname>Tenenholtz</surname></string-name>, <string-name><given-names>John E.</given-names> <surname>Kirsch</surname></string-name>, <string-name><given-names>Katherine P.</given-names> <surname>Andriole</surname></string-name>, <string-name><given-names>Mark H.</given-names> <surname>Michalski</surname></string-name>, and <string-name><given-names>Bernardo C.</given-names> <surname>Bizzo</surname></string-name>. <article-title>Using dicom metadata for radiological image series categorization: a feasibility study on large clinical brain mri datasets</article-title>. <source>Journal of Digital Imaging</source>, <volume>33</volume>(<issue>3</issue>):<fpage>747</fpage>&#x2013;<lpage>762</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="other"><string-name><given-names>Mark O.</given-names> <surname>Gueld</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Kohnen</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Keysers</surname></string-name>, <string-name><given-names>Henning</given-names> <surname>Schubert</surname></string-name>, <string-name><given-names>Berthold B.</given-names> <surname>Wein</surname></string-name>, <string-name><given-names>Joerg</given-names> <surname>Bredno</surname></string-name>, and <string-name><given-names>Thomas M.</given-names> <surname>Lehmann</surname></string-name>. <article-title>quality of dicom header information for image categorization</article-title>. <source>MedicalImaging 2002: PACS and Integrated Medical Information Systems: Design and Evaluation</source>, <year>2002</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="other"><string-name><given-names>Kevin</given-names> <surname>McEnery</surname></string-name>. <source>Dicom imaging metadata: Inconsistency of institution name and institution address information impacts interoperability</source>, <year>2021</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="other"><collab>Artificial intelligence in healthcare market size, share trends analysis report by component (software solutions, hardware, service), by application (robot assisted surgery, connected machines, clinical trials), and segment forecasts</collab>, 2021-2028, <year>2021</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="other"><collab>Artificial Market</collab>. <article-title>Artificial intelligence in healthcare market by offering, technology, end-use application, end user &#x007C;</article-title> <source>covid-19 impact analysis &#x007C; marketsandmarkets&#x2122;</source>, <year>2021</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="other"><string-name><given-names>Nancy</given-names> <surname>Pedano</surname></string-name>, <string-name><given-names>Adam E.</given-names> <surname>Flanders</surname></string-name>, <string-name><given-names>Lisa</given-names> <surname>Scarpace</surname></string-name>, <string-name><given-names>Tom</given-names> <surname>Mikkelsen</surname></string-name>, <string-name><given-names>Jennifer M.</given-names> <surname>Eschbacher</surname></string-name>, <string-name><given-names>Beth</given-names> <surname>Hermes</surname></string-name>, <string-name><given-names>Victor</given-names> <surname>Sisneros</surname></string-name>, <string-name><given-names>Jill</given-names> <surname>Barnholtz-Sloan</surname></string-name>, and <string-name><given-names>Quinn</given-names> <surname>Ostrom</surname></string-name>. <source>Radiology data from the cancer genome atlas low grade glioma [tcga-lgg] collection</source>, <year>2016</year>. <collab>Type: dataset</collab>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="other"><string-name><given-names>Roni</given-names> <surname>Zaharia</surname></string-name>. <source>Getting oriented using the image plane module</source>, <year>2013</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="other"><collab>Medical Imaging</collab>. <source>A matter of grayscale: Understanding dicom windows</source>, <year>2021</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="other"><collab>welcome to fastai &#x007C; fastai_2021</collab>, <year>2021</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="other"><string-name><given-names>Forrest N.</given-names> <surname>Iandola</surname></string-name>, <string-name><given-names>Song</given-names> <surname>Han</surname></string-name>, <string-name><given-names>Matthew W.</given-names> <surname>Moskewicz</surname></string-name>, <string-name><given-names>Khalid</given-names> <surname>Ashraf</surname></string-name>, <string-name><given-names>William J.</given-names> <surname>Dally</surname></string-name>, and <string-name><given-names>Kurt</given-names> <surname>Keutzer</surname></string-name>. <source>Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &#x003C;0.5mb model size</source>. <pub-id pub-id-type="arxiv">1602.07360</pub-id>, <year>2016</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>