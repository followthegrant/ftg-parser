<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.06.17.21259092</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Health Systems and Quality Improvement</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Evaluation of Domain Generalization and Adaptation on Improving Model Robustness to Temporal Dataset Shift in Clinical Medicine</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Guo</surname><given-names>Lin Lawrence</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Pfohl</surname><given-names>Stephen R</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Fries</surname><given-names>Jason</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Johnson</surname><given-names>Alistair</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Posada</surname><given-names>Jose</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Aftandilian</surname><given-names>Catherine</given-names></name>
<degrees>MD</degrees>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9385-7158</contrib-id>
<name><surname>Shah</surname><given-names>Nigam</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Sung</surname><given-names>Lillian</given-names></name>
<degrees>MD, PhD</degrees>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Program in Child Health Evaluative Sciences, The Hospital for Sick Children</institution>, Toronto, <country>Canada</country></aff>
<aff id="a2"><label>2</label><institution>Biomedical Informatics Research, Stanford University</institution>, Palo Alto, <country>US</country></aff>
<aff id="a3"><label>3</label><institution>Division of Haematology/Oncology, The Hospital for Sick Children</institution>, Toronto, <country>Canada</country></aff>
<aff id="a4"><label>4</label><institution>Division of Pediatric Hematology/Oncology, Stanford University</institution>, Palo Alto, <country>US</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label><bold>ADDRESS FOR CORRESPONDANCE</bold>: Lillian Sung MD, PhD, The Division of Haematology/Oncology, The Hospital for Sick Children, 555 University Avenue, Toronto, Ontario, M5G1X8, Telephone: 416-813-5287, Fax: 416-813-5979, Email: <email>lillian.sung@sickkids.ca</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.06.17.21259092</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>6</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>17</day>
<month>6</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>22</day>
<month>6</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="21259092.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<sec>
<title>Importance</title>
<p>Temporal dataset shift associated with changes in healthcare over time is a barrier to deploying machine learning-based clinical decision support systems. Algorithms that learn robust models by estimating invariant properties across time periods for domain generalization (DG) and unsupervised domain adaptation (UDA) might be suitable to proactively mitigate dataset shift.</p></sec>
<sec>
<title>Objective</title>
<p>To characterize the impact of temporal dataset shift on clinical prediction models and benchmark DG and UDA algorithms on improving model robustness.</p></sec>
<sec>
<title>Design, Setting, and Participants</title>
<p>In this cohort study, intensive care unit patients from the MIMIC-IV database were categorized by year groups (2008&#x2013;2010, 2011&#x2013;2013, 2014&#x2013;2016 and 2017&#x2013;2019). Tasks were predicting mortality, long length of stay, sepsis and invasive ventilation. Feedforward neural networks were used as prediction models. The baseline experiment trained models using empirical risk minimization (ERM) on 2008&#x2013;2010 (ERM[08-10]) and evaluated them on subsequent year groups. DG experiment trained models using algorithms that estimated invariant properties using 2008&#x2013;2016 and evaluated them on 2017&#x2013; 2019. UDA experiment leveraged unlabelled samples from 2017&#x2013;2019 for unsupervised distribution matching. DG and UDA models were compared to ERM[08-16] models trained using 2008-2016.</p></sec>
<sec>
<title>Main Outcome(s) and Measure(s)</title>
<p>Main performance measures were area-under-the-receiver-operating-characteristic curve (AUROC), area-under-the-precision-recall curve and absolute calibration error. Threshold-based metrics including false-positives and false-negatives were used to assess the clinical impact of temporal dataset shift and its mitigation strategies.</p></sec>
<sec>
<title>Results</title>
<p>In the baseline experiments, dataset shift was most evident for sepsis prediction (maximum AUROC drop, 0.090; 95&#x0025; confidence interval (CI), 0.080-0.101). Considering a scenario of 100 consecutively admitted patients showed that ERM[08-10] applied to 2017-2019 was associated with one additional false-negative among 11 patients with sepsis, when compared to the model applied to 2008-2010. When compared with ERM[08-16], DG and UDA experiments failed to produce more robust models (range of AUROC difference, &#x2212;0.003-0.050).</p></sec>
<sec>
<title>Conclusions and Relevance</title>
<p>DG and UDA failed to produce more robust models compared to ERM in the setting of temporal dataset shift. Alternate approaches are required to preserve model performance over time in clinical medicine.</p></sec>
<sec>
<title>KEY POINTS</title>
<sec>
<title>Question</title>
<p>Can algorithms that estimate invariant properties across environments for domain generalization and unsupervised domain adaptation improve the robustness of machine learning-derived clinical prediction models to temporal dataset shift&#x003F;</p></sec>
<sec>
<title>Findings</title>
<p>In this cohort study using 4 clinical outcomes, domain generalization and unsupervised domain adaptation algorithms did not meaningfully outperform the standard model training algorithm &#x2013; empirical risk minimization &#x2013; in learning robust models that generalize over time in the presence of temporal dataset shift.</p></sec>
<sec>
<title>Meaning</title>
<p>These findings highlight the difficulty of improving robustness to dataset shift with purely data-driven techniques that do not leverage prior knowledge of the nature of the shift and the requirement of alternate approaches to preserve model performance over time in clinical medicine.</p></sec>
</sec>
</abstract>
<kwd-group kwd-group-type="author">
<title>KEY WORDS</title>
<kwd>dataset shift</kwd>
<kwd>machine learning</kwd>
<kwd>clinical data</kwd>
<kwd>mimic</kwd>
</kwd-group>
<counts>
<page-count count="22"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>There was no funding support for this study.</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>As this study was conducted on de-identified electronic health records, Ethics Committee/Institutional Review Board approval was waived.</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>INTRODUCTION</title>
<p>The wide-spread adoption of electronic health records (EHRs) and the enhanced capacity to store and perform computation with large amounts of data have enabled the development of highly performant machine learning models for clinical outcome predictions.<sup><xref ref-type="bibr" rid="c1">1</xref></sup> The utility of these models critically depends on sustained performance to maintain safety,<sup><xref ref-type="bibr" rid="c2">2</xref></sup> end-users&#x2019; trust, and to outweigh the high cost of integrating each model into the clinical workflow.<sup><xref ref-type="bibr" rid="c3">3</xref></sup> However, this is hindered in the non-stationary healthcare environment by temporal dataset shift due to mismatch between the data distribution on which models were developed and the distribution to which models were applied.<sup><xref ref-type="bibr" rid="c4">4</xref></sup></p>
<p>There has been limited research on the impact of temporal dataset shift in clinical medicine.<sup><xref ref-type="bibr" rid="c5">5</xref></sup> Recent approaches largely relied on maintenance strategies consisting of performance monitoring, model updating and calibration over certain time intervals.<sup><xref ref-type="bibr" rid="c6">6</xref>-<xref ref-type="bibr" rid="c8">8</xref></sup> Another approach grouped clinical features into their underlying concepts to cope with a change in the record-keeping system.<sup><xref ref-type="bibr" rid="c9">9</xref></sup> Generally, these approaches either require detection of model degradation or rely on explicit knowledge or assumptions about the underlying cause of the shift. Complementary to these approaches would be ones that attempt to proactively produce robust models that incorporate relatively few assumptions on the nature of the shift.</p>
<p>The past decade of machine learning research offered numerous algorithms that learn robust models by using data from multiple environments to identify invariant properties. These algorithms were often developed for domain generalization (DG)<sup><xref ref-type="bibr" rid="c10">10</xref></sup> and unsupervised domain adaptation (UDA).<sup><xref ref-type="bibr" rid="c11">11</xref></sup> In the DG setting, the goal is to learn models that generalize to new environments unseen at training time. In the UDA setting, the goal is to adapt models to target environments using labeled samples from the source environment as well as a limited set of unlabeled samples from the target environment. If we consider EHR data across discrete time windows as related but distinct environments, then both DG and UDA settings may be suitable to combat the impact of temporal dataset shift. To date, these approaches have not been evaluated on improving model robustness to temporal dataset shift for clinical prediction tasks. Therefore, the objective was to benchmark learning algorithms for DG and UDA on mitigating the impact of temporal dataset shift on machine learning model performance in a set of clinical prediction tasks.</p>
</sec>
<sec id="s2">
<title>METHODS</title>
<sec id="s2a">
<title>Data Source</title>
<p>We used the MIMIC-IV database,<sup><xref ref-type="bibr" rid="c12">12</xref></sup> which contains deidentified EHRs of 382,278 patients admitted to an intensive care unit (ICU) or the emergency department at the Beth Israel Deaconess Medical Center (BIDMC) between 2008&#x2013;2019. For this cohort study, we considered ICU admissions sourced from the clinical information system MetaVision at the BIDMC, in which records from 53,150 patients were made available in the latest version of MIMIC-IV 1.0. Because of deidentification, the requirement for Institutional Review Board approval was waived.</p>
</sec>
<sec id="s2b">
<title>Cohort</title>
<p>Each patient&#x2019;s timeline in MIMIC-IV is anchored to a shifted (deidentified) year with which a <italic>year group</italic> and <italic>age</italic> are associated. The year group reflects the actual 3-year range (for example 2008&#x2013;2010) in which the shifted year occurred, and age reflects the patient&#x2019;s actual age in the shifted year. There are four available year groups in MIMIC-IV: 2008&#x2013;2010, 2011&#x2013;2013, 2014&#x2013;2016 and 2017&#x2013;2019. We included patients who were 18 years or older and randomly selected one ICU admission that occurred in the year group for each patient. As a result, each patient is represented once in our dataset and is associated with a single year group. We excluded ICU admissions less than 4 hours in duration.</p>
</sec>
<sec id="s2c">
<title>Outcomes</title>
<p>We defined four clinical outcomes. For each outcome, the task was to perform binary predictions over a time horizon with respect to the time of prediction, which was set as 4 hours after ICU admission. Long length of stay (<italic>Long LOS</italic>) was defined as ICU stay greater than three days from the prediction time. <italic>Mortality</italic> corresponded to in-hospital mortality within 7 days from the prediction time. <italic>Invasive ventilation</italic> corresponded to initiation of invasive ventilation within 24 hours from the prediction time. <italic>Sepsis</italic> corresponded to the development of sepsis according to the Sepsis-3 criteria<sup><xref ref-type="bibr" rid="c13">13</xref></sup> within 7 days from the prediction time. For invasive ventilation and sepsis, we excluded patients with these outcomes prior to the time of prediction. Further details on each outcome are presented in the <bold>eMethods</bold> in the Supplement.</p>
</sec>
<sec id="s2d">
<title>Features</title>
<p>Our feature extraction followed a common procedure<sup><xref ref-type="bibr" rid="c14">14</xref></sup> and obtained six categories of features including diagnoses, procedures, labs, prescriptions, ICU charts and demographics. Demographic features included age, biological sex, race, insurance, marital status and language. Clinical features were extracted over a set of time-intervals defined relative to the time of ICU admission as follows: 0-4 hours after ICU admission, 0-7 days prior, 7-30 days prior, 30-180 days prior, and 180 days-any time prior. For each time interval, we obtained counts of unique concept identifiers for diagnoses, procedures, prescriptions and labs with the exception that identifiers for diagnoses and procedures were not obtained in the 0-4 hours interval after admission as they were not available. We also obtained measurements for lab tests for each time interval, and measurements for chart events in the 0-4 hours interval after admission. In addition, we mapped each measurement variable in each time interval to the patient-level mean, minimum and maximum. Number of extracted features for each category and time interval are listed in the <bold>eMethods</bold> in the Supplement.</p>
<p>Feature preprocessing pruned features that had less than 25 patient observations, replaced non-zero count values with 1s, encoded measurement features to quintiles, and one-hot encoded all but count features. This process resulted in binary feature matrices that were extremely sparse. All feature preprocessing procedures were fit on the training set (e.g., to determine the boundaries of each quintile) and were subsequently applied to transform the validation and test sets.</p>
</sec>
<sec id="s2e">
<title>Model and Learning Algorithms</title>
<p>In all experiments, we leveraged fully connected feedforward neural network (NN) models for prediction, as they enable flexible learning for differentiable objectives across algorithms. The standard algorithm to learn models is the <italic>empirical risk minimization</italic> (ERM) algorithm in which the objective is to minimize average training error<sup><xref ref-type="bibr" rid="c15">15</xref></sup> without considerations of environment annotations (year groups in our case).</p>
<p>Algorithms for DG and UDA differ on the types of invariance they assume. <italic>Invariant risk minimization</italic> (IRM)<sup><xref ref-type="bibr" rid="c16">16</xref></sup> is a DG algorithm that learns a latent representation (i.e., hidden layer activations) where the optimal classifier leveraging that representation is the same for all environments. <italic>Group distributionally robust optimization</italic> (GroupDRO)<sup><xref ref-type="bibr" rid="c17">17</xref></sup> is a DG algorithm that does not &#x201C;learn&#x201D; invariances but instead minimizes training error in the worst-case training environment by increasing the importance of environments with larger errors. Algorithms that learn to match latent representation across environments can be leveraged for DG as well as UDA since these algorithms do not require outcome labels. These include <italic>correlation alignment</italic> (CORAL),<sup><xref ref-type="bibr" rid="c18">18</xref></sup> which seeks to match the mean and covariance of the distribution of the data encoded in the latent space across environments, and <italic>domain adversarial learning</italic> (AL),<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup> which matches the distributions using an adversarial network and an objective that minimizes discriminability between environments. The adversarial network used in this study is a NN model with one hidden layer of dimension 32.</p>
</sec>
<sec id="s2f">
<title>Model Development</title>
<p>We conducted baseline, DG and UDA experiments. The <italic>baseline</italic> experiment consisted of several aspects. First, to characterize temporal dataset shift on model performance, we trained models with ERM on the 2008&#x2013;2010 group (ERM[08-10]) and evaluated these models in each subsequent year group.</p>
<p>Next, to describe the extent of temporal dataset shift, we compared the performance of ERM[08-10] in each subsequent year group with models trained using ERM on that year group. Difference in performance in the target year group (2017-2019) between ERM[08-10] and models trained and evaluated on the target year group (ERM[17-19]) described the extent of temporal dataset shift in the extreme scenario in which models were developed on the earliest available data and were never updated. All models in the baseline experiment used ERM.</p>
<p>For <italic>DG</italic> and <italic>UDA</italic> experiments, model training was performed on 2008&#x2013;2016, with UDA also incorporating unlabelled samples from the target year group. Performance of DG and UDA models were compared with ERM models trained using 2008-2016 (ERM[08-16]) as these are the fairest ERM comparators for DG and UDA models. For models in the DG and UDA experiments, we focused on their performance in the target year group, but also described their performance in 2008-2016.</p>
<sec id="s2f1">
<title>Data splitting procedure</title>
<p>Data splitting procedure was performed separately for each task and experiment (see <bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>). The baseline experiment split each year group into 70&#x0025; training, 15&#x0025; validation and 15&#x0025; test sets. In DG and UDA experiments, the training set included 85&#x0025; data from 2008&#x2013;2010 and 2011&#x2013;2013, and 45&#x0025; of data from 2014&#x2013;2016. The validation set included 35&#x0025; of data from 2014&#x2013;2016 (chosen because of its temporal proximity to the target year group). The test set included the same 15&#x0025; from each year group as the baseline experiment, which allowed us to compare model performance across experiments and learning algorithms on the same patients. For UDA, training year groups were combined into one group, and unlabeled samples of various sizes (100, 500, 1000, and 1500) from the target year group (2017&#x2013;2019) were leveraged for unsupervised distribution matching.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Data Splitting Procedure for baseline, domain generalization (DG) and unsupervised domain adaptation (UDA) experiments. Different shades of the same color indicate that they were used to train or evaluate different models. For instance, in the baseline experiment, the training set of each year group was used to learn models for that year group. In the DG experiment, the training year groups were kept separate to allow DG algorithms to estimate invariance across the year groups. In comparison, in the UDA experiment, data from the training year groups were pooled, and unlabeled samples from the target year group were leveraged for unsupervised distribution matching between training and target year groups. In addition, ERM[08-16] models were learned on pooled data from the training year groups (2008-2016) to be used as ERM comparators for DG and UDA models.</p></caption>
<graphic xlink:href="21259092v1_fig1.tif"/>
</fig>
</sec>
<sec id="s2f2">
<title>Model training</title>
<p>We developed NN models on the training sets of each experiment for each task and selected hyperparameters based on performance in the validation sets. DG and UDA models used the same model hyperparameters as ERM[08-16], but involved an additional search over the algorithm-specific hyperparameter that modulated the impact of the algorithm on model learning. For all experiments, we trained 20 NN models using the selected hyperparameters for each combination of outcome, learning algorithm and experiment-specific characteristic (for example, year group in baseline experiment and size of unlabeled samples for UDA). Further details on the models, learning algorithms, as well as hyperparameter selection and model training procedures are presented in the <bold>eMethods</bold> in the Supplement.</p>
</sec>
<sec id="s2f3">
<title>Model evaluation</title>
<p>We evaluated models on the test sets of each experiment. Model performance was evaluated using area-under-receiver-operating-characteristic curve (AUROC), area-under-precision-recall curve (AUPRC) and the absolute calibration error (ACE).<sup><xref ref-type="bibr" rid="c21">21</xref></sup> ACE is a calibration measure similar to the integrated calibration index<sup><xref ref-type="bibr" rid="c22">22</xref></sup> in that it assesses overall model calibration by taking the average of the absolute deviations from an approximated perfect calibration curve. The difference is that ACE uses logistic regression for approximation instead of locally weighted regression such as LOESS.</p>
<p>To aid clinical interpretation of the impact of temporal dataset shift and its mitigation strategies, we translated change in performance to interpretable threshold-based metrics (including sensitivity and specificity) across clinically reasonable threshold levels. We chose the task with the most extreme temporal dataset shift. We setup a scenario with 100 hypothetical ICU patients and estimated the number of patients with and without a positive label using average prevalence from 2018 to 2019. We then illustrated the number of false-positive-(FP) and false-negative predictions (FN) for: (1) ERM[08-10] in 2008-2010, illustrating the results of initial model development with training and test sets in 2008-2010, and representing performance anticipated by clinicians applying the model to patients admitted in 2017-2019 if the model is not updated; (2) ERM[08-10] in 2017-2019, illustrating the actual performance of the earlier model on patients, or the impact of temporal dataset shift; (3) ERM[08-16] in 2017-2019, illustrating the ERM comparators for DG and UDA models; (4) models trained using a representative approach from DG or UDA; and (5) ERM[17-19] in which training and test sets are both using 2017-2019 data.</p>
</sec>
</sec>
<sec id="s2g">
<title>Statistical Analysis</title>
<p>For each combination of outcome, experiment-specific characteristic (e.g., year group in the baseline experiment), and evaluation metric (AUROC, AUPRC and ACE), we reported the median and 95&#x0025; confidence interval (CI) of the distribution over mean performance (across 20 NN models) in the test set obtained from 10,000 bootstrap iterations. To compare models (for example, learned using IRM vs. ERM[08-16]) in the target year group, metrics were computed over 10,000 bootstrap iterations and the resulting 95&#x0025; confidence interval of the differences were used to determine statistical significance.<sup><xref ref-type="bibr" rid="c23">23</xref></sup></p>
<p>Model training was performed on an Nvidia V100 GPU. Analyses were implemented in Python 3.8,<sup><xref ref-type="bibr" rid="c24">24</xref></sup> Scikit-learn 0.24<sup><xref ref-type="bibr" rid="c25">25</xref></sup> and Pytorch 1.7<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. The code for all analyses is open-source and available online<sup><xref rid="fn1" ref-type="fn">a</xref></sup>.</p>
</sec>
</sec>
<sec id="s3">
<title>RESULTS</title>
<p>Cohort characteristics for each year group and outcome are presented in <bold><xref rid="tbl1" ref-type="table">Table 1</xref>. <xref rid="fig2" ref-type="fig">Figure 2</xref></bold> shows performance measures (AUROC, AUPRC and ACE) of ERM[08-10] models in each year group vs. models trained on that year group. Largest temporal dataset shift was observed for sepsis predictions in 2017-2019 (drop in AUROC, 0.090; 95&#x0025; CI, 0.080-0.101).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Cohort Characteristics by Year Group</title></caption>
<graphic xlink:href="21259092v1_tbl1.tif"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Mean performance (AUROC, AUPRC, and ACE) of models in the baseline experiment. Solid blue lines depict models trained using 2008-2010 (ERM[08-10]) and evaluated in each year group. Dashed lines depict models trained and evaluated in each year group separately (comparators). Error bars indicate 95&#x0025; confidence interval obtained from 10,000 bootstrap iterations. Black circles indicate statistically significant differences in performance based on the 95&#x0025; confidence interval of the difference over 10,000 bootstrap iterations when comparing ERM[08-10] and comparators for each year group. The figure shows temporal dataset shift that is larger for Long LOS and Sepsis tasks. Abbreviations, ER: empirical risk minimization; LOS: length of stay; AUROC: area under the receiver operating characteristics curve; AUPRC: area under the precision recall curve; ACE: absolute calibration error.</p></caption>
<graphic xlink:href="21259092v1_fig2.tif"/>
</fig>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> illustrates change in the performance measures of DG and UDA models in the target year group (2017 &#x2013; 2019) relative to ERM[08-16]. In addition, change in performance measures of ERM[08-10] and ERM[17-19] are plotted in grey for comparison. ERM[08-16] performed better than ERM[08-10] (largest gain in AUROC, 0.049; 95&#x0025; CI, 0.041-0.057; <bold>eTable 1</bold> in the Supplement), but performed worse than ERM[17-19] (worst drop in AUROC, 0.071; 95&#x0025; CI, 0.062-0.081) with some exceptions in mortality and invasive ventilation predictions (<bold>eTable 2</bold> in the Supplement). Performance of DG and UDA models was similar to ERM[08-16] and while some models performed significantly better than ERM[08-16], others performed significantly worse with all differences being relatively small in magnitude (<bold>eTable 3</bold> in the Supplement). In addition, increasing the magnitude of the algorithm-specific hyperparameters of the DG and UDA algorithms did not result in performance gains (see <bold>e<xref rid="fig1" ref-type="fig">Figures 1</xref>, <xref rid="fig2" ref-type="fig">2</xref></bold>, and <bold><xref ref-type="fig" rid="fig3">3</xref></bold> in the Supplement).</p>
<p><bold><xref rid="tbl2" ref-type="table">Table 2</xref></bold> illustrates a clinical interpretation of temporal dataset shift in sepsis prediction using a scenario of 100 consecutively admitted patients to the ICU between 2017-2019 with a risk threshold of 10&#x0025; and an estimated outcome prevalence of 11&#x0025; (see <bold>eTable 5</bold> in the Supplement for results across thresholds from 5&#x0025; to 45&#x0025;). ERM[08-10] applied to 2017-2019 was associated with one additional FN among 11 patients with sepsis and 7 additional FP among 89 patients without sepsis when compared to the model applied to 2008-2010. FN with AL, as the representative mitigation approach, was similar to ERM[08-16].</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Clinical Interpretation of Temporal Dataset Shift in Sepsis Prediction<sup>a</sup></p></caption>
<graphic xlink:href="21259092v1_tbl2.tif"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Difference in mean performance of DG and UDA approaches relative to ERM[08-16] in the target year group (2017-2019). Performance of ERM[08-10] (train set 2008-2010 and test set 2017-2019, dashed line) and ERM[17-19] (train and test sets 2017-2019, solid line) models are also shown for comparison. Error bars indicate 95&#x0025; confidence interval obtained from 10,000 bootstrap iterations. Here, we show results from three of the four experimental conditions using differing number of unlabelled samples for UDA &#x2013; we did not observe meaningful differences across the number of unlabelled samples evaluated. Numerical representation of the performance measures relative to ERM[08-16] are presented in <bold>eTable 3</bold>.</p>
<p>Abbreviations, LOS: length of stay; ERM: empirical risk minimization; IRM: invariant risk minimization; AL: adversarial learning; GroupDRO: group distributionally robust optimization; CORAL: correlation alignment; AUROC: area under the receiver operating characteristics curve; AUPRC: area under the precision recall curve; ACE: absolute calibration error; domain generalization: DG; unsupervised domain adaptation: UDA.</p></caption>
<graphic xlink:href="21259092v1_fig3.tif"/>
</fig>
</sec>
<sec id="s4">
<title>DISCUSSION</title>
<p>Our results revealed heterogeneity in the impact of temporal dataset shift across clinical prediction tasks, with the largest impact on sepsis prediction. When compared to ERM[08-16], DG and UDA algorithms did not substantially improve model robustness. In some cases, DG and UDA algorithms produced less performant models than ERM. We also illustrated the impact of temporal dataset shift and the effect of mitigation approaches for clinical audiences so that they can determine whether the extent of dataset shift precludes utilization in practice.</p>
<p>The heterogeneity of impact by temporal dataset shift as revealed by our baseline experiment echoes the mixed results in model deterioration across several studies that made predictions of clinical outcomes in various populations.<sup><xref ref-type="bibr" rid="c5">5</xref></sup> This calls for careful investigation of potential model degradation due to temporal dataset shift at both the population and task level. In addition, these investigations should translate model degradation, typically measured as change in AUROC, into change in clinically relevant performance measures<sup><xref ref-type="bibr" rid="c27">27</xref></sup> or utility in allocation of resources,<sup><xref ref-type="bibr" rid="c28">28</xref></sup> and place its impact in the context of clinical decision making and downstream processes.<sup><xref ref-type="bibr" rid="c29">29</xref>-<xref ref-type="bibr" rid="c32">32</xref></sup></p>
<p>This study is one of the first to benchmark the capability of DG and UDA algorithms on EHR data across multiple clinical prediction tasks to mitigate the impact of temporal dataset shift. Our findings align with recent empirical evaluations of DG algorithms demonstrating that they do not outperform ERM under distribution shift across data sources or hospitals in real-world clinical datasets.<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup> The reasons underlying the failure of DG algorithms are topics of active research, with several recent works offering theory to explain why models derived with IRM and groupDRO are typically not more robust than ERM in practice.<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup> Furthermore, other work has demonstrated that UDA objectives based on distribution matching, such as AL and CORAL, failed to improve generalization to the target domain under shifts in the outcome rate or in the association between the outcome and features<sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>. These findings highlight the difficulty of improving robustness to dataset shift with purely data-driven techniques that do not leverage prior knowledge of the nature of the shift. Future research should explore methods that learn robust models by incorporating domain knowledge as to which causal mechanisms are likely to be stable or change across time<sup><xref ref-type="bibr" rid="c39">39</xref></sup>.</p>
<p>Strengths of this study include the use of multiple clinical outcomes and the illustration of temporal dataset shift and its mitigations using more clinically relevant metrics. There are several limitations in this study. First, the coarse characterization of temporal dataset shift did not offer insight about the rate at which model performance deteriorated. This was due to the deidentification in the MIMIC-IV database that left year group as the only time information that followed a correct chronological order across patients. Second, using data from the target year group to estimate best-case models is not realistic in real-world deployment as such data might not be available. Third, our assessment of clinical implications did not consider clinical use-cases in which the model alerts physicians of patients with the highest risks (i.e., acting on a threshold that is adaptively selected)<sup><xref ref-type="bibr" rid="c40">40</xref></sup>. In those scenarios, the amount of agreement in the ranking of risks between models need to be additionally considered.</p>
<p>In conclusion, DG and UDA failed to produce more robust models compared to ERM in the setting of temporal dataset shift. Alternate approaches are required to preserve model performance over time in clinical medicine.</p>
</sec>
<sec sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material>
<label>Supplemental Document</label>
<media xlink:href="supplements/259092_file02.docx" />
</supplementary-material>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>The data used for this study is publicly available. Instructions for access can be found at <ext-link ext-link-type="uri" xlink:href="https://mimic.mit.edu/iv/">https://mimic.mit.edu/iv/</ext-link>. The code for analyses is open source and available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sungresearch/mimic4ds_public">https://github.com/sungresearch/mimic4ds_public</ext-link>.</p>
</sec>
<ack>
<title>ACKNOWEDGEMENTS</title>
<p>LS is the Canada Research Chair in Pediatric Oncology Supportive Care.</p>
</ack>
<sec id="s5">
<title>AUTHOR CONTRIBUTIONS</title>
<p>Study concepts and design: All</p>
<p>Data acquisition: LLG, LS, AJ</p>
<p>Data analysis: LLG, SRP</p>
<p>Data interpretation: All</p>
<p>Drafting the manuscript or revising it critically for important intellectual content: All</p>
<p>Final approval of version to be published: All</p>
<p>Agreement to be accountable for all aspects of the work: All</p>
</sec>
<sec id="s6" sec-type="COI-statement">
<title>COMPETING INTERESTS</title>
<p>None</p>
</sec>
<sec id="s7">
<title>ETHICAL APPROVAL</title>
<p>As this study was conducted on de-identified electronic health records, Ethics Committee/Institutional Review Board approval was waived.</p>
</sec>
<sec id="s8">
<title>FUNDING</title>
<p>There was no funding support for this study</p>
</sec>
<ref-list>
<title>REFERENCES</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Rajkomar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Oren</surname> <given-names>E</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Scalable and accurate deep learning with electronic health records</article-title>. <source>NPJ Digit Med</source>. <year>2018</year>;<volume>1</volume>:<fpage>18</fpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Seneviratne</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>, <string-name><surname>Chu</surname> <given-names>L.</given-names></string-name> <article-title>Bridging the implementation gap of machine learning in healthcare</article-title>. <source>BMJ Innovations</source>. <year>2020</year>;<volume>6</volume>:<fpage>45</fpage>&#x2013;<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Sendak</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Balu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schulman</surname> <given-names>KA</given-names></string-name>. <article-title>Barriers to Achieving Economies of Scale in Analysis of EHR Data</article-title>. <source>A Cautionary Tale. Appl Clin Inform</source>. <year>2017</year>;<volume>8</volume>(<issue>3</issue>):<fpage>826</fpage>&#x2013;<lpage>831</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Moreno-Torres</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Raeder</surname> <given-names>T</given-names></string-name>, <string-name><surname>Alaiz-Rodr&#x00ED;guez</surname> <given-names>R</given-names></string-name>, <string-name><surname>Chawla</surname> <given-names>NV</given-names></string-name>, <string-name><surname>Herrera</surname> <given-names>F.</given-names></string-name> <article-title>A unifying view on dataset shift in classification</article-title>. <source>Pattern Recognit</source>. <year>2012</year>;<volume>45</volume>(<issue>1</issue>):<fpage>521</fpage>&#x2013;<lpage>530</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="other"><string-name><surname>Guo</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Pfohl</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal> <source>Systematic Review of Approaches to Preserve Machine Learning Performance in the Presence of Temporal Dataset Shift in Clinical Medicine. In</source>:<year>2021</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Davis</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Greevy</surname> <given-names>RA</given-names>, <suffix>Jr</suffix>.</string-name>, <string-name><surname>Lasko</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Walsh</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Matheny</surname> <given-names>ME</given-names></string-name>. <article-title>Detection of calibration drift in clinical prediction models to inform model updating</article-title>. <source>J Biomed Inform</source>. <year>2020</year>;<volume>112</volume>:<fpage>103611</fpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Davis</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Greevy</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Fonnesbeck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lasko</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Walsh</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Matheny</surname> <given-names>ME</given-names></string-name>. <article-title>A nonparametric updating method to correct clinical prediction model drift</article-title>. <source>J Am Med Inform Assoc</source>. <year>2019</year>;<volume>26</volume>(<issue>12</issue>):<fpage>1448</fpage>&#x2013;<lpage>1457</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Siregar</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nieboer</surname> <given-names>D</given-names></string-name>, <string-name><surname>Versteegh</surname> <given-names>MIM</given-names></string-name>, <string-name><surname>Steyerberg</surname> <given-names>EW</given-names></string-name>, <string-name><surname>Takkenberg</surname> <given-names>JJM</given-names></string-name>. <article-title>Methods for updating a risk prediction model for cardiac surgery: a statistical primer</article-title>. <source>Interact Cardiovasc Thorac Surg</source>. <year>2019</year>;<volume>28</volume>(<issue>3</issue>):<fpage>333</fpage>&#x2013;<lpage>338</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="other"><string-name><surname>Nestor</surname> <given-names>B</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>MBA</given-names></string-name>, <string-name><surname>Boag</surname> <given-names>W</given-names></string-name>, <etal>et al.</etal> <article-title>Feature Robustness in Non-stationary Health Records: Caveats to Deployable Model Performance in Common Clinical Machine Learning Tasks</article-title>. <source>Proceedings of the 4th Machine Learning for Healthcare Conference</source>; <year>2019</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="website"><string-name><surname>Zhou</surname> <given-names>K</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Qiao</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Xiang</surname> <given-names>T</given-names></string-name>, <string-name><surname>Loy</surname> <given-names>CC</given-names></string-name>. <article-title>Domain Generalization: A Survey</article-title>. <source>arXiv</source>. <year>2021</year>:<fpage>1</fpage>&#x2013;<lpage>21</lpage>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.02503">https://arxiv.org/abs/2103.02503</ext-link>. xPublished 31 Mar 2021. <date-in-citation content-type="access-date">Accessed 05 May 2021</date-in-citation>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="website"><string-name><surname>Wilson</surname> <given-names>G</given-names></string-name>, <string-name><surname>Cook</surname> <given-names>DJ</given-names></string-name>. <article-title>A Survey of Unsupervised Deep Domain Adaptation</article-title>. <source>ArXiv</source>. <year>2020</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.02849">https://arxiv.org/abs/1812.02849</ext-link>. <date-in-citation content-type="access-date">Accessed 21 May 21</date-in-citation>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="website"><collab>MIMIC-IV</collab>. <source>PhysioNet</source>; <year>2021</year>. <ext-link ext-link-type="uri" xlink:href="https://physionet.org/content/mimiciv/1.0/">https://physionet.org/content/mimiciv/1.0/</ext-link>. <date-in-citation content-type="access-date">Accessed 21 May 2021</date-in-citation>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Singer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Deutschman</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Seymour</surname> <given-names>CW</given-names></string-name>, <etal>et al.</etal> <article-title>The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3)</article-title>. <source>JAMA</source>. <year>2016</year>;<volume>315</volume>(<issue>8</issue>):<fpage>801</fpage>&#x2013;<lpage>810</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Reps</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Schuemie</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Suchard</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Ryan</surname> <given-names>PB</given-names></string-name>, <string-name><surname>Rijnbeek</surname> <given-names>PR</given-names></string-name>. <article-title>Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data</article-title>. <source>J Am Med Inform Assoc</source>. <year>2018</year>;<volume>25</volume>(<issue>8</issue>):<fpage>969</fpage>&#x2013;<lpage>975</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Varnik</surname> <given-names>V.</given-names></string-name> <article-title>Principles of risk minimization for learning theory</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>4</volume>; <year>1991</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="website"><string-name><surname>Arjovsky</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bottou</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gulrajani</surname> <given-names>I</given-names></string-name>, <string-name><surname>Lopez-Paz</surname> <given-names>D.</given-names></string-name> <article-title>Invariant Risk Minimization</article-title>. <source>ArXiv</source>. <year>2020</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.02893">https://arxiv.org/abs/1907.02893</ext-link>. xPublished 27 Mar 2020. <date-in-citation content-type="access-date">Accessed 21 May 21</date-in-citation>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="website"><string-name><surname>Sagawa</surname> <given-names>S</given-names></string-name>, <string-name><surname>Koh</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Hashimoto</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>P.</given-names></string-name> <article-title>Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization</article-title>. <source>ArXiv</source>. <year>2020</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1911.08731">https://arxiv.org/abs/1911.08731</ext-link>. xPublished 02 Apr 2020. <date-in-citation content-type="access-date">Accessed 21 May 21</date-in-citation>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="other"><string-name><surname>Sun</surname> <given-names>B</given-names></string-name>, <string-name><surname>Saenko</surname> <given-names>K.</given-names></string-name> <article-title>Deep CORAL: Correlation Alignment for Deep Domain Adaptation</article-title>. <source>European conference on computer vision; 2016; University of Massachusetts Lowell, Boston University</source>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Ganin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Ustinova</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ajakan</surname> <given-names>H</given-names></string-name>, <etal>et al.</etal> <article-title>Domain-Adversarial Training of Neural Networks</article-title>. <source>J Mach Learn Res</source>. <year>2016</year>;<volume>17</volume>:<fpage>1</fpage>&#x2013;<lpage>35</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="other"><string-name><surname>Pfohl</surname> <given-names>S</given-names></string-name>, <string-name><surname>Marafino</surname> <given-names>B</given-names></string-name>, <string-name><surname>Coulet</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rodriguez</surname> <given-names>F</given-names></string-name>, <string-name><surname>Pala-niappan</surname> <given-names>L</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>. <article-title>Creating Fair Models of Atherosclerotic Cardiovascular Disease</article-title>. <source>AAAI/ACM Conference on AI, Ethics, and Society (AIES&#x2019;19)</source>; <year>2019</year>; Honolulu, HI, USA.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Pfohl</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Foryciarz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>. <article-title>An empirical characterization of fair machine learning for clinical risk prediction</article-title>. <source>J Biomed Inform</source>. <year>2021</year>;<volume>113</volume>:<fpage>103621</fpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Austin</surname> <given-names>PC</given-names></string-name>, <string-name><surname>Steyerberg</surname> <given-names>EW</given-names></string-name>. <article-title>The Integrated Calibration Index (ICI) and related metrics for quantifying the calibration of logistic regression models</article-title>. <source>Stat Med</source>. <year>2019</year>;<volume>38</volume>(<issue>21</issue>):<fpage>4051</fpage>&#x2013;<lpage>4065</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="book"><string-name><surname>Efron</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tibshirani</surname> <given-names>R.</given-names></string-name> <source>An introduction to the bootstrap</source>. <publisher-loc>New York: Chapman &#x0026; Hall</publisher-loc>; <year>1993</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="website"><string-name><surname>Van Rossum</surname> <given-names>G</given-names></string-name>, <string-name><surname>Drake</surname> <given-names>F.</given-names></string-name> <source>Python Language Reference, version 3.8</source>. <ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link> <date-in-citation content-type="access-date">Accessed 21 May, 2021</date-in-citation>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J Mach Learn Res</source>. <year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>&#x2013;<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>S</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <etal>et al.</etal> <article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>. <source>NeurIPS</source>. <year>2019</year>;<volume>32</volume>:<fpage>8024</fpage>&#x2013;<lpage>8035</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>, <string-name><surname>Milstein</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bagley Ph</surname> <given-names>DS</given-names></string-name>. <article-title>Making Machine Learning Models Clinically Useful</article-title>. <source>JAMA</source>. <year>2019</year>;<volume>322</volume>(<issue>14</issue>):<fpage>1351</fpage>&#x2013;<lpage>1352</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="other"><string-name><surname>Ko</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>E</given-names></string-name>, <string-name><surname>Agrawal</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Improving Hospital Readmission Prediction using Individualized Utility Analysis</article-title>. <source>Journal of Biomedical Informatics</source>. <year>2021</year>:<fpage>103826</fpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Char</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>, <string-name><surname>Magnus</surname> <given-names>D.</given-names></string-name> <article-title>Implementing Machine Learning in Health Care - Addressing Ethical Challenges</article-title>. <source>N Engl J Med</source>. <year>2018</year>;<volume>378</volume>(<issue>11</issue>):<fpage>981</fpage>&#x2013;<lpage>983</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Morse</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Bagley</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>. <article-title>Estimate the hidden deployment cost of predictive models to improve patient care</article-title>. <source>Nature Medicine</source>. <year>2020</year>;<volume>26</volume>(<issue>1</issue>):<fpage>18</fpage>&#x2013;<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="other"><string-name><surname>Liu</surname> <given-names>VX</given-names></string-name>, <string-name><surname>Bates</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Wiens</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>. <article-title>The number needed to benefit</article-title>: <source>estimating the value of predictive analytics in healthcare. (1527-974X (Electronic)</source>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Asch</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>NH</given-names></string-name>. <article-title>Developing a delivery science for artificial intelligence in healthcare</article-title>. <source>npj Digital Medicine</source>. <year>2020</year>;<volume>3</volume>(<issue>1</issue>):<fpage>107</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="website"><string-name><surname>Koh</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Sagawa</surname> <given-names>S</given-names></string-name>, <string-name><surname>Marklund</surname> <given-names>H</given-names></string-name>, <etal>et al.</etal> <article-title>WILDS: A Benchmark of in-the-Wild Distribution Shifts</article-title>. <source>ArXiv</source>. <year>2021</year>:<fpage>1</fpage>&#x2013;<lpage>87</lpage>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2012.07421">https://arxiv.org/abs/2012.07421</ext-link>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="other"><string-name><surname>Zhang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Dullerud</surname> <given-names>N</given-names></string-name>, <string-name><surname>Seyyed-Kalantari</surname> <given-names>L</given-names></string-name>, <string-name><surname>Morris</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Joshi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ghassemi</surname> <given-names>M.</given-names></string-name> <article-title>An empirical framework for domain generalization in clinical settings</article-title>. <source>ACM Conference on Health, Inference, and Learning (ACM CHIL&#x2019;21)</source>; <year>2021</year>; Virtual Event.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="website"><string-name><surname>Rosenfeld</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ravikumar</surname> <given-names>P</given-names></string-name>, <string-name><surname>Risteski</surname> <given-names>A.</given-names></string-name> <article-title>The Risks of Invariant Risk Minimization</article-title>. <source>ArXiv</source>. <year>2020</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.05761">https://arxiv.org/abs/2010.05761</ext-link>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="other"><string-name><surname>Rosenfeld</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ravikumar</surname> <given-names>P</given-names></string-name>, <string-name><surname>Risteski</surname> <given-names>A.</given-names></string-name> <article-title>An Online Learning Approach to Interpolation and Extrapolation in Domain Generalization</article-title>. <source>arXiv</source>. <year>2021</year>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="other"><string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Winston</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kaushik</surname> <given-names>D</given-names></string-name>, <string-name><surname>Lipton</surname> <given-names>Z.</given-names></string-name> <article-title>Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment</article-title>. <source>Proceedings of the 36th International Conference on Machine Learning</source>; <year>2019</year>; Proceedings of Machine Learning Research.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="other"><string-name><surname>Zhao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Combes</surname> <given-names>RTD</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>G.</given-names></string-name> <article-title>On Learning Invariant Representations for Domain Adaptation</article-title>. <source>36th International Conference on Machine Learning, ICML 2019</source>; <year>2019</year>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="other"><string-name><surname>Subbaswamy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schulam</surname> <given-names>P</given-names></string-name>, <string-name><surname>Saria</surname> <given-names>S.</given-names></string-name> <article-title>Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport. Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</article-title>; <year>2019</year>; <source>Proceedings of Machine Learning Research</source>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Manz</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Parikh</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Small</surname> <given-names>DS</given-names></string-name>, <etal>et al.</etal> <article-title>Effect of Integrating Machine Learning Mortality Estimates With Behavioral Nudges to Clinicians on Serious Illness Conversations Among Patients With Cancer: A Stepped-Wedge Cluster Randomized Clinical Trial</article-title>. <source>JAMA Oncol</source>. <year>2020</year>;<volume>6</volume>(<issue>12</issue>):<fpage>e204759</fpage>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1">
<label><sup>a</sup></label>
<p><ext-link ext-link-type="uri" xlink:href="https://github.com/sungresearch/mimic4ds_public">https://github.com/sungresearch/mimic4ds_public</ext-link></p>
</fn>
</fn-group>
</back>
</article>