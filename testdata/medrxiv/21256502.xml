<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.05.04.21256502</article-id>
<article-version>1.2</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Dentistry and Oral Medicine</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>An Effective and Reliable Methodology for Deep Machine Learning Application in Caries Detection</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6878-1155</contrib-id>
<name><surname>Huang</surname><given-names>Yu-Ping</given-names></name>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9880-2372</contrib-id>
<name><surname>Lee</surname><given-names>Shyh-Yuan</given-names></name>
<xref ref-type="corresp" rid="cor1">&#x2020;</xref>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Dentistry, National Yang-Ming University</institution></aff>
<aff id="a2"><label>2</label><institution>Department of Stomatology, Taipei Veterans General Hospital</institution></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x2020;</label><email>sylee@ym.edu.tw</email></corresp>
<fn id="n1" fn-type="others"><label>&#x002A;</label><p><email>asd3926782@gm.ym.edu.tw</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.05.04.21256502</elocation-id>
<history>
<date date-type="received">
<day>04</day>
<month>5</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>11</day>
<month>5</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>5</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="21256502.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Early detection of dental caries has been one of the most predominant topics studied over the last few decades. Conventional examination through visual-tactile inspection and radiography can be inaccurate and destructive to teeth structure. The development of Optical Coherence Tomography (OCT) has given dentistry an alternative diagnostic technique, which has been proven by numerous studies, that it has better sensitivity, specificity, and non-invasive characteristics. The growing popularity of Artificial Intelligence (AI) also contributes to a more efficient and effective way of image-based detection and decision-making. Previous studies, which have attempted to employ AI for caries assessment, did not incorporate high-quality data. Hence, they were unable to produce valid and reliable results. This study highlights the importance of high-quality data and aims to bypass this issue, by implementing an improved methodology to the automated detection and diagnosis of dental caries depending on AI. A two-phase study was carried out to explore different methods for caries detection. Initially OCT was verified, by surveying experienced clinicians, to be a better imaging technique compared to radiography. Then, our study showed that Convolutional Neural Networks (CNNs) in the scope of AI surpassed the accuracy of human clinicians. The data was preprocessed and labelled with the ground truth corresponding to Micro-CT with rigorous definition. Statistical analysis performed was mainly based on weighted Kappa coefficient. The results suggested that OCT (<italic>&#x03BA;</italic> &#x003D; .699<italic>, SD</italic> &#x003D; .090) showed a higher accuracy than radiography (<italic>&#x03BA;</italic> &#x003D; .407<italic>, SD</italic> &#x003D; .049) and CNNs (<italic>&#x03BA;</italic> &#x003D; .860<italic>, SD</italic> &#x003D; .049) were rated higher than clinicians (<italic>&#x03BA;</italic> &#x003D; .679; <italic>SD</italic> &#x003D; .113), both within a .05 significance. The best result was carried out by ResNet-152, concluding diagnostic accuracy to be 95.21&#x0025; and sensitivity 98.85&#x0025;. The improved methodology of this study hopes to pave the way for future studies in AI application in Dentistry.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Dental caries</kwd>
<kwd>Caries detection</kwd>
<kwd>Machine learning</kwd>
<kwd>Deep learning</kwd>
<kwd>Oral diagnosis</kwd>
<kwd>Artificial intelligence</kwd>
<kwd>Deep convolutional neural networks</kwd>
<kwd>Optical coherence tomography</kwd>
</kwd-group>
<counts>
<page-count count="15"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>This research was partially supported by the Ministry of Science and Technology (MOST 108-2813-C-010-040-B) and the Department of Health, Taipei City Government (10901-62-017).</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>The study was approved by the Institutional Review Board of National Yang-Ming University (YM109025E).</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Dental caries is a widespread disease that people of all ages experience; it affects almost 80 percent of children and 90 percent of adults (<xref ref-type="bibr" rid="c1">Bader et al., 2001</xref>). Lesions begin from the demineralization of tooth surfaces that eventually evolve into larger cavities and pulpitis. Therefore, an effective modality for the early detection of dental caries is an essential topic in dental research (<xref ref-type="bibr" rid="c30">Zandon&#x00E1; and Zero, 2006</xref>). Over the past several decades, the most common approach for identifying caries lesions has been through visual and tactile examinations with the help of radiography and dental explorers (Haak et al., 2002). Inspection with radiographic imaging suggests locations of suspicious lesions, followed by tactile examination helping clinicians access carious lesions directly and more accurately. Unfortunately, there are several drawbacks to this methodology. Firstly, X-ray imaging provides 2D graphs for 3D tooth structures; this means it makes tracing subtle lesions virtually impossible, especially on overlapping structures, including lesions on occlusal, buccal, and lingual surfaces. Thus, intraoral radiography has a poor diagnostic rate of 21&#x0025; for enamel lesions and 44&#x0025; for dentin lesions (<xref ref-type="bibr" rid="c17">Molander et al., 1993</xref>). Secondly, tactile examination is experiential and relies on intuition, meaning that there is no quantifiable standard. Furthermore, several studies have indicated that tactile examination may cause irreversible damage to enamel structures (<xref ref-type="bibr" rid="c5">Ekstrand et al., 1987</xref>). Finally, the <xref ref-type="bibr" rid="c28">United Nations Scientific Committee on the Effects of Atomic Radiation (2017)</xref> also suggests that excessive radiation exposure on infants, children and adolescents should be avoided, as it can pose more risk than adulthood exposure (e.g. radiogenic tumor induction, cognitive defects, neuroendocrine abnormalities).</p>
<p>Thus far, there have been an array of advanced imaging devices developed for intraoral caries diagnosis, one of which is the Optical Coherence Tomography (OCT). The OCT is a diagnostic tool that relies on an optical interference mechanism (<xref ref-type="bibr" rid="c8">Fercher et al., 2003</xref>). There are several advantages that OCT has over traditional radiography. Firstly, data collected from a single OCT procedure consist of multiple scans, in sections, that can be constructed to create a 3D model without interference from adjacent structures, e.g. the alveolar bone surrounding a tooth. Secondly, the high-contrast resolution of OCT scans allows different tissue densities to be distinguished; hence, the possibility of tracing the caries progression is raised. Lastly, since OCT uses low-coherence broadband light instead of tactile movements and X-rays, the risks of tissue damage and radiation exposure are alleviated. Studies have increasingly shown their preference for OCT scans for intraoral caries diagnosis, due to its non-invasive nature and higher sensitivity, compared to radiographs and visual-tactile inspection (<xref ref-type="bibr" rid="c16">Luong et al., 2020</xref>; <xref ref-type="bibr" rid="c21">Schneider et al., 2020</xref>).</p>
<p>The rapid growth of Artificial Intelligence (AI), especially Convolutional Neural Network (CNN) in the scope of deep machine learning, reveals an exceptionally suitable method for analyzing and classifying images (<xref ref-type="bibr" rid="c10">Goodfellow et al., 2016</xref>). A recent study has also pointed out that these novel algorithms can bring a cost-effective solution to disease detection (<xref ref-type="bibr" rid="c22">Schwendicke et al., 2020</xref>). Therefore, it has become increasingly popular in medical analysis. For example, it was implemented in the classification of diabetic retinopathy, as well as, in the detection of cancerous tumor cells (<xref ref-type="bibr" rid="c15">Litjens et al., 2017</xref>). These applications have helped CNNs develop a reputation for high recognition rates, accuracy, and efficiency.</p>
<p>As an increasing amount of innovative AI research has been published, a popular saying, &#x201C;garbage in, garbage out,&#x201D; has emerged alerting researchers to pay attention to the quality of data and the reliability of methodology (<xref ref-type="bibr" rid="c19">Rockall, 2020</xref>). Recent studies that have attempted to apply CNN algorithms to caries detection have faced limitations. The limitations include labelling carious or non-carious teeth based solely on radiography, as seen in studies done by <xref ref-type="bibr" rid="c14">Lee et al. (2018)</xref> and <xref ref-type="bibr" rid="c26">Srivastava et al. (2017)</xref>, while radiography has been proved with a poor diagnostic rate. <xref ref-type="bibr" rid="c20">Salehi et al. (2019)</xref>, on the other hand, performed experiments using OCT images without clarifying and addressing how lesions were defined and labeled. Additionally, all above-mentioned studies solely emphasized the detection of caries without considering the &#x201C;depths&#x201D; of the lesions. When in fact, effective clinical management is dependent on the awareness of lesion depth (<xref ref-type="bibr" rid="c1">Bader et al., 2001</xref>). Lesions limited to the enamel layer require no treatment or merely plaque control; In contrast, operative treatments are required when demoralization and cavities develop in the middle or inner third of the dentin (<xref ref-type="bibr" rid="c7">Ekstrand et al., 2001</xref>). Hence, it is vital that further differentiation between lesion seriousness needs to be addressed (<xref ref-type="bibr" rid="c9">Frencken et al., 2012</xref>).</p>
<p>Overall, past studies which have attempted to apply CNN algorithms to the detection of caries have lacked validity and reliability on methodology. The impracticable classification also limited the potential for clinical decision-making. The objective of this study is to suggest a practical approach to develop a more reliable methodology that produces a more detailed classification of lesions. The aims of our paper are:</p>
<list list-type="alpha-lower">
<list-item><p>to establish a valid and reliable method for classifying carious lesions;</p></list-item>
<list-item><p>to provide quantitative research on diagnostic rates of radiography and OCT; and</p></list-item>
<list-item><p>to compare the diagnostic outcomes between experienced clinicians and our CNN models.</p></list-item>
</list>
</sec>
<sec id="s2">
<title>Materials and Method</title>
<sec id="s2a">
<title>Imaging Techniques</title>
<p>A self-developed swept-source OCT (SS-OCT) was used in this study. The system operated at a center wavelength of 1310 nm with an average power of 40 mW, a scan rate of 50kHz, and a frame rate of 160 fps. Each frame contained 200 scanning lines, and the resolution of each 2D image was 250 &#x00D7; 1024 <italic>px</italic>. Further detailed illustration, settings, and mechanisms are shown in <xref ref-type="sec" rid="s8">Appendix A</xref>.</p>
<p>The periapical films of the teeth samples were acquired with ScanX Intraoral Phosphor Plates (Air Techniques Inc., USA) and PY-70M Intraoral Imaging Systems (POYE Inc., Taiwan), operating with 70 <italic>kV p</italic> tube potential, 10 <italic>mA</italic> tube current, and 0.7 &#x00D7; 0.7 mm focus point. The films were processed with ScanX-Duo D1000F Digital Radiography System, exported in <italic>.tiff</italic> format with the resolution of 652 &#x00D7; 801 <italic>px</italic>.</p>
<p>Micro-CT is a device used for assessing dental caries by analyzing the mineral contents of teeth through non-destructive characteristics (<xref ref-type="bibr" rid="c27">Swain and Xue, 2009</xref>). The device is currently considered one of the best for assessment, even though it still relies on clinician judgement and verification. Therefore, Micro-CT was chosen to be the judgment basis in this study, as it can reach a deeper depth than OCT scans and present high-resolution data. The Micro-CT used in this study was High-Resolution U-CT (Milabs, Netherlands), operating with 50 <italic>kV</italic> tube voltage, 0.48 <italic>mA</italic> tube current, and 10 <italic>&#x00B5;m</italic> resolution.</p>
</sec>
<sec id="s2b">
<title>Convolutional Neural Network (CNN)</title>
<p>CNN is well-known for its powerful capability for image processing. Instead of conventional architectures that connect all the perceptrons costing tremendous computational time and power, CNN adopts a more efficient method by recognizing hierarchical patterns. Detailed CNN basics can be further observed in <xref ref-type="sec" rid="s9">Appendix B</xref>.</p>
<p>Throughout this study, using CNN algorithms for the detection and classification of caries were our main technique of interest. To gauge its effectiveness, the results were compared to experienced clinician detection and classification of caries. This allows us to determine the feasibility of using CNNs as assistance in differential diagnoses. A high recognition rate, accuracy, and efficiency were hypothesized. The CNN models adopted in our study were: AlexNet (<xref ref-type="bibr" rid="c12">Krizhevsky et al., 2012</xref>), VGG-16 (<xref ref-type="bibr" rid="c25">Simonyan and Zisserman, 2014</xref>), ResNet-152 (<xref ref-type="bibr" rid="c11">He et al., 2016</xref>), Xception (<xref ref-type="bibr" rid="c4">Chollet, 2016</xref>), and ResNeXt-101 (<xref ref-type="bibr" rid="c29">Xie et al., 2017</xref>), which are the classic and best models for image recognition over the recent years. The CNN model architectures adopted in our study remained the same as their original published versions. Additionally, the PyTorch framework (<xref ref-type="bibr" rid="c18">Paszke et al., 2019</xref>), based on Python programming language, was used in this study as the implementation of deep learning models.</p>
</sec>
<sec id="s2c">
<title>Data Collection</title>
<p>The study was approved by the Institutional Review Board of National Yang-Ming University (YM109025E). Sixty-three teeth with different levels of caries lesion were collected. Fractured teeth, previously restored teeth, and teeth with obvious cavities that can be visually differentiated, were not included. The teeth were immersed in 5&#x0025; sodium hypochlorite (NaOCl) for a week for disinfection and then stored in distilled water. For each tooth, we took one micro-CT scan as the reference, one periapical film and one scan of OCT for our research. The periapical films were clipped into a unified size (591 &#x00D7; 421 <italic>px</italic>); on the other hand, the OCT data was converted via our self-developed software to 200 sections <italic>.bmp</italic> images, with a unified size (420 &#x00D7; 209 <italic>px</italic>).</p>
<p>All the experimental data required classification. The micro-CT data and periapical films were coded according to a simplified version, shown in <xref rid="tbl1" ref-type="table">Table 1</xref>, of the widely-used criterion for radiographic examination formulated by <xref ref-type="bibr" rid="c6">Ekstrand et al. (1997)</xref> (see <xref ref-type="sec" rid="s10">Appendix C</xref>). Our self-developed criterion used for classifying the OCT images is also described in <xref rid="tbl1" ref-type="table">Table 1</xref>, which was based on the criterion developed by <xref ref-type="bibr" rid="c24">Shimada et al. (2010)</xref> (see <xref ref-type="sec" rid="s11">Appendix D</xref>). The implementation of rigorous labelling assured the codes of all experimental data to be mutually exclusive and independent. Supplemental pictures are shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Criteria of carious status classification in radiography and OCT</p></caption>
<graphic xlink:href="21256502v2_tbl1.tif"/>
</table-wrap>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Images of three different image data. (a) is the micro-CT data of a sample tooth in coronal view. (b)(c)(d)(e) correspond to four different segment as illustrated in (a). (f) is the periapical film of the tooth. (g)(h)(i)(j) are the OCT counterparts to (b)(c)(d)(e) of (a). Carious lesions are indicated at the points of red arrows, where discontinuous enamel surface and intensified OCT signals. With the corresponding criteria, (b)(c)(g)(h) are coded as 2 for dentine caries with an intensified OCT signal beyond the EDJ. (d)(f)(i) are coded as &#x27E8;1&#x27E9; because of the superficial demineralization and enamel breakdown. And (e)(j) are coded as &#x27E8;0&#x27E9;, represent intact tooth sections without any caries.</p></caption>
<graphic xlink:href="21256502v2_fig1.tif"/>
</fig>
</sec>
<sec id="s2d">
<title>Procedures</title>
<p>A two-phase study was conducted to explore different methods of carious examination. The first section aimed to verify the accuracy and effectiveness of OCT scans. To prove the hypothesis that the diagnostic capabilities of OCT images were better than periapical films. In the second section, the study shifted our objective to the comparison of interpreters, clinicians and CNN algorithms, to see if CNN algorithms were comparable with clinicians or even better in diagnosis. A supplemental flowchart is depicted in <xref ref-type="sec" rid="s12">Appendix E</xref>.</p>
<sec id="s2d1">
<title>Phase 1: Comparison of Diagnostic Capability of OCT versus Radiography</title>
<p>We took periapical films and OCT data of the sample teeth and asked five well-educated clinicians to examine the images. The clinicians were asked to make judgments of carious status individually for each tooth based on both types of data, following the criteria shown in <xref rid="tbl1" ref-type="table">Table 1</xref>. Afterward, their answers were scored and evaluated with the corresponding ground truth obtained by Micro-CT. The weighted Kappa coefficient was then calculated. Since this measurement took the possibility of the accidental match and the correlation in lesion extent into account, it is more reliable and convincing than pure accuracy (<xref ref-type="bibr" rid="c2">Ben-David, 2008</xref>; <xref ref-type="bibr" rid="c3">Chmura Kraemer et al., 2002</xref>).</p>
<p>The participants in this study were five resident doctors in the Dental Department of Taipei Veterans General Hospital. Before participating, they were informed that our study was interested in determining whether OCT or Radiography is better at caries detection; they were, however, not told what types of results were expected. The unit of comparison was a &#x201C;tooth&#x201D; to ensure some homogeneity between the two different data types.</p>
<p>Given the inherent disadvantages of radiographs compared to multi-sectional OCT data, if the clinicians asked for more images for confirmation, we provide additional shifted images with the commonly-used tube-shift technique (<xref ref-type="bibr" rid="c23">Seiler et al., 2018</xref>). Additionally, if there were any discrepancies between clinicians&#x2019; diagnoses of radiographs, a discussion was allowed, and a correction based on the mutual agreement was noted. To avoid any possible misunderstanding, the answer given under such circumstances was recorded independently and denoted as &#x201C;CR&#x201D; (Consensus on Radiography). This methodology was designed to encourage the best radiography diagnosis. However, clinicians were not allowed to discuss their diagnosis when using the OCT scans and were not given any supplemental data. Hence, if the results still show that OCT has superiority over radiography, it may indicate that OCT is the more effective method of caries detection.</p>
</sec>
<sec id="s2d2">
<title>Phase 2: Comparison of Diagnostic Ability of clinicians versus CNNs</title>
<p>The OCT data was once again given to five clinicians and processed using the five above-mentioned CNN models to see if there was a significant difference in their ability to detect caries.</p>
<p>Since the CNNs required a large database, the unit of the sample was changed from &#x201C;tooth&#x201D; to &#x201C;image&#x201D;. Therefore, a total of 748 cross-sectional 2D images were extracted from 63 OCT tooth data, which were then labeled into three distinct groups according to <xref rid="tbl1" ref-type="table">Table 1</xref>: 470 images with 0, 174 images with 1, and 104 images with the code 2. Images were divided into 599 (80&#x0025;) training data and 149 (20&#x0025;) testing data with opposite intentions. Training data was responsible for optimizing the parameters in the CNN models, while testing data evaluated the models. After the CNNs finished their training session, the test data was sent to both trained CNN models and clinicians for diagnoses. The obtained results were statistically analyzed and compared.</p>
<p>Similarly to phase 1 of the study, we offered clinicians the best possible accommodations. If there were any discrepancies amongst clinician diagnosis, discussion and correction based on the mutual agreement were permitted. The answers given under such circumstances were once again recorded independently and denoted as &#x201C;CC&#x201D; (Consensus of Clinicians). Hence, if the results still favored CNNs over clinicians, it may indicate that CNNs are more effective in caries detection compared to clinicians.</p>
<p>Finally, we further evaluated the performances of the result from the best CNN model. Considering that related researches were conducted in binary classification, we reformulated our best result into the corresponding form. The purpose was to assess the clinical feasibility of the outcome. The statistical measurement consisted of accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), all of which were considered essential indexes in medical screening.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>The diagnostic capabilities of OCT and periapical radiography, from phase 1 of the study, were compared in <xref rid="tbl2" ref-type="table">Table 2</xref>. Each individual weighted Kappa coefficient was computed, followed by the calculation of mean value, standard deviation, variance, together they summarize the average performance and data distribution.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Diagnostic capability&#x002A; of periapical radiography and OCT</p></caption>
<graphic xlink:href="21256502v2_tbl2.tif"/>
</table-wrap>
<p>The mean of the previously defined &#x201C;CR&#x201D; periapical radiography result was also calculated, allowing for comparisons to be drawn. Then the <italic>p</italic>-value was calculated, based on the Mann-Whitney U test between the Radiography and OCT groups, to examine the significance of differences between the clinicians and CNNs groups.</p>
<p>The results show that clinicians scored a higher Kappa value with OCT (<italic>M</italic> &#x003D; .699<italic>, SD</italic> &#x003D; .090) compared to radiography (<italic>M</italic> &#x003D; .407<italic>, SD</italic> &#x003D; .049) with a statistically significant difference at the .05 level (<italic>p</italic> &#x003D; .008). However, the variance of OCT (<italic>&#x03C4;</italic> &#x003D; .008) was slightly bigger than that of radiography (<italic>&#x03C4;</italic> &#x003D; 0.002). While, the &#x201C;CR&#x201D; answer showed a lower Kappa value (<italic>&#x03BA;</italic> &#x003D; .583) than that of OCT (<italic>&#x03BA;</italic> &#x003D; .699).</p>
<p>Given the affirmation that OCT had better diagnostic capability compared to periapical radiography, CNN algorithms were prompted to read OCT images in order to compare their diagnostic with clinicians in phase 2. The same statistical analysis was used in phase 2, as in the previous phase, and was tabulated in <xref rid="tbl3" ref-type="table">Table 3</xref>.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Diagnostic ability&#x002A; of clinicians and CNNs based on OCT</p></caption>
<graphic xlink:href="21256502v2_tbl3.tif"/>
</table-wrap>
<p>As shown in <xref rid="tbl3" ref-type="table">Table 3</xref>, CNNs (<italic>M</italic> &#x003D; .860<italic>, SD</italic> &#x003D; .049) outperformed clinicians (<italic>M</italic> &#x003D; .679<italic>, SD</italic> &#x003D; .113) on average when comparing the Kappa value. The variance of CNNs (<italic>&#x03C4;</italic> &#x003D; .002) was also much smaller than that of clinicians (<italic>&#x03C4;</italic> &#x003D; .013). The difference between the two groups was statistically significant at the .05 level (<italic>p</italic> &#x003D; .012). However, the mean values in the &#x201C;CC&#x201D; group (<italic>M</italic> &#x003D; .748) were still lower than those in CNNs (<italic>M</italic> &#x003D; .860). A visual comparison box plot can also be found in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, illustrating the Kappa value in each group.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Comparison of diagnostic capability of Radiograph versus OCT and diagnostic ability of clinician and CNN. Note that the calculation in different sections was based on different objects &#x2013; teeth or images.</p></caption>
<graphic xlink:href="21256502v2_fig2.tif"/>
</fig>
<p>The best experimental model, ResNet-152, was further evaluated, as shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. The model differentiated caries lesions on OCT images with an accuracy of 95.21&#x0025;. More specifically, the sensitivity was 98.85&#x0025;, the specificity was 89.83&#x0025;, and the PPV and NPV were 93.48&#x0025; and 98.15&#x0025;, respectively.</p>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>The majority of clinicians currently still rely on tactile examination and radiography to make dental caries diagnostics. However, this approach has proved to have low accuracy rates and is highly dependent on the doctor&#x2019;s training background, work experience and concentration during examination. The advanced imaging tool, OCT, working in tandem with the rise of AI, has provided a potentially better solution for caries detection. Increasing studies have attempted to use AI, OCT, or a combination of both to enhance caries detection. However, most of them lacked a valid and reliable methodology, and failed to achieve high-quality classification.</p>
<p>In order to develop a better detection technique, a reliable methodology based on high-quality data needs first to be developed. In our two-phase study, the well-defined criteria of caries classification and a reliable judgment basis, the Micro-CT, was implemented as the reference for scoring. The results indicated that clinicians could recognize carious patterns via OCT images more accurately than via periapical radiographs. Even the accuracy from clinicians in identifying radiographs is enhanced after discussion, it is still not comparable to that of OCT scans, let alone this is unrealistic in chairside to have a group of experienced clinicians discuss a single scan. However, the slightly higher variance in the OCT group revealed more uncertainty regarding OCT than radiography interpretation; this could be due to experience with radiographs. Furthermore, CNNs yielded excellent repeatability of results among OCT and Micro-CT more so than clinicians, which indicates that CNNs can detect and classify carious lesions more precisely and more consistently. The findings overall suggest that OCT is more effective than radiography in caries detection, and CNNs performed higher accuracy and consistency than clinicians on these tasks.</p>
<p>Our results were generally in line with previous studies (<xref ref-type="bibr" rid="c26">Srivastava et al., 2017</xref>; <xref ref-type="bibr" rid="c14">Lee et al., 2018</xref>; <xref ref-type="bibr" rid="c20">Salehi et al., 2019</xref>), however, are more well-rounded and address aspects that were overlooked in previous studies. Our study focused on the establishment of a detailed classification method that is more valid and reliable.</p>
<p>The validity of data is a central issue within the scope of deep machine learning. Without strict examination, the results at most correspond with existing labels but often have little application to real-world situations. If the labels are wrong, then even if the results completely matched with the labels, the results would have no value. Compared to previous studies, our data was compared to a more reliable device, the Micro-CT, which allowed for better diagnosis and also reached deeper depths of teeth. Additionally, all the definitions of classification were clearly delivered and evaluated through content analysis, hence, proves that this study has a higher criterion-related validity.</p>
<p>Secondly, while there is a general agreement that the seriousness of carious lesions plays a critical role in diagnosis and should be involved when considering the appropriate treatment plan, precise classification in previous studies were barely emphasized. In this study, we separated lesions of different degrees of demineralization and activity, particularly enamel and dentin lesions, to enhance treatment decision-making processes.</p>
<p>Moreover, while the architectural innovations of CNN have grown rapidly in recent years, related studies only adopted a singular model. This study adopted five different models, from the classical to the most innovative ones. AlexNet was first used in 2012 as a breakthrough in CNN, followed by VGGNet, which inherited the concept of AlexNet and deepened the model for better outcomes. However, it was not until ResNet that successfully solved the problem of gradient degradation by residual learning. ResNet, which allowed for a more profound model architecture to emerge, demonstrated extraordinary accuracy on image recognition. Soon afterwards, Xception and ResNext were developed mainly to enhance model performance with minimal efficiency sacrifice. Among all, the best model for our specific task was ResNet-152. According to our results, the high sensitivity (98.85&#x0025;) and NPV (98.15&#x0025;) were the most critical indicators in clinical trials, as they ensure a low false-negative rate and imply that few caries lesions would be missed. We highly recommend ResNet to future OCT readings and related applications. Overall, this study aimed to address the weakness overlooked by previous studies to verify their results and provide much more enhanced results.</p>
<p>Although this study has yielded significant results, the potential limitations should be noted; the most prominent limitation in this study was the manual verification process. The labels had to be manually marked by researchers, even using Micro-CT as a correspondence truth, where human errors are inevitable. A possible solution to alleviate this error in future research is through the use of image registration technique, which is commonly used in brain mapping but rarely validated in dental radiography. This would also allow the depths, extents, and boundaries of caries lesions to be automated corresponded. Large amounts of direct labels would be provided without manual labour, allowing for more detailed and precise detection study results. While this study has its limitations, it can still serve as a basis for further studies in related topics.</p>
</sec>
<sec id="s6">
<title>Conclusion</title>
<p>A clinically applicable automatic diagnostic methodology on caries detection was posed in this study. To the best of our knowledge, this study is the first to use Micro-CT as a solid reference, develop a three-tier classification and adopt five different CNN models. The results indicate that CNNs, with appropriate imaging techniques, have a large potential and practicality especially when implemented in clinics to provide patients with more adequate diagnoses.</p>
<p>With the rigorously defined experiments accompanied by comprehensive statistical analysis, the validity and compatibility of our results are more integrative and reliable. Our extensive methodology and experimental results are of great interest for further scientific research and clinical application, respectively. Hopefully, in the future, more AI-based clinical studies can include strict and reliable methodology.</p>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>The data that support the findings of this study are available on request from the corresponding author (Lee, SY). The data are not publicly available because they contain information that could compromise research participant privacy/consent.</p>
</sec>
<sec id="s7">
<title>Author Contribution</title>
<p>Yu-Ping Huang, contributed to conception, design, data acquisition, analysis, and interpretation, drafted and critically revised the manuscript; Shyh-Yuan Lee, contributed to conception, design, and critically revised the manuscript. All authors gave final approval and agreed to be accountable for all aspects of the work.</p>
</sec>
<ack>
<title>Acknowledgment</title>
<p>This research was partially supported by the Ministry of Science and Technology (MOST 108-2813-C-010-040-B) and the Department of Health, Taipei City Government (10901-62-017). The quality of this experiment was greatly enhanced by the assistance of Dr. Lyu Dong-Yuan, who assisted in setting and adjusting the OCT instrument. We would also like to thank Dr. Chen Li-Fen, Inst. of Brain Science, for their invaluable consultations and advice on deep learning. Additionally, the authors would like to thank all other colleagues who contributed to this study. All authors gave their final approval and agree to be accountable for all aspects of the work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Bader</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Shugars</surname>, <given-names>D. A.</given-names></string-name>, and <string-name><surname>Bonito</surname>, <given-names>A. J.</given-names></string-name> (<year>2001</year>). <article-title>Systematic reviews of selected dental caries diagnostic and management methods</article-title>. <source>Journal of dental education</source>, <volume>65</volume>(<issue>10</issue>):<fpage>960</fpage>&#x2013;<lpage>968</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Ben-David</surname>, <given-names>A.</given-names></string-name> (<year>2008</year>). <article-title>Comparison of classification accuracy using cohen&#x2019;s weighted kappa</article-title>. <source>Expert Systems with Applications</source>, <volume>34</volume>(<issue>2</issue>):<fpage>825</fpage>&#x2013;<lpage>832</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Chmura Kraemer</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Periyakoil</surname>, <given-names>V. S.</given-names></string-name>, and <string-name><surname>Noda</surname>, <given-names>A.</given-names></string-name> (<year>2002</year>). <article-title>Kappa coefficients in medical research</article-title>. <source>Statistics in medicine</source>, <volume>21</volume>(<issue>14</issue>):<fpage>2109</fpage>&#x2013;<lpage>2129</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><string-name><surname>Chollet</surname>, <given-names>F.</given-names></string-name> (<year>2016</year>). <article-title>Xception: Deep learning with depthwise separable convolutions</article-title>. <source>CoRR</source>, abs/1610.02357.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Ekstrand</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Qvist</surname>, <given-names>V.</given-names></string-name>, and <string-name><surname>Thylstrup</surname>, <given-names>A.</given-names></string-name> (<year>1987</year>). <article-title>Light microscope study of the effect of probing in occlusal surfaces</article-title>. <source>Caries research</source>, <volume>21</volume>(<issue>4</issue>):<fpage>368</fpage>&#x2013;<lpage>374</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Ekstrand</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ricketts</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Kidd</surname>, <given-names>E.</given-names></string-name> (<year>1997</year>). <article-title>Reproducibility and accuracy of three methods for assessment of demineralization depth on the occlusal surface: an in vitro examination</article-title>. <source>Caries research</source>, <volume>31</volume>(<issue>3</issue>):<fpage>224</fpage>&#x2013;<lpage>231</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Ekstrand</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ricketts</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Kidd</surname>, <given-names>E.</given-names></string-name> (<year>2001</year>). <article-title>Occlusal caries: pathology, diagnosis and logical management</article-title>. <source>Dental update</source>, <volume>28</volume>(<issue>8</issue>):<fpage>380</fpage>&#x2013;<lpage>387</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Fercher</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Drexler</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Hitzenberger</surname>, <given-names>C. K.</given-names></string-name>, and <string-name><surname>Lasser</surname>, <given-names>T.</given-names></string-name> (<year>2003</year>). <article-title>Optical coherence tomography-principles and applications</article-title>. <source>Reports on progress in physics</source>, <volume>66</volume>(<issue>2</issue>):<fpage>239</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Frencken</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Peters</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Manton</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Leal</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Gordan</surname>, <given-names>V. V.</given-names></string-name>, and <string-name><surname>Eden</surname>, <given-names>E.</given-names></string-name> (<year>2012</year>). <article-title>Minimal intervention dentistry for managing dental caries&#x2013;a review: report of a fdi task group</article-title>. <source>International dental journal</source>, <volume>62</volume>(<issue>5</issue>):<fpage>223</fpage>&#x2013;<lpage>243</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="book"><string-name><surname>Goodfellow</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Courville</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <source>Deep learning</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="other"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). <article-title>Deep residual learning for image recognition</article-title>. <source>In Proceedings of the IEEE conference on computer vision and pattern recognition</source>, pages <fpage>770</fpage>&#x2013;<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="other"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> (<year>2012</year>). <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>In Advances in neural information processing systems</source>, pages <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bottou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Haffner</surname>, <given-names>P.</given-names></string-name> (<year>1998</year>). <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source>, <volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>&#x2013;<lpage>2324</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Lee</surname>, <given-names>J.-H.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>D.-H.</given-names></string-name>, <string-name><surname>Jeong</surname>, <given-names>S.-N.</given-names></string-name>, and <string-name><surname>Choi</surname>, <given-names>S.-H.</given-names></string-name> (<year>2018</year>). <article-title>Detection and diagnosis of dental caries using a deep learning-based convolutional neural network algorithm</article-title>. <source>Journal of dentistry</source>, <volume>77</volume>:<fpage>106</fpage>&#x2013;<lpage>111</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Litjens</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kooi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bejnordi</surname>, <given-names>B. E.</given-names></string-name>, <string-name><surname>Setio</surname>, <given-names>A. A. A.</given-names></string-name>, <string-name><surname>Ciompi</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ghafoorian</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Van Der Laak</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Van Ginneken</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>S&#x00E1;nchez</surname>, <given-names>C. I.</given-names></string-name> (<year>2017</year>). <article-title>A survey on deep learning in medical image analysis</article-title>. <source>Medical image analysis</source>, <volume>42</volume>:<fpage>60</fpage>&#x2013;<lpage>88</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Luong</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Shimada</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Araki</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yoshiyama</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tagami</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Sadr</surname>, <given-names>A.</given-names></string-name> (<year>2020</year>). <article-title>Diagnosis of occlusal caries with dynamic slicing of 3d optical coherence tomography images</article-title>. <source>Sensors</source>, <volume>20</volume>(<issue>6</issue>):<fpage>1659</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Molander</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ahlqwist</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gr&#x00F6;ndahl</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Hollender</surname>, <given-names>L.</given-names></string-name> (<year>1993</year>). <article-title>Comparison of panoramic and intraoral radiography for the diagnosis of caries and periapical pathology</article-title>. <source>Dentomaxillofacial Radiology</source>, <volume>22</volume>(<issue>1</issue>):<fpage>28</fpage>&#x2013;<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><string-name><surname>Paszke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Massa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lerer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bradbury</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chanan</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Killeen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Gimelshein</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Antiga</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Desmaison</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kopf</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>DeVito</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Raison</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tejani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chilamkurthy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Steiner</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bai</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Chintala</surname>, <given-names>S.</given-names></string-name> (<year>2019</year>). <chapter-title>Pytorch: An imperative style, high-performance deep learning library</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Wallach</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Larochelle</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Beygelzimer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>d&#x2019;Alch&#x2019;
se-Buc</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Garnett</surname>, <given-names>R.</given-names></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source> <volume>32</volume>, pages <fpage>8024</fpage>&#x2013;<lpage>8035</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Rockall</surname>, <given-names>A.</given-names></string-name> (<year>2020</year>). <article-title>From hype to hope to hard work: developing responsible ai for radiology</article-title>. <source>Clinical radiology</source>, <volume>75</volume>(<issue>1</issue>):<fpage>1</fpage>&#x2013;<lpage>2</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Salehi</surname>, <given-names>H. S.</given-names></string-name>, <string-name><surname>Mahdian</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Murshid</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Judex</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Tadinada</surname>, <given-names>A.</given-names></string-name> (<year>2019</year>). <article-title>Deep learning-based quantitative analysis of dental caries using optical coherence tomography: an ex vivo study</article-title>. <source>In Lasers in Dentistry XXV</source>, volume <volume>10857</volume>, page <fpage>108570H</fpage>. International Society for Optics and Photonics.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Schneider</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ahrens</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Strumpski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>R&#x00FC;ger</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>H&#x00E4;fer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>H&#x00FC;ttmann</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Theisen-Kunde</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Schulz-Hildebrandt</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Haak</surname>, <given-names>R.</given-names></string-name> (<year>2020</year>). <article-title>An intraoral oct probe to enhanced detection of approximal carious lesions and assessment of restorations</article-title>. <source>Journal of clinical medicine</source>, <volume>9</volume>(<issue>10</issue>):<fpage>3257</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="other"><string-name><surname>Schwendicke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Rossi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>G&#x00F6;stemeyer</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Elhennawy</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cantu</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gaudin</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chaurasia</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gehrung</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Krois</surname>, <given-names>J.</given-names></string-name> (<year>2020</year>). <article-title>Cost-effectiveness of artificial intelligence for proximal caries detection</article-title>. <source>Journal of Dental Research</source>, page <fpage>0022034520972335</fpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Seiler</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Eppenberger</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>R&#x00FC;hli</surname>, <given-names>F.</given-names></string-name> (<year>2018</year>). <article-title>Application of portable digital radiography for dental investigations of ancient egyptian mummies during archaeological excavations: Evaluation and discussion of the advantages and limitations of different approaches and projections</article-title>. <source>Imaging science in dentistry</source>, <volume>48</volume>(<issue>3</issue>):<fpage>167</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Shimada</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sadr</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Burrow</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Tagami</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ozawa</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Sumi</surname>, <given-names>Y.</given-names></string-name> (<year>2010</year>). <article-title>Validation of swept-source optical coherence tomography (ss-oct) for the diagnosis of occlusal caries</article-title>. <source>Journal of dentistry</source>, <volume>38</volume>(<issue>8</issue>):<fpage>655</fpage>&#x2013;<lpage>665</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="other"><string-name><surname>Simonyan</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> (<year>2014</year>). <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1409.1556</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><surname>Srivastava</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pradhan</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Varadarajan</surname>, <given-names>S.</given-names></string-name> (<year>2017</year>). <article-title>Detection of tooth caries in bitewing radiographs using deep learning</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1711.07312</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Swain</surname>, <given-names>M. V.</given-names></string-name> and <string-name><surname>Xue</surname>, <given-names>J.</given-names></string-name> (<year>2009</year>). <article-title>State of the art of micro-ct applications in dental research</article-title>. <source>International journal of oral science</source>, <volume>1</volume>(<issue>4</issue>):<fpage>177</fpage>&#x2013;<lpage>188</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="book"><collab>United Nations Scientific Committee on the Effects of Atomic Radiation</collab> (<year>2017</year>). <source>Sources, Effects and Risks of Ionizing Radiation, United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) 2016 Report: Report to the General Assembly, with Scientific Annexes</source>. <publisher-name>United Nations</publisher-name>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><surname>Xie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Girshick</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Doll&#x00E1;r</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Tu</surname>, <given-names>Z.</given-names></string-name>, and <string-name><surname>He</surname>, <given-names>K.</given-names></string-name> (<year>2017</year>). <article-title>Aggregated residual transformations for deep neural networks</article-title>. <source>In Proceedings of the IEEE conference on computer vision and pattern recognition</source>, pages <fpage>1492</fpage>&#x2013;<lpage>1500</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Zandon&#x00E1;</surname>, <given-names>A. F.</given-names></string-name> and <string-name><surname>Zero</surname>, <given-names>D. T.</given-names></string-name> (<year>2006</year>). <article-title>Diagnostic tools for early caries detection</article-title>. <source>The Journal of the American Dental Association</source>, <volume>137</volume>(<issue>12</issue>):<fpage>1675</fpage>&#x2013;<lpage>1684</lpage>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<label>Appendices</label>
<sec id="s8">
<label>A</label>
<title>Mechanism of Our SS-OCT</title>
<p>The mechanism of our swept-source OCT (SS-OCT) system is illustrated in Appendix <xref rid="fig3" ref-type="fig">Fig. 3</xref>. A beam of light initially generated from the swept-source laser is separated into two parts right after going into the coupler and circulator. While one of them enters the sample arm, the other goes into the reference arm, both of which are then reflected. After reflection, the separated light beams merge back together. Optical path differences are created due to the different characteristics of the surfaces, resulting in distinctive interference patterns.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Setup of our SS-OCT scanning device</p></caption>
<graphic xlink:href="21256502v2_fig3.tif"/>
</fig>
</sec>
<sec id="s9">
<label>B</label>
<title>Introduction of Convolutional Neural Network</title>
<p>The basic structure was first carried out by <xref ref-type="bibr" rid="c13">LeCun et al. (1998)</xref> as shown in Appendix <xref rid="fig4" ref-type="fig">Fig. 4</xref>, which consists of an input and output layer with several convolution and pooling layers in between.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Illustration of a CNN model architecture</p></caption>
<graphic xlink:href="21256502v2_fig4.tif"/>
</fig>
<sec id="s9a">
<label>(a)</label>
<title>convolution</title>
<p>A convolution procedure is illustrated in Appendix <xref rid="fig5" ref-type="fig">Fig. 5</xref>. In the context of image processing, images (denoted <italic>A</italic>) are modeled as real matrices by its pixel value. Convolution serves as a binary operation among matrices of different shapes. For <inline-formula><alternatives><inline-graphic xlink:href="21256502v2_inline1.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="21256502v2_inline2.gif"/></alternatives></inline-formula>, <italic>A</italic> &#x002A; <italic>K</italic> would yield another matrix <inline-formula><alternatives><inline-graphic xlink:href="21256502v2_inline3.gif"/></alternatives></inline-formula>,
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="21256502v2_ueqn1.gif"/></alternatives>
</disp-formula>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Basic structure and function of a convolution layer</p></caption>
<graphic xlink:href="21256502v2_fig5.tif"/>
</fig>
where <italic>K</italic> is usually much smaller than <italic>A</italic> and referred to as the <italic>kernel</italic>. Convolutions with small kernels are usually for purposes of local feature extraction such as detecting edges, impulses or noise in images. Usually one needs to design an appropriate kernel, but the presence of convolution layers is to find the appropriate kernel via parameter optimization. We can view the convolution as a bi-linear operator
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="21256502v2_ueqn2.gif"/></alternatives>
</disp-formula></p>
<p>Sometimes, there can be multiple channels for both images and kernels. This motivates one to have the <italic>multi-channel convolution</italic> by linearly extending the original one. For <inline-formula><alternatives><inline-graphic xlink:href="21256502v2_inline4.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="21256502v2_inline5.gif"/></alternatives></inline-formula> where <italic>C</italic><sub>1</sub><italic>, C</italic><sub>2</sub> are the numbers of channels for <italic>A, K</italic> respectively. The resulting <italic>B</italic> &#x003D; <italic>A</italic> &#x002A; <italic>K</italic> would be simply
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="21256502v2_ueqn3.gif"/></alternatives>
</disp-formula></p>
</sec>
<sec id="s9b">
<label>(b)</label>
<title>pooling</title>
<p>The pooling technique decreases computational resources without losing much information. The most common methods are max-pooling and average-pooling as shown in <xref rid="fig6" ref-type="fig">figure 6</xref>. In a max-pooling layer of stride <italic>s</italic> and width <italic>w</italic>, a image <italic>A</italic> &#x2208; &#x211D;<sup><italic>n</italic>&#x00D7;<italic>m</italic></sup> is mapped to <italic>B</italic>1,
<disp-formula id="ueqn4">
<alternatives><graphic xlink:href="21256502v2_ueqn4.gif"/></alternatives>
</disp-formula></p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Demonstration of max-pooling and average-pooling techniques</p></caption>
<graphic xlink:href="21256502v2_fig6.tif"/>
</fig>
<p>On the other hand, for an average-pooling case, we have <italic>B</italic>2,
<disp-formula id="ueqn5">
<alternatives><graphic xlink:href="21256502v2_ueqn5.gif"/></alternatives>
</disp-formula></p>
<p>Interleaving convolution and pooling layers, concatenated by a fully connected layer at the end, would yield a complete CNN model which mimics an image processing pipeline. A CNN model takes in an image then output a probabilistic vector. Finally, it uses the well-known back-propagation algorithm to optimize its parameters in each layers until the model generalize well to data.</p>
<p>The CNN models adopted in our study were: AlexNet, VGG-16, ResNet-152, Xception, and ResNeXt-101. All are thought of as the best models in different periods. More specific architecture details should be found in the original papers (as in the references).</p>
</sec>
</sec>
<sec id="s10">
<label>C</label>
<title>Caries Classification in Radiographic Examination (Ekstrand et al., 1997)</title>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="21256502v2_utbl1.tif"/>
</table-wrap>
</sec>
<sec id="s11">
<label>D</label>
<title>Caries Classification of OCT Examination (Shimada et al., 2010)</title>
<table-wrap id="utbl2" orientation="portrait" position="float">
<graphic xlink:href="21256502v2_utbl2.tif"/>
</table-wrap>
</sec>
<sec id="s12">
<label>E</label>
<title>Overall Procedure of This Study</title>
<p>The workflow of this study is depicted in Appendix <xref rid="fig7" ref-type="fig">Fig. 7</xref>. The main objects of comparison are painted: the beige sets and the green sets represent the objects in the first and second parts, respectively, while the blue box (Micro-CT) is recognized as the reference of the ground truth.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><p>Illustration of the procedure in this study.</p></caption>
<graphic xlink:href="21256502v2_fig7.tif"/>
</fig>
</sec>
</app>
</app-group>
</back>
</article>