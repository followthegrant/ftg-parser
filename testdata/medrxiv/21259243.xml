<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.06.21.21259243</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Ophthalmology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Interpretable gender classification from retinal fundus images using BagNets</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Ilanchezian</surname><given-names>Indu</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5639-7209</contrib-id>
<name><surname>Kobak</surname><given-names>Dmitry</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Faber</surname><given-names>Hanna</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ziemssen</surname><given-names>Focke</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Berens</surname><given-names>Philipp</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3184-2353</contrib-id>
<name><surname>Ayhan</surname><given-names>Murat Se&#x00E7;kin</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institute for Ophthalmic Research, University of T&#x00FC;bingen</institution>, 72076 T&#x00FC;bingen, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>T&#x00FC;bingen AI Center, University of T&#x00FC;bingen</institution>, 72076 T&#x00FC;bingen, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution>University Eye Clinic, University of T&#x00FC;bingen</institution>, 72076 T&#x00FC;bingen, <country>Germany</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label> Corresponding author; email: <email>murat-seckin.ayhan@uni-tuebingen.de</email></corresp>
<fn id="n1" fn-type="others"><p>{<email>philipp.berens@uni-tuebingen.de</email>, <email>murat-seckin.ayhan@uni-tuebingen.de</email>}</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.06.21.21259243</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>6</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>21</day>
<month>6</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>6</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="21259243.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Deep neural networks (DNNs) are able to predict a person&#x2019;s gender from retinal fundus images with high accuracy, even though this task is usually considered hardly possible by ophthalmologists. Therefore, it has been an open question which features allow reliable discrimination between male and female fundus images. To study this question, we used a particular DNN architecture called BagNet, which extracts local features from small image patches and then averages the class evidence across all patches. The BagNet performed on par with the more sophisticated Inception-v3 model, showing that the gender information can be read out from local features alone. BagNets also naturally provide saliency maps, which we used to highlight the most informative patches in fundus images. We found that most evidence was provided by patches from the optic disc and the macula, with patches from the optic disc providing mostly male and patches from the macula providing mostly female evidence. Although further research is needed to clarify the exact nature of this evidence, our results suggest that there are localized structural differences in fundus images between genders. Overall, we believe that BagNets may provide a compelling alternative to the standard DNN architectures also in other medical image analysis tasks, as they do not require post-hoc explainability methods.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Retinal fundus image</kwd>
<kwd>Gender prediction</kwd>
<kwd>Interpretable deep neural networks</kwd>
<kwd>Bag-of-features models</kwd>
</kwd-group>
<counts>
<page-count count="11"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>This research was supported by the German Ministry of Science and Education (BMBF, 01GQ1601 and 01IS18039A) and the German Science Foundation (BE5601/4-2 and EXC 2064, project number 390727645). Additional funding was provided by Novartis AG through a research grant. The funding bodies did not have any influence in the study planning and design.</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>The use of the additional clinical retinal fundus images was permitted by the Institutional Ethics Committee of the University of Tuebingen and was performed in line with all relevant laws and regulations.</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>In recent years, deep neural networks (DNNs) have achieved physician-level accuracy in various image-based medical tasks, e.g. in radiology [<xref ref-type="bibr" rid="c21">21</xref>], dermatology [<xref ref-type="bibr" rid="c10">10</xref>], pathology [<xref ref-type="bibr" rid="c15">15</xref>] and ophthalmology [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. Moreover, in some cases DNNs have been shown to have good performance in tasks that are not straightforward for physicians: for example, they can accurately predict the gender from retinal images [<xref ref-type="bibr" rid="c25">25</xref>]. As this task is typically not clinically relevant, ophthalmologists are not explicitly trained for it. Nevertheless, the comparably poor performance of ophthalmologists at this task suggests that gender differences in fundus images are not obvious or salient. Even though saliency maps used by [<xref ref-type="bibr" rid="c25">25</xref>] and follow-up studies [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c5">5</xref>] have tentatively pointed at the optic disc, the macula, and retinal blood vessels as candidate regions for gender-related anatomical differences in fundus images, conclusive evidence is still lacking. Therefore the high gender prediction performance of DNNs has created lots of interest in the medical imaging community as one hope for DNNs is to unravel biomarkers that are not easily found by humans. Here, we performed a proof of principle study to make progress on the question of how DNNs are able to detect gender differences in retinal fundus. Our contribution is twofold: we (1) introduced BagNets [<xref ref-type="bibr" rid="c3">3</xref>] &#x2014; a &#x2018;local&#x2019; variant of the ResNet50 architecture [<xref ref-type="bibr" rid="c13">13</xref>] &#x2014; as an interpretable-by-design architecture for image analysis in ophthalmology and (2) used them to narrow down the hypothesis space for question at hand.</p>
<p>We trained the BagNets on a large collection of retinal fundus images obtained from the UK Biobank [<xref ref-type="bibr" rid="c27">27</xref>] (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). BagNets use a linear classifier on features extracted from image patches to compute local evidence for each class, which is then averaged over space to form the final prediction, without considering any global relationships. Thus, BagNets resemble &#x2018;bag-of-features&#x2019; models popular before deep learning [<xref ref-type="bibr" rid="c23">23</xref>]. Despite this simple bag-of-features approach, the BagNet performed on par with an Inception-v3 network in terms of gender prediction accuracy, indicating that gender can be determined from the local characteristics of the fundus image. Also, the BagNet architecture naturally allowed to construct saliency maps to highlight the most informative regions for gender prediction in the retina (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). We found that the macula contained most distinctive female patches, while the optic disk contained male ones. In addition, we showed that the decision of the BagNet was not simply caused by some exclusively female or male patches in the images, but rather by a change in both frequency and the degree of &#x2018;femaleness&#x2019; or &#x2018;maleness&#x2019; of individual patches. Overall, we argue that BagNets can be useful in medical imaging applications including both disease diagnosis and biomarker discovery, thanks to interpretability provided by their local architecture. Our code will be available upon publication.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1:</label>
<caption><p>A sketch of the gender prediction via BagNet33. <bold>(a)</bold> Example fundus image from the UK Biobank. The optic disc is the bright spot on the right, the macula is the slightly darker spot in the middle and the blood vessels are extending from the optic disc in darker red. <bold>(b)</bold> The BagNet33 extracts 2048-dimensional feature vectors from 33 &#x00D7;33 patches and stores them in the penultimate layer. Via spatial average pooling and a linear classifier, it then forms the final predictions for the gender. The same linear classifier can be applied directly to the feature representation in the penultimate layer to compute the local evidence, which can be visualized as a saliency map. Plotted with PlotNeuralNet [<xref ref-type="bibr" rid="c14">14</xref>].</p></caption>
<graphic xlink:href="21259243v1_fig1.tif"/>
</fig>
</sec>
<sec id="s2">
<label>2</label>
<title>Related Work</title>
<p>Previous work on gender prediction from fundus images have used either standard DNN architectures or simple logistic regression on top of expert-defined features. For example, [<xref ref-type="bibr" rid="c25">25</xref>] trained Inception-v3 networks on the UK Biobank dataset to predict cardiovascular risk factors from fundus images and found that DNNs were also capable of predicting the patient&#x2019;s gender (AUC = 0.97). A similar network was used by [<xref ref-type="bibr" rid="c9">9</xref>]. In both studies, the authors computed posthoc saliency maps to study the features driving the network&#x2019;s decisions. In a sample of 100 attention maps, [<xref ref-type="bibr" rid="c25">25</xref>] found that the optic disc, vessels, and other nonspecific parts of the images were frequently highlighted. However, this seems to be the case for almost all the dependent variables and it is very hard to derive testable hypotheses for gender specific differences. Likewise, [<xref ref-type="bibr" rid="c9">9</xref>] manually inspected a sample of occlusion maps and concluded that DNNs may use geometrical properties of the blood vessels at the optic disc for predicting gender. More recently, [<xref ref-type="bibr" rid="c5">5</xref>] demonstrated that DNNs can predict gender not only from retinal fundus images but also from OCT scans, where the foveal pit region seemed most informative based on gradient-based saliency maps. Taking a different approach, [<xref ref-type="bibr" rid="c29">29</xref>] used expert-defined image features in a simple logistic regression model. Although the performance of their model was worse (AUC = 0.78), they found various color-intensity-based metrics and the angle between certain retinal arteries to be significant predictors, but most effect sizes were small.</p>
<p>BagNets provide a compromise between linear classifiers operating on expert-defined features [<xref ref-type="bibr" rid="c29">29</xref>] and high-performing DNNs [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c5">5</xref>], which require complex post-hoc processing for interpretability [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c2">2</xref>]. In BagNets, a saliency map is also straightforward to compute by design, and it has been shown to provide more information about the location of class evidence than auxiliary interpretability methods [<xref ref-type="bibr" rid="c3">3</xref>]. Such native evidence-based maps returned by BagNets are interpretable as is, while standard saliency maps require fine-tuning and post-processing for compelling visualizations [<xref ref-type="bibr" rid="c2">2</xref>]. Thanks to these benefits, BagNets have also been used in the context of histopathological microscopy [<xref ref-type="bibr" rid="c24">24</xref>].</p>
</sec>
<sec id="s3">
<label>3</label>
<title>Methods</title>
<sec id="s3a">
<label>3.1</label>
<title>Data and preprocessing</title>
<p>The UK Biobank [<xref ref-type="bibr" rid="c27">27</xref>] offers a large-scale and multi-modal repository of health-related data from the UK. From this, we obtained records of over 84, 000 subjects with 174, 465 fundus images from both eyes and multiple visits per participant. Male and female subjects constituted 46% and 54% of the data, respectively. As a substantial fraction of the images were not gradable due to image quality issues (artefacts, high contrast, or oversaturation), we used the EyeQual networks [<xref ref-type="bibr" rid="c6">6</xref>] to filter out poor images. 47, 939 images (47% male, 53% female) passed the quality check by the EyeQual ensemble. We partitioned them into the training, validation and test sets with 75%, 10% and 15% of subjects, respectively, making sure that all images from each subject were allocated to the same set.</p>
<p>Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set. For all images, we applied a circular mask to capture the 95% central area and to remove camera artifacts at the borders.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Network architecture and training</title>
<p>We used BagNets [<xref ref-type="bibr" rid="c3">3</xref>] (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>) and standard Inception-v3 [<xref ref-type="bibr" rid="c28">28</xref>] network as implemented in Keras [<xref ref-type="bibr" rid="c4">4</xref>]. In a BagNet, neurons in the final layer have a receptive field restricted to <italic>q&#x00D7;q</italic> pixels, where we used <italic>q&#x2208; {</italic> 9, 17, 33<italic>}</italic>. The convolutional stack in the network extracts a 2048-dimensional feature vector for each <italic>q&#x00D7; q</italic> image patch. Patches were implicitly defined, with a stride for convolutions of 8 pixels for <italic>q</italic> = 33. Therefore local features were extracted for each patch on a 24 &#x00D7; 24 grid (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). A linear classifier combined these 2048 features to obtain the local class evidence which was then averaged across all image patches (average pooling layer).</p>
<p>All networks had been pretrained on ImageNet [<xref ref-type="bibr" rid="c26">26</xref>] by their respective developers. For our binary classification problem, we replaced the 1000-way softmax output layer with a single logistic output neuron (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). We initially trained only the output layer using the fundus images for 10 epochs. This was followed by fine-tuning all layers for 100 epochs. We used stochastic gradient descent (SGD) with the learning rate set to 0.01 and the batch size to 16. We used data augmentation via random rotations and flips, width and height shifts, random brightness, and random zooming operations. We picked the best epoch from the [95, 100] range based on the validation performance. We evaluated the final performance on both the test set and the data from the University Eye Hospital.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Generation of saliency maps</title>
<p>To compute saliency maps, we applied the weights <bold>w</bold> in the final classification layer of BagNet33 to the feature vectors, e.g. <bold>x</bold>, in its penultimate layer (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>), yielding the local evidence (logits) for each patch via <bold>w</bold> &#x00B7; <bold>x</bold> =&#x2211; <sub><italic>i</italic></sub> <italic>w</italic><sub><italic>i</italic></sub><italic>x</italic><sub><italic>i</italic></sub>. We clipped the resulting values to [&#x2212;75, 75] for visualization purposes. The resulting saliency maps were 24 &#x00D7; 24 (<xref rid="fig2" ref-type="fig">Fig. 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2:</label>
<caption><p>Saliency maps obtained by BagNet showing class evidence for each of the image patches on a 24 &#x00D7; 24 grid. Top row shows exemplary test images along with their saliency maps. Middle row shows the average saliency maps for correctly classified male and female patients. Bottom row shows the average fundus images corresponding to the middle row.</p></caption>
<graphic xlink:href="21259243v1_fig2.tif"/>
</fig>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Embedding of image patches</title>
<p>To explore which image patches were informative for classification, we used t-Stochastic Neighborhood Embeddings (t-SNE) [<xref ref-type="bibr" rid="c20">20</xref>], a non-linear dimensionality reduction method. To embed the feature representations of <italic>&#x003E;</italic>1, 000, 000 image patches extracted from the fundus images, we used FIt-SNE implementation [<xref ref-type="bibr" rid="c19">19</xref>] with uniform affinity kernel in the high-dimensional space across 15 nearest neighbours. We used PCA initialization to better preserve the global structure of the data and improve the reproducibility [<xref ref-type="bibr" rid="c16">16</xref>]. We used a heavy-tailed kernel <italic>k</italic>(<italic>d</italic>) = 1<italic>/</italic>(1 &#x002B; <italic>d</italic><sup>2</sup><italic>/&#x03B1;</italic>)<sup><italic>&#x03B1;</italic></sup> with <italic>&#x03B1;</italic> = 0.5 to emphasize cluster structure [<xref ref-type="bibr" rid="c17">17</xref>].</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Results</title>
<p>We trained BagNets with three different receptive field sizes to predict patient&#x2019;s gender from retinal fundus images based on the UK Biobank data. We evaluated their performances using prediction accuracy and the Area Under the Receiver Operating Characteristic curve (AUC) and compared to an Inception-v3 network (<xref rid="tbl1" ref-type="table">Table 1</xref>). BagNet33 and Inception-v3 performed on par with each other, while BagNet17 and BagNet9 performed worse. BagNet33 and Inception-v3 also generalized better to a new clinical dataset, albeit with a substantial drop in performance. Together, this suggests that the 33&#x00D7; 33 patches captured the relevant information for gender prediction better than smaller patches. Thus, for the remainder of the paper, we will focus our analysis on the BagNet33 (referring to it simply as BagNet).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Gender prediction performances of DNNs</p></caption>
<graphic xlink:href="21259243v1_tbl1.tif"/>
</table-wrap>
<p>We inspected saliency maps for gender prediction computed by evaluating the classifier on each feature representation in the penultimate layer (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, top). In a typical male example, we found that the optic disc provided high evidence for the male class, along with more scattered evidence around the major blood vessels. For a typical female example, high evidence was found for the female class in the macula. Averaging the saliency maps across all correctly classified male/female test images confirmed that the BagNet relied on the optic disc and the blood vessels to identify male images and on the macula to identify female ones (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, middle).</p>
<p>Interestingly, the individual and the average saliency maps also showed that the optic disc patches tended to always provide male evidence, to some extent even in correctly classified female images. Similarly, the macula patches tended to provide female evidence, even in correctly classified male images. The BagNet could nevertheless achieve high classification performance after averaging the class evidence across all patches.</p>
<p>As a sanity check, we show the averaged fundus images across all correctly classified male/female images in the bottom row of <xref rid="fig2" ref-type="fig">Fig. 2</xref>. These average images are nearly identical across genders, demonstrating that it is not the location, the size, or the shape of the optic disc or macula that drive the BagNet predictions.</p>
<p>To further explore the structure of local image features informative about gender, we embedded the 2048-dimensional feature representation of each image patch into 2D using t-SNE and colored them by the provided class evidence (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). We found that most image patches provided only weak evidence for either class, but some distinct clusters of patches had consistently high logits. We further explored these clusters and found that they consistently showed the optic disk with blood vessels (<bold>a</bold> and <bold>c</bold>) or the macula (<bold>b</bold> and <bold>d</bold>), in line with the saliency maps computed above (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). However, even though the clusters <bold>a</bold> and <bold>b</bold> consistently provided evidence for the male class, patches in these clusters occurred in true female and male fundus images alike (67% and 62% patches from male images, respectively). Similarly, clusters <bold>c</bold> and <bold>d</bold> provided evidence for the female class but yet came from male and female fundus images (39% and 45% patches from male images, respectively).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3:</label>
<caption><p>Visualization of image patches and associated class evidence via t-SNE. 213,696 patches extracted from 371 correctly classified test images (using training set images yielded a similar embedding; not shown). Four patches with high evidence (two male, two female) are shown from each of the four highlighted clusters. The fraction of male patches in each of these clusters is given below the corresponding exemplary patches. The colors show the logit class evidence. Note that the color does not indicate the correct label of each patch.</p></caption>
<graphic xlink:href="21259243v1_fig3.tif"/>
</fig>
<p>This raised the question of whether the BagNet&#x2019;s decisions were mostly driven by (i) male/female images having individual patches with stronger male/female evidence; or (ii) male/female images having a larger number of patches with male/female evidence (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). We found that both factors played a role in determining the final gender predictions, but the fraction of male/female patches seemed to be a stronger factor: Cohen&#x2019;s <italic>d</italic> = 1.82 and <italic>d</italic> = 1.63 for the difference in fraction of male (logit value <italic>&#x003E;</italic>50) and female (logit value <italic>&#x003C;&#x2212;</italic> 50) patches between genders, vs. <italic>d</italic> = 0.77 and <italic>d</italic> = 0.76 for the difference in the logit value of the most male and the most female patch. Thus, female images contained more patches providing strong female class evidence, and vice versa for male fundus images.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4:</label>
<caption><p>Two factors determine the gender predictions of the BagNet: the maximal strength of evidence and the frequency of strong evidence. <bold>(a)</bold> Kernel density estimate of all male (red) and female (blue) test set images. Horizontal axis: fraction of male patches, defined as having logit values above 50. Vertical axis: the absolute logit value of the most male patch. <bold>(b)</bold> The same for patches providing female evidence (logit values below &#x2212;50).</p></caption>
<graphic xlink:href="21259243v1_fig4.tif"/>
</fig>
</sec>
<sec id="s5">
<label>5</label>
<title>Discussion</title>
<p>In summary, we argued that the BagNet architecture is particularly suitable for medical image analysis, thanks to its built-in interpretability. Here we used BagNets to investigate the high accuracy of DNNs in gender prediction from retinal fundus images. BagNet33 achieved a performance similar to Inception-v3 despite having a much simpler architecture and using only local image features for prediction. This suggested that local features are sufficient for gender prediction and the global arrangement of these features is not essential for this task.</p>
<p>In BagNets, saliency maps can be readily computed without auxiliary gradient-based methods or layer-wise relevance propagation [<xref ref-type="bibr" rid="c22">22</xref>]. We used the native saliency maps of BagNets and a two-dimensional t-SNE embedding of image patches to identify the most informative regions for the gender prediction task in fundus images. This allowed us to go beyond the previous reports [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c9">9</xref>] and for the first time to provide conclusive evidence that the optic disk region contains features used to inform a male prediction and the macula region for a female prediction. We found that both the frequency of informative male/female patches and &#x2014; albeit to a lesser degree &#x2014; the strength of the most informative male/female patches were important factors for gender prediction by BagNets.</p>
<p>It is, however, not the case that the optic disc in males is substantially larger than in females, as can be seen in the average fundus images shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. The relative optic disc and macula sizes, shapes, brightness levels, etc. seem all to be roughly the same for both genders. Instead, our results suggest <italic>structural but localized</italic> differences in the male and female retinas, mainly within the optic disc and macula regions. This is supported by the previous findings showing that the retinal nerve fibre layer in the optic disk is slightly thicker in females [<xref ref-type="bibr" rid="c18">18</xref>] and that the macula is slightly thinner [<xref ref-type="bibr" rid="c5">5</xref>] and wider [<xref ref-type="bibr" rid="c8">8</xref>] in females. However, these previously reported gender differences have small to moderate effect sizes (Cohen&#x2019;s <italic>d</italic> = 0.11, <italic>d</italic> = 0.52, and <italic>d</italic> = 0.17 respectively for the comparisons referenced above; computed here based on reported means and standard deviations) and it is unclear if they alone can explain the BagNet performance.</p>
<p>Therefore, future work is needed to understand what exactly it is that allows the network to assign high male evidence to the optic disc patches from male patients and high female evidence to the optic disc patches from female patients. In this sense, the results presented here do not provide the final solution to the gender prediction mystery. Nevertheless, we believe that our results make a step in the right direction as they demonstrate structural but localized gender differences and reduce the problem complexity down to specific small patches of the fundus image that can be further analyzed separately.</p>
<p>We believe that BagNets may also be more widely applicable for clinically relevant diagnostic tasks involving medical images in ophthalmology and beyond, provided that they are coupled with reliable uncertainty estimation [<xref ref-type="bibr" rid="c1">1</xref>]. In many cases, pathologies often manifest in localized regions, which can be readily picked up by BagNets. For example, BagNets could be used to further explore clinically relevant changes underlying progressive diseases such as diabetic retinopathy. The interpretable architecture of BagNets may increase the trust of clinicians and patients, which is a critical issue for adoption of deep learning algorithms in medical practice [<xref ref-type="bibr" rid="c11">11</xref>].</p>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>The main collection of retinal fundus images were obtained from the UK Biobank. Due to the Material Transfer Agreement of UK Biobank, we can not share the data, but researchers can apply for access and subsidized fees are available.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Wieland Brendel for his support with BagNets. This research was supported by the German Ministry of Science and Education (BMBF, 01GQ1601 and 01IS18039A) and the German Science Foundation (BE5601/4-2 and EXC 2064, project number 390727645). Additional funding was provided by Novartis AG through a research grant. The funding bodies did not have any influence in the study planning and design.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="other"><string-name><surname>Ayhan</surname>, <given-names>M.S.</given-names></string-name>, <string-name><surname>K&#x00FC;hlewein</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Aliyeva</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Inhoffen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Ziemssen</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>: <article-title>Expert-validated estimation of diagnostic uncertainty for deep neural networks in diabetic retinopathy detection</article-title>. <source>Medical Image Analysis</source> p. <fpage>101724</fpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="other"><string-name><surname>Ayhan</surname>, <given-names>M.S.</given-names></string-name>, <string-name><surname>K&#x00FC;mmerle</surname>, <given-names>L.B.</given-names></string-name>, <string-name><surname>K&#x00FC;hlewein</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Inhoffen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Aliyeva</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ziemssen</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>: <article-title>Clinical validation of saliency maps for understanding deep neural networks in ophthalmology</article-title>. <source>medRxiv</source> (<year>2021</year>)</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="other"><string-name><surname>Brendel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>: <source>Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet. In: International Conference on Learning Representations</source> (<year>2019</year>)</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="website"><string-name><surname>Chollet</surname>, <given-names>F.</given-names></string-name>, <etal>et al.</etal>: <source>Keras</source> (<year>2015</year>), <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="other"><string-name><surname>Chueh</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>Hsieh</surname>, <given-names>Y.T.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>H.H.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>I.H.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>S.L.</given-names></string-name>: <article-title>Prediction of sex and age from macular optical coherence tomography images and feature analysis using deep learning</article-title>. <source>medRxiv</source> (<year>2020</year>)</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="other"><string-name><surname>Costa</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Campilho</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hooi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Smailagic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kitani</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Faloutsos</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Galdran</surname>, <given-names>A.</given-names></string-name>: <source>Eyequal: Accurate, explainable, retinal image quality assessment. In: 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</source>. pp. <fpage>323</fpage>&#x2013;<lpage>330</lpage> (<year>2017</year>)</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>De Fauw</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ledsam</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Romera-Paredes</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nikolov</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tomasev</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Blackwell</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Askham</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Glorot</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>O&#x2019;Donoghue</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Visentin</surname>, <given-names>D.</given-names></string-name>, <etal>et al.</etal>: <article-title>Clinically applicable deep learning for diagnosis and referral in retinal disease</article-title>. <source>Nature medicine</source> <volume>24</volume>(<issue>9</issue>), <fpage>1342</fpage> (<year>2018</year>)</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Delori</surname>, <given-names>F.C.</given-names></string-name>, <string-name><surname>Goger</surname>, <given-names>D.G.</given-names></string-name>, <string-name><surname>Keilhauer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Salvetti</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Staurenghi</surname>, <given-names>G.</given-names></string-name>: <article-title>Bimodal spatial distribution of macular pigment: evidence of a gender relationship</article-title>. <source>JOSA A</source> <volume>23</volume>(<issue>3</issue>), <fpage>521</fpage>&#x2013;<lpage>538</lpage> (<year>2006</year>)</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Dieck</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ibarra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Moghul</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Yeung</surname>, <given-names>M.W.</given-names></string-name>, <string-name><surname>Pantel</surname>, <given-names>J.T.</given-names></string-name>, <string-name><surname>Thiele</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pfau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fleckenstein</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pontikos</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Krawitz</surname>, <given-names>P.M.</given-names></string-name>: <article-title>Factors in Color Fundus Photographs That Can Be Used by Humans to Determine Sex of Individuals</article-title>. <source>Translational Vision Science &#x0026; Technology</source> <volume>9</volume>(<issue>7</issue>), <fpage>8</fpage>&#x2013;<lpage>8</lpage> (06 <year>2020</year>)</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Esteva</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kuprel</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Novoa</surname>, <given-names>R.A.</given-names></string-name>, <string-name><surname>Ko</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Swetter</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Blau</surname>, <given-names>H.M.</given-names></string-name>, <string-name><surname>Thrun</surname>, <given-names>S.</given-names></string-name>: <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>. <source>Nature</source> <volume>542</volume>(<issue>7639</issue>), <fpage>115</fpage> (<year>2017</year>)</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Grote</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>: <article-title>On the ethics of algorithmic decision-making in healthcare</article-title>. <source>Journal of medical ethics</source> <volume>46</volume>(<issue>3</issue>), <fpage>205</fpage>&#x2013;<lpage>211</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Gulshan</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Peng</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Coram</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stumpe</surname>, <given-names>M.C.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Narayanaswamy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Venugopalan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Widner</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Madams</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Cuadros</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal>: <article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title>. <source>Jama</source> <volume>316</volume>(<issue>22</issue>), <fpage>2402</fpage>&#x2013;<lpage>2410</lpage> (<year>2016</year>)</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="other"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name>: <source>Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition</source>. pp. <fpage>770</fpage>&#x2013;<lpage>778</lpage> (<year>2016</year>)</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="website"><string-name><surname>Iqbal</surname>, <given-names>H.</given-names></string-name>: <source>PlotNeuralNet</source> (<year>2018</year>), <ext-link ext-link-type="uri" xlink:href="https://github.com/HarisIqbal88/PlotNeuralNet">https://github.com/HarisIqbal88/PlotNeuralNet</ext-link>, accessed: <date-in-citation content-type="access-date">2021-02-26</date-in-citation></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Kiani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Uyumazturk</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Rajpurkar</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Langlotz</surname>, <given-names>C.P.</given-names></string-name>, <string-name><surname>Ball</surname>, <given-names>R.L.</given-names></string-name>, <string-name><surname>Montine</surname>, <given-names>T.J.</given-names></string-name>, <etal>et al.</etal>: <article-title>Impact of a deep learning assistant on the histopathologic classification of liver cancer</article-title>. <source>npj Digital Medicine</source> <volume>3</volume>(<issue>1</issue>), <fpage>1</fpage>&#x2013;<lpage>8</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Kobak</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>: <article-title>The art of using t-sne for single-cell transcriptomics</article-title>. <source>Nature communications</source> <volume>10</volume>(<issue>1</issue>), <fpage>1</fpage>&#x2013;<lpage>14</lpage> (<year>2019</year>)</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="book"><string-name><surname>Kobak</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Linderman</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Steinerberger</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kluger</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>: <chapter-title>Heavy-tailed kernels reveal a finer cluster structure in t-sne visualisations</chapter-title>. In: <source>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source>. pp. <fpage>124</fpage>&#x2013;<lpage>139</lpage>. <publisher-loc>Springer</publisher-loc> (<year>2019</year>)</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Rauscher</surname>, <given-names>F.G.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>E.Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Baniasadi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Wirkner</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kirsten</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Thiery</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Engel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Loeffler</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal>: <article-title>Sex-specific differences in circumpapillary retinal nerve fiber layer thickness</article-title>. <source>Ophthalmology</source> <volume>127</volume>(<issue>3</issue>), <fpage>357</fpage>&#x2013;<lpage>368</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Linderman</surname>, <given-names>G.C.</given-names></string-name>, <string-name><surname>Rachh</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hoskins</surname>, <given-names>J.G.</given-names></string-name>, <string-name><surname>Steinerberger</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kluger</surname>, <given-names>Y.</given-names></string-name>: <article-title>Fast interpolation-based t-sne for improved visualization of single-cell rna-seq data</article-title>. <source>Nature Methods</source> <volume>16</volume>, <fpage>243</fpage>&#x2013;<lpage>245</lpage> (<year>2019</year>)</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Maaten</surname>, <given-names>L.V.D.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name>: <article-title>Visualizing data using t-sne</article-title>. <source>Journal of Machine Learning Research</source> <volume>9</volume>, <fpage>2579</fpage>&#x2013;<lpage>2605</lpage> (<year>2008</year>)</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>McKinney</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Sieniek</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Godbole</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Godwin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Antropova</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ashrafian</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Back</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Chesus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Corrado</surname>, <given-names>G.C.</given-names></string-name>, <string-name><surname>Darzi</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal>: <article-title>International evaluation of an ai system for breast cancer screening</article-title>. <source>Nature</source> <volume>577</volume>(<issue>7788</issue>), <fpage>89</fpage>&#x2013;<lpage>94</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Montavon</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Samek</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>M&#x00FC;ller</surname>, <given-names>K.R.</given-names></string-name>: <article-title>Methods for interpreting and understanding deep neural networks</article-title>. <source>Digital Signal Processing</source> <volume>73</volume>, <fpage>1</fpage>&#x2013;<lpage>15</lpage> (<year>2018</year>)</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="other"><string-name><surname>O&#x2019;Hara</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Draper</surname>, <given-names>B.A.</given-names></string-name>: <article-title>Introduction to the bag of features paradigm for image classification and retrieval</article-title>. <source>arXiv preprint</source> <pub-id pub-id-type="arxiv">1101.3354</pub-id> (<year>2011</year>)</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="other"><string-name><surname>Paschali</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Naeem</surname>, <given-names>M.F.</given-names></string-name>, <string-name><surname>Simson</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Steiger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Mollenhauer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Navab</surname>, <given-names>N.</given-names></string-name>: <article-title>Deep learning under the microscope: improving the interpretability of medical imaging neural networks</article-title>. <source>arXiv preprint</source> <pub-id pub-id-type="arxiv">1904.03127</pub-id> (<year>2019</year>)</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Poplin</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Varadarajan</surname>, <given-names>A.V.</given-names></string-name>, <string-name><surname>Blumer</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>McConnell</surname>, <given-names>M.V.</given-names></string-name>, <string-name><surname>Corrado</surname>, <given-names>G.S.</given-names></string-name>, <string-name><surname>Peng</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>D.R.</given-names></string-name>: <article-title>Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</article-title>. <source>Nature Biomedical Engineering</source> <volume>2</volume>, <fpage>158</fpage>&#x2013;<lpage>164</lpage> (<year>2019</year>)</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="other"><string-name><surname>Russakovsky</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Deng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Krause</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Satheesh</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Karpathy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bernstein</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Fei-Fei</surname>, <given-names>L.</given-names></string-name>: <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision (IJCV)</source> (<year>2015</year>)</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Sudlow</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gallacher</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Beral</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Burton</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Danesh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Downey</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Elliott</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Green</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Landray</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal>: <article-title>Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age</article-title>. <source>Plos med</source> <volume>12</volume>(<issue>3</issue>), <fpage>e1001779</fpage> (<year>2015</year>)</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="other"><string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Vanhoucke</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Ioffe</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shlens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wojna</surname>, <given-names>Z.</given-names></string-name>: <source>Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition</source>. pp. <fpage>2818</fpage>&#x2013;<lpage>2826</lpage> (<year>2016</year>)</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Yamashita</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Asaoka</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Terasaki</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Murata</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tanaka</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nakao</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Sakamoto</surname>, <given-names>T.</given-names></string-name>: <article-title>Factors in Color Fundus Photographs That Can Be Used by Humans to Determine Sex of Individuals</article-title>. <source>Translational Vision Science &#x0026; Technology</source> <volume>9</volume>(<issue>2</issue>), <fpage>4</fpage>&#x2013;<lpage>4</lpage> (01 <year>2020</year>)</mixed-citation></ref>
</ref-list>
</back>
</article>