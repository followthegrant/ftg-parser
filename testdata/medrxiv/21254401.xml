<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2021.03.26.21254401</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Health Informatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Overcoming Underrepresentation in Clinical Datasets for Accurate Subpopulation-specific Prognosis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1835-773X</contrib-id>
<name><surname>Afrose</surname><given-names>Sharmin</given-names></name>
<degrees>BS</degrees>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Song</surname><given-names>Wenjia</given-names></name>
<degrees>BS</degrees>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7867-1160</contrib-id>
<name><surname>Nemeroff</surname><given-names>Charles B.</given-names></name>
<degrees>MD PhD</degrees>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0181-5888</contrib-id>
<name><surname>Lu</surname><given-names>Chang</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8969-2792</contrib-id>
<name><surname>Yao</surname><given-names>Danfeng (Daphne)</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="corresp" rid="cor1">&#x0023;</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Computer Science</institution>, Virginia Tech, Blacksburg, VA, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychiatry and Behavioral Sciences, the University of Texas at Austin Dell Medical School</institution>, Austin, TX, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Department of Chemical Engineering</institution>, Virginia Tech, Blacksburg, VA, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x0023;</label>Corresponding Author: Danfeng (Daphne) Yao, Department of Computer Science, Virginia Tech, Blacksburg, VA 24060, USA. <email>danfeng@vt.edu</email></corresp>
<fn id="n1" fn-type="equal"><label>&#x002A;</label><p>Contributed equally</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2021.03.26.21254401</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>3</month>
<year>2021</year>
</date>
<date date-type="rev-recd">
<day>26</day>
<month>3</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>04</day>
<month>4</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="21254401.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Clinical datasets are intrinsically imbalanced, dominated by overwhelming majority groups. Off-the-shelf machine learning models optimize the prognosis of majority patient types (e.g., healthy class), causing substantial errors on the minority prediction class (e.g., disease class) and minority subpopulations (e.g., Black or young patients). For example, missed death prediction is 36.6 times higher than non-death cases in a mortality benchmark. Racial and age disparities also exist. Conventional metrics such as AUC-ROC do not reflect these deficiencies. We design a double prioritized (DP) sampling technique to improve the accuracy for underrepresented subpopulations. We report our findings on four prediction tasks over two clinical datasets, and comparisons with eight existing sampling solutions. With DP, the recall of minority classes shows 35.4&#x2013;130.4&#x0025; improvement. Compared to the state-of-the-arts, DP sampling gives 1.2&#x2013;58.8 times more balanced recalls and precisions. Our method trains customized models for specific race or age groups, a departure from the one-model-fits-all-demographics paradigm. As underrepresented groups in clinical medicine are a daily occurrence, our contributions likely have broad implications.</p>
</abstract>
<counts>
<page-count count="35"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>Charles B. Nemeroff (CBN) declares consulting for the following companies in the last 12 months: ANeuroTech (division of Anima BV), Taisho Pharmaceutical, Inc., Takeda, Signant Health, Sunovion Pharmaceuticals, Inc., Janssen Research &#x0026; Development LLC, Magstim, Inc., Navitor Pharmaceuticals, Inc., Intra-Cellular Therapies, Inc., EMA Wellness, Acadia Pharmaceuticals, Axsome, Sage, BioXcel Therapeutics, Silo Pharma, XW Pharma, Neuritek, Engrail Therapeutics, Corcept Therapeutics Pharmaceuticals Company. CBN owns stock in Xhale, Seattle Genetics, Antares, BI Gen Holdings, Inc., Corcept Therapeutics Pharmaceuticals Company, EMA Wellness. CBN serves on the scientific advisory boards of ANeuroTech (division of Anima BV), Brain and Behavior Research Foundation (BBRF), Anxiety and Depression Association of America (ADAA), Skyland Trail, Signant Health, Laureate Institute for Brain Research (LIBR), Inc., Magnolia CNS. CBN is the board of directors of Gratitude America, ADAA, Xhale Smart, Inc. CBN has patents in antipsychotic drug delivery. The other authors have no competing interests.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>No external funding was received.</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>We submitted applications/forms to access the MIMIC III dataset from PhysioNet Team in MIT Laboratory for Computational Physiology and the SEER dataset from National Cancer Institute. We were granted to use the MIMIC III and SEER datasets after going through the registration procedures.</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Researchers have trained machine learning models to predict many diseases and conditions, including Alzheimer&#x2019;s disease<sup><xref ref-type="bibr" rid="c1">1</xref></sup>, heart disease<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, risk of developing diabetic retinopathy<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, cancer risk<sup><xref ref-type="bibr" rid="c4">4</xref></sup> and survivability<sup><xref ref-type="bibr" rid="c5">5</xref></sup>, genetic testing for diseases<sup><xref ref-type="bibr" rid="c6">6</xref></sup>, hypertrophic cardiomyopathy diagnosis<sup><xref ref-type="bibr" rid="c7">7</xref></sup>, psychosis<sup><xref ref-type="bibr" rid="c8">8</xref></sup>, PTSD<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, and COVID&#x2013;19<sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Neural network powered automatic image analysis has also been shown useful for fast disease detection, e.g., breast cancer<sup><xref ref-type="bibr" rid="c16">16</xref></sup>, macular degeneration<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, lung cancer<sup><xref ref-type="bibr" rid="c39">39</xref></sup>, prostate cancer<sup><xref ref-type="bibr" rid="c40">40</xref></sup>, bladder cancer41, at-risk organs<sup><xref ref-type="bibr" rid="c42">42</xref></sup>, and musculoskeletal disorder<sup><xref ref-type="bibr" rid="c43">43</xref></sup>. A study showed that deep learning algorithms diagnose breast cancer more accurately (AUC=0.994) than 11 pathologists<sup><xref ref-type="bibr" rid="c16">16</xref></sup>. Hospitals (e.g., Cleveland Clinic&#x2019;s partnership with Microsoft<sup><xref ref-type="bibr" rid="c10">10</xref></sup>, John Hopkins hospital partnership with GE)<sup><xref ref-type="bibr" rid="c11">11</xref></sup> are reported to use predictive analytics for monitoring patients&#x2019;health status and preventing emergencies<sup><xref ref-type="bibr" rid="c12">12</xref>&#x2013;<xref ref-type="bibr" rid="c15">15</xref></sup>.</p>
<p>However, clinical datasets are intrinsically imbalanced due to the naturally occurring frequencies of data<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. The data is not evenly distributed across prediction classes (e.g., disease class vs. healthy class), race, age, or other subgroups. One example is pregnant women, who are either excluded from all clinical trials or comprise too small of a sample size to be meaningful. Data imbalance is a major cause of biased prediction results<sup><xref ref-type="bibr" rid="c17">17</xref></sup>.</p>
<p>Biased prediction results may have serious consequences for some patients. For example, a recent study showed that automatic enrollment of high&#x2013;risk patients into the health program favors white patients, although black patients had 26.3&#x0025; more chronic health conditions than equally ranked white patients<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. Similarly, algorithmic osteoarthritis pain prediction shows 43&#x0025; racial disparities<sup><xref ref-type="bibr" rid="c19">19</xref></sup>. For non&#x2013; medical applications, researchers also identified serious biases in high&#x2013;profile machine learning applications, e.g., a widely deployed recidivism prediction tool<sup><xref ref-type="bibr" rid="c20">20</xref>&#x2013;<xref ref-type="bibr" rid="c22">22</xref></sup>, online advertisement system<sup><xref ref-type="bibr" rid="c23">23</xref></sup>, Amazon&#x2019;s recruiting engine<sup><xref ref-type="bibr" rid="c24">24</xref></sup>, and face recognition system<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. The lack of external validation and overclaiming causal effect in machine learning also raise concerns<sup><xref ref-type="bibr" rid="c26">26</xref></sup>.</p>
<p>A well-known approach to the data imbalance problem is sampling. Oversampling, e.g., replicated oversampling (ROS), is to balance dataset by adding samples of the minority class; undersampling, e.g., random under&#x2013;sampling (RUS), is to balance dataset by removing samples of the majority class<sup><xref ref-type="bibr" rid="c27">27</xref></sup>. An improvement is K&#x2013;nearest neighbor (K&#x2013;NN) classifier&#x2013;based undersampling technique<sup><xref ref-type="bibr" rid="c28">28</xref></sup> (e.g., Nearmiss1, Nearmiss2, NearMiss3, Distant) that select samples from majority class based on distance from minority class samples. State-of-the-art solutions are all oversampling methods, including Synthetic Minority Over-sampling Technique (SMOTE)<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, Adaptive Synthetic Sampling (ADASYN)<sup><xref ref-type="bibr" rid="c30">30</xref></sup>, and Gamma<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. All three methods generate new minority points based on existing minority samples, namely using linear interpolation<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, gamma distribution<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, or at the class border<sup><xref ref-type="bibr" rid="c30">30</xref></sup>.</p>
<p>However, although existing sampling techniques improve the recall of a minority class, its precision is drastically reduced, e.g., 27.7&#x0025; to 78.1&#x0025; decrease in our test across four minority demographic subgroups. In addition, existing sampling studies are only evaluated based on the accuracy of prediction classes (e.g., death vs. survival). How well sampling solutions improve predictions on minority demographic groups (e.g., Black or young patients with age &#x003C; 30) has not been reported.</p>
<p>For imbalanced datasets, conventional metrics such as overall accuracy and AUC&#x2013;ROC could be seriously misleading. We examine clinical prediction benchmark<sup><xref ref-type="bibr" rid="c14">14</xref></sup> on MIMIC III and cancer survival prediction<sup><xref ref-type="bibr" rid="c5">5</xref></sup> on SEER cancer dataset. Both training datasets are imbalanced, in terms of the gender, race, or age distribution. For example, for the in-hospital mortality (IHM) prediction with MIMIC III, 70.6&#x0025; data represents White patients, whereas only 9.6&#x0025; represents Black patients. MIMIC III and SEER also have data imbalance problems among the two class labels (e.g., death vs. survival). For the IHM prediction, 86.5&#x0025; data belongs to patients who did not die in ICU, whereas only 13.5&#x0025; of data belongs to the patient who died in hospital. These data imbalances result in prediction biases. A typical neural network based machine learning model<sup><xref ref-type="bibr" rid="c14">14</xref></sup> that we tested correctly predicts 98.1&#x0025; of non-death cases, but only 30.5&#x0025; for death cases. Meanwhile, overall accuracy (computed over all patients) is 0.90 and AUC&#x2013;ROC is 0.86, as a result of the overwhelmingly good performance in the majority class. These high overall scores are misleading.</p>
<p>We present a new oversampling technique, double prioritized (DP) sampling. DP sampling improves the prediction accuracy of specific minority demographic groups. DP differs from state-of-the-art sampling methods in two main aspects. First, when duplicating minority class samples, it prioritizes specific underrepresented groups, as opposed to sampling across the entire patient population. Second, DP uses metrics to incrementally identify the optimal amount of sample duplication, as opposed to arbitrarily forcing the class ratio to be 1:1. Our experiments show that DP improves minority class recalls without substantially impacting precisions. We also define a new metric dual-class divergence, which captures the tradeoff between precision and recall &#x2013; smaller divergence values indicating more balanced precision and recall. DP&#x2019;s dual-class divergence is 1.2&#x2013;58.8 times lower than the state-of-the-art sampling methods in the mortality prediction task. Coupled with comparable recall values, these results suggest that DP sampling is more effective at correcting data imbalance for clinical machine learning.</p>
<p>Our findings have broad implications in clinical practice, as underrepresented groups in clinical medicine are a daily occurrence. Our work suggests the strong feasibility of training customized prediction models for specific subpopulations, an improvement over the one-model-fits-all-demographics paradigm. The results highlighting racial data imbalance and model specificity also have implications in genetics, because of differences in the frequency of common genetic variants in ethnic groups.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Double prioritized (DP) sampling</title>
<p>DP prioritizes a specific demographic subgroup (e.g., Asian) that suffers from data imbalance by only replicating minority prediction class (C1) cases from this group (e.g., Asian in-hospital death). In contrast, existing oversampling methods are designed for the whole population by generating more C1 cases across all demographic subgroups without differentiation. Another feature of DP sampling algorithm is its ability to gradually and dynamically identify the optimal class ratio, as opposed to simply making the class ratio reach 1:1. DP sampling incrementally increases the number of duplicated units and chooses the optimal unit number based on resulting models&#x2019;performance. <xref rid="fig1" ref-type="fig">Figure 1</xref> shows the machine learning workflow with DP sampling.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p><bold>Workflow for improving data balance in machine learning prognosis prediction using double prioritized (DP) oversampling.</bold></p>
<p><italic>Bias testing</italic> assesses the distributions of demographic groups and prediction classes in training data; <italic>DP samplin</italic>g prepares a number of new training datasets by incrementally duplicating a specific minority group, each dataset used for training a machine learning model; <italic>model selection</italic> identifies the optimal model based on balanced accuracy and AUC-PR metrics; <italic>prediction</italic> applies the selected model on new patient data.</p></caption>
<graphic xlink:href="21254401v1_fig1.tif"/>
</fig>
<p><italic>Bias testing</italic> examines the ratio among different demographic subgroups (e.g., gender, ethnicity, age) and ratio among different prediction classes (e.g., death vs. survival). If a group has a relatively low number of samples, sampling is required.</p>
<p><italic>DP sampling</italic> replicates minority class samples in the training dataset for a target demographic group up to <italic>n</italic> times. <italic>n</italic> is pre-defined. Using DP, we obtain <italic>n</italic>&#x002B;1 sets of training datasets (including the original one). Each dataset is used to train and generate a machine learning model.</p>
<p><italic>Model selection</italic> is to identify the optimal machine learning model among the <italic>n</italic>&#x002B;1 models. We evaluate model performance using a test dataset and choose a final model <italic>M&#x002A;</italic> as follows. For each model, we compute its balanced accuracy and PR_C1 metrics on the unsampled test dataset. We identify the top three models with the highest balanced accuracy values and select the model that has the highest PR_C1. No sampling is applied to the test dataset.</p>
<p><italic>Prediction</italic> applies model <italic>M&#x002A;</italic> to new patients&#x2019;records and obtains a binary class label.</p>
</sec>
<sec id="s2b">
<title>Machine learning models and metrics</title>
<p>Following Harutyunyan <italic>et al</italic>,<sup><xref ref-type="bibr" rid="c14">14</xref></sup> for the clinical prediction tasks, patients&#x2019;data is preprocessed into time&#x2013;series records and fed into LSTM models. Cancer survivability prediction utilizes a multilayer perceptron (MLP) model, following Hegselmann <italic>et al</italic>.<sup><xref ref-type="bibr" rid="c5">5</xref></sup> The model parameters remained constant in different sampling techniques (supplementary table 1). Sampling techniques are applied on training datasets before feeding the data into the model.</p>
<p>Evaluation metrics include accuracy, balanced accuracy, AUC&#x2013;ROC score, precision, recall, AUC&#x2013;PR, and F1 of minority and majority prediction classes, whole population, and various demographic subgroups, including gender (male, female), ethnicity (White, Black, Hispanic, Asian), and 8 age groups. We also define a new metric divergence to capture the disparity between precision and recall. <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> shows the dual-class divergence computation for both classes C1 and C0. Single-class divergence for C1 or C0 can also be computed (<xref ref-type="disp-formula" rid="eqn10">supplementary equations 10</xref>-<xref ref-type="disp-formula" rid="eqn11">11</xref>).
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="21254401v1_eqn1.gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s2c">
<title>Clinical datasets studied</title>
<p>We use MIMIC III<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup> and SEER<sup><xref ref-type="bibr" rid="c35">35</xref></sup> cancer datasets, both collected in US. We test existing machine learning models in a clinical prediction benchmark<sup><xref ref-type="bibr" rid="c14">14</xref></sup> for MIMIC III and cancer survival prediction<sup><xref ref-type="bibr" rid="c5">5</xref></sup> for SEER. We study a total of four binary classification tasks, in&#x2013;hospital mortality (IHM) prediction and decompensation prediction from the clinical prediction benchmark,<sup><xref ref-type="bibr" rid="c14">14</xref></sup> 5-year breast cancer survivability (BCS) prediction, and 5-year lung cancer survivability (LCS) prediction. In what follows, we denote the minority prediction class as Class 1 (or C1) and the majority class as Class 0 (or C0).</p>
<p><xref rid="fig2" ref-type="fig">Figure 2B</xref> shows the percentages of different subgroup sizes for training dataset used in BCS prediction. The BCS training set contains 199,000 samples, of which 87.3&#x0025; are in Class 0 (i.e., patients diagnosed with breast cancer and survived more than 5 years) and 0.6&#x0025; are males. The majority race group (81&#x0025;) is White. When categorizing by age, 70&#x0025; of the patients are between 40 and 70. The LCS training dataset (of size 164,443) follows similar imbalanced distributions (<xref rid="figS3" ref-type="fig">supplementary figure S3B</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p><bold>Prediction results under the original machine learning models (no sampling) and training data statistics for the 5-year breast cancer survivability (BCS) in (A) and the in-hospital mortality (IHM) tasks in (B).</bold></p>
<p>Rec_C1, Prec_C1, PR_C1, Rec_C0, Prec_C0, PR_C0, Acc, Bal_Acc, ROC stand for Recall Class 1, Precision Class 1, Area Under the Precision-Recall Curve Class 1, Recall Class 0, Precision Class 0, Area Under the Precision-Recall Curve Class 0, Accuracy, Balanced Accuracy, Area under the ROC Curve, respectively. <bold>(A)</bold> Prediction class, racial, gender, age group distribution, and prediction results for the BCS prediction. Class 1, representing death 5 years after breast cancer diagnosis, is the minority prediction class. Class 0, representing survival after 5 years, is the majority prediction class. <bold>(B)</bold> Statistics of SEER BCS dataset. <bold>(C)</bold> Prediction class, racial, gender, age group distribution, and prediction results for the IHM prediction. Class 1, representing death after staying 48 hours in intensive care units at the hospital, is the minority prediction class. Class 0, representing survival after staying 48 hours in intensive care units, is the majority prediction class. <bold>(D)</bold> Statistics of MIMIC III IHM dataset.</p></caption>
<graphic xlink:href="21254401v1_fig2.tif"/>
</fig>
<p><xref rid="fig2" ref-type="fig">Figure 2D</xref> shows the composition of IHM training data, which contains 14,681 time-series samples from MIMIC III. The majority of the records (86.5&#x0025;) belong to Class 0 (i.e., patients who do not die in hospital). The rest (13.5&#x0025;) belong to Class 1 (i.e., the patients who die in hospital). 70.6&#x0025; of the patients are White and 76&#x0025; belong to the age range [50, 90). The training set contains insufficient data for the young adult population. Distributions of the decompensation training dataset (of size 2,377,768) are similar (<xref rid="figS3" ref-type="fig">supplementary figure S3D</xref>).</p>
</sec>
<sec id="s2d">
<title>Other sampling techniques compared</title>
<p>The eight existing sampling approaches being compared include four undersampling techniques (namely, random undersampling, NearMiss1, NearMiss3, distant method), and four oversampling techniques (namely, replicated oversampling, SMOTE, ADASYN, Gamma). Undersampling balances the distribution of the two prediction classes by selecting only a subset of the majority class cases. Oversampling balances the dataset by populating the minority class.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Accuracy disparity between C0 and C1 without sampling</title>
<p>Without any sampling, the original machine learning model demonstrates substantial accuracy disparity between the majority prediction class C0 and the minority prediction class C1. <xref rid="fig2" ref-type="fig">Figure 2A</xref> shows the 5-year breast cancer survivability (BCS) prediction results for various subpopulations. For the [30, 40) age group, the recall, precision, and AUC-PR for C0 are all over 0.9, while for C1 merely 0.41, 0.69, and 0.57 are observed, respectively. A similar trend is observed for the in-hospital mortality (IHM) prediction with the MIMIC III dataset (<xref rid="fig2" ref-type="fig">figure 2C</xref>). For example, 1.9&#x0025; of non-death cases (class C0) in IHM prediction are wrong, whereas the missed mortality prediction (class C1) is 69.5&#x0025;, 36.6 times. For Black patients, while recall, precision, and AUC-PR are all above 0.9 for C0, C1 recall is 0.18, which means that for every 100 black patients who die in hospital, the model would mispredict 82 of them.</p>
<p>The overall accuracy and AUC-ROC combine the results of both C0 and C1 classes. These values are consistently high (&#x003E; 0.85 in most cases) across all tasks and subgroups, even when C1 recalls are dismal (<xref rid="fig2" ref-type="fig">figure 2</xref>). These values are dominated by the overwhelmingly high precision and recall (&#x003E; 0.9 in most cases) of the majority prediction class C0. Thus, these commonly used metrics in prediction do not reflect the minority class performance under data imbalance.</p>
</sec>
<sec id="s3b">
<title>Accuracy disparity across demographic subgroups without sampling</title>
<p>Besides disparity between prediction classes, the original model also shows disparity across demographic subgroups. For the BCS task (<xref rid="fig2" ref-type="fig">figure 2A</xref>), the disparity among age subgroups is severe. The C1 recall of age group &#x003C;30 (0.29) is only 39&#x0025; of that of the 90&#x002B; age group (0.75), resulting in a large 0.46 gap. This young group&#x2019;s C1 recall (0.29) is also significantly lower than the whole population&#x2019;s (0.50). &#x003C;30 group also has the lowest C1 precision, 0.20 lower than [80, 90) population. Racial disparity is relatively low, as the largest C1 recall difference is 0.13 between Asian (0.44) and Black (0.57) and C1 precisions are all in the range of [0.68, 0.75].</p>
<p>For the IHM prediction (<xref rid="fig2" ref-type="fig">figure 2C</xref>), Black patients have the lowest C1 recall (0.18), lower than the whole group (0.31) and Hispanic patients (0.5). Black also has the lowest C1 precision (0.46), similar to Asian (0.50), both much lower than White (0.70). The disparity among C1 recalls of various age subgroups is low, all in the range of [0.23, 0.39]. Most subgroups have somewhat similar C1 precision values, except the &#x003C;30 group. Young patients in the &#x003C;30 group have a low C1 precision of 0.25, substantially lower than the whole population (0.68).</p>
<p>Both gender groups perform similarly in both tasks, despite the fact that male patients only account for 0.6&#x0025; of the samples in the SEER dataset for BCS prediction (<xref rid="fig2" ref-type="fig">figures 2B</xref>). Young patients in the &#x003C;30 age group account for only 0.6&#x0025; and 4&#x0025; in SEER (<xref rid="fig2" ref-type="fig">figures 2B</xref>) and MIMIC III datasets (<xref rid="fig2" ref-type="fig">figure 2D</xref>), respectively. Their predictions are consistently poor. Despite the large disparity in C1 performance, C0 precisions and recalls are consistently high for all subgroups, with most values above 0.90. Despite small sample sizes, some minority demographic groups (e.g., 90&#x002B; groups in BCS prediction) have high prediction accuracies even without sampling.</p>
</sec>
<sec id="s3c">
<title>Tradeoff between C1 precision and recall</title>
<p>The eight existing sampling methods improve the recall of the minority class C1, while drastically decreasing the precision of C1, i.e., introducing more false positives (<xref rid="fig3" ref-type="fig">figure 3</xref>). For example, for Black patients in the BCS prediction, the C1 recall increment ranges from 28.3&#x0025; (NearMiss3) to 72.0&#x0025; (NearMiss1) when compared to the original model after applying existing sampling methods (<xref rid="fig3" ref-type="fig">figure 3A</xref>). Although this tradeoff between precision and recall is expected, the decrease in precision is rather significant for some existing sampling methods, e.g., 65.3&#x0025; reduction for NearMiss1. For patients in age 90&#x002B; group in the IHM prediction, C1 recall in sampled models shows 180.2&#x0025; (RUS) to 280.6&#x0025; (NearMiss1) increase, compared to the original model (<xref rid="fig3" ref-type="fig">figure 3B</xref>). It means that more mortality cases are correctly predicted. In the meantime, existing sampling methods show 27.7&#x0025; (SMOTE) to 51.0&#x0025; (Distant) decrease in C1 precision, compared to the original, giving more false positives.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p><bold>5-year breast cancer survivability (BCS) prediction for Black patients and in-hospital mortality (IHM) prediction for age group 90&#x002B; under various sampling conditions, including DP and the original machine learning model without any sampling.</bold></p>
<p><bold>(A)</bold> Prediction results from the original model (left) and different sampling models (right) for Black patients in the BCS prediction with the SEER dataset. <bold>(B)</bold> Prediction results from the original model (left) and different sampling models (right) for age group 90&#x002B; in the IHM prediction with the MIMIC III dataset.</p></caption>
<graphic xlink:href="21254401v1_fig3.tif"/>
</fig>
<p>Among the eight existing sampling methods, three undersampling methods (namely, NearMiss1, NearMiss3, and distant method) perform worse than the others, in terms of C1 AUC-PR (<xref rid="fig3" ref-type="fig">figure 3</xref>). In all cases, sampling does not substantially impact majority class C0 performance. AUC-PR C0 scores of all sampled models are comparable to that of the original model. Similar trends are observed for age group [30, 40) for BCS prediction and Black patients in IHM prediction (<xref rid="figS1" ref-type="fig">supplementary figure S1</xref>).</p>
</sec>
<sec id="s3d">
<title>DP increases C1 recall while balancing precision</title>
<p>DP differs from existing sampling methods in that it increases the original model&#x2019;s C1 recall without substantially sacrificing C1 precision (<xref rid="fig3" ref-type="fig">figures 3</xref>, <xref rid="fig4" ref-type="fig">4</xref>). For example, DP increases C1 recall by 130.4&#x0025; for age 90&#x002B; group in IHM prediction, while showing higher C1 precisions than all other sampling techniques (<xref rid="fig4" ref-type="fig">figure 4B</xref>). Compared with state-of-the-art solutions (e.g., Gamma, ADASYN, SMOTE), DP sampling offers a substantially more balanced performance for minority class C1. We further measure them based on the divergence metric next.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p><bold>Divergence scores and Class 1 (C1) recall and precision values under various sampling conditions for 5-year breast cancer survivability (BCS) prediction with the SEER dataset and in-hospital mortality (IHM) prediction with the MIMIC III dataset.</bold></p>
<p>Divergence represents the difference in precision and recall score. A low divergence score with a high recall is desirable. Original represents the original machine learning without any sampling. <bold>(A)</bold> Divergence scores (top) and C1 precision and recall (bottom) for Black patients in the BCS prediction. <bold>(B)</bold> Divergence scores (top) and C1 precision and recall (bottom) for age group 90&#x002B; in the IHM prediction.</p></caption>
<graphic xlink:href="21254401v1_fig4.tif"/>
</fig>
<p>In terms of both dual-class and Cl divergences, DP produces lower scores than state-of-the-art sampling solutions (e.g., Gamma, ADASYN, SMOTE) (<xref rid="fig4" ref-type="fig">figure 4</xref> top). Lower divergence indicates more balanced recall and precision. While producing recall values comparable to state-of-the-arts, DP gives balanced C1 precisions and recalls (<xref rid="fig4" ref-type="fig">figure 4</xref> bottom). For BCS prediction, existing sampling techniques show 1.33 (SMOTE) to 4.62 (NearMiss1) times higher dual-class divergence than DP for Black (<xref rid="fig4" ref-type="fig">figure 4A</xref>). For IHM prediction, existing samplings show 24.4 (RUS) to 58.8 (NearMiss1) times higher dual-class divergence than DP for 90&#x002B; patients (<xref rid="fig4" ref-type="fig">figure 4B</xref>). Similar trends are observed for [30, 40) group in BCS prediction (the exception of NearMiss3) and Black patients in IHM prediction (<xref rid="figS2" ref-type="fig">supplementary figure S2</xref>).</p>
</sec>
<sec id="s3e">
<title>Individually optimized subgroups with DP sampling</title>
<p>We use DP to optimize the 6 underrepresented demographic subgroups separately, which generates 6 different machine learning models for each prediction task. Each model is specifically trained to predict for the target population. We show the C1 percentages in the training datasets within subgroups before and after applying DP sampling in both BCS and IHM predictions (supplementary tables 2-3). Compared to the original model, DP sampling significantly increases the recalls of most subgroups in both SEER and MIMIC III (<xref rid="fig5" ref-type="fig">figure 5</xref>). C1 precision is reduced compared to the original model without any sampling, consistent with earlier observations. For Asian and &#x003C;30 age group in the IHM prediction, DP does not improve the original models&#x2019;C1 recalls, partly due to missing attributes and different feature representations in the very small number of test samples. For example, the test dataset only has 3 deceased patients in the &#x003C;30 age group and 9 deceased Asian patients.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p><bold>Minority class C1 performance, in terms of precision and recall, of six underrepresented demographic subgroups with DP sampling compared to the original machine learning model without any sampling in two tasks.</bold></p>
<p><bold>(A)</bold> The 5-year breast cancer survivability (BCS) prediction with the SEER dataset. <bold>(B)</bold> The in-hospital mortality (IHM) prediction with the MIMIC III dataset.</p></caption>
<graphic xlink:href="21254401v1_fig5.tif"/>
</fig>
<p>We repeat all the above experiments for the other two tasks, namely 5-year lung cancer survivability (LCS) prediction in SEER and decompensation (i.e., deterioration after 24 hours) prediction in MIMIC III and observe similar patterns (<xref rid="figS3" ref-type="fig">supplementary figures S3</xref>&#x2013;<xref ref-type="fig" rid="figS8">S8</xref>).</p>
</sec>
<sec id="s3f">
<title>Model specificity evaluation</title>
<p>In our cross-group experiments, we use the DP model trained for group A (e.g., Black) to predict group B (e.g., Hispanic). This cross application aims to evaluate the specificity of machine learning models with respect to race and age. We perform both cross-race and cross-age-group experiments for BCS prediction (<xref rid="fig6" ref-type="fig">figure 6</xref>) and IHM prediction (<xref rid="figS9" ref-type="fig">supplementary figure S9</xref>). In most cases, the C1 recall and balanced accuracy are the highest when the race or age group of patients being predicted on matches the race or age group that the DP model is designed for.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p><bold>Minority class (C1) recall and balanced accuracy results from cross-group experiments where DP models trained for a specific demographic group is applied to patients of other groups in the 5-year breast cancer survivability (BCS) prediction.</bold></p>
<p>DP trained for Black represents the machine learning model for Black patients that is obtained using the double prioritized sampling method, similarly for Hispanic and Asian. Performance of the original machine learning model without DP or any sampling is also shown. C1 recalls and balanced accuracies of four trained machine learning models being applied to three races for the BCS prediction are shown in <bold>(A)</bold> and <bold>(B)</bold>, respectively. Similarly, cross-age-group results for the BCS prediction are shown in <bold>(C)</bold> and <bold>(D)</bold>. In all DP rows (except &#x003C;30), highest values are where the race/age group of patients being predicted on matches the race/age group that the DP model is designed for.</p></caption>
<graphic xlink:href="21254401v1_fig6.tif"/>
</fig>
<p>BCS model&#x2019;s race specificity is obvious. For example, when predicting Asian patients&#x2019;breast cancer survivability, the DP Asian model (0.769) outperforms the DP Black model (0.439), DP Hispanic model (0.364), and the original model without DP (0.439) in terms of C1 recall (<xref rid="fig6" ref-type="fig">figure 6A</xref>). A similar-but-less-pronounced trend is observed in the IHM prediction for Hispanic and Black, i.e., DP models specifically trained for them outperform other models when being used to predict Hispanic or Black patients, respectively (<xref rid="figS9" ref-type="fig">supplementary figure S9A, B</xref>). These observations indicate DP models&#x2019;specificity with respect to race, also confirming the need for training specialized machine learning models for individual underrepresented ethnic groups.</p>
<p>Model specificity is distinctively observed for the 90&#x002B; age group, as the C1 recall on 90&#x002B; patients is the highest when its specific DP model is used in the BCS prediction (<xref rid="fig6" ref-type="fig">figure 6C</xref>) and IHM prediction (<xref rid="figS9" ref-type="fig">supplementary figure S9C</xref>). When making BCS prediction on 90&#x002B; years old patients, DP 90&#x002B; model (0.830) outperforms the DP &#x003C;30 model (0.661), DP [30, 40) model (0.714), and the original model (0.750) in terms of C1 recall (<xref rid="fig6" ref-type="fig">figure 6C</xref>). A more drastic trend is observed in the IHM prediction, when the DP 90&#x002B; model gives 2 to 3 times C1 recall than the other models (<xref rid="figS9" ref-type="fig">supplementary figure S9C</xref>).</p>
<p>The model specificity between [30, 40) and &#x003C; 30 age groups is weak. For example, when predicting BCS on age group &#x003C;30, the DP [30, 40) model outperforms the DP &#x003C;30 model, suggesting possibly merging the two age groups during training in the future (<xref rid="fig6" ref-type="fig">figure 6C</xref>). The overall age specificity in the IHM prediction (<xref rid="figS9" ref-type="fig">supplementary figure S9C</xref>) is weaker than that of BCS prediction.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Because underrepresentation is prevalent in clinical medicine, our findings likely have broad implications beyond the specific datasets and minority groups studied. Fully understanding the accuracy gaps associated with imbalanced data helps reduce life-threatening mistakes. A key first step is to identify the minority prediction class and minority demographic groups in the training dataset. Vast disparity exists between minority C1 and majority C0 classes and among demographic subgroups. For example, young patients under 40 are underrepresented in SEER and MIMIC III and consistently exhibit low C1 recalls.</p>
<p>Our results suggest that prioritized oversampling is highly effective for improving C1 recalls of minority demographic groups. DP&#x2019;s main feature is maintaining the balance between C1 recall and precision (i.e., low divergence), while improving C1 recall. By duplicating specific minority demographic C1 samples, as opposed to the entire C1 population (as in all existing sampling methods), DP improves the model&#x2019;s specificity for that subgroup.</p>
<p>Conventional machine learning prognoses follow a one-model-fits-all-demographics paradigm. In contrast, DP sampling enables one to train models for specific underrepresented age or racial groups, not having to use the same model for the entire patient population. Our age model specificity results strongly suggest training a specific machine learning model for the oldest-old age group (typically defined as 85&#x002B;)<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, a growing population in the US<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. Our experiment also suggests that machine learning prognosis models need to recognize racial heterogeneity, as we find that a model optimized for one race (e.g., White) may not predict well on another race. This trend is consistent in both the SEER and MIMIC III datasets, indicating the existence of unique racial features. Models from adjacent age groups, e.g., &#x003C;30 and [30, 40), exhibit some compatibility. DP&#x2019;s ability to support heterogeneity in machine learning design is also potentially useful for prediction problems where diverse patterns are expected, e.g., distinct posttraumatic stress responses in subpopulations<sup><xref ref-type="bibr" rid="c34">34</xref></sup>.</p>
<p>Model specialization still needs to rely on the whole group samples. Training a model solely based on particular subgroup samples (e.g., Black patients) gives poor results (Supplementary Discussion), worse than the original model on almost all metrics, due to small sample sizes. This result suggests the importance of involving all samples in training, which forms a necessary starting point for further model optimization. The whole population training takes the full advantage of shared evolutionary features before subsequent model specialization.</p>
<p>Existing sampling practices artificially force the class ratio to reach 1:1, which does not necessarily benefit the minority class performance. In contrast, DP gradually identifies the optimal units of duplicated samples based on metrics. We observe that after a certain number of units, further increase may lead to plateaued recall but substantially decreased precision in C1. This observation shows the importance of dynamically monitoring the minority class performance during sampling.</p>
<p>When training and testing machine learning models, using multiple metrics (e.g., balanced accuracy, separate metrics for C1) is crucial. Commonly used metrics (e.g., AUC-ROC, accuracy) are heavily influenced by the majority class and fail to reflect minority performance, when used on imbalanced datasets. Our new divergence metric is useful for capturing the tradeoff between C1 recall and precision. We envision that DP oversampling is universally applicable to all medical datasets, given their intrinsic data imbalance characteristic. Future directions include exploring how data underrepresentation impacts the quality of medical image analysis, as well as mutation-based evolutionary computation<sup><xref ref-type="bibr" rid="c44">44</xref></sup>.</p>
</sec>
</body>
<back>
<sec id="s5">
<title>Contributors</title>
<p>DY conceived and designed the study. DY and CL conceived the DP sampling method. SA conducted experiments on MIMIC III and analyzed the data. WS conducted experiments on SEER and analyzed the data. SA and WS cross checked the validity of each other&#x2019;s data. SA and WS designed the sampling comparisons. SA, WS, and DY wrote the manuscript. CL and CBN provided strategic guidance. All authors proofread the manuscript and provided feedback.</p>
</sec>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>The MIMIC III and SEER data used in this study are not publicly downloadable but can be requested at their original sites. Parties interested in data access should visit the MIMIC III website (<ext-link ext-link-type="uri" xlink:href="https://mimic.physionet.org/gettingstarted/access/">https://mimic.physionet.org/gettingstarted/access/</ext-link>) and the SEER website (<ext-link ext-link-type="uri" xlink:href="https://seer.cancer.gov/data/access.html">https://seer.cancer.gov/data/access.html</ext-link>) to submit access requests.</p>
<p>
<ext-link ext-link-type="uri" xlink:href="https://mimic.physionet.org/gettingstarted/access/">https://mimic.physionet.org/gettingstarted/access/</ext-link>
</p>
<p>
<ext-link ext-link-type="uri" xlink:href="https://seer.cancer.gov/data/access.html">https://seer.cancer.gov/data/access.html</ext-link>
</p>
</sec>
<sec id="s6">
<title>Declaration of competing interests</title>
<p>CBN declares consulting for the following companies in the last 12 months: ANeuroTech (division of Anima BV), Taisho Pharmaceutical, Inc., Takeda, Signant Health, Sunovion Pharmaceuticals, Inc., Janssen Research &#x0026; Development LLC, Magstim, Inc., Navitor Pharmaceuticals, Inc., Intra-Cellular Therapies, Inc., EMA Wellness, Acadia Pharmaceuticals, Axsome, Sage, BioXcel Therapeutics, Silo Pharma, XW Pharma, Neuritek, Engrail Therapeutics, Corcept Therapeutics Pharmaceuticals Company. CBN owns stock in Xhale, Seattle Genetics, Antares, BI Gen Holdings, Inc., Corcept Therapeutics Pharmaceuticals Company, EMA Wellness. CBN serves on the scientific advisory boards of ANeuroTech (division of Anima BV), Brain and Behavior Research Foundation (BBRF), Anxiety and Depression Association of America (ADAA), Skyland Trail, Signant Health, Laureate Institute for Brain Research (LIBR), Inc., Magnolia CNS. CBN is the board of directors of Gratitude America, ADAA, Xhale Smart, Inc. CBN has patents in antipsychotic drug delivery. The other authors have no competing interests.</p>
</sec>
<sec id="s7">
<title>Data Sharing</title>
<p>The MIMIC III and SEER data used in this study are not publicly downloadable but can be requested at their original sites. Parties interested in data access should visit the MIMIC III website (<ext-link ext-link-type="uri" xlink:href="https://mimic.physionet.org/gettingstarted/access/">https://mimic.physionet.org/gettingstarted/access/</ext-link>) and the SEER website (<ext-link ext-link-type="uri" xlink:href="https://seer.cancer.gov/data/access.html">https://seer.cancer.gov/data/access.html</ext-link>) to submit access requests.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1</label><mixed-citation publication-type="journal"><string-name><surname>Parisot</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ktena</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Ferrante</surname> <given-names>E</given-names></string-name>, <etal>et al.</etal> <article-title>Disease prediction using graph convolutional networks: application to autism spectrum disorder and Alzheimer&#x2019;s disease</article-title>. <source>Medical image analysis</source> <year>2018</year>; <volume>48</volume>: <fpage>117</fpage>&#x2013; <lpage>30</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2</label><mixed-citation publication-type="journal"><string-name><surname>Malav</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kadam</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kamat</surname> <given-names>P.</given-names></string-name> <article-title>Prediction of heart disease using k-means and artificial neural network as Hybrid Approach to Improve Accuracy</article-title>. <source>International Journal of Engineering and Technology</source> <year>2017</year>; <volume>9</volume>(<issue>4</issue>): <fpage>3081</fpage>&#x2013;<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3</label><mixed-citation publication-type="website"><string-name><surname>Bora</surname> <given-names>A</given-names></string-name>, <string-name><surname>Balasubramanian</surname> <given-names>S</given-names></string-name>, <string-name><surname>Babenko</surname> <given-names>B</given-names></string-name>, <etal>et al.</etal> <article-title>Predicting the risk of developing diabetic retinopathy using deep learning</article-title>. <source>The Lancet Digital Health</source> <year>2020</year>; published online November 26. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S2589-7500(20)30250-8">https://doi.org/10.1016/S2589-7500(20)30250-8</ext-link>.</mixed-citation></ref>
<ref id="c4"><label>4</label><mixed-citation publication-type="journal"><string-name><surname>Ten Haaf</surname> <given-names>K</given-names></string-name>, <string-name><surname>Jeon</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tammem&#x00E4;gi</surname> <given-names>MC</given-names></string-name>, <etal>et al.</etal> <article-title>Risk prediction models for selection of lung cancer screening candidates: A retrospective validation study</article-title>. <source>PLoS Med</source> <year>2017</year>; <volume>14</volume>(<issue>4</issue>): <fpage>e1002277</fpage>.</mixed-citation></ref>
<ref id="c5"><label>5</label><mixed-citation publication-type="other"><string-name><surname>Hegselmann</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gruelich</surname> <given-names>L</given-names></string-name>, <string-name><surname>Varghese</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dugas</surname> <given-names>M.</given-names></string-name> <article-title>Reproducible Survival Prediction with SEER Cancer Data</article-title>. <source>Machine Learning for Healthcare Conference</source> <year>2018</year>: <fpage>49</fpage>&#x2013;<lpage>66</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6</label><mixed-citation publication-type="journal"><string-name><surname>Tandy-Connor</surname> <given-names>S</given-names></string-name>, <string-name><surname>Guiltinan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Krempely</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>False-positive results released by direct-to-consumer genetic tests highlight the importance of clinical confirmation testing for appropriate patient care</article-title>. <source>Genetics in Medicine</source> <year>2018</year>; <volume>20</volume>(<issue>12</issue>): <fpage>1515</fpage>&#x2013;<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7</label><mixed-citation publication-type="website"><string-name><surname>Augusto</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Davies</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Bhuva</surname> <given-names>AN</given-names></string-name>, <etal>et al.</etal> <article-title>Diagnosis and risk stratification in hypertrophic cardiomyopathy using machine learning wall thickness measurement: a comparison with human test-retest performance</article-title>. <source>The Lancet Digital Health</source> <year>2020</year>; published online December 3. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S2589-7500(20)30267-3">https://doi.org/10.1016/S2589-7500(20)30267-3</ext-link>.</mixed-citation></ref>
<ref id="c8"><label>8</label><mixed-citation publication-type="journal"><string-name><surname>Raket</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Jaskolowski</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kinon</surname> <given-names>BJ</given-names></string-name>, <etal>et al.</etal> <article-title>Dynamic ElecTronic hEalth reCord deTection (DETECT) of Individuals at Risk of a First Episode of Psychosis: A Case-Control Development and Validation Study</article-title>. <source>The Lancet Digital Health</source> <year>2020</year>; <volume>2</volume>: <fpage>e229</fpage>&#x2013;<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9</label><mixed-citation publication-type="journal"><string-name><surname>Pullano</surname> <given-names>G</given-names></string-name>, <string-name><surname>Valdano</surname> <given-names>E</given-names></string-name>, <string-name><surname>Scarpa</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rubrichi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Colizza</surname> <given-names>V.</given-names></string-name> <article-title>Evaluating the Effect of Demographic Factors, Socioeconomic Factors, and Risk Aversion on Mobility During the COVID-19 Epidemic in France Under Lockdown: A Population-based Study</article-title>. <source>Lancet Digit Health</source> <year>2020</year>; <volume>2</volume>(<issue>12</issue>): <fpage>e638</fpage>&#x2013;<lpage>e49</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10</label><mixed-citation publication-type="website"><string-name><surname>Gauher</surname> <given-names>S</given-names></string-name>, <string-name><surname>Boylu</surname> <given-names>F.</given-names></string-name> <article-title>Cleveland Clinic to Identify At-Risk Patients in ICU using Cortana Intelligence</article-title>. <source>Microsoft</source> <year>2016; published online September 26</year>. <ext-link ext-link-type="uri" xlink:href="https://docs.microsoft.com/en-us/archive/blogs/machinelearning/cleveland-clinic-to-identify-at-risk-patients-in-icu-using-cortana-intelligence-suite">https://docs.microsoft.com/en-us/archive/blogs/machinelearning/cleveland-clinic-to-identify-at-risk-patients-in-icu-using-cortana-intelligence-suite</ext-link> (accessed <date-in-citation content-type="access-date">December 15, 2020</date-in-citation>).</mixed-citation></ref>
<ref id="c11"><label>11</label><mixed-citation publication-type="website"><collab>Command Center to Improve Patient Flow</collab>. <source>Johns Hopkins Medicine</source> <year>2016; published online March 1</year>. <ext-link ext-link-type="uri" xlink:href="https://www.hopkinsmedicine.org/news/articles/command-center-to-improve-patient-flow">https://www.hopkinsmedicine.org/news/articles/command-center-to-improve-patient-flow</ext-link> (accessed <date-in-citation content-type="access-date">December 15, 2020</date-in-citation>).</mixed-citation></ref>
<ref id="c12"><label>12</label><mixed-citation publication-type="journal"><string-name><surname>Awad</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bader-El-Den</surname> <given-names>M</given-names></string-name>, <string-name><surname>McNicholas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Briggs</surname> <given-names>J.</given-names></string-name> <article-title>Early hospital mortality prediction of intensive care unit patients using an ensemble learning approach</article-title>. <source>International Journal of Medical Informatics</source> <year>2017</year>; <volume>108</volume>: <fpage>185</fpage>&#x2013;<lpage>95</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13</label><mixed-citation publication-type="website"><string-name><surname>Sennaar</surname> <given-names>K.</given-names></string-name> <article-title>How America&#x2019;s 5 Top Hospitals are Using Machine Learning Today</article-title>. <source>Emerj</source> <year>2020</year>; published online March 24. <ext-link ext-link-type="uri" xlink:href="https://emerj.com/ai-sector-overviews/top-5-hospitals-using-machine-learning/">https://emerj.com/ai-sector-overviews/top-5-hospitals-using-machine-learning/</ext-link> (accessed <date-in-citation content-type="access-date">December 15, 2020</date-in-citation>)</mixed-citation></ref>
<ref id="c14"><label>14</label><mixed-citation publication-type="journal"><string-name><surname>Harutyunyan</surname> <given-names>H</given-names></string-name>, <string-name><surname>Khachatrian</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kale</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Ver Steeg</surname> <given-names>G</given-names></string-name>, <string-name><surname>Galstyan</surname> <given-names>A.</given-names></string-name> <article-title>Multitask learning and benchmarking with clinical time series data</article-title>. <source>Scientific Data</source> <year>2019</year>; <volume>6</volume>(<issue>1</issue>): <fpage>1</fpage>&#x2013;<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15</label><mixed-citation publication-type="other"><string-name><surname>Johnson</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Pollard</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Mark</surname> <given-names>RG</given-names></string-name>. <article-title>Reproducibility in critical care: a mortality prediction case study</article-title>. <source>Machine Learning for Healthcare Conference</source> <year>2017</year>: <fpage>361</fpage>&#x2013;<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16</label><mixed-citation publication-type="journal"><string-name><surname>Bejnordi</surname> <given-names>BE</given-names></string-name>, <string-name><surname>Veta</surname> <given-names>M</given-names></string-name>, <string-name><surname>Van Diest</surname> <given-names>PJ</given-names></string-name>, <etal>et al.</etal> <article-title>Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</article-title>. <source>JAMA</source> <year>2017</year>; <volume>318</volume>(<issue>22</issue>): <fpage>2199</fpage>&#x2013;<lpage>210</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17</label><mixed-citation publication-type="journal"><string-name><surname>Johnson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname> <given-names>TM</given-names></string-name>. <article-title>Survey on deep learning with class imbalance</article-title>. <source>Journal of Big Data</source> <year>2019</year>; <volume>6</volume>(<issue>1</issue>):<fpage>1</fpage>&#x2013;<lpage>54</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18</label><mixed-citation publication-type="journal"><string-name><surname>Obermeyer</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Powers</surname> <given-names>B</given-names></string-name>, <string-name><surname>Vogeli</surname> <given-names>C</given-names></string-name>, <string-name><surname>Mullainathan</surname> <given-names>S.</given-names></string-name> <article-title>Dissecting racial bias in an algorithm used to manage the health of populations</article-title>. <source>Science</source> <year>2019</year>; <volume>366</volume>(<issue>6464</issue>): <fpage>447</fpage>&#x2013;<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19</label><mixed-citation publication-type="journal"><string-name><surname>Pierson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cutler</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Leskovec</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mullainathan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Obermeyer</surname> <given-names>Z.</given-names></string-name> <article-title>An algorithmic approach to reducing unexplained pain disparities in underserved populations</article-title>. <source>Nature Medicine</source> <year>2021</year>; <volume>27</volume>(<issue>1</issue>): <fpage>136</fpage>&#x2013;<lpage>40</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20</label><mixed-citation publication-type="website"><string-name><surname>Yong</surname> <given-names>E.</given-names></string-name> <article-title>A Popular Algorithm Is No Better at Predicting Crimes Than Random People</article-title>. <source>The Atlantic</source> <year>2018</year>; published online January 17. <ext-link ext-link-type="uri" xlink:href="https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/">https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/</ext-link> (accessed <date-in-citation content-type="access-date">December 20, 2020</date-in-citation>).</mixed-citation></ref>
<ref id="c21"><label>21</label><mixed-citation publication-type="journal"><string-name><surname>Dressel</surname> <given-names>J</given-names></string-name>, <string-name><surname>Farid</surname> <given-names>H.</given-names></string-name> <article-title>The accuracy, fairness, and limits of predicting recidivism</article-title>. <source>Science Advances</source> <year>2018</year>; <volume>4</volume>(<issue>1</issue>): <fpage>eaao5580</fpage>.</mixed-citation></ref>
<ref id="c22"><label>22</label><mixed-citation publication-type="website"><string-name><surname>Angwin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mattu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kirchner</surname> <given-names>L.</given-names></string-name> <article-title>Machine Bias: There&#x2019;s software used across the country to predict future criminals and it&#x2019;s biased against blacks</article-title>. <source>PROPUBLICA</source> <year>2016; published online May 23</year>. <ext-link ext-link-type="uri" xlink:href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</ext-link> (accessed <date-in-citation content-type="access-date">December 15, 2020</date-in-citation>).</mixed-citation></ref>
<ref id="c23"><label>23</label><mixed-citation publication-type="journal"><string-name><surname>Sweeney</surname> <given-names>L.</given-names></string-name> <article-title>Discrimination in Online Ad Delivery</article-title>. <source>Queue</source> <year>2013</year>; <volume>11</volume>(<issue>3</issue>): <fpage>10</fpage>&#x2013;<lpage>29</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24</label><mixed-citation publication-type="website"><string-name><surname>Dastin</surname> <given-names>J.</given-names></string-name> <article-title>Amazon scraps secret AI recruiting tool that showed bias against women</article-title>. <source>REUTERS</source> <year>2018</year>; published online October 10. <ext-link ext-link-type="uri" xlink:href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G</ext-link> (accessed <date-in-citation content-type="access-date">December 15 2020</date-in-citation>).</mixed-citation></ref>
<ref id="c25"><label>25</label><mixed-citation publication-type="other"><string-name><surname>Buolamwini</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gebru</surname> <given-names>T.</given-names></string-name> <article-title>Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</article-title>. <person-group person-group-type="editor">In: <string-name><surname>Sorelle</surname> <given-names>AF</given-names></string-name>, <string-name><surname>Christo</surname> <given-names>W</given-names></string-name>, editors</person-group>. <source>Proceedings of the 1st Conference on Fairness, Accountability and Transparency. PMLR</source> <year>2018</year>: <fpage>77</fpage>&#x2013;<lpage>91</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26</label><mixed-citation publication-type="website"><string-name><surname>Wilkinson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Arnold</surname> <given-names>KF</given-names></string-name>, <string-name><surname>Murray</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>van Smeden</surname> <given-names>M</given-names></string-name>, <string-name><surname>Carr</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sippy</surname> <given-names>R</given-names></string-name>, <string-name><surname>de Kamps</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beam</surname> <given-names>A</given-names></string-name>, <string-name><surname>Konigorski</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lippert</surname> <given-names>C</given-names></string-name>, <string-name><surname>Gilthorpe</surname> <given-names>MS</given-names></string-name>. <article-title>Time to reality check the promises of machine learning-powered precision medicine</article-title>. <source>The Lancet Digital Health</source> <year>2020</year>; published online September 16. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S2589-7500(20)30200-4">https://doi.org/10.1016/S2589-7500(20)30200-4</ext-link>.</mixed-citation></ref>
<ref id="c27"><label>27</label><mixed-citation publication-type="other"><string-name><surname>Van Hulse</surname> <given-names>J</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname> <given-names>T</given-names></string-name>, <string-name><surname>Napolitano</surname> <given-names>A.</given-names></string-name> <article-title>Experimental Perspectives on Learning from Imbalanced Data</article-title>. <source>Proceedings of the 24th international conference on Machine learning</source> <year>2007</year>: <fpage>935</fpage>&#x2013;<lpage>942</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28</label><mixed-citation publication-type="other"><string-name><surname>Mani</surname> <given-names>I</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>I.</given-names></string-name> <article-title>kNN Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction</article-title>. <source>Proceedings of Workshop on Learning from Imbalanced Datasets</source> <year>2003</year>.</mixed-citation></ref>
<ref id="c29"><label>29</label><mixed-citation publication-type="journal"><string-name><surname>Chawla</surname> <given-names>NV</given-names></string-name>, <string-name><surname>Bowyer</surname> <given-names>KW</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>LO</given-names></string-name>, <string-name><surname>Kegelmeyer</surname> <given-names>WP</given-names></string-name>. <article-title>SMOTE: synthetic minority over-sampling technique</article-title>. <source>J Artif Int Res</source> <year>2002</year>; <volume>16</volume>(<issue>1</issue>): <fpage>321</fpage>&#x2013;<lpage>57</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30</label><mixed-citation publication-type="other"><string-name><surname>He</surname> <given-names>H</given-names></string-name>, <string-name><surname>Bai</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Garcia</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Li</surname> <given-names>S.</given-names></string-name> <article-title>ADASYN: Adaptive synthetic sampling approach for imbalanced learning</article-title>. <source>IEEE International Joint Conference on Neural Networks</source> <year>2008</year>: <fpage>1322</fpage>&#x2013;<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c31"><label>31</label><mixed-citation publication-type="journal"><string-name><surname>Kamalov</surname> <given-names>F</given-names></string-name>, <string-name><surname>Denisov</surname> <given-names>D.</given-names></string-name> <article-title>Gamma distribution-based sampling for imbalanced data</article-title>. <source>Knowledge-Based Systems</source> <year>2020</year>; <volume>207</volume>: <fpage>106368</fpage>.</mixed-citation></ref>
<ref id="c32"><label>32</label><mixed-citation publication-type="journal"><string-name><surname>Johnson</surname> <given-names>AEW</given-names></string-name>, <string-name><surname>Pollard</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Shen</surname> <given-names>L</given-names></string-name>, <etal>et al.</etal> <article-title>MIMIC-III, a freely accessible critical care database</article-title>. <source>Scientific Data</source> <year>2016</year>; <volume>3</volume>(<issue>1</issue>): <fpage>160035</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33</label><mixed-citation publication-type="journal"><string-name><surname>Galatzer-Levy</surname> <given-names>IR</given-names></string-name>, <string-name><surname>Karstoft</surname> <given-names>KI</given-names></string-name>, <string-name><surname>Statnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shalev</surname> <given-names>AY</given-names></string-name>. <article-title>Quantitative forecasting of PTSD from early trauma responses: A machine learning application</article-title>. <source>J Psychiatr Res</source>. <year>2014</year> <month>Dec</month>; <volume>59</volume>: <fpage>68</fpage>&#x2013;<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34</label><mixed-citation publication-type="journal"><string-name><surname>Galatzer-Levy</surname> <given-names>IR</given-names></string-name>, <string-name><surname>Bonanno</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Bush</surname> <given-names>DEA</given-names></string-name>, <string-name><surname>LeDoux</surname> <given-names>JE</given-names></string-name>. <article-title>Heterogeneity in threat extinction learning: substantive and methodological considerations for identifying individual difference in response to stress</article-title>. <source>Front. Behav. Neurosci</source>. <year>2013</year>. <volume>7</volume>.</mixed-citation></ref>
<ref id="c35"><label>35</label><mixed-citation publication-type="website"><collab>SEER Incidence Data, 1975 &#x2013; 2017</collab>. <source>National Cancer Institute, Surveillance, Epidemiology, and End Results Program</source>. <ext-link ext-link-type="uri" xlink:href="https://seer.cancer.gov/data/">https://seer.cancer.gov/data/</ext-link></mixed-citation></ref>
<ref id="c36"><label>36</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname> <given-names>SB</given-names></string-name>, <string-name><surname>Oh</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Park</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Choi</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Wee</surname> <given-names>JH</given-names></string-name>. <article-title>Differences in youngest-old, middle-old, and oldest-old patients who visit the emergency department</article-title>. <source>Clin Exp Emerg Med</source>. <year>2018</year> <month>Dec</month>; <volume>5</volume>(<issue>4</issue>): <fpage>249</fpage>&#x2013;<lpage>255</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37</label><mixed-citation publication-type="website"><collab>2017 Profile of Older Americans</collab>. <source>Administration for Community Living</source>. <year>2018</year>. Available at: <ext-link ext-link-type="uri" xlink:href="https://acl.gov/sites/default/files/Aging%20and%20Disability%20in%20America/2017OlderAmericansProfile.pdf">https://acl.gov/sites/default/files/Aging%20and%20Disability%20in%20America/2017OlderAmericansProfile.pdf</ext-link></mixed-citation></ref>
<ref id="c38"><label>38</label><mixed-citation publication-type="journal"><string-name><surname>Yan</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Weeks</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Xin</surname> <given-names>H</given-names></string-name>, <string-name><surname>Swaroop</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chew</surname> <given-names>EY</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>W.</given-names></string-name> <article-title>Deep-learning-based prediction of late age-related macular degeneration progression</article-title>. <source>Nature Machine Intelligence</source> <year>2020</year>. <volume>2</volume>: <fpage>141</fpage>&#x2013;<lpage>150</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39</label><mixed-citation publication-type="journal"><string-name><surname>Mukherjee</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schicht</surname> <given-names>A</given-names></string-name>, <string-name><surname>Balagurunathan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Napel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gillies</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>S</given-names></string-name>, <string-name><surname>Thieme</surname> <given-names>A</given-names></string-name>, <string-name><surname>Leung</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gevaert</surname> <given-names>O.</given-names></string-name> <article-title>A shallow convolutional neural network predicts prognosis of lung cancer patients in multi-institutional computed tomography image datasets</article-title>. <source>Nature Machine Intelligence</source> <year>2020</year>. <volume>2</volume>: <fpage>274</fpage>&#x2013;<lpage>282</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40</label><mixed-citation publication-type="journal"><string-name><surname>Tolkach</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dohmg&#x00F6;rgen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Toma</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kristiansen</surname> <given-names>G.</given-names></string-name> <article-title>High-accuracy prostate cancer pathology using deep learning</article-title>. <source>Nature Machine Intelligence</source> <year>2020</year>. <volume>2</volume>: <fpage>411</fpage>&#x2013;<lpage>418</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>P</given-names></string-name>, <string-name><surname>McGough</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>Pathologist-level interpretable whole-slide cancer diagnosis with deep learning</article-title>. <source>Nature Machine Intelligence</source> <year>2019</year>. <volume>1</volume>: <fpage>236</fpage>&#x2013;<lpage>245</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42</label><mixed-citation publication-type="journal"><string-name><surname>Tang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <etal>et al.</etal> <article-title>Clinically applicable deep learning framework for organs at risk delineation in CT images</article-title>. <source>Nature Machine Intelligence</source> <year>2019</year>. <volume>1</volume>: <fpage>480</fpage>&#x2013;<lpage>491</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43</label><mixed-citation publication-type="journal"><string-name><surname>Varma</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gardner</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>Automated abnormality detection in lower extremity radiographs using deep learning</article-title>. <source>Nature Machine Intelligence</source> <year>2019</year>. <volume>1</volume>: <fpage>578</fpage>&#x2013;<lpage>583</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44</label><mixed-citation publication-type="journal"><string-name><surname>Miikkulainen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Forrest</surname> <given-names>S.</given-names></string-name> <article-title>A biological perspective on evolutionary computation</article-title>. <source>Nature Machine Intelligence</source> <year>2021</year>. <volume>3</volume>: <fpage>9</fpage>&#x2013;<lpage>15</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s8">
<title>Supplementary Information</title>
<sec id="s8a">
<title>Supplementary Methods</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supplementary Table 1:</label>
<caption><p>Learning Parameters for Four Prediction Models</p></caption>
<graphic xlink:href="21254401v1_tblS1.tif"/>
</table-wrap>
<p>For the IHM prediction task with MIMIC III datasets, training involves 100 epochs or stops early based on validation performance. For DP, we run for 50 epochs up to 20 additional specific minority units. For the Decomp prediction task with MIMIC III datasets, training involves 50 epochs or stops early based on validation performance. For DP experiments, we run for 5 epochs up to 20 additional specific minority units. The SEER cancer dataset is smaller, thus for the cancer prediction tasks, we run 20 epochs for the original code and 6 epochs for sampling experiments. Each epoch produces a machine learning model; to choose the final model, we first identify the top three models based on balanced accuracy and then select the one with the highest precision&#x2013;recall curve value of the minority class (denoted as PR_C1).</p>
<p>For random undersampling technique, we randomly select the majority samples for three times and build models from these three training datasets. We use soft voting ensemble technique to average the result from the models. For SEER dataset, 80&#x0025; is used for training and 10&#x0025; for testing. Following Hegselmann <italic>et al</italic>.<sup><xref ref-type="bibr" rid="c5">5</xref></sup>, our results reported are validation results. For MIMIC III, the percentages are 70&#x0025; for training, 15&#x0025; for validation, and 15&#x0025; for testing.</p>
</sec>
<sec id="s8b">
<title>Supplementary Equations</title>
<p>BCS Class 1: Patient does not survive more than 5 years after breast cancer diagnosis;</p>
<p>IHM Class 1: Based on the first 48 hours ICU information, patient dies in ICU</p>
<p>LCS Class 1: Patient survives more than 5 years after lung cancer diagnosis</p>
<p>Decomp Class 1: Patient&#x2019;s health deteriorates after 24 hours
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="21254401v1_eqn1a.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="21254401v1_eqn2.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="21254401v1_eqn3.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="21254401v1_eqn4.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="21254401v1_eqn5.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="21254401v1_eqn6.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="21254401v1_eqn7.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="21254401v1_eqn8.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="21254401v1_eqn9.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="21254401v1_eqn10.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="21254401v1_eqn11.gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s8c">
<title>Supplementary Results</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure S1:</label>
<caption><p><bold>5-year breast cancer survivability (BCS) prediction for age group [30,40) and in-hospital mortality (IHM) prediction for Black patients under various sampling conditions, including DP and the original machine learning model without any sampling.</bold></p>
<p><bold>(A)</bold> Prediction results from the original model (left) and different sampling models (right) for age group [30,40) in the BCS prediction with the SEER dataset. Class 1, representing death 5 years after breast cancer diagnosis, is the minority prediction class. Class 0, representing survival after 5 years, is the majority class. <bold>(B)</bold> Prediction results from the original model (left) and different sampling models (right) for Black patients in the IHM prediction with the MIMIC III dataset. Class 1, representing death after staying 48 hours in intensive care units at the hospital, is the minority prediction class. Class 0, representing survival after staying 48 hours in intensive care units, is the majority prediction class.</p></caption>
<graphic xlink:href="21254401v1_figS1.tif"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure S2:</label>
<caption><p><bold>Divergence scores and Class 1 (C1) recall and precision values under various sampling conditions for 5-year breast cancer survivability (BCS) prediction with the SEER dataset and in-hospital mortality (IHM) prediction with the MIMIC III dataset.</bold></p>
<p>Divergence represents the difference in precision and recall score. A low divergence score with a high recall is desirable. Original represents the original machine learning without any sampling. <bold>(A)</bold> Divergence scores (top) and C1 precision and recall (bottom) for age group [30, 40) in the BCS prediction. <bold>(B)</bold> Divergence scores (top) and C1 precision and recall (bottom) for Black patients in the IHM prediction</p></caption>
<graphic xlink:href="21254401v1_figS2.tif"/>
</fig>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supplementary Table 2:</label>
<caption><p>Breast Cancer Survivability (BCS) prediction results with the SEER dataset on 6 minority demographic groups for the original model and the DP model. The added DP units column shows the optimal number of additional units of specific C1 subgroup samples in DP sampling. All C1 percentages and numbers refer to the training datasets. Testing dataset is not sampled.</p></caption>
<graphic xlink:href="21254401v1_tblS2.tif"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Supplementary Table 3:</label>
<caption><p>In-Hospital Mortality (IHM) prediction results with the MIMIC III dataset on 6 minority demographic groups for the original model and the DP model. The added DP units column shows the optimal number of additional units of specific C1 subgroup samples in DP sampling. All C1 percentages and numbers refer to the training datasets. Testing dataset is not sampled</p></caption>
<graphic xlink:href="21254401v1_tblS3.tif"/>
</table-wrap>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplementary Figure S3:</label>
<caption><p><bold>Prediction results under the original machine learning models (no sampling) and training data statistics for the 5-year lung cancer survivability (LCS) and the decompensation tasks.</bold></p>
<p>Rec_C1, Prec_C1, PR_C1, Rec_C0, Prec_C0, PR_C0, Acc, Bal_Acc, ROC stand for Recall Class 1, Precision Class 1, Area Under the Precision-Recall Curve Class 1, Recall Class 0, Precision Class 0, Area Under the Precision-Recall Curve Class 0, Accuracy, Balanced Accuracy, Area under the ROC Curve, respectively. <bold>(A)</bold> Prediction class, racial, gender, age group distribution, and prediction results for the LCS prediction. The minority Class 1 represents patients who survive lung cancer for at least 5 years after the diagnosis. <bold>(B)</bold> Statistics of SEER LCS dataset. <bold>(C)</bold> Prediction class, racial, gender, age group distribution, and prediction results for the decompensation prediction. The minority Class 1 represents patients whose health deteriorates after 24 hours. <bold>(D)</bold> Statistics of MIMIC III decompensation dataset.</p></caption>
<graphic xlink:href="21254401v1_figS3.tif"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplementary Figure S4:</label>
<caption><p><bold>Lung cancer survivability (LCS) prediction results under various sampling conditions.</bold></p>
<p>(<bold>A</bold>) Prediction results from the original model (left) and different sampling models for Asian patients (right). (<bold>B</bold>) Prediction results from the original model (left) and different sampling models for age group [30, 40) (right).</p></caption>
<graphic xlink:href="21254401v1_figS4.tif"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Supplementary Figure S5:</label>
<caption><title>Decompensation prediction results under various sampling conditions.</title>
<p>Because of the large training data size (2,377,768), we have to exclude the sampling methods that require expensive pairwise distance computation. (<bold>A</bold>) Prediction results from the original model (left) and different sampling models for Black patients (right). (<bold>B</bold>) Prediction results from the original model (left) and different sampling models for age group 90&#x002B; (right).</p></caption>
<graphic xlink:href="21254401v1_figS5.tif"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Supplementary Figure S6:</label>
<caption><p><bold>Divergence scores, C1 recall and precision values under various sampling conditions for the LCS prediction with the SEER dataset.</bold></p>
<p>Divergence represents the difference in precision and recall score. A low divergence score with a high recall is desirable. <bold>(A)</bold> Divergence scores (top), C1 precision and recall (bottom) for Asian patients. <bold>(B)</bold> Divergence scores (top), C1 precision and recall (bottom) for [30, 40) age group.</p></caption>
<graphic xlink:href="21254401v1_figS6.tif"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Supplementary Figure S7:</label>
<caption><p><bold>Divergence scores, C1 recall and precision values under various sampling conditions for the decompensation prediction with the MIMIC III dataset.</bold></p>
<p>Because of the large training data size (2,377,768), we have to exclude the sampling methods that require expensive pairwise distance computation. Divergence represents the difference in precision and recall score. A low divergence score with a high recall is desirable. <bold>(A)</bold> Divergence scores (top), C1 precision and recall (bottom) for Black patients. <bold>(B)</bold> Divergence scores (top), C1 precision and recall (bottom) for 90&#x002B; age group.</p></caption>
<graphic xlink:href="21254401v1_figS7.tif"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Supplementary Figure S8:</label>
<caption><p><bold>Minority class C1 performance, in terms of precision and recall, of six underrepresented demographic subgroups with DP sampling compared to the original machine learning model without any sampling in two tasks.</bold></p>
<p><bold>(A)</bold> The 5-year lung cancer survivability (LCS) prediction with the SEER dataset. <bold>(B)</bold> The decompensation prediction with the MIMIC III dataset.</p></caption>
<graphic xlink:href="21254401v1_figS8.tif"/>
</fig>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Supplementary Table 4:</label>
<caption><p>Lung Cancer Survivability (LCS) prediction results with the SEER dataset on 6 minority demographic groups for the original model and the DP model. The added DP units column shows the optimal number of additional units of specific C1 subgroup samples in DP sampling. All C1 percentages and numbers refer to the training datasets. Testing dataset is not sampled.</p></caption>
<graphic xlink:href="21254401v1_tblS4.tif"/>
</table-wrap>
<table-wrap id="tblS5" orientation="portrait" position="float">
<label>Supplementary Table 5:</label>
<caption><p>Decompensation prediction results with the MIMIC III dataset on 6 minority demographic groups for the original model and the DP Model. The added DP units column shows the optimal number of additional units of specific C1 subgroup samples in DP sampling. All C1 percentages and numbers refer to the training datasets. Testing dataset is not sampled.</p></caption>
<graphic xlink:href="21254401v1_tblS5.tif"/>
</table-wrap>
<fig id="figS9" position="float" fig-type="figure">
<label>Supplementary Figure S9:</label>
<caption><p><bold>Minority class (C1) recall and balanced accuracy results from the cross-group experiment where DP models trained for a specific demographic group is applied to patients of other groups in the in-hospital mortality (IHM) prediction.</bold></p>
<p>DP trained for Black represents the machine learning model for Black patients that is obtained using the double prioritized sampling method, similarly for Hispanic and Asian. Performance of the original machine learning model without DP or any sampling is also shown. C1 recalls and balanced accuracies of four trained machine learning models being applied to three races for the IHM prediction are shown in <bold>(A)</bold> and <bold>(B)</bold>, respectively. Similarly, cross-age-group results for the IHM prediction are shown in <bold>(C)</bold> and <bold>(D)</bold>.</p></caption>
<graphic xlink:href="21254401v1_figS9.tif"/>
</fig>
</sec>
<sec id="s8d">
<title>Supplementary Discussion</title>
<p>Results of the 5-year lung cancer survivability (LCS) prediction on SEER dataset and decompensation prediction on MIMIC III dataset are consistent with earlier observations. In the LCS prediction, the minority Class 1 represents patients who survive lung cancer for at least 5 years after the diagnosis. For LCS, the recall, precision, AUC-PR are all above 0.93 for Class 0, while the values for Class 1 are only 0.60, 0.72, and 0.73, respectively (<xref rid="figS3" ref-type="fig">Supplementary Figure S3A</xref>). Regarding the disparity among demographic subgroups, the original model misses only 15&#x0025; of survival cases in the age [30, 40) group, while it misses a significant amount of them in age [70, 80) and age [80, 90) subgroups.</p>
<p>For decompensation prediction, the minority Class 1 represents patients whose health condition deteriorates after 24 hours. We also observe the accuracy disparity among Class 1 and Class 0. For example, C1 recall is merely 0.13, while C0 recall is near perfect (<xref rid="figS3" ref-type="fig">Supplementary Figure S3C</xref>). The disparity also exists among demographic subgroups, e.g., C1 precision is 0.91 for Asians and only 0.35 for Hispanics, which means the model incorrectly predicts only 9&#x0025; of Asian patients whereas the model provides incorrect predictions for 65&#x0025; of Hispanic Patients in the test dataset.</p>
<p>For LCS prediction, sampling results for two minority demographic groups, namely Asian and age group [30, 40) are shown in <xref rid="figS4" ref-type="fig">Supplementary Figure S4</xref>. Because of the class distribution in the [30, 40) subgroup is more balanced (33&#x0025; Class 1, Supplementary Table 4), the original C1 precision and recall are rather good (0.85 for both). After applying DP, both slightly increase by 2.5&#x0025;. Results of other sampling methods in LCS prediction (<xref rid="figS4" ref-type="fig">Supplementary Figure S4</xref>) follow the similar pattern as the BCS prediction.</p>
<p>For the decompensation prediction, we apply two most commonly used sampling techniques, random undersampling (RUS) and replicated oversampling (ROS). We have to exclude other sampling techniques as their pairwise quadratic distance computation is expensive for 2,377,768 patients&#x2019;time series training dataset. Overall, RUS performs the worst in terms of both C1 recall and precision (<xref rid="figS5" ref-type="fig">Supplementary Figure S5</xref>) as RUS discards around 94&#x0025; of data (decompensation C1 is 2&#x0025;) which contributes to a huge loss of information. ROS shows a higher recall with a low precision rate than DP. When applying ROS sampling on Black patients, C1 recall increases 320.2&#x0025;, whereas C1 precision decreases 88.9&#x0025; compared to the original model.</p>
<p>Consistent with other prediction tasks, DP shows low divergence between precision and recall (<xref rid="figS6" ref-type="fig">Supplementary Figures S6</xref> and S7). In the LCS prediction, for Asian patients, other sampling techniques show 1.07 (SMOTE) to 7.90 (NearMiss1) times dual-class divergence compared to DP&#x2019;s (<xref rid="figS6" ref-type="fig">Supplementary Figures S6A</xref>). For age [30, 40) patients, DP shows perfectly balanced precision and recall (0 divergence). For the decompensation prediction on Black patients, the DP model improves C1 recalls by 158&#x0025; and shows 3.5 times lower divergence score compared to the original model (<xref rid="figS7" ref-type="fig">Supplementary Figure S7A</xref>). Supplementary Table 5 shows the number of additional units of specific C1 subgroup samples in DP for the decompensation prediction. For all subgroups in the LCS and decompensation predictions, DP increases C1 recalls while balancing C1 precisions (<xref rid="figS8" ref-type="fig">Supplementary Figure S8</xref>), consistent with earlier observations.</p>
<sec id="s8d1">
<title>Comparing DP with Subgroup-only Training</title>
<p>For comparison, we conduct a subgroup-only experiment, where we train a machine learning model on the much smaller demographic-specific dataset. The training data only contains samples of a specific subgroup, e.g., Black patients, excluding all other races. In the IHM prediction task, there are 1,407 Black patients out of 14,681 records. Learning parameters stay the same. No sampling is done. We observe that training a model solely based on particular subgroup samples (e.g., Black patients) gives poor results, much worse than the original model or DP on almost all metrics, due to small sample sizes. In IHM prediction for Black patients, the subgroup training approach shows 40.2&#x0025; decrease in C1 recall from the original model (without sampling) and 66.7&#x0025; decrease from the DP model. For BCS prediction, C1 recall and precision of most minority race subgroups are lower than the original model, e.g., 12.9&#x0025; decrease for Hispanic C1 recall and 6.7&#x0025; for Black C1 precision. These findings suggest the importance of training the initial machine learning model from the entire dataset covering the whole patient population, as DP does.</p>
</sec>
</sec>
</sec>
</back>
</article>