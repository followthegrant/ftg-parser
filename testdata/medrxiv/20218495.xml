<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2020.10.26.20218495</article-id>
<article-version>1.2</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neurology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Speech cortical activation and connectivity in typically developing children and those with listening difficulties</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9450-9486</contrib-id>
<name><surname>Stewart</surname><given-names>Hannah J.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Cash</surname><given-names>Erin K.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Hunter</surname><given-names>Lisa L.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Maloney</surname><given-names>Thomas</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Vannest</surname><given-names>Jennifer</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Moore</surname><given-names>David R.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Division of Psychology and Language Sciences, University College London</institution>, London, <country>UK</country></aff>
<aff id="a2"><label>2</label><institution>Communication Sciences Research Center, Cincinnati Children&#x2019;s Hospital Medical Center</institution>, Cincinnati, Ohio, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Pediatric Neuroimaging Research Consortium, Cincinnati Children&#x2019;s Hospital Medical Center</institution>, Cincinnati, Ohio, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Department of Pediatrics, University of Cincinnati College of Medicine</institution>, Ohio, <country>USA</country></aff>
<aff id="a5"><label>5</label><institution>Department of Otolaryngology, College of Medicine, University of Cincinnati</institution>, Cincinnati, Ohio, <country>USA</country></aff>
<aff id="a6"><label>6</label><institution>Manchester Centre for Audiology and Deafness, University of Manchester</institution>, Manchester, M13 9PL, <country>UK</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label><bold>Correspondence:</bold> Hannah J. Stewart, Division of Psychology and Language Sciences, University College London, London UK, <email>h.stewart@ucl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2020.10.26.20218495</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>10</month>
<year>2020</year>
</date>
<date date-type="rev-recd">
<day>20</day>
<month>4</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>4</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="20218495.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Listening difficulties (LiD) in people who have normal audiometry (LiD) are a widespread but poorly understood form of hearing impairment. Recent research suggests that childhood LiD are cognitive rather than auditory in origin. We assessed that hypothesis using behavioral testing and fMRI with 43 typically developing children and 42 age matched (6-13 years old) children with LiD, categorized by caregiver report (ECLiPS). The children with LiD had clinically normal hearing. For sentence listening tasks, we found no group differences in fMRI brain cortical activation by increasingly complex speech, from phonology to intelligibility to semantics. Using resting state fMRI, we examined the temporal connectivity of cortical auditory and related speech perception networks. Significant group differences were found only in cortical connections engaged by more complex speech processing. The strength of the affected connections was related to the children&#x2019;s performance on tests of dichotic listening, speech-in-noise, attention, memory and verbal vocabulary. Together, these results support the hypothesis that childhood LiD reflects cognitive and language rather than auditory deficits.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>listening difficulties</kwd>
<kwd>pediatric</kwd>
<kwd>fMRI</kwd>
<kwd>resting state</kwd>
<kwd>connectivity</kwd>
<kwd>speech perception</kwd>
</kwd-group>
<counts>
<page-count count="35"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>This research was supported by NIH R01DC014078, NIH 2UL1TR001425 and by Cincinnati Children's Research Foundation. DRM is supported in part by the NIHR Manchester Biomedical Research Centre.</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>Cincinnati Children's Hospital Medical Center Institutional Review Board.
Ethical approval given.</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>INTRODUCTION</title>
<p>Hearing loss is the one of most prevalent causes, globally, of years lived with disability (<xref ref-type="bibr" rid="c75">Vos et al., 2016</xref>). However, about half of US adults who have difficulty hearing or listening have a normal pure-tone audiogram, the standard clinical test of hearing loss (<xref ref-type="bibr" rid="c24">Edwards, 2020</xref>). Their difficulty may be due to a number of causes including &#x2018;sub-clinical&#x2019; ear pathology, and impaired central auditory nervous system (CANS) or higher cortical function (<xref ref-type="bibr" rid="c51">Moore, 2018</xref>). Listening difficulties (LiD) are also frequently reported among children with clinically normal hearing, but with academic, language and attention problems (<xref ref-type="bibr" rid="c21">Dillon &#x0026; Cameron, 2021</xref>; D. R. <xref ref-type="bibr" rid="c53">Moore et al., 2018</xref>), a condition sometimes referred to as developmental &#x2018;auditory processing disorder&#x2019;.</p>
<p>The mechanisms underlying childhood LiD without hearing loss remain poorly understood. Speech understanding requires coalescence of acoustic features from bottom-up streaming in the auditory pathway with top-down linguistic processes deriving from widespread cortical areas involved with semantic representation and cognitive functions, notably memory and attention (<xref ref-type="bibr" rid="c40">Hickok &#x0026; Poeppel, 2007</xref>; <xref ref-type="bibr" rid="c67">R&#x00F6;nnberg et al., 2019</xref>; <xref ref-type="bibr" rid="c73">Shinn-Cunningham, 2008</xref>). Whether LiD are primarily bottom-up or top-down in nature is a long-standing debate (<xref ref-type="bibr" rid="c56">Neijenhuis et al., 2019</xref>). Interactions between cortical auditory and related speech perception systems, resulting in impaired higher level processing of target sounds, may hold the key since poor performance on complex auditory tasks such as speech listening in noise is more associated with cognitive function than with more simple auditory tasks (<xref ref-type="bibr" rid="c52">Moore et al., 2010</xref>). To identify cortical areas used in this process, we adopted fMRI methods developed to examine the hierarchy of cortical speech processing in adults (<xref ref-type="bibr" rid="c32">Halai et al., 2015</xref>; <xref rid="c70" ref-type="bibr">Scott, 2000</xref>).</p>
</sec>
<sec id="s2">
<title>RESULTS</title>
<sec id="s2a">
<title>LiD - auditory or cognitive?</title>
<p>Child participants (6-13 years old; <xref rid="tbl1" ref-type="table">Table 1</xref>) scoring within the bottom 10th percentile on a caregiver checklist of everyday listening skills, the ECLiPS (<xref ref-type="bibr" rid="c5">Barry et al., 2015</xref>; <xref ref-type="bibr" rid="c65">Roebuck &#x0026; Barry, 2018</xref>), and children with a diagnosis of auditory processing disorder (see Methods), were classified as having LiD (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). All participants had sensitive audiograms (0.25 &#x2013; 16 kHz, <xref rid="fig1" ref-type="fig">Figure 1A</xref>), normal ear function (D. R. <xref ref-type="bibr" rid="c55">Moore et al., 2020</xref>; <xref ref-type="bibr" rid="c61">Petley et al., 2021</xref>) and the auditory brainstem and envelope following responses of the children with LiD did not significantly differ from the typically developing children (<xref ref-type="bibr" rid="c41">Hunter et al., 2021</xref>), suggestive of normal sub-cortical auditory function. Children. We first used task-based and then resting state fMRI (rs-fMRI) to assess how speech perception networks function in typically developing (TD) children and children with LiD.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Participant details broken down by scan type - fMRI speech listening task and the resting state (RS).</p></caption>
<graphic xlink:href="20218495v2_tbl1.tif"/>
</table-wrap>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>All children had normal tone hearing and were able to perform the speech perception task in the scanner. (A) Group mean hearing thresholds. (B) Total ECLiPS score. (C) fMRI paradigm, the children were asked if the picture matched who spoke the sentence. If it was a match, they pressed the right button (the stickered hand); if it was not a match, they pressed the left button. Data acquisition (grey shading) was turned off/on for the presentation of the auditory stimuli (HUSH/sparse scanning). (D-F) Spectrograms of &#x2018;The two children are laughing&#x2019;. Time is represented on the x-axis (0.0&#x2013;1.60 s) and frequency on the y-axis (0.0&#x2013;4.4 kHz). The shading of the trace in each time/frequency region is controlled by the amount of energy in the signal at that particular frequency and time (red = more energy, blue = less energy). Clear speech is intelligible with intonation. Rotated speech is not intelligible, though some phonetic features and some of the original intonation are preserved. Rotated noise-vocoded speech is completely unintelligible, but preserves the character of the envelope and some spectral detail. (G) Group mean response times and accuracy for clear sentences, rotated sentences and rotated&#x002B;vocoded sentences. Error bars represent the standard error of the mean. TD children are in grey and LiD in green.</p></caption>
<graphic xlink:href="20218495v2_fig1.tif"/>
</fig>
</sec>
<sec id="s2b">
<title>Task-based cortical activation</title>
<p>During task-based MRI acquisition intervals (<xref rid="fig1" ref-type="fig">Figure 1C</xref>), children listened to clear (&#x2018;human&#x2019;; <xref rid="fig1" ref-type="fig">Figure 1D</xref>) or distorted (&#x2018;alien&#x2019;) spoken sentences that we asked them to match to a visual cartoon representation. Distorted sentences were speech that was rotated (<xref rid="fig1" ref-type="fig">Figure 1E</xref>) or rotated AND vocoded (<xref rid="fig1" ref-type="fig">Figure 1F</xref>) to retain prosody while successively eliminating meaning and spectral cues. We were able to explore the neural processing of different levels of speech listening by contrasting the different sentence types, as per <xref ref-type="bibr" rid="c32">Halai et al (2015)</xref>, modified from <xref ref-type="bibr" rid="c70">Scott et al (2000)</xref>. These levels were given proxy analysis labels to describe the skills required for successful speech perception ability when the two conditions were subtracted from one another, an analysis contrast. By contrasting sentences that had intonation, phonetics and prosody with those having only prosody (&#x2018;Phonology&#x2019;: rotated &#x003E; rotated&#x002B;vocoded), we targeted the bridge from bottom-up to top-down processing where auditory information is classified into linguistically meaningful units (<xref ref-type="bibr" rid="c10">Brodbeck et al., 2018</xref>). As part of the top-down processes, sentences are judged for intelligibility by the listener. We accessed this judgement by contrasting sentences that had intelligibility, intonation, phonetics and prosody to those retaining intonation, phonetics and prosody (&#x2018;Intelligibility&#x2019;: clear &#x003E; rotated). Finally, meaning was attached to the intelligible sentence, a process we accessed by contrasting two sentence types, one with all speech attributes and the other lacking all attributes except prosody (&#x2018;Semantics&#x2019;: clear &#x003E; rotated&#x002B;vocoded).</p>
<p>All children responded with high accuracy on the sentence recognition task, and did so rapidly, (<xref rid="fig1" ref-type="fig">Figure 1G</xref>) suggesting that they maintained attention throughout the task. TD children were more accurate than children with LiD (F(1, 83) = 8.77, p = .004, &#x03B7;<sub>p</sub><sup>2</sup> = .096), but there was no difference in accuracy between sentence types (F(1.78, 148.03) = 2.11, p = .13, &#x03B7;<sub>p</sub><sup>2</sup> = .025. There was no significant difference in reaction time between the groups (F(1, 83) = .89, p = .35, &#x03B7;<sub>p</sub><sup>2</sup> = .011). While there was no difference in accuracy between sentence types (F(1.78, 148.03) = 2.11, p = .13, &#x03B7;<sub>p</sub><sup>2</sup> = .025), across all participants, clear sentences were responded to fastest (F(2, 166) = 11.50, p &#x003C; .001, &#x03B7;<sub>p</sub><sup>2</sup> = .12) compared to rotated (p &#x003C; .001, d = -.41) and rotated&#x002B;vocoded (p &#x003C; .001, d = -.46) sentences. There were no interactions.</p>
<p>Consistent with a top-down, linguistic model, we predicted that all stimuli would activate the primary auditory cortex (i.e., Heschl&#x2019;s gyrus) equally in all children, but that intelligibility and semantic stimuli would activate the speech processing areas (e.g. superior temporal gyrus, Wernicke&#x2019;s) less in children with LiD than in TD children. However, the two groups did not differ significantly in cortical activation in the Phonology, Intelligibility or Semantics contrasts after correcting for multiple comparisons (<xref rid="fig2" ref-type="fig">Figures 2A-C</xref>, threshold = 2.3, MCC .95). Averaged across all children (n = 85), the contrasts (Figure D-F) showed bilateral activation for all stages of speech listening (Phonology, Intelligibility and Semantics). Coordinates for the maximum intensity of the activated regions are shown in Supplementary Material Table 2. The Phonology contrast (rotated &#x003E; rotated&#x002B;vocoded; <xref rid="fig2" ref-type="fig">Figure 2D</xref>) showed bilateral activation in the middle and superior temporal gyrus, including Heschl&#x2019;s gyrus, and temporal pole. Activation was also found in the left hemisphere in the temporal fusiform cortex, angular gyrus and lateral occipital cortex. The Intelligibility contrast (clear &#x003E; rotated; <xref rid="fig2" ref-type="fig">Figure 2E</xref>) produced similar activation to the Phonology contrast, bilaterally in the middle and superior temporal gyrus (anterior and posterior) and left frontal orbital cortex. Activation extended anteriorly along the left temporal gyrus and into Broca&#x2019;s area. The Semantics contrast (clear &#x003E; rotated&#x002B;vocoded; <xref rid="fig2" ref-type="fig">Figure 2F</xref>) also showed bilateral activation in the auditory cortices (middle and superior temporal gyrus including Heschl&#x2019;s gyrus and planum temporale) with activation extending along the left temporal fusiform and frontal orbital cortices and right parahippocampal gyrus.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Selected regions of interest (ROIS, from the fMRI task) used in the rs-fMRI ROI-to-ROI analysis.</p>
<p>Threshold = 4.0, MCC .95. The full table of all ROIs can be found in Supplementary Material Table 1.</p></caption>
<graphic xlink:href="20218495v2_tbl2.tif"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>LiD and TD groups showed similar areas of cortical activation in all three contrasts from the fMRI listening task, with no statistical difference between the two groups after correcting for multiple comparisons. We took the cortical activation across all participants and created parcellated ROIs (<xref ref-type="fig" rid="fig3">Figure 3</xref>) for use in the rs-fMRI analysis (figure 4). Second level GLM analysis for (A-C) groups (threshold = 2.3, MCC .95) and (D-F) all participants (threshold = 4.0, MCC .95) in the fMRI task, coordinates (&#x00B1;60, -5, -10). Coordinates for maximum intensity voxels for (D-F) can be found in Supplementary Material <xref rid="tbl2" ref-type="table">Table 2</xref>. Contrasts are: (A, D) Phonology (green: rotated &#x003E; rotated&#x002B;vocoded), (B, E) Intelligibility (blue: clear &#x003E; rotated) and (C, F) Semantics (red: clear &#x003E; rotated&#x002B;vocoded). Images are in neurological orientation. MRIcroGL was used for visualization.</p></caption>
<graphic xlink:href="20218495v2_fig2.tif"/>
</fig>
</sec>
<sec id="s2c">
<title>Cortical functional connectivity</title>
<p>We predicted that children with LiD would show diminished functional connectivity in speech perception networks compared with TD children. To investigate how those cortical areas work together, we used the task-based cortical activation results to inform a region-of-interest (ROI) based functional connectivity analysis of a separate rs-fMRI acquired in the same scanning session. The rs-fMRI scan allowed measurement of the temporal correlation of spontaneous fluctuations in the BOLD signal between anatomically separated brain regions (<xref ref-type="bibr" rid="c8">Biswal, 2012</xref>), capturing spontaneous interactions between brain regions in functionally associated networks (<xref ref-type="bibr" rid="c15">Damoiseaux et al., 2006</xref>). Networks of ROIs suitable for rs-fMRI connectivity analysis were created by parceling activation produced by the three task contrasts from above (Phonology, Intelligibility, and Semantics), combined across groups, using regions from the pediatric ADHD-200 sample (<xref ref-type="bibr" rid="c6">Bellec et al., 2017</xref>; <xref ref-type="bibr" rid="c14">Craddock et al., 2012</xref>). This data-driven, spatially-constrained method divided Phonology-derived cortical activity into 17 ROIs, Intelligibility into 23 ROIs, and Semantics into 31 ROIs (<xref rid="fig3" ref-type="fig">Figure 3</xref>; Brodman&#x2019;s area, maximum intensity coordinates and ROI sizes are in Supplementary Material Table 1). Connectivity was compared between groups (TD = 42, LiD = 39), controlling for age, using a general linear model (GLM) for each of the three networks (<xref rid="fig3" ref-type="fig">Figure 3A-C</xref>, Supplementary Material Table 1).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>ROIs used for the rs-functional connectivity and their networks across all participants. The cortical activity across all participants in the fMRI task (see Figure 2 D-F) covered large areas and so they were parcellated into smaller ROIs (A-C) for the rs-functional connectivity analysis by applying data-driven spatially constrained parcellation to the areas of activation from the fMRI sentence recognition task using the pediatric ADHD-200 sample. The red lines (D-F) indicate the ROI-to-ROI connections analyzed in each network. Maximum intensity coordinates can be found in Supplementary Material Table 1. Networks are: (A, D) Phonology (green: rotated &#x003E; rotated&#x002B;vocoded), (B, E) Intelligibility (blue: clear &#x003E; rotated) and (C, F) Semantics (red: clear &#x003E; rotated&#x002B;vocoded). Images are in neurological orientation. For visualization, BrainNet software was used to display foci in (A-C) (Xia, Wang &#x0026; He, 2013).</p></caption>
<graphic xlink:href="20218495v2_fig3.tif"/>
</fig>
<p><xref rid="fig3" ref-type="fig">Figure 3 (D-F)</xref> shows resting state functional connectivity in the three speech networks, summed across groups. In the Phonology network, each group had connectivity among regions covering bilateral middle and superior temporal gyri, temporal pole, planum temporale, and left planum polare and supramarginal gyrus (<xref rid="fig3" ref-type="fig">Figure 3D</xref>). No significant group differences (p-FDR) were found in the Phonology network (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Each group had connectivity within the Intelligibility network covering bilateral middle/superior temporal gyrus and temporal pole along with left pars opercularis, frontal orbital cortex and supramarginal gyrus (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). After false discovery rate (FDR) correction, TD children were found to have a significantly weaker temporal correlation between ROIs in Broca&#x2019;s area (left pars opercularis) and left middle temporal gyrus (posterior) compared to children with LiD (connection 14-23 in <xref rid="fig4" ref-type="fig">Figures 4B, D</xref>, <xref rid="tbl3" ref-type="table">Table 3</xref>). In the Semantics network, both groups of children had connectivity between bilateral middle and superior temporal gyrus, left Heschl&#x2019;s gyrus, pars triangularis, frontal orbital cortex, planum temporale, temporal fusiform gyrus and right parahippocampal gyrus and planum polare (<xref rid="fig3" ref-type="fig">Figure 3F</xref>). Group comparisons showed that, compared to children with LiD, the TD children had weaker temporal correlations between ROIs in the right parahippocampal gyrus and left Heschl&#x2019;s gyrus, left middle temporal gyrus, right superior temporal gyrus and right planum temporale (connections 16-9, 16-19, 16-1 and 16-5, respectively, in <xref rid="fig4" ref-type="fig">Figures 4C, D</xref>, <xref rid="tbl3" ref-type="table">Table 3</xref>). However, they also had stronger temporal correlations between ROIs in the left temporal fusiform cortex and right superior temporal gyrus (connections 10-8 and 10-27). <xref rid="fig4" ref-type="fig">Figure 4D</xref> shows this polarity of hyperconnectivity, among children with LiD, for connections associated with the right parahippocampal gyrus (connections 16-1, 16-5, 16-9, 16-19) and, for TD children, connections associated with the left fusiform cortex (connections 10-8, 10-27). <xref rid="fig4" ref-type="fig">Figure 4D</xref> also shows the strong effect sizes of each of these group differences.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>No significant group difference (TD &#x003E; LiD, corrected for age) was found in the temporal connectivity of the Phonology network. Group differences were found in the more advanced speech listening networks (Intelligibility and Semantics). See <xref rid="fig4" ref-type="fig">Figure 4</xref>.</p></caption>
<graphic xlink:href="20218495v2_tbl3.tif"/>
</table-wrap>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>ROI-to-ROI resting state connectivity: the difference between the groups&#x2019; listening networks grew from no statistical differences in the phonology network to a minor difference in the intelligibility network to more widespread differences in the semantics network. (A-C) The group comparisons without the effect of age for (A) Phonology, (B) Intelligibility and (C) Semantics networks. Thicker, more saturated color lines represent stronger connections between cortical areas. Note that the colored bar connectivity z score scales vary slightly between connectivity wheels. (D) Details of the ROI-to-ROI connectivity values (left axis) for each group (LiD green, TD grey) and effect sizes (yellow line, right axis) of the group comparisons without the effect of age. The connections plotted are the ones highlighted as having a significant group difference in the GLM comparing groups without the effect of age (B and C). Connections Intelligibility 14-23 and Semantics 16-1,16-5, 16-9 and 16-19 all show the TD group as having weaker connectivity than the LiD group. While Semantics 10-8 and 10-27 show the TD group as having stronger connectivity than the LiD group.</p></caption>
<graphic xlink:href="20218495v2_fig4.tif"/>
</fig>
</sec>
<sec id="s2d">
<title>Relation of cortical functional connectivity to behavioral measures</title>
<p>We finally explored how the cortical connectivity related to behavioral tasks assessing speech in noise ability, dichotic listening and cognition. As shown by <xref ref-type="bibr" rid="c61">Petley et al. (2021)</xref> performance on these behavioural tasks was significantly better in the TD than in the LiD group.</p>
<p>For Semantics connections involving the left posterior temporal fusiform cortex (10-8, 10-27; <xref rid="fig5" ref-type="fig">Fig. 5</xref>), significant positive correlations were found for dichotic listening ability (SCAN: competing words and sentences). In contrast, remaining Intelligibility and Semantics connections had a series of significant negative correlations for speech in noise and dichotic listening ability (SCAN: auditory figure ground and competing words) and cognition (NIH toolbox: verbal vocabulary, selective attention, task switching and episodic memory). Interpretation of direction of functional connectivity is ambiguous; an increase in connectivity strength does not necessarily mean improved ability (<xref ref-type="bibr" rid="c58">Parente et al., 2018</xref>). However, these correlations show that, as the children&#x2019;s behavioural test scores improved, the connection strength within the Intelligibility network decreased, Semantics network connections linked to the right posterior parahippocampal gyrus decreased, and connections linked to the left posterior temporal fusiform cortex increased. The remaining behavioral measures did not relate significantly to connectivity (<xref rid="fig5" ref-type="fig">Figure 5</xref>, Supplementary Material Table 3 and <xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Heat map of brain and behavioral score correlations. Colored squares indicate significant low-moderate Pearson correlations (p &#x003C; .05) between the connections in Figure 3D and speech in noise, auditory processing skills and cognition. Correlations associated with the Broca and right posterior parahippocampal gyrus connections are negative while correlations associated with the left posterior temporal fusiform cortex connections are positive. See Supplementary Materials Table 3 for correlation details. Note that the direction of the correlations flip for the same connections as in the group comparisons in Figure 3 B-D. Intelligibility 14-23 and Semantics 16-1,16-5, 16-9 and 16-19 showed that the TD group had weaker connectivity compared to the LiD group and these connections show negative correlations with behavioral scores. While Semantics 10-8 and 10-27 showed that the TD group had stronger connectivity compared to the LiD group and these connections show positive correlations with behavioral scores.</p></caption>
<graphic xlink:href="20218495v2_fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>DISCUSSION</title>
<p>Using an active sentence listening task we found that areas of cortical activation extended bilaterally across the superior temporal lobe and into memory and language areas (e.g., right parahippocampal gyrus, Broca&#x2019;s, left temporal fusiform cortex) as the stimuli presented increasingly complex speech elements from phonology to intelligibility to clear speech. These data shed new light on speech processing in the brain. Cortical areas of activation were unaffected by LiD. Functional connectivity between speech activation-defined sites (i) increased in spread with speech complexity, and (ii) differed between TD and children with LiD only during processing of more complex speech.</p>
<p>Behavioural measures of performance were consistent with these observations. No significant correlations were observed between temporal connectivity and sensory-dominant tasks (e.g, LiSN-S advantage scores and SCAN filtered words). However, significant correlations were observed between temporal connectivity and more cognitive-dominant tasks, especially speech-in-noise and task switching. Overall, the data are consistent with the hypothesis that children with LiD have primarily cognitive rather than sensory deficits.</p>
<sec id="s3a">
<title>Lateralization of speech in 6-12 year old children</title>
<p>Lateralization of speech processing in adults has been debated across and within imaging modalities (fMRI see <xref ref-type="bibr" rid="c27">Evans &#x0026; McGettigan, 2017</xref>; EEG e.g., <xref ref-type="bibr" rid="c3">Assaneo et al., 2019</xref>). For example, Rauschecker &#x0026; Scott&#x2019;s (2009) unilateral model suggested that the left anterior STG is the hub of successful speech perception, while Hickok and Poeppel&#x2019;s (2000) bilateral model proposed a perceptual pathway in each hemisphere processing speech sounds up to the level of semantics (<xref ref-type="bibr" rid="c40">Hickok &#x0026; Poeppel, 2007</xref>). A middle ground has been proposed by <xref ref-type="bibr" rid="c59">Peelle (2012)</xref>, with right hemisphere dominated activation for &#x201C;unconnected&#x201D; speech (i.e., phonemes, syllables, single words) and left hemisphere dominated activation for &#x201C;connected&#x201D; speech (i.e., phrases, sentences and narratives).</p>
<p>Using simple but complete sentence stimuli, we consistently found bilateral activation in children for all speech listening contrasts. This activation did not significantly differ between groups, after FDR correction, suggesting that children with LiD use the same cortical areas as TD children when listening to increasingly complex speech stimuli. However, it is possible that multi-voxel pattern analysis (MVPA) may find finer group differences in the hierarchy for speech processing (e.g., <xref ref-type="bibr" rid="c57">Okada et al., 2010</xref>). While the same cortical areas are used by the children with LiD, it is possible that they do so on a different time frame from the TD children. Unfortunately, fMRI does not provide sufficient time resolution to address this possibility.</p>
<p>Bilateral activation reported here differs from the report of <xref ref-type="bibr" rid="c70">Scott et al. (2000)</xref> who found a left lateralized pathway for speech comprehension using PET. Our results extend those of <xref ref-type="bibr" rid="c32">Halai et al. (2015)</xref> who found bilateral activation using continuous MRI. Both these studies tested young adults and used similar stimuli to those used here, but in continuous, passive speech presentation, where comprehension was assessed after the scanning session, outside the scanning room. In contrast, we required children to provide a behavioural response to each short sentence presentation as a check that they were paying attention. In addition, we used a sparse/HUSH protocol compared to the previous (<xref ref-type="bibr" rid="c32">Halai et al., 2015</xref>) continuous fMRI scanning protocols. Scanner noise superimposed on speech listening has been shown to increase listening effort (<xref ref-type="bibr" rid="c60">Peelle et al., 2010</xref>) and engage different cortical areas at varying intensities. For example, in a meta-analysis of 57 speech comprehension studies, <xref ref-type="bibr" rid="c2">Adank (2012)</xref> found that continuous scanning more strongly activated regions of the supplementary motor area and anterior cingulate gyrus, while sparse scanning showed more extensive activation in the STS.</p>
<p>Our results support the hypothesis of bilateral processing in speech listening in a 6-12 year old population. The study reported here focused on speech listening in children with and without LiD, rather than on differences across the tested age range. Bilateral activation produced by a speech comprehension task has previously been shown in preschoolers (Sroka et al., 2015; Holland et al., 2007), with an increase in lateralization for skills that develop after age 5 (e.g. word-picture matching and syntactic prosody) (Holland et al., 2007). A meta-analysis of 27 developmental (ages 4-13) fMRI language comprehension studies suggested that increasing lateralization in the developing brain is associated with increasing sensitivity to syntax (<xref ref-type="bibr" rid="c26">Enge et al., 2020</xref>). However, using an alternative approach (<xref ref-type="bibr" rid="c62">Plante et al., 2015</xref>), cortical activation in young adults became more left-lateralized as they learned an unfamiliar language. Plante&#x2019;s results thus suggest that increasing speech lateralization in older children may occur through language learning rather than through maturation. Recently, transcranial magnetic stimulation (TMS) has provided clear evidence that stimulation of either left or right STG decreases understanding of speech-in-noise in adults (<xref rid="c45" ref-type="bibr">Kennedy-Higgins et al., 2020</xref>). Together, these brain imaging and stimulation results suggest that bilateral cortical activation is a life-long, obligate aspect of speech perception. The results reported here are the first in an ongoing longitudinal study where the same cohort is being invited back for behavioural and neuroimaging assessment two and four years after this baseline.</p>
</sec>
<sec id="s3b">
<title>Temporal connectivity: Brain and behavior</title>
<p>Phonology is the system of processing the smallest units of speech sounds and their linguistically appropriate combinations. We found no group difference in the temporal connectivity of this network. As we progressed to the Intelligibility network, we found that the temporal connection between the left inferior frontal gyrus (Intelligibility ROI#14 in <xref rid="tbl2" ref-type="table">Table 2</xref>) and left posterior middle temporal gyrus (Intelligibility ROI#23 in <xref rid="tbl2" ref-type="table">Table 2</xref>) was stronger in the children with LiD. These areas are well known for language production (Broca&#x2019;s in the inferior frontal gyrus, <xref ref-type="bibr" rid="c31">Hagoort, 2014</xref>) and comprehension (left MTG, (<xref ref-type="bibr" rid="c1">Acheson &#x0026; Hagoort, 2013</xref>; <xref ref-type="bibr" rid="c22">Dronkers et al., 2004</xref>). Posterior MTG has been associated with lexical and semantic access in a sound-to-meaning network (<xref ref-type="bibr" rid="c38">Hickok &#x0026; Poeppel, 2000</xref>, <xref ref-type="bibr" rid="c39">2004</xref>). It is unknown whether stronger or weaker temporal connectivity is beneficial (<xref ref-type="bibr" rid="c58">Parente et al., 2018</xref>). However, the relationship found here between connectivity strength and an individual child&#x2019;s behavioural results provides evidence on this key issue. Children with stronger connectivity performed more poorly on cognitive tasks assessing vocabulary, switching attention and episodic memory. Reduced connectivity may thus be indicative of increased processing efficiency and/or suppression of a task-relevant network, in this case language. Note that the behavioural measures used here have been shown to be either little affected by age (audiogram: <xref ref-type="bibr" rid="c41">Hunter et al., 2021</xref>) or were standardized across age (ECLiPS, LiSN-S, SCAN and NIH toolbox; Petley et al., 2020).</p>
<p>Group comparisons in the Semantics network highlight two focal points. First, the right parahippocampal gyrus, associated with memory encoding and retrieval (<xref ref-type="bibr" rid="c48">Luck et al., 2010</xref>), had stronger temporal connections with auditory areas (left Heschl&#x2019;s, right planum temporale and bilateral STG) in children with LiD. Second, the left temporal fusiform cortex, associated with word recognition and the recovery of meaning from an impoverished acoustic signal (<xref ref-type="bibr" rid="c17">Davis &#x0026; Johnsrude, 2003</xref>), had stronger temporal connections with auditory areas (right STG), but in TD children. Again the relationship between individuals in connectivity strength and behavioural results shines light on the function of these connections. The first group of connections (right parahippocampal gyrus, Semantics ROI #16 in <xref rid="tbl2" ref-type="table">Table 2</xref>) show negative correlations between connectivity strength and dichotic listening, speech in noise and attention (selective and switching). The second group of connections (left temporal fusiform cortex, Semantics ROI #10 in <xref rid="tbl2" ref-type="table">Table 2</xref>) show positive correlations between connectivity strength, and dichotic listening and verbal vocabulary. This suggests a major difference between groups is connectivity associated with speech processing. Stronger connections from the right parahippocampal gyrus and weaker connections from the left temporal fusiform cortex were associated with impaired speech listening.</p>
<p>The forebrain focal points identified in this study highlight language production, memory encoding and retrieval, and word recognition as target areas of further research in children with LiD. Combined, these focal points suggest that children with LiD have difficulty matching meaning to word comprehension in an auditory environment. The connectivity analysis also highlights the left inferior frontal gyrus that, along with the superior temporal and premotor regions, has been shown to be recruited for statistical learning of language (<xref ref-type="bibr" rid="c16">Davis &#x0026; Gaskell, 2009</xref>; <xref ref-type="bibr" rid="c42">Karuza et al., 2013</xref>). Another avenue of research could therefore be how children with LiD learn language compared to their peers. Language development is supported by distributed neural networks connecting both cortical and subcortical regions throughout the brain (<xref ref-type="bibr" rid="c47">Lieberman, 2002</xref>; <xref ref-type="bibr" rid="c50">Mesulam, 1990</xref>). However, such networks have not been closely or systematically examined in children with LiD.</p>
<p>Assessing where the &#x2018;break&#x2019; in speech listening occurs would provide a clearer avenue for research into effective evidence-based treatments. Future studies could utilize paradigms with anomalous and mispronounced words (e.g. <xref ref-type="bibr" rid="c66">Roebuck et al., 2018</xref>). Imaging techniques could be used in parallel to assess whether the cortical networks used in such tasks are affected by LiD. Complementary time-sensitive techniques (EEG, MEG) could investigate whether this difficulty is due to a bottleneck in processing leading to increased listening effort, cognitive effort or fatigue. As LiD may build up over time (<xref ref-type="bibr" rid="c65">Roebuck &#x0026; Barry, 2018</xref>), it is important to assess the children&#x2019;s ability throughout the task rather than using summary values (<xref ref-type="bibr" rid="c49">McGarrigle et al., 2020</xref>).</p>
</sec>
<sec id="s3c">
<title>Relationship to neurodevelopmental disorders and cognitive function</title>
<p>Children identified with LiD have difficulty in speech listening compared to TD children (<xref ref-type="bibr" rid="c61">Petley et al., 2021</xref>). These groups were distinguished by connectivity differences between cortical areas associated with language production, memory and word recognition, rather than by the activity or connectivity of primary auditory cortical areas. However, at least 50% of children referred/diagnosed with auditory processing disorder (APD), an alternate clinical label that has been used for LiD, have also been diagnosed with developmental language disorder (DLD), dyslexia/reading disorders, attention deficit/hyperactivity disorder (ADHD) or more than one of these other neurodevelopmental disorders (<xref ref-type="bibr" rid="c18">Dawes &#x0026; Bishop, 2010</xref>; <xref ref-type="bibr" rid="c28">Ferguson et al., 2011</xref>; <xref ref-type="bibr" rid="c30">Gokula et al., 2019</xref>; <xref ref-type="bibr" rid="c53">Moore et al., 2018</xref>; <xref ref-type="bibr" rid="c72">Sharma et al., 2009</xref>). This high level of comorbidity was echoed in this study, where a background caregiver questionnaire showed that half of the children with LiD also reported a diagnosis of ADHD, 9% autism spectrum disorders and 26% had seen a speech language pathologist. This high comorbidity raises the question of whether the group effects reported here were a result of LiD or one of these other disorders. We do not yet know, but we are currently addressing that question using a web-based resource (Neurosynth; <xref ref-type="bibr" rid="c78">Yarkoni et al., 2011</xref>) that allows functional connectivity analysis of brain areas defined by a meta-analysis of published fMRI activation coordinates. This analysis is being applied to children with a primary diagnosis of attention or language disorders as well as to the two groups of children reported here. What we do know is that the children identified here as having LiD had specific, atypical neurological characteristics associated with language comprehension.</p>
<p>Further investigation into the neurodevelopmental basis of LiD may also aid in the investigation of language disorders. Our results show typical phonology but impaired non-phonological speech connectivity in children with LiD. This differs from DLD, which presents with both abilities impaired, and dyslexia, which presents with impaired phonological and typical non-phonological abilities in reading (<xref ref-type="bibr" rid="c7">Bishop &#x0026; Snowling, 2004</xref>; <xref ref-type="bibr" rid="c19">Delage &#x0026; Durrleman, 2018</xref>). It is possible that altered cortical language processing leads to LiD. However, it could be that altered cortical language processing may be a consequence of LiD.</p>
<p>The results presented here highlight the importance of non-auditory factors, specifically language, in audiometric testing. There is a growing recognition of the importance of speech perception and, specifically, speech-in-noise (SiN) intelligibility in everyday hearing (<xref ref-type="bibr" rid="c46">Killion et al., 2004</xref>; C. <xref ref-type="bibr" rid="c74">Smits et al., 2013</xref>). It has been proposed that such testing could supplement, or even replace pure tone detection as an audiometric gold standard (<xref ref-type="bibr" rid="c37">Hewitt, 2018</xref>). However, both SiN test instructions and test items pose a challenge to language and memory as well as auditory function. While those cognitive aspects of auditory testing and learning have been dismissed as procedural issues (<xref ref-type="bibr" rid="c36">Hawkey et al., 2004</xref>), they are an intimate component of a SiN test. These results provide insight into mechanisms of how speech perception may be disrupted in LiD, a common form of auditory impairment in both children (<xref ref-type="bibr" rid="c53">Moore et al., 2018</xref>) and adults (<xref rid="c24" ref-type="bibr">Edwards et al., 2020</xref>). They also add to a growing literature on the role of cognitive function in hearing (<xref ref-type="bibr" rid="c54">Moore et al., 2014</xref>; <xref ref-type="bibr" rid="c68">R&#x00F6;nnberg et al., 2013</xref>; <xref ref-type="bibr" rid="c71">Sharma et al., 2019</xref>).</p>
</sec>
</sec>
<sec id="s4">
<title>Conclusions</title>
<p>Our results provide the first multifaceted neurological profile for children classified with LiD, based on caregiver report and normal peripheral auditory function. Children with LiD recruited the same cortical areas as their peers when listening to increasing complexities of speech. The temporal connections between these areas revealed significant differences between the groups only at the semantic level of speech listening. These differences were related to dichotic listening, speech-in-noise, attention, memory and verbal vocabulary abilities. Overall, the data are consistent with the hypothesis that children with LID are primarily affected by cognitive and language deficits.</p>
</sec>
<sec id="s5">
<title>METHODS</title>
<sec id="s5a">
<title>Participants</title>
<p>Eighty-five participants aged 6-12 years completed the fMRI sentence task and 81 participants completed the rs-fMRI (see <xref rid="tbl1" ref-type="table">Table 1</xref>). 90% of participants who completed the fMRI sentence task also completed the rs-fMRI. All participants had normal audiometric hearing with thresholds &#x003C; 25 dB HL at all octave-interval frequencies from 0.25 - 8 kHz in both ears (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). In this paper we focus on the cortical results and their relationship with behavioural responses from the baseline of our longitudinal &#x2018;SICLID&#x2019; study examining correlates of LiD in children. Extensive analysis of the audiometric function and behavioural responses of these children are reported elsewhere (<xref ref-type="bibr" rid="c41">Hunter et al., 2021</xref>; <xref rid="c61" ref-type="bibr">Petley et al., 2020</xref>; Hunter et al., in prep.).</p>
<p>Caregivers of all participants completed a well-validated checklist of everyday listening and related skills (the ECLiPS; Barry &#x0026; <xref rid="c54" ref-type="bibr">Moore, 2014</xref>). Those scoring within the <italic>clinical interest</italic> range on the ECLiPS (&#x003C; 10th percentile of ECLiPS standardized scores), or had a diagnosis of auditory processing disorder (APD; n=14), were classified as children with LiD (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Participants scoring within the <italic>normal</italic> range on the ECLiPS and with no history of developmental disorders or delays were classified as TD children (<xref ref-type="bibr" rid="c61">Petley et al., 2021</xref>).</p>
<p>Eligibility for the study included English as the child&#x2019;s native language, an absence of any neurologic, psychiatric or intellectual (IQ &#x003C; 80) condition that would prevent or restrict their ability to complete testing procedures. In addition to this the TD participants were eligible if they had no developmental delay, attention or language disorder. Eligibility was determined based on caregiver responses on a medical and educational history &#x2018;Background&#x2019; questionnaire.</p>
<p>This study was approved by the Institutional Review Board of Cincinnati Children&#x2019;s Hospital (CCH) Research Foundation. Prior to completion of study-related imaging and behavioural testing, caregivers reviewed the informed consent form with a study staff member. Children aged 11 and above were also assented using a child-friendly version of the consent document, per institutional policy. All participants received financial compensation for their participation.</p>
</sec>
<sec id="s5b">
<title>MRI acquisition</title>
<p>MRI was performed via a 3T Philips Ingenia scanner with a 64-channel head coil and Avotec audiovisual system. All participants were awake and non-sedated throughout the scanning. The protocol included a high-resolution T1-weighted anatomical scan, fMRI sentence task (4.9 minutes) and rs-fMRI (5 minutes). The fMRI sentence task was acquired with a sparse scanning protocol (&#x2018;HUSH&#x2019;, details below); TR/TE = 2000/30 ms, voxel size = 2.5 &#x00D7; 2.5 &#x00D7; 3.5 mm, 39 axial slices. A total of 147 volumes was acquired by alternating scanning for 6 seconds (3 volumes) and not scanning for 6 seconds, 49 times. Cardiac and respiration signals were collected during the fMRI sentence task using the scanner&#x2019;s wireless respirator bellows and Peripheral Pulse Oximeter. The rs-fMRI acquisition was acquired with TR/TE = 2000/30 ms, voxel size = 2.5 &#x00D7; 2.5 &#x00D7; 3.5 mm, 39 axial slices in ascending slice order and 150 volumes. The high-resolution T1-weighted anatomical scan was acquired with TR/TE=8.1/3.7 ms, FOV 25.6 x 25.6 x 16.0 cm, matrix 256 x 256 and slice thickness = 1mm.</p>
</sec>
<sec id="s5c">
<title>fMRI task</title>
<p>With sound levels reaching 118.4 &#x00B1; 1.3 dB (A) in a 3T MRI system (<xref ref-type="bibr" rid="c63">Price et al., 2001</xref>) special considerations must be made when planning an auditory-based MRI study. In order to protect the participant from the loud environment, foam ear plugs and MRI safe circumaural headphones were worn. The scanner noise may also produce masking of the desired stimuli. Therefore, in the fMRI task we used a &#x2018;Hemodynamics Unrelated to Sounds of Hardware&#x2019; (HUSH) scanning protocol (<xref ref-type="bibr" rid="c20">Deshpande et al., 2016</xref>; <xref ref-type="bibr" rid="c23">Edmister et al., 1999</xref>; <xref ref-type="bibr" rid="c33">Hall et al., 1999</xref>; <xref ref-type="bibr" rid="c69">Schmithorst &#x0026; Holland, 2004</xref>) - a sparse temporal sampling protocol where there was no gradient coil noise during presentation of the auditory stimuli. We also used a talker identification task instead of a speech recognition task. Instead of asking the children <italic>what</italic> they heard, we asked them <italic>who</italic> had said it. The children responded with button presses throughout the task so we could ensure they maintained attention to the task.</p>
<p>Sixteen linguistically simple BKB sentences, designed to be familiar to young children, were recorded by a single male North American speaker, mirroring the paradigm used by <xref ref-type="bibr" rid="c70">Scott et al. (2000)</xref> and <xref ref-type="bibr" rid="c32">Halai et al. (2015)</xref>. These were intelligible, clear speech stimuli (e.g. <xref rid="fig1" ref-type="fig">Figure 1D</xref>). Rotated speech stimuli (<xref rid="fig1" ref-type="fig">Figure 1E</xref>) were created by rotating each sentence spectrally around 2 kHz using the <xref ref-type="bibr" rid="c9">Blesser (1972)</xref> technique. Rotated speech was not intelligible, though some phonetic features and some of the original intonation was preserved. Rotated&#x002B;vocoded speech stimuli (<xref rid="fig1" ref-type="fig">Figure 1F</xref>) were created by applying 6 band noise-vocoding to the rotated speech stimuli. While the rotated noise-vocoded speech was completely unintelligible, the character of the envelope and some spectral detail was preserved.</p>
<p>Participants were told that they would be completing a matching game where they would hear a sentence and then see a picture (of a man or an alien). If the picture matched who said the sentence (man - clear speech, alien - rotated or rotated&#x002B;vocoded speech), the participant pressed a button with their right thumb, if the picture did not match, they pressed a second button with their left thumb. The participants were asked to respond as fast and as accurately as possible. A sticker was placed on the participants&#x2019; right hand to provide a reminder as to which hand was correct for matching voice and picture.</p>
<p>Before scanning, each participant was familiarized with the sentence task and completed three practice trials with verbal feedback from the tester. If a trial was completed incorrectly, the stimuli and instructions were reintroduced until the participant showed understanding. During scanning, each participant completed 48 matching trials, 16 of each sentence type, with no feedback. To maintain scanner timings the behavioural task continued regardless of whether the participant responded. However, if the child did not press a response button on three trials in a row the tester provided reminders/encouragement over the scanner intercom between stimulus presentations.</p>
</sec>
<sec id="s5d">
<title>fMRI data analysis</title>
<p>First-level fMRI data were processed using FSL (FMRIB Software Library, <ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/">https://fsl.fmrib.ox.ac.uk/fsl/</ext-link>). The T1 brain data were extracted using BET and normalized and resampled to the 2 mm isotropic MNI ICBM 152 non-linear 6th generation template using FLIRT.</p>
<p>For the sparse HUSH acquisition, the volumes were separated and combined into three files according to the volume&#x2019;s order during the scanner-on period. Each of the three files was pre-processed separately and first-level statistics computed. The three statistical images were then averaged together using a one sample t-test. This was done to account for the difference in intensity among the volumes due to T2&#x002A; relaxation effects. The pre-processing steps included the following. FSL&#x2019;s BET was used for brain extraction of the functional data. Outlying functional volumes were detected with &#x2018;fsl_motion_outliers&#x2019; using the RMS intensity difference metric. AFNI&#x2019;s &#x2018;3dretroicor&#x2019; was used to regress out the cardiac and respiration signals using a RETROICOR approach (<xref ref-type="bibr" rid="c29">Glover et al., 2000</xref>). Motion correction was carried out by MCFLIRT. A GLM was used to regress motion-related artifacts from the data using 6 regressors for the motion parameters and an additional regressor for each outlying volume. The amount of motion during the scans (the number of outlying volumes for each participant) did not differ between groups, <italic>p</italic> = .62 (<xref rid="tbl1" ref-type="table">Table 1</xref>). The data were spatially smoothed using a Gaussian kernel with a sigma of 3 mm and temporally filtered with a high pass filter with a sigma of 30 seconds. The results were interpolated to a 2 mm isotropic voxel size and aligned to the Montreal Neurological Institute (MNI) template by first co-registering it with the participant&#x2019;s T1 using FSL&#x2019;s FLIRT.</p>
<p>Second-level analysis was also conducted using FSL. A GLM approach was used to create group activation maps based on contrasts between conditions for all participants (i.e. regardless of LiD/TD status) with age as a covariate. Group composite images were thresholded using a family-wise error correction (p &#x003C; 0.001) and clustering threshold of k = 4 voxels. Three BOLD activation contrasts were used to search for brain loci responding to different aspects of listening to language (<xref ref-type="bibr" rid="c32">Halai et al., 2015</xref>; modified from <xref rid="c70" ref-type="bibr">Scott, 2000</xref>). First, a &#x2018;Semantics&#x2019; activation map, whereby the signal with intelligibility, intonation, phonetics, prosody and sound was contrasted with one lacking all of these attributes except prosody and sound (clear &#x003E; rotated&#x002B;vocoded). Second, an &#x2018;Intelligibility&#x2019; activation map contrasted the signal with all speech attributes to one retaining intonation, phonetics, prosody and sound (clear &#x003E; rotated). Third, a &#x2018;Phonetics&#x2019; activation map contrasted a signal with intonation, phonetics, prosody and sound with one having only prosody and sound (rotated &#x003E; rotated&#x002B;vocoded).</p>
<p>Behavioural responses from the fMRI task were assessed using a 2 (group: TD, LiD) &#x00D7; 3 (sentence type: clear, rotated, rotated&#x002B;vocoded) repeated measures analysis of variance (ANOVA) for accuracy and again for RT (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). Where the assumption of sphericity was violated, degrees of freedom were corrected using Greenhouse-Geisser estimates of sphericity.</p>
</sec>
<sec id="s5e">
<title>Resting state fMRI</title>
<p>We maintained visual fixation during this period with a cross in the center of a black screen. During this second scan the participant was asked to lie still, keep eyes open and let their mind wander; they were not performing an exogenous task. Eyes were monitored by the tester through CCTV and no child fell asleep during the task.</p>
<p>For the rs-fMRI scan, pre-processing and analysis was performed in the CONN toolbox using standard spatial and temporal pipelines (<xref ref-type="bibr" rid="c77">Whitfield-Gabrieli &#x0026; Nieto-Castanon, 2012</xref>). For spatial smoothing a FWHM of 8mm was used. The Artifact Detection Tool (ART, <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/artifact_detect">https://www.nitrc.org/projects/artifact_detect</ext-link>) within CONN was used to regress out framewise motion. The number of frames regressed out was compared between groups with no significant group differences, <italic>p</italic> = .44 (<xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
</sec>
<sec id="s5f">
<title>Resting state ROI-to-ROI analysis</title>
<p>The areas of activation from the fMRI task&#x2019;s three contrasts were used to define the ROIs of advancing speech listening networks (Phonology, Intelligibility and Semantics). However, as these areas of activation were large, we applied the parcellation from the pediatric ADHD-200 sample (<xref ref-type="bibr" rid="c6">Bellec et al., 2017</xref>) to parcellate each network. This created smaller and more appropriate ROIs for connectivity analysis of each network (<xref rid="fig3" ref-type="fig">Figure 3D-F</xref>, Supplementary Material Table 1).</p>
<p>A ROI-to-ROI analysis was used in Conn to test the functional relationship between each pair of ROIs identified in the fMRI sentence listening task. The mean time course of all voxels within each ROI was used to calculate individual pairwise Pearson correlations. The <italic>r</italic> values were normalized to <italic>z</italic> values via Fisher&#x2019;s z-transformation. We then used these <italic>z</italic> values to explore the relationship between the three listening networks and behavioural measures. Statistical thresholds were set to p &#x003C; .05 (corrected) at the single voxel level, and the resulting connections were thresholded at seed-level by intensity FDR (p &#x003C; .05).</p>
</sec>
<sec id="s5g">
<title>Caregiver questionnaire</title>
<sec id="s5g1">
<title>Everyday listening skills - ECLiPS (<xref ref-type="bibr" rid="c4">Barry &#x0026; Moore, 2015</xref>)</title>
<p>The ECLiPS is a standardized parental-report measure of listening and communication difficulties. Caregivers rated 38 simple statements on a five-point scale, ranging from strongly disagree to strongly agree. Children who score below the 10th percentile of the standardized composite scores (M = 10; SD = 3) are identified as experiencing clinically significant difficulties in listening and communication.</p>
</sec>
</sec>
<sec id="s5h">
<title>Behavioral measures</title>
<p>Resting state temporal connections with significant group differences were correlated with behavioural measures (see <xref ref-type="bibr" rid="c61">Petley et al., 2021</xref> for further detail and data). Study data were collected and managed using REDCap electronic data capture tools hosted at Cincinnati Children&#x2019;s Hospital (<xref ref-type="bibr" rid="c35">Harris et al., 2009</xref>, 2019). REDCap (Research Electronic Data Capture) is a secure, web-based software platform designed to support data capture for research studies, providing 1) an intuitive interface for validated data capture; 2) audit trails for tracking data manipulation and export procedures; 3) automated export procedures for seamless data downloads to common statistical packages; and 4) procedures for data integration and interoperability with external sources.</p>
<sec id="s5h1">
<title>Listening in Spatialized Noise-Sentences (<xref ref-type="bibr" rid="c11">Cameron &#x0026; Dillon, 2007</xref>, <xref ref-type="bibr" rid="c12">2008</xref>, <xref ref-type="bibr" rid="c13">2009</xref>)</title>
<p>LiSN-S (US version) is a standardized test assessing speech in noise ability. Binaural target (T) sentences were presented through headphones along with two other distracting sentences (D1, D2). The children were asked to repeat the sentences of the target voice only. Distracting sentences remained constant at 55 db SPL. After each correct trial the target voice descended in level (4 dB), but if the child incorrectly repeated back over 50% of the sentence the level increased (by 2 dB).</p>
<p>Four listening conditions are made by manipulating D1 and D2 with respect to T (same voice, different voices; same direction, 0&#x00B0;, different direction, &#x00B1; 90&#x00B0; azimuth). Three difference scores are calculated in order to control for language and cognitive demands: Talker advantage (different voices - same voice); Spatial advantage (different directions &#x2013; same direction); and Total advantage (different voices and directions &#x2013; same voices and directions). The LISN-S software calculated the difference scores for each participant.</p>
</sec>
<sec id="s5h2">
<title>Auditory processing disorder tests - SCAN-3:C (<xref ref-type="bibr" rid="c44">Keith, 2009</xref>; 2000)</title>
<p>The SCAN-3 is a US-standardized test battery often used by audiologists to diagnose APD in children (<xref ref-type="bibr" rid="c25">Emanuel et al., 2011</xref>). Subtests used in our battery were Auditory Figure Ground - assessing the ability to repeat words presented against background multi-talker speech at signal/noise ratio of &#x002B;8 dB; Competing words - a dichotic listening task where the child repeats different words presented simultaneously to each ear, but repeating that from a designated ear first; Filtered words - assessing ability to identify words that are low pass filtered at 750 Hz; and Competing Sentences - a dichotic listening task where different sentences are presented simultaneously to each ear, and the child is asked to repeat the sentence from a designated ear. Both subtest and a standardized composite score are calculated.</p>
</sec>
<sec id="s5h3">
<title>Cognition - NIH toolbox (<xref ref-type="bibr" rid="c76">Weintraub et al., 2013</xref>)</title>
<p>The NIH toolbox - Cognition Battery is a collection of US-standardized tests from which we used measures of selective attention (Flanker Inhibitory Control and Attention Test), episodic memory (Picture Sequence Memory Test), executive functioning (Dimensional Change Card Sort Test) and picture vocabulary. Each visually administered test took 5 - 15 minutes to complete on an iPad. Age-corrected subtest and an overall &#x2018;early childhood composite&#x2019; scores were calculated for each participant.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>The dataset generated during and analysed during the current study are available from the corresponding author. ROIs used in the rs-fMRI analysis are available at GitHub.</p>
<p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/stewarthannahj/ROIs_SICLiD.git">https://github.com/stewarthannahj/ROIs_SICLiD.git</ext-link>
</p>
</sec>
<sec id="s6">
<title>DATA AVAILABILITY</title>
<p>Due to ethics requirements the dataset generated and analysed during the current study are available from the corresponding author.</p>
</sec>
<ack>
<title>ACKNOWLEDGEMENTS</title>
<p>Thanks to Dr Kim Leikin and Prof. Scott K Holland for aspects of study design and to Prof. Stuart Rosen and Dr Peter Chiu for the scripts to rotate and vocode the stimuli for the fMRI sentence task. Thanks also to Audrey Perdew, Nicholette Sloat and Dr Ronan McGarrigle for assisting in MRI acquisition. Many thanks to the MRI techs for many hours of scanning.</p>
</ack>
<sec id="s7">
<title>FUNDING</title>
<p>This research was supported by NIH R01DC014078, NIH 2UL1TR001425 and by Cincinnati Children&#x2019;s Research Foundation. DRM is supported in part by the NIHR Manchester Biomedical Research Centre.</p>
</sec>
<sec id="s8">
<title>COMPETING INTERESTS</title>
<p>The authors report no competing interests.</p>
</sec>
<sec id="s9">
<title>CONTRIBUTIONS</title>
<p>HJS - study design, MRI acquisition, MRI preprocessing and analysis, behavioural analysis, figures and tables, manuscript preparation and revisions</p>
<p>EC - behavioural and rs-fMRI analysis, figures, assisted with manuscript preparation and revisions</p>
<p>LLH - study design, manuscript preparation and revisions</p>
<p>TM - fMRI preprocessing, assisted with manuscript preparation</p>
<p>JV - study design, analysis and presentation, manuscript preparation and revisions</p>
<p>DRM - study design and presentation, funding, manuscript preparation and revisions</p>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item>
<term>CANS</term><def><p>central auditory nervous system</p></def></def-item>
<def-item>
<term>DLD</term><def><p>developmental language disorder</p></def></def-item>
<def-item>
<term>rs-fMRI</term><def><p>resting state-fMRI</p></def></def-item>
<def-item>
<term>FOV</term><def><p>field-of-view</p></def></def-item>
<def-item>
<term>GLM</term><def><p>general linear model</p></def></def-item>
<def-item>
<term>HUSH</term><def><p>Hemodynamics Unrelated to Sounds of Hardware LiD listening difficulties</p></def></def-item>
<def-item>
<term>MCC</term><def><p>multiple comparison correction</p></def></def-item>
<def-item>
<term>MEG</term><def><p>magnetoencephalography</p></def></def-item>
<def-item>
<term>MTG</term><def><p>middle temporal gyrus</p></def></def-item>
<def-item>
<term>MVPA</term><def><p>multi-voxel pattern analysis</p></def></def-item>
<def-item>
<term>p-FDR</term><def><p>false detection rate</p></def></def-item>
<def-item>
<term>ROI</term><def><p>region-of-interest</p></def></def-item>
<def-item>
<term>SLI</term><def><p>specific language impairment</p></def></def-item>
<def-item>
<term>STG</term><def><p>superior temporal gyrus</p></def></def-item>
<def-item>
<term>STS</term><def><p>superior temporal sulcus</p></def></def-item>
<def-item>
<term>TD</term><def><p>typical developing</p></def></def-item>
<def-item>
<term>TE</term><def><p>echo time</p></def></def-item>
<def-item>
<term>TMS</term><def><p>transcranial magnetic stimulation</p></def></def-item>
<def-item>
<term>TR</term><def><p>repetition time</p></def></def-item>
</def-list>
</glossary>
</sec>
<ref-list>
<title>REFERENCES</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Acheson</surname>, <given-names>D. J.</given-names></string-name>, &#x0026; <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name> (<year>2013</year>). <article-title>Stimulating the brain&#x2019;s language network: Syntactic ambiguity resolution after TMS to the inferior frontal gyrus and middle temporal gyrus</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>25</volume>(<issue>10</issue>), <fpage>1664</fpage>&#x2013;<lpage>1677</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Adank</surname>, <given-names>P.</given-names></string-name> (<year>2012</year>). <article-title>The neural bases of difficult speech comprehension and speech production: Two Activation Likelihood Estimation (ALE) meta-analyses</article-title>. <source>Brain and Language</source>, <volume>122</volume>(<issue>1</issue>), <fpage>42</fpage>&#x2013;<lpage>54</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Assaneo</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Orpella</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ripolles</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Diego-Balaguer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &#x0026; others. (<year>2019</year>). <article-title>The lateralization of speech-brain coupling is differentially modulated by intrinsic auditory and top-down mechanisms</article-title>. <source>Frontiers in Integrative Neuroscience</source>, <volume>13</volume>, <fpage>28</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><string-name><surname>Barry</surname>, <given-names>J. G.</given-names></string-name>, &#x0026; <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name> (<year>2015</year>). <source>Evaluation of children&#x2019;s listening and processing skills (ECLiPS)</source>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="other"><string-name><surname>Barry</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Tomlin</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name>, &#x0026; <string-name><surname>Dillon</surname>, <given-names>H.</given-names></string-name> (<year>2015</year>). <source>Use of Questionnaire-Based Measures in the Assessment of Listening Difficulties in School-Aged Children</source>. <fpage>1</fpage>&#x2013;<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bellec</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Chu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chouinard-Decorte</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Benhajali</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Margulies</surname>, <given-names>D. S.</given-names></string-name>, &#x0026; <string-name><surname>Craddock</surname>, <given-names>R. C.</given-names></string-name> (<year>2017</year>). <article-title>The neuro bureau adhd-200 preprocessed repository</article-title>. <source>Neuroimage</source>, <volume>144</volume>, <fpage>275</fpage>&#x2013;<lpage>286</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bishop</surname>, <given-names>D. V.</given-names></string-name>, &#x0026; <string-name><surname>Snowling</surname>, <given-names>M. J.</given-names></string-name> (<year>2004</year>). <article-title>Developmental dyslexia and specific language impairment: Same or different?</article-title> <source>Psychological Bulletin</source>, <volume>130</volume>(<issue>6</issue>), <fpage>858</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Biswal</surname>, <given-names>B. B.</given-names></string-name> (<year>2012</year>). <article-title>Resting state fMRI: a personal history</article-title>. <source>Neuroimage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>938</fpage>&#x2013;<lpage>944</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Blesser</surname>, <given-names>B.</given-names></string-name> (<year>1972</year>). <article-title>Speech perception under conditions of spectral transformation: I</article-title>. <source>Phonetic characteristics. Journal of Speech and Hearing Research</source>, <volume>15</volume>(<issue>1</issue>), <fpage>5</fpage>&#x2013;<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>L. E.</given-names></string-name>, &#x0026; <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> (<year>2018</year>). <article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title>. <source>Current Biology</source>, <volume>28</volume>(<issue>24</issue>), <fpage>3976</fpage>-<lpage>3983.e5</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2018.10.042">https://doi.org/10.1016/j.cub.2018.10.042</ext-link></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Cameron</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Dillon</surname>, <given-names>H.</given-names></string-name> (<year>2007</year>). <article-title>Development of the listening in spatialized noise-sentencestest (LISN-S)</article-title>. <source>Ear and Hearing</source>, <volume>28</volume>(<issue>2</issue>), <fpage>196</fpage>&#x2013;<lpage>211</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/AUD.0b013e318031267f">https://doi.org/10.1097/AUD.0b013e318031267f</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Cameron</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Dillon</surname>, <given-names>H.</given-names></string-name> (<year>2008</year>). <article-title>The listening in spatialized noise-sentences test (LISN-S): Comparison to the prototype LISN and results from children with either a suspected (central) auditory processing disorder or a confirmed language disorder</article-title>. <source>Journal of the American Academy of Audiology</source>, <volume>19</volume>(<issue>5</issue>), <fpage>377</fpage>&#x2013;<lpage>391</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3766/jaaa.19.5.2">https://doi.org/10.3766/jaaa.19.5.2</ext-link></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Cameron</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Dillon</surname>, <given-names>H.</given-names></string-name> (<year>2009</year>). <source>Listening in Spatialized Noise&#x2013;Sentences test (LiSN-S)</source>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Craddock</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>James</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Holtzheimer III P.</surname> <given-names>E.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>X. P.</given-names></string-name>, &#x0026; <string-name><surname>Mayberg</surname>, <given-names>H. S.</given-names></string-name> (<year>2012</year>). <article-title>A whole brain fMRI atlas generated via spatially constrained spectral clustering</article-title>. <source>Human Brain Mapping</source>, <volume>33</volume>(<issue>8</issue>), <fpage>1914</fpage>&#x2013;<lpage>1928</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Damoiseaux</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Rombouts</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Barkhof</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Scheltens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Stam</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &#x0026; <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name> (<year>2006</year>). <article-title>Consistent resting-state networks across healthy subjects</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>103</volume>(<issue>37</issue>), <fpage>13848</fpage>&#x2013;<lpage>13853</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name>, &#x0026; <string-name><surname>Gaskell</surname>, <given-names>M. G.</given-names></string-name> (<year>2009</year>). <article-title>A complementary systems account of word learning: Neural and behavioural evidence</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>364</volume>(<issue>1536</issue>), <fpage>3773</fpage>&#x2013;<lpage>3800</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name>, &#x0026; <string-name><surname>Johnsrude</surname>, <given-names>I. S.</given-names></string-name> (<year>2003</year>). <article-title>Hierarchical processing in spoken language comprehension</article-title>. <source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source>, <volume>23</volume>(<issue>8</issue>), <fpage>3423</fpage>&#x2013;<lpage>3431</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/23/8/3423">https://doi.org/23/8/3423</ext-link> [pii]</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Dawes</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>Bishop</surname>, <given-names>D. V. M.</given-names></string-name> (<year>2010</year>). <article-title>Psychometric profile of children with auditory processing disorder and children with dyslexia</article-title>. <source>Archives of Disease in Childhood</source>, <volume>95</volume>(<issue>6</issue>), <fpage>432</fpage>&#x2013;<lpage>436</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1136/adc.2009.170118">https://doi.org/10.1136/adc.2009.170118</ext-link></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Delage</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Durrleman</surname>, <given-names>S.</given-names></string-name> (<year>2018</year>). <article-title>Developmental dyslexia and specific language impairment: Distinct syntactic profiles?</article-title> <source>Clinical Linguistics &#x0026; Phonetics</source>, <volume>32</volume>(<issue>8</issue>), <fpage>758</fpage>&#x2013;<lpage>785</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Deshpande</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Tan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Altaye</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Holland</surname>, <given-names>S. K.</given-names></string-name> (<year>2016</year>). <article-title>FMRI as a preimplant objective tool to predict postimplant oral language outcomes in children with cochlearimplants</article-title>. <source>Ear and Hearing</source>, <volume>37</volume>(<issue>4</issue>), <fpage>e263</fpage>&#x2013;<lpage>e272</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="other"><string-name><surname>Dillon</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Cameron</surname>, <given-names>S.</given-names></string-name> (<year>2021</year>). <article-title>Separating the causes of listening difficulties in children</article-title>. <source>Ear and Hearing</source>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Dronkers</surname>, <given-names>N. F.</given-names></string-name>, <string-name><surname>Wilkins</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Van Valin</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Redfern</surname>, <given-names>B. B.</given-names></string-name>, &#x0026; <string-name><surname>Jaeger</surname>, <given-names>J. J.</given-names></string-name> (<year>2004</year>). <article-title>Lesion analysis of the brain areas involved in language comprehension</article-title>. <source>Cognition</source>, <volume>92</volume>(<issue>1&#x2013;2</issue>), <fpage>145</fpage>&#x2013;<lpage>177</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2003.11.002">https://doi.org/10.1016/j.cognition.2003.11.002</ext-link></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Edmister</surname>, <given-names>W. B.</given-names></string-name>, <string-name><surname>Talavage</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Ledden</surname>, <given-names>P. J.</given-names></string-name>, &#x0026; <string-name><surname>Weisskoff</surname>, <given-names>R. M.</given-names></string-name> (<year>1999</year>). <article-title>Improved auditory cortex imaging using clustered volume acquisitions</article-title>. <source>Human Brain Mapping</source>, <volume>7</volume>(<issue>2</issue>), <fpage>89</fpage>&#x2013;<lpage>97</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><surname>Edwards</surname>, <given-names>B.</given-names></string-name> (<year>2020</year>). <source>Emerging technologies, market segments, and MarkeTrak 10 insights in hearing health technology</source>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="other"><string-name><surname>Emanuel</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Ficca</surname>, <given-names>K. N.</given-names></string-name>, &#x0026; <string-name><surname>Korczak</surname>, <given-names>P.</given-names></string-name> (<year>2011</year>). <article-title>Survey of the diagnosis and management of auditory processing disorder</article-title>. <source>American Journal of Audiology</source>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><surname>Enge</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Friederici</surname>, <given-names>A. D.</given-names></string-name>, &#x0026; <string-name><surname>Skeide</surname>, <given-names>M. A.</given-names></string-name> (<year>2020</year>). <article-title>A meta-analysis of fMRI studies of language comprehension in children</article-title>. <source>NeuroImage</source>, <fpage>116858</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Evans</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>McGettigan</surname>, <given-names>C.</given-names></string-name> (<year>2017</year>). <article-title>Comprehending auditory speech: Previous and potential contributions of functional MRI</article-title>. <source>Language, Cognition and Neuroscience</source>, <volume>32</volume>(<issue>7</issue>), <fpage>829</fpage>&#x2013;<lpage>846</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Ferguson</surname>, <given-names>M. a</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Riley</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name> (<year>2011</year>). <article-title>Communication, listening, cognitive and speech perception skills in children with auditory processing disorder (APD) or Specific Language Impairment (SLI)</article-title>. <source>Journal of Speech, Language, and Hearing Research : JSLHR</source>, <volume>54</volume>(<issue>1</issue>), <fpage>211</fpage>&#x2013;<lpage>227</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/1092-4388(2010/09-0167)">https://doi.org/10.1044/1092-4388(2010/09-0167)</ext-link></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>T.-Q.</given-names></string-name>, &#x0026; <string-name><surname>Ress</surname>, <given-names>D.</given-names></string-name> (<year>2000</year>). <article-title>Image-based method for retrospective correction of physiological motion effects in fMRI: RETROICOR</article-title>. <source>Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</source>, <volume>44</volume>(<issue>1</issue>), <fpage>162</fpage>&#x2013;<lpage>167</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Gokula</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Cupples</surname>, <given-names>L.</given-names></string-name>, &#x0026; <string-name><surname>Valderrama Valenzuela J.</surname> <given-names>T</given-names></string-name>. (<year>2019</year>). <article-title>Comorbidity of auditory processing, attention, and memory in children with word reading difficulties</article-title>. <source>Frontiers in Psychology</source>, <volume>10</volume>, <fpage>2383</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name> (<year>2014</year>). <article-title>Nodes and networks in the neural architecture for language: Broca&#x2019;s region and beyond</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>28</volume>, <fpage>136</fpage>&#x2013;<lpage>141</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Halai</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Parkes</surname>, <given-names>L. M.</given-names></string-name>, &#x0026; <string-name><surname>Welbourne</surname>, <given-names>S. R.</given-names></string-name> (<year>2015</year>). <article-title>Dual-echo fMRI can detect activations in inferior temporal lobe during intelligible speech comprehension</article-title>. <source>NeuroImage</source>, <volume>122</volume>, <fpage>214</fpage>&#x2013;<lpage>221</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2015.05.067">https://doi.org/10.1016/j.neuroimage.2015.05.067</ext-link></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Hall</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Haggard</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Akeroyd</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Palmer</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Summerfield</surname>, <given-names>A. Q.</given-names></string-name>, <string-name><surname>Elliott</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Gurney</surname>, <given-names>E. M.</given-names></string-name>, &#x0026; <string-name><surname>Bowtell</surname>, <given-names>R. W.</given-names></string-name> (<year>1999</year>). <article-title>&#x201C;Sparse&#x201D; temporal sampling in auditory fMRI</article-title>. <source>Human Brain Mapping</source>, <volume>7</volume>(<issue>3</issue>), <fpage>213</fpage>&#x2013;<lpage>223</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Harris</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Minor</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Elliott</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Fernandez</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>O&#x2019;Neal</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>McLeod</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Delacqua</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Delacqua</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Kirby</surname>, <given-names>J.</given-names></string-name>, &#x0026; others. (<year>2019</year>). <article-title>The REDCap consortium: Building an international community of software platform partners</article-title>. <source>Journal of Biomedical Informatics</source>, <volume>95</volume>, <fpage>103208</fpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Harris</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Thielke</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Payne</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gonzalez</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Conde</surname>, <given-names>J. G.</given-names></string-name>, &#x0026; others. (<year>2009</year>). <article-title>A metadata-driven methodology and workflow process for providing translational research informatics support</article-title>. <source>J Biomed Inform</source>, <volume>42</volume>(<issue>2</issue>), <fpage>377</fpage>&#x2013;<lpage>381</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Hawkey</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Amitay</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name> (<year>2004</year>). <article-title>Early and rapid perceptual learning</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume>(<issue>10</issue>), <fpage>1055</fpage>&#x2013;<lpage>1056</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Hewitt</surname>, <given-names>D.</given-names></string-name> (<year>2018</year>). <article-title>Audiometry and Its Discontents</article-title>. <source>Hearing Review</source>, <volume>25</volume>(<issue>1</issue>), <fpage>20</fpage>&#x2013;<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, &#x0026; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> (<year>2000</year>). <article-title>Towards a functional neuroanatomy of speech perception</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>4</volume>(<issue>4</issue>), <fpage>131</fpage>&#x2013;<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, &#x0026; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> (<year>2004</year>). <article-title>Dorsal and ventral streams: A framework for understanding aspects of the functional anatomy of language</article-title>. <source>Cognition</source>, <volume>92</volume>(<issue>1&#x2013;2</issue>), <fpage>67</fpage>&#x2013;<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, &#x0026; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> (<year>2007</year>). <article-title>The cortical organization of speech processing</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>8</volume>(<issue>5</issue>), <fpage>393</fpage>&#x2013;<lpage>402</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Hunter</surname>, <given-names>L. L.</given-names></string-name>, <string-name><surname>Blankenship</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sloat</surname>, <given-names>N. T.</given-names></string-name>, <string-name><surname>Perdew</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stewart</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name> (<year>2021</year>). <article-title>Peripheral auditory involvement in childhood listening difficulty</article-title>. <source>Ear and Hearing</source>, <volume>42</volume>(<issue>1</issue>), <fpage>29</fpage>&#x2013;<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Karuza</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Starling</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Tivarus</surname>, <given-names>M. E.</given-names></string-name>, &#x0026; <string-name><surname>Bavelier</surname>, <given-names>D.</given-names></string-name> (<year>2013</year>). <article-title>The neural correlates of statistical learning in a word segmentation task: An fMRI study</article-title>. <source>Brain and Language</source>, <volume>127</volume>(<issue>1</issue>), <fpage>46</fpage>&#x2013;<lpage>54</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Keith</surname>, <given-names>R. W.</given-names></string-name> (<year>2000</year>). <article-title>Development and Standardization of SCAN-C Test for Auditory Processing Disorders in Children</article-title>. <source>J Am Acad Audiol</source>, <volume>11</volume>(<issue>8</issue>), <fpage>438</fpage>&#x2013;<lpage>445</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Keith</surname>, <given-names>R. W.</given-names></string-name> (<year>2009</year>). <article-title>SCAN-3 for Children</article-title>. <source>Tests of Auditory Processing Disorders. Pearson</source>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Kennedy-Higgins</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Devlin J.</surname> <given-names>T.</given-names></string-name>, <string-name><surname>Nuttall H.</surname> <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Adank</surname>, <given-names>P</given-names></string-name>. (<year>2020</year>). <article-title>The causal role of left and right superior temporal gyri in speech perception in noise: A transcranial magnetic stimulation study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>32</volume>(<issue>6</issue>), <fpage>1092</fpage>&#x2013;<lpage>1103</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Killion</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Niquette</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Gudmundsen</surname>, <given-names>G. I.</given-names></string-name>, <string-name><surname>Revit</surname>, <given-names>L. J.</given-names></string-name>, &#x0026; <string-name><surname>Banerjee</surname>, <given-names>S</given-names></string-name>. (<year>2004</year>).<article-title>Development of a quick speech-in-noise test for measuring signal-to-noise ratio loss in normal-hearing and hearing-impaired listeners</article-title>. <source>Journal of Acoustical Society of America</source>, <volume>116</volume>, <fpage>2395</fpage>&#x2013;<lpage>2405</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Lieberman</surname>, <given-names>P.</given-names></string-name> (<year>2002</year>). <article-title>On the nature and evolution of the neural bases of human language. American Journal of Physical Anthropology</article-title>: <source>The Official Publication of the American Association of Physical Anthropologists</source>, <volume>119</volume>(<issue>S35</issue>), <fpage>36</fpage>&#x2013;<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Luck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Danion</surname>, <given-names>J.-M.</given-names></string-name>, <string-name><surname>Marrer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pham</surname>, <given-names>B.-T.</given-names></string-name>, <string-name><surname>Gounot</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Foucher</surname>, <given-names>J.</given-names></string-name> (<year>2010</year>). <article-title>The right parahippocampal gyrus contributes to the formation and maintenance of bound information in working memory</article-title>. <source>Brain and Cognition</source>, <volume>72</volume>(<issue>2</issue>), <fpage>255</fpage>&#x2013;<lpage>263</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="other"><string-name><surname>McGarrigle</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rakusen</surname>, <given-names>L.</given-names></string-name>, &#x0026; <string-name><surname>Mattys</surname>, <given-names>S.</given-names></string-name> (<year>2020</year>). <source>Effortful listening under the microscope: Examining relations between pupillometric and subjective markers of effort and tiredness from listening</source>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Mesulam</surname>, <given-names>M.-M.</given-names></string-name> (<year>1990</year>). <article-title>Large-scale neurocognitive networks and distributed processing for attention, language, and memory</article-title>. <source>Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society</source>, <volume>28</volume>(<issue>5</issue>), <fpage>597</fpage>&#x2013;<lpage>613</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Moore</surname>, <given-names>D R.</given-names></string-name> (<year>2018</year>). <article-title>Auditory processing disorder (APD)</article-title>. <source>Ear and Hearing</source>, <volume>39</volume>(<issue>4</issue>), <fpage>617</fpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Ferguson</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Edmondson-Jones</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Ratib</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Riley</surname>, <given-names>A.</given-names></string-name> (<year>2010</year>). <article-title>Nature of Auditory Processing Disorder in Children</article-title>. <source>Pediatrics</source>, <volume>126</volume>(<issue>2</issue>), <fpage>E382</fpage>&#x2013;<lpage>E390</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1542/peds.2009-2826">https://doi.org/10.1542/peds.2009-2826</ext-link></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Sieswerda</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Grainger</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Bowling</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Perdew</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eichert</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Alston</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hilbert</surname>, <given-names>L. W.</given-names></string-name>, <string-name><surname>Summers</surname>, <given-names>L.</given-names></string-name>, &#x0026; others. (<year>2018</year>). <article-title>Referral and diagnosis of developmental auditory processing disorder in a large, United States hospital-based audiology service</article-title>. <source>Journal of the American Academy of Audiology</source>, <volume>29</volume>(<issue>5</issue>), <fpage>364</fpage>&#x2013;<lpage>377</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Edmondson-Jones</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dawes</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fortnum</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>McCormack</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pierzycki</surname>, <given-names>R. H.</given-names></string-name>, &#x0026; <string-name><surname>Munro</surname>, <given-names>K. J.</given-names></string-name> (<year>2014</year>). <article-title>Relation between speech-in-noise threshold, hearing loss and cognition from 40&#x2013;69 years of age</article-title>. <source>PloS One</source>, <volume>9</volume>(<issue>9</issue>), <fpage>e107720</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Hugdahl</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Stewart</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Vannest</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Perdew</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Sloat</surname>, <given-names>N. T.</given-names></string-name>, <string-name><surname>Cash</surname>, <given-names>E. K.</given-names></string-name>, &#x0026; <string-name><surname>Hunter</surname>, <given-names>L. L.</given-names></string-name> (<year>2020</year>). <article-title>Listening Difficulties in Children: Behavior and Brain Activation Produced by Dichotic Listening of CV Syllables</article-title>. <source>Frontiers in Psychology</source>, <volume>11</volume>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2020.00675">https://doi.org/10.3389/fpsyg.2020.00675</ext-link></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Neijenhuis</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Campbell</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Cromb</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Luinge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>de Wit</surname>, <given-names>E.</given-names></string-name> (<year>2019</year>). <article-title>An Evidence-based Perspective on &#x2018;Misconceptions&#x2019; Regarding Pediatric Auditory Processing Disorder</article-title>. <source>Frontiers in Neurology</source>, <volume>10</volume>, <fpage>287</fpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Okada</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Rong</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Venezia</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Matchin</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Hsieh</surname>, <given-names>I. H.</given-names></string-name>, <string-name><surname>Saberi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name>, &#x0026; <string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name> (<year>2010</year>). <article-title>Hierarchical organization of human auditory cortex: Evidence from acoustic invariance in the response to intelligible speech</article-title>. <source>Cerebral Cortex</source>, <volume>20</volume>(<issue>10</issue>), <fpage>2486</fpage>&#x2013;<lpage>2495</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhp318">https://doi.org/10.1093/cercor/bhp318</ext-link></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Parente</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Frascarelli</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mirigliani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Di Fabio</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Biondi</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Colosimo</surname>, <given-names>A.</given-names></string-name> (<year>2018</year>). <article-title>Negative functional brain networks</article-title>. <source>Brain Imaging and Behavior</source>, <volume>12</volume>(<issue>2</issue>), <fpage>467</fpage>&#x2013;<lpage>476</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Peelle</surname>, <given-names>J. E.</given-names></string-name> (<year>2012</year>). <article-title>The hemispheric lateralization of speech processing depends on what &#x201C;speech&#x201D; is: A hierarchical perspective</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>6</volume>, <fpage>309</fpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Peelle</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Eason</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Schmitter</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schwarzbauer</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name> (<year>2010</year>). <article-title>Evaluating an acoustically quiet EPI sequence for use in fMRI studies of speech and auditory processing</article-title>. <source>Neuroimage</source>, <volume>52</volume>(<issue>4</issue>), <fpage>1410</fpage>&#x2013;<lpage>1419</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="other"><string-name><surname>Petley</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hunter</surname>, <given-names>L. L.</given-names></string-name>, <string-name><surname>Motlagh Zadeh</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sloat</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Perdew</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name> (<year>2021</year>). <article-title>Listening Difficulties in Children with Normal Audiograms: Relation to Hearing and Cognition</article-title>. <source>MedRxiv</source>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2020.10.05.20205468">https://doi.org/10.1101/2020.10.05.20205468</ext-link></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Plante</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Almryde</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Vance</surname>, <given-names>C. J.</given-names></string-name>, &#x0026; <string-name><surname>Asbj&#x00F8;rnsen</surname>, <given-names>A. E.</given-names></string-name> (<year>2015</year>). <article-title>Language lateralization shifts with learning by adults</article-title>. <source>Laterality: Asymmetries of Body, Brain and Cognition</source>, <volume>20</volume>(<issue>3</issue>), <fpage>306</fpage>&#x2013;<lpage>325</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Price</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>De Wilde</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Papadaki</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Curran</surname>, <given-names>J. S.</given-names></string-name>, &#x0026; <string-name><surname>Kitney</surname>, <given-names>R. I.</given-names></string-name> (<year>2001</year>). <article-title>Investigation of acoustic noise on 15 MRI scanners from 0.2 T to 3 T</article-title>. <source>Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine</source>, <volume>13</volume>(<issue>2</issue>), <fpage>288</fpage>&#x2013;<lpage>293</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Rauschecker</surname>, <given-names>J. P.</given-names></string-name>, &#x0026; <string-name><surname>Scott</surname>, <given-names>S. K.</given-names></string-name> (<year>2009</year>). <article-title>Maps and streams in the auditory cortex: Nonhuman primates illuminate human speech processing</article-title>. <source>Nature Neuroscience</source>, <volume>12</volume>(<issue>6</issue>), <fpage>718</fpage>&#x2013;<lpage>724</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Roebuck</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Barry</surname>, <given-names>J. G.</given-names></string-name> (<year>2018</year>). <article-title>Parental perception of listening difficulties: An interaction between weaknesses in language processing and ability to sustain attention</article-title>. <source>Scientific Reports</source>, <volume>8</volume>(<issue>1</issue>), <fpage>1</fpage>&#x2013;<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Roebuck</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sindberg</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Weismer</surname>, <given-names>S. E.</given-names></string-name> (<year>2018</year>). <article-title>The role of language in nonlinguistic stimuli: Comparing inhibition in children with language impairment</article-title>. <source>Journal of Speech Language and Hearing Research</source>, <volume>61</volume>(<issue>5</issue>), <fpage>1216</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/2018_JSLHR-L-17-0294">https://doi.org/10.1044/2018_JSLHR-L-17-0294</ext-link></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>R&#x00F6;nnberg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Holmer</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Rudner</surname>, <given-names>M.</given-names></string-name> (<year>2019</year>). <article-title>Cognitive hearing science and ease of language understanding</article-title>. <source>International Journal of Audiology</source>, <volume>58</volume>(<issue>5</issue>), <fpage>247</fpage>&#x2013;<lpage>261</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>R&#x00F6;nnberg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lunner</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zekveld</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>S&#x00F6;rqvist</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Danielsson</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lyxell</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dahlstr&#x00F6;m</surname>, <given-names>&#x00D6;.</given-names></string-name>,<string-name><surname>Signoret</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Stenfelt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pichora-Fuller</surname>, <given-names>M. K.</given-names></string-name>, &#x0026; others. (<year>2013</year>). <article-title>The Ease of Language Understanding (ELU) model: Theoretical, empirical, and clinical advances</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>7</volume>, <fpage>31</fpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Schmithorst</surname>, <given-names>V. J.</given-names></string-name>, &#x0026; <string-name><surname>Holland</surname>, <given-names>S. K.</given-names></string-name> (<year>2004</year>). <article-title>Event-related fMRI technique for auditory processing with hemodynamics unrelated to acoustic gradient noise</article-title>. <source>Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</source>, <volume>51</volume>(<issue>2</issue>), <fpage>399</fpage>&#x2013;<lpage>402</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Scott</surname>, <given-names>S. K.</given-names></string-name>, <string-name><surname>Blank</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Wise</surname>, <given-names>R. J. S.</given-names></string-name> (<year>2000</year>). <article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title>. <source>Brain</source>, <volume>123</volume>(<issue>12</issue>), <fpage>2400</fpage>&#x2013;<lpage>2406</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/123.12.2400">https://doi.org/10.1093/brain/123.12.2400</ext-link></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Sharma</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Purdy</surname>, <given-names>S. C.</given-names></string-name>, &#x0026; <string-name><surname>Humburg</surname>, <given-names>P.</given-names></string-name> (<year>2019</year>). <article-title>Cluster analyses reveals subgroups of children with suspected auditory processing disorders</article-title>. <source>Frontiers in Psychology</source>, <volume>10</volume>, <fpage>2481</fpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><string-name><surname>Sharma</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Purdy</surname>, <given-names>S. C.</given-names></string-name>, &#x0026; <string-name><surname>Kelly</surname>, <given-names>A. S.</given-names></string-name> (<year>2009</year>). <article-title>Comorbidity of Auditory Processing, Language, and Reading Disorders</article-title>. <source>Journal of Speech Language and Hearing Research</source>, <volume>52</volume>(<issue>3</issue>), <fpage>706</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/1092-4388(2008/07-0226)">https://doi.org/10.1044/1092-4388(2008/07-0226)</ext-link></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G.</given-names></string-name> (<year>2008</year>). <article-title>Object-based auditory and visual attention</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>12</volume>(<issue>5</issue>), <fpage>182</fpage>&#x2013;<lpage>186</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><string-name><surname>Smits</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goverts</surname>, <given-names>S. T.</given-names></string-name>, &#x0026; <string-name><surname>Festen</surname>, <given-names>J. M.</given-names></string-name> (<year>2013</year>). <article-title>The digits-in-noise test: Assessing auditory speech recognition abilities in noise</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>133</volume>(<issue>3</issue>), <fpage>1693</fpage>&#x2013;<lpage>1706</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><string-name><surname>Vos</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Arora</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Barber</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Bhutta</surname>, <given-names>Z. A.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Casey</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Charlson</surname>, <given-names>F. J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>A. Z.</given-names></string-name>, &#x0026; others. (<year>2016</year>). <article-title>Global, regional, and national incidence, prevalence, and years lived with disability for 310 diseases and injuries, 1990&#x2013;2015: A systematic analysis for the Global Burden of Disease Study 2015</article-title>. <source>The Lancet</source>, <volume>388</volume>(<issue>10053</issue>), <fpage>1545</fpage>&#x2013;<lpage>1602</lpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><string-name><surname>Weintraub</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Dikmen</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Heaton</surname>, <given-names>R. K.</given-names></string-name>, <string-name><surname>Tulsky</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Zelazo</surname>, <given-names>P. D.</given-names></string-name>, <string-name><surname>Bauer</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Carlozzi</surname>, <given-names>N. E.</given-names></string-name>, <string-name><surname>Slotkin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Blitz</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wallner-Allen</surname>, <given-names>K.</given-names></string-name>, &#x0026; others. (<year>2013</year>). <article-title>Cognition assessment using the NIH Toolbox</article-title>. <source>Neurology</source>, <volume>80</volume>(<issue>11 Supplement 3</issue>), <fpage>S54</fpage>&#x2013;<lpage>S64</lpage>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><string-name><surname>Whitfield-Gabrieli</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Nieto-Castanon</surname>, <given-names>A.</given-names></string-name> (<year>2012</year>). <article-title>Conn: A Functional Connectivity Toolbox for Correlated and Anticorrelated Brain Networks</article-title>. <source>Brain Connectivity</source>, <volume>2</volume>(<issue>3</issue>), <fpage>125</fpage>&#x2013;<lpage>141</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1089/brain.2012.0073">https://doi.org/10.1089/brain.2012.0073</ext-link></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, &#x0026; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name> (<year>2011</year>). <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nature Methods</source>, <volume>8</volume>(<issue>8</issue>), <fpage>665</fpage>&#x2013;<lpage>670</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>