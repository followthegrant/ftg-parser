<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2020.10.23.20218461</article-id>
<article-version>1.2</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Health Informatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Detection and Segmentation of Lesion Areas in Chest CT Scans For The Prediction of COVID-19</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ter-Sarkisov</surname><given-names>Aram</given-names></name>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff><institution>CitAI, Artificial Intelligence Research Centre, Department of Computer Science, City, University of London</institution></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author; email: <email>alex.ter-sarkisov@city.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2020</year>
</pub-date>
<elocation-id>2020.10.23.20218461</elocation-id>
<history>
<date date-type="received">
<day>23</day>
<month>10</month>
<year>2020</year>
</date>
<date date-type="rev-recd">
<day>03</day>
<month>11</month>
<year>2020</year>
</date>
<date date-type="accepted">
<day>03</day>
<month>11</month>
<year>2020</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2020, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2020</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/">http://creativecommons.org/licenses/by-nd/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="20218461.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>In this paper we compare the models for the detection and segmentation of Ground Glass Opacity and Consolidation in chest CT scans. These lesion areas are often associated both with common pneumonia and COVID-19. We train a Mask R-CNN model to segment these areas with high accuracy using three approaches: merging masks for these lesions into one, deleting the mask for Consolidation, and using both masks separately. The best model achieves the mean average precision of <bold>44</bold>.<bold>68</bold>% using MS COCO criterion for instance segmentation across all accuracy thresholds. The classification model, COVID-CT-Mask-Net, which learns to predict the presence of COVID-19 vs common pneumonia vs control, achieves the <bold>93</bold>.<bold>88</bold>% COVID-19 sensitivity, <bold>95</bold>.<bold>64</bold>% overall accuracy, <bold>95</bold>.<bold>06</bold>% common pneumonia sensitivity and <bold>96</bold>.<bold>91</bold>% true negative rate on the COVIDx-CT test split (21192 CT scans) using a small fraction of the training data. We also analyze the effect of Non-Maximum Suppression of overlapping object predictions, both on the segmentation and classification accuracy. The full source code, models and pretrained weights are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexTS1980/COVID-CT-Mask-Net">https://github.com/AlexTS1980/COVID-CT-Mask-Net</ext-link>.</p>
</abstract>
<counts>
<page-count count="8"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>No funding involved</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>Not applicable</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Fixed the results of segmentation evaluation (Table 4). Now it is obvious that models with NMS=0.75 outperform models with NMS=0.25 across all mask types.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>A number of publications showed both commonalities and differences in the manifestation of COVID-19 and common pneumonia (CP) in chest CT scans. Both conditions give rise to lesions like Ground Glass Opacity (GGO) and Consolidation (C), but they manifest in different ways. In COVID-19 patients GGO is present more often (number of lesions/scan slice), and tends to be bilateral, subsegmental C areas are also present more often compared to the patients with CP [<xref ref-type="bibr" rid="c13">ZYW<sup>&#x002B;</sup>20</xref>, <xref ref-type="bibr" rid="c5">LFBL20</xref>]. The absolute majority of patients with COVID-19 display either GGO, or Consolidation, or a mix of both [<xref ref-type="bibr" rid="c15">ZZX<sup>&#x002B;</sup>20</xref>], and GGO lesions are more diffused, larger in size, and spread over larger areas [<xref ref-type="bibr" rid="c5">LFBL20</xref>]. The problem with these findings is that many of them are not statistically significant, e.g. the difference in the location of lesions in [<xref ref-type="bibr" rid="c5">LFBL20</xref>], and sample sizes are quite small (e.g. <italic>n</italic> = 34 in [<xref ref-type="bibr" rid="c13">ZYW<sup>&#x002B;</sup>20</xref>]). As a result, a number of machine learning methods were recently developed to assist experts in determining the diagnosis using chest CT scans.</p>
<p>The two-class problems (COVID-19 vs CP, COVID-19 vs Control) is inherently easier to solve due to fewer false positives than the three-class problem (COVID-19 vs CP vs Control). Some of the best solutions for the two-class problems presented in [<xref ref-type="bibr" rid="c14">ZZHX20</xref>, <xref ref-type="bibr" rid="c12">ZLS<sup>&#x002B;</sup>20</xref>] include DenseNet169, ResNet50 and DRE-Net [<xref ref-type="bibr" rid="c8">SZL<sup>&#x002B;</sup></xref>]. Solutions for the three-class problem using chest CT scans include ResNet18 [<xref ref-type="bibr" rid="c1">BGCB20</xref>], ResNet50 [<xref ref-type="bibr" rid="c7">LQX<sup>&#x002B;</sup>20</xref>], COVIDNet-CT [<xref ref-type="bibr" rid="c3">GWW20</xref>] and multiscale spatial pyramid [<xref ref-type="bibr" rid="c11">YWR<sup>&#x002B;</sup>20</xref>] as feature extractors. The disadvantage of most COVID-19 detectors is either evaluating the model on a small amount of data [<xref ref-type="bibr" rid="c1">BGCB20</xref>, <xref ref-type="bibr" rid="c7">LQX<sup>&#x002B;</sup>20</xref>], implying weak capacity for generalization, or dependence on a large dataset and data balancing tricks [<xref ref-type="bibr" rid="c3">GWW20</xref>, <xref ref-type="bibr" rid="c11">YWR<sup>&#x002B;</sup>20</xref>] for training models.</p>
<p>Semantic segmentation is the prediction of object&#x2019;s masks from images by predicting the class at a pixel level. Semantic segmentation models like FCN and U-Net are widely used to segment GGO, C and other lesions. These predicted masks are often used in combination with the extracted features to predict the class of the image, [<xref ref-type="bibr" rid="c12">ZLS<sup>&#x002B;</sup>20</xref>, <xref ref-type="bibr" rid="c10">WGM<sup>&#x002B;</sup>20</xref>], improving the final prediction over the baseline feature extractor. Models like Mask R-CNN [<xref ref-type="bibr" rid="c4">HGDG17</xref>] solve a the combined problem of object detection (localization) using bounding boxes and prediction of the object&#x2019;s mask, known as instance segmentation. In this paper we compare three ways to predict instances of lesions&#x2019; masks. First, we use only masks for GGO areas, merging C with the background. Secondly, we merge GGO and C masks in a single &#x2018;lesion&#x2019; mask. Finally, we keep separate masks for GGO and C instances (this approach was first presented in [<xref ref-type="bibr" rid="c9">TS20</xref>]). The first two are 1&#x002B;1 class problem (1 object class &#x002B; background, the latter is a 2&#x002B;1 problem (2 object classes &#x002B; background). Our choices are explained by the observations that areas with GGO have larger sizes and are observed more frequently than areas with C in COVID-19 patients, hence GGO class alone may be sufficient for COVID-19 prediction.</p>
<p>We show that merging GGO and C masks into one class (&#x2018;lesion&#x2019;) both improves the segmentation precision and the accuracy of the classification model built on top of the segmentation model compared to using only GGO mask. We measure the model&#x2019;s accuracy using MS COCO convention of Intersect over Union (IoU) thresholds [<xref ref-type="bibr" rid="c6">LMB<sup>&#x002B;</sup>14</xref>]. Mask R-CNN segmentation achieves a precision of <bold>61</bold>.<bold>92</bold>%@<bold>0</bold>.<bold>5</bold> IoU, <bold>45</bold>.<bold>22</bold>%@<bold>0</bold>.<bold>75</bold> IoU and mean average precision of <bold>44</bold>.<bold>68</bold>% (across all IoU thresholds). The classifier, COVID-CT-Mask-Net [<xref ref-type="bibr" rid="c9">TS20</xref>] built on top of the merged masks model, achieves a COVID sensitivity of <bold>93</bold>.<bold>55%</bold> and an overall accuracy of <bold>96</bold>.<bold>33%</bold>. The classifier built on top the model with separate masks, achieves a COVID sensitivity of <bold>93</bold>.<bold>88</bold>% and an overall accuracy of <bold>95</bold>.<bold>64</bold>% on COVIDx-CT test split of the CNCB CT scans dataset. Compared to other solutions for a 3-class problem, we use only a small fraction of the dataset to get these results: 5% of the COVIDx-CT training split and 3% of the total data.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Data</title>
<p>The segmentation problems solved in the paper are shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>. The 2-class problem, <xref rid="fig1" ref-type="fig">Figure 1b</xref> was first solved in [<xref ref-type="bibr" rid="c9">TS20</xref>]. We compare this problem to two 1-class problems: For the first one, <xref rid="fig1" ref-type="fig">Figure 1c</xref>, we only consider GGO as the positive class and train the model to detect its instances (predict the bounding box coordinates and segment the positive area within it). Consolidation (C) masks are discarded (merged with the background). For the second problem, <xref rid="fig1" ref-type="fig">Figure 1d</xref>, we merge the masks for GGO and C into one class (&#x2018;lesion&#x2019;), thus increasing the prevalence of the positive class in the error space, compared to only GGO.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Segmentation masks for the same CT scan slice. <xref ref-type="fig" rid="fig1">Figure 1a</xref>: input raw image. 1b: 2-class problem, red: GGO masks, blue: C masks. <xref ref-type="fig" rid="fig1">figure 1c</xref>: 1-class problem (only GGO). <xref ref-type="fig" rid="fig1">Figure 1d</xref>: 1-class problem (merged masks for GGO and C). White masks are the lungs areas. Best viewed in color.</p></caption>
<graphic xlink:href="20218461v2_fig1.tif"/>
</fig>
<p>We use the same dataset split of 500 training &#x002B; 150 validation images with varying representation of either class in each image as in [<xref ref-type="bibr" rid="c9">TS20</xref>]. Many images are purely negative (only background mask). To train Mask R-CNN model to solve these problems, we extract bounding box coordinates of each lesion object from the masks, and either use 3 (2 positive &#x002B; 1 background label) or 2 (1 positive&#x002B;1 background) labels for objects. We define each object as the area isolated from other areas of the same class either by the background or by the area of a different class. Lung mask is merged with the background for all problems. Except the usual normalization using global mean and standard deviation, no other data augmentations or balancing (resampling, class balancing, image rotations, etc) were applied to the data at any stage, unlike in many other solutions, e.g. [<xref ref-type="bibr" rid="c3">GWW20</xref>].</p>
<p>For the classification problem us re-use the train/validation/test splits in [<xref ref-type="bibr" rid="c9">TS20</xref>, <xref ref-type="bibr" rid="c3">GWW20</xref>]. We sample 3000 images from the COVIDx-CT [<xref ref-type="bibr" rid="c3">GWW20</xref>] train split (1000 images/class), and use their full validation (21036 CT scans) and test (21191 CT scans) splits. As a result of our approach, we use less that 5% of the COVIDx-CT training data split, and 3% of the total CNCB CT scans data [<xref ref-type="bibr" rid="c12">ZLS<sup>&#x002B;</sup>20</xref>]. Each image is the same size as in the segmentation data, 512<italic>&#x00D7;</italic> 512 <italic>&#x00D7;</italic> 3 pixels, all alpha-channels removed. The training split used in this paper is the same as in [<xref ref-type="bibr" rid="c9">TS20</xref>], to have a fair comparison. As with the segmentation problem, no other data normalization tenchinques were used apart from the image global normalization.</p>
</sec>
<sec id="s3">
<label>3</label>
<title>Models and Experiments</title>
<p>We study in-depth the effect of non-maximum suppression (NMS) threshold, a criterion for discarding overlapping bounding box predictions in the Region Proposal Network (RPN) at train and test stages and Region of Interest (RoI) at test stage. High threshold values mean that a larger number of overlapping predictions is kept in the model. At the training stage of the segmentation model, low NMS in the RPN implies that a lower number of high-scoring predictions will be passed to RoI, and, a lower number of high-scoring predictions will be processed by RoI, both at train and test stages. This is because RoI, after passing the region of interest through the classification &#x2018;head&#x2019; (two fully connected layers and a class&#x002B;bounding box layer), can still classify this region as background, even if at the RPN stage the prediction was derived from the &#x2018;positive&#x2019; anchor [<xref ref-type="bibr" rid="c4">HGDG17</xref>]. The hyperparameters of the segmentation model are set in <xref rid="tbl1" ref-type="table">Table 1</xref>. The model computes 4 loss functions: two by RPN (objectness and bounding box coordinates) and two by RoI (class and bounding box coordinates). For our training and evaluation we use the torchvision v0.3.0.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Key hyperparameters of the segmentation models. RPN output is the number of predictions after the NMS step, RoI output is the maximum number of predictions at test stage after the NMS stage, RPN score<sub><italic>&#x03B8;</italic></sub> is the threshold for positive predictions at train time, RoI score<sub><italic>&#x03B8;</italic></sub> is the threshold for object confidence at test time. In RoI, NMS threshold is used only in testing.</p></caption>
<graphic xlink:href="20218461v2_tbl1.tif"/>
</table-wrap>
<p>In COVID-CT-Mask-Net, see <xref rid="fig2" ref-type="fig">Figure 2</xref>, Mask R-CNN layers, including RPN and RoI are set to test mode: they don&#x2019;t compute any losses. Therefore, RoI uses NMS threshold to filter predictions. A larger number of overlapping positive prediction can prompt the model to learn to associate them with a particular class, e.g. they are more prevalent in COVID-19 rather than common pneumonia. If the NMS threshold is low, the model will have to learn to associate a small number of distant predictions with the particular condition, which is likely to be a harder problem, because of the similarities between COVID-19 and common pneumonia. RoI score<sub><italic>&#x03B8;</italic></sub> is set to<italic>&#x2212;</italic>0.01 to accept all predictions regardless of confidence score, to keep the input size in the classification module <bold>S</bold> of fixed size. The details of the architecture of the classification model (including the <italic>de-batchification</italic> of the RoI predictions) are presented in <xref rid="fig2" ref-type="fig">Figure 2</xref> and [<xref ref-type="bibr" rid="c9">TS20</xref>], and its hyperparameters in <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Key hyperparameters of COVID-CT-Mask-Net. Backbone network, anchor scales and sizes are the same as in <xref ref-type="table" rid="tbl1">Table 1</xref>. Both RPN and RoI modules are set to the test mode. RoI score<sub><italic>&#x03B8;</italic></sub> is set to&#x2212; 0.01 to accept all predictions, even with low scores, to maintain the fixed batch size that is passed to the classification module <bold>S</bold>. The value of <bold>S</bold> is the total number of trainable parameters in it.</p></caption>
<graphic xlink:href="20218461v2_tbl2.tif"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Architecture of the COVID-CT-Mask-Net classification model.</p></caption>
<graphic xlink:href="20218461v2_fig2.tif"/>
</fig>
<sec id="s3a">
<label>3.1</label>
<title>Experimental results</title>
<p>Each segmentation model was trained using Adam optimzier with the same learning rate of 1<italic>e</italic> &#x2212; 5 and weight regularization coefficient 1<italic>e &#x2212;</italic>3 for 100 epochs, the best models for each configuration are reported in <xref rid="tbl4" ref-type="table">Table 4</xref>. Training of each model took about 3h on a GPU with 8Gb VRAM. All classifiers were trained with the same configuration for 50 epochs, which took about 8 hours on the same GPU. The sizes of the models are presented in <xref rid="tbl3" ref-type="table">Table 3</xref>, the difference in size between all segmentation models presented here is minuscule (<italic>&#x003C;</italic> 1000 parameters). The architecture and the training of models with separate masks is exactly the same as in [<xref ref-type="bibr" rid="c9">TS20</xref>], the only difference, that explains better results in <xref rid="tbl4" ref-type="table">Tables 4</xref>-<xref ref-type="table" rid="tbl6">6</xref> is due to the removal of very small objects (less than 10 <italic>&#x00D7;</italic> 10 pixels) and reduction of unnecessary large sample sizes during the training of RPN and RoI, from 1024/1024 in [<xref ref-type="bibr" rid="c9">TS20</xref>] to 256/256 in this paper.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Comparison of the models&#x2019; sizes and data splits used to training, validation and testing. The number of the trainable parameters in COVID-CT-Mask-Net is due to the fact that we only train the module <bold>S</bold> and batch normalization layers in the backbone.</p></caption>
<graphic xlink:href="20218461v2_tbl3.tif"/>
</table-wrap>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><p>Average precision of segmentation models.</p></caption>
<graphic xlink:href="20218461v2_tbl4.tif"/>
</table-wrap>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5:</label>
<caption><p>Sensitivity (precision) and overall accuracy results on COVIDx-CT test data (21192 images). Best results in bold.</p></caption>
<graphic xlink:href="20218461v2_tbl5.tif"/>
</table-wrap>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>Table 6:</label>
<caption><p>Comparison to other models. The results for COVIDNet-CT were obtained by running the publicly available model (<ext-link ext-link-type="uri" xlink:href="https://github.com/haydengunraj/COVIDNet-CT">https://github.com/haydengunraj/COVIDNet-CT</ext-link>) on the same test split using inference method, results for the other two models are taken from the publication. Last column is the share of COVID observations in the test split. Test split for COVNet has 438 images, ResNet18 90 images.</p></caption>
<graphic xlink:href="20218461v2_tbl6.tif"/>
</table-wrap>
<p>To measure the accuracy of the segmentation models, we use the average precision (AP), a benchmark tool for datasets labelled at an instance level like MS COCO[<xref ref-type="bibr" rid="c6">LMB<sup>&#x002B;</sup>14</xref>] and Pascal VOC [<xref ref-type="bibr" rid="c2">EVGW<sup>&#x002B;</sup>10</xref>]. We adapt the MS COCO convention and report values for three thresholds: AP@0.5, AP@0.75 and AP (primary challenge metric). The first two use Intersect over Union (IoU) between predicted and ground truth bounding boxes with thresholds equal to 0.5 and 0.75. The latter averages over thresholds between 0.5 and 0.95 with a 0.05 step (a total of 10 thresholds). For details see [<xref ref-type="bibr" rid="c6">LMB<sup>&#x002B;</sup>14</xref>]. We adapt the implementation of average precision computation from <ext-link ext-link-type="uri" xlink:href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</ext-link>. Confidence threshold for considering the object (RoI<sub><italic>&#x03B8;</italic></sub> hyperparameter) is 0.75 across all models. Only predictions with confidence scores <italic>&#x003E;</italic>RoI<sub><italic>&#x03B8;</italic></sub> are considered for computing (m)AP, the rest are discarded. RoI NMS<sub><italic>&#x03B8;</italic></sub> is always the same as RPN.</p>
<p>Images in <xref rid="fig3" ref-type="fig">Figure 3</xref> illustrate the difference between the two NMS thresholds across each all mask types. Each column corresponds to a particular CT scan slice. The bottom row is the ground truth masks with both segmented lesion regions. Rows 1,3,5 are models that use NMS threshold of 0.25, rows 2,4,6 use NMS threshold of 0.75. Rows 1,2 are models that were trained only with the GGO mask. Models in rows 3,4 were trained with merged masks. Models in rows 5,6 were trained using both masks. Models with a higher NMS threshold output a larger number of predictions overall (except, for example, in <xref rid="fig3" ref-type="fig">Figure 3e</xref>, the models with the merged GGO and C masks, row 3 with low NMS and row 4 with high NMS), many of them overlapping. This is a consequence of the fact that a particular predicted region can have a high enough confidence score in RPN to be passed on to RoI, but then RoI classification &#x2018;head&#x2019; outputs a confidence score lower than RoI score<sub><italic>&#x03B8;</italic></sub>, hence that region will be classified as background. In case of a low NMS, an overlapping prediction with a slightly lower score would be discarded at RPN stage. In case of the high NMS, it would be added to the pool of predictions, and RoI could extract a confidence score exceeding RoI score<sub><italic>&#x03B8;</italic></sub> from this second prediction, therefore, models with high NMS produce more predictions overall, both true and false positives.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Predicted masks for a number of CT scans. Row 7: ground truth masks, red: GGO, blue: C. Rows 1,3,5: models with NMS=0.25. Rows 2,4,6: models with NMS=0.75. Rows 1,2: models trained only with the GGO mask, Rows 3,4: models trained with the merged GGO and C masks. Rows 5,6: models trained with separate masks for both classes. All mask predictions are overlaid with bounding boxes and RoI confidence scores. Best viewed in color.</p></caption>
<graphic xlink:href="20218461v2_fig3.tif"/>
</fig>
<p>Evaluation results of the segmentation model are summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>. Models using high NMS threshold of 0.75 outperform the ones with low NMS threshold of 0.25 across all mask types. The model that learns from merged GGO and C masks with high NMS confidently outperforms GGO-only at every level of the IoU threshold. Apart from the NMS effect described above, GGO and C areas in CT scans have many commonalities, so if the model learns to segment GGO only, then Consolidation and background have the same label. As a result, the model associates some important patterns with the background rather than the object class. Results for separate GGO and C masks are mostly better than for only GGO, but worse than for the merged masks. We explain this by the fact that overall C is not very well represented in the dataset (see [<xref ref-type="bibr" rid="c9">TS20</xref>] for details of the data analysis), and therefore the model often confuses it with GGO features, or fails to learn certain important features because of their under-representation in the data.</p>
<p>Results of the COVID-CT-Mask-Net evaluation are presented in <xref rid="tbl5" ref-type="table">Table 5</xref>, and the comparison of the best models we trained (highest COVID sensitivity and highest overall accuracy) in <xref rid="tbl6" ref-type="table">Table 6</xref>. All results are a significant improvement over the baseline COVID-CT-Mask-Net model in [<xref ref-type="bibr" rid="c9">TS20</xref>], which we beat by 3.08% (COVID sensitivity) and 5.10% (overall accuracy). Comparing the segmentation and classification results though, the advantage of the segmentation models learning from merged masks doesn&#x2019;t immediately translate into the advantage for solving the classification problem. Overall, the classifiers derived from these models are slightly better than the classifiers derived from the segmentation models for two classes, and noticeably better than GGO-only models. This advantage, though is much smaller than than the gap in the AP and mAP metrics for the corresponding segmentation problems.</p>
<p>Compared to benchmark models, we beat COVIDNet-CT [<xref ref-type="bibr" rid="c3">GWW20</xref>] by 1.07% in COVID-19 sensitivity.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Conclusions</title>
<p>In this paper we compared a number of Mask R-CNN models that detect and segment instances of two types of lesions in chest CT scans. We established that merging lesion masks for Ground Glass Opacity and Consolidation into a single lesion mask greatly improves the predictive power and the precision of the instance segmentation model compared to other approaches. We extended these model to predict COVID-19, common pneumonia and control classes using COVID-CT-Mask-Net architecture. On a large COVIDx-CT dataset (21192 chest CT scan slices), the classification model derived from the best segmentation model achieved the COVID-19 sensitivity of 92.68% and an overall accuracy of 96.33%, and the model derived from the segmentation model using both masks achieved a COVID-19 sensitivity of 93.88% and an overall accuracy of 95.64%. The source code and the pretrained models are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexTS1980/COVID-CT-Mask-Net">https://github.com/AlexTS1980/COVID-CT-Mask-Net</ext-link>.</p>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>All data, algorithms, code, pretrained weights, etc are publicly available.</p>
<p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/AlexTS1980/COVID-CT-Mask-Net">https://github.com/AlexTS1980/COVID-CT-Mask-Net</ext-link>
</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[BGCB20]</label><mixed-citation publication-type="other"><string-name><given-names>Charmaine</given-names> <surname>Butt</surname></string-name>, <string-name><given-names>Jagpal</given-names> <surname>Gill</surname></string-name>, <string-name><given-names>David</given-names> <surname>Chun</surname></string-name>, and <string-name><given-names>Benson A</given-names> <surname>Babu</surname></string-name>. <article-title>Deep learning system to screen coronavirus disease 2019 pneumonia</article-title>. <source>Applied Intelligence</source>, pages <fpage>1</fpage>&#x2013;<lpage>7</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c2"><label>[EVGW<sup>&#x002B;</sup>10]</label><mixed-citation publication-type="journal"><string-name><given-names>Mark</given-names> <surname>Everingham</surname></string-name>, <string-name><given-names>Luc</given-names> <surname>Van Gool</surname></string-name>, <string-name><given-names>Christopher KI</given-names> <surname>Williams</surname></string-name>, <string-name><given-names>John</given-names> <surname>Winn</surname></string-name>, and <string-name><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <article-title>The pascal visual object classes (voc) challenge</article-title>. <source>International journal of computer vision</source>, <volume>88</volume>(<issue>2</issue>):<fpage>303</fpage>&#x2013;<lpage>338</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c3"><label>[GWW20]</label><mixed-citation publication-type="other"><string-name><given-names>Hayden</given-names> <surname>Gunraj</surname></string-name>, <string-name><given-names>Linda</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>Alexander</given-names> <surname>Wong</surname></string-name>. <article-title>Covidnet-ct: A tailored deep convolutional neural network design for detection of covid-19 cases from chest ct images</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2009.05383</pub-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c4"><label>[HGDG17]</label><mixed-citation publication-type="other"><string-name><given-names>Kaiming</given-names> <surname>He</surname></string-name>, <string-name><given-names>Georgia</given-names> <surname>Gkioxari</surname></string-name>, <string-name><given-names>Piotr</given-names> <surname>Doll&#x00E1;r</surname></string-name>, and <string-name><given-names>Ross Girshick. Mask</given-names> <surname>r-cnn</surname></string-name>. In <source>Proceedings of the IEEE international conference on computer vision</source>, pages <fpage>2961</fpage>&#x2013;<lpage>2969</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c5"><label>[LFBL20]</label><mixed-citation publication-type="other"><string-name><given-names>Xiao</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Xu</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>Yun</given-names> <surname>Bian</surname></string-name>, and <string-name><given-names>Jianping</given-names> <surname>Lu</surname></string-name>. <article-title>Comparison of chest ct findings between covid-19 pneumonia and other types of viral pneumonia: a two-center retrospective study</article-title>. <source>European radiology</source>, pages <fpage>1</fpage>&#x2013;<lpage>9</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c6"><label>[LMB<sup>&#x002B;</sup>14]</label><mixed-citation publication-type="book"><string-name><given-names>Tsung-Yi</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Maire</surname></string-name>, <string-name><given-names>Serge</given-names> <surname>Belongie</surname></string-name>, <string-name><given-names>James</given-names> <surname>Hays</surname></string-name>, <string-name><given-names>Pietro</given-names> <surname>Perona</surname></string-name>, <string-name><given-names>Deva</given-names> <surname>Ramanan</surname></string-name>, <string-name><surname>Piotr</surname> <given-names>Doll&#x00E1;r</given-names></string-name>, and <string-name><given-names>C</given-names> <surname>Lawrence Zitnick.</surname></string-name> <chapter-title>Microsoft coco: Common objects in context</chapter-title>. In <source>European conference on computer vision</source>, pages <fpage>740</fpage>&#x2013;<lpage>755</lpage>. <publisher-name>Springer</publisher-name>, <year>2014</year>.</mixed-citation></ref>
<ref id="c7"><label>[LQX<sup>&#x002B;</sup>20]</label><mixed-citation publication-type="other"><string-name><given-names>Lin</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Lixin</given-names> <surname>Qin</surname></string-name>, <string-name><given-names>Zeguo</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>Youbing</given-names> <surname>Yin</surname></string-name>, <string-name><given-names>Xin</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Bin</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Junjie</given-names> <surname>Bai</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Zhenghan</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>Qi</given-names> <surname>Song</surname></string-name>, <etal>et al.</etal> <article-title>Artificial intelligence distinguishes covid-19 from community acquired pneumonia on chest ct</article-title>. <source>Radiology</source>, <year>2020</year>.</mixed-citation></ref>
<ref id="c8"><label>[SZL<sup>&#x002B;</sup>]</label><mixed-citation publication-type="other"><string-name><given-names>Ying</given-names> <surname>Song</surname></string-name>, <string-name><given-names>Shuangjia</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Liang</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Xiang</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Xiaodong</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Ziwang</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Jianwen</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Huiying</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>Yusheng</given-names> <surname>Jie</surname></string-name>, <string-name><given-names>Ruixuan</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Yutian</given-names> <surname>Chong</surname></string-name>, <string-name><given-names>Jun</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Yunfei</given-names> <surname>Zha</surname></string-name>, and <string-name><given-names>Yuedong</given-names> <surname>Yang</surname></string-name>. <article-title>Deep learning enables accurate diagnosis of novel coronavirus (covid-19) with ct images</article-title>. <source>medRxiv</source>.</mixed-citation></ref>
<ref id="c9"><label>[TS20]</label><mixed-citation publication-type="other"><string-name><given-names>Aram</given-names> <surname>Ter-Sarkisov</surname></string-name>. <article-title>Covid-ct-mask-net: Prediction of covid-19 from ct scans using regional features</article-title>. <source>medRxiv</source>, <year>2020</year>.</mixed-citation></ref>
<ref id="c10"><label>[WGM<sup>&#x002B;</sup>20]</label><mixed-citation publication-type="other"><string-name><given-names>Yu-Huan</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Shang-Hua</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>Jie</given-names> <surname>Mei</surname></string-name>, <string-name><given-names>Jun</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>Deng-Ping</given-names> <surname>Fan</surname></string-name>, <string-name><given-names>Chao-Wei</given-names> <surname>Zhao</surname></string-name>, and <string-name><given-names>Ming-Ming</given-names> <surname>Cheng</surname></string-name>. <article-title>Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2004.07054</pub-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c11"><label>[YWR<sup>&#x002B;</sup>20]</label><mixed-citation publication-type="journal"><string-name><given-names>Tao</given-names> <surname>Yan</surname></string-name>, <string-name><given-names>Pak Kin</given-names> <surname>Wong</surname></string-name>, <string-name><given-names>Hao</given-names> <surname>Ren</surname></string-name>, <string-name><given-names>Huaqiao</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Jiangtao</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>Yang</given-names> <surname>Li</surname></string-name>. <article-title>Automatic distinction between covid-19 and common pneumonia using multi-scale convolutional neural network on chest ct scans</article-title>. <source>Chaos, Solitons &#x0026; Fractals</source>, <volume>140</volume>:<fpage>110153</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c12"><label>[ZLS<sup>&#x002B;</sup>20]</label><mixed-citation publication-type="other"><string-name><given-names>Kang</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Xiaohong</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Jun</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Zhihuan</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Ye</given-names> <surname>Sang</surname></string-name>, <string-name><given-names>Xingwang</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Yunfei</given-names> <surname>Zha</surname></string-name>, <string-name><given-names>Wenhua</given-names> <surname>Liang</surname></string-name>, <string-name><given-names>Chengdi</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Ke</given-names> <surname>Wang</surname></string-name>, <etal>et al.</etal> <article-title>Clinically applicable ai system for accurate diagnosis, quantitative measurements, and prognosis of covid-19 pneumonia using computed tomography</article-title>. <source>Cell</source>, <year>2020</year>.</mixed-citation></ref>
<ref id="c13"><label>[ZYW<sup>&#x002B;</sup>20]</label><mixed-citation publication-type="other"><string-name><given-names>Dahai</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>Feifei</given-names> <surname>Yao</surname></string-name>, <string-name><given-names>Lijie</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Ling</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Yongjun</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>Jun</given-names> <surname>Ye</surname></string-name>, <string-name><given-names>Feng</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>Hui</given-names> <surname>Zhao</surname></string-name>, and <string-name><given-names>Rongbao</given-names> <surname>Gao</surname></string-name>. <article-title>A comparative study on the clinical features of covid-19 pneumonia to other pneumonias</article-title>. <source>Clinical Infectious Diseases</source>, <year>2020</year>.</mixed-citation></ref>
<ref id="c14"><label>[ZZHX20]</label><mixed-citation publication-type="other"><string-name><given-names>Jinyu</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>Yichen</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Xuehai</given-names> <surname>He</surname></string-name>, and <string-name><given-names>Pengtao</given-names> <surname>Xie</surname></string-name>. <article-title>Covid-ct-dataset: a ct scan dataset about covid-19</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2003.13865</pub-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c15"><label>[ZZX<sup>&#x002B;</sup>20]</label><mixed-citation publication-type="journal"><string-name><given-names>Wei</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>Zheng</given-names> <surname>Zhong</surname></string-name>, <string-name><given-names>Xingzhi</given-names> <surname>Xie</surname></string-name>, <string-name><given-names>Qizhi</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>Jun</given-names> <surname>Liu</surname></string-name>. <article-title>Ct scans of patients with 2019 novel coronavirus (covid-19) pneumonia</article-title>. <source>Theranostics</source>, <volume>10</volume>(<issue>10</issue>):<fpage>4606</fpage>, <year>2020</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>