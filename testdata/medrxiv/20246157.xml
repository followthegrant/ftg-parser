<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MEDRXIV</journal-id>
<journal-title-group>
<journal-title>medRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">medRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/2020.12.09.20246157</article-id>
<article-version>1.2</article-version>
<article-categories>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Epidemiology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Assessing the Performance of COVID-19 Forecasting Models in the U.S.</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0194-214X</contrib-id>
<name><surname>Colonna</surname><given-names>Kyle J.</given-names></name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0643-1971</contrib-id>
<name><surname>Cooke</surname><given-names>Roger M.</given-names></name>
<xref ref-type="aff" rid="a2">b</xref>
<xref ref-type="aff" rid="a3">c</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9837-1026</contrib-id>
<name><surname>Evans</surname><given-names>John S.</given-names></name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<aff id="a1"><label>a</label><institution>Environmental Health Department, Harvard T.H. Chan School of Public Health, Harvard University</institution>, Boston, MA 02115</aff>
<aff id="a2"><label>b</label><institution>Resources for the Future</institution>, Washington, DC 20036</aff>
<aff id="a3"><label>c</label><institution>Department of Mathematics, Delft University of Technology</institution>, Delft, <country>The Netherlands</country> 2628 XE</aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label><bold>Corresponding Author:</bold> Kyle J. Colonna, <bold>Address:</bold> 665 Huntington Ave., Building 1, Room 1301, Boston, MA, 02115 USA, <bold>Phone Number:</bold> 484-832-4003, <bold>Email:</bold> <email>kcolonna@g.harvard.edu</email></corresp>
<fn id="n1" fn-type="con"><p><bold>Author Contributions:</bold> <italic>Concept and design</italic>: Colonna, Evans</p></fn>
<fn><p><italic>Acquisition, analysis, or interpretation of data:</italic> Colonna, Cooke</p></fn>
<fn><p><italic>Drafting of the manuscript</italic>: Colonna, Evans</p></fn>
<fn><p><italic>Critical revision of the manuscript for important intellectual content</italic>: All authors</p></fn>
<fn><p><italic>Supervision</italic>: Evans</p></fn>
<fn id="n2" fn-type="others"><p><bold>Competing Interest Statement:</bold> The authors declare they have no competing interests that might be perceived to influence the results and/or discussion reported in this manuscript. There has also been no prior discussion with an editor.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2021</year>
</pub-date>
<elocation-id>2020.12.09.20246157</elocation-id>
<history>
<date date-type="received">
<day>09</day>
<month>12</month>
<year>2020</year>
</date>
<date date-type="rev-recd">
<day>05</day>
<month>5</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>06</day>
<month>5</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2021</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="20246157.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Dozens of coronavirus disease 2019 (COVID-19) forecasting models have been created, however, little information exists on their performance. Here we examined the performance of nine commonly-used COVID-19 forecasting models, as well as equal- and performance-weighted ensembles, based on their predictive accuracy and precision, and their probabilistic &#x2018;statistical accuracy (aka calibration)&#x2019; and &#x2018;information&#x2019; scores (measures commonly employed in the evaluation of expert judgment) (Cooke, 1991). Data on observed COVID-19 mortality in eight states, selected to reflect differences in racial demographics and COVID-19 case rates, over eight weeks in the summer of 2020 and eight weeks in the winter of 2021, provided the basis for evaluating model forecasts and exploring the stability/robustness of the results. Two models exhibited superior performance with both predictive and probabilistic measures during both pandemic phases. Models that performed poorly reflected &#x2018;overconfidence&#x2019; with tight forecast distributions. Models also systematically under-predicted mortality when cases were rising and over-predicted when cases were falling. Performance-weighted ensembles consistently outperformed the equal-weighted ensemble, with the Cooke&#x2019;s Classical Model-weighted ensemble outperforming the predictive-performance-weighted ensemble. Model performance depended on the time-frame of interest and racial composition, with better predictive forecasts in the near-term and for states with relatively high proportions of non-Hispanic Blacks. Performance also depended on case rate, with better predictive forecasts for states with relatively low case rates but better probabilistic forecasts for states with relatively high case rates. Both predictive and probabilistic performance are important, and both deserve consideration by model developers and those interested in using these models to inform policy.</p>
<sec>
<title>Significance Statement</title>
<p>Coronavirus disease 2019 (COVID-19) forecasting models can provide critical information for decision-making; however, there has been little published information on their performance. We examined the COVID-19 mortality forecasting performance of nine commonly-used and oft-cited models, as well as density-averaged equal- and performance-weighted ensembles of these models, during two phases of the pandemic. Only two of the nine models demonstrated superior predictive and probabilistic performance during both phases of the pandemic. Most of the models exhibited overconfidence, with overly-narrow forecast intervals. Performance-weighted ensembles demonstrated some advantages over the equal-weighted ensemble, primarily in probabilistic performance. As might be anticipated, predictions of near term mortality (next week) were better than predictions of mortality four weeks later.</p>
</sec>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>COVID-19</kwd>
<kwd>COVID-19 Decision-Making</kwd>
<kwd>Forecasting</kwd>
<kwd>Uncertainty Analysis</kwd>
<kwd>Cooke&#x2019;s Classical Model</kwd>
</kwd-group>
<counts>
<page-count count="13"/>
</counts>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>Kyle J. Colonna's involvement was funded by the Harvard Population Health Sciences PhD scholarship. Roger M. Cooke's involvement was pro bono. John S. Evans' involvement was funded by the Department of Environmental Health and the Harvard Cyprus Initiative at the T.F. Chan School of Public Health.</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>No IRB/oversight body approval or exemption was necessary as the data is publicly available.</p><p>All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Author Roger M. Cooke added; Additional results; Figure 1 revised; Additional tables for Supplemental Information; rewritten manuscript and supplemental information text.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>Effective non-pharmaceutical interventions (NPIs), or community mitigation strategies, are crucial in combating the spread of contagious illnesses like coronavirus disease 2019 (COVID-19) (<xref rid="c1" ref-type="bibr">1</xref>). Community NPIs, such as social distancing guidelines, restrictions, closures, and lockdowns, can effectively delay and diminish an epidemic peak (<xref rid="c1" ref-type="bibr">1</xref>-<xref rid="c3" ref-type="bibr">3</xref>), also known as flattening the epidemic curve (<xref rid="c2" ref-type="bibr">2</xref>). However, these NPIs can also have immediate educational and economic consequences (<xref rid="c4" ref-type="bibr">4</xref>-<xref rid="c6" ref-type="bibr">6</xref>). To make decisions on the implementation of community NPIs amidst the COVID-19 pandemic, the relevant stakeholders (e.g. government officials, community leaders, school administrators etc.) may desire estimates of the number of coronavirus cases, hospitalizations, and deaths likely to occur in their region of interest within the coming weeks.</p>
<p>Information about, and forecasts from, dozens of COVID-19 forecasting models have been made available via the University of Massachusetts Amherst Reich Lab&#x2019;s COVID-19 Forecast Hub (<xref rid="c7" ref-type="bibr">7</xref>). All of the modeling groups that have participated have provided predictions (or, central estimates), and most also quantitatively characterize the uncertainty in their predictions &#x2013; often giving an interquartile range (25% LCL to 75% UCL) and a <italic>90%</italic> confidence interval (5% LCL to 95% UCL) for each prediction (<xref rid="c8" ref-type="bibr">8</xref>).</p>
<p>Unfortunately, little peer-reviewed information about the predictive accuracy (i.e., how close are predictions to the actual observed values) or precision (i.e., how close are predictions to each other) of the forecasting models is widely available. Thus, stakeholders lack a scientific basis for deciding which models to trust and how much confidence to place in the forecasts they provide. Quite recently, one study in pre-print has become available (<xref rid="c9" ref-type="bibr">9</xref>). It compared the median absolute percent error of eight models stratified across world region, month of model estimation, and weeks of extrapolation (<xref rid="c9" ref-type="bibr">9</xref>). Collectively, the eight models released in July over a twelve week forecasting range had a median average percent error of about <italic>25%</italic>, with errors tending to increase with longer forecasts and the best performing model varying by region (<xref rid="c9" ref-type="bibr">9</xref>). While this information is quite useful and provides a sense of the typical bias of model predictions, other aspects of model performance may also be of interest. Users may also care about the precision of predictions and about the modeler&#x2019;s ability to properly characterize the uncertainty in their predictions.</p>
<p>Some may wonder whether better forecasts might be obtained by averaging the forecasts of two or more individual models. The aforementioned Reich Lab has created one such &#x2018;ensemble&#x2019; model (<xref rid="c10" ref-type="bibr">10</xref>), which has been prominently featured by the Centers for Disease Control and Prevention (CDC) (<xref rid="c11" ref-type="bibr">11</xref>). It involves an equal-weighted quantile averaged combination of individual model forecasts and is not performance-weighted (<xref rid="c10" ref-type="bibr">10</xref>). In the expert judgment literature it has been clearly demonstrated that performance-weighted combinations of expert opinion consistently outperform equally weighted combinations (<xref rid="c12" ref-type="bibr">12</xref>,<xref rid="c13" ref-type="bibr">13</xref>). Additionally, ensembles that average forecast quantiles have been shown to perform worse than those that average densities, producing overconfident results (i.e., tight forecast distributions that do not adequately capture the realization), and the results from averaging quantiles do not reflect the distributions provided by each included model (<xref rid="c12" ref-type="bibr">12</xref>, <xref rid="c14" ref-type="bibr">14</xref>). These issues deserve consideration in the development of any ensemble forecasting models.</p>
<p>The analysis presented below compares model forecasts with subsequent observations using several measures that reflect predictive (i.e., accuracy and precision) and probabilistic (i.e., statistical accuracy and information) performance. We also construct three ensemble models, one equal-weighted and two performance-weighted, all of which average forecast densities, and compare their performance with each other and with the best performing individual models. Lastly, we explore whether the available models tend to provide better forecasts under certain circumstances (i.e., differing case rates, racial demographics, and forecast time periods/phases of the pandemic).</p>
</sec>
<sec id="s2">
<label>2.</label>
<title>Results</title>
<sec id="s2a">
<label>2.1.</label>
<title>Performance Criteria</title>
<p>Two aspects of performance were evaluated for each model/ensemble&#x2013; (i) predictive performance (i.e., the accuracy and precision of the model/ensemble&#x2019;s central estimates, or predictions); and (ii) probabilistic performance (the modeler&#x2019;s ability to characterize the uncertainty in their predictions, reflected by their reported confidence intervals).</p>
<p>First, to evaluate the performance of the models&#x2019; central estimates, each observation, O<sub>j</sub>, was divided by the corresponding point prediction, P<sub>m,,j</sub>, of model m, to obtain the accuracy ratio, R<sub>m,j</sub> &#x2013; where j is an index reflecting the date, state, and time interval:
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="20246157v2_ueqn1.gif"/></alternatives>
</disp-formula>
</p>
<p>The distribution of the resulting accuracy ratios was then examined. For each model, m, the geometric mean (GM<sub>m =</sub> exp(mean<sub>m</sub>(ln(O<sub>j</sub> / P<sub>m,j</sub>)) taken over all calibration variables, j, and geometric standard deviation (GSD<sub>m</sub> = exp(SD<sub>m</sub>(ln(O<sub>j</sub> / P<sub>m,j</sub>)), also over all j, were computed and used as respective measures of the observed accuracy and precision of the model&#x2019;s central estimates.</p>
<p>Second, to evaluate the models&#x2019; ability to characterize the uncertainty in their predictions, the probabilistic performance of each model was assessed using the Cooke&#x2019;s Classical Model (CM) method (<xref rid="c15" ref-type="bibr">15</xref>). This method was initially designed for the evaluation of the performance of formally-elicited structured expert judgment (SEJ) &#x2013; where an expert&#x2019;s ability to meaningfully characterize the uncertainty in his or her estimates is arguably as important as the predictions they provide (<xref rid="c15" ref-type="bibr">15</xref>) &#x2013; and has been employed in many studies (<xref rid="c16" ref-type="bibr">16</xref>-<xref rid="c18" ref-type="bibr">18</xref>). We believe the CM can also be applied to the forecasts provided by the models, as the true observations are unknown at the time of forecasting and the modeling groups may serve as the &#x2018;expert&#x2019; while their forecasts may serve as their &#x2018;judgments&#x2019;.</p>
<p>The CM assesses &#x2018;statistical accuracy&#x2019;, or &#x2018;calibration&#x2019;, C, using Shannon&#x2019;s relative information statistic, I<sub>s</sub>, which compares the assessed and observed probabilities of calibration variables falling within various inter-quantile ranges (<xref rid="c15" ref-type="bibr">15</xref>, <xref rid="c19" ref-type="bibr">19</xref>). When assessing 5 quantiles, 5%, 25%, 50%, 75%, 95%, there are 6 inter quantile ranges for each model, m. The calibration score for each model, C<sub>m</sub>, on each variable takes the following formula:
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="20246157v2_ueqn2.gif"/></alternatives>
</disp-formula>
</p>
<p>Where &#x03C7;<sup>2</sup> is the chi-square distribution function with five degrees of freedom, s<sub>k</sub> is the percentage of the N realizations falling in the inter-quantile interval, k, for m, and p<sub>k</sub> is the probability which should apply to k. As the divergence between stated and observed probabilities increases, I<sub>s</sub> increases and C moves toward 0.</p>
<p>The CM assesses &#x2018;information&#x2019;, I, by comparing the width of the confidence intervals given by each model with the &#x2018;intrinsic range&#x2019; of each calibration variable (<xref rid="c15" ref-type="bibr">15</xref>, <xref rid="c19" ref-type="bibr">19</xref>). The intrinsic range for a variable is defined as the difference between the largest forecasted or observed value and the smallest forecasted or observed value (<xref rid="c15" ref-type="bibr">15</xref>, <xref rid="c19" ref-type="bibr">19</xref>). This range is expanded slightly by multiplying it by a user-defined expansion factor, 1&#x002B; F, where F is typically a small fraction (for our data, the default 10% was used) (<xref rid="c15" ref-type="bibr">15</xref>, <xref rid="c19" ref-type="bibr">19</xref>). Using this framework, the information score for each model, I<sub>m</sub>, on each variable is defined as:
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="20246157v2_ueqn3.gif"/></alternatives>
</disp-formula>
</p>
<p>Where p<sub>k</sub> are the probabilities provided by the model, m, and r<sub>k</sub> are the probabilities from a uniform (or log-uniform) probability density function over the intrinsic range. Models which concentrate their forecasts in a narrow range will have high information scores.<sup><xref ref-type="fn" rid="fn1">&#x002A;</xref></sup> From these two scores, together with the theory of proper scoring rules, the CM calculates performance weights as the product of calibration and information scores and then normalizes these so that they sum to 1 across all models (<xref rid="c15" ref-type="bibr">15</xref>, <xref rid="c19" ref-type="bibr">19</xref>). See <bold><italic>SI appendix</italic>, Notes 1 &#x0026; 2</bold>, for more details.</p>
</sec>
<sec id="s2b">
<label>2.2.</label>
<title>Individual Model Performance</title>
<p>The predictive and probabilistic performance results for each individual forecasting model are summarized in <bold><xref rid="fig1" ref-type="fig">Fig. 1</xref></bold> for both the summer 2020 and winter 2021 time periods. More quantitative detail on the performance results is provided in <bold>Table S1</bold> of the <bold><italic>SI appendix</italic></bold>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><p>Accuracy (GM) vs. precision (GSD) of the central estimates provided by the individual models for summer 2020 (<italic>Top Left</italic>) and winter 2021 (<italic>Top Right</italic>) periods. Calibration vs. information scores based on the forecast densities provided by the individual models for the summer 2020 (<italic>Bottom Left</italic>) and winter 2021 (<italic>Bottom Right</italic>) periods.</p></caption>
<graphic xlink:href="20246157v2_fig1.tif"/>
</fig>
<p>Focusing initially on the predictive performance of the models during the summer 2020 period, we see that &#x2013; (i) all models, except model C, have a GM greater than 1, indicating systemic underprediction of COVID-19 mortality, (ii) typically model predictions have little bias (&#x2264; 35%) with precision within a factor of 2, (iii) models A, D, and I have excellent accuracy (GM = 1.01, 1.11, and 1.09, respectively) and good precision (GSD = 1.57, 1.71, and 1.47, respectively), and (iv) models B, C, and E have substantial bias (&#x003E; 0.35%) and models B, E, G, and H have far worse precision (&#x003E; 2). When focusing on the probabilistic performance of models during this period, only three (or perhaps, four) of the nine models considered perform at all well &#x2013; models A, D, F, and to a lesser extent, G. The models that have the highest information scores (models B, E, H, and I) all have extremely low calibration scores (&#x003C;&#x003C; 0.01), suggesting &#x2018;overconfidence&#x2019; &#x2013; i.e., that their stated confidence intervals are far too narrow while simultaneously poorly capturing the true value.</p>
<p>Looking at the predictive performance during the winter 2021 period, we see that &#x2013; (i) models F, G, and H no longer provide forecasts, (ii) all six remaining models, except model E, have a GM less than 1, indicating systemic overprediction of mortality, (iii) the remaining models again have little bias (&#x003C; 30%) with precision within a factor of 2, except model C, (iv) model D exhibits even better accuracy (GM = 0.99) and precision (GSD = 1.33) than it did in the summer 2020 period, and model E now has excellent accuracy (GM = 1.04) and better precision (GSD = 1.52), (v) model A has good precision (GSD = 1.53) but so-so accuracy (GM = 0.76), and (vi) the models generally have better accuracy and precision than the summer 2020 period. For the probabilistic model performance, models A and D exhibit the best calibration scores (0.17 and 0.21, respectively), and again models B, E, and I are informative but their calibration is incredibly poor (&#x003C;&#x003C;0.01). Interestingly, unlike accuracy and precision, model calibration and information scores generally seem to be worse during the winter period.</p>
<p>The random expert hypothesis was tested to assess whether the putative differences in performance between models is due to noise. This hypothesis has been rejected for both periods of assessment. More detail on this analysis can be found in <bold><italic>SI appendix</italic>, Note 3</bold>.</p>
</sec>
<sec id="s2c">
<label>2.3.</label>
<title>Ensemble Model Designs</title>
<p>It is possible that better and more robust estimates might be obtained by producing an ensemble based on weighted combinations of the forecasts provided by the individual models. Two approaches of potential interest are &#x2013; (i) equal-weighted combinations, and (ii) performance-weighted combinations. In addition to a density-averaged equal-weighted ensemble, two density-averaged performance-weighted ensembles are considered, based on &#x2013; (a) &#x2018;predictive-performance&#x2019; weights and (b) CM weights.</p>
<p>For &#x2018;predictive-performance&#x2019; weighting, each random variable from each individual model is weighted in inverse proportion to the model&#x2019;s predictive variance (Weight<sub>m</sub> &#x221D; 1 / Var(O<sub>j</sub> / P<sub>m,j</sub>); i.e., inverse-variance weighting the distribution of ratios of observations to predictions for each individual model). This weighting method is based on, but is not to be confused with, the inverse-variance weighting method commonly used in meta-analysis (<xref rid="c20" ref-type="bibr">20</xref>).</p>
<p>The equal-weighted ensemble is established by equally-weighting the probability densities, not quantiles, given by the individual model forecasts. The predictive-performance-weighted ensemble and the CM-weighted ensemble both apply performance-weights to the probability densities of the individual model forecasts. The CM-weighted ensemble also uses a threshold significance level of 0.05 for its normalized CM-weights (calibration&#x002A;information), to gain robustness without significant loss in performance (<xref rid="c18" ref-type="bibr">18</xref>, <xref rid="c19" ref-type="bibr">19</xref>). More detail on why this cutoff was used is available in <bold><italic>SI appendix</italic>, Note 2</bold> and <bold>Note 4</bold>.</p>
</sec>
<sec id="s2d">
<label>2.4.</label>
<title>Ensemble Model Performance</title>
<p>The predictive and probabilistic performance results for each of the density-averaged ensemble models are summarized in <bold><xref rid="tbl1" ref-type="table">Table 1</xref></bold> for both the summer 2020 and winter 2021 time periods.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Ensemble model performance for summer 2020 (<italic>Left</italic>) and winter 2021 (<italic>Right</italic>)</title></caption>
<graphic xlink:href="20246157v2_tbl1.tif"/>
</table-wrap>
<p>Both performance-weighted ensembles would be expected to outperform the equal-weighted ensemble (<xref rid="c12" ref-type="bibr">12</xref>, <xref rid="c13" ref-type="bibr">13</xref>) and they do fulfill this expectation on almost all measures of performance, however, the differences in predictive performance are often small. The accuracy of the performance-based combinations is appreciably better than the equal-weighted combination in the summer 2020 data, but this advantage is not seen in the winter 2021 data. Very small differences in precision are observed in both data sets.</p>
<p>To explore whether these differences in accuracy between the summer 2020 and winter 2021 periods were due to the fact that three of the nine models included in the summer 2020 analysis were not reflected in the winter 2021 data, we reassessed model performance in the summer of 2020 using only data from the six models that were available during both seasons. The differences in accuracy seen in the summer of 2020 between the equal-weighted and performance-weighted combinations were even greater (equal weight = 1.23, predictive weight = 0.94, CM weight = 1.02) than those observed in the full data set. More detail on this analysis is provided in <bold><italic>SI appendix</italic>, Table S2</bold>.</p>
<p>There are substantial differences in probabilistic performance. The calibration scores of the CM-weighted ensemble (0.54 in summer 2020; 0.44 in winter 2021) are far better than those of the equal-weighted ensemble (0.03 during the summer, 0.17 during the winter) or the predictive-performance-weighted ensemble (0.04 during the summer, 0.25 during the winter). There are also small differences in information scores, where again the CM-weighted ensemble slightly outperforms the other two ensembles &#x2013; resulting in substantially higher CM weights during both periods.</p>
</sec>
<sec id="s2e">
<label>2.5.</label>
<title>Performance by Domain</title>
<p>It is also interesting to compare the performance of the individual models across three domains of interest &#x2013; (i) race (i.e., states which are heavily non-Hispanic White vs. states with relatively large non-Hispanic Black populations), (ii) COVID-19 case rates (i.e., states with a relatively low amount of weekly cases per 100,000 population vs. states with a relatively high amount of weekly cases per 100,000 population), and (iii) forecast period (i.e., forecasts of mortality for the upcoming week vs. forecasts of mortality for the week ending four weeks from the date on which the forecast was made). <bold><xref rid="tbl2" ref-type="table">Table 2</xref></bold> evaluates the performance of the equal-weighted density-averaged ensemble stratified by these three different domains (i.e., eight forecasts per domain).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Equal-weighted ensemble model performance for summer 2020 (<italic>Left</italic>) and winter 2021 (<italic>Right</italic>) stratified by different domains</title></caption>
<graphic xlink:href="20246157v2_tbl2.tif"/>
</table-wrap>
<p>Both the accuracy and precision of predictions for states with relatively high non-Hispanic Black populations are better than for states with a largely non-Hispanic White population. This difference appears to be stable and is seen in both the summer 2020 and winter 2021 data. However, there is no evidence of any stable differences in the calibration and information scores of forecasts depending on the racial composition of the state.</p>
<p>The summer 2020 data suggests that predictions are more accurate and precise in states with low case rates, and this remains true in the more recent winter 2021 data, but the difference in accuracy is not as substantial. On the other hand, it would appear that model calibration scores are somewhat better in states with high case rates. This difference appears to be stable and is seen with both the summer 2020 and winter 2021 data.</p>
<p>Lastly, as expected, the predictions of deaths in the near-term appear to be more accurate and precise than those in the mid-term, especially for the summer 2020 data. This remains true for the more recent winter 2020 data, but the differences in both accuracy and precision are smaller. There is some evidence that calibration scores are better for near-term forecasts in the summer data, but this does not persist in the winter data.</p>
</sec>
</sec>
<sec id="s3">
<label>3.</label>
<title>Discussion</title>
<p>The results suggest that, when considering both predictive (i.e., accuracy and precision) and probabilistic (i.e. statistical accuracy and information) forecast performance during both independent eight-week time periods (i.e., summer 2020 and winter 2021), two forecasting models, A and D, clearly outperform the other seven models, with model D demonstrating the most consistent dominance in all performance measures.</p>
<p>The fact that all but three of the nine models considered (models A, D, and F) have such low calibration scores for both time periods and that most of those also have high information scores indicates pervasive overconfidence. It is understandably difficult to predict changes in human behavior, thus, it is clear from these results that many modelers need to re-evaluate how they are quantifying uncertainty in their predictions.</p>
<p>We found that the tendency of models to underpredict in the summer of 2020 was replaced by a tendency to overpredict in the winter of 2021 &#x2013; suggesting that perhaps, in general, model predictions lag the actual changes in disease rates, which were increasing in the summer of 2020 and decreasing in the winter of 2021.</p>
<p>All of the assessed individual models are variations of a susceptible-exposed-infectious-removed (SEIR) compartmental model (<xref rid="c21" ref-type="bibr">21</xref>-<xref rid="c29" ref-type="bibr">29</xref>). They all forecast at the U.S. state level, and some also forecast at the national and county levels (<xref rid="c8" ref-type="bibr">8</xref>). Every model provides forecasts at the daily scale, which may then be aggregated to the weekly scale for at least four weeks ahead from forecast date (<xref rid="c8" ref-type="bibr">8</xref>).</p>
<p>However, while all models use mortality data to inform compartment transition rates, they varied in how they incorporated case, hospitalization, demographic, and mobility data (<xref rid="c21" ref-type="bibr">21</xref>-<xref rid="c29" ref-type="bibr">29</xref>). Both models A and D utilize case and mortality data, and model A utilizes demographic data, but neither model incorporates hospitalization or mobility data (<xref rid="c21" ref-type="bibr">21</xref>, <xref rid="c24" ref-type="bibr">24</xref>). Using case and mortality data, models A and D extrapolate deaths from lagged case counts, with a 14-day lag for model A (<xref rid="c21" ref-type="bibr">21</xref>) and a varying 15- to 30-day lag for model D (<xref rid="c24" ref-type="bibr">24</xref>). Additionally, while models A and D, like most other models, incorporate time-varying COVID-19 transmission rates which may indirectly reflect social distancing measures, neither model directly incorporates changes in social distancing measures for forecasts in the near-term (although, model A may incorporate changes in social distancing measures for forecasts past three weeks) (<xref rid="c21" ref-type="bibr">21</xref>, <xref rid="c24" ref-type="bibr">24</xref>).</p>
<p>The density-averaged performance-weighted ensembles outperform the density-averaged equal-weighted ensemble, but the differences in predictive performance are modest. In contrast, when evaluated in terms of probabilistic performance there are clear advantages to performance weighting using the CM weights.</p>
<p>Across the three domains and two performance attributes considered, consistent and substantial performance differences were seen only in a few instances &#x2013; and these differed depending on whether one focuses on predictive or probabilistic performance. Predictive performance is generally better for forecasts in states with relatively high non-Hispanic Black populations and relatively low case rates, but the observed differences are often modest. Probabilistic performance is consistently better when case rates are high. Finally, as might be expected, near-term predictions (i.e., deaths in the next week) outperform mid-term predictions (i.e., deaths in the week ending four weeks from forecast date).</p>
<p>Our sense is that these results are more suggestive than conclusive, because &#x2013; (i) due to the selection criteria, only nine models are examined; (ii) they are based on model performance in one eight-week period in the summer of 2020 and one eight-week period in the winter of 2021; (iii) they are limited to eight states; (iv) they consider only forecasts of mortality and no other outcomes, such as cases or hospitalizations, that may be of interest for decision-makers; (v) our analysis does not attempt to explain what model design choices lead to better or worse performance.</p>
<p>Conversely, our analysis has several strengths &#x2013; (i) it evaluates the performance of a set of leading models which have been used to forecast COVID-19 mortality in the US; (ii) it considers both predictive (i.e., accuracy and precision) and probabilistic (i.e., statistical accuracy and information) performance; (iii) it assesses several approaches for constructing ensemble models; (iv) it examines performance differences across states selected to reflect differences in racial composition and variations in COVID-19 case rate; and (v) the analysis considers data from two distinct phases of the pandemic.</p>
</sec>
<sec id="s4">
<label>4.</label>
<title>Materials and Methods</title>
<p>Our analysis involves a comparison of model forecasts with subsequent observations of weekly deaths from COVID-19 in four states (Idaho, Louisiana, New York and Maine) over an eight-week period during the summer of 2020 and four other states (Georgia, Maryland, Vermont, and Wyoming) over a subsequent eight-week period during the winter of 2021.</p>
<p>The states considered in our analysis were selected on the basis of recent case rates of COVID-19 (cases/100,000 population within the previous week) (<xref rid="c30" ref-type="bibr">30</xref>) and racial composition (majority non-Hispanic Black vs. majority non-Hispanic White) (<xref rid="c31" ref-type="bibr">31</xref>). Racial composition was of interest as the COVID-19 mortality rate for non-Hispanic Black Americans at the time of analysis was more than twice that of non-Hispanic White Americans (<xref rid="c32" ref-type="bibr">32</xref>). With these two domains in mind, our goal was to assess two states for each time period with relatively high case rates (Idaho and Louisiana; Georgia and Vermont); two with relatively low case rates (Maine and New York; Maryland and Wyoming); two with a relatively high fraction of population reported as non-Hispanic Black (Louisiana and New York; Georgia and Maryland); and two with a relatively high fraction of population reported as non-Hispanic White (Idaho and Maine; Vermont and Wyoming). This was done to assess how models perform forecasting for states under varying circumstances. More detail on how the case rates and racial composition for states were determined, as well as how states were selected, is available in <bold>SI appendix, Note 5</bold> and <bold>Tables S3 &#x0026; S4</bold>.</p>
<p>We were interested in the models&#x2019; ability to forecast COVID-19 deaths in both the near-term and the medium-term. Near-term performance was gauged using projected COVID-19 deaths in the week immediately after the forecast was made. Medium-term performance was gauged using projected COVID-19 deaths in the week ending four weeks after the forecast was made.</p>
<p>Our evaluations of model performance for the eight states and the two forecast periods of interest (week ending one week in the future and week ending four weeks in the future) were examined twice within each time period &#x2013; once for forecasts made on June 13<sup>th</sup>, 2020, or January 10<sup>th</sup>, 2021, and a second time for forecasts made on July 11<sup>th</sup>, 2020, or February 7<sup>th</sup>, 2021 (no overlap in forecasts). In total, 16 comparisons of model forecasts with observed deaths were made for each time period and each model.</p>
<p>Of the many models providing data to the COVID-19 Forecast Hub&#x2019;s data repository (<xref rid="c8" ref-type="bibr">8</xref>), only the models that provided weekly COVID-19 mortality forecasts made on the same day of the week, with forecasts at the weekly time-scale for all four of the initially assessed states and were without missing forecasts were selected for this analysis. These include: OliverWyman-Navigator (Model A) (<xref rid="c21" ref-type="bibr">21</xref>), MOBS-GLEAM_COVID (Model B) (<xref rid="c22" ref-type="bibr">22</xref>), JHU_IDD-CovidSP (Model C) (<xref rid="c23" ref-type="bibr">23</xref>), UMass-MechBayes (Model D) (<xref rid="c24" ref-type="bibr">24</xref>), UCLA-SuEIR (Model E) (<xref rid="c25" ref-type="bibr">25</xref>), YYG-ParamSearch (Model F) (<xref rid="c26" ref-type="bibr">26</xref>), UT-Mobility (Model G) (<xref rid="c27" ref-type="bibr">27</xref>), USACE-ERDC_SEIR (Model H) (<xref rid="c28" ref-type="bibr">28</xref>), and Covid19Sim-Simulator (Model I) (<xref rid="c29" ref-type="bibr">29</xref>).</p>
</sec>
<sec sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material>
<label>Supplemental Information</label>
<media xlink:href="supplements/246157_file02.pdf" />
</supplementary-material>
</sec>
</body>
<back>
<sec sec-type="data-availability">
<title>Data Availability</title>
<p>Observed state COVID-19 mortality and case data was gathered from the Centers for Disease Control and Prevention (CDC) (30). State population and racial composition data was collected from one-year estimates from the Census Bureau's 2018 and 2019 American Community Survey (ACS) (31). Tables S3 &#x0026; S4 in the SI appendix provide the racial composition statistics and case rate data.
Model forecasting data was gathered from the COVID-19 Forecast Hub's publicly available structured data storage repository on GitHub (8). Tables S5 &#x0026; S6 in the SI appendix provide the model and ensemble predictions, their uncertainty distributions, and the subsequent observations of COVID-19 mortality.</p>
</sec>
<sec id="s5">
<title>Code Availability</title>
<p>Data was analyzed using Microsoft Excel and EXCALIBUR (a software package for using Cooke&#x2019;s Classical Model) (<xref rid="c33" ref-type="bibr">33</xref>).</p>
</sec>
<sec id="s6">
<title>Data Availability</title>
<p>Observed state COVID-19 mortality and case data was gathered from the Centers for Disease Control and Prevention (CDC) (<xref rid="c30" ref-type="bibr">30</xref>). State population and racial composition data was collected from one-year estimates from the Census Bureau&#x2019;s 2018 and 2019 American Community Survey (ACS) (<xref rid="c31" ref-type="bibr">31</xref>). <bold>Tables S3 &#x0026; S4</bold> in the <bold><italic>SI appendix</italic></bold> provide the racial composition statistics and case rate data.</p>
<p>Model forecasting data was gathered from the COVID-19 Forecast Hub&#x2019;s publicly available structured data storage repository on GitHub (<xref rid="c8" ref-type="bibr">8</xref>).<bold>Tables S5 &#x0026; S6</bold> in the <bold><italic>SI appendix</italic></bold> provide the model and ensemble predictions, their uncertainty distributions, and the subsequent observations of COVID-19 mortality.</p>
</sec>
<ack>
<title>ACKNOWLEDGMENTS</title>
<p>We want to thank Willy Aspinall, Jouni Tuomisto, and Jacqueline Macdonald for contributing to our thoughts about this and for their feedback on our early drafts of the paper.</p>
</ack>
<sec id="s7">
<title>Funding Sources</title>
<p>Kyle J. Colonna&#x2019;s involvement was funded by the Harvard Population Health Sciences PhD scholarship. Roger M. Cooke&#x2019;s involvement was pro bono. John S. Evans&#x2019; involvement was funded by the Department of Environmental Health and the Harvard Cyprus Initiative at the T.F. Chan School of Public Health.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="website"><collab>Centers for Disease Control and Prevention, Nonpharmaceutical Interventions (NPIs</collab>). <source>Centers for Disease Control and Prevention</source> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/nonpharmaceutical-interventions/index.html">https://www.cdc.gov/nonpharmaceutical-interventions/index.html</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>R. M.</given-names> <surname>Anderson</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Heesterbeek</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Klinkenberg</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Hollingsworth</surname></string-name>, <article-title>How will country-based mitigation measures influence the course of the COVID-19 epidemic&#x00E3;</article-title> <source>Lancet</source> <volume>395</volume>, <fpage>931</fpage>&#x2013;<lpage>934</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Flaxman</surname></string-name>, <etal>et al.</etal>, <article-title>Estimating the effects of non-pharmaceutical interventions on COVID-19 in Europe</article-title>. <source>Nature</source> <volume>584</volume>, <fpage>257</fpage>&#x2013;<lpage>261</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="other"><collab>International Monetary Fund Research Dept., World Economic Outlook, April 2020?: The Great Lockdown</collab>. <source>International Monetary Fund</source> (<year>2020</year>) (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="other"><collab>The United Nations Educational, Scientific and Cultural Organization, Adverse consequences of school closures</collab>. <source>The United Nations Educational, Scientific and Cultural Organization</source> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://en.unesco.org/covid19/educationresponse/consequences">https://en.unesco.org/covid19/educationresponse/consequences</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Nicola</surname></string-name>, <etal>et al.</etal>, <article-title>The socio-economic implications of the coronavirus pandemic (COVID-19): A review</article-title>. <source>Int J Surg</source> <volume>78</volume>, <fpage>185</fpage>&#x2013;<lpage>193</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="website"><collab>University of Massachusetts, Reich Lab, The COVID-19 Forecast Hub</collab>. <source>University of Massachusetts</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://covid19forecasthub.org/">https://covid19forecasthub.org/</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="website"><collab>University of Massachusetts, Reich Lab, Data from &#x201C;reichlab/covid19-forecast-hub.&#x201D; Github</collab>. Available at <ext-link ext-link-type="uri" xlink:href="https://github.com/reichlab/covid19-forecast-hub">https://github.com/reichlab/covid19-forecast-hub</ext-link>. Deposited 3 May 2021.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="website"><string-name><given-names>J.</given-names> <surname>Friedman</surname></string-name>, <etal>et al.</etal>, <article-title>Predictive performance of international COVID-19 mortality forecasting models</article-title>. <source>medRxiv [Preprint]</source> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2020.07.13.20151233">https://doi.org/10.1101/2020.07.13.20151233</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>)</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="website"><string-name><given-names>E. L.</given-names> <surname>Ray</surname></string-name>, <etal>et al.</etal>, <article-title>Ensemble Forecasts of Coronavirus Disease 2019 (COVID-19) in the U.S</article-title>. <source>medRxiv [Preprint]</source> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2020.08.19.20177493">https://doi.org/10.1101/2020.08.19.20177493</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>)</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="website"><collab>Centers for Disease Control and Prevention, Forecasts of COVID-19 Deaths</collab>. <source>Centers for Disease Control and Prevention</source> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html">https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>A. R.</given-names> <surname>Colson</surname></string-name>, <string-name><given-names>R. M.</given-names> <surname>Cooke</surname></string-name>, <article-title>Cross validation for the classical model of structured expert judgment</article-title>. <source>Reliability Engineering &#x0026; System Safety</source> <volume>163</volume>, <fpage>109</fpage>&#x2013;<lpage>120</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>A. R.</given-names> <surname>Colson</surname></string-name>, <string-name><given-names>R. M.</given-names> <surname>Cooke</surname></string-name>, <article-title>Expert Elicitation: Using the Classical Model to Validate Experts&#x2019; Judgments</article-title>. <source>Rev Environ Econ Policy</source> <volume>12</volume>, <fpage>113</fpage>&#x2013;<lpage>132</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Bamber</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Aspinall</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Cooke</surname></string-name>, <article-title>A commentary on &#x201C;how to interpret expert judgment assessments of twenty-first century sea-level rise&#x201D; by Hylke de Vries and Roderik SW van de Wal</article-title>. <source>Climatic Change</source> <volume>137</volume>, <fpage>321</fpage>&#x2013;<lpage>328</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="book"><string-name><given-names>R. M.</given-names> <surname>Cooke</surname></string-name>, <source>Experts in Uncertainty: Opinion and Subjective Probability in Science</source> (<publisher-loc>Oxford University Press</publisher-loc>, <year>1991</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>R. M.</given-names> <surname>Cooke</surname></string-name>, <etal>et al.</etal>, <article-title>A Probabilistic Characterization of the Relationship between Fine Particulate Matter and Mortality: Elicitation of European Experts</article-title>. <source>Environ. Sci. Technol</source>. <volume>41</volume>, <fpage>6598</fpage>&#x2013;<lpage>6605</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Hald</surname></string-name>, <etal>et al.</etal>, <article-title>World Health Organization Estimates of the Relative Contributions of Food to the Burden of Disease Due to Selected Foodborne Hazards: A Structured Expert Elicitation</article-title>. <source>PLoS One</source> <volume>11</volume> (<year>2016</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>J. L.</given-names> <surname>Bamber</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Oppenheimer</surname></string-name>, <string-name><given-names>R. E.</given-names> <surname>Kopp</surname></string-name>, <string-name><given-names>W. P.</given-names> <surname>Aspinall</surname></string-name>, <string-name><given-names>R. M.</given-names> <surname>Cooke</surname></string-name>, <article-title>Ice sheet contributions to future sea-level rise from structured expert judgment</article-title>. <source>PNAS</source> <volume>116</volume>, <fpage>11195</fpage>&#x2013;<lpage>11200</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><string-name><given-names>R.</given-names> <surname>Cooke</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Goossens</surname></string-name>, <chapter-title>Procedures Guide for Structured Expert Judgment</chapter-title>. <source>European Communities</source>, <publisher-loc>Luxembourg</publisher-loc>, <publisher-name>EUR</publisher-name> (<year>2000</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="book"><string-name><given-names>J.</given-names> <surname>Hartung</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Knapp</surname></string-name>, <string-name><given-names>B. K.</given-names> <surname>Sinha</surname></string-name>, <source>Statistical Meta-Analysis with Applications</source> (<publisher-name>Wiley</publisher-name>, <year>2008</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="website"><string-name><surname>Oliver</surname> <given-names>Wyman</given-names></string-name>, <article-title>Oliver Wyman COVID-19 Pandemic Navigator</article-title>. <source>Oliver Wyman</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://pandemicnavigator.oliverwyman.com/">https://pandemicnavigator.oliverwyman.com/</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="website"><collab>Laboratory fo the Modeling of Biological &#x002B; Socio-Technical Systems, COVID-19 Mobility</collab>. <source>The Gleam Project</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://covid19.gleamproject.org/mobility">https://covid19.gleamproject.org/mobility</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="website"><collab>Infectious Disease Dynamics, Projects</collab> COVID-19. <source>Johns Hopkins University</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="http://www.iddynamics.jhsph.edu/projects/covid-19">http://www.iddynamics.jhsph.edu/projects/covid-19</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="website"><string-name><given-names>D.</given-names> <surname>Sheldon</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Gibson</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Reich</surname></string-name>, <source>Data from &#x201C;dsheldon/covid.&#x201D; Github</source>. Available at <ext-link ext-link-type="uri" xlink:href="https://github.com/dsheldon/covid">https://github.com/dsheldon/covid</ext-link>. Deposited on 12 <month>April</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="website"><collab>Statistical Machine Learning Lab, UCLAML Combating COVID-19</collab>. <source>University of California, Los Angeles</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://covid19.uclaml.org/index.html">https://covid19.uclaml.org/index.html</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="website"><string-name><given-names>Y.</given-names> <surname>Gu</surname></string-name>, <article-title>COVID-19 Projections Using Machine Learning</article-title>. <source>Youyang Gu</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://covid19-projections.com/">https://covid19-projections.com/</ext-link> (accessed <date-in-citation content-type="access-date">on 3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="website"><collab>COVID-19 Modeling Consortium, COVID-19 Mortality Projections for US States</collab>. <source>The University of Texas at Austin</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://covid-19.tacc.utexas.edu/dashboards/us/">https://covid-19.tacc.utexas.edu/dashboards/us/</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="website"><collab>Engineer Research and Development Center, Data from &#x201C;erdc-cv19/seir-model.&#x201D; Github</collab>. Available at <ext-link ext-link-type="uri" xlink:href="https://github.com/erdc-cv19/seir-model">https://github.com/erdc-cv19/seir-model</ext-link>. Deposited on 3 <month>August</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="website"><collab>Massachusetts General Hospital Institute for Technology Assessment, COVID-19 Simulator</collab>. <source>Massachusetts General Hospital Institute for Technology Assessment</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://covid19sim.org/">https://covid19sim.org/</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="website"><collab>Centers for Disease Control and Prevention, COVID-19 Response, COVID-19 Case Surveillance Public Data Access, Summary, and Limitations</collab>. <source>Centers for Disease Control and Prevention</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://www.census.gov/programs-surveys/acs/data.html">https://www.census.gov/programs-surveys/acs/data.html</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="website"><string-name><given-names>U.S. Census</given-names> <surname>Bureau</surname></string-name>, <article-title>American Community Survey Data</article-title>. <source>The United States Census Bureau</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36">https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="website"><collab>Centers for Disease Control and Prevention, Hospitalization and Death by Race/Ethnicity</collab>. <source>Centers for Disease Control and Prevention</source> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html">https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html</ext-link> (accessed <date-in-citation content-type="access-date">18 August 2020</date-in-citation>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="website"><collab>LightTwist Software, Excalibur</collab>. <source>LightTwist Software</source>. <ext-link ext-link-type="uri" xlink:href="https://lighttwist-software.com/excalibur/">https://lighttwist-software.com/excalibur/</ext-link> (accessed <date-in-citation content-type="access-date">3 May 2021</date-in-citation>).</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1">
<label><sup>&#x002A;</sup></label>
<p>It is important to note that, while a precision score of 1 is optimal (i.e., all predictions are equally close to each other relative to the observation), the higher the information score the better (i.e., the tighter the forecast distribution, the more information the forecast provides).</p>
</fn>
</fn-group>
</back>
</article>