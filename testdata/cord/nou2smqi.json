{"cord_uid": "nou2smqi", "sha": "123ecdc45828c8378ddb577140f698e90d116280", "source_x": "ArXiv", "title": "Designing Rotationally Invariant Neural Networks from PDEs and Variational Methods", "doi": "", "pmcid": "", "pubmed_id": "", "license": "arxiv", "abstract": "Partial differential equation (PDE) models and their associated variational energy formulations are often rotationally invariant by design. This ensures that a rotation of the input results in a corresponding rotation of the output, which is desirable in applications such as image analysis. Convolutional neural networks (CNNs) do not share this property, and existing remedies are often complex. The goal of our paper is to investigate how diffusion and variational models achieve rotation invariance and transfer these ideas to neural networks. As a core novelty we propose activation functions which couple network channels by combining information from several oriented filters. This guarantees rotation invariance within the basic building blocks of the networks while still allowing for directional filtering. The resulting neural architectures are inherently rotationally invariant. With only a few small filters, they can achieve the same invariance as existing techniques which require a fine-grained sampling of orientations. Our findings help to translate diffusion and variational models into mathematically well-founded network architectures, and provide novel concepts for model-based CNN design.", "publish_time": "2021-08-31", "authors": "Alt, Tobias; Schrader, Karl; Weickert, Joachim; Peter, Pascal; Augustin, Matthias", "journal": "", "mag_id": "", "who_covidence_id": "", "arxiv_id": "2108.13993", "pdf_json_files": ["document_parses/pdf_json/123ecdc45828c8378ddb577140f698e90d116280.json"], "pmc_json_files": [""], "url": "https://arxiv.org/pdf/2108.13993v1.pdf", "s2_id": "237363805"}