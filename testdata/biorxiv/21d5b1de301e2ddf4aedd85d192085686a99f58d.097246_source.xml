<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/097246</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;097246</article-id><article-id pub-id-type="other" hwp:sub-type="slug">097246</article-id><article-id pub-id-type="other" hwp:sub-type="tag">097246</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Pathology" hwp:journal="biorxiv"><subject>Pathology</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">DeepScope: Nonintrusive Whole Slide Saliency Annotation and Prediction from Pathologists at the Microscope</article-title></title-group><author-notes hwp:id="author-notes-1"><fn fn-type="other" hwp:id="fn-1"><p hwp:id="p-1"><email hwp:id="email-1">ajs625@cornell.edu</email>, <email hwp:id="email-2">sirintrs@mskcc.org</email>, <email hwp:id="email-3">alahmadh@mskcc.org</email>, <email hwp:id="email-4">schueffp@mskcc.org</email>, <email hwp:id="email-5">fuchst@mskcc.org</email></p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Schaumberg Andrew J."><surname>Schaumberg</surname><given-names>Andrew J.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Sirintrapun S. Joseph"><surname>Sirintrapun</surname><given-names>S. Joseph</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Al-Ahmadie Hikmat A."><surname>Al-Ahmadie</surname><given-names>Hikmat A.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Schüffler Peter J."><surname>Schüffler</surname><given-names>Peter J.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Fuchs Thomas J."><surname>Fuchs</surname><given-names>Thomas J.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-3" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-2" hwp:rel-id="aff-4">4</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Memorial Sloan Kettering Cancer Center and the Tri-Institutional Training Program in Computational Biology and Medicine</institution> New York, NY, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Weill Cornell Graduate School of Medical Sciences</institution> New York, NY, <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2 xref-aff-3-3"><label>3</label><institution hwp:id="institution-3">Department of Pathology, Memorial Sloan Kettering Cancer Center</institution> New York, NY, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1 xref-aff-4-2"><label>4</label><institution hwp:id="institution-4">Department of Medical Physics, Memorial Sloan Kettering Cancer Center</institution> New York, NY, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2016-12-29T12:03:24-08:00">
    <day>29</day><month>12</month><year>2016</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-01-19T08:39:14-08:00">
    <day>19</day><month>1</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2016-12-29T12:25:15-08:00">
    <day>29</day><month>12</month><year>2016</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-01-19T09:05:18-08:00">
    <day>19</day><month>1</month><year>2017</year>
  </pub-date><elocation-id>097246</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2016-12-28"><day>28</day><month>12</month><year>2016</year></date>
<date date-type="accepted" hwp:start="2017-01-19"><day>19</day><month>1</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="097246.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/097246v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="097246.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/097246v2/097246v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/097246v2/097246v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">Modern digital pathology departments have grown to produce whole-slide image data at petabyte scale, an unprecedented treasure chest for medical machine learning tasks. Unfortunately, most digital slides are not annotated at the image level, hindering large-scale application of supervised learning. Manual labeling is prohibitive, requiring pathologists with decades of training and outstanding clinical service responsibilities. This problem is further aggravated by the United States Food and Drug Administration’s ruling that primary diagnosis must come from a glass slide rather than a digital image. We present the first end-to-end framework to overcome this problem, gathering annotations in a nonintrusive manner during a pathologist’s routine clinical work: (i) microscope-specific 3D-printed commodity camera mounts are used to video record the glass-slide-based clinical diagnosis process; (ii) after routine scanning of the whole slide, the video frames are registered to the digital slide; (iii) motion and observation time are estimated to generate a spatial and temporal saliency map of the whole slide. Demonstrating the utility of these annotations, we train a convolutional neural network that detects diagnosis-relevant salient regions, then report accuracy of 85.15% in bladder and 91.40% in prostate, with 75.00% accuracy when training on prostate but predicting in bladder, despite different pathologists examining the different tissues. When training on one patient but testing on another, AUROC in bladder is 0.7929±0.1109 and in prostate is 0.9568±0.0374. Our tool is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://bitbucket.org/aschaumberg/deepscope" ext-link-type="uri" xlink:href="http://bitbucket.org/aschaumberg/deepscope" hwp:id="ext-link-2">https://bitbucket.org/aschaumberg/deepscope</ext-link>.</p></abstract><counts><page-count count="11"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-2">Introduction</title><p hwp:id="p-4">Computational pathology<sup>[<xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>]</sup> relies on training data annotated by human experts on digital images. However, the bulk of a pathologist’s daily clinical work remains manual on analog light microscopes. A noninterfering system which translates this abundance of expert knowledge at the microscope into labeled digital image data is desired.</p><p hwp:id="p-5">Tracking a pathologist’s viewing path along the analyzed tissue slide to detect local image saliency has been previously proposed. These approaches include whole slide images displayed on one or more monitors with an eye-tracker<sup>[<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>]</sup>, mouse-tracker<sup>[<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>]</sup> or viewport-tracker<sup>[<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>]</sup> – but may suffer confounds including peripheral vision<sup>[<xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]</sup>, head turning<sup>[<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>]</sup>, distracting extraneous detail<sup>[<xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>]</sup>, monitor resolution<sup>[<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>]</sup>, multimonitor curvature<sup>[<xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>]</sup>, and monitor bezel field of view fragmentation<sup>[<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>]</sup>. Only our approach does not change the pathologist’s medical practice from the microscope. The microscope is a class I device appropriate for primary diagnosis according to the United States Food and Drug Administration, while whole slide imaging devices are class III<sup>[<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>]</sup>.</p><p hwp:id="p-6">In light of the confounds of alternatives, its centuries of use in pathology, and its favorable regulatory position for primary diagnosis, we believe the microscope is the gold standard for measuring image region saliency. Indeed, there is prior work annotating regions of interest at the microscope for cytology technicians to automatically position the slide for a pathologist<sup>[<xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]</sup>.</p><p hwp:id="p-7">We therefore propose a new, noninterfering workflow for automated video-based detection of region saliency using pathologist viewing time at the microscope (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig 1</xref>). Using a commodity digital camera, rather than a custom embedded eye-tracking device<sup>[<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>,<xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>]</sup>, we video record the pathologist’s entire field of view at a tandem microscope to obtain slide region viewing times and register these regions to whole slide image scans.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-8">Proposed microscope-based saliency predictor pipeline workflow. The pathology session is recorded, the slide is scanned, the video frames are registered to scan patches. Lens change detection guides registration and viewing time is recorded for periods without motion. A convolutional neural net learns to classify patches as salient (long looks) or not.</p></caption><graphic xlink:href="097246_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-9">Second, we train a convolutional neural network [CNN] on these observation times to predict whether or not a whole slide image region is viewed by a pathologist at the microscope for more than 0.1 seconds. As more videos become available, our CNN predicting image saliency may be further trained and improved, through online learning.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-3">Methods and Materials</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-4">Pathologists</title><p hwp:id="p-10">Pathologists were assistant attending rank with several years experience each. Trainees have different, less efficient, slide viewing strategies<sup>[<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>,<xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">14</xref>]</sup>. Region viewing times and path were automatically recorded during a pathologist’s routine slide analysis, without interference.</p></sec><sec id="s2g" hwp:id="sec-4"><title hwp:id="title-5">Patient slides</title><p hwp:id="p-11">Two bladder cancer patients were studied by SJS. Two prostate cancer patients were studied were studied by HAA. One slide per patient was used, for four slides total (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig 2</xref>).</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-12">Bladder cancer left, prostate cancer right. Training, validation, testing done on top slides, with additional same-tissue testing on bottom slides. For cross-tissue testing, top slide tested against other top slide. Viewing time heatmap for top left bladder shown in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig 7</xref>. Note how the top bladder has more edges than the more solid bottom bladder, while the prostates have similar tissue texture. We believe this impacts interpatient accuracy, shown in <xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Fig 9</xref>.</p></caption><graphic xlink:href="097246_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2b" hwp:id="sec-5"><title hwp:id="title-6">Scan preprocessing</title><p hwp:id="p-13">Microscope slides, inspected by a pathologist, were scanned at 0.5±0.003 microns per pixel [px], using an Aperio AT2 scanner. The resulting SVS data file consists of multiple levels, where level 0 is not downsampled, level 1 is downsampled by a factor of 4, level 2 by a factor of 16, and level 3 by a factor of 32. From each level, 800x800px patches were extracted via the OpenSlide software library<sup>[<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>]</sup>. In bladder, adjacent patches in a level overlap at least 50%, to avoid windowing artifacts in registration. In prostate, adjacent patches overlap at least 75%, to best center the pathologist’s field of view on the little tissue in a needle biopsy. Patches evenly cover the entire level without gaps. Scans were either taken before a technician applied marker to the slides, to indicate regions of interest to the pathologist, or after markings were scrubbed from the slide. However, these marks were evident in the pathologist videos discussed in the next section.</p></sec><sec id="s2c" hwp:id="sec-6"><title hwp:id="title-7">Video acquisition</title><p hwp:id="p-14">A Panasonic Lumix DMC-FH10 camera with a 16.1 megapixel charge-coupled device [CCD], capable of 720p motion JPEG video at 30 frames per second, was mounted on a second head of an Olympus BX53F multihead teaching microscope to record the pathologist’s slide inspection. Microscope objective lens magnifications were 4x, 10x, 20x, 40x, and 100x. Eyepiece lens magnifications was 10x. The pathologist was told to ignore the device and person recording video at the microscope during inspection. The mount (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig 1</xref>) for this camera was designed in OpenSCAD and 3D-printed on a MakerBot 2 using polylactic acid [PLA] filament.</p></sec><sec id="s2d" hwp:id="sec-7"><title hwp:id="title-8">Camera choice</title><p hwp:id="p-15">Many expensive microscope-mounted cameras exist, such as the Lumenera INFINITY-HD and Olympus DP27, which have very good picture quality and frame rate. The Lumenera INFINITY-HD is a CMOS camera, not CCD, so slide movement will skew the image rather than blur it, and we did not want to confound image registration or motion detection with rolling shutter skew. Both cameras trim the field of view to a center-most rectangle for viewing on a computer monitor, which is a loss of information, and we instead assign viewing time to the entire pathologist-viewed 800x800px PNG patch from the SVS file representing the whole slide scan image. Both cameras do not have USB or Ethernet ports carrying a video feed accessible as a webcam, for registration to the whole slide scan. The Olympus DP27 may be accessible as a Windows TWAIN device, but we could not make this work in Linux. Finally, the HDMI port on both carries high-quality but encrypted video information that we cannot record, and we did not wish to buy a Hauppauge HDMI recording device, because we had a cheaper commodity camera on hand already. We also considered automated screenshots of the video feed in Aperio ImageScope as displayed on a computer monitor, but we observed a lower frame rate and detecting lens change is complicated because the entire field of view is not available. Recording low-quality video on a commodity camera to a SecureDigital [SD] memory card is inexpensive, captures the entire field of view, and is generally applicable in any hospital. For this pilot study, we used only one camera for video recording, rather than two different microscope cameras, potentially eliminating a confound for how many pixels are moving during rapid short movements of the slide. For 3D printing requisite camera mounts, open source tools are available.</p></sec><sec id="s2e" hwp:id="sec-8"><title hwp:id="title-9">Video preprocessing and registration</title><p hwp:id="p-16">A Debian Linux computer converted individual slide inspection video frames to PNG files using the ffmpeg program. OpenCV software detected slide movement via optical flow<sup>[<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]</sup>, comparing the current video frame with the preceding video frame, shown in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig 3</xref>. We defined slide movement to start if 10% or more of pixels in the entire field of view of the camera have a movement vector of at least one, and defined slide movement to stop if 2% or fewer of the pixels in the entire field of view of the camera have a movement vector of at least one. The entire field of view of the camera is 640x480px, a small subset of these capture the circular field of view at the microscope eyepiece, with the remaining pixels being black (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig 3</xref>). The representative frame among consecutive unmoving frames moved the least. The ImageJ<sup>[<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]</sup> SURF<sup>[<xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]</sup><xref rid="fn1" ref-type="fn" hwp:id="xref-fn-2-1" hwp:rel-id="fn-2"><sup>1</sup></xref> and OpenCV software libraries registered each representative to an 800x800px image patch taken from the high-resolution Aperio slide scanner. Each patch aggregated total pathologist time.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-17">Optical flow, showing pixel movement grid. The frame has few moving pixels before <italic toggle="yes">(left)</italic> and after <italic toggle="yes">(right)</italic> pathologist moves the slide. A pathologist looks at a slide region for the duration of consecutive stationary frames.</p></caption><graphic xlink:href="097246_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-18">The partially automated registration process starts with initial manual registration of a frame, followed by automated registration within the preceding registration’s spatial neighborhood. The OpenCV implementation of random sample consensus [RANSAC] follows a point set registration procedure to calculate a rigid body transformation between the shared SURF interest points in the video frame and an image patch, to find the distance in pixels that the video frame is off-center from the patch, with the least off-center image patch selected as the best registration, because the pathologist’s fovea is in approximately the same place in this video frame and image patch (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig 4</xref>). The ImageJ SURF implementation of interest points shared between two images provides a set of corresponding interest point pairs, one from the frame and one from the patch, enabling point set registration. A final manual curation ensures correctness. This process reduces manual effort in that automatic registrations are rarely far away from the correct registration, and when automatic registrations are incorrect, the manual curation has only a small, local neighborhood to search to make the correction, then automatic registration may proceed from the correction. Fully automated image registration is not part of this study.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-19">The best image registration for a given video frame (same frame top left and top right) from the commodity camera at the microscope eyepiece compared to two different high-quality patches (bottom left and bottom right) from the whole slide scan image minimizes the length of the green line, which is the distance from the center of the patch to the center of the frame mapped into the patch’s coordinate space.</p></caption><graphic xlink:href="097246_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-20">During slide inspection, the pathologist may switch objective lens magnification. Lens change is detected automatically when the field of view bounding box of nonblack pixels changes size (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig 5</xref>), and is important because SURF is scale-invariant so registrations may otherwise proceed at an unchanged magnification.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-21">Lens change detection: the normal non-black pixel bounding box is initially 415x415px. A change to 415x282px indicates the pathologist changing the lens, thus changing slide magnification. Note some pixels that may appear black are called non-black due to difficult to perceive noise in the image, which effects calculated bounding box size. All images shown at same scale trimmed to bounding box.</p></caption><graphic xlink:href="097246_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s2f" hwp:id="sec-9"><title hwp:id="title-10">Deep learning</title><p hwp:id="p-22">We used Caffe<sup>[<xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>]</sup> for deep learning of convolutional features in a binary classification model given the 800x800px image patches labeled with pathologist viewing times in seconds. To adapt for our purpose CaffeNet (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Fig 6</xref>), which is similar to AlexNet<sup>[<xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>]</sup>, we re-initialized its top layer’s weights after ImageNet<sup>[<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>]</sup> pre-training. Two output neurons were connected to the re-initialized layer, then training followed on augmented 800x800px patches for 10,000 iterations in Caffe. In bladder, our model simply predicted whether or not a pathologist viewed an 800x800px patch more than 0.1 sec (30 fps camera). In prostate, due to the higher overlap between adjacent patches and less tissue available, to be salient a patch met at least one of these criteria: (1) viewed more than 0.1 sec, (2) immediately above, below, left, or right of at least two patches viewed more than 0.1 sec, or (3) above, below, left, right, or diagonal from at least three patches viewed more than 0.1 sec such that all three are not on the same side. In this way, image patches highly overlapping in the neighborhood of salient patches were not themselves considered nonsalient if a pathologist happened to jump over them during observation.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-6"><p hwp:id="p-23">Caffenet neuron counts, convolutional layers, dropout<sup>[<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>]</sup> layers, and fully-connected layers.</p></caption><graphic xlink:href="097246_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec></sec><sec id="s3" hwp:id="sec-10"><label>3</label><title hwp:id="title-11">Experiments</title><p hwp:id="p-24">Urothelial carcinoma (bladder) in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig 7</xref> was analyzed first, with HAA inspecting at the microscope. Viewed regions at the microscope corresponded to the whole-slide scan SVS file at magnification levels 2 and 1. We restricted our analysis to level 2, having insufficient level 1 data. We split level 2 into three regions: left, center, and right. Due to over 50% overlap among the slide’s total 54 800x800px level 2 patches, we excluded the center region from analysis, but retained the left and right, which did not overlap (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Fig 8</xref>).</p><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-7"><p hwp:id="p-25">Pathologist viewing times in seconds at the microscope for low (<italic toggle="yes">left</italic>, 10x, level 2) and high magnification (<italic toggle="yes">right</italic>, 20x, level 1), registered to the same urothelial carcinoma slide scan.</p></caption><graphic xlink:href="097246_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><fig id="fig8" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8:</label><caption hwp:id="caption-8"><p hwp:id="p-26">Scaled image patches of left and right sides of bladder patient 1 slide. Middle excluded here and not used in analysis, to isolate left and right sides from each other. Note far left and far right have less tissue, but tissue is present for training. Also note the overlap among patches is evenly distributed and greater than 50%.</p></caption><graphic xlink:href="097246_fig8" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-27">In bladder, we considered a negative example to be a patch viewed for 0.1 seconds (3 frames or fewer, 30 fps) or less, and a positive example viewed for more than 0.1 seconds (4 frames or more). This threshold produced 9 positive and 9 negative examples on the left side, and the same number on the right side. We performed three-fold cross validation on the left side (6+ and 6− examples training set, 3+ and 3− examples validation set), then used the model with the highest validation accuracy on the right side to calculate test accuracy, an estimate of generalization error (<xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Fig 9</xref>). This cross validation was duplicated ten times on the left side, each time estimating test accuracy, to calculate a confidence interval. We then duplicated this training/validating on the left and testing on the right.</p><fig id="fig9" position="float" fig-type="figure" orientation="portrait" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3 xref-fig-9-4 xref-fig-9-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9:</label><caption hwp:id="caption-9"><p hwp:id="p-28">Ten three-fold cross validation trials for bladder [BLCA] and prostate [PRAD], evaluated for intrapatient training/validating on left while testing on the right and vice versa. Each model is evaluated against a different patient (interpatient), slides in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig 2</xref>). The needle for prostate cancer biopsy may standardize the distribution of prostate tissue in the whole slide, maintaining a higher accuracy of the prostate classifier on an interpatient basis than the bladder cancer classifier. The bladder patients are transurethral resections taken by cuts rather than a standard gauge needle.</p></caption><graphic xlink:href="097246_fig9" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-29">Training and validation data were augmented. For a 800x800px patch, all 1 degree rotations through 360 degrees were saved, then cropped to the centermost 512x512px, then scaled to 256x256px. Caffe then randomly cropped 256x256px patches to 227x227px for each iteration of CaffeNet learning. No images in the validation set were derived from the training set, and vice versa. A training set is two concatenated folds, with the remaining fold as validation. We used validation accuracy reported by Caffe after training completed, then averaged over all three folds. We did not augment the test set comprising 9 positive and 9 negative examples. In addition to the bladder cancer slide, we analyzed two prostate cancer needle biopsy slides, with SJS inspecting these slides.</p></sec><sec id="s4" hwp:id="sec-11"><label>4</label><title hwp:id="title-12">Results</title><p hwp:id="p-30">In bladder, when training/validating on the left side and testing on the right, mean test accuracy is 0.781±0.0423 (stdev) with 95% confidence interval [CI] from 0.750 to 0.811 (df=9, Student’s T). When training/validating on the right and testing on the left, mean test accuracy is 0.922±0.0468 with 0.889−0.956 95% CI. Overall mean test accuracy is 85.15%. The left and right test accuracies differ (p=0.000135, Wilcoxon rank-sum, n=20), while validation accuracies do not (p=0.9118, n=20). This may suggest nonhomogenous information content throughout the slide. The pathologist started and ended slide inspection on the right. The second bladder had different morphology and model accuracy reduced to 0.678±0.0772, 0.623−0.734 95% CI. Moreover, the second bladder had only 7 positive examples available, whereas both prostates and the first bladder had at least 9 positive examples available.</p><p hwp:id="p-31">For the first prostate slide, training on the left side and testing on the right, we find accuracy 0.867±0.0597, 0.824−0.909 95% CI. Training on the right and testing on left, we find 0.961±0.0457, 0.928−0.994 95% CI. Overall mean test accuracy is 91.40%. Taking the best model learned from this first prostate (right side, test accuracy 100%, 18/18), we tested on the second prostate’s right side (because the left did not have 9 positive training examples) and find 0.967±0.0287, 0.946−0.987 95% CI. We also tested this model on the bladder cancer slide, and find 0.780 accuracy on the left and 0.720 on the right (9+ and 9− training examples each), mean accuracy 75.00%. The best bladder cancer model predicts every patch is not salient in both prostates, presumably because the little tissue in prostate is insufficient for a positive saliency prediction.</p><p hwp:id="p-32">Interpatient AUROC for bladder and prostate is shown in <xref rid="fig10" ref-type="fig" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Fig 10</xref>. In prostate, nine salient and nine nonsalient examples are drawn from the second patient. Average AUROC was calculated from ten such draws, achieving a mean±stdev of 0.9568±0.0374 and 95% CI of 0.9301−0.9835. Over all 17 salient and 13 nonsalient patches used from the second prostate patient, the AUROC is 0.9615. In bladder, due to fewer patches available in the small slide, only seven salient and seven nonsalient examples are drawn from the second patient. Average AUROC was calculated for ten such draws, achieving 0.7929±0.1109 and 95% CI of 0.7176−0.8763. Over all 7 salient and 17 nonsalient patches used from the second bladder patient, the AUROC is 0.7437. These nonoverlapping confidence intervals are evidence the bladder cancer classifier distinguishes salient from nonsalient patches less well than the prostate cancer classifier, and a Mann-Whitney U test indeed finds the difference in classifier performance by these ten draws each from bladder and prostate is significant (p=0.0001325) (<xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Fig 9</xref>).</p><fig id="fig10" position="float" fig-type="figure" orientation="portrait" hwp:id="F10" hwp:rev-id="xref-fig-10-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/FIG10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">fig10</object-id><label>Figure 10:</label><caption hwp:id="caption-10"><p hwp:id="p-33">Interpatient area under the receiver operating characteristic [AUROC] for bladder and prostate, with dashed black curve for average AUROC over draws of the data and blue line for all data used from the patient.</p></caption><graphic xlink:href="097246_fig10" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-34">The deep convolutional network CaffeNet emits a score from 0 to 1 when predicting if an image patch is salient or not, and when taking a score of greater than 0.5 to be salient, the p-value from Fisher’s Exact Test is 1.167e-7 in prostate (16 true positives, 1 false negative, 0 false positives, 13 true negatives) and 0.009916 in bladder (7 true positives, 0 false negatives, 7 false positives, 10 true negatives), indicating our trained CaffeNet classifier accurately predicts salient and nonsalient regions in both tissues.</p></sec><sec id="s5" hwp:id="sec-12"><label>5</label><title hwp:id="title-13">Conclusion</title><p hwp:id="p-35">Collecting image-based expert annotations for the deluge of medical data at modern hospitals is one of the biggest bottlenecks for the application of large-scale supervised machine learning. We address this with a novel framework that combines a commodity camera, 3D-printed mount, and software stack to build a predictive model for saliency on whole slides, i.e. where a pathologist looks to make a diagnosis. The registered regions from the digital slide scan are markedly higher quality than the camera frames, since they do not suffer from debris, vignetting, and other artifacts. The proposed CNN is able to predict salient slide regions with a test accuracy of 85-91%. We plan to scale up this pilot study to more patients, tissues, and pathologists.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-11"><p hwp:id="p-36">Accuracies of ten trials of three-fold cross validation in bladder. Validation and test accuracies for a single slide video of urothelial carcinoma (patient 1, slide at upper left in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig 2</xref>, performance plotted at left in <xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-4" hwp:rel-id="F9">Fig 9</xref>)), left side of the slide versus right side. Mean test error training/validating on left and testing on right [leftright] is 0.781 at stdev 0.0423 with 95% confidence interval from 0.750 to 0.811 (df=9, Student’s t), while mean test error right to left [rightleft] is 0.922 at stdev 0.0468 with 95% confidence interval from 0.889 to 0.956 (df=9, Student’s T). In cases where best validation accuracies (highlighted in yellow) tie in multiple models, all tied models are used to evaluate test accuracy and their results averaged. Testing the best classifier (highlighted in cyan, highest test accuracy on this and other folds, secondarily highest mean validation accuracy) on draws of the data on the second bladder patient, accuracies are 0.643, 0.786, 0.714, 0.786, 0.714, 0.714, 0.643, 0.571, 0.643, and 0.571.</p></caption><graphic xlink:href="097246_tbl1" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;097246v2/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-12"><p hwp:id="p-37">Accuracies of ten trials of three-fold cross validation in prostate. Validation and test accuracies for a single slide video of prostate adenocarcinoma (patient 1, slide at upper right in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig 2</xref>, performance plotted at right in <xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-5" hwp:rel-id="F9">Fig 9</xref>)), left side of the slide versus right side. Mean test error training/validating leftright is 0.781 at stdev 0.0423 with 95% confidence interval from 0.750 to 0.811 (df=9, Student’s t), while mean test error rightleft is 0.922 at stdev 0.0468 with 95% confidence interval from 0.889 to 0.956 (df=9, Student’s T). Testing the best classifier on draws of the data on the second prostate patient, accuracies are 0.944, 1, 0.944, 0.944, 1, 0.944, 0.944, 0.944, 1, and 1.</p></caption><graphic xlink:href="097246_tbl2" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-14">Acknowledgments</title><p hwp:id="p-38">AJS was supported by NIH/NCI grant F31CA214029 and the Tri-Institutional Training Program in Computational Biology and Medicine (via NIH training grant T32GM083937). This research was funded in part through the NIH/NCI Cancer Center Support Grant P30CA008748. AJS thanks Du Cheng and the Medical Student Executive Committee of Weill Cornell Medical College for free 3D printing access and instruction. AJS thanks Mariam Aly for taking the photo of the camera on the orange 3D-printed mount in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig 1</xref>. We acknowledge fair use of part of a doctor stick figure image in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig 1</xref> from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.123rf.com/" ext-link-type="uri" xlink:href="http://www.123rf.com/" hwp:id="ext-link-3">123rf.com</ext-link>. AJS thanks Mark Rubin for helpful pathology discussion. AJS thanks Paul Tatarsky and Juan Perin for Caffe install help on the Memorial Sloan Kettering supercomputer. Several GPUs were used in this research, one of which we gratefully acknowledge was provided by NVIDIA Corporation as part of a GPU Research Center award to TJF.</p></ack><fn-group hwp:id="fn-group-1"><fn id="fn1" hwp:id="fn-2" hwp:rev-id="xref-fn-2-1"><label><sup>1</sup></label><p hwp:id="p-39">ImageJ SURF is released under the GNU GPL and is available for download from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://labun.com/imagej-surf/" ext-link-type="uri" xlink:href="http://labun.com/imagej-surf/" hwp:id="ext-link-4">http://labun.com/imagej-surf/</ext-link></p></fn></fn-group><ref-list hwp:id="ref-list-1"><title hwp:id="title-15">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Ball R."><given-names>R.</given-names> <surname>Ball</surname></string-name> and <string-name name-style="western" hwp:sortable="North C."><given-names>C.</given-names> <surname>North</surname></string-name>. <article-title hwp:id="article-title-2">The effects of peripheral vision and physical navigation on large scale visualization</article-title>. <source hwp:id="source-1">Proceedings of Graphics Interface 2008</source>, pages <fpage>9</fpage>–<lpage>16</lpage>, <year>2008</year>. ISSN <issn>978-1-56881-423-0</issn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="https://dl.acm.org/citation.cfm?id=1375717" ext-link-type="uri" xlink:href="https://dl.acm.org/citation.cfm?id=1375717" hwp:id="ext-link-5">https://dl.acm.org/citation.cfm?id=1375717</ext-link>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>[2]</label><citation publication-type="book" citation-type="book" ref:id="097246v2.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Ball R."><given-names>R.</given-names> <surname>Ball</surname>,</string-name> <string-name name-style="western" hwp:sortable="North C."><given-names>C.</given-names> <surname>North</surname></string-name>, and <string-name name-style="western" hwp:sortable="Bowman D."><given-names>D.</given-names> <surname>Bowman</surname></string-name>. <chapter-title>Move to Improve: Promoting Physical Navigation to Increase User Performance with Large Displays</chapter-title>. pages <fpage>191</fpage>–<lpage>200</lpage>. <publisher-loc>ACM</publisher-loc>, <year>2007</year>. ISBN <isbn>978-1-59593-593-9</isbn>. doi: <pub-id pub-id-type="doi">10.1145/1240624.1240656</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1145/1240624.1240656" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/1240624.1240656" hwp:id="ext-link-6">http://dx.doi.org/10.1145/1240624.1240656</ext-link>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="book" citation-type="book" ref:id="097246v2.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bay H."><given-names>H.</given-names> <surname>Bay</surname></string-name>, <string-name name-style="western" hwp:sortable="Tuytelaars T."><given-names>T.</given-names> <surname>Tuytelaars</surname></string-name>, and <string-name name-style="western" hwp:sortable="Van Gool L."><given-names>L.</given-names> <surname>Van Gool</surname></string-name>. <chapter-title>SURF: Speeded Up Robust Features</chapter-title>. In <person-group hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Leonardis A."><given-names>A.</given-names> <surname>Leonardis</surname></string-name>,<string-name name-style="western" hwp:sortable="Bischof H."><given-names>H.</given-names> <surname>Bischof</surname></string-name>, and <string-name name-style="western" hwp:sortable="Pinz A."><given-names>A.</given-names> <surname>Pinz</surname></string-name></person-group>, editors, <source hwp:id="source-2">Computer Vision – ECCV 2006</source>, <volume>volume 3951</volume>, pages <fpage>404</fpage>–<lpage>417</lpage>. <publisher-loc>Springer Berlin Heidelberg</publisher-loc>, <year>2006</year>. ISBN <isbn>978-3-540-33832-1</isbn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1007/11744023_32" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/11744023_32" hwp:id="ext-link-7">http://dx.doi.org/10.1007/11744023_32</ext-link>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Begelman G."><given-names>G.</given-names> <surname>Begelman</surname></string-name>, <string-name name-style="western" hwp:sortable="Lifshits M."><given-names>M.</given-names> <surname>Lifshits</surname></string-name>, and <string-name name-style="western" hwp:sortable="Rivlin E."><given-names>E.</given-names> <surname>Rivlin</surname></string-name>. <article-title hwp:id="article-title-3">Visual positioning of previously defined ROIs on microscopic slides</article-title>. <source hwp:id="source-3">IEEE Transactions on Information Technology in Biomedicine</source>, <volume>10</volume>(<issue>1</issue>):<fpage>42</fpage>–<lpage>50</lpage>, Jan. <year>2006</year>. ISSN <issn>1089-7771</issn>. doi: <pub-id pub-id-type="doi">10.1109/titb.2005.856856</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1109/titb.2005.856856" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/titb.2005.856856" hwp:id="ext-link-8">http://dx.doi.org/10.1109/titb.2005.856856</ext-link>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Brunye T."><given-names>T.</given-names> <surname>Brunye</surname></string-name>, <string-name name-style="western" hwp:sortable="Carney P."><given-names>P.</given-names> <surname>Carney</surname></string-name>, <string-name name-style="western" hwp:sortable="Allison K."><given-names>K.</given-names> <surname>Allison</surname></string-name>, <string-name name-style="western" hwp:sortable="Shapiro L."><given-names>L.</given-names> <surname>Shapiro</surname></string-name>, <string-name name-style="western" hwp:sortable="Weaver D."><given-names>D.</given-names> <surname>Weaver</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Elmore J."><given-names>J.</given-names> <surname>Elmore</surname></string-name>. <article-title hwp:id="article-title-4">Eye Movements as an Index of Pathologist Visual Expertise: A Pilot Study</article-title>. <source hwp:id="source-4">PLoS ONE</source>, <volume>9</volume>(<issue>8</issue>):<fpage>e103447</fpage>, Aug. <year>2014</year>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0103447</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1371/journal.pone.0103447" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0103447" hwp:id="ext-link-9">http://dx.doi.org/10.1371/journal.pone.0103447</ext-link>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>[6]</label><citation publication-type="book" citation-type="book" ref:id="097246v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Deng J."><given-names>J.</given-names> <surname>Deng</surname></string-name>, <string-name name-style="western" hwp:sortable="Dong W."><given-names>W.</given-names> <surname>Dong</surname></string-name>, <string-name name-style="western" hwp:sortable="Socher R."><given-names>R.</given-names> <surname>Socher</surname></string-name>, <string-name name-style="western" hwp:sortable="Li L.-J."><given-names>L.-J.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Li K."><given-names>K.</given-names> <surname>Li</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Fei-Fei L."><given-names>L.</given-names> <surname>Fei-Fei</surname></string-name>. <chapter-title>ImageNet: A large-scale hierarchical image database</chapter-title>. pages <fpage>248</fpage>–<lpage>255</lpage>. <publisher-loc>IEEE</publisher-loc>, June <year>2009</year>. ISBN <isbn>978-1-4244-3992-8</isbn>. <pub-id pub-id-type="doi">doi: 10.1109/cvpr.2009.5206848</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1109/cvpr.2009.5206848" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/cvpr.2009.5206848" hwp:id="ext-link-10">http://dx.doi.org/10.1109/cvpr.2009.5206848</ext-link>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Eivazi S."><given-names>S.</given-names> <surname>Eivazi</surname></string-name>, <string-name name-style="western" hwp:sortable="Bednarik R."><given-names>R.</given-names> <surname>Bednarik</surname></string-name>, <string-name name-style="western" hwp:sortable="Leinonen V."><given-names>V.</given-names> <surname>Leinonen</surname>,</string-name> <string-name name-style="western" hwp:sortable="Fraunberg Mikael von und zu"><given-names>Mikael von und zu</given-names> <surname>Fraunberg</surname></string-name>, and <string-name name-style="western" hwp:sortable="Jaaskelainen J."><given-names>J.</given-names> <surname>Jaaskelainen</surname></string-name>. <article-title hwp:id="article-title-5">Embedding an Eye Tracker Into a Surgical Microscope: Requirements, Design, and Implementation</article-title>. <source hwp:id="source-5">IEEE Sensors Journal</source>, <volume>16</volume>(<issue>7</issue>):<fpage>2070</fpage>–<lpage>2078</lpage>, Apr. <year>2016</year>. ISSN <issn>1530-437X</issn>. <pub-id pub-id-type="doi">doi: 10.1109/jsen.2015.2501237</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1109/jsen.2015.2501237" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/jsen.2015.2501237" hwp:id="ext-link-11">http://dx.doi.org/10.1109/jsen.2015.2501237</ext-link>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>[8]</label><citation publication-type="book" citation-type="book" ref:id="097246v2.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Farneback G."><given-names>G.</given-names> <surname>Farneback</surname></string-name>. <chapter-title>Two-frame Motion Estimation Based on Polynomial Expansion</chapter-title>. pages <fpage>363</fpage>–<lpage>370</lpage>. <publisher-loc>Springer-Verlag</publisher-loc>, <year>2003</year>. ISBN <isbn>3-540-40601-8</isbn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://portal.acm.org/citation.cfm?id=1763974.1764031" ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1763974.1764031" hwp:id="ext-link-12">http://portal.acm.org/citation.cfm?id=1763974.1764031</ext-link>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Fuchs T."><given-names>T.</given-names> <surname>Fuchs</surname></string-name> and <string-name name-style="western" hwp:sortable="Buhmann J."><given-names>J.</given-names> <surname>Buhmann</surname></string-name>. <article-title hwp:id="article-title-6">Computational pathology: challenges and promises for tissue analysis</article-title>. <source hwp:id="source-6">Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society</source>, <volume>35</volume>(<issue>7-8</issue>):<fpage>515</fpage>–<lpage>530</lpage>, Oct. <year>2011</year>. ISSN <issn>1879-0771</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.compmedimag.2011.02.006</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="ttp://dx.doi.org/10.1016/j.compmedimag.2011.02.006" ext-link-type="uri" xlink:href="ttp://dx.doi.org/10.1016/j.compmedimag.2011.02.006" hwp:id="ext-link-13">http://dx.doi.org/10.1016/j.compmedimag.2011.02.006</ext-link>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Goode A."><given-names>A.</given-names> <surname>Goode</surname></string-name>, <string-name name-style="western" hwp:sortable="Gilbert B."><given-names>B.</given-names> <surname>Gilbert</surname></string-name>, <string-name name-style="western" hwp:sortable="Harkes J."><given-names>J.</given-names> <surname>Harkes</surname></string-name>, <string-name name-style="western" hwp:sortable="Jukic D."><given-names>D.</given-names> <surname>Jukic</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Satyanarayanan M."><given-names>M.</given-names> <surname>Satyanarayanan</surname></string-name>. <article-title hwp:id="article-title-7">OpenSlide: A vendor-neutral software foundation for digital pathology</article-title>. <source hwp:id="source-7">Journal of pathology informatics</source>, <volume>4</volume>, <year>2013</year>. ISSN <issn>2229-5089</issn>. doi: <pub-id pub-id-type="doi">10.4103/2153-3539.119005</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.4103/2153-3539.119005" ext-link-type="uri" xlink:href="http://dx.doi.org/10.4103/2153-3539.119005" hwp:id="ext-link-14">http://dx.doi.org/10.4103/2153-3539.119005</ext-link>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="other" citation-type="journal" ref:id="097246v2.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Jia Y."><given-names>Y.</given-names> <surname>Jia</surname></string-name>, <string-name name-style="western" hwp:sortable="Shelhamer E."><given-names>E.</given-names> <surname>Shelhamer</surname></string-name>, <string-name name-style="western" hwp:sortable="Donahue J."><given-names>J.</given-names> <surname>Donahue</surname></string-name>, <string-name name-style="western" hwp:sortable="Karayev S."><given-names>S.</given-names> <surname>Karayev</surname></string-name>, <string-name name-style="western" hwp:sortable="Long J."><given-names>J.</given-names> <surname>Long</surname></string-name>, <string-name name-style="western" hwp:sortable="Girshick R."><given-names>R.</given-names> <surname>Girshick</surname></string-name>, <string-name name-style="western" hwp:sortable="Guadarrama S."><given-names>S.</given-names> <surname>Guadarrama</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Darrell T."><given-names>T.</given-names> <surname>Darrell</surname></string-name>. <article-title hwp:id="article-title-8">Caffe: Convolutional Architecture for Fast Feature Embedding</article-title>. June <year>2014</year>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://arxiv.org/abs/1408.5093v1.pdf" ext-link-type="uri" xlink:href="http://arxiv.org/abs/1408.5093v1.pdf" hwp:id="ext-link-15">http://arxiv.org/abs/1408.5093v1.pdf</ext-link>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="book" citation-type="book" ref:id="097246v2.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Keerativittayanun S."><given-names>S.</given-names> <surname>Keerativittayanun</surname></string-name>, <string-name name-style="western" hwp:sortable="Rakjaeng K."><given-names>K.</given-names> <surname>Rakjaeng</surname></string-name>, <string-name name-style="western" hwp:sortable="Kondo T."><given-names>T.</given-names> <surname>Kondo</surname></string-name>, <string-name name-style="western" hwp:sortable="Kongprawechnon W."><given-names>W.</given-names> <surname>Kongprawechnon</surname></string-name>, <string-name name-style="western" hwp:sortable="Tungpimolrut K."><given-names>K.</given-names> <surname>Tungpimolrut</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Leelasawassuk T."><given-names>T.</given-names> <surname>Leelasawassuk</surname></string-name>. <chapter-title>Eye tracking system for Ophthalmic Operating Microscope</chapter-title>. pages <fpage>653</fpage>–<lpage>656</lpage>. <publisher-loc>IEEE</publisher-loc>, Aug. <year>2009</year>. ISBN <isbn>978-4-907764-34-0</isbn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5333310" ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5333310" hwp:id="ext-link-16">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5333310</ext-link>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="other" citation-type="journal" ref:id="097246v2.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Krizhevsky A."><given-names>A.</given-names> <surname>Krizhevsky</surname></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><given-names>I.</given-names> <surname>Sutskever</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Hinton G."><given-names>G.</given-names> <surname>Hinton</surname></string-name>. <article-title hwp:id="article-title-9">Imagenet classification with deep convolutional neural networks</article-title>. <year>2012</year>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://papers.nips.cc/paper/4824-imagenet-classification-w" ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-w" hwp:id="ext-link-17">http://papers.nips.cc/paper/4824-imagenet-classification-w</ext-link>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2"><label>[14]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Krupinski E."><given-names>E.</given-names> <surname>Krupinski</surname></string-name>, <string-name name-style="western" hwp:sortable="Tillack A."><given-names>A.</given-names> <surname>Tillack</surname></string-name>, <string-name name-style="western" hwp:sortable="Richter L."><given-names>L.</given-names> <surname>Richter</surname></string-name>, <string-name name-style="western" hwp:sortable="Henderson J."><given-names>J.</given-names> <surname>Henderson</surname></string-name>, <string-name name-style="western" hwp:sortable="Bhattacharyya A."><given-names>A.</given-names> <surname>Bhattacharyya</surname></string-name>, <string-name name-style="western" hwp:sortable="Scott K."><given-names>K.</given-names> <surname>Scott</surname></string-name>, <string-name name-style="western" hwp:sortable="Graham A."><given-names>A.</given-names> <surname>Graham</surname></string-name>, <string-name name-style="western" hwp:sortable="Descour M."><given-names>M.</given-names> <surname>Descour</surname></string-name>, <string-name name-style="western" hwp:sortable="Davis J."><given-names>J.</given-names> <surname>Davis</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Weinstein R."><given-names>R.</given-names> <surname>Weinstein</surname></string-name>. <article-title hwp:id="article-title-10">Eye-movement study and human performance using telepathology virtual slides. Implications for medical education and differences with experience</article-title>. <source hwp:id="source-8">Human Pathology</source>, <volume>37</volume>(<issue>12</issue>):<fpage>1543</fpage>–<lpage>1556</lpage>, Dec. <year>2006</year>. ISSN <issn>00468177</issn>. doi: <pub-id pub-id-type="doi">10.1016/j. humpath.2006.08.024</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1016/j.humpath.2006.08.024" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.humpath.2006.08.024" hwp:id="ext-link-18">http://dx.doi.org/10.1016/j.humpath.2006.08.024</ext-link>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="book" citation-type="book" ref:id="097246v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Mercan E."><given-names>E.</given-names> <surname>Mercan</surname></string-name>, <string-name name-style="western" hwp:sortable="Aksoy S."><given-names>S.</given-names> <surname>Aksoy</surname></string-name>, <string-name name-style="western" hwp:sortable="Shapiro L."><given-names>L.</given-names> <surname>Shapiro</surname></string-name>, <string-name name-style="western" hwp:sortable="Weaver D."><given-names>D.</given-names> <surname>Weaver</surname></string-name>, <string-name name-style="western" hwp:sortable="Brunye T."><given-names>T.</given-names> <surname>Brunye</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Elmore J."><given-names>J.</given-names> <surname>Elmore</surname></string-name>. <chapter-title>Localization of Diagnostically Relevant Regions of Interest in Whole Slide Images</chapter-title>. pages <fpage>1179</fpage>–<lpage>1184</lpage>. <publisher-loc>IEEE</publisher-loc>, Aug. <year>2014</year>. ISBN <isbn>1051-4651</isbn>. doi: <pub-id pub-id-type="doi">10.1109/icpr.2014.212</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1109/icpr.2014.212" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/icpr.2014.212" hwp:id="ext-link-19">http://dx.doi.org/10.1109/icpr.2014.212</ext-link>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Parwani A."><given-names>A.</given-names> <surname>Parwani</surname></string-name>, <string-name name-style="western" hwp:sortable="Hassell L."><given-names>L.</given-names> <surname>Hassell</surname></string-name>, <string-name name-style="western" hwp:sortable="Glassy E."><given-names>E.</given-names> <surname>Glassy</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Pantanowitz L."><given-names>L.</given-names> <surname>Pantanowitz</surname></string-name>. <article-title hwp:id="article-title-11">Regulatory barriers surrounding the use of whole slide imaging in the United States of America</article-title>. <source hwp:id="source-9">Journal of pathology informatics</source>, <volume>5</volume>(<issue>1</issue>), <year>2014</year>. ISSN <issn>2229-5089</issn>. doi: <pub-id pub-id-type="doi">10.4103/2153-3539.143325</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.4103/2153-3539.143325" ext-link-type="uri" xlink:href="http://dx.doi.org/10.4103/2153-3539.143325" hwp:id="ext-link-20">http://dx.doi.org/10.4103/2153-3539.143325</ext-link>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Raghunath V."><given-names>V.</given-names> <surname>Raghunath</surname></string-name>, <string-name name-style="western" hwp:sortable="Braxton M."><given-names>M.</given-names> <surname>Braxton</surname></string-name>, <string-name name-style="western" hwp:sortable="Gagnon S."><given-names>S.</given-names> <surname>Gagnon</surname></string-name>, <string-name name-style="western" hwp:sortable="Brunye T."><given-names>T.</given-names> <surname>Brunye</surname></string-name>, <string-name name-style="western" hwp:sortable="Allison K."><given-names>K.</given-names> <surname>Allison</surname></string-name>, <string-name name-style="western" hwp:sortable="Reisch L."><given-names>L.</given-names> <surname>Reisch</surname></string-name>, <string-name name-style="western" hwp:sortable="Weaver D."><given-names>D.</given-names> <surname>Weaver</surname></string-name>, <string-name name-style="western" hwp:sortable="Elmore J."><given-names>J.</given-names> <surname>Elmore</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Shapiro L."><given-names>L.</given-names> <surname>Shapiro</surname></string-name>. <article-title hwp:id="article-title-12">Mouse cursor movement and eye tracking data as an indicator of pathologists’ attention when viewing digital whole slide images</article-title>. <source hwp:id="source-10">Journal of pathology informatics</source>, <volume>3</volume>, <year>2012</year>. ISSN <issn>2153-3539</issn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://view.ncbi.nlm.nih.gov/pubmed/23372984" ext-link-type="uri" xlink:href="http://view.ncbi.nlm.nih.gov/pubmed/23372984" hwp:id="ext-link-21">http://view.ncbi.nlm.nih.gov/pubmed/23372984</ext-link>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Randell R."><given-names>R.</given-names> <surname>Randell</surname></string-name>, <string-name name-style="western" hwp:sortable="Ambepitiya T."><given-names>T.</given-names> <surname>Ambepitiya</surname></string-name>, <string-name name-style="western" hwp:sortable="Mello-Thoms C."><given-names>C.</given-names> <surname>Mello-Thoms</surname></string-name>, <string-name name-style="western" hwp:sortable="Ruddle R."><given-names>R.</given-names> <surname>Ruddle</surname></string-name>, <string-name name-style="western" hwp:sortable="Brettle D."><given-names>D.</given-names> <surname>Brettle</surname></string-name>, <string-name name-style="western" hwp:sortable="Thomas R."><given-names>R.</given-names> <surname>Thomas</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Treanor D."><given-names>D.</given-names> <surname>Treanor</surname></string-name>. <article-title hwp:id="article-title-13">Effect of Display Resolution on Time to Diagnosis with Virtual Pathology Slides in a Systematic Search Task</article-title>. <source hwp:id="source-11">Journal of Digital Imaging</source>, <volume>28</volume>(<issue>1</issue>):<fpage>68</fpage>–<lpage>76</lpage>, <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1007/s10278-014-9726-8</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1007/s10278-014-9726-8" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10278-014-9726-8" hwp:id="ext-link-22">http://dx.doi.org/10.1007/s10278-014-9726-8</ext-link>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Schneider C."><given-names>C.</given-names> <surname>Schneider</surname></string-name>, <string-name name-style="western" hwp:sortable="Rasband W."><given-names>W.</given-names> <surname>Rasband</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Eliceiri K."><given-names>K.</given-names> <surname>Eliceiri</surname></string-name>. <article-title hwp:id="article-title-14">NIH Image to ImageJ: 25 years of image analysis</article-title>. <source hwp:id="source-12">Nature methods</source>, <volume>9</volume>(<issue>7</issue>):<fpage>671</fpage>–<lpage>675</lpage>, July <year>2012</year>. ISSN <issn>1548-7105</issn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://view.ncbi.nlm.nih.gov/pubmed/22930834" ext-link-type="uri" xlink:href="http://view.ncbi.nlm.nih.gov/pubmed/22930834" hwp:id="ext-link-23">http://view.ncbi.nlm.nih.gov/pubmed/22930834</ext-link>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Shupp L."><given-names>L.</given-names> <surname>Shupp</surname></string-name>, <string-name name-style="western" hwp:sortable="Ball R."><given-names>R.</given-names> <surname>Ball</surname></string-name>, <string-name name-style="western" hwp:sortable="Yost B."><given-names>B.</given-names> <surname>Yost</surname></string-name>, <string-name name-style="western" hwp:sortable="Booker J."><given-names>J.</given-names> <surname>Booker</surname>,</string-name> and <string-name name-style="western" hwp:sortable="North C."><given-names>C.</given-names> <surname>North</surname></string-name>. <article-title hwp:id="article-title-15">Evaluation of viewport size and curvature of large, high-resolution displays</article-title>. pages <fpage>123</fpage>–<lpage>130</lpage>. <source hwp:id="source-13">Canadian Information Processing Society</source>, <year>2006</year>. ISBN <isbn>1-56881-308-2</isbn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="https://dl.acm.org/citation.cfm?id=1143079.1143100" ext-link-type="uri" xlink:href="https://dl.acm.org/citation.cfm?id=1143079.1143100" hwp:id="ext-link-24">https://dl.acm.org/citation.cfm?id=1143079.1143100</ext-link>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Srivastava N."><given-names>N.</given-names> <surname>Srivastava</surname></string-name>, <string-name name-style="western" hwp:sortable="Hinton G."><given-names>G.</given-names> <surname>Hinton</surname></string-name>, <string-name name-style="western" hwp:sortable="Krizhevsky A."><given-names>A.</given-names> <surname>Krizhevsky</surname></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><given-names>I.</given-names> <surname>Sutskever</surname>,</string-name> and <string-name name-style="western" hwp:sortable="Salakhutdinov R."><given-names>R.</given-names> <surname>Salakhutdinov</surname></string-name>. <source hwp:id="source-14">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</source>. <volume>volume 15</volume>, pages <fpage>1929</fpage>–<lpage>1958</lpage>, June <year>2014</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>[22]</label><citation publication-type="journal" citation-type="journal" ref:id="097246v2.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Starkweather G."><given-names>G.</given-names> <surname>Starkweather</surname></string-name>. <article-title hwp:id="article-title-16">58.4: DSHARP — A Wide Screen Multi-Projector Display</article-title>. <source hwp:id="source-15">SID Symposium Digest of Technical Papers</source>, <volume>34</volume>(<issue>1</issue>):<fpage>1535</fpage>–<lpage>1537</lpage>, May <year>2003</year>. doi: <pub-id pub-id-type="doi">10.1889/1.1832577</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1889/1.1832577" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1889/1.1832577" hwp:id="ext-link-25">http://dx.doi.org/10.1889/1.1832577</ext-link>.</citation></ref></ref-list></back></article>
