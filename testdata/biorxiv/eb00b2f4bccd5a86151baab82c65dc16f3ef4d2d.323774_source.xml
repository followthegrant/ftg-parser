<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/323774</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;323774</article-id><article-id pub-id-type="other" hwp:sub-type="slug">323774</article-id><article-id pub-id-type="other" hwp:sub-type="tag">323774</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Speed of time-compressed forward replay flexibly changes in human episodic memory</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author e-mail: <email hwp:id="email-1">michelmann.seb@gmail.com</email></corresp><corresp id="cor2" hwp:id="corresp-2" hwp:rev-id="xref-corresp-2-1"><label>**</label>Corresponding author e-mail: <email hwp:id="email-2">s.hanslmayr@bham.ac.uk</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Michelmann Sebastian"><surname>Michelmann</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Staresina Bernhard P."><surname>Staresina</surname><given-names>Bernhard P.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Bowman Howard"><surname>Bowman</surname><given-names>Howard</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Hanslmayr Simon"><surname>Hanslmayr</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor2" hwp:id="xref-corresp-2-1" hwp:rel-id="corresp-2">**</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4"><label>1</label><institution hwp:id="institution-1">University of Birmingham, School of Psychology, Centre for Human Brain Health</institution>;</aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">University of Kent, School of Computing</institution></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-05-16T12:56:56-07:00">
    <day>16</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-05-16T12:56:56-07:00">
    <day>16</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-05-16T13:02:56-07:00">
    <day>16</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-05-16T13:02:56-07:00">
    <day>16</day><month>5</month><year>2018</year>
  </pub-date><elocation-id>323774</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-05-16"><day>16</day><month>5</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-05-16"><day>16</day><month>5</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-05-16"><day>16</day><month>5</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license hwp:id="license-1"><p hwp:id="p-1">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="323774.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/323774v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="323774.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/323774v1/323774v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/323774v1/323774v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Summary</title><p hwp:id="p-2">Remembering information from continuous past episodes is a complex task. On the one hand, we must be able to recall events in a highly accurate way that often includes exact timing; on the other hand, we can ignore irrelevant details and skip to events of interest. We here track continuous episodes, consisting of different sub-events, as they are recalled from memory. In behavioral and MEG data, we show that memory replay is temporally compressed and proceeds in a forward direction. Neural replay is characterized by the reinstatement of temporal patterns from encoding. These fragments of activity reappear on a compressed timescale. Herein, the replay of sub-events takes longer than the transition from one sub-event to another. This identifies episodic memory replay as a dynamic process in which participants replay fragments of fine-grained temporal patterns and are able to skip flexibly across sub-events.</p></abstract><counts><page-count count="37"/></counts></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-2">Introduction</title><p hwp:id="p-3">Episodic memory retrieval is a flexible process that operates at different timescales <sup><xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref></sup>. In some instances, it is crucial for our behavior to mentally replay events at the same speed as the initial experience: Re-enacting a classic movie scene relies on a temporally accurate representation of dialogue and events. In other instances, it would be highly dysfunctional to recall our memories at the same speed they originally unfolded: We have to be able to reconstruct how we came to work today without zoning out at our desk for thirty minutes and must therefore be able to flexibly adjust the speed of our memory replay.</p><p hwp:id="p-4">Previous work has already put forward that memory replay in humans could be forward and compressed: Studies that related the timescale between retrieval and perception of a particular event asked participants to mentally navigate routes based on their memories. The duration of memory replay (i.e. mental navigation) was found to be faster than the real navigation, but varied substantially between participants <sup><xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>,<xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref></sup>. Interestingly, findings of neural replay in rodents mirror this compression, by showing that hippocampal place cells, which correspond to certain positions along the animal’s path, later fire again on a faster timescale than during navigation. This is interpreted as reflecting compressed replay of past trajectories <sup><xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>,<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref></sup>. One recent study in humans observed the reactivation of static representations in electrocorticography (ECoG) and found patterns of oscillatory gamma power reappearing faster than during perception <sup><xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref></sup>. On the other hand, several studies find reappearing temporal patterns from perception in memory, demonstrating that some patterns are replayed at the same speed <sup><xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>–<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref></sup>. Notably, a recent study that applied functional magnetic resonance imaging (fMRI), managed to track continuous memory reinstatement over long episodes (50min). Spatial patterns reappeared during the free recall of narratives; recall was temporally compressed, but varied between participants <sup><xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref></sup>.</p><p hwp:id="p-5">Despite these indications about the speed of replay in behavioral and neural data, no study so far has tried to directly read out the temporal dynamics of memory replay in humans on a fine-grained temporal scale. Importantly, the recent advent of multivariate methods in neuroscience has now opened new avenues for the investigation of these processes: By leveraging multivariate patterns in combination with electrophysiology, it is now possible to track representations from perception in a time-resolved manner, as they reappear during memory retrieval <sup><xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref>–<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>,<xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>–<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref></sup>. Importantly, simultaneous EEG and multi-unit recordings in primates demonstrate an intimate relation between neural firing and the phase of slow oscillations in the EEG <sup><xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref></sup>. Therefore, information about neural patterns can be captured and tracked in human electrophysiology via oscillatory phase <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>,<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref>,<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref></sup>.</p><p hwp:id="p-6">Capitalizing on these methodological advances, we here investigate the flexible dynamics of episodic memory replay in continuous mnemonic representations. Studying these trajectories during memory replay requires a paradigm that prompts participants to evoke continuous representations with distinct subevents from memory. This will make it possible to track fragments of these representations in episodic memory via multivariate analysis methods. To this end, we asked subjects to associate static word-cues with ‘video-episodes’ consisting of a sequence of three distinct scenes. The three dynamic scenes thus formed a continuous six-second-long video. In encoding-trials, we presented a word-cue during one of the scenes. This allowed us to prompt memory replay in a natural way, i.e. we asked participants to recall in which of the three scene-positions they had learned an association during encoding. After completing this part of the task, we asked about the video-episode itself and confirmed memory accuracy. In a behavioral experiment, we investigated direction and speed of replay via measuring reaction times to the scene-position response. In a separate MEG study, we leveraged the content-specific phase patterns that each scene elicited and used them as handles to track the direction and speed of replay of the video-episodes. If memory replay were indeed compressed, we expected to find evidence for this compression in reaction times and in the reinstatement of neural patterns. This replay could either be forward or backward. In line with previous findings, we expected to find evidence for reactivation of temporal patterns, signifying replay at the same speed for fragments of neural activity <sup><xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>–<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref></sup>. We further hypothesized that the disparity between accurate representations and overall compression would be due to a flexible mechanism that allows subjects to skip between sub-events, as they replay episodes. Therefore, replay within sub-events should occur at a slower pace, whereas skipping between sub-events should occur fast.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-3">Results</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-4">Compressed and forward memory-replay in reaction times</title><p hwp:id="p-7">In the behavioral experiment, participants associated word-cues with one of three scenes within video-episodes (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1a</xref>). We used four continuous video-episodes, each consisting of three individual dynamic scenes. A trial-unique word-cue appeared in one scene during a video-episode. After a brief distractor task (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1b</xref>) subjects performed, in alternation, either a cued-recall (CR) retrieval task or an associative-recognition (AR) task (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1d</xref>, top). The AR task was included as a control condition, because active replay is arguably not required for recognition. In the CR blocks, we presented participants with the word-cues (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1d</xref>, top-left). Their task was to recall the scene-position that was associated with the word-cue as quickly as possible. In AR blocks, subjects successively saw the word-cues superimposed on screenshots from encoding and were asked to decide as quickly as possible whether this association was intact or rearranged (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1d</xref>, top-right).</p><p hwp:id="p-8">To address the direction and speed of memory replay, reaction times (RTs) at retrieval were compared between associations that were learned in the first, second and third scene-position of a video-episode (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1d</xref>, bottom). We only used RTs for correct hit trials (correct recall in CR and correctly recognized intact associations in AR blocks) and excluded trials in which the subjects were wrong or guessed (see Supplemental Information for the same analysis including correct guesses). If the CR condition indeed elicited replay in the forward direction, we should observe faster reaction times with CR, but not AR, for associations that were learned earlier during encoding. Furthermore, if replay was compressed, the delay between reaction times to different scene-positions should be smaller than the duration of the scenes segments themselves. The resulting 3x2 repeated measures ANOVA tested the factors position and condition. A significant main effect of position (<italic toggle="yes">F</italic><sub>1.85, 42.48</sub> = 5.884, <italic toggle="yes">p</italic> = 0.007, log-RT: <italic toggle="yes">F</italic><sub>1.79, 41.26</sub> = 3.375, <italic toggle="yes">p</italic> = 0.049) and a position by condition interaction (<italic toggle="yes">F</italic><sub>1.75, 40.34</sub>5.9, <italic toggle="yes">p</italic> = 0.008, log-RT: <italic toggle="yes">F</italic><sub>1.76, 40.58</sub> = 5.606, <italic toggle="yes">p</italic> = 0.009) were obtained. Both effects were driven by the cued-recall condition (ANOVA position only: <italic toggle="yes">F</italic><sub>1.79, 41.19</sub> = 9.082, <italic toggle="yes">p</italic> = 0.001, log-RT: <italic toggle="yes">F</italic><sub>1.60, 36.90</sub> = 8.207, <italic toggle="yes">p</italic> = 0.002): During encoding, individual scenes of each video-episode lasted 2 seconds. During CR retrieval, however, associations that were learned in the first scene-position of a video-episode (mean RT = 2.5s) were recalled on average 116ms faster than associations that were learned in the second scene-position (<italic toggle="yes">t</italic><sub>23</sub> = −1.870, <italic toggle="yes">p</italic> = 0.037, log-RT: <italic toggle="yes">t</italic><sub>23</sub> = −2.4, <italic toggle="yes">p</italic> = 0.012). Associations that were learned in the second scene-position (mean RT = 2.617s) were recalled on average 176ms faster than associations that were learned in the third scene-position (<italic toggle="yes">t</italic><sub>23</sub> = −2.767, <italic toggle="yes">p</italic> = 0.006, log-RT: <italic toggle="yes">t</italic><sub>23</sub> = −2.274, <italic toggle="yes">p</italic> = 0.016, (mean RT = 2.793s)). The replay of the video-episodes was therefore forward and compressed during CR, which replicated our findings from a behavioral pilot experiment (see Supplemental Information). The average RT difference of 146ms per position corresponds to a compression factor of 13.7 during replay.</p><p hwp:id="p-9">Might the effects be due to asymmetrical encoding of scene-positions? One could argue that associations have a higher saliency when they are presented in the first scene-position, leading to higher confidence and shorter RTs during retrieval. Additionally, subjects can take more time to rehearse early associations during the remainder of the video-episode, perhaps resulting in the weakest memory trace for the last scene. Importantly, however, if the serial position merely affects the overall strength of the memory trace in our paradigm, we should observe comparable effects on cued recall (CR) and associative recognition (AR). Conversely, if the effect is contingent on the need to mentally replay scene after scene, serial position at encoding should only exert an effect on the CR task.</p><p hwp:id="p-10">Importantly, no differences in reaction times between scene-positions were evident in the AR task (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1d</xref>, right; ANOVA: <italic toggle="yes">F</italic><sub>1.64, 37.66</sub> = 0.708, <italic toggle="yes">p</italic> = 0.472, log-RT: <italic toggle="yes">F</italic><sub>1.61,</sub> <sub>36.95</sub> = 0.793, <italic toggle="yes">p</italic> = 0.435, pairwise comparisons of positions: all <italic toggle="yes">p</italic>s &gt; 0.199, all <italic toggle="yes">BF</italic><sub>01</sub> &gt; 2.158). Together with the significant position by condition interaction, this confirms that the position effect on RTs is specific to the CR task and rules out a saliency-based explanation. Finally, we observed a significant main effect of condition with unscaled (<italic toggle="yes">F</italic><sub>1.00, 23.00</sub> = 62.349, <italic toggle="yes">p</italic> &lt; 0.001) and log-transformed (<italic toggle="yes">F</italic><sub>1.00, 23.00</sub> = 95.036, <italic toggle="yes">p</italic> &lt; 0.001) reaction times. This was due to faster RTs in associative-recognition blocks (<italic toggle="yes">t</italic><sub>23</sub> = −7.896, <italic toggle="yes">p</italic> &lt; 0.001, log-RT: <italic toggle="yes">t</italic><sub>23</sub> = − 9.7487, <italic toggle="yes">p</italic> &lt; 0.001). Taken together these results are evidence that successful recall of elements from a continuous video-episode relies on compressed forward replay.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11 xref-fig-1-12 xref-fig-1-13 xref-fig-1-14 xref-fig-1-15 xref-fig-1-16 xref-fig-1-17 xref-fig-1-18 xref-fig-1-19 xref-fig-1-20 xref-fig-1-21 xref-fig-1-22 xref-fig-1-23"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Fig. 1</label><caption hwp:id="caption-1"><title hwp:id="title-5">Experimental design and behavioral results</title><p hwp:id="p-11"><bold>(a)</bold> During encoding subjects repeatedly saw one out of four video-episodes. In one of three scenes that comprise a video-episode, a word-cue appeared in the center of the screen. (<bold>b)</bold> In the distractor block participants identified either the bigger or the smaller one of 2 simple sums. (<bold>c)</bold> In the MEG experiment, participants saw the static word-cue during retrieval for 3.5 seconds, followed by a fixation cross for 250ms - 750ms. Subsequently they first picked the scene-position in which they learned the association and then confirmed the correct video-episode. <bold>(d)</bold> In the cued-recall (CR) condition of the behavioral experiment (left) participants selected the correct scene position as quickly as possible during retrieval. In an associative-recognition (AR) control condition (right) they decided whether the presented association (word superimposed on a screenshot) was intact or rearranged. In CR blocks, subjects were faster to recall an association that was learned in earlier scene-positions during encoding (bottom left). Importantly, in the control condition, they performed the same encoding task and needed source memory for AR retrieval, however no modulation of reaction times was found. The y-axis denotes the difference to each participant’s average reaction time in the respective condition. Spaghetti-plots show individual subjects. Boxplots are 25<sup>th</sup> and 75<sup>th</sup> percentile and the median; whiskers are maxima and minima, excluding outliers. Red dots within the boxplots depict the arithmetic mean. Significant differences are marked with a star, n.s. denotes non-significant in a post-hoc paired t-test comparison.</p></caption><graphic xlink:href="323774_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Low frequency phase patterns from encoding reappear during successful memory retrieval</title><p hwp:id="p-12">In the MEG experiment, participants performed the same CR task as in the behavioral experiment, with the only difference being that they gave responses after the word-cue disappeared (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig. 1c</xref>). In a first step, we asked whether perceptual content could be distinguished based on oscillatory phase. To this end, we compared the inter-trial phase coherence (ITPC) between encoding-trials grouped according to their video-content against the ITPC between trials grouped randomly. This has been used previously to reveal the content specific entrainment of cortical rhythms to naturalistic dynamic stimuli <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">8</xref>,<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-3" hwp:rel-id="ref-16">16</xref></sup>. The four video-episodes showed reliably distinguishable phase patterns during encoding (<italic toggle="yes">p</italic><sub>cluster</sub> &lt; 0.001, <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2a</xref>, left and middle). The significant cluster (across time space and sensors) contained robust differences in the lower frequencies and showed a maximum over occipito-parietal sensors (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2a</xref>, middle). Consistent with our previous results <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-4" hwp:rel-id="ref-8">8</xref></sup>, strongest differences were observed at the onset of each scene. Importantly, the frequency band centered at 8 Hz was included in the cluster, which was previously linked to the reinstatement of phase patterns <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-5" hwp:rel-id="ref-8">8</xref></sup>. Testing the 8 Hz phase differences on the source level revealed one broad cluster of content specificity during encoding (<italic toggle="yes">p</italic><sub>cluster</sub> &lt; 0.001). Averaging t-values across this significant cluster over time revealed highest values in occipital and parietal locations (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2a</xref> right). Together, these results show that every sub-scene within a video-episode was associated with a content specific fingerprint in oscillatory phase, which was maximal in a parieto-occipital region. In the following, we used these sub-scene specific phase patterns at the center frequency of 8 Hz as handles to track replay in memory.</p><p hwp:id="p-13">In a first step, we tested whether these phase-patterns of the video-episodes were reactivated in memory. Therefore, we first contrasted phase-similarity between encoding-retrieval combinations of the same video-episodes (e.g. watching video A, recalling video A) with encoding-retrieval combinations of different video-episodes (e.g. watching video A, recalling video B). Similarity between encoding and retrieval phase patterns was analyzed with a sliding-window approach (window size = 1 sec), providing a time resolved measure of memory replay <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-6" hwp:rel-id="ref-8">8</xref>,<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>,<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref></sup> (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2c</xref>). On the source level, analysis was restricted to an anatomically defined occipito-parietal region of interest (ROI) following the results from the encoding phase and previous studies showing memory replay in these regions<sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-7" hwp:rel-id="ref-8">8</xref>,<xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>–<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref></sup> (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig 2b</xref>). Evidence for replay was found for hit trials (Hits; <italic toggle="yes">p<sub>cluster</sub></italic> = 0.034; <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Supplemental Fig 3a</xref>, also see <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Supplemental Fig 3b</xref> for unmasked maps of t-values), suggesting that replay of video-episodes can be tracked in the phase of an 8Hz oscillation. Notably, we found no such replay effect for Misses, i.e. trials in which subjects either guessed, or did not remember the correct scene-position and/or video-episode. Furthermore, a direct contrast between Hits and Misses revealed significantly stronger replay for Hits compared to Misses (<italic toggle="yes">p</italic><sub><italic toggle="yes">cluster</italic></sub> = 0.030, <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig. 2d</xref>), demonstrating the functional significance of this pattern-reinstatement for memory.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Fig. 2:</label><caption hwp:id="caption-2"><title hwp:id="title-7">Reinstatement of oscillatory patterns from encoding</title><p hwp:id="p-14"><bold>(a)</bold> During encoding, the different video-episodes elicited content specific phase patterns. The left panel shows the averaged t-values across sensors in the cluster of significant content-specificity. Topographies in the middle are t-values within the same cluster (across time, frequency and space), averaged across time and across all frequencies (top) or only for 8 Hz (bottom). Both topographies show maximal values over occipital and parietal sensors. The right panel shows the average t-values across time on virtual sensors, within the temporo-spatial cluster of significant differences at 8Hz. Occipital and parietal sensors expressed the maximal t-values. <bold>(b)</bold> Occipito-parietal region of interest (ROI) that we used for statistical testing of content-specific reactivation. <bold>(c)</bold> Time course of content specific phase during encoding, averaged across the ROI. Below, the sliding window approach is illustrated, in which all possible time windows from encoding were compared to each retrieval time window via phase coherence. Subsequently, combinations of same and different content combinations were contrasted. <bold>(d)</bold> Cluster of significant differences between content-specific reactivation for successfully remembered and forgotten associations.</p></caption><graphic xlink:href="323774_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-8">Compressed replay is forward</title><p hwp:id="p-15">The above findings confirm that content specific patterns of activity from encoding, are reinstated in a purely memory driven way. This motivated us to ask in which direction and at what relative speed patterns from encoding unfold during retrieval. Do patterns from the beginning of the video-episodes, for instance, also reappear earlier during memory retrieval?</p><p hwp:id="p-16">To this end, we divided the encoding interval into 6 non-overlapping windows, centered at 0.5, 1.5, 2.5, 3.5, 4.5, and 5.5 seconds. We then analyzed the phase-similarity to these windows across the retrieval interval. The latency at which patterns reappear should be reflected in the distribution of phase-similarity across time. Consequently, we compared these distributions between the distinct time windows from encoding (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3a</xref>).</p><p hwp:id="p-17">Specifically, to test the direction of replay statistically across subjects, we used the following approach: We cumulated the similarity distributions across the whole retrieval time. This provided the cumulated similarity (CS) for every subject and every encoding-window. Similarity started at the beginning of the retrieval interval with a value of zero. It ended at the end of the retrieval interval, with a value of one (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig 3c</xref>). If phase-similarity to an encoding-window “A” cumulates earlier than phase-similarity to an encoding-window “B”, then the cumulated similarity for “A” is higher compared to “B” and consequently “A” is replayed earlier during retrieval than “B”. In other words, when the CS of one phase-pattern is higher than the CS of another, then the evidence for replay of that phase-pattern is leading over the other at that point. If, however replay of a phase-pattern is lagging behind the replay of another, the CS should be lower at that time point. We tested this relation statistically at every time point by comparing the cumulated similarity across all windows for each subject. The overall tendency is tested best by fitting a line across all six encoding windows. Herein, a negative slope indicates forward replay, since earlier windows have higher values in the CS than later windows, a positive slope signifies backward replay.</p><p hwp:id="p-18">Results revealed significant forward replay in two time windows (i.e. 135ms to 1919ms, and 3458ms to 3473ms after cue presentation, see Online Methods for some notes of caution regarding the interpretation of the exact time-window). We can therefore conclude that there is a dominance of early encoding-patterns in early time points at retrieval, relative to late encoding-patterns. This supports the notion of forward replay (see also Supplementary Information for additional evidence supporting forward replay) and corroborates the finding of forward replay from the behavioral experiment.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9 xref-fig-3-10 xref-fig-3-11 xref-fig-3-12 xref-fig-3-13 xref-fig-3-14"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Fig. 3:</label><caption hwp:id="caption-3"><title hwp:id="title-9">Chronometry of memory replay</title><p hwp:id="p-19"><bold>(a)</bold> The 6 non-overlapping time windows from encoding illustrated next to a video-episode (left). The average similarity densities to these windows are on the right. The blue bar denotes where replay was significantly slower within scenes (see e). <bold>(b)</bold> Cross correlations of similarity densities within this window show the adaptive pattern. The matrix shows the combination of windows that are correlated in each cell. The times in ms at which cross correlation is biggest are displayed in the color-coded cells. In this, lags between windows within scenes are bigger than lags between windows across scenes (right, top); with strict forward replay, all scenes would be replayed in order (right, bottom). <bold>(c)</bold> Illustration of the cumulative similarity (CS) approach used to test replay-dynamics. If evidence for a window statistically precedes evidence for another during retrieval, its cumulated similarity is higher. Fitting a line through those subsequent encoding windows will therefore result in a negative slope. <bold>(d)</bold> Average slope of lines fit across all windows’ CS, for each subject and time point. Negative slope indicates that earlier encoding-windows have higher CS values and signify forward replay. <bold>(e)</bold> Contrast of average slopes from the average fit across windows within scenes and a fit across all windows, supporting an adaptive replay framework. The blue bars in d and e denote significance.</p></caption><graphic xlink:href="323774_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-10">Speed of replay is flexible</title><p hwp:id="p-20">In neural data, the forward direction of replay was evidenced by the tracking of content specific temporal patterns. Notably, however the reactivation of temporal patterns signifies that participants replayed <italic toggle="yes">fragments</italic> of the video-episodes at roughly the same speed as during encoding. Hence, these data already indicate that memory replay is not the straightforward recapitulation of the original experience. Instead, flexible processes must be at work to reconcile the overall compression of memory with the reappearance of temporal patterns.</p><p hwp:id="p-21">We hypothesized that the disparity between locally detailed patterns and the global compression was possible through the flexible skipping between salient components in memory (e.g. sub-events); in our data, the boundaries between scenes were salient elements within the video-episodes. We therefore investigated whether these boundaries would serve as stepping stones enabling participants to skip through their memories on a faster time-scale. Consequently, we tested statistically whether the speed of replay slowed down within scenes, since the skipping between the scenes of a video-episode should be easier and more likely than skipping within the individual scenes.</p><p hwp:id="p-22">To this end, we extended the method of fitting a line across CSs to compare the compression of replay within individual scenes (i.e. within sub-events) to the overall compression level. Specifically, calculating the slope of the fitted line allows for an estimation of the speed of replay. This slope indicates the lag between replayed patterns in the retrieval interval, such that steep slopes indicate a long lag (i.e. slow replay). We fitted a separate line for each pair of encoding-windows that belonged to the same scene across their respective CSs and averaged the slopes across the three lines. The time interval between 442ms and 2350ms displayed slopes significantly below zero, confirming forward replay within scenes. More importantly, between 550ms and 2350ms at retrieval, slopes of windows within a scene were significantly steeper (i.e. replay was slower) compared to the slope obtained across all encoding-windows (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3e</xref>). This means that when participants replayed the first and second part of a scene, this replay was less compressed than we expected from the global compression level of the whole video-episode. Consequently, this also means that subjects did not recapitulate every scene successively in every trial. Taken together, these results show that memory replay does not occur at a constant speed; instead, the speed of replay seems to change flexibly depending on the replayed interval (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3b</xref>, right). Finally, we repeated these tests with those trials in which subjects did not remember the correct positional-scene or video-episode; however, we found no significant time-points for any of the contrasts, which demonstrates the implication of these replay effects in memory. In a further control analysis, we excluded the first 800ms of the retrieval interval for the similarity analysis in order to rule out that event related potentials (ERPs) drove similarities. Again, we found significant negative slopes between 812ms and 1212ms and slower replay within scenes in that window.</p><p hwp:id="p-23">These results statistically support a flexible forward replay strategy. Via cross-correlations, we next derived a descriptive measure of the delay between the six sub-events during flexible memory replay (550ms–2350ms). The cross-correlation was computed on pairs of averaged and smoothed similarity distributions (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3b</xref>), which retained a time lag value for every combination of the six sub-events. The adaptive replay that we found is also visible in the pattern of time lags and can be illustrated with shorter lags between time windows that belong to different scenes compared to time windows that belong to the same scene (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 3B</xref>, right). In contrast, to illustrate a strict and inflexible forward replay strategy, lags between the sub-events should increase linearly according to their position at encoding (illustrated in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3B</xref>, right).</p></sec></sec><sec id="s3" hwp:id="sec-7"><title hwp:id="title-11">Discussion</title><p hwp:id="p-24">In this study, we tracked the replay of continuous episodes from memory. We used a novel paradigm in which participants associated unique word-cues with one out of three distinct scenes in seamless video-episodes. We prompted replay by asking volunteers in which exact position (1, 2, or 3) they had learned each word-cue. Behavioral and neural data indicated that replay of memories takes place in a forward direction and at a compressed speed, i.e. memory replay was faster relative to perception. Notably, on a neural level, we found indications for different speeds of replay: Fragments of temporal patterns reappeared at the same speed and the speed of replay within sub-events (i.e. scenes) of continuous video-episodes was slower than the overall compression level.</p><p hwp:id="p-25">Importantly, our finding of different compression levels implies that memory replay acts in a flexible way. The disparity between the slower speed of replay within scenes and the overall compression is an aggregated observation that cannot hold on a single trial level. Specifically, it signifies that replay is not a simple concatenation of fragments because in a single trial, the sequential replay of three scenes would take longer than the overall compression permits. Consequently, participants must be able to skip between replayed fragments; importantly the slower speed of replay within scenes denotes that on average, the skipping between sub-events must take place on a faster temporal scale than the skipping within sub-events. A plausible interpretation of the observed pattern is therefore that replay of relevant information is initiated from the boundaries between scenes and that participants can flexibly skip between them. Event boundaries <sup><xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref></sup> have been previously shown to trigger replay events during memory encoding <sup><xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref></sup>. They could therefore also serve as starting points during memory retrieval, to initiate the replay of information on a fine-grained temporal scale. Mechanistically, the hippocampus has been suggested to preserve the temporal order of experiences <sup><xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref></sup> and interactions between the hippocampus and visual cortex have been observed during memory replay in sleeping rodents <sup><xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref></sup>. In our data, we consistently found reinstatement of fine-grained temporal patterns in sensory-specific regions <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-8" hwp:rel-id="ref-8">8</xref></sup>. It is therefore possible that the hippocampus exerts control over sensory areas, when those regions execute the vivid reinstatement of sensory information. Specifically, information-rich and temporally accurate representations could rely on sensory cortices whereas the hippocampus initiates replay, based on a sparse code <sup><xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref></sup>. At first glance, the reinstatement of temporal patterns is also at odds with the observation of compression in general. An important implication from the finding of temporal pattern reinstatement under global compression is therefore that the accurate reinstatement of patterns must be limited to fragments of the original perception. In other words, subjects possibly omit non-informative (perhaps redundant) parts of the video-episodes and therefore replay a shorter episode in memory, which contains less information. Previous work on mental simulation of paths supports this interpretation. The duration that participants take to mentally simulate a path increases, when this path includes more turns <sup><xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">3</xref></sup>. In the same way, the duration of replay might depend on the overall number of relevant elements within a video-episode.</p><p hwp:id="p-26">Another crucial result from our experiments is the forward direction of replay. This finding is in line with recent studies showing anticipatory activation of familiar paths in the visual cortex <sup><xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref></sup> and evidence of forward replay of long narratives <sup><xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref></sup>. Notably, in the rodent literature, the task of spatial navigation appears to determine whether replay is backward or forward. At the end of a path, awake rodents replay in a backward fashion <sup><xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">4</xref></sup>, whereas animals that plan the path towards a goal display an anticipatory activation of place-cells in the forward direction <sup><xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref></sup>. Task requirements in our design could indeed have prompted participants to step mentally through the video-episodes in a forward manner. Speculatively, other designs (e.g. tasks requiring recency judgments) might therefore cause a backwards replay. This would be well in line with the flexibility in memory replay that we observed in the neural data, since a flexible mechanism could arguably guide replay in a forward and backward direction when skipping through events. An interesting additional question arising from this is whether replay of fine-grained temporal patterns in the cortex can also be backwards.</p><p hwp:id="p-27">Importantly our study also demonstrates how one can investigate these open questions. The design that we used to trigger the replay of distinct sub-events in a continuous episode can easily be adapted to a working memory context and our method to track oscillatory patterns allows for the investigation of replay in working memory, during rest and during sleep. We have repeatedly shown how to use the similarity in oscillatory phase to track content-specific reactivation, even when the exact onset of memory-reactivation is unknown. We here extended our previously developed method <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-9" hwp:rel-id="ref-8">8</xref></sup> to track distinct sub-events from continuous representations: In a statistically robust way we aggregated evidence across several repetitions and compared their distribution across time.</p><p hwp:id="p-28">This investigation of temporal dynamics during human episodic memory replay has only recently become an option, when the tracking of multivariate patterns was extended to human electrophysiology <sup><xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">7</xref>,<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-10" hwp:rel-id="ref-8">8</xref>,<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-3" hwp:rel-id="ref-10">10</xref>,<xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>,<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>,<xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>,<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref></sup>. Leveraging a novel paradigm in combination with a method that can detect the individual fingerprints in oscillatory patterns, we were now able to observe the fine-grained dynamics of memory replay on a behavioral and on a neural basis. Our data render memory replay as a flexible process, namely the compression level varies within replayed episodes: Some fragments reappear on a timescale that resembles the original perception and replay is less compressed within sub-events of continuous episodes, which suggests that participants were able to flexibly skip between sub-events during memory replay.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-12">Acknowledgements</title><p hwp:id="p-29">The authors would like to thank the Sir Peter Mansfield Imaging Centre (SPMIC), specifically Dr. George O’Neill, Dr. Benjamin A.E Hunt and Dr. Lauren Gascoyne for their help with data collection.</p></ack><sec hwp:id="sec-8"><title hwp:id="title-13">Author Contributions</title><p hwp:id="p-30">Conceived and designed the experiments: SM BPS SH.</p><p hwp:id="p-31">Performed the experiments: SM.</p><p hwp:id="p-32">Analyzed the data: SM under supervision of SH.</p><p hwp:id="p-33">Contributed reagents/materials/analysis tools: SM SH HB.</p><p hwp:id="p-34">Wrote the paper: SM SH commented and edited by BPS HB.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-14">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Tulving E."><surname>Tulving</surname>, <given-names>E.</given-names></string-name> <article-title hwp:id="article-title-2">What is episodic memory?</article-title> <source hwp:id="source-1">Curr. Dir. Psychol. Sci.</source> <volume>2</volume>, <fpage>67</fpage>–<lpage>70</lpage> (<year>1993</year>).</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Arnold A. E. G. F."><surname>Arnold</surname>, <given-names>A. E. G. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iaria G."><surname>Iaria</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Ekstrom A. D."><surname>Ekstrom</surname>, <given-names>A. D.</given-names></string-name> <article-title hwp:id="article-title-3">Mental simulation of routes during navigation involves adaptive temporal compression</article-title>. <source hwp:id="source-2">Cognition</source> <volume>157</volume>, <fpage>14</fpage>–<lpage>23</lpage> (<year>2016</year>).</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bonasia K."><surname>Bonasia</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blommesteyn J."><surname>Blommesteyn</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Moscovitch M."><surname>Moscovitch</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-4">Memory and navigation: Compression of space varies with route length and turns</article-title>. <source hwp:id="source-3">Hippocampus</source> <volume>26</volume>, <fpage>9</fpage>–<lpage>12</lpage> (<year>2016</year>).</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Foster D. J."><surname>Foster</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wilson M. A."><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> <article-title hwp:id="article-title-5">Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title>. <source hwp:id="source-4">Nature</source> <volume>440</volume>, <fpage>680</fpage>–<lpage>683</lpage> (<year>2006</year>).</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Carr M. F."><surname>Carr</surname>, <given-names>M. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jadhav S. P."><surname>Jadhav</surname>, <given-names>S. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Frank L. M."><surname>Frank</surname>, <given-names>L. M.</given-names></string-name> <article-title hwp:id="article-title-6">Hippocampal replay in the awake state: A potential substrate for memory consolidation and retrieval</article-title>. in <source hwp:id="source-5">Nature Neuroscience</source> <volume>14</volume>, <fpage>147</fpage>–<lpage>153</lpage> (<year>2011</year>).</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Yaffe R. B."><surname>Yaffe</surname>, <given-names>R. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shaikhouni A."><surname>Shaikhouni</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arai J."><surname>Arai</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Inati S. K."><surname>Inati</surname>, <given-names>S. K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zaghloul K. A."><surname>Zaghloul</surname>, <given-names>K. A.</given-names></string-name> <article-title hwp:id="article-title-7">Cued Memory Retrieval Exhibits Reinstatement of High Gamma Power on a Faster Timescale in the Left Temporal Lobe and Prefrontal Cortex</article-title>. <source hwp:id="source-6">J. Neurosci.</source> <volume>37</volume>, <fpage>4472</fpage>–<lpage>4480</lpage> (<year>2017</year>).</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Staudigl T."><surname>Staudigl</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vollmar C."><surname>Vollmar</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Noachtar S."><surname>Noachtar</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hanslmayr S."><surname>Hanslmayr</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-8">Temporal-Pattern Similarity Analysis Reveals the Beneficial and Detrimental Effects of Context Reinstatement on Human Memory</article-title>. <source hwp:id="source-7">J. Neurosci.</source> <volume>35</volume>, <fpage>5373</fpage>–<lpage>5384</lpage> (<year>2015</year>).</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3 xref-ref-8-4 xref-ref-8-5 xref-ref-8-6 xref-ref-8-7 xref-ref-8-8 xref-ref-8-9 xref-ref-8-10 xref-ref-8-11 xref-ref-8-12 xref-ref-8-13 xref-ref-8-14"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Michelmann S."><surname>Michelmann</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bowman H."><surname>Bowman</surname>, <given-names>H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hanslmayr S."><surname>Hanslmayr</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-9">The Temporal Signature of Memories: Identification of a General Mechanism for Dynamic Memory Replay in Humans</article-title>. <source hwp:id="source-8">PLoS Biol.</source> <volume>14</volume>, (<year>2016</year>).</citation></ref><ref id="c9" hwp:id="ref-9"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Zhang H."><surname>Zhang</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-10">Gamma Power Reductions Accompany Stimulus-Specific Representations of Dynamic Events</article-title>. <source hwp:id="source-9">Curr. Biol.</source> <volume>25</volume>, <fpage>635</fpage>–<lpage>640</lpage> (<year>2015</year>).</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2 xref-ref-10-3"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Wimber M."><surname>Wimber</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maaß A."><surname>Maaß</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Staudigl T."><surname>Staudigl</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Richardson-Klavehn A."><surname>Richardson-Klavehn</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hanslmayr S."><surname>Hanslmayr</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-11">Rapid memory reactivation revealed by oscillatory entrainment</article-title>. <source hwp:id="source-10">Curr. Biol.</source> <volume>22</volume>, <fpage>1482</fpage>–<lpage>6</lpage> (<year>2012</year>).</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Chen J."><surname>Chen</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-12">Shared memories reveal shared structure in neural activity across individuals</article-title>. <source hwp:id="source-11">Nat. Neurosci.</source> <volume>20</volume>, <fpage>115</fpage>–<lpage>125</lpage> (<year>2017</year>).</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-13">Representational similarity analysis – connecting the branches of systems neuroscience</article-title>. <source hwp:id="source-12">Front. Syst. Neurosci.</source> <volume>2</volume>, <fpage>1</fpage>–<lpage>28</lpage> (<year>2008</year>).</citation></ref><ref id="c13" hwp:id="ref-13"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Staresina B. P."><surname>Staresina</surname>, <given-names>B. P.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-14">Hippocampal pattern completion is linked to gamma power increases and alpha power decreases during recollection</article-title>. <source hwp:id="source-13">Elife</source> <volume>5</volume>, (<year>2016</year>).</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Yaffe R. B."><surname>Yaffe</surname>, <given-names>R. B.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-15">Reinstatement of distributed cortical oscillations occurs with precise spatiotemporal dynamics during successful memory retrieval</article-title>. <source hwp:id="source-14">Proc. Natl. Acad. Sci</source>. <volume>111</volume>, <fpage>18727</fpage>–<lpage>18732</lpage> (<year>2014</year>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Kurth-Nelson Z."><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barnes G."><surname>Barnes</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sejdinovic D."><surname>Sejdinovic</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan R."><surname>Dolan</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-16">Temporal structure in associative retrieval</article-title>. <source hwp:id="source-15">Elife</source> <volume>4</volume>, <fpage>e04919</fpage> (<year>2015</year>).</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2 xref-ref-16-3 xref-ref-16-4"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Ng B. S. W."><surname>Ng</surname>, <given-names>B. S. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Logothetis N. K."><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kayser C."><surname>Kayser</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-17">EEG Phase Patterns Reflect the Selectivity of Neural Firing</article-title>. <source hwp:id="source-16">Cereb. Cortex</source> <volume>23</volume>, <fpage>389</fpage>–<lpage>398</lpage> (<year>2013</year>).</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Schyns P. G."><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thut G."><surname>Thut</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Gross J."><surname>Gross</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-18">Cracking the Code of Oscillatory Activity</article-title>. <source hwp:id="source-17">PLoS Biol.</source> <volume>9</volume>, <fpage>e1001064</fpage> (<year>2011</year>).</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Lachaux J.-P."><surname>Lachaux</surname>, <given-names>J.-P.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-19">Studying single-trials of phase-synchronous activity in the brain</article-title>. <source hwp:id="source-18">Int. J. Bifurc. Chaos</source> <volume>10</volume>, <fpage>2429</fpage>–<lpage>2439</lpage> (<year>2000</year>).</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Mormann F."><surname>Mormann</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lehnertz K."><surname>Lehnertz</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="David P."><surname>David</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="E. Elger C."><surname>E. Elger</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-20">Mean phase coherence as a measure for phase synchronization and its application to the EEG of epilepsy patients</article-title>. <source hwp:id="source-19">Phys. D Nonlinear Phenom.</source> <volume>144</volume>, <fpage>358</fpage>–<lpage>369</lpage> (<year>2000</year>).</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Ji D."><surname>Ji</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wilson M. A."><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> <article-title hwp:id="article-title-21">Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title>. <source hwp:id="source-20">Nat. Neurosci.</source> <volume>10</volume>, <fpage>100</fpage>–<lpage>107</lpage> (<year>2007</year>).</citation></ref><ref id="c21" hwp:id="ref-21"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Albers A. M."><surname>Albers</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kok P."><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Toni I."><surname>Toni</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dijkerman H. C."><surname>Dijkerman</surname>, <given-names>H. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="De Lange F. P."><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name> <article-title hwp:id="article-title-22">Shared representations for working memory and mental imagery in early visual cortex</article-title>. <source hwp:id="source-21">Curr. Biol.</source> <volume>23</volume>, <fpage>1427</fpage>–<lpage>1431</lpage> (<year>2013</year>).</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Ekman M."><surname>Ekman</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kok P."><surname>Kok</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="de Lange F. P."><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> <article-title hwp:id="article-title-23">Time-compressed preplay of anticipated events in human primary visual cortex</article-title>. <source hwp:id="source-22">Nat. Commun.</source> <volume>8</volume>, <fpage>15276</fpage> (<year>2017</year>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Bosch S. E."><surname>Bosch</surname>, <given-names>S. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jehee J. F. M."><surname>Jehee</surname>, <given-names>J. F. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fernandez G."><surname>Fernandez</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Doeller C. F."><surname>Doeller</surname>, <given-names>C. F.</given-names></string-name> <article-title hwp:id="article-title-24">Reinstatement of Associative Memories in Early Visual Cortex Is Signaled by the Hippocampus</article-title>. <source hwp:id="source-23">J. Neurosci.</source> <volume>34</volume>, <fpage>7493</fpage>–<lpage>7500</lpage> (<year>2014</year>).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Radvansky G. A."><surname>Radvansky</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zacks J. M."><surname>Zacks</surname>, <given-names>J. M.</given-names></string-name> <article-title hwp:id="article-title-25">Event boundaries in memory and cognition</article-title>. <source hwp:id="source-24">Curr. Opin. Behav. Sci.</source> <volume>17</volume>, <fpage>133</fpage>–<lpage>140</lpage> (<year>2017</year>).</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Sols I."><surname>Sols</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="DuBrow S."><surname>DuBrow</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davachi L."><surname>Davachi</surname>, <given-names>L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fuentemilla L."><surname>Fuentemilla</surname>, <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-26">Event Boundaries Trigger Rapid Memory Reinstatement of the Prior Events to Promote Their Representation in Long-Term Memory</article-title>. <source hwp:id="source-25">Curr. Biol.</source> <volume>27</volume>, <fpage>3499</fpage>–<lpage>3504.e4</lpage> (<year>2017</year>).</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Davachi L."><surname>Davachi</surname>, <given-names>L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="DuBrow S."><surname>DuBrow</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-27">How the hippocampus preserves order: the role of prediction and context</article-title>. <source hwp:id="source-26">Trends Cogn. Sci.</source> <volume>19</volume>, <fpage>92</fpage>–<lpage>99</lpage> (<year>2015</year>).</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Hanslmayr S."><surname>Hanslmayr</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Staresina B. P."><surname>Staresina</surname>, <given-names>B. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bowman H."><surname>Bowman</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-28">Oscillations and Episodic Memory: Addressing the Synchronization/Desynchronization Conundrum</article-title>. <source hwp:id="source-27">Trends in Neurosciences</source> <volume>39</volume>, <fpage>16</fpage>–<lpage>25</lpage> (<year>2016</year>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Johnson A."><surname>Johnson</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Redish A. D."><surname>Redish</surname>, <given-names>A. D.</given-names></string-name> <article-title hwp:id="article-title-29">Neural Ensembles in CA3 Transiently Encode Paths Forward of the Animal at a Decision Point</article-title>. <source hwp:id="source-28">J. Neurosci.</source> <volume>27</volume>, <fpage>12176</fpage>–<lpage>12189</lpage> (<year>2007</year>).</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Jafarpour A."><surname>Jafarpour</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fuentemilla L."><surname>Fuentemilla</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Horner A. J."><surname>Horner</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Penny W."><surname>Penny</surname>, <given-names>W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Duzel E."><surname>Duzel</surname>, <given-names>E.</given-names></string-name> <article-title hwp:id="article-title-30">Replay of very early encoding representations during recollection</article-title>. <source hwp:id="source-29">J. Neurosci.</source> <volume>34</volume>, <fpage>242</fpage>–<lpage>8</lpage> (<year>2014</year>).</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Coltheart M."><surname>Coltheart</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-31">The MRC psycholinguistic database</article-title>. <source hwp:id="source-30">Q. J. Exp. Psychol. Sect. A</source> <volume>33</volume>, <fpage>497</fpage>–<lpage>505</lpage> (<year>1981</year>).</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Brysbaert M."><surname>Brysbaert</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="New B."><surname>New</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-32">Moving beyond Kučera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English</article-title>. <source hwp:id="source-31">Behav. Res. Methods</source> <volume>41</volume>, <fpage>977</fpage>–<lpage>990</lpage> (<year>2009</year>).</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Brainard D. H."><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> <article-title hwp:id="article-title-33">The Psychophysics Toolbox</article-title>. <source hwp:id="source-32">Spat. Vis.</source> <volume>10</volume>, <fpage>433</fpage>–<lpage>436</lpage> (<year>1997</year>).</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Ratcliff R."><surname>Ratcliff</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-34">Group reaction time distributions and an analysis of distribution statistics</article-title>. <source hwp:id="source-33">Psychol. Bull.</source> <volume>86</volume>, <fpage>446</fpage>–<lpage>461</lpage> (<year>1979</year>).</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2 xref-ref-34-3 xref-ref-34-4 xref-ref-34-5"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Oostenveld R."><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fries P."><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maris E."><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Schoffelen J.-M."><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name> <article-title hwp:id="article-title-35">FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source hwp:id="source-34">Comput. Intell. Neurosci.</source> <volume>2011</volume>, <fpage>1</fpage>–<lpage>9</lpage> (<year>2011</year>).</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>35.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Delorme A."><surname>Delorme</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Makeig S."><surname>Makeig</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-36">EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source hwp:id="source-35">J. Neurosci. Methods</source> <volume>134</volume>, <fpage>9</fpage>–<lpage>21</lpage> (<year>2004</year>).</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Tal I."><surname>Tal</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Abeles M."><surname>Abeles</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-37">Cleaning MEG artifacts using external cues. J. Neurosci</article-title>. <source hwp:id="source-36">Methods</source> <volume>217</volume>, <fpage>31</fpage>–<lpage>38</lpage> (<year>2013</year>).</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Stolk A."><surname>Stolk</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Todorovic A."><surname>Todorovic</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schoffelen J. M."><surname>Schoffelen</surname>, <given-names>J. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oostenveld R."><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-38">Online and offline tools for head movement compensation in MEG</article-title>. <source hwp:id="source-37">Neuroimage</source> <volume>68</volume>, <fpage>39</fpage>–<lpage>48</lpage> (<year>2013</year>).</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Long N. M."><surname>Long</surname>, <given-names>N. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burke J. F."><surname>Burke</surname>, <given-names>J. F.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kahana M. J."><surname>Kahana</surname>, <given-names>M. J.</given-names></string-name> <article-title hwp:id="article-title-39">Subsequent memory effect in intracranial and scalp EEG</article-title>. <source hwp:id="source-38">Neuroimage</source> <volume>84</volume>, <fpage>488</fpage>–<lpage>494</lpage> (<year>2014</year>).</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1 xref-ref-39-2"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Tzourio-Mazoyer N."><surname>Tzourio-Mazoyer</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-40">Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>. <source hwp:id="source-39">Neuroimage</source> <volume>15</volume>, <fpage>273</fpage>–<lpage>289</lpage> (<year>2002</year>).</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Tallon-Baudry C."><surname>Tallon-Baudry</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bertrand O."><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Delpuech C."><surname>Delpuech</surname>, <given-names>C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Pernier J."><surname>Pernier</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-41">Stimulus specificity of phase-locked and non-phase-locked 40 Hz visual responses in human</article-title>. <source hwp:id="source-40">J. Neurosci.</source> <volume>16</volume>, <fpage>4240</fpage>–<lpage>9</lpage> (<year>1996</year>).</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Busch N. A."><surname>Busch</surname>, <given-names>N. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dubois J."><surname>Dubois</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="VanRullen R."><surname>VanRullen</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-42">The Phase of Ongoing EEG Oscillations Predicts Visual Perception</article-title>. <source hwp:id="source-41">J. Neurosci.</source> <volume>29</volume>, <fpage>7869</fpage>–<lpage>7876</lpage> (<year>2009</year>).</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Rouder J. N."><surname>Rouder</surname>, <given-names>J. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Speckman P. L."><surname>Speckman</surname>, <given-names>P. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sun D."><surname>Sun</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morey R. D."><surname>Morey</surname>, <given-names>R. D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Iverson G."><surname>Iverson</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-43">Bayesian t tests for accepting and rejecting the null hypothesis</article-title>. <source hwp:id="source-42">Psychon. Bull. Rev.</source> <volume>16</volume>, <fpage>225</fpage>–<lpage>237</lpage> (<year>2009</year>).</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1 xref-ref-43-2"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Maris E."><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oostenveld R."><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-44">Nonparametric statistical testing of EEG-and MEG-data</article-title>. <source hwp:id="source-43">J. Neurosci. Methods</source> <volume>164</volume>, <fpage>177</fpage>–<lpage>190</lpage> (<year>2007</year>).</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simmons W. K."><surname>Simmons</surname>, <given-names>W. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bellgowan P. S."><surname>Bellgowan</surname>, <given-names>P. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Baker C. I."><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> <article-title hwp:id="article-title-45">Circular analysis in systems neuroscience: The dangers of double dipping</article-title>. <source hwp:id="source-44">Nat. Neurosci.</source> <volume>12</volume>, <fpage>535</fpage>–<lpage>540</lpage> (<year>2009</year>).</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="323774v1.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Benjamini Y."><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hochberg Y."><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-46">Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source hwp:id="source-45">J. R. Statisitical Soc. Ser. B</source> <volume>57</volume>, <fpage>289</fpage>–<lpage>300</lpage> (<year>1995</year>).</citation></ref></ref-list><sec id="s4" hwp:id="sec-9"><title hwp:id="title-15">Methods</title><sec id="s4a" hwp:id="sec-10"><title hwp:id="title-16">Participants</title><sec id="s4a1" hwp:id="sec-11"><title hwp:id="title-17">Balancing pilots</title><p hwp:id="p-35">The balancing pilots were realized to balance the difficulty between videos that later served as scene 1, 2 and 3 of a video-episode. For each of the 2 balancing pilot experiments, 18 subjects were tested (36 total). In the first balancing pilot, 16 female and 2 male, right handed subjects participated that were on average 18.67 years old (youngest: 18, oldest: 20). In the second balancing pilot, 15 female and 3 male right handed subjects were tested. Their average age was 21.39 years (youngest: 18, oldest: 47). 2 additional subjects were tested in balancing pilot 2, however their behavioral performance was at chance and they were excluded from the analysis.</p></sec><sec id="s4a2" hwp:id="sec-12"><title hwp:id="title-18">Behavioral pilot and experiment</title><p hwp:id="p-36">For the behavioral pilot only 12 subjects (8 female, 4 male) participated that were on average 22.58 years old (youngest: 19, oldest: 29). 2 of the female participants were left handed, the rest were right handed. Data from 24 right handed volunteers (18 female, 6 male) was acquired for the behavioral experiment. The average age of this sample amounted to 22.79 years (youngest: 20, oldest 34).</p></sec><sec id="s4a3" hwp:id="sec-13"><title hwp:id="title-19">MEG experiment</title><p hwp:id="p-37">For the MEG experiment 24 volunteers (13 male, 11 female) participants were tested. Subjects were between 18 and 34 years old (mean: 23.92 years). 6 participants were left handed, 18 participants were right handed. 1 of the 24 subjects was excluded after pre-processing because of a persistent electrical artifact in the data that could not be removed with filtering.</p><p hwp:id="p-38">6 additional subjects (4 female, 2 male) aged 19 to 28 years (mean: 22) were recorded but not analyzed. 2 subjects moved excessively throughout the recording session (maximal movement: 1.8 cm and 2.7 cm), 1 subject moved excessively throughout the session (maximal movement 1.4 cm) and fell asleep during the experiment. 1 subject felt unwell and aborted the experiment after approx. 10 % of the recording session, 1 subject only completed approx. 70 % of the recording session and moved more than 2 cm throughout the experiment. Finally 1 subject was lost due to technical failure during the recording. After preprocessing, the maximal movement of included participants across all trials (i.e. the range of all positions) was on average 5.89mm (s.d. = 2.62, min = 1.69, max = 9.09).</p><p hwp:id="p-39">All included and excluded participants in the pilot studies, behavioral experiments and the MEG experiment, were native English speakers. Before participation they were screened for any neurological or psychiatric disorders. Their informed consent was obtained according to the ethical approval that was granted by the University of Birmingham Research Ethics Committee (ERN_15– 0335A), complying with the Declaration of Helsinki.</p></sec></sec><sec id="s4b" hwp:id="sec-14"><title hwp:id="title-20">Material and experimental set up</title><sec id="s4b1" hwp:id="sec-15"><title hwp:id="title-21">Videos</title><p hwp:id="p-40">For each of the balancing pilots, a total of 12 short video-clips were used. Videos stemmed from a pool that was provided by Landesfilmdienst Baden-Württemberg, Germany, some of them were additionally edited. Each video-clip was a 2-second-long colored, dynamic scene that featured a single action (i.e. a ship sailing or a diver jumping into the water). During the task, video-clips were always superimposed with a transparent text box (white box with alpha value 0.9) in which the word-cue could appear. According to the behavioral results from balancing pilot 1, we edited or changed some of the scenes before the second balancing pilot. The final video-clips were 12 different scenes that belonged to four general topics. For the behavioral experiments and the MEG experiment, the video-clips were then grouped into four seamless sequences of frames that formed a video-episode (i.e. a sequence of three scenes that belong to a general topic and form a short story). The 3 scenes of each video-episode were clearly distinguishable.</p><p hwp:id="p-41">According to the second balancing pilot, scenes that were assigned to be in 1st, 2nd or 3rd position of video-episodes, did not differ significantly in difficulty (percent correct responses), when associated with a word-cue. Pairwise comparisons with t-test of positions 1 and 2 (<italic toggle="yes">t</italic><sub>17</sub> = 0.86, <italic toggle="yes">p</italic> = 0.4), 2 and 3 (<italic toggle="yes">t</italic><sub>17</sub> = 0.15, <italic toggle="yes">p</italic> = 0.88) and 1 and 3 (<italic toggle="yes">t</italic><sub>17</sub> = 1.4693, <italic toggle="yes">p</italic> = 0.16) and Bayes-Factor analysis supported the null Hypothesis of no difference between positions. This was supported either by substantial (<italic toggle="yes">BF</italic><sub>01</sub>&gt;1.6) or strong (<italic toggle="yes">BF</italic><sub>01</sub>&gt;3.3) evidence for the comparison of positions 1 and 2 (<italic toggle="yes">BF</italic><sub>01</sub> = 2.97) of positions 2 and 3 (<italic toggle="yes">BF</italic><sub>01</sub> = 4.07) and of positions 1 and 3 (<italic toggle="yes">BF</italic><sub>01</sub> = 1.65). Importantly, reaction times in the second balancing pilot did not differ significantly between the video-clips that we finally assigned to be in position 1, 2 or 3. Pairwise comparisons with t-test of assigned positions 1 and 2 (<italic toggle="yes">t</italic><sub>17</sub> = −0.59, <italic toggle="yes">p</italic> = 0.56), 2 and 3 (<italic toggle="yes">t</italic><sub>17</sub> = −0.31, <italic toggle="yes">p</italic> = 0.76), and 1 and 3 (<italic toggle="yes">t</italic><sub>17</sub> = −1, <italic toggle="yes">p</italic> = 0.33) and Bayes-Factor analysis supported the null Hypothesis for the comparison of positions 1 and 2 (<italic toggle="yes">BF</italic><sub>01</sub> = 3.53) of positions 2 and 3 (<italic toggle="yes">BF</italic><sub>01</sub> = 3.95), and positions 1 and 3 (<italic toggle="yes">BF</italic><sub>01</sub> = 2.67).</p></sec><sec id="s4b2" hwp:id="sec-16"><title hwp:id="title-22">Word-cues</title><p hwp:id="p-42">Word-cues were downloaded from the MRC Psycholinguistic Database <sup><xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref></sup>. For the balancing pilots, we divided 540 word-cues into 18 lists. Those lists did not differ in Kucera-Francis written frequency (mean = 20.80, s.d. = 8.55), concreteness (mean = 506.50, s.d. = 90.07), imageability (mean = 521.04, s.d. = 69.51), number of syllables (mean = 1.63, s.d. = 0.68), number of letters (mean = 5.61, s.d. = 1.42) or word-frequencies taken from SUBTLEXus (mean = 15.22, s.d. = 14.07); specifically, “Subtlwf” was used <sup><xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref></sup>. In the balancing pilots, 12 of the lists were associated with a video-clip and 6 of the lists were assigned to become a distractor word. Across subjects each list was associated with every movie once and served as a distractor word six times. This was done to additionally control for list specific effects across subjects. An additional 9 words were randomly selected for practice.</p><p hwp:id="p-43">For the behavioral pilot, the behavioral experiment and the MEG experiment, we divided 360 word-cues into 12 lists. Those lists were likewise balanced for Kucera-Francis written frequency (mean = 20.41, s.d. = 7.47), concreteness (mean = 518.72, s.d. = 78.39), imageability (mean = 530.78, s.d. = 60.17), number of syllables (mean = 1.56, s.d. = 0.62), number of letters (mean = 5.44, s.d. = 1.30) and word-frequencies taken from SUBTLEXus (mean = 15.07, s.d. = 13.04); again, “Subtlwf” was used <sup><xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">31</xref></sup>. Across participants, each of the lists was associated with every video-clip twice. An additional 6 words were randomly selected for practice.</p></sec><sec id="s4b3" hwp:id="sec-17"><title hwp:id="title-23">Response options</title><p hwp:id="p-44">To create the response options (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1c-d</xref>), we took Screenshots from the video-clips. In the balancing pilots, we adjusted brightness and contrast, so that no screenshot appeared more salient. For the behavioral pilot, the behavioral experiment and the MEG experiment the numbers 1, 2 and 3 were framed by a square which resembled a frame from an old film. Those represented the first response options, i.e. the choice between scene 1, scene 2 or scene 3. For the second response, i.e. the response about the correct video-episode, the 3 screenshots from the concatenated video-clips were presented next to each other for each of the 4 choices. In the control condition of the behavioral experiment, the response option intact/rearranged was realized with a screenshot which was of the same size as the videos during presentations. This screenshot was superimposed by a transparent textbox containing a word-cue. The words intact and rearranged were displayed at the left and right of the textbox as response options. The left/right position of these options was balanced across participants.</p></sec><sec id="s4b4" hwp:id="sec-18"><title hwp:id="title-24">Behavioral setup</title><p hwp:id="p-45">Visual content was presented on an LED monitor (Samsung syncmaster 940n at a distance of approximately 60 cm from the subject’s eyes. The monitor was set to a refresh rate of 60 Hz. On a screen size of 1280 x 1024 pixels, the video-clips had the dimension of 360 pixels in width and 288 pixels in height on the screen. “Helvetica” was chosen as the general text font, font size was set to 22 for instructions and to 28 for word-cues. Black text (rgb: 0, 0, 0) and movies were presented against a white background (rgb: 255, 255, 255).</p></sec><sec id="s4b5" hwp:id="sec-19"><title hwp:id="title-25">MEG setup</title><p hwp:id="p-46">MEG was recorded at the Sir Peter Mansfield Imaging Center (SPMIC) in Nottingham, UK. Subjects performed the experiment in a seated position at a distance of approximately 60 cm from a white screen. The image was projected onto the screen using a PROPixx projector (VPixx Technologies, Saint-Bruno, Canada) that operated at a refresh rate of 60Hz and a resolution of 1920 × 1080 px. The projected image appeared at a size of approx. 40 × 22.5 cm on the screen. Accordingly, the video-clip appeared in a dimension of approx. 15 × 12 cm. An eye tracker (EyeLink 1000 plus, SR Research, Ontario, Canada) was placed in front of the screen. The tracker was mounted in an upwards facing orientation, slightly below the visible display, on a small wooden board. In this setup it tracked the subject’s left eye from below and from a distance of approximately 55 cm.</p></sec></sec><sec id="s4c" hwp:id="sec-20"><title hwp:id="title-26">Procedure</title><sec id="s4c1" hwp:id="sec-21"><title hwp:id="title-27">Balancing pilots</title><p hwp:id="p-47">The balancing pilots were realized to ensure that no material specific differences between the first, second or third position of a video-episode were to be expected in the following experiments. To this end, the video-clips that were considered were presented as single scenes during learning blocks, where they were superimposed by a transparent text box, containing a word-cue. Upon informed consent and completion of screening questionnaires, participants sat down in front of the screen and received a standardized instruction for the task. All subjects saw the video-clips for familiarization and completed a practice version of the task before starting. Participants performed 15 runs of an encoding block, in which their task was to vividly associate the word-cue with the corresponding video-clip, a short distractor block, in which they did some easy math and a retrieval block in which they retrieved the associated video-clips as quickly as possible, whilst presented with a word-cue.</p><p hwp:id="p-48">During encoding a fixation cross was displayed for 2 seconds. Then the video-clip and word-cue played for 2 seconds. Finally, a fixation cross appeared again for 2 seconds. In every encoding block, each video-clip was presented twice amounting to a total of 24 trials in every block. The video-clips were presented in a balanced but randomized order such that no movie was presented more than 2 times in a row.</p><p hwp:id="p-49">In the distractor block, subjects solved simple math problems. For 45 seconds they were either presented with the word bigger or with the word smaller and two single digit sums (e.g. 4+5 and 3+2). Their task was to select the correct sum (i.e. either the bigger or the smaller sum). Feedback was given in the form of the words “correct” and “wrong” appearing in green and red respectively on the screen.</p><p hwp:id="p-50">For each retrieval block the current 24 cues were mixed with 12 new distractor words in a randomized way, such that items corresponding to the same video-clip or items corresponding to a distractor did not appear more than 2 times in a row.</p><p hwp:id="p-51">In the retrieval block subjects were asked to select the video corresponding to the word-cue as fast as possible. The target (i.e. the screenshot from the correct clip) and two lures (two screenshots from a different clip) were presented on three positions around the word-cue. The positions formed a triangle with equal distance from the center to the left and right and 2/3 of that distance above the word-cue. In addition to those three response options, a question mark was displayed below the center of the screen. This response option was available in order to indicate that no video-clip-screenshot was identified as the correct target.</p><p hwp:id="p-52">In order to control for effects from specific screen positions, the mapping of targets to positions was randomized but balanced, such that the target was presented on every position 8 times. To control for item specific effects, the two lures that were presented with the target were assigned, such that every video-clip-screenshot served four times as a lure to a target and the same screenshot was never on both lure-positions. The 12 additional distractor words were, by definition, only paired with lures, we balanced the random mapping, such that every video-clip-screenshot served once as a lure on every position.</p><p hwp:id="p-53">In order to respond, participants placed the index finger of their dominant hand on the number 2 of the numeric keypad of the keyboard. The index finger rested there as long as no response was required. When presented with the cue-word subjects could either press one of the numbers 4, 6 and 8 which corresponded spatially to the response options (screenshots) on the screen and were in approximately the same distance from the starting position (number 2), or they could press 0 which corresponded to the question mark. Available buttons were highlighted with colored stickers to facilitate orientation.</p><p hwp:id="p-54">Whilst presented with the word-cue, subjects had maximally 4 seconds to select their answer. At the end of every retrieval block participants were reminded that associations from the previous block were now irrelevant and had the opportunity to take a self-paced break.</p></sec><sec id="s4c2" hwp:id="sec-22"><title hwp:id="title-28">Behavioral pilot and behavioral experiment</title><p hwp:id="p-55">In the behavioral pilot (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Fig. 1a, b, d</xref> top-left), subjects saw video-episodes that consisted of 3 distinct scenes. Those scenes comprised of the video-clips from balancing pilot 2, which ensured that no material specific differences were to be expected between position 1, 2 and 3 of the video-episodes; not in memory performance and most importantly not in reaction time. Participants first completed the screening questionnaire and gave informed consent. After instruction with the task, they saw the video-episodes twice for familiarization and were instructed to pay attention to their 3-scene-structure, such that they could confidently identify the first, second and third scene of each video-episode.</p><p hwp:id="p-56">After a short practice version of the task, the experiment started. It was again a sequence of encoding, distractor and retrieval blocks. In each encoding block subjects learned a series of associations (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Fig. 1a</xref>). They first saw a fixation cross on the screen for 2 seconds. After that one of the four video-episodes played for 6 seconds. During this video-episode a transparent textbox was overlaid on the video. In one of the three scenes, a word-cue appeared in the textbox and disappeared again with the end of the scene. Subjects were instructed to form a vivid association between the word and the precise scene of the video-episode, such that they could later recall that exact scene and video-episode upon presentation with the word-cue. We randomized the presentation of the associations in a balanced way, such that no video-episode was presented more than twice in a row and a word-cue did not appear in the same position more than twice in a row. Additionally, every position within every video-episode was associated with a word cue once within 12 subsequent associations.</p><p hwp:id="p-57">After each video-episode a fixation-cross showed for 1 second then subjects rated the plausibility of the association between word-cue and scene. Three response options were labelled with “not plausible”, “plausible” and “very plausible” and could be selected with the buttons 4, 5 and 6 on the numerical pad of the keyboard. The plausibility rating served to keep participants engaged in the task and support memory formation. In the distractor block, subjects were presented again for 45 seconds with simple math problems and had to decide which one of two single digit sums was either bigger or smaller (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-12" hwp:rel-id="F1">Fig. 1b</xref>). For the retrieval block the word-cues were now randomized again in a balanced way, such that word-cues corresponding to the same video-episode regardless of position, or to the same position regardless of video-episode, did not appear more than twice in a row.</p><p hwp:id="p-58">Retrieval blocks (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-13" hwp:rel-id="F1">Fig. 1d</xref>, top-left) started with a fixation cross, displayed for 2 seconds. Then a word-cue appeared in the center of the screen and the three framed numbers appeared on a triangle around the word-cue. Participants were instructed to select, as quickly as possible, in which of the three scenes they learned the word. For this choice they only saw the numbers 1, 2 and 3; after they made this choice, screenshots forming the four video episodes appeared in the four corners of the screen. Participants were asked to indicate now, to which of the four episodes the selected scene belonged. The position of the numbers 1, 2 and 3 as well as the mappings of the four screenshot-sequences to the screen positions were randomized in a balanced way, namely all possible permutations of 1, 2 and 3 were randomly mapped onto the three positions within 6 subsequent trials and all possible permutations of the four positions of the video-episode screenshots were used within 24 trials. This was done to control for any potential effects from specific screen positions on reaction times or position specific response preferences. In order to respond, volunteers were asked to place the index finger of their dominant hand on the number 5 of the numerical pad on the keyboard. The surrounding numbers 4, 6 and 8, which form a triangle around the number 5 were highlighted with red stickers and served as the response options for the scene-response (first response: 1, 2 or 3). Those buttons corresponded spatially to the position of the permuted numbers 1, 2 and 3 on the screen. Accordingly the buttons 1, 7, 9 and 3 which form a square on the numerical pad, were available for the second response which informed about the correct video-episode. Importantly subjects were instructed to make all responses with the index finger of their dominant hand and go back to the starting position after every response, i.e. leave the finger resting on the button 5. At the end of every retrieval trial, a scale appeared on which subjects rated the confidence in their response. Three options were labelled with “guess”, “sure” and “very sure” and corresponded to the buttons 4, 5 and 6 on the numerical pad.</p><p hwp:id="p-59">Participants performed a variable amount of runs of encoding, distractor and retrieval blocks that varied in length according to their individual memory performance. The first block comprised of 24 items, subsequently its length was adjusted. If more than 70% of items were recalled correctly in the last block (i.e. correct scene and movie were selected), 12 items were added to the next block, if less than 50% were recalled correctly, 12 items were removed from the following block. All blocks comprised at least of 12 associations that had to be learned. All participants completed 360 trials in total.</p><p hwp:id="p-60">In the final behavioral experiment (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-14" hwp:rel-id="F1">Fig. 1a, b, d</xref> top row) subjects performed exactly the same task as in the behavioral pilot experiment, however, every other block was performed with a different retrieval task. Specifically subjects performed the same learning paradigm, yet they did alternating retrieval blocks of cued-recall (CR, see above) and associative recognition (AR). In the AR blocks (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-15" hwp:rel-id="F1">Fig. 1d</xref>, top-right) subjects were presented with a screenshot of a single video-clip, representing one of three scenes within a video-episode. The center of the screenshot was again superimposed with the transparent textbox containing one of the previously learned word-cues. The association between word-cue and video-clip could either be intact, i.e. the word was learned in this exact position within the video-episode, or it could be rearranged. In the latter scenario, a different video-clip from the same video-episode was superimposed by the word-cue. This means that word-cues were either presented in the correct position or in the wrong position within the video-clip. Participants were again instructed to decide as quickly as they could, whether the association was intact or rearranged. Block-size was adjusted in the same way with percent of correct responses measured as 200*(Hits - False Alarms)/N, with Hits being the number of correctly identified intact associations and False Alarms referring to the number of rearranged associations that were declared intact and N referring to the number of trials in the last block. Response buttons for the intact/rearranged choice were 4 and 6 on the numerical pad, which are in equal distance from the number 5, where the index finger of participants’ dominant hand rested comfortably at the beginning of each trial. After the experiment participants answered a few interview questions regarding eventual strategies and their subjective experience of the task.</p></sec><sec id="s4c3" hwp:id="sec-23"><title hwp:id="title-29">MEG experiment</title><p hwp:id="p-61">In the MEG experiment (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-16" hwp:rel-id="F1">Fig. 1a, b, c</xref>) volunteers learned associations between video-episodes and word-cues in the same way as in the behavioral experiment. Memory retrieval was similar to the behavioral pilot experiment (i.e. a cued-recall task); however a fast response was not required (see below). Upon informed consent and screening questionnaires, participants received the instructions for the task on a laptop outside the scanner. They familiarized themselves with the video-episodes twice, paying close attention to their structure. It was ensured that every participant was able to identify the three different scenes of a video-episode. In a short practice, they performed a block of encoding, distractor and retrieval with the six example words. The head-localization coils of the MEG system were attached to the participants’ head and their positions were logged along with the shape of the participant’s head (see Data Collection). Subsequently, volunteers were seated in a comfortable position under the MEG helmet. Subjects used a single button on each of two response pads with their left and right index finger. After the eye tracker was mounted and calibrated, the experiment started.</p><p hwp:id="p-62">The MEG experiment was again a sequence of encoding, distractor and retrieval blocks. In each encoding block (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-17" hwp:rel-id="F1">Fig. 1a</xref>), subjects learned a series of associations between scenes in video-episodes and unique word-cues. Participants first saw a fixation-cross on the screen for 1 second. After that one of the four video-episodes played for 6 seconds overlaid with a transparent textbox. In one of the three scenes of the video-episode, the unique word-cue appeared in the textbox and disappeared again with the end of the scene. The task was again to form a vivid association between the word and the precise scene of the video-episode in order to later recall the exact scene and video-episode, when only presented with the word-cue.</p><p hwp:id="p-63">After the video-episode, the fixation-cross appeared again for 500ms. Finally, the two response options ‘plausible’ and ‘not plausible’ appeared on the left and right of the screen. Subjects used the left or right button to indicate whether the association between video-scene and word-cue was plausible to them. This task kept participants engaged and supported their memory performance.</p><p hwp:id="p-64">The order of presentation was randomized in a balanced way: no video-episode was presented more than twice in a row and a word-cue did not appear in the same position more than twice. Additionally every position within every video-episode was associated with a word cue once within 12 subsequent associations. In the distractor block (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-18" hwp:rel-id="F1">Fig. 1b</xref>) subjects solved simple math problems for 45 seconds: They had to decide which one of two single digit sums was either bigger or smaller, using a left or right button press. Participants received feedback on the distractor task in form of the words “correct” or “wrong” displayed in green or red respectively. For the retrieval block the word-cues were now randomized again in a balanced way, such that word-cues corresponding to the same video-episode regardless of position, or to the same position regardless of video-episode, did not appear more than twice in a row.</p><p hwp:id="p-65">Trials of the retrieval block (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-19" hwp:rel-id="F1">Fig. 1c</xref>) started with a fixation cross that was displayed for 1 second. Then a word-cue appeared in the center of the screen for 3.5 seconds. In this time interval subjects remembered in which exact scene they had seen this word. For a random time interval between 250ms and 750ms, a fixation cross was shown again, then the response options appeared. The time interval for retrieval was chosen based on reaction-time data from the behavioral experiments, such that participants could comfortably remember the correct association. The first response option required the selection of the correct scene. To this end pictograms featuring the numbers 1, 2 and 3 were displayed on the top of the screen. The mapping of the numbers 1, 2 and 3 to the three screen-positions was randomized in a balanced way such that all possible permutations appeared within 6 subsequent trials. Participants could now move a red square, which framed the current selection. By pressing the left button they changed their selection by moving the frame clockwise. This selection was confirmed by pressing the right button. Note that this button assignment ensured that subjects would always prepare the same response during the retrieval trial, regardless of the memory content. This is important to control for trivial but systematic differences that correlate with memory content in the retrieval interval.</p><p hwp:id="p-66">After the position was selected, the two other position pictograms were overlaid with transparency (alpha = 0.9), such that the selected option remained highlighted on the screen. The concatenated screenshots from the video-episodes appeared below the position-pictograms and the red selection frame could be moved clockwise with the left button. Again the selection was confirmed with the right button. To ensure that subjects followed instructions and tried to recall the correct position as soon as they were presented with the word-cue (and did not wait until the response options were presented), there was a time limit of 4 seconds to select the correct position and again to select the correct movie. To allow for flexibility due to hasty or imprecise selections, 200ms were added to this time limit, whenever the selection-frame was moved. Participants did not know about this increment; all participants selected their responses quickly but not hastily. If the time limit was exceeded, the message ‘too slow’ appeared at the center of the screen for 5 seconds. Altogether the time limits were designed, such that subjects could comfortably remember the correct association during the presentation of the word-cue, and were eager to select the two responses straight away. After the associated video-episode was selected, unselected response options were overlaid with transparency for 300ms, then the two options ‘guess’ and ‘know’ were presented on a new screen to give the participant the opportunity to communicate, whether the selected answers were based on a guess.</p><p hwp:id="p-67">Participants performed a variable amount of runs of encoding, distractor and retrieval blocks. The blocks varied in length according to individual memory performance. The first block comprised of 24 items, subsequently its length was adjusted. If more than 90% of items were recalled correctly in the last block (i.e. correct scene and movie were selected), 24 items were added to the next block, If more than 70% of items were recalled correctly, 12 items were added to the next block, if less than 50% were recalled correctly, 12 items were removed from the following block, if less than 40% were recalled correctly, 24 items were removed. All blocks comprised at least 12 associations that had to be learned, i.e. block-size was never reduced below 12 items; all participants learned and recalled a total of 360 associations.</p></sec></sec><sec id="s4d" hwp:id="sec-24"><title hwp:id="title-30">Data Collection</title><p hwp:id="p-68">Stimulus presentation and the collection of behavioral data was realized on a standard desktop computer running MATLAB 2014b (MathWorks) under Windows 7, 64 Bit version. Stimuli were presented through the Psychophysics Toolbox Version 3 <sup><xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref></sup>. In the behavioral experiments, responses were collected from button presses on the numerical pad of a wired keyboard (Model 1576, Microsoft Corporation, Redmond, US). In the MEG experiment, fiber optic response pads were used.</p><p hwp:id="p-69">Neurophysiological data were collected with 275-channel CTF MEG (CTF, Coquitlam, BC, Canada) at the Sir Peter Mansfield Imaging Center (SPMIC) in Nottingham, UK. The system was used in third-order gradiometer configuration, recording at a sampling frequency of 600 Hz over the whole duration of the experiment. Three localization coils that were attached to the participants’ left preauricular point (LPA), right preauricular point (RPA) and to a point slightly above the nasion (NAS) were energized during the recording session. This was done to localize the head position relative to the sensors.</p><p hwp:id="p-70">Head digitization was collected with a Polhemus ISOTRAK device (Colchester, Vermont, USA). A minimum of 500 points on the scalp were logged relative to the positions of the three fiducial points (LPA, RPA, NAS). Individual anatomical data was acquired via magnetic resonance imaging (MRI) (3T Achieva scanner; Philips, Eindhoven, the Netherlands) with an MPRAGE sequence covering the whole head at 1mm<sup>3</sup> resolution. MRIs were either measured at the SPMIC or at the Birmingham University Imaging Centre (BUIC).</p><p hwp:id="p-71">For 17 of the included subjects (23), eye tracking (Eyelink 1000 Plus, SR Research, Ontario, Canada) was recorded on a separate Computer provided by the manufacturer at a sampling rate of 2000 Hz. The data was additionally written into 3 analog input channels of the MEG system via the EyeLink Analog Card. The eye tracker was used in remote mode tracking the pupil and corneal reflection with a 16mm lens. It was calibrated and validated using 13 points on 80% of the screen, which contained all of the task relevant information.</p></sec><sec id="s4e" hwp:id="sec-25"><title hwp:id="title-31">Analysis of Reaction Times</title><p hwp:id="p-72">We defined reaction time (RT) as the time to the first response after onset of the word-cue (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-20" hwp:rel-id="F1">Fig. 1d</xref>, bottom-row). All RTs faster than 200ms were considered implausible and discarded from further analysis. Additionally RTs that were 2.5 standard deviations above the mean RT were discarded. The means of remaining RTs were then tested statistically. To account for the non-normal distribution of RTs <sup><xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref></sup>, all statistical tests are also reported for log-transformed RTs.</p></sec><sec id="s4f" hwp:id="sec-26"><title hwp:id="title-32">Preprocessing of Neural Data</title><p hwp:id="p-73">The data was preprocessed in MATLAB 2015a (MathWorks) with a combination of functions from the Fieldtrip toolbox for EEG/MEG analysis <sup><xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref></sup> and custom written scripts.</p><p hwp:id="p-74">For the sensor level analysis, the 3<sup>rd</sup> order gradiometer correction was first applied, then the continuous recording was filtered with a Butterworth IIR filter of 4<sup>th</sup> order with a stopband of 49.5 to 50.5 and its harmonics (99.5 - 100.5, 149.5 - 150.5, 199.5 - 200.5, and 249.5 - 250.5) to reduce the line noise artifact. Additionally the data was filtered with a stopband of 59 – 60 to attenuate noise with a center frequency of 59.5 Hz.</p><p hwp:id="p-75">Subsequently, the data was segmented into trials that started 1.5 seconds prior to video-onset and ended 7.5 seconds after video-onset at encoding. Trials at retrieval started 1.5 seconds prior to the onset of the word-cue and ended 5 seconds after onset of the word-cue. If available, the dataset was combined with the downsampled and segmented trials from the eye tracking.</p><p hwp:id="p-76">To remove activity from eye blinks and noise, and to detect heartbeats, Independent Component Analysis (ICA) was used <sup><xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref></sup>. For the computation of the ICA unmixing matrix, trials containing coarse artifacts or showing strong muscle activity were heuristically excluded. Additionally, the data was downsampled to 250 Hz and cut to 1 second-long segments; the obtained unimxing matrix was then applied to all original trials.</p><p hwp:id="p-77">When possible, we compared independent components with the eye tracking data; we removed those components that picked up eye-blinks or eye-movement related activity. Additional components that picked up electrical noise were removed from the data. A copy of components which contained a clear R-wave of the QRS complex in a heartbeat was stored for later peak-detection and regression. All remaining components were projected back to a channel representation.</p><p hwp:id="p-78">Finally, all data was inspected visually and trials containing artifacts were removed from later analysis.</p><p hwp:id="p-79">After visual inspection, 84.26 % (S.D. = 8.29 %) of trials remained.</p><p hwp:id="p-80">Heartbeats were removed with a regression based approach: An iterative peak detection algorithm was applied to the ICA-component showing the clearest R-wave; it served as a proxy for ECG. This was done only for the remaining trials after visual inspection. Before peak-detection the heartbeat-component was highpass-filtered (4Hz, 4<sup>th</sup> order Butterworth). The peak detection algorithm first calculated a plausible maximum of heartbeats that were not to be exceeded. The signal was z-scored and thresholded. Local peaks were detected by finding local maxima in clusters of z-scores that were above threshold. Subsequently the threshold was lowered stepwise, down to a z-score of 2. With lowering threshold, increasingly bigger areas around the peaks were excluded from further peak detection. If the maximum number of plausible peaks was exceeded, the threshold was no longer lowered. A heartbeat template was now created by averaging 500ms long segments around the peaks. Gaps in the continuous recording were subsequently zero-padded in order to convolve the component with the template. Peak detection was then repeated on the convolved time course and a new template was built from these peaks for subsequent convolution <sup><xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref></sup>. After a few repetitions, the template converged and the resulting peaks were controlled manually, even though errors rarely needed to be corrected.</p><p hwp:id="p-81">Instead of simply subtracting the averaged template from the data, the trials were now split into four big segments and a general linear model (GLM) was built around the peaks in each segment. A high pass filter (1Hz, 4<sup>th</sup> order Butterworth) was applied to the data, only for the purpose of fitting the model. The GLM consisted of a separate repeated measure factor for each time point in the heartbeat, beginning 280ms before the peak and ending 720ms after the peak <sup><xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">36</xref></sup>. Additionally, a separate factor was included for every heartbeat, which modelled the offset between 280ms pre-peak and 720ms post-peak. Furthermore an offset factor for the overall segment was included. The solved model was then applied to every channel. The data model ŷ was built by using only the repeated measure factors, which modelled each time point within the heartbeat (i.e. the beta weights for offsets were set to 0). After visual inspection, this resulting model of the heartbeat was subtracted from each original channel.</p><p hwp:id="p-82">For the source level analysis, the anatomical data was first aligned to the digitized head positions. This was done by extracting the surface of the head from the anatomical MRI; in a first step a rough alignment was done manually, then the Iterative Closest Point (ICP) algorithm implemented in fieldtrip <sup><xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">34</xref></sup> was used to match the surface to the point-cloud of the head digitization, finally this solution was controlled and eventually corrected again manually. The transformation to the aligned space was subsequently applied to the segmentation of the brain, which was likewise extracted from the anatomical images. To correct for head movements, the average head positions within the trials were first clustered, such that one positional-cluster was built for every 10 trials. Subsequently a separate lead field was computed for every cluster and then averaged. Hereby, an average lead field across all trials was obtained for each participant <sup><xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref></sup>. Importantly ‘all trials’ refers to the trials that were included in a given contrast (e.g. for the contrast of Hits and Misses at retrieval, encoding trials were not included in the computation of the lead field). Before the source level analysis, the 3<sup>rd</sup> order gradiometer correction was applied to the cut raw-data, lead fields were adjusted accordingly. Finally the data was demeaned and bandpass filtered between 4 and 15 Hz. The position of virtual sensors in individual brains was derived from a 1 cm spaced grid, which was placed 6mm below the surface of the cortex into the MNI brain and then spatially warped into individual brains. This was done via the inverse of the transformation describing their normalization and resulted in 1407 individual virtual sensor positions which were anatomically equivalent. Finally, to reconstruct activity on virtual sensors a linearly constrained minimum variance (lcmv) beamforming approach, implemented in the Fieldtrip toolbox <sup><xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-3" hwp:rel-id="ref-34">34</xref></sup>, was used. Filter coefficients were again computed on all data in a given contrast.</p></sec><sec id="s4g" hwp:id="sec-27"><title hwp:id="title-33">Analysis of oscillatory power (Supplemental Information)</title><p hwp:id="p-83">To estimate oscillatory power at retrieval (<xref ref-type="fig" rid="figS1" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Supplemental Fig. 1</xref>), the Fourier-transformed data was multiplied with a complex Morlet wavelet of six cycles. This was done in steps of 10ms for every full frequency between 2 and 40Hz. The raw power was then obtained from the squared amplitude of the Fourier spectrum. Across all trials within the contrast (i.e. Hits and Misses), a baseline was computed as the average power between 1 second pre-stimulus and 4 second after stimulus onset <sup><xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref></sup>. Trials were then normalized by subtracting the baseline and dividing by it (activity<sub>tf</sub> - baseline<sub>f</sub>)/baseline<sub>f</sub>, with t indexing time and f indexing frequency.</p></sec><sec id="s4h" hwp:id="sec-28"><title hwp:id="title-34">Region of Interest (ROI)</title><p hwp:id="p-84">An occipito-parietal region of interest (ROI) was derived from the AAL atlas <sup><xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref></sup> (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2b</xref>). To obtain the ROI in form of a group of virtual sensors, the sensor-positions in MNI-space were assigned to the nearest described AAL-region, based on their Euclidean distance. The occipito-parietal ROI comprised of bilateral AAL-regions: angular gyrus, calcarine sulcus, cuneus, inferior occipital cortex, inferior parietal lobule, lingual gyrus, middle occipital gyrus, precuneus, superior occipital gyrus, superior parietal lobule, supramarginal gyrus.</p></sec><sec id="s4i" hwp:id="sec-29"><title hwp:id="title-35">Content specific oscillatory phase at encoding</title><p hwp:id="p-85">During encoding, participants repeatedly watched the same video-episodes. Hence, it was possible to assess content specific properties if they were more similar between trials of same content than between trials of different content (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Fig. 2a</xref>). In order to determine whether the ongoing oscillatory phase was specific to individual perceptual content, trials were grouped into 4 sets according to the video-episode that was perceived. The complex Fourier spectrum was again derived by multiplying the Fourier-transformed data with a complex Morlet wavelet of six cycles. Then, inter-trial phase coherence <sup><xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref></sup> (ITPC) was computed across the trials of same content (i.e. for each of the four trial-groups). This was done at every full frequency between 2 and 40 Hz in steps of 10ms starting 1 second before the onset of the video-episodes and ending 7 seconds after the offset of the video-episodes. Following that, the trials were shuffled and grouped randomly into 4 sets of mixed-content-trials. Sets were of equal size to the 4 sets of same-content-trials. Again ITPC was computed separately for each of the 4 sets. To balance the contribution of the 4 sets, a Rayleigh Z-correction was applied with N*ITPC<sup><xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref></sup>, where N refers to the number of trials in a set. Finally the corrected ITPC was averaged across the 4 sets in the ordered and in the shuffled condition. Their difference indicated content specificity of phase which could be statistically tested <sup><xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-4" hwp:rel-id="ref-16">16</xref>,<xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref></sup>. The analysis in source-space was done in the same way using the virtual sensors; however the frequency was restricted to 8 Hz.</p></sec><sec id="s4j" hwp:id="sec-30"><title hwp:id="title-36">Content specific phase similarity between encoding and retrieval</title><p hwp:id="p-86">The reactivation of temporal patterns (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2c-d</xref>, <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Supplemental Fig. 3</xref>) was estimated on virtual sensors for the frequency of 8 Hz. To this end, the oscillatory phase coherence between encoding and retrieval was contrasted between trial-combinations of same content (e.g. watching video-episode A, recalling video-episode A) and random trial-combinations of different content (e.g. watching video-episode A, recalling video-episode B). The combinations were balanced, such that in both conditions (same vs. different combinations) exactly the same trials were used in the same amount of combinations. We only changed the pairing between encoding and retrieval trials. For each trial-combination, 1-second long windows from the encoding trial were now compared to every time point at retrieval starting at the onset of the word-cue and ending at its offset after 3.5 seconds. This comparison was done with a sliding window approach. As a metric of phase-similarity, the phase coherence across time <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-11" hwp:rel-id="ref-8">8</xref>,<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>,<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref></sup> (i.e. across the 1 second window) was computed. All possible windows from encoding were used in this sliding window approach, with the first window ranging from 0 to 1 seconds and the last window ranging from 5 to 6 seconds during the video-episode (compare <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2 c</xref>). Note that the response options set on between 250ms and 750ms after the word-offset, additionally the first response-screen did not contain content-information (only the numbers 1, 2, and 3) and all responses required a button-press on the left button. Therefore, no confounds from the response interval were expected to bleed into the tested interval. Oscillatory phase was estimated by multiplying the Fourier-transformed data with a complex Morlet wavelet of six cycles in steps of 15.6ms for consistency with our previous analyses <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-12" hwp:rel-id="ref-8">8</xref></sup>. The average similarity between all time-windows and combinations was subsequently averaged to derive a single value of similarity for combinations of same content and a single value for combinations of different content at each virtual sensor. Note that this method enables the investigation of highly dynamic patterns in a robust way, because a measure that captures dynamic changes in ongoing oscillations is accumulated across encoding time, retrieval time and ten thousands of trial-combinations.</p></sec><sec id="s4k" hwp:id="sec-31"><title hwp:id="title-37">Time courses of Replay</title><p hwp:id="p-87">To observe the temporal scale of reactivation (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Fig. 3</xref>), the distribution of similarity to the remembered stimulus content (i.e. phase coherence) across retrieval was compared between different sliding windows from encoding. By definition, a distribution is normalized to an area under curve of 1 and therefore accounts for differences in total similarity between windows. To robustly compare the distribution of similarity between 6 non-overlapping windows, phase-coherence was accumulated across time, such that at the beginning of the retrieval time, zero similarity to all windows was present and at the end of retrieval (i.e. at 3.5 seconds after word onset) 100 % of similarity was reached (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Fig. 3c</xref>). This made it possible to compare at each time point, whether the similarity to a window had come up earlier than to another window. In other words: If patterns from window “A” tend to appear earlier than patterns from window “B” across subjects, then the cumulated similarity to window A should be statistically higher than the cumulated similarity to window “B”, at several time points.</p><p hwp:id="p-88">In order to test for a general tendency for forward replay, a line was fitted across all 6 windows and tested against a slope of 0 (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-10" hwp:rel-id="F3">Fig. 3d</xref>). Hence a negative slope of this line means that earlier windows from encoding appear earlier during retrieval. In order to test the hypothesis that the replay of individual scenes takes place on a slower timescale (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-11" hwp:rel-id="F3">Fig. 3e</xref>), 3 lines were fitted across the 2 non-overlapping windows within each scene, and their slope was averaged. A more negative average slope of these 3 lines compared the slope of the line across all windows supports the hypothesis that replaying individual scenes takes place on a slower temporal scale.</p><p hwp:id="p-89">Importantly this way of cumulating the similarity distributions allows for robust testing across subjects at the expense of introducing temporal dependencies between time points. Specifically, if more similarity to a window is present at an early point this can propagate to later points, if similarity thereafter increases at the same speed for all windows. The extent of significant time intervals should therefore be interpreted with caution. Another disadvantage of this method is that the slope is interval scaled and its absolute value is not interpretable.</p><p hwp:id="p-90">In order to compensate for this disadvantage and quantify the actual lag between time windows from encoding descriptively (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-12" hwp:rel-id="F3">Fig. 3a-b</xref>), the distributions of similarity were averaged across subjects and smoothed with a moving average kernel of 250ms, to attenuate noise (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-13" hwp:rel-id="F3">Fig. 3a</xref>, right). The cross-correlation between distributions was then computed to estimate the lag between them: The shape of one similarity distribution is matched to another (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-14" hwp:rel-id="F3">Fig. 3b</xref>). This was done within the time interval in which the slowing down of replay was observed; specifically in which the slope for lines fitted within a scene was significantly more negative than the slope across all windows (i.e. between 550ms and 2350ms at retrieval).</p></sec><sec id="s4l" hwp:id="sec-32"><title hwp:id="title-38">Statistical analyses</title><sec id="s4l1" hwp:id="sec-33"><title hwp:id="title-39">Behavioral performance and Reaction times</title><p hwp:id="p-91">Behavioral performance was tested with a repeated-measures-ANOVA, on the percent of correct responses. Post-hoc tests were then performed with 2 separate ANOVAs for the final behavioral experiment and with a series of one-sample t-test (see Supplemental Information).</p><p hwp:id="p-92">RTs in the balancing pilots were first contrasted with one-sample t-tests. In order to statistically test the null hypothesis the Scaled JZS Bayes Factor <sup><xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref></sup> to the one-sample t-tests was computed. RTs in the behavioral pilot experiment were compared with a repeated-measures-ANOVA with the factor position (1, 2 and 3). In the final behavioral experiment, a 2×3-repeated-measures-ANOVA was computed with the factors retrieval task (cued-recall vs. associative recognition) and position (1, 2 and 3). Post-hoc tests were then performed with 2 separate ANNOVAs. Reaction times for the 3 different positions were subsequently compared with a series of post-hoc one-sample t-tests. Greenhouse-Geisser correction was used with all ANOVAs, null-effects of interest were tested with Bayesian t-tests <sup><xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">42</xref></sup>.</p></sec><sec id="s4l2" hwp:id="sec-34"><title hwp:id="title-40">Content specific oscillatory phase at encoding</title><p hwp:id="p-93">Content specific phase at encoding was statistically tested by contrasting average ITPC across arranged groups with the average ITPC across shuffled groups. This was done with a series of t-test at every time point between 0 and 6 seconds after onset of the video-episode, at every frequency between 2 and 40 Hz and at every sensor. Multiple comparison correction was done via Monte-Carlo permutation of contrast labels as implemented in the fieldtrip toolbox <sup><xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-4" hwp:rel-id="ref-34">34</xref>,<xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref></sup>. 3-dimensional clusters and cluster-sums were formed across time, frequency and sensors. Cluster-sums in the original contrast were compared to the distribution of cluster-sums under random label assignment in order to derive p-values. The cluster-forming threshold corresponded to the critical t-value (alpha &lt; 0.05) of a single-sided one-sample t-test, 1000 random permutations were drawn. On the source level content specific phase was assessed for the frequency of 8Hz. Again the ITPC of arranged groups and the ITPC of shuffled groups were contrasted with a one sample t-test that was computed at every time point and every virtual sensor. Clusters were summed across neighboring sensors and time points in 1000 random permutations. To obtain time courses within the parieto-occipital ROI, t-values were averaged across all virtual sensors within the ROI.</p></sec><sec id="s4l3" hwp:id="sec-35"><title hwp:id="title-41">Content specific phase similarity between encoding and retrieval</title><p hwp:id="p-94">Based on previous results <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-13" hwp:rel-id="ref-8">8</xref></sup>, statistical testing for content specific reactivation was done for the frequency of 8 Hz, restricted to an occipito-parietal region of interest (ROI) derived from the AAL atlas <sup><xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-2" hwp:rel-id="ref-39">39</xref></sup>. Averaged similarity values of encoding-retrieval combinations were contrasted between combinations of same content and combinations of different content. This was done with a one-sample t-test on every virtual sensor within the ROI. Subsequently t-values were thresholded with a t-value corresponding to a one-sided alpha value of 0.05; clusters were built across neighboring virtual sensors. Statistical testing was done again via 1000 random permutations. Cluster-sums in the original contrast were compared to the distribution of cluster-sums under random label assignment in order to derive p-values. A series of post-hoc t-tests was done on every time-point at retrieval in order to estimate the contribution to the effect from encoding windows (see Supplemental Information, specifically <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-4" hwp:rel-id="F6">supplemental Fig. 3a</xref>).</p></sec><sec id="s4l4" hwp:id="sec-36"><title hwp:id="title-42">Time courses of replay</title><p hwp:id="p-95">Time courses were obtained by averaging across the ROI, which allows for an unbiased investigation of the time-courses of reactivation (see Supplemental Information, specifically <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-5-1" hwp:rel-id="F5">supplemental Fig. 2a</xref>). Specifically, the cluster correction approach results in a biased noise-distribution within the cluster of significant reactivation. This renders the interpretation of its shape and any post-hoc analysis on sensors within the cluster problematic <sup><xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-2" hwp:rel-id="ref-43">43</xref></sup>, see also <sup><xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref></sup>. Since 86.46% of the t-values in the ROI were positive, we therefore decided to average across all virtual sensors within the anatomical ROI for the analyses of all time courses that were statistically tested.</p><p hwp:id="p-96">Likewise, similarity densities were computed on the averaged similarity values across all virtual sensors within the ROI. The cumulated similarity density distributions for 6 non-overlapping encoding-windows were obtained for every subject. Consequently at every retrieval time-point a line could be fitted across 6 values for every subject. The slope of that line was subsequently subjected to a t-test against 0 across all subjects. The resulting time-course of t-values across the whole retrieval time was finally subjected to a multiple comparisons correction by controlling the false discovery rate <sup><xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref></sup>. To compare the speed of replay within scenes, to the overall speed, the average slope fitted across two windows each (windows within scenes) was statistically tested against the slope across all encoding windows with a series of one-sample t-tests. T-values were obtained again at every time point during retrieval and the false discovery rate was controlled in order to correct for multiple comparisons. To estimate at which time-points reinstatement could be detected best (Supplemental Information, <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-11" hwp:rel-id="F2">Fig. 2a</xref>), a series of one-sample t-tests was computed at every retrieval time point, between encoding-retrieval similarity of same content combinations and encoding-retrieval combinations of different content combinations (see Supplemental Information). Finally, the average similarity to all encoding time points was compared within the ROI, between trials in which an association from the first, second or third scene was recalled (Supplemental Information, <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-12" hwp:rel-id="F2">Fig. 2b</xref>). This was done with a repeated-measures-ANOVA with the factor position and pairwise post-hoc t-tests.</p></sec><sec id="s4l5" hwp:id="sec-37"><title hwp:id="title-43">Oscillatory power (Supplemental Information)</title><p hwp:id="p-97">Baseline corrected oscillatory power was contrasted on the sensor level with a series of one-sample t-tests. Multiple-comparison correction was realized with a cluster-based Monte-Carlo permutation as implemented in the fieldtrip toolbox <sup><xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-5" hwp:rel-id="ref-34">34</xref></sup>. 1000 permutations of contrast-labels were used; the clusters were formed from neighboring values below a threshold (see below). Neighboring values were derived across time from 0 to 4 seconds after the onset of the word-cue, across frequency from 2 to 40 Hz and spatially across sensors The threshold was the t-value which corresponds to a threshold of alpha = 0.05 for a single sided test. The maximal cluster-sum of real data was then compared to the distribution of maximal cluster-sums under random permutations in order to derive a p-value. In order to find the most robust frequencies that showed oscillatory power decreases, a t-test was computed for the average power difference across time (0 – 4s), sensors and frequencies. On the source level, baseline-corrected power at 8 Hz was averaged over time between 0 and 4 seconds and subjected to a one-sample t-test. Multiple comparison correction was addressed with the same cluster-based permutation approach; however, clusters were formed across neighboring virtual sensors.</p></sec></sec></sec><sec id="s5" sec-type="supplementary-material" hwp:id="sec-38"><title hwp:id="title-44">Supplemental Information</title><sec id="s5a" hwp:id="sec-39"><title hwp:id="title-45">Behavioral performance</title><p hwp:id="p-98">In the behavioral pilot experiment, participants recalled the correct position and video-episode in 72.29% (<italic toggle="yes">SD</italic> = 11.69%) of the trials. A main effect of position indicated decreasing performance when an association had been learned in a later scene of a video-episode (ANOVA: <italic toggle="yes">F</italic><sub>1.27, 13.95</sub> = 4.988, <italic toggle="yes">p</italic> = 0.036, means: 74.86%, 72.29%, 69.72%); post-hoc tests indicated that only associations from the first position were recalled more often than associations from the third position (<italic toggle="yes">t</italic><sub>11</sub> = 6.27, <italic toggle="yes">p</italic> &lt; 0.001).</p><p hwp:id="p-99">In the alternating blocks of the behavioral experiment, participants recalled on average 69.47% (<italic toggle="yes">SD</italic> = 23.21%) of the correct word-scene associations in cued-recall (CR) blocks. They further recognized 90.27% (<italic toggle="yes">SD</italic> = 10.74%) of intact associations (Hits) and erroneously named 12.40% (<italic toggle="yes">SD</italic> = 14.38%) of rearranged associations intact (False Alarms) in an associative-recognition (AR) blocks. Performance in CR (i.e. percent correct responses) and in AR (i.e. percent Hits minus percent False Alarms) was compared with a 2×3 ANOVA. This revealed a significant main effect of condition (<italic toggle="yes">F</italic><sub>1, 23</sub> = 38.30, p &lt; 0.001), driven by a better performance in the associative-recognition blocks (<italic toggle="yes">t</italic><sub>23</sub> = 6.189, <italic toggle="yes">p</italic> &lt; 0.001) and a significant factor position (<italic toggle="yes">F</italic><sub>1.84, 42.24</sub> = 1.145, <italic toggle="yes">p</italic> = 0.002, interaction condition with position <italic toggle="yes">n.s.</italic>). This was driven by a slightly better performance in the cued-recall condition, for associations that were learned in the second position of a video-episode (ANOVA: <italic toggle="yes">F</italic><sub>1.58,36.24</sub> = 2.794, <italic toggle="yes">p</italic> = 0.086, position 1 vs. 2: <italic toggle="yes">t</italic><sub>23</sub> = −2.804, <italic toggle="yes">p</italic> = 0.02, position 2 vs. 3: <italic toggle="yes">t</italic><sub>23</sub> = 1.961, <italic toggle="yes">p</italic> = 0.062) and a worse performance in associative-recognition for associations that were learned in the third position (ANOVA: <italic toggle="yes">F</italic><sub>1.86,42.68</sub> = 5.552, <italic toggle="yes">p</italic> = 0.008, position 2 vs. 3: <italic toggle="yes">t</italic><sub>23</sub> = 3.879, <italic toggle="yes">p</italic> &lt; 0.001, all other <italic toggle="yes">ps</italic> &gt; 0.14).</p><p hwp:id="p-100">In the MEG experiment subjects remembered on average 63.54% (<italic toggle="yes">SD</italic> = 11.768%) of associations, excluding guesses. After preprocessing on average 200.348 trials (<italic toggle="yes">SD</italic> = 38.645) remained for known correct associations and an additional 116 trials (<italic toggle="yes">SD</italic> = 39.425) were guessed or incorrect responses.</p></sec><sec id="s5b" hwp:id="sec-40"><title hwp:id="title-46">Reaction time in the behavioral experiment (including correct guesses)</title><p hwp:id="p-101">The analyses of reaction times were repeated including those trials in which participants indicated that they had guessed the response. The 2x3 ANOVA of RTs revealed a significant main effect of condition (<italic toggle="yes">F</italic><sub>1.00, 23.00</sub> = 66.254, <italic toggle="yes">p</italic> &lt; 0.001, log-RT: <italic toggle="yes">F</italic><sub>1.00, 23.00</sub> = 98.52, <italic toggle="yes">p</italic> &lt; 0.001) driven by overall faster reactions in the associative-recognition condition (<italic toggle="yes">t</italic><sub>23</sub> = −8.14, <italic toggle="yes">p</italic> &lt; 0.001, log-RT: <italic toggle="yes">t</italic><sub>23</sub> = −9.619, <italic toggle="yes">p</italic> &lt; 0.001). A significant main effect of scene-position (<italic toggle="yes">F</italic><sub>1.90, 43.67</sub> = 5.304, <italic toggle="yes">p</italic> = 0.010, log-RT: <italic toggle="yes">F</italic><sub>1.87, 43.09</sub> = 2.823, <italic toggle="yes">p</italic> = 0.074) and the interaction of scene-position with retrieval-condition (<italic toggle="yes">F</italic><sub>1.96, 45.11</sub> = 5.041, <italic toggle="yes">p</italic> = 0.011, log-RT: <italic toggle="yes">F</italic><sub>1.89, 43.39</sub> = 5.771, <italic toggle="yes">p</italic> = 0.007) were both due to a strong forward replay effect in the cued-recall condition (ANOVA: <italic toggle="yes">F</italic><sub>1.80, 41.36</sub> = 8.796, <italic toggle="yes">p</italic> = 0.001, log-RT: <italic toggle="yes">F</italic><sub>1.64, 37.82</sub> = 8.304, <italic toggle="yes">p</italic> = 0.002). Specifically, associations that were learned in the first scene-position of a video-episode (mean RT = 2.5 sec) were recalled on average 132ms faster than associations that were learned in the second scene-position (<italic toggle="yes">t</italic><sub>23</sub> = −1.752, <italic toggle="yes">p</italic> = 0.047, log-RT: <italic toggle="yes">t</italic><sub>23</sub> = −2.127, <italic toggle="yes">p</italic> = 0.022). Associations that were learned in the second scene-position (mean RT = 2.617 sec) were recalled on average 170ms faster than associations that were learned in the third scene-position (<italic toggle="yes">t</italic><sub>23</sub> = −2.864, <italic toggle="yes">p</italic> = 0.004, log-RT: <italic toggle="yes">t</italic><sub>23</sub> = −2.539, p = 0.009).</p><p hwp:id="p-102">In the AR condition, subjects performed the exact same encoding task, which also required source-memory. Importantly, no differences in reaction times were evident between associations that were learned in the first, second or third position during encoding (ANOVA: <italic toggle="yes">F</italic><sub>1.44, 33.09</sub> = 0.185, <italic toggle="yes">p</italic> = 0.759, log-RT: <italic toggle="yes">F</italic><sub>1.52, 35.05</sub> = 0.591, <italic toggle="yes">p</italic> = 0.515, pairwise comparisons of positions: all <italic toggle="yes">p</italic>s &gt; 0.5, Bayes-Factor supporting the null Hypothesis: position 1 vs. 2, <italic toggle="yes">BF</italic><sub>01</sub> = 3.771, position 2 vs. 3, <italic toggle="yes">BF</italic><sub>01</sub> = 4.466, position 1 vs. 3, <italic toggle="yes">BF</italic><sub>01</sub> = 4.504, log-RT: all <italic toggle="yes">ps</italic> &gt; 0.39, position 1 vs. 2, <italic toggle="yes">BF</italic><sub>01</sub> = 3.688, position 2 vs. 3, <italic toggle="yes">BF</italic><sub>01</sub> = 3.317, position 1 vs. 3, <italic toggle="yes">BF</italic><sub>01</sub> = 4.048).</p></sec><sec id="s5c" hwp:id="sec-41"><title hwp:id="title-47">Reaction times in the behavioral pilot</title><p hwp:id="p-103">In the behavioral pilot experiment, participants associated word-cues with one of three scenes within video-episodes (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-21" hwp:rel-id="F1">Fig. 1a</xref>). Four continuous video-episodes each comprised of three individual scenes. A trial unique word-cue appeared in one scene during a video-episode. After a brief distractor task (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-22" hwp:rel-id="F1">Fig. 1b</xref>) a cued-recall task (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-23" hwp:rel-id="F1">Fig. 1d</xref>, top-left) was conducted where participants were presented with the word cues. Their task was to recall the scene-position that was associated with the word-cue as quickly as possible. After that, participants indicated which video-episode out of four was associated with the word.</p><p hwp:id="p-104">Faster reaction times to associations that were associated with early position compared to later positions were observed (ANOVA: <italic toggle="yes">F</italic><sub>1.40, 15.41</sub> = 4.257, <italic toggle="yes">p</italic> = 0.045, ANOVA of log-transformed RTs: <italic toggle="yes">F</italic><sub>1.58,</sub> <sub>17.38</sub> = 4.903, <italic toggle="yes">p</italic> = 0.027). On average, reaction times (RT) to first scene-positions were faster than RTs to second scene-positions (2.044 vs. 2.212 sec., <italic toggle="yes">t</italic><sub>11</sub> = −3.558, <italic toggle="yes">p</italic> = 0.005, log-RT: <italic toggle="yes">t</italic><sub>11</sub> = −3.626, <italic toggle="yes">p</italic> = 0.004), and trended to be faster than for third scene-positions (2.221 sec, <italic toggle="yes">t</italic><sub>11</sub> = −2.05, <italic toggle="yes">p</italic> = 0.065; log-RT: <italic toggle="yes">t</italic><sub>11</sub> = - 2.227, <italic toggle="yes">p</italic> = 0.048). RTs for second scene-positions were only numerically, but not significantly, faster compared to third scene-positions. These results suggest that memory replay is forward and compressed. During encoding, individual scenes of each video-episode lasted 2 seconds. During retrieval, however, subjects took on average 167.8ms longer to recall an association from the second scene-position and an additional 10ms longer to recall an association from the third scene-position. Importantly, these effects cannot be explained by material specific differences between positions, because balancing pilot experiments ensured that there were no differences in RTs when the scenes from position 1, 2 and 3 were associated with a word-cue in isolation (see Online Methods).</p></sec><sec id="s5d" hwp:id="sec-42"><title hwp:id="title-48">Broad decreases in oscillatory power accompany successful memory reinstatement</title><p hwp:id="p-105">Successful memory reinstatement was associated with strong and sustained decreases in oscillatory power. Successfully remembered associations were those trials, in which subjects knew that they had identified the correct scene and the correct video-episode. Those trials were contrasted with the trials in which subjects either indicated a guess, or in which they selected the wrong scene-position and/or video-episode. A broad cluster emerged, in which oscillatory power was significantly lower when memory-retrieval was successful (<italic toggle="yes">p</italic><sub>cluster</sub> &lt; 0.001, <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Supplemental Fig. 1</xref>, middle). This cluster included a sustained power-decrease in the lower alpha band. In a series of post-hoc t-tests, the same contrast was now tested on averaged oscillatory power across time and sensors. Inspection of t-values confirmed a local peak at 8 Hz (<italic toggle="yes">t</italic><sub>22</sub> = −3.367, <italic toggle="yes">p</italic> = 0.001, <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Supplemental Fig. 1</xref>, right) which we previously linked to replay during episodic memory reinstatement <sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-14" hwp:rel-id="ref-8">8</xref></sup>. In order to derive the topography for the average power decrease at 8 Hz across time, a separate t-test was computed on every sensor. Maximal t-values were located over central sensors extending over right parietal sensors. The average power at 8Hz was next contrasted at every virtual sensor, resulting in an estimate of the spatial extent of power decreases in source space. Bilateral central and occipito-parietal areas as well as the medial temporal lobe displayed power decreases at this frequency (<xref ref-type="fig" rid="figS1" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Supplemental Fig. 1</xref>, left). These findings replicate our previous findings of broad power decreases with a sustained decrease at 8Hz, in a paradigm that prompts subjects to replay a dynamic stimulus from memory.</p><fig id="figS1" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Supplemental Fig. 1:</label><caption hwp:id="caption-4"><title hwp:id="title-49">Oscillatory correlates of successful memory</title><p hwp:id="p-106">Successful memory was associated with broad decreases in oscillatory power. The middle panel shows the t-values averaged across time and sensors within the significant cluster. Low frequencies displayed a sustained effect over time. T-tests of the average power decrease across time and sensors expressed two local peaks in t-values, at 8 and 14 Hz (right panel). The topography of t-tests for the 8 Hz frequency at every sensor included central sensors and extended over right parietal sensors (right panel, top). Source reconstructions of the average power at 8Hz revealed power decreases on bilateral central and occipito-parietal areas as well as the medial temporal lobe.</p></caption><graphic xlink:href="323774_figS1" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s5e" hwp:id="sec-43"><title hwp:id="title-50">Time course of replay and further evidence for forward replay</title><p hwp:id="p-107">We further investigated the time-course of reinstatement of the video-episodes. To this end, we computed a t-test of content-specificity at every time point during retrieval; precisely we assessed the average content-specificity across the whole ROI (<xref ref-type="fig" rid="figS2" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Supplemental Fig. 2a</xref>). Three peaks emerged at 442ms (<italic toggle="yes">t</italic><sub>22</sub> = 2.363, <italic toggle="yes">p</italic> = 0.014), 1042ms (<italic toggle="yes">t</italic><sub>22</sub> = 2.022, <italic toggle="yes">p</italic> = 0.028) and at 2163ms (<italic toggle="yes">t</italic><sub>22</sub> = 2.258, <italic toggle="yes">p</italic> = 0.017). Interestingly the last peak corresponds roughly to the period in which we observed average reaction times in the behavioral experiments.</p><p hwp:id="p-108">Next, we further pursued forward replay. Following the behavioral results, we predicted that subjects replay overall more of the video-episodes when they have to recall later scene-positions (<xref ref-type="fig" rid="figS2" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Supplemental Fig. 2b</xref>, left). We reasoned that subjects accumulate evidence in a forward direction, until the correct association is identified. In the behavioral experiments, this would cause the increase in RT for associations that were learned later during encoding. Hence, in analogy to the analysis of the behavioral experiment, we split the retrieval trials according to the remembered scene-position. We assessed the average similarity to all sub-scenes in the corresponding video-episodes from encoding and compared it between trials: We contrasted trials in which an association from the first, second or third positional-scene of a video-episode was remembered. In this overall similarity to the corresponding video-episode should be higher, when participants recalled associations from later scene-positions. Specifically, if subjects stop replaying consecutive scenes, once they found the correct position of an association, then overall similarity should increase linearly with the to-be-remembered scene-position (i.e. similarity 1 &lt; 2 &lt; 3). In a first ANOVA there was no significant difference in similarity depending on the remembered scene-position (<italic toggle="yes">F</italic><sub>1.83, 40.31</sub> = 2.384, <italic toggle="yes">p</italic> = 0.109, linear contrast: <italic toggle="yes">F</italic><sub>1, 22</sub> = 3.63, <italic toggle="yes">p</italic> = 0.07). There was, however, significantly less similarity to encoding, in trials in which subjects remembered an association from the first positional-scene compared to trials in which the third scene was recalled (<italic toggle="yes">t</italic><sub>22</sub> = −1.909, <italic toggle="yes">p</italic> = 0.035, <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Supplemental Fig. 2b</xref>). Note that this is equivalent to testing the hypothesis of a linear increase (i.e. directed linear contrast) with planned contrasts in the ANOVA.</p><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Supplemental Fig. 2:</label><caption hwp:id="caption-5"><title hwp:id="title-51">Time course of reinstatement and forward replay in ROI</title><p hwp:id="p-109"><bold>(a)</bold> Time course of replay tested across the ROI. Three peaks are evident at 442ms, 1042ms and 2163ms. <bold>(b)</bold> Illustration of forward replay in which an association from the first, second or third scene was remembered (left) and difference to average similarity to encoding across the ROI (right, error bars are standard error of the mean). This shows higher similarity to encoding when associations from the third vs. first scene were remembered.</p></caption><graphic xlink:href="323774_figS2" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s5f" hwp:id="sec-44"><title hwp:id="title-52">Reinstatement of encoding patterns and further evidence for forward replay</title><p hwp:id="p-110">Overall we found a cluster of significant evidence for the reactivation of phase patterns from encoding during retrieval for hit trials (Hits; <italic toggle="yes">p<sub>cluster</sub></italic> = 0.034; <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Supplemental Fig 3a</xref>, <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Supplemental Fig 3b</xref> for unmasked maps of t-values). In this, we wanted to assess how much each sub-part from the video-episodes contributed to this effect. To this end, we computed a series of post-hoc t-tests for every encoding time-window. We obtained the highest t-values for the reinstatement of earlier time-windows during encoding (<xref ref-type="fig" rid="figS3" hwp:id="xref-fig-6-7" hwp:rel-id="F6">Supplemental Fig 3a</xref>, right).</p><p hwp:id="p-111">If participants start to replay from the beginning of a video-episode and typically progress until they have the correct word-scene association in memory (compare <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Supplemental Fig 2b</xref>, left), then early time windows from encoding should be reactivated more often and more thoroughly (see above), therefore this pattern is consistent with forward replay.</p><fig id="figS3" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6 xref-fig-6-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;323774v1/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Supplemental Fig. 3:</label><caption hwp:id="caption-6"><title hwp:id="title-53">Content specific pattern reinstatement</title><p hwp:id="p-112"><bold>(a)</bold> Cluster of significant reactivation of phase-patterns from encoding for successfully remembered associations (left) and contribution to effect (right). Early encoding windows express the highest t-values and contribute more to the effect than later ones. (<bold>b)</bold> Unmasked map of t-values for reactivation of phase-patterns from encoding in successfully remembered trials.</p></caption><graphic xlink:href="323774_figS3" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec></sec></back></article>
