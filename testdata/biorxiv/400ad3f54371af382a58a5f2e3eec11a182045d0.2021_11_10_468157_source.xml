<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.11.10.468157</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.11.10.468157</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.11.10.468157</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.11.10.468157</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.11.10.468157</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">HMD-AMP: Protein Language-Powered Hierarchical Multi-label Deep Forest for Annotating Antimicrobial Peptides</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding Author. Email: <email hwp:id="email-1">liyu@cse.cuhk.edu.hk</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Yu Qinze"><surname>Yu</surname><given-names>Qinze</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Dong Zhihang"><surname>Dong</surname><given-names>Zhihang</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Fan Xingyu"><surname>Fan</surname><given-names>Xingyu</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Zong Licheng"><surname>Zong</surname><given-names>Licheng</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-5"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3664-6722</contrib-id><name name-style="western" hwp:sortable="Li Yu"><surname>Li</surname><given-names>Yu</given-names></name><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3664-6722"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4"><label>1</label><institution hwp:id="institution-1">Department of Computer Science and Engineering, CUHK</institution>, Hong Kong SAR, <country>China</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3"><label>2</label><institution hwp:id="institution-2">The CUHK Shenzhen Research Institute</institution>, Hi-Tech Park, Nanshan, Shenzhen, 518057, <country>China</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2"><label>3</label><institution hwp:id="institution-3">University of Electronic Science and Technology of China</institution>, Chengdu, Sichuan, <country>China</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-11-13T12:45:14-08:00">
    <day>13</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-11-13T12:45:14-08:00">
    <day>13</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-11-13T12:51:03-08:00">
    <day>13</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-11-13T12:51:03-08:00">
    <day>13</day><month>11</month><year>2021</year>
  </pub-date><elocation-id>2021.11.10.468157</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-11-10"><day>10</day><month>11</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-11-10"><day>10</day><month>11</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-11-13"><day>13</day><month>11</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="468157.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.11.10.468157v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="468157.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.11.10.468157v1/2021.11.10.468157v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.11.10.468157v1/2021.11.10.468157v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Identifying the targets of an antimicrobial peptide is a fundamental step in studying the innate immune response and combating antibiotic resistance, and more broadly, precision medicine and public health. There have been extensive studies on the statistical and computational approaches to identify (i) whether a peptide is an antimicrobial peptide (AMP) or a non-AMP and (ii) which targets are these sequences effective to (Gram-positive, Gram-negative, etc.). Despite the existing deep learning methods on this problem, most of them are unable to handle the small AMP classes (anti-insect, anti-parasite, etc.). And more importantly, some AMPs can have multiple targets, which the previous methods fail to consider. In this study, we build a diverse and comprehensive multi-label protein sequence database by collecting and cleaning amino acids from various AMP databases. To generate efficient representations and features for the small classes dataset, we take advantage of a protein language model trained on 250 million protein sequences. Based on that, we develop an end-to-end hierarchical multi-label deep forest framework, HMD-AMP, to annotate AMP comprehensively. After identifying an AMP, it will further predict what targets the AMP can effectively kill from eleven available classes. Extensive experiments suggest that our framework outperforms state-of-the-art models in both the binary classification task and the multi-label classification task, especially on the minor classes. Compared with the previous deep learning methods, our method improves the performance on macro-AUROC by 11%. The model is robust against reduced features and small perturbations and produces promising results. We believe HMD-AMP will both contribute to the future wet-lab investigations of the innate structural properties of different antimicrobial peptides and build promising empirical underpinnings for precise medicine with antibiotics.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">antimicrobial peptides</kwd><kwd hwp:id="kwd-2">deep forest</kwd><kwd hwp:id="kwd-3">protein language model</kwd><kwd hwp:id="kwd-4">multi-label classification</kwd></kwd-group><counts><page-count count="16"/></counts></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-4">Introduction</title><p hwp:id="p-4">Antimicrobial peptides (AMPs) are potent, broad-spectrum antibiotics, which can help us combat diseases such as bacterial infections. For example, they have been found and synthesized to combat <italic toggle="yes">pseudomonas aeruginosa</italic> [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>], to heal wounds [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>], and to even potentially work against coronavirus [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]. Meanwhile, natural antimicrobial peptides have become an exciting area of research over the recent decades due to rather an inevitable risk associated with antibiotics: some antimicrobials can bring detrimental effects on the body’s normal microbial content by indiscriminately attacking both the pathological occurring and beneficial ones, which damage the essential functions of our lungs, intestines and other organs. These side-effects root from the broad-spectrum property of some antibiotics, which could cause serious repercussions on immunity, nutrition and worse still, leading to a relative overgrowth of certain bacteria and fungi [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>–<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>]. The latter repercussion could further lead to secondary infection such as <italic toggle="yes">clostridioides difficile</italic>, which stems from the overgrowth of microorganisms that are antibiotic-resistant [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]. Consequently, to alleviate the harm caused by antibiotics, inferences on the target of AMPs and how different peptide sequences kill targets such as viral pathogens and gram-positive bacteria are vital for major progress in antimicrobial peptide research and full utilization of AMP functions.</p><p hwp:id="p-5">Scientific contributions to antimicrobial peptide research include a wide range of wet-lab studies and com-putational biology studies. Examples of the former include finding out novel AMPs such as SAAP-148 that combats drug-resistant bacteria and biofilm [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>] and LL-37 that works against <italic toggle="yes">staphylococcus aureus biofilm</italic> [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>], extracting antimicrobial from tropical fruits [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>] and studying lipid and metal nanoparticles for antimicrobial peptide delivery [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>]. While wet-lab research is crucial for knowledge discoveries in this domain, their limited generality considering the required time investment for each analysis makes it difficult to evaluate AMPs at scale. With the advance of statistical and computational methodologies, we have observed exciting recent progress on this challenge. These contributions can be divided into three categories. First and foremost, there are improving data availability, such as DBAASP [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>], LAMP [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>], CAMP [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>], APD-3 [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>], DAMPD [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>] and DRAMP 2.0 [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref>]. The abundance of data and computational resources enables the scientific community to train large-scale models. Second, there are computational design and syntheses efforts using methods like semi-supervised learning [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>]. Finally, we have observed an accelerated growth of efforts on computational and statistical approaches analyzing AMPs, including statistical inferences like AntiBP2 [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>], propensity score-based binary response models [<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>] and novel multi-level pseudo-amino acid composition in iAMP-2L [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>] using fuzzy k-nearest neighbors. In recent years, AMP classification becomes a question of interest in the machine learning community, too. The discussion over the possibility of identifying novel antibacterial peptides using chemoinformatics and machine learning can be dated as early as 2009 [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>]. As the machine learning toolkit expands and computational resources become more affordable, the methods applied in this question also become more diverse. For example, one study uses random forest in their AmPEP framework to predict antimicrobial peptides using distribution patterns of amino acids [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>]; other studies use a deep regression model to perform antimicrobial peptide design [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]. There are even studies using deep generative networks and molecular dynamics simulations to speed up antimicrobial peptide discoveries [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>]. Perhaps one of the most notable development that sparked our interests leveraged the power of convolutional neural networks (CNN) and long-short-term-memory networks (LSTM) [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>] for the task of antimicrobial peptides classification. The paper provides a novel use of deep learning methodologies on large-scale AMP databases to identify the target of antimicrobial peptides.</p><p hwp:id="p-6">At the same time, there are several major challenges with computational approaches. Most antimicrobial databases include only sequences that are antimicrobials (positive labels), meaning that generating negative samples is challenging. Although there were efforts synthesizing some negative samples in previous studies, the “negative” cases are constructed in a way that is too easy to classify, which does not reflect the true intrinsic structures of non-AMPs. In addition, most antimicrobial databases include only gram-positive and gram-negative cases. In situations where multi-label data are provided, the class distribution tends to go extremely imbalanced. With respect to available models, traditional methodologies tend to overfit and fail to deliver similar performance on other homogeneous tasks. As a result, model performances of existing frameworks tend to decrease noticeably as the complexity of class distribution increases.</p><p hwp:id="p-7">In this study, we address these problems by proposing a novel, end-to-end deep learning framework, HMD-AMP. We curated a challenging dataset that more closely aligns with the structural diversity of AMPs and non-AMPs. Our architecture comprises the following major components: an embedding layer of protein sequences, a protein language encoder, a feature transformer and a hierarchical deep forest framework making binary classifications (AMP or non-AMP) and multi-label classifications. Our framework is then compared with other state-of-the-art models in a binary task (Task 1: classification of AMP/non-AMP) and a multi-label task (Task 2: classification of effectiveness among 11 possible antimicrobial targets). Our framework outperforms all SOTA models in both tasks. We then evaluate the performance with an ablation study and a reduced feature test, and our findings are robust against data and feature perturbations.</p><p hwp:id="p-8">The rest of this paper is organized as follows. We start by a more thorough overview of our end-to-end framework and an evaluation of the associated ‘model problem’ and the ‘data problem’ with previous approaches in greater details in <xref ref-type="sec" rid="s2" hwp:id="xref-sec-2-1" hwp:rel-id="sec-2">Section 2</xref>. We then compare our method side-by-side with many state-of-the-art approaches in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-5-1" hwp:rel-id="sec-5">section 3</xref>, followed by an introduction of our dataset and our experiment results, which include an ablation study, and a sensitivity analysis with reduced model sizes for evaluating the model robustness. Finally, we elaborate the observed limitations and potential future work based on our findings in <xref ref-type="sec" rid="s4" hwp:id="xref-sec-15-1" hwp:rel-id="sec-15">Section 4</xref> with a short discussion of potential applications.</p></sec><sec id="s2" hwp:id="sec-2" hwp:rev-id="xref-sec-2-1"><label>2</label><title hwp:id="title-5">Methods</title><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-6">Overview of HMD-AMP</title><p hwp:id="p-9">HMD-AMP is a supervised machine learning framework consisting of one feature extraction model as well as two prediction models. Given a protein sequence as input, HMD-AMP first extracts its features and then uses the generated features as the inputs to the prediction models. Our prediction models are designed with a two-level prediction strategy including a prediction model that predicts whether a given protein sequence is an AMP, and a second prediction model annotating the AMP’s antimicrobial activities [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>, <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]. Specifically, HMD-AMP performs feature extraction and function prediction respectively, and it deploys a hierarchical structure of our AMP dataset labeling space. Accordingly, given any sequence analyzed by the HMD-AMP framework, the first model extracts the structural feature. The extracted features are then used as inputs to the prediction model to predict whether the sequence is an AMP or not. Furthermore, if the sequence is predicted as an AMP, the next prediction model predicts the sequence into 11 biological functions (see <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>), and biological functions of an AMP are virtually its target groups. This hierarchical framework aims to use the detailed structural information of AMPs to improve the accuracy of prediction model results and to help alleviating the data imbalance problem.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-10">Overview of HMD-AMP. Top panel: HMD-AMP is consisting of one feature extraction model as well as two prediction models. The feature extraction model processes protein sequences into feature vectors. The first prediction model’s inputs are feature vectors, and the model predicts a protein is an AMP or not. If the protein is an AMP, the second prediction model predicts its 11 biological functions (multi-label classification). Bottom panel: the way sliding windows scan the features.</p></caption><graphic xlink:href="468157v1_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-11">For the feature extraction part of HMD-AMP, the model is an ESM-1b Transformer [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>], which takes the raw sequences as inputs. Then, the outputs of the feature extraction model are used as the inputs for the following deep forest [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>] model. The structure of the hierarchical classification model is illustrated in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref> (top panel). <xref ref-type="sec" rid="s2b" hwp:id="xref-sec-4-1" hwp:rel-id="sec-4">Section 2.2</xref> elaborates each component of our proposed model structure.</p></sec><sec id="s2b" hwp:id="sec-4" hwp:rev-id="xref-sec-4-1"><label>2.2</label><title hwp:id="title-7">Deep learning model</title><p hwp:id="p-12">At the feature extraction level, the model is an ESM-1b transformer [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref>]: a transformer-based self-supervised protein language model trained on the UniRef UR50/50 database [<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>]. ESM-1b processes inputs as character amino acids sequences, using positional embeddings instead of making assumptions on the ordering of the input. From ESM-1b, we obtain residue-level sequence embeddings. In order to get the protein-level embeddings as the inputs to the function prediction model, we average across all residue positions of residue-level sequence embeddings, hence getting a 1280-dimension feature vector for each sequence.</p><p hwp:id="p-13">Transformer’s [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>] self-attention mechanism and its ability to model long-range dependencies, which reflect structural properties of protein sequences, enable themselves to predict amino acid residual contact because the attention maps generated within the Transformer naturally correspond to the information between the various residues in the sequence.</p><p hwp:id="p-14">Then, at each function prediction level, the model is a deep forest [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">29</xref>] model, which demonstrates a cascade forest structure. Each level of the cascade receives the feature information processes by its previous level and feeds its outputs to the next level as inputs. Each cascade level is an ensemble of decision tree forests, and different types of forests are included to make the model diverse. In our model design, at each level, we deploy two completely-random tree forests and two random forests: for these forests, the inputs are embeddings obtained from the feature extraction model. Each random forest contains 1000 trees, it randomly selects <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="468157v1_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> features as candidate features (<italic toggle="yes">d</italic> being the number of input features) and the one with the best <italic toggle="yes">gini</italic> value is chosen for the split. For our 11 labels in the multi-label classification, we compute their <italic toggle="yes">gini</italic> values as follows:
<disp-formula id="eqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="468157v1_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
where <italic toggle="yes">p</italic><sub><italic toggle="yes">k</italic></sub> is the probability that the sample has the label <italic toggle="yes">k</italic>. Every complete-random tree forest also contains 1000 completely random trees, which are designed to detect important motifs across the inputs.</p><p hwp:id="p-15">To enhance the model’s ability to handle feature relationships, a multi-grained scanning procedure is designed as shown in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1</xref> (bottom panel). Specifically, sliding windows scan the given input features. Our inputs are 1280-dimension raw feature vectors, and a window size of 100 is used. For sequence data, a 100-dimension feature vector will be generated by sliding the window for one feature. As a result, a total of 1181 feature vectors are produced for each iteration. Feature vectors extracted from positive/negative training examples are regarded as corresponding instances, and these instances are used to train the forest and then to generate the estimated class distributions, which would be converted to class vectors. Finally, the class vectors are concatenated as transformed features. Take the binary classification task as an example: we have 2 classes, and 1181 2-dimension class vectors are produced by each forest. As a result, the 9448-dimension transformed feature vector is taken as the counterpart of the original 1280-dimension raw feature vector.</p><p hwp:id="p-16">In general, each level of the cascade receives feature information processed by its preceding level and feeds its processed results to the next level as inputs until there is no significant performance gain, when the training process terminates. This process makes the deep forest appropriately determines the complexity of its model by termination. Also, deep forest does not rely on backpropagation, so it is suitable for training data with either imbalance labels or small sample sizes, hence preventing the model from overfitting. Further, two mechanisms are added to help the deep forest performs well in predicting specific antimicrobial activities. The first mechanism is a measure-aware feature reuse [<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. That is, if the confidence of the current layer is lower than the threshold determined during training, the better representation of the previous layer is partially reused. The confidence to each label is an estimation of the respective label distributions. To accommodate labels with smaller representation, we choose macro-AUC as the confidence-computing metric (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1</xref> top panel). Macro-AUC is a label-based measure [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>] that is defined as follows:
<disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="468157v1_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
which is per-class raw average of AUC, where <italic toggle="yes">Y</italic> is the true label; <italic toggle="yes">Y</italic>.<sub><italic toggle="yes">j</italic></sub> is the <italic toggle="yes">j</italic>-th column of the label matrix, and ‘+’ (‘-’) is the relevant (irrelevant) note. &#x01d4ae;<sub>macro</sub> is the set of correctly ordered instance pairs on each label:
<disp-formula id="eqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="468157v1_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
where the <italic toggle="yes">f</italic><sub><italic toggle="yes">ij</italic></sub> means the confidence score of <italic toggle="yes">i</italic>-th instance on <italic toggle="yes">j</italic>-th label,</p><p hwp:id="p-17">Therefore, the confidence computing method of each label is shown as <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-4-1" hwp:rel-id="disp-formula-4">Equation 4</xref>. Here, <italic toggle="yes">m</italic> refers to the number of sequences and <italic toggle="yes">p</italic><sub><italic toggle="yes">ij</italic></sub> means Pr[<italic toggle="yes">ŷ</italic><sub><italic toggle="yes">ij</italic></sub>=1].
<disp-formula id="eqn4" hwp:id="disp-formula-4" hwp:rev-id="xref-disp-formula-4-1">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="468157v1_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
The second mechanism is the measure-aware layer growth, which focuses on the learning of representation, and it efficiently enhances the representation through various measures while reducing overfitting and controlling model complexity. For the cascade forest, we artificially set the maximal depth of the layers as 20 in the initialization step. If the model has grown to the maximal number, the training process terminates. In the initialization step, we also initialize the performance vector, which records the performance value on training data in each layer. Here, we still choose macro-AUC as the measure to indicate performance. During each layer <italic toggle="yes">t</italic>, the forest is fitted according to the training data so that we get this layer’s classifier <italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic></sub>. With the classifier, we predict the representation <italic toggle="yes">H</italic><sub><italic toggle="yes">t</italic></sub> (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-5-1" hwp:rel-id="disp-formula-5">Equation 5</xref>), where X is the training data, and G<sub><italic toggle="yes">t</italic>−1</sub> is the representation of the last layer. Then we obtain the new representation of the current layer by measure-aware feature reuse. Due to the layer growth being measure-aware, the model needs to compute the macro-AUC after fitting each layer. When the measure is not getting better in recent three layers, an early stopping mechanism forces the deep forest to stop growing even if it yet reaches the maximum depth. At the same time, the best performance layer index is recorded. Therefore, with such well-designed mechanisms, multi-label deep forest [<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">32</xref>] is very appropriate for solving multi-label problems.
<disp-formula id="eqn5" hwp:id="disp-formula-5" hwp:rev-id="xref-disp-formula-5-1">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="468157v1_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives>
</disp-formula>
</p></sec></sec><sec id="s3" hwp:id="sec-5" hwp:rev-id="xref-sec-5-1"><label>3</label><title hwp:id="title-8">Results</title><p hwp:id="p-18">In this section, we analyze our model in comparison with several published, state-of-the-art methods and train them with our dataset. In particular, we start by describing the different settings of the experiments and evaluating our proposed model compared to these SOTA models.</p><sec id="s3a" hwp:id="sec-6"><label>3.1</label><title hwp:id="title-9">Alternative methods</title><p hwp:id="p-19">We first introduce the benchmark result produced by Veltri et al. [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>] combining CNN and LSTM for antimicrobial recognition. In addition, the development of meta learning makes Model-Agnostic Meta-Learning (MAML) [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>] a suitable candidate. Instead of learning a model that can be used directly for prediction, such meta-learning methods learn how to learn a model faster and better instead. Meanwhile, we include the <italic toggle="yes">Probabilistic Model-Agnostic Meta-Learning</italic> [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>], an extension of the original MAML, as another candidate. We encourage interested readers to refer to their initial manuscripts for details beyond our brief summary. MAML can be interpreted as approximate inference for the posterior [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>]. It uses the <italic toggle="yes">maximum a posteriori</italic> (MAP) value. The algorithm evaluates the variational lower-bound for the logarithm of the approximate likelihood, which can be written as
<disp-formula id="eqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="468157v1_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
In this bound, it essentially performs approximate inference via MAP on <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">i</italic></sub> to obtain <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="468157v1_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>, and uses the variational distribution for <italic toggle="yes">θ</italic> only. Then, the inference network is given by
<disp-formula id="eqn7" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-9"><graphic xlink:href="468157v1_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
The training is performed by backpropagating gradients, and this process includes a term for the likelihood <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="468157v1_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> and the KL-divergence between the sample <italic toggle="yes">θ</italic> ∼ <italic toggle="yes">q</italic><sub><italic toggle="yes">ψ</italic></sub> and the prior <italic toggle="yes">p</italic>(<italic toggle="yes">θ</italic>). We try to implement both MAML and PMAML against CNN-LSTM [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-3" hwp:rel-id="ref-25">25</xref>] benchmark. Indeed, the parameters randomly initialized by PMAML are hard to train, and such PMAML model only performs well if we use parameters obtained from the trained MAML model for the training of PMAML.</p><p hwp:id="p-20">Another approach to highlight is the AMAP [<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>], which is a hierarchical multi-label prediction model that annotates the biological functions of AMP sequences, using extreme Gradient Boosting (XGBoost) [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>]. XGBoost is based on boosted trees, and it learns by minimizing the objective function:
<disp-formula id="eqn8" hwp:id="disp-formula-8">
<alternatives hwp:id="alternatives-11"><graphic xlink:href="468157v1_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
where <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="468157v1_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula>. Here, <italic toggle="yes">l</italic> (ŷ<sub>i</sub>, y<sub><italic toggle="yes">i</italic></sub>) is the loss function of predicted model output ŷ<sub>i</sub> and actual output y<sub><italic toggle="yes">i</italic></sub> for all examples. Ω (<italic toggle="yes">f</italic><sub><italic toggle="yes">k</italic></sub>) is a regularization function that is based on the number of trees <italic toggle="yes">T</italic> and the norm of the vector of scores <italic toggle="yes">u</italic> at the <italic toggle="yes">k</italic>-leaf of the trees. The regularization parameters <italic toggle="yes">γ</italic> and <italic toggle="yes">λ</italic> control the relative contribution of the two regularization factors in contrast to the minimization of the loss function. We include AMAP in our comparisons in <xref ref-type="sec" rid="s3d" hwp:id="xref-sec-9-1" hwp:rel-id="sec-9">Section 3.4</xref>.</p></sec><sec id="s3b" hwp:id="sec-7"><label>3.2</label><title hwp:id="title-10">Datasets</title><p hwp:id="p-21">We compile a comprehensive multi-label AMP database with a high degree of confidence. Specifically, we collect and clean amino acids sequences from three published AMP databases: Database of Antimicrobial Activity and Structure of Peptides (DBAASP) [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref>], an update to LAMP database linking antimicrobial peptide (LAMP2) [<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>], and data repository of antimicrobial peptides (DRAMP) [<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>]. Then, we remove the identical and duplicate sequences from our database. The resulting database is composed of 18514 high-quality sequences, coupled with labels of 11 antimicrobial activities classes (<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref> and <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>). In the design of our machine learning models, these sequences are taken as positive examples.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-2"><p hwp:id="p-22">Target groups and the number of peptides in each category in our positive dataset.</p></caption><graphic xlink:href="468157v1_tbl1" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-3"><p hwp:id="p-23">Peptides’ label amount in our positive dataset.</p></caption><graphic xlink:href="468157v1_tbl2" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap><p hwp:id="p-24">We extract 12659 peptides with the highest BLAST [<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>] similarity scores against the AMPs in our multi-label AMP database from Uniprot [<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>] and these peptides show no antimicrobial activity. Specifically, to avoid sequence and composition biases that affect our machine learning, we filter peptides using an approach similar to previous work [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-4" hwp:rel-id="ref-25">25</xref>] and remove all peptides with more than 40% sequence identity to each other using CDHIT [<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>]. It leaves a total of 8534 peptides and we use these as negative peptides to train and evaluate our model. We use 5-fold stratified cross-validation to evaluate the performance of our model, the dataset would randomly be divided into five folds, and at each time, four of them are chosen for the model training and the remaining one fold is used to test the trained model. As a result, average results are generated from repeating the above procedure five times.</p></sec><sec id="s3c" hwp:id="sec-8"><label>3.3</label><title hwp:id="title-11">Implementation details</title><p hwp:id="p-25">We use the Tensorflow toolkit to write our code and train HMD-AMP with 2 NVIDIA GeForce RTX 3090 GPUs. We train the first deep forest model on the whole dataset. Because our negative set bears a strong resemblance to the positive set, deep forest is forced to learn to be a more powerful model. Then, we perform the next level multi-label deep forest model with our positive set, the dataset contains more than 18000 AMP sequences, and each of them has 11 labels, indicating which target groups the sequence resists. When training the models, we first input the protein sequence directly into ESM-1b model, and for each sequence, we can obtain a 1280-dimension embedding vector, which is the result of averaging across all residue positions of residue-level sequence embeddings. Such an embedding vector is then fed into a deep forest model, with two random forests and two complete-random tree forests with 1000 completely random trees for the binary classification. If one sequence is predicted to be an AMP, the multi-label deep forest annotates its target groups, this model has the same tree structure with the binary classification model.</p></sec><sec id="s3d" hwp:id="sec-9" hwp:rev-id="xref-sec-9-1"><label>3.4</label><title hwp:id="title-12">Performance comparison</title><p hwp:id="p-26">Here, we evaluate the models’ performance we proposed above. We refer to Veltri et al.’s work [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-5" hwp:rel-id="ref-25">25</xref>] as DL, and DL model trained by MAML [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">34</xref>] and PMAML [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">35</xref>] algorithms are called MAML+DL and PMAML+DL respectively. We use a 5-fold stratified cross-validation to evaluate the performance of HMD-AMP. In this experiment, we randomly divide our dataset into five folds. Each time, we choose four folds from the dataset for the model training and test the trained model on the remaining one. To avoid data bias, average results are generated from repeating the above procedure five times.</p><sec id="s3d1" hwp:id="sec-10"><label>3.4.1</label><title hwp:id="title-13">Binary classification performance comparison</title><p hwp:id="p-27">As shown in <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref>, for the AMP/non-AMP classification, HMD-AMP shows higher recall (0.953) and F1-score (0.954) with promising accuracy (0.956) and precision (0.955) than existing methods. One observation is that deep network methods generally have better performances on binary (AMP/non-AMP) classification than AMAP. One of the reasons is when there is enough data for training, deep models can often fit better functions for prediction, whereas AMAP is considered a traditional machine learning model.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3:</label><caption hwp:id="caption-4"><p hwp:id="p-28">The AMP/non-AMP classification results between different methods</p></caption><graphic xlink:href="468157v1_tbl3" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap></sec><sec id="s3d2" hwp:id="sec-11"><label>3.4.2</label><title hwp:id="title-14">Multi-label classification performance comparison</title><p hwp:id="p-29">HMD-AMP beats the state-of-the-art results (as shown in <xref rid="tbl4" ref-type="table" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Table 4</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>) in this more challenging task. HMD-AMP significantly outperforms across all measures including accuracy, precision, recall, and macro-AUC. Despite all methods having relatively high accuracy (still far behind the HMD-AMP) on the AMP biological functions classification, the simple DL method cannot correctly predict labels with only a small amount of data (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>), and our HMD-AMP outperforms DL by more than 30% on both precision and recall scores. For example, in our dataset, only 41 peptides have resistance to Protista, DL has little effect on the classification of this label, leading to many false negatives. Actually, both DL and MAML+DL have a very inconsistent performance across different classes, especially in terms of recall. In contrast, HMD-AMP is quite stable across different classes regardless of precision or recall (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3</xref>). For AMAP, its two types of sequence-based feature representation help the method performs well on multi-label classification. Nevertheless, our method outperforms AMAP by about 10% on both precision and recall scores. Although PMAML+DL has relatively good performance on 4 evaluation metrics (still about 5% behind our method on both precision and recall), it is largely based on parameters derived from the trained MAML+DL. If randomly initialized parameters are used, it would be difficult for PMAML+DL to obtain good performance.</p><table-wrap id="tbl4" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tbl4</object-id><label>Table 4:</label><caption hwp:id="caption-5"><p hwp:id="p-30">The AMP biological functions (multi-label) classification results between different methods</p></caption><graphic xlink:href="468157v1_tbl4" position="float" orientation="portrait" hwp:id="graphic-13"/></table-wrap><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-6"><p hwp:id="p-31"><bold>a</bold>: macro-average ROC curves comparison of 5 models. <bold>b</bold>: ROC curves of our model, including macro/micro-average and 11 biological function labels.</p></caption><graphic xlink:href="468157v1_fig2" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-7"><p hwp:id="p-32">Detailed prediction performance comparison on each biological function label.</p></caption><graphic xlink:href="468157v1_fig3" position="float" orientation="portrait" hwp:id="graphic-15"/></fig></sec></sec><sec id="s3e" hwp:id="sec-12"><label>3.5</label><title hwp:id="title-15">Ablation study</title><p hwp:id="p-33">With the aforementioned experiments validating the strengths of our model in multi-label classification, we develop a new experiment to see if our method works with very little data. We select 50, 100, 200 data points from our positive dataset to train HMD-AMP, MAML+DL, and PMAML+DL, and see whether our model still delivers consistent performance. We make sure that the selected data contain positive and negative data for all 11 labels. Again, every model is validated by using a 5-fold cross-validation test reporting the performance averaged over five trials, where each trial leaves out a different 20% of the selected data as a test set to validate the performance of the model trained on the other 80% of the selected data. It is worth mentioning that our method works well even with little data: we show the result in <xref rid="tbl5" ref-type="table" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Table 5</xref>. When the number of data points is 200, our method outperforms the other two models on accuracy (0.928), precision (0.841), and macro-AUC (0.916). It is not surprising that PMAML has a slightly better recall score (0.821), because PMAML was designed to solve few-shot problems and it can be trained to converge at great speed. When the number of data points is 100, our method outperforms the other two models on accuracy (0.925) and macro-AUC (0.860), and PMAML gets the best score on precision (0.810) and recall (0.780), and when the number of data points is 50, PMAML shows slightly higher performance on all 4 measures. Although our approach is slightly behind PMAML, it still shows favorable performance and is superior to MAML across all metrics. The result indicates that our method is also suitable for the small sample problem.</p><table-wrap id="tbl5" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tbl5</object-id><label>Table 5:</label><caption hwp:id="caption-8"><p hwp:id="p-34">sensitivity analysis results</p></caption><graphic xlink:href="468157v1_tbl5" position="float" orientation="portrait" hwp:id="graphic-16"/></table-wrap><p hwp:id="p-35">To evaluate the effectiveness of the feature extraction model and the prediction model respectively, we conduct the model replacement test. To investigate the efficiency of the feature extraction model, we apply deep forest and take the raw representation of the sequence, i.e., one-hot encoding, as inputs. In this experiment, we name it Deep Forest. Further, to analyze our prediction model’s importance, we change the prediction model into the random forest [<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>] and retain our feature extraction model. The random forest has 1000 trees. Likewise, we name this method as Random Forest. <xref rid="tbl6" ref-type="table" hwp:id="xref-table-wrap-6-1" hwp:rel-id="T6">Table 6</xref> and <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4a</xref> show the experimental results of HMD-AMP, Random Forest, and Deep Forest, all three methods have high accuracy, but Random Forest has low precision and recall scores. Through inspection, we find that Random Forest could hardly recognize some labels with unbalanced data (such as Protista and Mollicute): its correct predictions of few true positive cases lead to a large number of false negatives. Even a small number of false positives could result in low precision. Besides, we find that Deep Forest performs well on unbalanced labels, which is due to the large size of trees in forests (All trees in the deep forest are averaged to generate an estimate of the distribution of classes) and the measure-aware layer growth mechanism. However, HMD-AMP has been consistently ranked as the best performance across all the measures, which indicates that our feature extraction model provides effective features. These features help deep forest using more suitable motifs to make the prediction.</p><table-wrap id="tbl6" orientation="portrait" position="float" hwp:id="T6" hwp:rev-id="xref-table-wrap-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tbl6</object-id><label>Table 6:</label><caption hwp:id="caption-9"><p hwp:id="p-36">ablation test results</p></caption><graphic xlink:href="468157v1_tbl6" position="float" orientation="portrait" hwp:id="graphic-17"/></table-wrap><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-10"><p hwp:id="p-37"><bold>a</bold>: Ablation study of the feature extraction model and prediction model. <bold>b</bold>: A 2D t-SNE projection. Different colors represent AMP’s different label-combination, and data points in the same cluster share a high similarity.</p></caption><graphic xlink:href="468157v1_fig4" position="float" orientation="portrait" hwp:id="graphic-18"/></fig></sec><sec id="s3f" hwp:id="sec-13"><label>3.6</label><title hwp:id="title-16">Reduced feature model analysis</title><p hwp:id="p-38">To verify the similarity of input features with exactly same labels, we perform visual processing on the feature vectors. We apply t-SNE [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>], which is a non-linear cluster recognition algorithm, for data dimensionality reduction. T-SNE finds similarity patterns in data points with multiple features, and we can see data points with high similarity converge into a cluster by projecting dimensionality reduction results onto a two-dimensional plane. We use t-SNE to reduce a 1280-dimension feature vector to a 2-dimension vector. For each label combination, we assign different numbers and use the numbers to distinguish the AMP of different labels. Some AMP sequences from our dataset are selected, and the result is shown in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4b</xref>. From the result, we clearly see differentiated clusters, which indicates the feature extraction model nicely mines the structural and functional features inside the protein sequences.</p><p hwp:id="p-39">However, 1280-dimension vector features cause time-burden and memory-consumption, we try to reduce features’ amount and choose valid features. we adopt Local Interpretable Model-agnostic Explanations (LIME) [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>]. LIME is an explanation technique that explains the prediction of our classifier in an interpretable and faithful manner by learning an interpretable model locally around the prediction. LIME gets each feature’s global weight (<xref ref-type="fig" rid="figB1" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure B.1</xref>) by averaging the value of local prediction, and we select 48 features with the highest global weight (<xref ref-type="fig" rid="figB2" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure B.2</xref>) as new features to train our prediction model. Despite using only 48 features, HMD-AMP’s performance still outperforms AMAP, DL, and MAML+DL on both the AMP/non-AMP classification and the AMP biological functions classification. Reduced features (48 features) model performance has a slight drop of about 0.7% on accuracy, 0.8% on precision and recall (<xref rid="tbl7" ref-type="table" hwp:id="xref-table-wrap-7-1" hwp:rel-id="T7">Table 7</xref>) compared with HMD-AMP using 1280 features on the AMP/non-AMP classification, but it greatly improves the training speed.</p><table-wrap id="tbl7" orientation="portrait" position="float" hwp:id="T7" hwp:rev-id="xref-table-wrap-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBL7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tbl7</object-id><label>Table 7:</label><caption hwp:id="caption-11"><p hwp:id="p-40">Reduced feature model performance</p></caption><graphic xlink:href="468157v1_tbl7" position="float" orientation="portrait" hwp:id="graphic-19"/></table-wrap></sec><sec id="s3g" hwp:id="sec-14"><label>3.7</label><title hwp:id="title-17">Model application</title><p hwp:id="p-41">We apply our model to 20 newly synthesized antimicrobial peptides candidate sequences (<xref ref-type="table" rid="tblA1" hwp:id="xref-table-wrap-8-1" hwp:rel-id="T8">Table A.1</xref>) [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">24</xref>]. In fact, only two of these sequences (YI12 and FK13) were resistant to gram-positive and gram-negative, while the other 18 were not antimicrobial peptides. We use HMD-AMP to predict 20 peptides, and find that YI12 and FK13 are predicted to have resistance to gram-positive and gram-negative. Besides, 8 of the non-AMP peptides are identified by HMD-AMP, in contrast to the method [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-3" hwp:rel-id="ref-24">24</xref>] recognizing all 20 peptides as AMP. Our HMD-AMP consistently outperforms methods like AmpGram [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>], AMPA [<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>], AMPScanner [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-6" hwp:rel-id="ref-25">25</xref>], and CAMP [<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>], which predict YI12 as non-AMP, and AMAP [<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">37</xref>], which predicts more non-AMP as AMP than our method.</p><p hwp:id="p-42">Another interesting observation is that FK13 has the highest gram-positive (0.893, rank:1) and gram-negative (0.930, rank:1) probability among the 12 peptides that HMD-AMP predicted as AMPs, YI12 also has promising probability on gram-positive (0.799, rank:5), gram-negative (0.829, rank:4). The result shows that our model not only predicts the existing AMP, but also carries out functional annotations for AMPs that are newly synthetic or have yet existed.</p></sec></sec><sec id="s4" hwp:id="sec-15" hwp:rev-id="xref-sec-15-1"><label>4</label><title hwp:id="title-18">Conclusion</title><p hwp:id="p-43">We develop a hierarchical method, HMD-AMP, to facilitate the detection of antimicrobial peptides, providing detailed AMPs’ biological functions annotations. Comprehensive experiments including cross-fold validation, sensitivity test, ablation study, and new peptides test, demonstrate the effectiveness and robustness of our proposed method, where our model consistently and significantly outperforms all counterparts. Most known AMP prediction methods only classify sequences into AMPs/non-AMPs, and methods that predict AMPs’ biological functions are mostly based on statistics, sequence comparison, and traditional machine learning models. However, former methods’ performances are worse than our model, they don’t recognize labels with small amounts of data well, leading to very low recall.</p><p hwp:id="p-44">Moreover, since the deep learning model trained on meta-learning algorithms performs well on little data, and it is developed to help the model learn faster in different scenarios, extending the model to do multi-tasks is a promising research direction. In the future, we will try to combine our method with the meta-learning algorithm and make it able to classify protein sequences into more groups, not limited to AMP. Also, the 20 peptides classification result in section ‘Model application’ enlightens us to further develop HMD-AMP as a generative model, which can sample large amounts of peptides and generate new AMPs.</p><p hwp:id="p-45">We believe that HMD-AMP can serve as a powerful tool to promote the application of antimicrobial peptides and alleviate the global threat of antibiotic resistant genes. In the future, we will incorporate other dimensions of information, such as 3D structural information [<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>, <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>], into our framework to further improve our method’s performance and extend the application scenarios.</p></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-19">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Mwangi J."><surname>Mwangi</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-2">The antimicrobial peptide zy4 combats multidrug-resistant pseudomonas aeruginosa and acinetobacter baumannii infection</article-title>. <source hwp:id="source-1">Proceedings of the National Academy of Sciences</source> <volume>116</volume>, <fpage>26516</fpage>–<lpage>26522</lpage> (<year>2019</year>).</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Thapa R. K."><surname>Thapa</surname>, <given-names>R. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Diep D. B."><surname>Diep</surname>, <given-names>D. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tønnesen H. H."><surname>Tønnesen</surname>, <given-names>H. H.</given-names></string-name> <article-title hwp:id="article-title-3">Topical antimicrobial peptide formulations for wound healing: Current developments and future prospects</article-title>. <source hwp:id="source-2">Acta biomaterialia</source> <volume>103</volume>, <fpage>52</fpage>–<lpage>67</lpage> (<year>2020</year>).</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Elnagdy S."><surname>Elnagdy</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="AlKhazindar M."><surname>AlKhazindar</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-4">The potential of antimicrobial peptides as an antiviral therapy against covid-19</article-title>. <source hwp:id="source-3">ACS Pharmacology &amp; Translational Science</source> <volume>3</volume>, <fpage>780</fpage>–<lpage>782</lpage> (<year>2020</year>).</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Rafii F."><surname>Rafii</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutherland J. B."><surname>Sutherland</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Cerniglia C. E."><surname>Cerniglia</surname>, <given-names>C. E.</given-names></string-name> <article-title hwp:id="article-title-5">Effects of treatment with antimicrobial agents on the human colonic microflora</article-title>. <source hwp:id="source-4">Therapeutics and clinical risk management</source> <volume>4</volume>, <fpage>1343</fpage> (<year>2008</year>).</citation></ref><ref id="c5" hwp:id="ref-5"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Price L. B."><surname>Price</surname>, <given-names>L. B.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-6">Staphylococcus aureus cc398: host adaptation and emergence of methicillin resistance in livestock</article-title>. <source hwp:id="source-5">MBio</source> <volume>3</volume>, <fpage>e00305</fpage>–<lpage>11</lpage> (<year>2012</year>).</citation></ref><ref id="c6" hwp:id="ref-6"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Solomon S. L."><surname>Solomon</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oliver K. B."><surname>Oliver</surname>, <given-names>K. B.</given-names></string-name> <article-title hwp:id="article-title-7">Antibiotic resistance threats in the united states: stepping back from the brink</article-title>. <source hwp:id="source-6">American family physician</source> <volume>89</volume>, <fpage>938</fpage>–<lpage>941</lpage> (<year>2014</year>).</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>[7]</label><citation publication-type="book" citation-type="book" ref:id="2021.11.10.468157v1.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><collab hwp:id="collab-1">Organization, W. H. et al</collab>. <source hwp:id="source-7">Antimicrobial resistance: global report on surveillance</source> (<publisher-name>World Health Organization</publisher-name>, <year>2014</year>).</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Saha S."><surname>Saha</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-8">Increasing antibiotic resistance in clostridioides difficile: a systematic review and meta-analysis</article-title>. <source hwp:id="source-8">Anaerobe</source> <volume>58</volume>, <fpage>35</fpage>–<lpage>46</lpage> (<year>2019</year>).</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.9" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="de Breij A."><surname>de Breij</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-9">The antimicrobial peptide saap-148 combats drug-resistant bacteria and biofilms</article-title>. <source hwp:id="source-9">Science translational medicine</source> <volume>10</volume> (<year>2018</year>).</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Kang J."><surname>Kang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dietz M. J."><surname>Dietz</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Li B."><surname>Li</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-10">Antimicrobial peptide ll-37 is bactericidal against staphylococcus aureus biofilms</article-title>. <source hwp:id="source-10">PLoS One</source> <volume>14</volume>, <fpage>e0216676</fpage> (<year>2019</year>).</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Makowski M."><surname>Makowski</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Silva Í.C."><surname>Silva</surname>, <given-names>Í.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pais do Amaral C."><surname>Pais do Amaral</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gonçalves S."><surname>Gonçalves</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Santos N. C."><surname>Santos</surname>, <given-names>N. C.</given-names></string-name> <article-title hwp:id="article-title-11">Advances in lipid and metal nanoparticles for antimicrobial peptide delivery</article-title>. <source hwp:id="source-11">Pharmaceutics</source> <volume>11</volume>, <fpage>588</fpage> (<year>2019</year>).</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Pirtskhalava M."><surname>Pirtskhalava</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-12">Dbaasp v. 2: an enhanced database of structure and antimicrobial/cytotoxic activity of natural and synthetic peptides</article-title>. <source hwp:id="source-12">Nucleic acids research</source> <volume>44</volume>, <fpage>D1104</fpage>–<lpage>D1112</lpage> (<year>2016</year>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Zhao X."><surname>Zhao</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu H."><surname>Wu</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lu H."><surname>Lu</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li G."><surname>Li</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Huang Q."><surname>Huang</surname>, <given-names>Q.</given-names></string-name> <article-title hwp:id="article-title-13">Lamp: a database linking antimicrobial peptides</article-title>. <source hwp:id="source-13">PloS one</source> <volume>8</volume>, <fpage>e66557</fpage> (<year>2013</year>).</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="book" citation-type="book" ref:id="2021.11.10.468157v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Thomas S."><surname>Thomas</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Karnik S."><surname>Karnik</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barai R. S."><surname>Barai</surname>, <given-names>R. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jayaraman V. K."><surname>Jayaraman</surname>, <given-names>V. K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Idicula-Thomas S."><surname>Idicula-Thomas</surname>, <given-names>S.</given-names></string-name> <publisher-name>Camp: a useful resource for research on antimicrobial peptides</publisher-name>. <source hwp:id="source-14">Nucleic acids research</source> <volume>38</volume>, <fpage>D774</fpage>–<lpage>D780</lpage> (<year>2010</year>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Wang G."><surname>Wang</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li X."><surname>Li</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wang Z."><surname>Wang</surname>, <given-names>Z.</given-names></string-name> <article-title hwp:id="article-title-14">Apd3: the antimicrobial peptide database as a tool for research and education</article-title>. <source hwp:id="source-15">Nucleic acids research</source> <volume>44</volume>, <fpage>D1087</fpage>–<lpage>D1093</lpage> (<year>2016</year>).</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Seshadri Sundararajan V."><surname>Seshadri Sundararajan</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-15">Dampd: a manually curated antimicrobial peptide database</article-title>. <source hwp:id="source-16">Nucleic acids research</source> <volume>40</volume>, <fpage>D1108</fpage>–<lpage>D1112</lpage> (<year>2012</year>).</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>[17]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Das P."><surname>Das</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-16">Pepcvae: Semi-supervised targeted design of antimicrobial peptide sequences</article-title>. <source hwp:id="source-17">arXiv preprint</source> <pub-id pub-id-type="arxiv">1810.07743</pub-id> (<year>2018</year>).</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Lata S."><surname>Lata</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mishra N. K."><surname>Mishra</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Raghava G. P."><surname>Raghava</surname>, <given-names>G. P.</given-names></string-name> <article-title hwp:id="article-title-17">Antibp2: improved version of antibacterial peptide prediction</article-title>. <source hwp:id="source-18">BMC bioinformatics</source> <volume>11</volume>, <fpage>1</fpage>–<lpage>7</lpage> (<year>2010</year>).</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Randou E. G."><surname>Randou</surname>, <given-names>E. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Veltri D."><surname>Veltri</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Shehu A."><surname>Shehu</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-18">Binary response models for recognition of antimicrobial peptides</article-title>. <source hwp:id="source-19">In Proceedings of the International Conference on Bioinformatics, Computational Biology and Biomedical Informatics</source>, <fpage>76</fpage>–<lpage>85</lpage> (<year>2013</year>).</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Xiao X."><surname>Xiao</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang P."><surname>Wang</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin W.-Z."><surname>Lin</surname>, <given-names>W.-Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jia J.-H."><surname>Jia</surname>, <given-names>J.-H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Chou K.-C."><surname>Chou</surname>, <given-names>K.-C.</given-names></string-name> <article-title hwp:id="article-title-19">iamp-2l: a two-level multi-label classifier for identifying antimicrobial peptides and their functional types</article-title>. <source hwp:id="source-20">Analytical biochemistry</source> <volume>436</volume>, <fpage>168</fpage>–<lpage>177</lpage> (<year>2013</year>).</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Fjell C. D."><surname>Fjell</surname>, <given-names>C. D.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-20">Identification of novel antibacterial peptides by chemoinformatics and machine learning</article-title>. <source hwp:id="source-21">Journal of medicinal chemistry</source> <volume>52</volume>, <fpage>2006</fpage>–<lpage>2015</lpage> (<year>2009</year>).</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>[22]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Bhadra P."><surname>Bhadra</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yan J."><surname>Yan</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li J."><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fong S."><surname>Fong</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Siu S. W."><surname>Siu</surname>, <given-names>S. W.</given-names></string-name> <article-title hwp:id="article-title-21">Ampep: Sequence-based prediction of antimicrobial peptides using distribution patterns of amino acid properties and random forest</article-title>. <source hwp:id="source-22">Scientific reports</source> <volume>8</volume>, <fpage>1</fpage>–<lpage>10</lpage> (<year>2018</year>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Witten J."><surname>Witten</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Witten Z."><surname>Witten</surname>, <given-names>Z.</given-names></string-name> <article-title hwp:id="article-title-22">Deep learning regression model for antimicrobial peptide design</article-title>. <source hwp:id="source-23">BioRxiv</source> <fpage>692681</fpage> (<year>2019</year>).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2 xref-ref-24-3"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Das P."><surname>Das</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-23">Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations</article-title>. <source hwp:id="source-24">Nature Biomedical Engineering</source> <volume>5</volume>, <fpage>613</fpage>–<lpage>623</lpage> (<year>2021</year>).</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2 xref-ref-25-3 xref-ref-25-4 xref-ref-25-5 xref-ref-25-6"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Veltri D."><surname>Veltri</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamath U."><surname>Kamath</surname>, <given-names>U.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Shehu A."><surname>Shehu</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-24">Deep learning improves antimicrobial peptide recognition</article-title>. <source hwp:id="source-25">Bioinformatics</source> <volume>34</volume>, <fpage>2740</fpage>–<lpage>2747</lpage> (<year>2018</year>).</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Zou Z."><surname>Zou</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tian S."><surname>Tian</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gao X."><surname>Gao</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Li Y."><surname>Li</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-25">mldeepre: Multi-functional enzyme function prediction with hierarchical multi-label deep learning</article-title>. <source hwp:id="source-26">Frontiers in Genetics</source> <volume>9</volume>, <fpage>714</fpage> (<year>2019</year>).</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Li Y."><surname>Li</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-26">Hmd-arg: hierarchical multi-task deep learning for annotating antibiotic resistance genes</article-title>. <source hwp:id="source-27">Microbiome</source> <volume>9</volume>, <fpage>1</fpage>–<lpage>12</lpage> (<year>2021</year>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2"><label>[28]</label><citation publication-type="website" citation-type="web" ref:id="2021.11.10.468157v1.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Rives A."><surname>Rives</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-27">Biological structure and function emerge from scaling unsupervised learning to 250 mil-lion protein sequences</article-title>. <source hwp:id="source-28">bioRxiv</source> (<year>2019</year>). URL <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.biorxiv.org/content/10.1101/622803v4" ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/622803v4" hwp:id="ext-link-2">https://www.biorxiv.org/content/10.1101/622803v4</ext-link>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2"><label>[29]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Zhou Z.-H."><surname>Zhou</surname>, <given-names>Z.-H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Feng J."><surname>Feng</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-28">Deep forest</article-title>. <source hwp:id="source-29">arXiv preprint</source> <pub-id pub-id-type="arxiv">1702.08835</pub-id> (<year>2017</year>).</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Suzek B. E."><surname>Suzek</surname>, <given-names>B. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang H."><surname>Huang</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGarvey P."><surname>McGarvey</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mazumder R."><surname>Mazumder</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wu C. H."><surname>Wu</surname>, <given-names>C. H.</given-names></string-name> <article-title hwp:id="article-title-29">Uniref: comprehensive and non-redundant uniprot reference clusters</article-title>. <source hwp:id="source-30">Bioinformatics</source> <volume>23</volume>, <fpage>1282</fpage>–<lpage>1288</lpage> (<year>2007</year>).</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Vaswani A."><surname>Vaswani</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-30">Attention is all you need</article-title>. <source hwp:id="source-31">In Advances in neural information processing systems</source>, <fpage>5998</fpage>–<lpage>6008</lpage> (<year>2017</year>).</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><label>[32]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Yang L."><surname>Yang</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu X.-Z."><surname>Wu</surname>, <given-names>X.-Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jiang Y."><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zhou Z.-H."><surname>Zhou</surname>, <given-names>Z.-H.</given-names></string-name> <article-title hwp:id="article-title-31">Multi-label learning with deep forest</article-title>. <source hwp:id="source-32">arXiv preprint</source> <pub-id pub-id-type="arxiv">1911.06557</pub-id> (<year>2019</year>).</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="book" citation-type="book" ref:id="2021.11.10.468157v1.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Wu X.-Z."><surname>Wu</surname>, <given-names>X.-Z.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zhou Z.-H."><surname>Zhou</surname>, <given-names>Z.-H.</given-names></string-name> <chapter-title>A unified view of multi-label performance measures</chapter-title>. <source hwp:id="source-33">In International Conference on Machine Learning</source>, <fpage>3780</fpage>–<lpage>3788</lpage> (<publisher-name>PMLR</publisher-name>, <year>2017</year>).</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2"><label>[34]</label><citation publication-type="book" citation-type="book" ref:id="2021.11.10.468157v1.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Finn C."><surname>Finn</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abbeel P."><surname>Abbeel</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Levine S."><surname>Levine</surname>, <given-names>S.</given-names></string-name> <chapter-title>Model-agnostic meta-learning for fast adaptation of deep networks</chapter-title>. <source hwp:id="source-34">In International Conference on Machine Learning</source>, <fpage>1126</fpage>–<lpage>11</lpage> (<publisher-name>PMLR</publisher-name>, <year>2017</year>).</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2"><label>[35]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Finn C."><surname>Finn</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xu K."><surname>Xu</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Levine S."><surname>Levine</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-32">Probabilistic model-agnostic meta-learning</article-title>. <source hwp:id="source-35">arXiv preprint</source> <pub-id pub-id-type="arxiv">1806.02817</pub-id> (<year>2018</year>).</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>[36]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Grant E."><surname>Grant</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Finn C."><surname>Finn</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Levine S."><surname>Levine</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Darrell T."><surname>Darrell</surname>, <given-names>T.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Griffiths T."><surname>Griffiths</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-33">Recasting gradient-based meta-learning as hierarchical bayes</article-title>. <source hwp:id="source-36">arXiv preprint</source> <pub-id pub-id-type="arxiv">1801.08930</pub-id> (<year>2018</year>).</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2"><label>[37]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Gull S."><surname>Gull</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shamim N."><surname>Shamim</surname>, <given-names>N.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Minhas F."><surname>Minhas</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-34">Amap: Hierarchical multi-label prediction of biologically active and antimicrobial peptides</article-title>. <source hwp:id="source-37">Computers in biology and medicine</source> <volume>107</volume>, <fpage>172</fpage>–<lpage>181</lpage> (<year>2019</year>).</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>[38]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Chen T."><surname>Chen</surname>, <given-names>T.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Guestrin C."><surname>Guestrin</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-35">Xgboost: A scalable tree boosting system</article-title>. <source hwp:id="source-38">In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</source>, <fpage>785</fpage>–<lpage>794</lpage> (<year>2016</year>).</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>[39]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Ye G."><surname>Ye</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-36">Lamp2: a major update of the database linking antimicrobial peptides</article-title>. <source hwp:id="source-39">Database</source> <volume>2020</volume> (<year>2020</year>).</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>[40]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Shi G."><surname>Shi</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-37">Dramp 3.0: an enhanced comprehensive data repository of antimicrobial peptides</article-title>. <source hwp:id="source-40">Nucleic Acids Research</source> (<year>2021</year>).</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>[41]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Madden T."><surname>Madden</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-38">The blast sequence analysis tool</article-title>. <source hwp:id="source-41">The NCBI handbook</source> <volume>2</volume>, <fpage>425</fpage>–<lpage>436</lpage> (<year>2013</year>).</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>[42]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><collab hwp:id="collab-2">Consortium, U</collab>. <article-title hwp:id="article-title-39">Uniprot: a worldwide hub of protein knowledge</article-title>. <source hwp:id="source-42">Nucleic acids research</source> <volume>47</volume>, <fpage>D506</fpage>–<lpage>D515</lpage> (<year>2019</year>).</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>[43]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Fu L."><surname>Fu</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niu B."><surname>Niu</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhu Z."><surname>Zhu</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu S."><surname>Wu</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Li W."><surname>Li</surname>, <given-names>W.</given-names></string-name> <article-title hwp:id="article-title-40">Cd-hit: accelerated for clustering the next-generation sequencing data</article-title>. <source hwp:id="source-43">Bioinformatics</source> <volume>28</volume>, <fpage>3150</fpage>–<lpage>3152</lpage> (<year>2012</year>).</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>[44]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Breiman L."><surname>Breiman</surname>, <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-41">Random forests</article-title>. <source hwp:id="source-44">Machine learning</source> <volume>45</volume>, <fpage>5</fpage>–<lpage>32</lpage> (<year>2001</year>).</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>[45]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Van der Maaten L."><surname>Van der Maaten</surname>, <given-names>L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G."><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-42">Visualizing data using t-sne</article-title>. <source hwp:id="source-45">Journal of machine learning research</source> <volume>9</volume> (<year>2008</year>).</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2"><label>[46]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Ribeiro M. T."><surname>Ribeiro</surname>, <given-names>M. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Singh S."><surname>Singh</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Guestrin C."><surname>Guestrin</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-43">“ why should i trust you?” explaining the predictions of any classifier</article-title>. <source hwp:id="source-46">In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</source>, <fpage>1135</fpage>–<lpage>1144</lpage> (<year>2016</year>).</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>[47]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Burdukiewicz M."><surname>Burdukiewicz</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-44">Proteomic screening for prediction and design of antimicrobial peptides with ampgram</article-title>. <source hwp:id="source-47">International journal of molecular sciences</source> <volume>21</volume>, <fpage>4310</fpage> (<year>2020</year>).</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>[48]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Torrent M."><surname>Torrent</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-45">Ampa: an automated web server for prediction of protein antimicrobial regions</article-title>. <source hwp:id="source-48">Bioinfor-matics</source> <volume>28</volume>, <fpage>130</fpage>–<lpage>131</lpage> (<year>2012</year>).</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>[49]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Waghu F. H."><surname>Waghu</surname>, <given-names>F. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barai R. S."><surname>Barai</surname>, <given-names>R. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gurung P."><surname>Gurung</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Idicula-Thomas S."><surname>Idicula-Thomas</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-46">Campr3: a database on sequences, structures and signatures of antimicrobial peptides</article-title>. <source hwp:id="source-49">Nucleic acids research</source> <volume>44</volume>, <fpage>D1094</fpage>–<lpage>D1097</lpage> (<year>2016</year>).</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>[50]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Lam J. H."><surname>Lam</surname>, <given-names>J. H.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-47">A deep learning framework to predict binding preference of rna constituents on protein surface</article-title>. <source hwp:id="source-50">Nature communications</source> <volume>10</volume>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2019</year>).</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>[51]</label><citation publication-type="other" citation-type="journal" ref:id="2021.11.10.468157v1.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Wei J."><surname>Wei</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen S."><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zong L."><surname>Zong</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gao X."><surname>Gao</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Li Y."><surname>Li</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-48">Protein-rna interaction prediction with deep learning: Structure matters</article-title>. <source hwp:id="source-51">arXiv preprint</source> <pub-id pub-id-type="arxiv">2107.12243</pub-id> (<year>2021</year>).</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>[52]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.11.10.468157v1.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="DeLano W. L."><surname>DeLano</surname>, <given-names>W. L.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-49">Pymol: An open-source molecular graphics tool</article-title>. <source hwp:id="source-52">CCP4 Newsletter on protein crystallography</source> <volume>40</volume>, <fpage>82</fpage>–<lpage>92</lpage> (<year>2002</year>).</citation></ref></ref-list><app-group hwp:id="app-group-1"><app id="app1" hwp:id="app-1"><label>Appendix</label><sec id="s5" hwp:id="sec-16"><label>A</label><title hwp:id="title-20">Structure investigation of the novel AMPs</title><p hwp:id="p-46">We draw the 3D structure graphs of YI12 and FK13, and find that ‘YLR’ subsequence in YI12 and ‘WLK’ subsequence in FK13 (stick model in <xref ref-type="fig" rid="figA1" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure A.1</xref>) can be aligned, and the RMSD (Root-mean-square deviation of atomic positions) score is 0.111. We suspect that these three loci make two sequences resistant to gram-positive and gram-negative. However, many existing loci prediction tools failed to predict YI12 as an AMP, and we are unable to validate the above conjecture with the existing computational tools. In the future, we will try to develop more accurate generative models and loci prediction models based on our proposed method, discovering new AMPs and identifying their functional loci.</p><table-wrap id="tblA1" orientation="portrait" position="float" hwp:id="T8" hwp:rev-id="xref-table-wrap-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/TBLA1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T8</object-id><object-id pub-id-type="publisher-id">tblA1</object-id><label>Table A.1:</label><caption hwp:id="caption-12"><p hwp:id="p-47">Sequences of the 20 peptides.</p></caption><graphic xlink:href="468157v1_tblA1" position="float" orientation="portrait" hwp:id="graphic-20"/></table-wrap><fig id="figA1" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIGA1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">figA1</object-id><label>Figure A.1:</label><caption hwp:id="caption-13"><p hwp:id="p-48">3D structure graphs of YI12 and FK13 generated by PyMOL [<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]</p></caption><graphic xlink:href="468157v1_figA1" position="float" orientation="portrait" hwp:id="graphic-21"/></fig></sec><sec id="s6" hwp:id="sec-17"><label>B</label><title hwp:id="title-21">Feature analysis</title><p hwp:id="p-49">We adopt Local Interpretable Model-agnostic Explanations (LIME) [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">46</xref>]. And LIME is an explanation technique that explains the prediction of our classifier in an interpretable and faithful manner by learning an interpretable model locally around the prediction. We obtain 1280 features’ effect on the prediction (global weights) by using the LIME framework (<xref ref-type="fig" rid="figB1" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure B.1</xref>). And those global weights of features are predicted based on the average value of local prediction. In fact, the main function of LIME framework is to find features that have the most positive impact on the model. And these selected features could help the model better fit the given data. We select 48 features (<xref ref-type="fig" rid="figB2" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure B.2</xref>) with the highest weight, and test whether the model trained with these features performs well or not.</p><fig id="figB1" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIGB1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figB1</object-id><label>Figure B.1:</label><caption hwp:id="caption-14"><p hwp:id="p-50">Global weights of 1280 features. We get 1280 features’ global weights in the classification task by applying LIME. Features with weights greater than 0 have positive effects on prediction, while features with weights less than 0 have negative effects.</p></caption><graphic xlink:href="468157v1_figB1" position="float" orientation="portrait" hwp:id="graphic-22"/></fig><fig id="figB2" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.10.468157v1/FIGB2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figB2</object-id><label>Figure B.2:</label><caption hwp:id="caption-15"><p hwp:id="p-51">We select 48 features with the highest weight among 1280 features. The X-axis shows indexes of 48 features. These features are used to train our function prediction model, in order to achieve faster training speed and less computational resource consumption.</p></caption><graphic xlink:href="468157v1_figB2" position="float" orientation="portrait" hwp:id="graphic-23"/></fig></sec></app></app-group></back></article>
