<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/393983</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;393983</article-id><article-id pub-id-type="other" hwp:sub-type="slug">393983</article-id><article-id pub-id-type="other" hwp:sub-type="tag">393983</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Task complexity interacts with state-space uncertainty in the arbitration process between model-based and model-free reinforcement-learning at both behavioral and neural levels</article-title></title-group><author-notes hwp:id="author-notes-1"><fn id="n1" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>*</label><p hwp:id="p-1">Joint senior authors</p></fn><corresp id="cor1" hwp:id="corresp-1">Correspondence to: <email hwp:id="email-1">swlee@kaist.ac.kr</email>; <email hwp:id="email-2">jdoherty@caltech.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4513-9087</contrib-id><name name-style="western" hwp:sortable="Kim Dongjae"><surname>Kim</surname><given-names>Dongjae</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-4513-9087"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Park Geon Yeong"><surname>Park</surname><given-names>Geon Yeong</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-3"><name name-style="western" hwp:sortable="O’Doherty John P."><surname>O’Doherty</surname><given-names>John P.</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-1" hwp:rel-id="aff-6">6</xref><xref ref-type="aff" rid="a7" hwp:id="xref-aff-7-1" hwp:rel-id="aff-7">7</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6266-9613</contrib-id><name name-style="western" hwp:sortable="Lee Sang Wan"><surname>Lee</surname><given-names>Sang Wan</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-6266-9613"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">Department of Bio and Brain Engineering</institution>, Korea Advanced Institute of Science Technology (KAIST) Daejeon 34141, <country>Republic of Korea</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3"><label>2</label><institution hwp:id="institution-2">Brain and Cognitive Engineering Program</institution>, Korea Advanced Institute of Science Technology (KAIST) Daejeon 34141, <country>Republic of Korea</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">KI for Health Science Technology</institution>, Korea Advanced Institute of Science Technology (KAIST) Daejeon 34141, <country>Republic of Korea</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">KI for Artificial Intelligence</institution>, Korea Advanced Institute of Science Technology (KAIST) Daejeon 34141, <country>Republic of Korea</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Computation &amp; Neural Systems</institution>, California Institute of Technology Pasadena, CA 91125, <country>USA</country></aff><aff id="a6" hwp:id="aff-6" hwp:rev-id="xref-aff-6-1"><label>6</label><institution hwp:id="institution-6">Behavioral &amp; Social Neuroscience</institution>, California Institute of Technology Pasadena, CA 91125, <country>USA</country></aff><aff id="a7" hwp:id="aff-7" hwp:rev-id="xref-aff-7-1"><label>7</label><institution hwp:id="institution-7">Division of Humanities and Social Sciences</institution>, California Institute of Technology Pasadena, CA 91125, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-08-17T04:03:10-07:00">
    <day>17</day><month>8</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-08-17T04:03:10-07:00">
    <day>17</day><month>8</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-08-17T04:11:16-07:00">
    <day>17</day><month>8</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-08-17T04:11:16-07:00">
    <day>17</day><month>8</month><year>2018</year>
  </pub-date><elocation-id>393983</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-08-16"><day>16</day><month>8</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-08-16"><day>16</day><month>8</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-08-17"><day>17</day><month>8</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license hwp:id="license-1"><p hwp:id="p-2">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="393983.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/393983v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="393983.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/393983v1/393983v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/393983v1/393983v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">SUMMARY</title><p hwp:id="p-3">A major open question concerns how the brain governs the allocation of control between two distinct strategies for learning from reinforcement: model-based and model-free reinforcement learning. While there is evidence to suggest that the reliability of the predictions of the two systems is a key variable responsible for the arbitration process, another key variable has remained relatively unexplored: the role of task complexity. By using a combination of novel task design, computational modeling, and model-based fMRI analysis, we examined the role of task complexity alongside state-space uncertainty in the arbitration process between model-based and model-free RL. We found evidence to suggest that task complexity plays a role in influencing the arbitration process alongside state-space uncertainty. Participants tended to increase model-based RL control in response to increasing task complexity. However, they resorted to model-free RL when both uncertainty and task complexity were high, suggesting that these two variables interact during the arbitration process. Computational fMRI revealed that task complexity interacts with neural representations of the reliability of the two systems in the inferior prefrontal cortex bilaterally. These findings provide insight into how the inferior prefrontal cortex negotiates the trade-off between model-based and model-free RL in the presence of uncertainty and complexity, and more generally, illustrates how the brain resolves uncertainty and complexity in dynamically changing environment.</p><sec hwp:id="sec-1"><title hwp:id="title-2">SUMMARY OF FINDINGS</title><p hwp:id="p-4">- Elucidated the role of state-space uncertainty and complexity in model-based and model-free RL.</p><p hwp:id="p-5">- Found behavioral and neural evidence for complexity-sensitive prefrontal arbitration.</p><p hwp:id="p-6">- High task complexity induces explorative model-based RL.</p></sec></abstract><counts><page-count count="34"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-2"><title hwp:id="title-3">INTRODUCTION</title><p hwp:id="p-7">An influential computational account of reward-related learning and decision-making built on the basis of a large body of empirical evidence suggests that there exists two distinct mechanisms for controlling instrumental actions: a model-free RL system that learns values for actions based on the history of rewards obtained on those actions (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Balleine and Dickinson, 1998</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Dickinson, 1985</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Graybiel, 2008</xref>), and a model-based RL (MB) system that computes action-values flexibly based on its knowledge about state-action-state transitions incorporated into an internal model of the structure of the world (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Doya et al., 2002</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Kuvayev et al., 1996</xref>). These two systems have different relative advantages and disadvantages for a behaving agent. For example, while model-free RL can be computationally cheap and efficient, it achieves this at the cost of a lack of flexibility, thereby potentially exposing the agent to inaccurate behavior following a change in the subjective value of the goal or outcome, or a rapid change in environmental contingencies (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Balleine and Dickinson, 1998</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Daw et al., 2005</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">Dickinson, 1985</xref>). On the other hand, model-based RL is computationally expensive as it requires active computation of expected values and planning, but retains flexibility in that it can rapidly adjust a behavioral policy following a change in the features of the outcome or environmental contingencies. The theoretical trade-offs that exist between these two systems has, alongside empirical evidence for the parallel existence of these two modes of computation in the behavior of animals and humans (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Akam et al., 2015</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-3" hwp:rel-id="ref-4">Balleine and Dickinson, 1998</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Balleine and O’Doherty, 2010</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Beierholm et al., 2011</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Doll et al., 2012</xref>, <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">2016</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Gläscher et al., 2010</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Gremel and Costa, 2013</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Gruner et al., 2016</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Hasz and Redish, 2018</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Kool et al., 2017</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Linnebank et al., 2018</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">McDannald et al., 2011</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Miller et al., 2017</xref>; <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Russek et al., 2017</xref>; Sang Wan <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Simon and Daw, 2011</xref>; <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Skatova et al., 2013</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">van Steenbergen et al., 2017</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Wunderlich et al., 2012</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Yin and Knowlton, 2004</xref>; <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Yin et al., 2005</xref>), prompted interest in elucidating how it is the trade-off between these systems is actually managed in the brain.</p><p hwp:id="p-8">One influential proposal is that there exists an arbitration process that allocates control to the two systems according to various criteria (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">Daw et al., 2005</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Kool et al., 2017</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Pezzulo et al., 2018</xref>). One variant of this theory suggests that estimates about the uncertainty in the predictions of the two systems mediates the trade-off between the respective controllers, such that under situations where the model-free system has unreliable predictions, the model-based system is assigned greater weight over behavior, while under situations where the model-free system has more accurate predictions, it will be assigned greater behavioral control (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-3" hwp:rel-id="ref-10">Daw et al., 2005</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Lee et al., 2014</xref>).</p><p hwp:id="p-9">One of the challenges in building a computationally and biologically plausible theory of the arbitration process, is the necessity that the arbitration process should not by itself be so computationally expensive as to render the relative savings in computational cost associated with being model-free vs between model-based to be rendered moot. For instance, for uncertainty-based arbitration, the computation of uncertainty might involve a computationally expensive process, while accurately estimating the potential gain to being model-based could under some implementations, actually require model-based computations to estimate that potential gain, which in many situations would defeat the purpose of the trade-off i.e. it could be more efficient to just remain model-based and dispense with arbitration altogether. To this end, practical computational theories of arbitration have examined computationally cheap approximations that might be utilized to mediate the arbitration process. According to Lee et al., uncertainty is approximated via a mechanism that tracks cumulative predictions errors induced in the two systems (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Lee et al., 2014</xref>). Model-free uncertainty is approximated via the average accumulation of reward-prediction errors, while model-based uncertainty is approximated via the accumulation of errors in state-prediction (so called state-prediction errors).</p><p hwp:id="p-10">Utilizing this framework, Lee et al., examined the neural correlates of the arbitration process (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-3" hwp:rel-id="ref-27">Lee et al., 2014</xref>). In that study, a region of bilateral ventrolateral prefrontal cortex was found to track the reliabilities (an approximation of the inverse of uncertainty) in the predictions of the two systems, consistent with a role for this brain region in the arbitration process itself. However, as alluded to above, the relative uncertainty or reliability in the predictions of the two systems is only one-component of the trade-off between the two controllers. Another equally important element of this trade-off is the relative computational cost of engaging in model-based control.</p><p hwp:id="p-11">The goal of the present study is to investigate the role of computational cost in the arbitration process, alongside relative uncertainty. We experimentally manipulated computational cost by means of adjusting the complexity of the planning problem faced by the model-based controller. This was achieved by subjecting participants to a multi-step Markov Decision Problem in which the number of actions available in each state was experimentally manipulated. In one condition, which we called low complexity, two actions were available, while in another condition, which we called high complexity, four actions were available. In addition to manipulating complexity, we also, as in our previous study (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-4" hwp:rel-id="ref-27">Lee et al., 2014</xref>), manipulated the uncertainty in the state-space, by utilizing a state-transition structure in the MDP that invoked high levels of state-prediction errors (i.e. one where the transitions are maximally uncertainty), and a transition structure where the transitions are either high or low in uncertainty. Thus, we manipulated two variables in a factorial design: state-space complexity (low vs high), and state-space uncertainty (low vs high).</p><p hwp:id="p-12">This design allowed us to investigate the ways in which uncertainty and complexity interact to drive the arbitration process, thereby allowing us to assess the interaction between (model-based) reliability and at least one simple proxy of computational cost. While participants were undergoing this novel behavioral task, we also simultaneously measured brain activity with fMRI. This allowed us to investigate the contribution of state-space complexity alongside state-space uncertainty in mediating arbitration at both behavioral and neural levels. In order to accommodate the effects of task complexity in the arbitration process, we extended our previous arbitration model to endow this arbitration scheme with the capability of adjusting the arbitration process as a function of complexity. On the neural level, we focused on the ventrolateral prefrontal cortex as our main brain region of interest, given this was the main region implicated in arbitration in our previous study. We hypothesized that an arbitration model which is sensitive to both the complexity of the state-space, and the degree of uncertainty in the state-space transitions would provide a better account of behavioral and fMRI data than would an arbitration model that was sensitive only to state-space uncertainty. We further hypothesized that under conditions of high state-space complexity, the model-based controller would be selected against, because the complexity of the state-space would overwhelm the planning requirements of the model-based system, forcing participants to rely instead on model-free control. Our findings support our first hypothesis, and partially support our second hypothesis.</p></sec><sec id="s2" hwp:id="sec-3"><title hwp:id="title-4">RESULTS</title><sec id="s2a" hwp:id="sec-4"><title hwp:id="title-5">Markov decision task with varying degrees of uncertainty and complexity</title><p hwp:id="p-13">To investigate the role of uncertainty and complexity in arbitration control, we designed a novel two-stage Markov decision process task (MDP; <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1A</xref>), in which we systematically manipulated two task variables across blocks of trials, state-transition uncertainty, and state-space complexity (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1B</xref>). The amount of state-transition uncertainty is controlled by means of the state-action-state transition probability. The state-action-state transition probability varies between the two conditions: high uncertainty (0.5 versus 0.5) and low uncertainty (0.9 versus 0.1). Switching between the two uncertainty conditions is designed to effect a change in the average amount of state prediction errors (SPE), thereby effectively translating into the reliability of the MB system. For instance, the high uncertainty condition will elicit a large amount of SPE, essentially resulting in a decrement in MB prediction performance. In the low uncertainty condition, the SPE will decrease or stay low on average as the MB refines an estimate of the state-action-state transition probabilities. On the other hand, the performance of MF is less affected by the amount of state-transition uncertainty (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-4" hwp:rel-id="ref-10">Daw et al., 2005</xref>). The second variable, the number of available choices, is intended to manipulate the task complexity. The total number of available choices is two and four in the low and high complexity condition, respectively. To prevent state-space representations from being too complex, we limit the number of available choices to two in the first stage of each trial, while the choice availability in the 2<sup>nd</sup> stages are either set to two or four. The manipulation of choice availability creates wide variability in the number of ways to achieve each goals, causing the difficulty level on each trial to range from easy to arduous. This design therefore provides four different types of conditions (<italic toggle="yes">low/high x uncertainty/complexity</italic>; see Figure S1). Participants then make sequential choices in order to obtain different colored tokens.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><title hwp:id="title-6">Task Design.</title><p hwp:id="p-14"><bold>(A)</bold> Two-stage Markov decision task. Participants choose between two to four options, followed by a transition according to a certain state-transition probability <italic toggle="yes">p</italic>, resulting in participants moving from one state to the other. The probability of a successful transitions to a desired state is proportional to the estimation accuracy of the state-transition probability, and it is constrained by the entropy of the true probability distribution of the state-transition. For example, the probability of a successful transition to a desired state cannot exceed 0.5 if <italic toggle="yes">p</italic>=(0.5, 0.5) (the highest entropy case). <bold>(B)</bold> Illustration of experimental conditions. (Left box) A low and high state-transition uncertainty condition corresponds to the state-transition probability p=(0.9,0.1) and (0.5,0.5), respectively. (Right box) The low and high state-space complexity condition corresponds to the case where two and four choices are available, respectively. In the first state, only two choices are always available, in the following state, two or four options are available depending on the complexity condition. <bold>(C)</bold> Participants make two sequential choices in order to obtain different colored tokens (silver, blue, and red) whose values change over trials. On each trial, participants are informed of the “currency”, i.e. the current values of each token. In each of the subsequent two states (represented by fractal images), they make a choice by pressing one of available buttons (L1, L2, R1, R2). Choice availability information is shown at the bottom of the screen; bold and light grey circles indicate available and unavailable choices, respectively. <bold>(D)</bold> Illustration of the task. Each grey circle indicates a state. Bold arrows and lines indicate participants’ choices and subsequent state-transition according to the state-transition probability, respectively. Each outcome state (state 4-11) is associated with a reward (colored tokens or no token represented by a grey mosaic image). The reward probability is 0.8.</p></caption><graphic xlink:href="393983_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-15">Another feature of the MDP is that on each trial participants could take actions in order to obtain one of three different tokens, a silver, red or blue token (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1C</xref>). On each trial, the relative value of the tokens, in terms of the rate of exchange of each token for real world money (US cents), was flexibly assigned, as revealed at the beginning of each trial. So for example, on a given trial, the silver token if won on that trial might yield 1 US cents, the red token, 9 US cents, and the blue token 3 cents, while the allocations could be different on the next trial. This design feature is intended to induce trial-by-trial changes in goal-values, thereby also inducing variance in reward-prediction errors and hence the reliability of MF across trials.</p><p hwp:id="p-16">Twenty-four adult participants (twelve females, age between 19 and 55) performed the task, and among them 22 participants were scanned with fMRI. The task performance of subjects in terms of both the total amount of earned reward and the proportion of optimal choices is significantly greater than chance level in all conditions (t-test; p&lt;1e-5).</p></sec><sec id="s2b" hwp:id="sec-5"><title hwp:id="title-7">Computational model of arbitration control incorporating uncertainty and complexity</title><p hwp:id="p-17">In our computational model, the dynamical interaction between MB and MF RL is based on a two-state transition model (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Dayan and Abbott, 2001</xref>) (see <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>). Note that this type of recurrent structure has been previously found to account well for the arbitration process between MB and MF RL in both behavioral (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-5" hwp:rel-id="ref-27">Lee et al., 2014</xref>) and fMRI data (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-6" hwp:rel-id="ref-27">Lee et al., 2014</xref>). In this model, each state depends on its previous state (an endogenous input) and inputs from the environment (exogenous variables), such as a state, reward, and perceived task complexity. In this model, preference for MB and MF RL, indicated by the model choice probability (P<sub>MB</sub>), is a function of prediction uncertainty and task complexity. The prediction uncertainty refers to estimation uncertainty about state-action-state transitions and rewards. They are computed based on the state prediction error (SPE) and the reward prediction error (RPE), the key variables for MB and MF learning, respectively. We specifically hypothesized that task complexity influences the transition between MB and MF. Note that when the environment is perfectly stable (i.e., a fixed amount of state-transition uncertainty and a fixed level of task complexity), the particulars of this model converge to a stable mixture of MB and MF RL (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Daw et al., 2011</xref>).</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><title hwp:id="title-8">Computational model of arbitration control incorporating uncertainty and complexity.</title><p hwp:id="p-18">The circle-and-arrow illustration depicts a two-state dynamic transition model, in which the current state depends on the previous state (an endogenous variable) and input from the environment (exogenous variables). The environmental input includes the state-transition which elicits state prediction errors (SPEs), rewards that elicit reward prediction error (RPEs), and the task complexity. The arrow refers to the transition rate from MB to MF RL or vice versa, which is a function of SPE, RPE, and task complexity. The circle refers to the state, defined as the probability of choosing MB RL (P<sub>MB</sub>). Q(s,a) refers to the values of the currently available actions (a) in the current state (s). The value is then translated into action, indicated by the action choice probability P(a|s).</p></caption><graphic xlink:href="393983_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-19">The process of our computational model is described as follows: first, in response to the agent’s action on each trial, the environment provides the model with the state-action-state transition, token values, and task complexity. These observations are then used to compute the transition rates (MB⟶MF and MF⟶MB), which subsequently determines the model choice probability PMB. Second, the model integrates MB and MF value estimations to compute an overall integrated action value (Q(s,a) of <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2</xref>), which is subsequently translated into an action (P(a|s) of <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2</xref>). It is noted that we use this framework to formally implement various hypotheses about the effect of uncertainty and complexity on RL. For instance, the configuration of the model that best accounts for subjects’ choice behavior would specify the way people combine MB and MF RL to tailor their behavior to account for the degree of uncertainty and complexity of the environment.</p></sec><sec id="s2c" hwp:id="sec-6"><title hwp:id="title-9">Effect of uncertainty and complexity on reinforcement learning</title><p hwp:id="p-20">To determine how task complexity is embedded into the arbitration control process, we formulated a variety of possible model implementations, which we could then fully permute and test in a large scale model comparison, described as follows:</p><p hwp:id="p-21">The first factor incorporated into the hypothesis set is the effect of complexity on arbitration control. We considered a number of different ways in which complexity could impact on the allocation of control between MF and MB systems. The first of these variables was the type of modulation, i.e. whether modulation was excitatory or inhibitory. That is, does complexity influence on the arbitration process positively or negatively? (see Methods – subsection (1) Sign of modulation). The second variable was the direction of modulation, that is does complexity effect the transition of behavioral control from model-free to model-based, from model-based to model-free, or does it affect the transition in both directions (see Methods – subsection (2) Direction of modulation). We then also considered the extent to which the effects of uncertainty also interacted with the effects of complexity, specifying three possibilities: the version that there is no interaction between complexity and uncertainty, and the other two versions that these two variables interact in different ways (see Methods – subsection (3) Type of modulation).</p><p hwp:id="p-22">The second factor is the effect of complexity on the implementation of the choice itself. That is, we tested whether task complexity impacted on the soft-max choice temperature that sets the stochasticity of the choices of the integrated model. For this we compared the case in which there is no effect of complexity on the choice temperature parameter (Null), and a case in which increasing complexity increases the degree of explorative choices, and when increasing complexity decreases the degree of explorative choices (see Methods – subsection 3. Effect of complexity on exploration).</p><p hwp:id="p-23">Another factor that we considered concerns the implementation of the model-free RL algorithm. Recall that on the beginning of each trial, the participant is presented with the current value of each of the three tokens. One possible implementation of model-free RL is that it could ignore those token values completely, and treat each trial the same irrespective of what values are signaled to accrue to the particular tokens. However, given these tokens are salient stimuli that signal specific states in their own right, such a possibility seems unlikely. Rather it seems more plausible that the token values would be embedded into the state space itself, on which the model-free RL agent learns. Accordingly, we built a modified model-free RL algorithm that divided up the task into three unique state-space representations depending on which token was the dominant goal (which depends on which token was the most valuable on a given trial), a model variant we call the 3Q model. Yet another, albeit much less plausible possibility is that 3 completely separate model-free strategies exist to learn about the separate values of each possible goal, which we call the 3MF model. Thus, we tested 3 classes of model-free agent, ranging from a simple model-free agent that doesn’t differentiate between token states, a model that treats the most valuable token as identifying one of three relevant subsets of the state-space (red token goal, blue token goal and silver token goal) and a model that assumes 3 independent model-free agents for each selected goal.</p><p hwp:id="p-24">The effect of uncertainty on the transition rate was implemented based on our previous model and findings (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-7" hwp:rel-id="ref-27">Lee et al., 2014</xref>). Each of these factors (and sub-factors) was fully tested in each possible combination with each other factor, rendering a total of 117 model variants, that also included as a baseline model, the original arbitration scheme from our 2014 paper that only incorporated uncertainty as a variable and not complexity (see <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2</xref>). We then compared the extent to which each of those models could explain the behavioral data using across all of these models. For this, we fit each version of the model to each individual subject’s data, and then ran a Bayesian model selection (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Stephan et al., 2009</xref>), with an exceedance probability <italic toggle="yes">p</italic>&gt;0.99 (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>; For more details, see Model Comparison in Supplemental Experimental Procedures).</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><title hwp:id="title-10">Model comparison analysis on behavioral data.</title><p hwp:id="p-25"><bold>(A)</bold> We ran a large scale Bayesian model selection analysis to compare different versions of arbitration control. These model variants were broadly classified as reflecting the effect of complexity on the transition between MB and MF RL (13 = 1 + 2×2×3 types), the effect of effect of complexity on exploration (3 variants), and the form of the MF controller (3 variants) each of which is classified by a type of goal-driven MF (3 types), an effect of complexity on transition between MB and MF RL, and an effect of complexity on exploration (3 types). Lee2014 refers to the original arbitration model (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-8" hwp:rel-id="ref-27">Lee et al., 2014</xref>). <bold>(B)</bold> Results of the Bayesian model selection analysis. Among a total of 117 versions, we show only 41 major cases for simplicity, including the original arbitration model and other 40 different versions that show non-trivial performance, (but the same result holds if running the full model comparison across the 117 versions). The model that best accounts for behavior is the version {3Q model, interaction type2, excitatory modulation on MF→MB, explorative} (exceedance probability &gt; 0.99; model parameters are shown in the supplementary Table S1).</p></caption><graphic xlink:href="393983_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-26">We found that one specific model variant provided a dominant account of the behavioral data, with an exceedance probability of 0.99. In the best fitting model variant, an increase in task complexity exerted a positive modulatory effect on the transition between model-free and model-based control. That is, the best-fitting model supported an effect of complexity on arbitration such that an increase in complexity produced an <italic toggle="yes">increased</italic> tendency to transition from model-free to model-based control. Recall, that this is not compatible with our initial hypothesis that increased complexity would generally tax the accuracy of the model-based controller, thereby resulting in an increase in model-free control. However, the best-fitting model also proscribed that uncertainty and complexity interact, such that under conditions of both high uncertainty AND high complexity, model-free control would become favored. We will describe in more detail the nature of this interaction in the section below. Secondly, the best-fitting model had the feature that increasing complexity increases the degree of exploration, favoring the hypothesis that subjects tend to explore more under conditions of high-task complexity. Finally, the best fitting model-variant also had the feature that the state-space for the model-free agent was sub-divided according to which goal was currently selected (assuming that the goal selected corresponded to the maximum token value) i.e. the 3Q model, as opposed to a single model-free agent that ignores token values, or separate model-free agents. In summary, we found evidence for an arbitration model that assumes an effect of both uncertainty and complexity on arbitration between MB and MF RL, and furthermore that these two variables interact to drive arbitration as detailed in the following section.</p></sec><sec id="s2d" hwp:id="sec-7"><title hwp:id="title-11">Tradeoff between model-based and model-free reinforcement learning in the presence of uncertainty and complexity</title><p hwp:id="p-27">To gain a better insight into the role of uncertainty and complexity together on choice of the MB vs MF RL strategy according to the best fitting model, we examined the behavior of the model for the four blocked experimental conditions: low/high uncertainty x low/high complexity (see <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4A</xref>). For this we examined the model weight P<sub>MB</sub> which is the (dynamically changing) weight assigned to the model-based controller through the arbitration process on a trial by trial basis according to the best fitting model. This P<sub>MB</sub> weight was binned and averaged within each subject according to whether or not the trial was high or low in complexity and high or low in state-space uncertainty, and the fitted model-weights were then averaged across participants. Note that these are model fits, and thereby illustrate the behavior of the model as fit to the behavioral data, rather than being directly informative about participants’ actual behavior. In essence, this is a way to understand the behavioral predictions of the model itself. When interrogating the fitted model in this way we found an effect of uncertainty and complexity on the weighting between model-based and model-free control (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4A</xref>; 2-way repeated measures ANOVA; <italic toggle="yes">p</italic>&lt;1e-4 for the main effect of both state-transition uncertainty and task complexity; <italic toggle="yes">p</italic>=0.039 for the interaction effect). Specifically, according to the model, MB control is preferred when the degree of task complexity increases, whereas MF is favored when the amount of state-space uncertainty increases. A more intriguing finding is that the increase in state-transition uncertainty tends to nullify the effect of task complexity or vice versa.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-28"><bold>(A)</bold> Degree of model-based engagement (weighting allocated to the MB strategy; P<sub>MB</sub>) predicted by our computational model of arbitration control (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3B</xref>). For this, we ran a deterministic simulation in which our computational model experiences exactly the same episode of events as each individual subject, and we generated the trial-by-trial output: the model-based weight (P<sub>MB</sub>). Error bars are SEM across subjects. <bold>(B)</bold> Participants’ choice optimality. To compute the choice optimality, we computed the degree of match between subjects’ actual choice and an ideal agent’s choice corrected for the number of options. For example, the max value 1 refers to the case in which a subject makes the same choice as the ideal agent’s. The ideal agent is assumed to have a full access to information of state-transition uncertainty and task complexity. Both of the model preference and the choice optimality were calculated for the four experimental conditions (low/high state-transition uncertainty x low/high task complexity). Shown in red boxes are the effect of the two experimental variables on each measure (2-way repeated measures ANOVA). Error bars are SEM across subjects.</p></caption><graphic xlink:href="393983_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-29">To provide a more direct test of the extent to which participants actual behavior is consistent with the arbitration model predictions, we defined an independent behavioral measure which we call choice optimality. Choice optimality quantifies the extent to which participants on a given trial took the objectively best choice had they complete access to the task state-space, and a perfect ability to plan actions in that state-space. It is defined as the ratio of trials on which the subject’s choice matches with the choice of the ideal agent assumed to have a full access to information of state-transition uncertainty and task complexity. Choice optimality is a good proxy of the extent to which participants engage in model-based control, because in principle assuming complete knowledge of the state-space and no cognitive constraints, the model-based agent will always choose more optimally than a model-free agent. Consistent with the model predictions illustrated in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4A</xref>, we found a strong effect of uncertainty and complexity on choice optimality in participant’s actual behavior (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4B</xref>; 2-way repeated measures ANOVA; <italic toggle="yes">p</italic>&lt;1e-4 for both the main effect and the interaction effect). Consistent with the prediction of the arbitration control model, the effect of complexity on choice optimality diminishes when the state-space uncertainty becomes high.</p><p hwp:id="p-30">In summary, these results provide both a computational and behavioral account of how participants regulate the tradeoff between MB and MF RL in the presence of uncertainty and task complexity: they tend to favor use of a MB RL strategy under conditions of high compared to low task complexity, while at the same time they tend to resort to MF RL when the amount of state-space uncertainty increases to the level at which the MB RL strategy can no longer provide reliable predictions. However, these variables interact such that under conditions of both high complexity and high uncertainty, model-free control is favored over and above the effects of each of these two variables alone.</p></sec><sec id="s2e" hwp:id="sec-8"><title hwp:id="title-12">Neural representations of model-based and model-free RL</title><p hwp:id="p-31">To we ran a general linear model analysis (GLM) on the fMRI data in which each variable of the computational model that best fit behavior is regressed against the fMRI data (see Methods for model-specification details). First, we replicated previous findings indicating neural encoding of prediction error signals, SPE and RPE, the two key variables necessary for the update of the value of the MB and MF RL (For full details, see the supplementary Table S2). Consistent with previous findings, we found that SPE signals were encoded in dorsolateral prefrontal cortex (dlPFC) (<italic toggle="yes">p</italic>&lt;0.05 family-wise error (FWE) corrected) (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">Gläscher et al., 2010</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-9" hwp:rel-id="ref-27">Lee et al., 2014</xref>) and RPE signals in the ventral striatum (significantly at <italic toggle="yes">p</italic>&lt;0.05 small volume corrected) (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-10" hwp:rel-id="ref-27">Lee et al., 2014</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">McClure et al., 2003</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">O’Doherty et al., 2003</xref>). Second, we replicated neural correlates of the chosen value signal for the MB and MF controllers. The MB value signal was associated with BOLD activity in multiple areas within the PFC (<italic toggle="yes">p</italic>&lt;0.05 cluster-level corrected), whereas the MF value signal was found in supplementary motor area (SMA) (<italic toggle="yes">p</italic>&lt;0.05 FWE corrected) and notably posterior putamen (significantly at <italic toggle="yes">p</italic>&lt;0.05 small volume corrected), which has previously been implicated in MF valuation (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Tricomi et al., 2009</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">Wunderlich et al., 2012</xref>). Third, we found evidence in the ventromedial prefrontal cortex (vmPFC) for an integrated value signal that combines model-based and model-free value predictions according to their weighted combination as determined by the arbitration process (Q(s,a) shown in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3B</xref>; significantly at <italic toggle="yes">p</italic>&lt;0.05 small volume corrected). In addition, we found evidence for the implementation of the goal-driven MF model (Figure S2), which is that the activity of medial frontal gyrus was found to be bilaterally correlated with the goal change signals (<italic toggle="yes">p</italic>&lt;0.05 FWE corrected), the information necessary for the agent to determine whether it caches out a MF value signal in order to achieve a new goal.</p></sec><sec id="s2f" hwp:id="sec-9"><title hwp:id="title-13">Arbitration signals in prefrontal cortex</title><p hwp:id="p-32">We then examined the fMRI data for evidence of signals pertaining to the arbitration process. Replicating our previous results (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-11" hwp:rel-id="ref-27">Lee et al., 2014</xref>), we found evidence that the inferior lateral prefrontal cortex (ilPFC) bilaterally encodes reliability signals for both the MB and the MF controllers alone (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig 5A</xref>). But what we found to be best correlated with the activity of ilPFC is the maximum of the reliability of the MB and MF systems, that is, when using a regressor in which the reliability value of whichever system was most reliable on a given trial is input as the value for that trial (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5A</xref>; <italic toggle="yes">p</italic>&lt;0.05 cluster-level corrected). These findings are again fully consistent with our previous findings (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-12" hwp:rel-id="ref-27">Lee et al., 2014</xref>).</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><title hwp:id="title-14">Neural signatures of model-free and model-based reinforcement learning and arbitration control.</title><p hwp:id="p-33"><bold>(A)</bold> Bilateral ilPFC encodes reliability signals for both the MB and the MF systems. Note that the two signals are not highly correlated (absolute mean correlation &lt; 0.3); this task design was previously shown to successfully dissociate the two types of RL (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-13" hwp:rel-id="ref-27">Lee et al., 2014</xref>). Threshold is set at p&lt;0.005. <bold>(B)</bold> (Left) Inferior-lateral prefrontal cortex bilaterally encodes reliability information on each trial of both MB and MF RL, as well as whichever strategy that provides more accurate predictions (“max reliability” (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-14" hwp:rel-id="ref-27">Lee et al., 2014</xref>)). (Right) The mean percent signal change for a parametric modulator encoding a max reliability signal in the inferior lateral prefrontal cortex (lPFC). The signal has been split into two equal-sized bins according to the 50th and 100th percentile. The error bars are SEM across subjects.</p></caption><graphic xlink:href="393983_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s2g" hwp:id="sec-10"><title hwp:id="title-15">Model comparison against fMRI data</title><p hwp:id="p-34">Next we aimed to formally test our hypothesis of uncertainty and complexity-sensitive arbitration control against the previous hypothesis that arbitration control is based solely on changes in prediction uncertainty alone (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-15" hwp:rel-id="ref-27">Lee et al., 2014</xref>). For this we compared two separate arbitration models against the fMRI data. One was the best fitting model described above in which both task complexity and reliability are taken into account as playing a role in driving the arbitration process. The second, was a model in which only reliability was involved in the arbitration process, as first utilized by Lee et al. (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-16" hwp:rel-id="ref-27">Lee et al., 2014</xref>). We then compared the fit of these two models to the fMRI data in two brain regions, ilPFC for the reliability signals and vmPFC for the valuation signals. For this we ran a Bayesian model selection analysis (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">Stephan et al., 2009</xref>), using spherical ROIs centered on the coordinates from our 2014 study, thereby ensuring independence of the ROI selection from the current dataset. In a majority of voxels in both regions of interest, reliability signals from the model incorporating both reliability and task complexity were preferred over the previous model incorporating reliability only (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref>). These findings support our hypothesis that the model in which complexity is taken into account provides a better account of prefrontal mediated arbitration control.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-6"><title hwp:id="title-16">Results of a Bayesian model selection analysis.</title><p hwp:id="p-35">The red blobs and table show the voxels and the number of voxels, respectively, that favor each model with an exceedance probability &gt; 0.95, indicating that the corresponding model provides a significantly better account for the BOLD activity in that region. <italic toggle="yes">Lee2014</italic> refers to an arbitration control that takes into account only uncertainty as used by (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-17" hwp:rel-id="ref-27">Lee et al., 2014</xref>), <italic toggle="yes">Current model</italic> refers to the arbitration control model that was selected in the model comparison based on the behavioral data which incorporates both prediction uncertainty and task complexity. For an unbiased test, the coordinates of the ilPFC and the vmPFC ROIs were taken from (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-18" hwp:rel-id="ref-27">Lee et al., 2014</xref>).</p></caption><graphic xlink:href="393983_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec><sec id="s2h" hwp:id="sec-11"><title hwp:id="title-17">Modulation of inferior prefrontal reliability signal by complexity</title><p hwp:id="p-36">The above findings demonstrate that reliability signals from an arbitration model that takes into account task complexity, provides a better account of fMRI activity than reliability signals derived from a model that does not incorporate complexity, reflecting an implicit contribution of complexity to the arbitration process. However, these findings do not provide direct evidence for an explicit representation of task complexity in the ventrolateral prefrontal arbitration region. To test for an explicit contribution of task complexity to the arbitration signal, we ran an additional fMRI analysis in which we included a parametric regressor denoting the onset of trials of high task vs low task complexity as a separate regressor of interest. We then entered another additional regressor, which corresponds to the formal interaction of Max reliability with task complexity. That is, to test for areas in which the reliability signal was modulated differently depending on whether a specific trial was high or low in complexity. While we found no significant effect of the main effect of task complexity in our main regions of interest (Table S2), we found evidence for a significant interaction effect of complexity and reliability. Specifically, a region of inferior lateral prefrontal cortex bilaterally was found to show a significant negative interaction between complexity and reliability (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. 7A</xref>). This region was found to overlap substantively with the regions of inferior lateral prefrontal cortex found to exhibit a main effect of reliability (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig. 7B</xref>). To visualize the effect of the interaction in this region we in a post-hoc analysis, extracted the average % signal change from the clusters exhibiting the interaction in left and right ilPFC respectively. We binned the signal according to whether reliability was high or low, and whether complexity was high or low, shown in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Fig. 7B</xref>. Reliability signals were plotted separately for model-free and model-based reliability, although the results are similar for max reliability. As can be seen, the reliability signals show evidence of being attenuated particularly when complexity is high relative to when complexity is low. This provides evidence that these two signals relevant for driving arbitration interact with each other in ilPFC.</p><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;393983v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-7"><p hwp:id="p-37">Bilateral ilPFC was found to exhibit a significant interaction between complexity and reliability (Max reliability x complexity). Statistical significance of the negative effects is illustrated by the cyan colormap. The threshold is set at p&lt;0.005. (Right) The brain region reflecting the interaction effect largely overlaps with the brain area implicated in arbitration control. The red and blue regions refers to the main effect of max reliability and the interaction between reliability and task complexity, respectively, thresholded at p&lt;0.001. <bold>(B)</bold> Plot of average signal change extracted from left and right ilPFC clusters showing the interaction, shown separately for reliability signals derived from the MF and MB controllers. Data are split into two equal-sized bins according to the 50th and 100th percentile of the reliability signal, and shown for the trials in the low and high complexity condition separately. The error bars are SEM across subjects.</p></caption><graphic xlink:href="393983_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec></sec><sec id="s3" hwp:id="sec-12"><title hwp:id="title-18">DISCUSSION</title><p hwp:id="p-38">We provide evidence supporting the interaction of two key variables in driving arbitration of control between model-based and model-free reinforcement learning. In addition to replicating our previous finding implicating the uncertainty (or reliability) of the predictions made by the model-based and model-free controllers in moderating the influence of these two systems over behavior, we found evidence that the complexity of the state-space, which is a contributor to the computational cost faced by the model-based controller, also contributes to the setting of the balance between these two systems. These behavioral findings were supported by evidence that a region of the brain previously implicated in the arbitration process, the inferior lateral prefrontal cortex not only encodes signals related to the reliability of the predictions of the two systems that would support an uncertainty-based arbitration mechanism, but furthermore that activity in this region is better accounted for by an arbitration model that also incorporates the effects of task complexity into the arbitration process. Moreover, we found evidence that task complexity and reliability appear to directly interact in this region. Taken together, these findings help to advance our understanding of the contribution of two key variables to the arbitration process at behavioral and neural levels.</p><p hwp:id="p-39">We found direct evidence for a contribution of task complexity on arbitration. First, in our large scale model comparison we found empirical support for a version of the arbitration process in which the complexity variable has a positive modulation effect on the transitions from MF to MB. Second, this is corroborated by the fact that the best fitting model exhibited an increased preference for MB over MF in the high complexity condition on average. Third, in an independent behavioral analysis which uses choice optimality to quantify the extent to which choice behavior is guided by MB RL, we found that subjects’ choice optimality increases with the degree of task complexity. These results together suggest that an increase in task complexity creates an overall bias towards MB RL, contrary to our initial hypothesis in which we considered that increased complexity would tax the model-based system resulting in increased model-free control. Another interesting finding supporting this idea is that an increase in task complexity makes choices more flexible and explorative. In summary, these findings suggest that humans attempt to resolve task complexity by engaging a more explorative MB RL strategy.</p><p hwp:id="p-40">We also found an effect of state-space uncertainty on the arbitration process. Specifically, very high state-space uncertainty makes subjects resort more to a MF RL strategy. This effect arises because high state-space uncertainty results in a lowered reliability of the predictions of the model-based controller, thereby resulting in a reduced contribution of behavior of the model-based controller. It should be noted that, the model-based controller should generally compute a more accurate prediction than its model-free counterpart, which by contrast necessarily generates approximate value predictions. However, this holds only under the situation where the model-based controller has access to a reliable model of the state-space. If its state-space model is not reliable or accurate, then the model-based controller cannot generate accurate predictions about the value of different actions. In this task, we influence the extent to which the model-based controller has access to reliable predictions about the state-space by directly modulating the state-space uncertainty. Thus, under conditions in which the state-space model is highly unreliable, humans appear to rely more on model-free control. Conversely, as we have shown previously (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-19" hwp:rel-id="ref-27">Lee et al., 2014</xref>), if the reliability of the model-free controller is decreased, participants will all else being equal rely more on model-based control. Thus, the uncertainty in the predictions of these controllers, appears to play a key role in underpinning the arbitration process between them.</p><p hwp:id="p-41">In addition to the main effects of complexity and state-space uncertainty, we have shown that these two variables interact. Under conditions where both state-space uncertainty is high and complexity is high, the model-based system appears to be disproportionately affected, in that participants abandon model-based control in favor of model-free control. Thus, our hypothesis about an effect of complexity resulting in decreased model-based control was borne out in a qualified manner, in that this effect only happens when state-space uncertainty is high. This finding suggests that the arbitration process takes into account the effects of both of these variables at the same time, and dynamically finds a tradeoff between them. Participants appear to use the MB RL strategy to resolve uncertainty and complexity, but owing to the fact that MB RL is more cognitively demanding than MF RL, they resort to the default strategy, MF RL, when the performance gain of MB RL does not outweigh the level of cognitive load required for MB RL.</p><p hwp:id="p-42">The present findings are also relevant to the predictions of expected value of control (EVC) theory (<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Narayanan et al., 2013</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Shenhav et al., 2013</xref>). One prediction of the EVC theory is that increased control signal intensity would lock the MB system in, thereby letting the MF system take over control over behavior. Another related finding is that MB learning with prior training remains intact under cognitive load (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Economides et al., 2015</xref>). While the theory predicts that increasing task difficulty brings about increase in the intensity of the cognitive control signal, the theory itself does not offer any predictions about how control signal intensity influences RL. Our computational model explains how the brain chooses between MB and MF RL with varying amount of cognitive control intensity, and furthermore, why this choice is made.</p><p hwp:id="p-43">The model comparison analysis of the present study also revealed that task complexity affects transitions from MF to MB RL, but not transitions in the other direction. This finding provides further evidence to support the existence of an asymmetry in arbitration control such that arbitration is performed in a way that selectively gates the MF system (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-20" hwp:rel-id="ref-27">Lee et al., 2014</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-3" hwp:rel-id="ref-49">Wunderlich et al., 2012</xref>). These results may be reasonable from an evolutionary perspective in that the implementation of MF learning in parts of the basal ganglia may have arisen earlier on in the evolutionary history of adaptive intelligence, while later on, cortically mediated MB control may have emerged so as to deal with more complex situations.</p><p hwp:id="p-44">Our study also advances understanding into the nature of the computations being implemented in the inferior prefrontal cortex during the arbitration process. This region was found to correlate not only with the reliability of the predictions of the two control systems, as shown previously (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-21" hwp:rel-id="ref-27">Lee et al., 2014</xref>), but this region also was found to incorporate information about task complexity. The reliability signal itself was found to reflect the effects of task complexity, as shown by the formal model comparison in which activity in this region was better accounted for by an arbitration model that incorporated task complexity compared to a model that ignores task complexity. Moreover, we found that when testing directly for an interaction between the reliability signals and complexity, an overlapping region of inferior prefrontal cortex was found to show evidence for a significant interaction between these signals. These findings therefore demonstrate that task complexity directly modulates the putative neural correlates of the arbitration process.</p><p hwp:id="p-45">While our findings advance our understanding of the nature of the arbitration process, it is also important to acknowledge that a number of open questions remain. A fundamental open question concerns how the arbitration computations within inferior prefrontal cortex are actually implemented at the neuronal level. While our findings show correlations between various arbitration-related signals in this region such as reliability and complexity, how these signals are utilized at the neural level to implement the arbitration process is not yet addressed. Building on the present study and earlier studies investigating executive control mechanisms in prefrontal-striatal circuitry (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Aron et al., 2003</xref>, <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">2004</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">Balleine and O’Doherty, 2010</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Burguière et al., 2013</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Cockburn and Frank, 2011</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Coutureau and Killcross, 2003</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Donoso et al., 2014</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Gremel and Costa, 2013</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Koechlin et al., 2003</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Robbins, 1996</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Rushworth et al., 2011</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Smith et al., 2012</xref>; <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Tanji and Hoshi, 2008</xref>), more biologically plausible models of the arbitration process will need to be developed to go beyond the algorithmic level used in the present study. Furthermore, in order to guide the development of such models it will ultimately be important to get a better understanding of the underlying neuronal dynamics in these prefrontal regions during the arbitration process, using techniques with better spatial and temporal resolution than available with fMRI.</p><p hwp:id="p-46">An important limitation of the task we have used here, is that we have studied behavior under conditions of high instability and/or variability in terms of the transitions between model-based and model-free control. This is done by design, because to detect arbitration related signals both behaviorally and neurally within the framework of an fMRI study, we needed to maximize the variance in the transition between these two different forms of behavioral control. However, in real-world behavior, it would be expected that the transitions between model-based and model-free control would typically be evolving at a slower pace. One of the main advantages of model-free control is the lower computational cost entailed by engaging cached values learned via model-free RL compared to model-based RL. A natural consequence of this is that the model-based system should cease computing action-values when the model-free system is in control, as otherwise the computational cost advantage gained by increasing model-free control would be moot. A limitation of the present arbitration model is that it assumes that model-based values continue to be computed throughout the task. This is so because we did not find behavioral evidence that such signals ceased to be estimated during the task, which would be manifested at the behavioral level by a complete dominance of model-free control over behavior. However, we suspect that in real-world behavior, the model-based system would eventually go offline, and this should be reflected ultimately in behavioral dominance of model-free control. More generally, it will be important to study the behavior of model-based and model-free controllers across a wide range of tasks and experimental conditions in order to gain a more complete understanding of the nature of the arbitration process.</p><p hwp:id="p-47">Finally, it should be noted that the cognitive complexity manipulation we used here by which we increased the number of available state-spaces available in the decision problem, can also impact on the model-free system, because the increase in the number of actions that need to be learned, means that the model-free system has less opportunity to sample those state-action pairs thereby having less opportunity to acquire accurate value representations. Thus, the trade-off faced by the model-based and model-free system under these conditions is more complicated than merely reflecting the sole effects of computational cost on the model-based system. Future studies could therefore focus on more clearly separating the effects of computational load from sampling history.</p><p hwp:id="p-48">In conclusion, we provide behavioral and neural evidence for the effect of prediction uncertainty and task complexity on RL. First, the present findings provide a theoretical insight into how the brain dynamically combines different RL strategies to deal with uncertainty and complexity. Second, the findings help us decipher the asymmetrical nature of arbitration control – supporting the notion that the primary function of arbitration control is to regulate the MF system, as opposed to exerting control on both systems to an equal extent. Third, we found that such an arbitration control principle is best reflected in neural activity patterns in the inferior lateral prefrontal cortex, the same area we previously found to play a pivotal role in arbitration control, thereby significantly advancing our understanding of the theory of prefrontal arbitration between MB and MF RL. Taken together, the findings in the present study foster a deeper appreciation of the role of ventrolateral prefrontal cortex in arbitration control.</p><sec id="s3a" hwp:id="sec-13"><title hwp:id="title-19">Experimental Procedures</title><sec id="s3a1" hwp:id="sec-14"><title hwp:id="title-20">Participants</title><p hwp:id="p-49">Twenty four right-handed volunteers (ten females, with an age range between 19 and 55) participated in the study, 22 of whom were scanned with fMRI. They were screened prior to the experiment to exclude those with a history of neurological or psychiatric illness. All participants gave informed consent, and the study was approved by the Institutional Review Board of the California Institute of Technology.</p></sec><sec id="s3a2" hwp:id="sec-15"><title hwp:id="title-21">Stimuli</title><p hwp:id="p-50">The image set for the experiment consisted of 126 fractal images to represent states, three kinds of color coins (red, blue, and silver), and four kinds of fractal images to represent outcome states associated with each color coin (red, blue, silver, and none). The colors of the outcome state image were accompanied by numerical amounts which indicate the amount of money that subjects could receive in that state. Before the experiment began, the stimulus computer randomly chose a subset of eleven fractal images to be subsequently used to represent each state in that specific participant.</p></sec><sec id="s3a3" hwp:id="sec-16"><title hwp:id="title-22">Task</title><p hwp:id="p-51">Participants performed a sequential two-choice Markov decision task, in which they need to make two sequential choices (by pressing one of four buttons: L1, L2, R1, R2) to obtain a monetary outcome (token) at the end stage. Making no choice in 4 seconds had a computer make a random choice to proceed and that trial was marked as a penalizing trial. Each trial begins with a presentation of values of each token in that trial, followed by a presentation a fractal image representing a starting state. The presentation of each state is accompanied by choice availability information shown at the bottom of the screen. Only two choices (L1 and R1) are available in the starting state (S1). The starting state is the same across all trials. Making a choice in the starting state is followed by a presentation of another fractal image representing one of ten states (S2-S11). The states were intersected by a variable temporal interval drawn from a uniform distribution between 1 to 4 seconds. The inter-trial interval was also sampled from a uniform distribution between 1 to 4 seconds. The reward was displayed for 2 seconds. At the beginning of the experiment, subjects were informed that they need to learn about the states and corresponding outcomes to collect as many coins as possible and that they will get to keep the money they cumulatively earned at the end of the experiment. Participants were not informed about the specific state-transition probabilities used in the task except they were told that the contingencies might change during the course of the experiment. In the pre-training session, to give participants an opportunity to learn about the task structure, they were given 100 trials in which they can freely navigate the state space by making any choices. During this session, the state-transition probability was fixed at (0.5,0.5) and the values of all color tokens are fixed at 5, indicating that any token color would yield the same amount of monetary reward. The experiment proceeded in five separate scanning sessions of 80 trials each on average.</p><p hwp:id="p-52">In order to effectively dissociate the model-free strategy from the model-based, the experimental design of the present study introduces two task parameters: specific goal-condition and state transition probabilities. First, to create a situation in which the model-based control is preferred over the model-free control, the present experimental design introduced a generalized version of the specific goal-condition (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-22" hwp:rel-id="ref-27">Lee et al., 2014</xref>), in which all token values are randomly drawn from a uniform probability distribution U(1,10) from trial-to-trial. If participants reached the outcome state associated with a token, then they would gain the corresponding monetary amount. Note that this goal-value manipulation is intended to encourage participants to act on a stable model-based control strategy, as opposed to developing separate multiple model-free strategies for each color tokens in the absence of the model-based control.</p><p hwp:id="p-53">Second, to create a situation where the model-free control overrides the model-based control and to further dissociate the model-free from the model-based, changes to the state transition probabilities were implemented. Two types of state-transition probability were used – (0.9,0.1) and (0.5,0.5) (a <italic toggle="yes">low</italic> and a <italic toggle="yes">high state-transition uncertainty condition</italic>, respectively). They are the probabilities that the choice is followed by going into the two consecutive states. For example, if you make a left choice at the state 1 and the state transition probability is (0.9,0.1) at that moment, then the probability of your next state being state 2 is 0.9 and the probability for state 3 is 0.1. The order of the block conditions was randomized. The blocks with the state transition probability (0.9,0.1) consists of three to five trials, whereas those with (0.5,0.5) consists of five to seven trials; it was previously shown that with (0.9,0.1) participants feel that the state transition is congruent with the choice, whereas with (0.5,0.5) the state transition is random (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-23" hwp:rel-id="ref-27">Lee et al., 2014</xref>). Furthermore, the changes at these rates ensures that tonically varying changes in model-based vs model-free control can be detected at experimental frequencies appropriate for fMRI data. The state-transition probability value was not informed to participants; estimation of state-transition probabilities can be made by using the model-based strategy.</p><p hwp:id="p-54">To manipulate the state-space complexity, the present study also introduced the third task parameter, the number of available choices. Two types of choice sets were used – (L, R) and (L1, L2, R1, R2) (a <italic toggle="yes">low</italic> and a <italic toggle="yes">high state-space complexity condition</italic>, respectively). The order of the block conditions was randomized. To preclude the task being too complex, changes in the number of available choices occur only in the second stage of each trial, while in the first stage the number is always limited to two (L and R).</p></sec><sec id="s3a4" hwp:id="sec-17"><title hwp:id="title-23">Computational model of arbitration</title><p hwp:id="p-55">Computational models of arbitration used in this study are based on the previous proposal of arbitration control (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-24" hwp:rel-id="ref-27">Lee et al., 2014</xref>). The original arbitration model uses a dynamic two-state transition (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Dayan and Abbott, 2001</xref>) to determine the extent to which the control is allocated to a model-based learner (MB) (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-3" hwp:rel-id="ref-19">Gläscher et al., 2010</xref>) and to a model-free SARSA learner (MF) (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Sutton and Barto, 1998</xref>) at each moment in time. Specifically, the change of the control weight <italic toggle="yes">P</italic><sub>MB</sub> (the probability of choosing a model-based strategy) is given by the difference between two types of transition: MF→MB and MB→MF:
<disp-formula id="eqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="393983_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
</p><p hwp:id="p-56">Where <italic toggle="yes">α</italic>, <italic toggle="yes">β</italic> refers to the transition rate MF→MB and MB→MF, respectively.</p><p hwp:id="p-57">The transition rate <italic toggle="yes">α</italic> (MF→MB) is found to be a function of reliability of the MF strategy that reflects the average amount of reward prediction error (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-25" hwp:rel-id="ref-27">Lee et al., 2014</xref>):
<disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="393983_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
where <italic toggle="yes">x</italic> refers to MF reliability and the two free parameters <italic toggle="yes">A</italic>, <italic toggle="yes">B</italic> refers to the maximum transition rate and the steepness, respectively. Likewise, the transition rate <italic toggle="yes">β</italic> (MB→MF) is defined as a function of MB reliability that reflects the posterior estimation of the amount of state prediction error (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-26" hwp:rel-id="ref-27">Lee et al., 2014</xref>).</p></sec></sec><sec id="s3b" hwp:id="sec-18"><title hwp:id="title-24">Computational hypotheses on arbitration incorporating prediction uncertainty and task complexity</title><sec id="s3b1" hwp:id="sec-19"><label>1.</label><title hwp:id="title-25">Goal-driven MF type</title><p hwp:id="p-58">To test the hypothesis on ‘goal-driven MF’ (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3A</xref>), we implemented the following versions.</p><list list-type="order" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-59">1MF model : refers to the null hypothesis that the MB and the goal-independent MF interacts.</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-60">3Q model : refers to the hypothesis that the MB interacts with a single MF with goal-dependent state-action value sets. Specifically, the MF learns a state x action x goal(red/blue/silver) value matrix with a single learning rate.</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-61">3MF model : refers to the hypothesis that the MB interacts with goal-dependent multiple MFs. Specifically, each goal is associated with an independent MF (red, blue, and silver) with a separate learning rate.</p></list-item></list></sec><sec id="s3b2" hwp:id="sec-20"><label>2.</label><title hwp:id="title-26">Effect of complexity on transition between MB and MF RL</title><p hwp:id="p-62">To test the effect of the state-space complexity on arbitration control, we define a transition rate as a function of both reliability and complexity (see the right box of <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figure 3A</xref>). The following variants of the transition function (2) were used.</p><list list-type="order" hwp:id="list-2"><list-item hwp:id="list-item-4"><p hwp:id="p-63">Type of interaction</p><p hwp:id="p-64">For simplicity, we only show the variants of the transition rate <italic toggle="yes">α</italic> (MF→MB). The same rule can be applicable to the transition rate <italic toggle="yes">β</italic>.</p><p hwp:id="p-65">-Null : assumes that there is no complexity effect on arbitration control; refer to the equation (2).</p><p hwp:id="p-66">-Interaction1 : assumes that there is a direct interaction effect (complexity (z) x reliability (x)) on arbitration control.
<disp-formula id="eqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="393983_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula>
</p><p hwp:id="p-67">-Interaction2 : assumes that there is a indirect interaction effect on arbitration control. Although there is no interaction term (zx), the transition rate is a function of both complexity (z) and reliability (x).
<disp-formula id="eqn4" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="393983_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula>
</p></list-item><list-item hwp:id="list-item-5"><p hwp:id="p-68">Sign of modulation</p><p hwp:id="p-69">-Positive : assumes that the complexity has a positive influence on the transition rate. We set z=1 and 2 for a low and high complexity condition, respectively.</p><p hwp:id="p-70">-Negative : assumes that the complexity has a negative influence on the transition rate. We set z=2 and 1 for a low and high complexity condition, respectively.</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-71">Direction of modulation</p><p hwp:id="p-72">Bidirectional : assumes that the complexity influence the both transition rates MB→MF and MF→MB. This means that the above rules (the type of interaction and the sign of modulation) are applied to both transition rates.</p><p hwp:id="p-73">-MB→MF : assumes that the complexity influence the transition rates MB→MF only.</p><p hwp:id="p-74">-MF→MB : assumes that the complexity influence the both transition rates MF→MB only.</p></list-item></list><p hwp:id="p-75">To test the effect of the state-space complexity on exploration, we define an exploration parameter as a function of complexity (see the bottom-left box of <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 3A</xref>).
<disp-formula id="eqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="393983_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula>
</p></sec><sec id="s3b3" hwp:id="sec-21"><label>3.</label><title hwp:id="title-27">Effect of complexity on exploration</title><p hwp:id="p-76">To test the hypothesis that increasing complexity increases the degree of explorative choices (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3A</xref>), we set τ(<italic toggle="yes">z</italic>) = 1 <italic toggle="yes">and</italic> 0.5 for the low and high complexity condition, respectively. For testing the hypothesis that increasing complexity decreases the degree of explorative choices, we set τ(<italic toggle="yes">z</italic>) = 0.5 <italic toggle="yes">and</italic> 1.</p><p hwp:id="p-77">Note that we compared prediction performance of all combinations of the above cases, and for simplicity we showed the results of only 41 major cases (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Figure 3B</xref>); in most of cases prediction performance is far below than the stringent threshold (exceedance probability <italic toggle="yes">p</italic>=1e-3).</p></sec><sec id="s3b4" hwp:id="sec-22"><title hwp:id="title-28">Relationship between our computational model and a simple mixture of MB and MF RL</title><p hwp:id="p-78">In a stable environment (i.e., a fixed amount of state-transition uncertainty and a fixed level of task complexity), the state of our computational model converges to a fixed point. This is specified by the steady-state model choice probability:
<disp-formula id="eqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="393983_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives>
</disp-formula>
</p><p hwp:id="p-79">Then by using (1), we get
<disp-formula id="ueqn1" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="393983_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-14"/></alternatives>
</disp-formula>
</p><p hwp:id="p-80">Note that this is the equivalent of a simple mixture of MB and MF RL (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-4" hwp:rel-id="ref-11">Daw et al., 2011</xref>).</p></sec><sec id="s3b5" hwp:id="sec-23"><title hwp:id="title-29">fMRI data acquisition</title><p hwp:id="p-81">Functional imaging was performed on a 3T Siemens (Erlangen, Germany) Tim Trio scanner located at the Caltech Brain Imaging Center (Pasadena, CA) with a 32 channel radio frequency coil for all the MR scanning sessions. To reduce the possibility of head movement related artifact, participants’ heads were securely positioned with foam position pillows. High resolution structural images were collected using a standard MPRAGE pulse sequence, providing full brain coverage at a resolution of 1 mm × 1 mm × 1 mm. Functional images were collected at an angle of 30° from the anterior commissure-posterior commissure (AC-PC) axis, which reduced signal dropout in the orbitofrontal cortex. Forty-five slices were acquired at a resolution of 3 mm × 3 mm × 3 mm, providing whole-brain coverage. A one-shot echo-planar imaging (EPI) pulse sequence was used (TR = 2800 ms, TE = 30 ms, FOV = 100 mm, flip angle = 80°).</p></sec><sec id="s3b6" hwp:id="sec-24"><title hwp:id="title-30">fMRI data analysis</title><p hwp:id="p-82">The SPM12 software package was used to analyze the fMRI data (Wellcome Department of Imaging Neuroscience, Institute of Neurology, London, UK). The first four volumes of images were discarded to avoid T1 equilibrium effects. Slice-timing correction was applied to the functional images to adjust for the fact that different slices within each image were acquired at slightly different points in time. Images were corrected for participant motion, spatially transformed to match a standard echo-planar imaging template brain, and smoothed using a 3D Gaussian kernel (6 mm FWHM) to account for anatomical differences between participants. This set of data was then analyzed statistically. A high-pass filter with a cutoff at 129 seconds was used. Full details of the GLM design matrix are provided in Supplementary Methods.</p></sec><sec id="s3b7" hwp:id="sec-25"><title hwp:id="title-31">Bayesian model selection analyses on fMRI data</title><p hwp:id="p-83">To formally test which version of arbitration control provides the best account of responses in inferior lateral prefrontal cortex, we ran a Bayesian model selection (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-3" hwp:rel-id="ref-44">Stephan et al., 2009</xref>). We chose three models -<italic toggle="yes">α</italic>, <italic toggle="yes">β</italic>, <italic toggle="yes">α</italic>, <italic toggle="yes">β</italic><sub>2</sub><italic toggle="yes">α</italic><sub>2</sub>, <italic toggle="yes">β</italic> the original arbitration model (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-27" hwp:rel-id="ref-27">Lee et al., 2014</xref>) and the two other versions that we found in Bayesian model selection analysis on behavioral data exhibit the second best and the best performance, respectively. We used a spherical ROI centered on the coordinates from the previous study (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-28" hwp:rel-id="ref-27">Lee et al., 2014</xref>) with a radius of 10mm.</p></sec></sec></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-32">Acknowledgements</title><p hwp:id="p-84">We thank Peter Dayan for insightful comments and Ralph Lee for his assistance. This work was funded by grants R01DA033077 and R01DA040011 to J.P.O.D. from the National Institute on Drug Abuse. This work was also supported by an Institute for Information &amp; Communications Technology Promotion (IITP) grant funded by the Korea government (No. 2017-0-00451), the ICT R&amp;D program of MSIP/IITP. [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion], the Brain Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT &amp; Future Planning (NRF-2016M3C7A1914448), the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2017R1C1B2008972), the research fund of the Korea Advanced Institute of Science and Technology (KAIST) under Grant code G04150045, and Samsung Research Funding Center of Samsung Electronics under Project Number SRFC-TC1603-06.</p></ack><sec id="s4" hwp:id="sec-26"><title hwp:id="title-33">Author contributions</title><p hwp:id="p-85">SL and JOD conceived and designed the study. SL implemented the behavioral task and ran the fMRI study. DK, GP, and SL designed computational models and analyzed the data. SL, JOD, and DK wrote the paper. All authors approved the final version for submission.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-34">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Akam T."><surname>Akam</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Costa R."><surname>Costa</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-2">Simple Plans or Sophisticated Habits? State, Transition and Learning Interactions in the Two-Step Task</article-title>. <source hwp:id="source-1">PLoS Comput. Biol</source>. <volume>11</volume>, <fpage>e1004648</fpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Aron A.R."><surname>Aron</surname>, <given-names>A.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fletcher P.C."><surname>Fletcher</surname>, <given-names>P.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bullmore E.T."><surname>Bullmore</surname>, <given-names>E.T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sahakian B.J."><surname>Sahakian</surname>, <given-names>B.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Robbins T.W."><surname>Robbins</surname>, <given-names>T.W.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-3">Erratum: Stop-signal inhibition disrupted by damage to right inferior frontal gyrus in humans</article-title>. <source hwp:id="source-2">Nat. Neurosci</source>. <volume>6</volume>, <fpage>115</fpage>–<lpage>116</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Aron A.R."><surname>Aron</surname>, <given-names>A.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Robbins T.W."><surname>Robbins</surname>, <given-names>T.W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Poldrack R.A."><surname>Poldrack</surname>, <given-names>R.A.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-4">Inhibition and the right inferior frontal cortex</article-title>. <source hwp:id="source-3">Trends Cogn. Sci</source>. <volume>8</volume>, <fpage>170</fpage>–<lpage>177</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2 xref-ref-4-3"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Balleine B.W."><surname>Balleine</surname>, <given-names>B.W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dickinson A."><surname>Dickinson</surname>, <given-names>A.</given-names></string-name> (<year>1998</year>). <article-title hwp:id="article-title-5">Goal-directed instrumental action: contingency and incentive learning and their cortical substrates</article-title>. <source hwp:id="source-4">Neuropharmacology</source> <volume>37</volume>, <fpage>407</fpage>–<lpage>419</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Balleine B.W."><surname>Balleine</surname>, <given-names>B.W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="O’Doherty J.P."><surname>O’Doherty</surname>, <given-names>J.P.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-6">Human and Rodent Homologies in Action Control: Corticostriatal Determinants of Goal-Directed and Habitual Action</article-title>. <source hwp:id="source-5">Neuropsychopharmacology</source> <volume>35</volume>, <fpage>48</fpage>–<lpage>69</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Beierholm U.R."><surname>Beierholm</surname>, <given-names>U.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anen C."><surname>Anen</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Quartz S."><surname>Quartz</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bossaerts P."><surname>Bossaerts</surname>, <given-names>P.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-7">Separate encoding of model-based and model-free valuations in the human brain</article-title>. <source hwp:id="source-6">Neuroimage</source> <volume>58</volume>, <fpage>955</fpage>–<lpage>962</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Burguière E."><surname>Burguière</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Monteiro P."><surname>Monteiro</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Feng G."><surname>Feng</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Graybiel A.M."><surname>Graybiel</surname>, <given-names>A.M.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-8">Optogenetic stimulation of lateral orbitofronto-striatal pathway suppresses compulsive behaviors</article-title>. <source hwp:id="source-7">Science</source> <volume>340</volume>, <fpage>1243</fpage>–<lpage>1246</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="book" citation-type="book" ref:id="393983v1.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Cockburn J."><surname>Cockburn</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Frank M."><surname>Frank</surname>, <given-names>M.</given-names></string-name> (<year>2011</year>). <chapter-title>Reinforcement Learning, Conflict Monitoring, and Cognitive Control: An Integrative Model of Cingulate-Striatal Interactions and the ERN</chapter-title>. <source hwp:id="source-8">In Neural Basis of Motivational and Cognitive Control</source>, <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Mars R."><given-names>R.</given-names> <surname>Mars</surname></string-name>, <string-name name-style="western" hwp:sortable="Sallet J."><given-names>J.</given-names> <surname>Sallet</surname></string-name>, <string-name name-style="western" hwp:sortable="Rushworth M."><given-names>M.</given-names> <surname>Rushworth</surname></string-name>, and <string-name name-style="western" hwp:sortable="Yeung N."><given-names>N.</given-names> <surname>Yeung</surname></string-name></person-group>, eds. (<publisher-name>MIT Press</publisher-name>), pp. <fpage>311</fpage>–<lpage>331</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Coutureau E."><surname>Coutureau</surname>, <given-names>E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Killcross S."><surname>Killcross</surname>, <given-names>S.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-9">Inactivation of the infralimbic prefrontal cortex reinstates goal-directed responding in overtrained rats</article-title>. <source hwp:id="source-9">Behav. Brain Res</source>. <volume>146</volume>, <fpage>167</fpage>–<lpage>174</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2 xref-ref-10-3 xref-ref-10-4"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niv Y."><surname>Niv</surname>, <given-names>Y.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-10">Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source hwp:id="source-10">Nat. Neurosci</source>. <volume>8</volume>, <fpage>1704</fpage>–<lpage>1711</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3 xref-ref-11-4"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S.J."><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour B."><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dolan R.J."><surname>Dolan</surname>, <given-names>R.J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-11">Model-Based Influences on Humans’ Choices and Striatal Prediction Errors</article-title>. <source hwp:id="source-11">Neuron</source> <volume>69</volume>, <fpage>1204</fpage>–<lpage>1215</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><citation publication-type="book" citation-type="book" ref:id="393983v1.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Abbott L.F."><surname>Abbott</surname>, <given-names>L.F.</given-names></string-name> (<year>2001</year>). <source hwp:id="source-12">Theoretical neuroscience : computational and mathematical modeling of neural systems</source> (<publisher-name>Massachusetts Institute of Technology Press</publisher-name>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Dickinson A."><surname>Dickinson</surname>, <given-names>A.</given-names></string-name> (<year>1985</year>). <article-title hwp:id="article-title-12">Actions and Habits: The Development of Behavioural Autonomy</article-title>. <source hwp:id="source-13">Philos. Trans. R. Soc. B Biol. Sci</source>. <volume>308</volume>, <fpage>67</fpage>–<lpage>78</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Doll B.B."><surname>Doll</surname>, <given-names>B.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon D.A."><surname>Simon</surname>, <given-names>D.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-13">The ubiquity of model-based reinforcement learning</article-title>. <source hwp:id="source-14">Curr. Opin. Neurobiol</source>. <volume>22</volume>, <fpage>1075</fpage>–<lpage>1081</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Doll B.B."><surname>Doll</surname>, <given-names>B.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bath K.G."><surname>Bath</surname>, <given-names>K.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Frank M.J."><surname>Frank</surname>, <given-names>M.J.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-14">Variability in Dopamine Genes Dissociates Model-Based and Model-Free Reinforcement Learning</article-title>. <source hwp:id="source-15">J. Neurosci</source>. <volume>36</volume>, <fpage>1211</fpage>–<lpage>1222</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Donoso M."><surname>Donoso</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Collins A.G.E."><surname>Collins</surname>, <given-names>A.G.E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Koechlin E."><surname>Koechlin</surname>, <given-names>E.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-15">Human cognition. Foundations of human reasoning in the prefrontal cortex</article-title>. <source hwp:id="source-16">Science</source> <volume>344</volume>, <fpage>1481</fpage>–<lpage>1486</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Samejima K."><surname>Samejima</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Katagiri K."><surname>Katagiri</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kawato M."><surname>Kawato</surname>, <given-names>M.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-16">Multiple Model-Based Reinforcement Learning</article-title>. <source hwp:id="source-17">Neural Comput</source>. <volume>14</volume>, <fpage>1347</fpage>–<lpage>1369</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Economides M."><surname>Economides</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kurth-Nelson Z."><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lübbert A."><surname>Lübbert</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Guitart-Masip M."><surname>Guitart-Masip</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dolan R.J."><surname>Dolan</surname>, <given-names>R.J.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-17">Model-Based Reasoning in Humans Becomes Automatic with Training</article-title>. <source hwp:id="source-18">PLOS Comput. Biol</source>. <volume>11</volume>, <fpage>e1004463</fpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2 xref-ref-19-3"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Gläscher J."><surname>Gläscher</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N."><surname>Daw</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="O’Doherty J.P."><surname>O’Doherty</surname>, <given-names>J.P.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-18">States versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning</article-title>. <source hwp:id="source-19">Neuron</source> <volume>66</volume>, <fpage>585</fpage>–<lpage>595</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Graybiel A.M."><surname>Graybiel</surname>, <given-names>A.M.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-19">Habits, Rituals, and the Evaluative Brain</article-title>. <source hwp:id="source-20">Annu. Rev. Neurosci</source>. <volume>31</volume>, <fpage>359</fpage>–<lpage>387</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Gremel C.M."><surname>Gremel</surname>, <given-names>C.M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Costa R.M."><surname>Costa</surname>, <given-names>R.M.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-20">Orbitofrontal and striatal circuits dynamically encode the shift between goal-directed and habitual actions</article-title>. <source hwp:id="source-21">Nat. Commun</source>. <volume>4</volume>, <fpage>2264</fpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Gruner P."><surname>Gruner</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anticevic A."><surname>Anticevic</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Pittenger C."><surname>Pittenger</surname>, <given-names>C.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-21">Arbitration between Action Strategies in Obsessive-Compulsive Disorder</article-title>. <source hwp:id="source-22">Neurosci</source>. <volume>22</volume>, <fpage>188</fpage>–<lpage>198</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Hasz B.M."><surname>Hasz</surname>, <given-names>B.M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Redish A.D."><surname>Redish</surname>, <given-names>A.D.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-22">Deliberation and Procedural Automation on a Two-Step Task for Rats</article-title>. <source hwp:id="source-23">Front. Integr. Neurosci</source>. <volume>12</volume>, <fpage>30</fpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Koechlin E."><surname>Koechlin</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ody C."><surname>Ody</surname>, <given-names>C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kouneiher F."><surname>Kouneiher</surname>, <given-names>F.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-23">The architecture of cognitive control in the human prefrontal cortex</article-title>. <source hwp:id="source-24">Science</source> <volume>302</volume>, <fpage>1181</fpage>–<lpage>1185</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Kool W."><surname>Kool</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S.J."><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Cushman F.A."><surname>Cushman</surname>, <given-names>F.A.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-24">Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems</article-title>. <source hwp:id="source-25">Psychol. Sci</source>. <volume>28</volume>, <fpage>1321</fpage>–<lpage>1333</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Kuvayev L."><surname>Kuvayev</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kuvayev L."><surname>Kuvayev</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Sutton R.S."><surname>Sutton</surname>, <given-names>R.S.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-25">Model-Based Reinforcement Learning with an Approximate, Learned Model</article-title>. <source hwp:id="source-26">Proc. NINTH YALE Work. Adapt. Learn. Syst</source>. <volume>8</volume>, <fpage>101</fpage>–<lpage>105</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2 xref-ref-27-3 xref-ref-27-4 xref-ref-27-5 xref-ref-27-6 xref-ref-27-7 xref-ref-27-8 xref-ref-27-9 xref-ref-27-10 xref-ref-27-11 xref-ref-27-12 xref-ref-27-13 xref-ref-27-14 xref-ref-27-15 xref-ref-27-16 xref-ref-27-17 xref-ref-27-18 xref-ref-27-19 xref-ref-27-20 xref-ref-27-21 xref-ref-27-22 xref-ref-27-23 xref-ref-27-24 xref-ref-27-25 xref-ref-27-26 xref-ref-27-27 xref-ref-27-28"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Lee S.W."><surname>Lee</surname>, <given-names>S.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shimojo S."><surname>Shimojo</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="O’Doherty J.P."><surname>O’Doherty</surname>, <given-names>J.P.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-26">Neural Computations Underlying Arbitration between Model-Based and Model-free Learning</article-title>. <source hwp:id="source-27">Neuron</source> <volume>81</volume>, <fpage>687</fpage>–<lpage>699</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="other" citation-type="journal" ref:id="393983v1.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Linnebank F.E."><surname>Linnebank</surname>, <given-names>F.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kindt M."><surname>Kindt</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="de Wit S."><surname>de Wit</surname>, <given-names>S.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-27">Investigating the balance between goal-directed and habitual control in experimental and real-life settings</article-title>. <source hwp:id="source-28">Learn. Behav</source>. <fpage>1</fpage>–<lpage>14</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="McClure S.M."><surname>McClure</surname>, <given-names>S.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Berns G.S."><surname>Berns</surname>, <given-names>G.S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Montague P.R."><surname>Montague</surname>, <given-names>P.R.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-28">Temporal Prediction Errors in a Passive Learning Task Activate Human Striatum</article-title>. <source hwp:id="source-29">Neuron</source> <volume>38</volume>, <fpage>339</fpage>–<lpage>346</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="McDannald M.A."><surname>McDannald</surname>, <given-names>M.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lucantonio F."><surname>Lucantonio</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burke K.A."><surname>Burke</surname>, <given-names>K.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niv Y."><surname>Niv</surname>, <given-names>Y.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Schoenbaum G."><surname>Schoenbaum</surname>, <given-names>G.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-29">Ventral striatum and orbitofrontal cortex are both required for model-based, but not model-free, reinforcement learning</article-title>. <source hwp:id="source-30">J. Neurosci</source>. <volume>31</volume>, <fpage>2700</fpage>–<lpage>2705</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Miller K.J."><surname>Miller</surname>, <given-names>K.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick M.M."><surname>Botvinick</surname>, <given-names>M.M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Brody C.D."><surname>Brody</surname>, <given-names>C.D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-30">Dorsal hippocampus contributes to model-based planning</article-title>. <source hwp:id="source-31">Nat. Neurosci</source>. <volume>20</volume>, <fpage>1269</fpage>–<lpage>1276</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Narayanan N.S."><surname>Narayanan</surname>, <given-names>N.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cavanagh J.F."><surname>Cavanagh</surname>, <given-names>J.F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frank M.J."><surname>Frank</surname>, <given-names>M.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Laubach M."><surname>Laubach</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-31">Common medial frontal mechanisms of adaptive control in humans and rodents</article-title>. <source hwp:id="source-32">Nat. Neurosci</source>. <volume>16</volume>, <fpage>1888</fpage>–<lpage>1895</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="O’Doherty J.P."><surname>O’Doherty</surname>, <given-names>J.P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Friston K."><surname>Friston</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Critchley H."><surname>Critchley</surname>, <given-names>H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dolan R.J."><surname>Dolan</surname>, <given-names>R.J.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-32">Temporal Difference Models and Reward-Related Learning in the Human Brain</article-title>. <source hwp:id="source-33">Neuron</source> <volume>38</volume>, <fpage>329</fpage>–<lpage>337</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Pezzulo G."><surname>Pezzulo</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rigoli F."><surname>Rigoli</surname>, <given-names>F.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Friston K.J."><surname>Friston</surname>, <given-names>K.J.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-33">Hierarchical Active Inference: A Theory of Motivated Control</article-title>. <source hwp:id="source-34">Trends Cogn. Sci</source>. <volume>22</volume>, <fpage>294</fpage>–<lpage>306</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Robbins T.W."><surname>Robbins</surname>, <given-names>T.W.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-34">Dissociating executive functions of the prefrontal cortex</article-title>. <source hwp:id="source-35">Philos. Trans. R. Soc. Lond. B. Biol. Sci</source>. <volume>351</volume>, <fpage>1463</fpage>–<lpage>70</lpage>; discussion 1470-1.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Rushworth M.F.S."><surname>Rushworth</surname>, <given-names>M.F.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Noonan M.P."><surname>Noonan</surname>, <given-names>M.P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boorman E.D."><surname>Boorman</surname>, <given-names>E.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walton M.E."><surname>Walton</surname>, <given-names>M.E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Behrens T.E."><surname>Behrens</surname>, <given-names>T.E.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-35">Frontal Cortex and Reward-Guided Learning and Decision-Making</article-title>. <source hwp:id="source-36">Neuron</source> <volume>70</volume>, <fpage>1054</fpage>–<lpage>1069</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Russek E.M."><surname>Russek</surname>, <given-names>E.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Momennejad I."><surname>Momennejad</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick M.M."><surname>Botvinick</surname>, <given-names>M.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S.J."><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-36">Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title>. <source hwp:id="source-37">PLOS Comput. Biol</source>. <volume>13</volume>, <fpage>e1005768</fpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Lee Sang Wan"><given-names>Sang Wan</given-names> <surname>Lee</surname></string-name>, <string-name name-style="western" hwp:sortable="Prenzel O."><surname>Prenzel</surname>, <given-names>O.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bien Zeungnam"><given-names>Zeungnam</given-names> <surname>Bien</surname></string-name> (<year>2013</year>). <article-title hwp:id="article-title-37">Applying human learning principles to user-centered IoT systems</article-title>. <source hwp:id="source-38">Computer (Long. Beach. Calif)</source>. <volume>46</volume>, <fpage>46</fpage>–<lpage>52</lpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Shenhav A."><surname>Shenhav</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick M.M."><surname>Botvinick</surname>, <given-names>M.M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Cohen J.D."><surname>Cohen</surname>, <given-names>J.D.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-38">The expected value of control: an integrative theory of anterior cingulate cortex function</article-title>. <source hwp:id="source-39">Neuron</source> <volume>79</volume>, <fpage>217</fpage>–<lpage>240</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Simon D.A."><surname>Simon</surname>, <given-names>D.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-39">Neural correlates of forward planning in a spatial decision task in humans</article-title>. <source hwp:id="source-40">J. Neurosci</source>. <volume>31</volume>, <fpage>5526</fpage>–<lpage>5539</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Skatova A."><surname>Skatova</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chan P.A."><surname>Chan</surname>, <given-names>P.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Daw N.D."><surname>Daw</surname>, <given-names>N.D.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-40">Extraversion differentiates between model-based and model-free strategies in a reinforcement learning task</article-title>. <source hwp:id="source-41">Front. Hum. Neurosci</source>. <volume>7</volume>, <fpage>525</fpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Smith K.S."><surname>Smith</surname>, <given-names>K.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Virkud A."><surname>Virkud</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Deisseroth K."><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Graybiel A.M."><surname>Graybiel</surname>, <given-names>A.M.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-41">Reversible online control of habitual behavior by optogenetic perturbation of medial prefrontal cortex</article-title>. <source hwp:id="source-42">Proc. Natl. Acad. Sci. U. S. A</source>. <volume>109</volume>, <fpage>18932</fpage>–<lpage>18937</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="van Steenbergen H."><surname>van Steenbergen</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Watson P."><surname>Watson</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wiers R.W."><surname>Wiers</surname>, <given-names>R.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hommel B."><surname>Hommel</surname>, <given-names>B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="de Wit S."><surname>de Wit</surname>, <given-names>S.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-42">Dissociable corticostriatal circuits underlie goal-directed vs. cue-elicited habitual food seeking after satiation: evidence from a multimodal MRI study</article-title>. <source hwp:id="source-43">Eur. J. Neurosci</source>. <volume>46</volume>, <fpage>1815</fpage>–<lpage>1827</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2 xref-ref-44-3"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Stephan K."><surname>Stephan</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Penny W."><surname>Penny</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daunizeau J."><surname>Daunizeau</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moran R."><surname>Moran</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Friston K."><surname>Friston</surname>, <given-names>K.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-43">Bayesian Model Selection for Group Studies</article-title>. <source hwp:id="source-44">Neuroimage</source> <volume>49</volume>, <fpage>1004</fpage>–<lpage>1017</lpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="book" citation-type="book" ref:id="393983v1.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Sutton R.S."><surname>Sutton</surname>, <given-names>R.S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Barto A.G."><surname>Barto</surname>, <given-names>A.G.</given-names></string-name> (<year>1998</year>). <source hwp:id="source-45">Reinforcement Learning</source> (<publisher-name>MIT press</publisher-name>).</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Tanji J."><surname>Tanji</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hoshi E."><surname>Hoshi</surname>, <given-names>E.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-44">Role of the Lateral Prefrontal Cortex in Executive Behavioral Control</article-title>. <source hwp:id="source-46">Physiol. Rev</source>. <volume>88</volume>, <fpage>37</fpage>–<lpage>57</lpage>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Tricomi E."><surname>Tricomi</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine B.W."><surname>Balleine</surname>, <given-names>B.W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="O’Doherty J.P."><surname>O’Doherty</surname>, <given-names>J.P.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-45">A specific role for posterior dorsolateral striatum in human habit learning</article-title>. <source hwp:id="source-47">Eur. J. Neurosci</source>. <volume>29</volume>, <fpage>2225</fpage>–<lpage>2232</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Wang J.X."><surname>Wang</surname>, <given-names>J.X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kurth-Nelson Z."><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kumaran D."><surname>Kumaran</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tirumala D."><surname>Tirumala</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Soyer H."><surname>Soyer</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leibo J.Z."><surname>Leibo</surname>, <given-names>J.Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hassabis D."><surname>Hassabis</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Botvinick M."><surname>Botvinick</surname>, <given-names>M.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-46">Prefrontal cortex as a meta-reinforcement learning system</article-title>. <source hwp:id="source-48">Nat. Neurosci</source>. <volume>21</volume>, <fpage>860</fpage>–<lpage>868</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2 xref-ref-49-3"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Wunderlich K."><surname>Wunderlich</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dolan R.J."><surname>Dolan</surname>, <given-names>R.J.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-47">Mapping value based planning and extensively trained choice in the human brain</article-title>. <source hwp:id="source-49">Nat. Neurosci</source>. <volume>15</volume>, <fpage>786</fpage>–<lpage>791</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Yin H.H."><surname>Yin</surname>, <given-names>H.H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Knowlton B.J."><surname>Knowlton</surname>, <given-names>B.J.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-48">Contributions of striatal subregions to place and response learning</article-title>. <source hwp:id="source-50">Learn. Mem</source>. <volume>11</volume>, <fpage>459</fpage>–<lpage>463</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><citation publication-type="journal" citation-type="journal" ref:id="393983v1.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Yin H.H."><surname>Yin</surname>, <given-names>H.H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ostlund S.B."><surname>Ostlund</surname>, <given-names>S.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knowlton B.J."><surname>Knowlton</surname>, <given-names>B.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Balleine B.W."><surname>Balleine</surname>, <given-names>B.W.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-49">The role of the dorsomedial striatum in instrumental conditioning</article-title>. <source hwp:id="source-51">Eur. J. Neurosci</source>. <volume>22</volume>, <fpage>513</fpage>–<lpage>523</lpage>.</citation></ref></ref-list></back></article>
