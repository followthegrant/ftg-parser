<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/197426</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;197426</article-id><article-id pub-id-type="other" hwp:sub-type="slug">197426</article-id><article-id pub-id-type="other" hwp:sub-type="tag">197426</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Hierarchical Brain Network for Face and Voice Integration of Emotion Expression</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp hwp:id="corresp-1">Address all correspondence to Jodie Davies-Thompson (<email hwp:id="email-1">j.daviesthompson@gmail.com</email>) and Olivier Collignon (<email hwp:id="email-2">olivier.collignon@uclouvain.be</email>)</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Davies-Thompson Jodie"><surname>Davies-Thompson</surname><given-names>Jodie</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Elli Giulia V."><surname>Elli</surname><given-names>Giulia V.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Rezk Mohamed"><surname>Rezk</surname><given-names>Mohamed</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Benetti Stefania"><surname>Benetti</surname><given-names>Stefania</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Ackeren Markus van"><surname>Ackeren</surname><given-names>Markus van</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Collignon Olivier"><surname>Collignon</surname><given-names>Olivier</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-2" hwp:rel-id="aff-4">4</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5"><label>1</label><institution hwp:id="institution-1">Crossmodal Perception and Plasticity Laboratory, Center of Mind/Brain Sciences, University of Trento</institution>, <country>Italy</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">National Institute of Health Research (NIHR) Nottingham Biomedical Research Centre, UK, and Otology and Hearing Group, Division of Clinical Neuroscience, School of Medicine, University of Nottingham</institution>, <country>UK</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Department of Psychological &amp; Brain Sciences, John Hopkins University</institution>, Baltimore, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1 xref-aff-4-2"><label>4</label><institution hwp:id="institution-4">Institute of research in Psychology (IPSY), Institute of Neuroscience (IoNS), University of Louvain (UcL)</institution></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-10-02T15:21:32-07:00">
    <day>2</day><month>10</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-10-02T15:21:32-07:00">
    <day>2</day><month>10</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-10-02T15:29:06-07:00">
    <day>2</day><month>10</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-10-02T15:29:06-07:00">
    <day>2</day><month>10</month><year>2017</year>
  </pub-date><elocation-id>197426</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-10-02"><day>02</day><month>10</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2017-10-02"><day>02</day><month>10</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-10-02"><day>02</day><month>10</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="197426.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/197426v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="197426.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/197426v1/197426v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/197426v1/197426v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">ABSTRACT</title><p hwp:id="p-2">The brain has separate specialized computational units to process faces and voices located in occipital and temporal cortices. However, humans seamlessly integrate signals from the faces and voices of others for optimal social interaction. How are emotional expressions, when delivered by different sensory modalities (faces and voices), integrated in the brain? In this study, we characterized the brains’ response to faces, voices, and combined face-voice information (congruent, incongruent), which varied in expression (neutral, fearful). Using a whole-brain approach, we found that only the right posterior superior temporal sulcus (rpSTS) responded more to bimodal stimuli than to face or voice alone but only when the stimuli contained emotional expression. Face-and voice-selective regions of interest extracted from independent functional localizers, similarly revealed multisensory integration in the face-selective rpSTS only; further, this was the only face-selective region that also responded significantly to voices. Dynamic Causal Modeling revealed that the rpSTS receives unidirectional information from the face-selective fusiform face area (FFA), and voice-selective temporal voice area (TVA), with emotional expression affecting the connections strength. Our study promotes a hierarchical model of face and voice integration, with convergence in the rpSTS, and that such integration depends on the (emotional) salience of the stimuli.</p><sec hwp:id="sec-1"><title hwp:id="title-2">SIGNIFICANCE STATEMENT</title><p hwp:id="p-3">Emotional content of faces and voices are seamlessly integrated; however, it is unclear how early this information is integrated by the brain. 24 participants undertook fMRI while viewing videos of neutral and fearful faces, voices, and bimodal faces and voices, and independent localiser scans. We found no evidence of multisensory integration (MSI) in face-selective fusiform face area (FFA) or voice-selective temporal voice area (TVA); instead MSI occurred in the right posterior superior temporal sulcus (rpSTS). Dynamic causal modeling revealed that rpSTS receives unidirectional information from FFA and TVA, with emotional expression affecting their connections strength. This is the first study to conclusively test and demonstrate that MSI of emotional faces and voices follows a hierarchical integrative model converging in rpSTS.</p></sec></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-3">KEY WORDS</title><kwd hwp:id="kwd-1">Emotional expression</kwd><kwd hwp:id="kwd-2">faces</kwd><kwd hwp:id="kwd-3">fMRI</kwd><kwd hwp:id="kwd-4">multisensory</kwd><kwd hwp:id="kwd-5">voice</kwd></kwd-group><counts><page-count count="32"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-2"><title hwp:id="title-4">INTRODUCTION</title><p hwp:id="p-4">The ability to quickly and accurately recognize emotional expressions is a fundamental cognitive skill for effective social interactions. Typically, emotion is simultaneously expressed through separate sensory channels, like the face and the voice, and this information is seamlessly integrated in a coherent percept producing enhanced discrimination and faster reaction times (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Massaro and Egan, 1996</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">De Gelder and Vroomen, 2000</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Collignon et al., 2008</xref>). Perceiving a facial expression can alter the percept of a vocal emotional expression (<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Pye and Bestelmeyer, 2015</xref>), and emotional input from one unattended sensory input can systematically influence the judgment of the attended sensory stream (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">De Gelder and Vroomen, 2000</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">Collignon et al., 2008</xref>). Additionally, audiovisual integration of emotion expressions can arise pre attentively (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Föcker et al., 2011</xref>) and is unconstrained by available attentional resources (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Vroomen et al., 2001</xref>). Together, this suggests that the integration of facial and vocal emotion information may be a rather automatic process (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-2" hwp:rel-id="ref-66">Vroomen et al., 2001</xref>). However, the neural mechanisms underlying how emotional signals from the face and the voice are integrated, remains unclear. More specifically, it is debated whether integration of facial and vocal emotion information occurs within a distinct convergence zone or could already be observed within the face and voice-selective networks.</p><p hwp:id="p-5">Influential models of identity recognition suggest two largely distinct brain networks for face and voice processing, with a shared multisensory convergence zone for the integration of information across sensory modalities (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Bruce and Young, 1986</xref>; <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Burton et al., 1990</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Haxby et al., 2000</xref>; <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Watson et al., 2014a</xref>). Some studies have suggested that the integration of facial and vocal emotional signals occur in multisensory regions like the amygdala, orbitofrontal cortex, and posterior superior temporal sulcus (STS) (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Romanski, 2007</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Kreifelts et al., 2009</xref>; <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Peelen et al., 2010</xref>; <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-2" hwp:rel-id="ref-67">Watson et al., 2014a</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Hölig et al., 2017</xref>); regions separate from the core face and voice-selective network (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Campanella and Belin, 2007</xref>). However, in these studies, it is unclear if the “multisensory” region was located within face-or voice-selective regions, or reflected an independent convergence zone.</p><p hwp:id="p-6">Alternatively, integration of emotional information of faces and voices could occur at early stages of processing via reciprocal connections between regions across the face and voice preferential networks. Recent models of identity recognition have indeed suggested that integration of faces and voices could occur within unisensory regions (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">Von Kriegstein et al., 2005</xref>; <xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">von Kriegstein and Giraud, 2006</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Blank et al., 2011</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Joassin et al., 2011</xref>) as a result of direct structural (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">Blank et al., 2011</xref>) and functional (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-2" hwp:rel-id="ref-64">Von Kriegstein et al., 2005</xref>) connectivity between face-selective regions in the fusiform cortex (fusiform face area, FFA; (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Kanwisher et al., 1997</xref>) and voice-selective regions in the middle temporal gyrus (temporal voice area, TVA; (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Belin et al., 2002</xref>). Interestingly, electrode studies in macaques have suggested that a significant proportion of neurons in voice-selective regions in auditory cortex respond more to audiovisual stimuli as compared to unimodal vocal or facial sitmuli (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Ghazanfar et al., 2005</xref>; <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Perrodin et al., 2014</xref>), and are sensitive to asynchronies in the onset of faces and voices (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Perrodin et al., 2015</xref>).</p><p hwp:id="p-7">Finally, face and voice regions may together form a general social processing network, with all regions involved in the processing of both visual and auditory human stimuli. For example, viewing point-light displays depicting biological motion or geometric shapes depicting social interactions appear to activate both the face-selective FFA and posterior STS (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Bonda et al., 1996</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Castelli et al., 2000</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Grossman and Blake, 2002</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Schultz et al., 2003</xref>) suggesting that these regions may play a more abstract role in social perception. Such a model would propose that so-called ‘unisensory’ regions would not only respond to their preferred stimulus, but also respond to both unisensory facial and vocal expressions of emotion.</p><p hwp:id="p-8">Our study was specifically designed to test these contrasting views on whether integration of information from faces and voices occurs in earlier or later stages of processing, and how emotional expressions alter this integration.</p></sec><sec id="s2" hwp:id="sec-3"><title hwp:id="title-5">MATERIALS AND METHODS</title><sec id="s2a" hwp:id="sec-4"><title hwp:id="title-6">Participants</title><p hwp:id="p-9">24 healthy right-handed participants (12 females; mean age: 26 years, SD: 5) with no history of neurological dysfunction, and with self-reported normal or corrected-to-normal vision, and normal hearing took part in the study. The protocol was approved by the institutional review boards of the University of Trento and the Center for Mind/Brain Sciences (CIMeC), and written informed consent was obtained for all subjects in accordance with The Code of Ethics of the World Medical Association, Declaration of Helsinki (Rickham, 1964).</p></sec><sec id="s2b" hwp:id="sec-5"><title hwp:id="title-7">Imaging parameters</title><p hwp:id="p-10">Subjects were scanned in a Bruker BioSpin MedSpec 4T scanner at the Center of Mind/Brain Sciences (CIMeC), University of Trento. T2<sup>*</sup>-weighted scans using echo planar imaging were used to collect data for all functional scans from 37 sequential axial slices (TR = 2200ms, TE = 30ms, FA = 76°, FOV = 192mm, 3mm slice thickness, voxel size = 3x3mm). These were co-registered onto a T1 weighted 3D MPRAGE anatomical image (TR = 2700ms, TE = 4.18ms, 176 axial slices, FOV = 256mm, 1mm slice thickness, voxel size = 1x1mm), from each participant.</p></sec><sec id="s2c" hwp:id="sec-6"><title hwp:id="title-8">Face localizer</title><p hwp:id="p-11">To identify regions responding preferentially to faces, a face localiser scan was carried out for each subject. Subjects viewed blocks containing images from one of the four different categories: neutral faces, fearful faces, Fourier scrambled faces, or objects. Face images were 20 front on photographs of different actors (10 female) with either neutral or fearful expressions, taken from the Radboud Faces Database (<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Langner et al., 2010</xref>). Faces were cropped to their natural chin and hairline in order to remove external features. The object stimuli consisted of 20 pictures of objects (e.g., cars, houses, etc.). The scrambled face stimuli were created by applying a Fourier phase randomization procedure to the neutral face images, which preserves only the low-level properties (e.g., luminance, spatial frequency, colors) (e.g.(<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Sadr and Sinha, 2004</xref>)). All images were framed in a white rectangle (220x270 pixels) and had comparable mean luminance and contrast (measured using SHINE Toolbox in MATLAB – (<xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Willenbockel et al., 2010</xref>). In order to minimize an after-image effect, visual stimuli were presented centrally but with slight changes in their position around the fixation point, varying either in the x-or in the y-axis by 30 pixels. Each image was presented for 1s followed by a 50ms black screen. There were 20 images per block with each block lasting approximately 21 <sub>s</sub>. Each condition was repeated 8 times. Blocks were separated by a 7-9 <sub>s</sub> fixation blank screen (mean = 8s; jitter = 1s) and were presented in a fixed order (neutral face, object, fearful face, scrambled faces). The localizer was split into two runs: each lasting approximately 8 minutes each. Subjects performed a one back task, pressing a button when the same stimulus was repeated twice in a row.</p></sec><sec id="s2d" hwp:id="sec-7"><title hwp:id="title-9">Voice localizer</title><p hwp:id="p-12">To identify regions responding preferentially to voices, a voice localiser scan was carried out for each subject. Subjects viewed blocks heard sounds from one of the five different categories: neutral voices, fearful voices, scrambled neutral voices, scrambled fearful voices, or objects. The voice stimuli consisted in the articulation of the vowel /<italic toggle="yes">a</italic>/ recorded by 10 different actors (half of them female) expressing either neutral or fearful emotion (taken from the Montreal Affective Voices – (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Belin et al., 2008</xref>)). The objects stimuli were non-verbal object sounds referring to non-living objects, namely human action sounds (e.g. lighting a match), bells and musical instruments (e.g. Christmas bells) and automated machinery (e.g. hair dryer). Scrambled voice and object sounds were created in MATLAB (The MathWorks, Inc., Natick, Massachusetts, United States). Each voice and object sound was submitted to a fast Fourier transformation and the resulting components were separated into frequency windows of ∼ 700 Hz based on their center frequency. Scrambling was then performed by randomly intermixing the magnitude and phase of each Fourier component within each of these frequency windows separately (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">Belin et al., 2000</xref>; 2002). The inverse Fourier transform was then applied on the resulting signal. The output was a sound of the same length of the original sound with similar energy within each frequency band. For scrambled voices, the envelope of the original voice was further applied on the output signal (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1C</xref>). All auditory stimuli had the same duration (1s; 5ms ramps at the start-end of the sound), and were normalized for loudness using a root mean square (RMS) function.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-13">Stimuli presentation and example stimuli used in the experiment. Stimuli were presented in an event-related design. Each run lasted ∼ 15 minutes, and were repeated 3 times. Face (F) and voice (V) stimuli could be either fearful (F) or neutral (N) emotional expressions. In the bimodal condition, voices and faces were presented together as either congruent (neutral face and voice, or fearful face and voice) or incongruent (neutral face and fearful voice, or fearful face and neutral voice).</p></caption><graphic xlink:href="197426_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-14">Each auditory stimulus lasted for 1s followed by 50ms of silence. There were 20 auditory stimuli per block with each block lasting approximately 21s. Each condition was repeated 8 times. Blocks were separated by a 7 9s fixation blank screen (mean = 8s; jitter = 1s) and were presented in a fixed order (neutral voice, scrambled neutral voice, fearful voice, scrambled fearful voice, object sounds). The localizer was split into two runs, each lasting approximately 9 minutes. Subjects were asked to respond via button press when two sequentially presented stimuli were identical.</p></sec><sec id="s2e" hwp:id="sec-8"><title hwp:id="title-10">Multisensory experiment</title><p hwp:id="p-15">There were eight experimental conditions which consisted of 4 unimodal conditions: (i) <italic toggle="yes">neutral faces (FaceN)</italic>; (ii) <italic toggle="yes">fearful faces (FaceF)</italic>; (iii) <italic toggle="yes">neutral voices (VoiceN)</italic>; (iv) <italic toggle="yes">fearful voices (VoiceF)</italic>; and 4 bimodal conditions: (v) congruent neutral faces with neutral voices (<italic toggle="yes">CongN</italic>); (vi) congruent fearful faces with fearful voices (<italic toggle="yes">CongF</italic>); (vii) incongruent neutral faces with fearful voices (<italic toggle="yes">IncongNF-FV</italic>); (viii) incongruent fearful faces with neutral voices (<italic toggle="yes">IncongFF-NV</italic>). <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref> shows examples of each condition.</p><p hwp:id="p-16">Stimuli were 28 video clips of 14 professional actors (7 female) pronouncing the vowel /<italic toggle="yes">a</italic>/ while performing either neutral or fearful facial expressions and voice intonations (<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Simon et al., 2008</xref>). The videos were selected from a set of 62 clips based on the ratings by 20 independent judges’ (half of them female) who decided which emotion was performed (fear/neutral) and how representative it was (0 Not at all – 10 Very much). All selected videos were correctly recognized by at least 19 judges, and had a minimum representative mean score of 7. The audio tracks from these videos were used in the unimodal voice conditions, and the videos without the audio tracks were used in the unimodal face conditions. Incongruent conditions were created using the video and audio tracks of opposite emotional expressions performed by the same actor. All stimuli (112) were the same duration (1s). All audio signals were equalized in energy using Root Mean Square normalization.</p><p hwp:id="p-17">An event-related design was used to present the stimuli. Each trial consisted of a single stimulus from one of the eight conditions presented for 1s, followed by a black screen for 5 8s (mean = 6.5s; jitter = 1s). Each of the 8 condition were repeated 14 times, resulting in a total of 112 trials in a single run lasting approximately 15 minutes. Each run was repeated 3 times, with each of the 3 runs containing stimuli performed by different actors. To monitor attentional load across stimulus conditions, subjects performed an orthogonal gender discrimination task for each stimuli in which they responded via a button press (right middle and index finger), as to whether the stimuli was that of a male or female.</p></sec><sec id="s2f" hwp:id="sec-9"><title hwp:id="title-11">fMRI Analysis</title><p hwp:id="p-18">Analysis of the MRI data was carried out using Statistical Parametric Mapping (SPM8 – Welcome Department of Imaging Neuroscience, 2009), implemented in MATLAB R2008a (The MathWorks, Inc.). The initial 4 functional volumes of data from each scan were removed to minimize the effects of magnetic saturation. Preprocessing of the functional MR data included, in the following order: slice time correction to the middle slice; realignment of functional time series; co-registration with the anatomical data; segmentation of the structural; normalization of the structural and of the functional images into standardized Montreal Neurological Institute (MNI) space; and smoothing (Gaussian kernel, 4mm FWHM) of the functional images.</p><p hwp:id="p-19">In the fMRI data analysis, single subjects were entered into a fixed-effect analysis (FFX). Changes in the regional blood oxygenation level dependent (BOLD) signal were estimated using a general linear model (GLM). Regressors of interest consisted of experimental events time-locked to the stimulus onset and convolved with a standard hemodynamic response function. Motion parameters obtained during preprocessing were included as regressors of no interest. A high-pass filter with a cut-off frequency of 128Hz was used to correct for low-frequency noise and signal drift.</p></sec><sec id="s2g" hwp:id="sec-10"><title hwp:id="title-12">Whole-brain analysis (multisensory experiment)</title><p hwp:id="p-20">Statistical images from the single-subject level analysis were spatially smoothed (Gaussian kernel 6mm FWHM) and entered into a mixed-effects group level (RFX) analysis. Statistical inferences were then performed at a threshold of p &lt; 0.05 resel corrected for multiple comparisons. This correction uses a principle similar to Gaussian random field theory to estimate the number of independent samples within a dataset (<xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Worsley et al., 1998</xref>), and is therefore less conservative than a Bonferroni correction.</p><p hwp:id="p-21">Multisensory regions across the entire brain were defined as responding more to bimodal stimulation as compared to both unimodal conditions, using a conjunction (AND) analysis (bimodal-visual) ∩ (bimodal-auditory) (<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">Van Atteveldt et al., 2004</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Ethofer et al., 2013</xref>). We analysed the two emotions separately for neutral (<italic toggle="yes">CongN-FaceN</italic>) ∩ (<italic toggle="yes">CongN-VoiceN</italic>), and fearful (<italic toggle="yes">CongF-FaceF</italic>) ∩ (<italic toggle="yes">CongF-VoiceF</italic>) conditions. We chose this criteria, because, given the spatial limitations of fMRI, the ‘additive model’ (<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Perrault et al., 2003</xref>, <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">2005</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Stanford et al., 2005</xref>) may result in regions containing heterogeneous populations of multisensory and unisensory neurons, failing to show multisensory responses (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Beauchamp, 2005</xref>; <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Laurienti et al., 2005</xref>). Meanwhile, the more liberal ‘mean criterion’ model (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Calvert, 2001</xref>; <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">Beauchamp, 2005</xref>) can result in an area incorrectly identified as being multisensory, if this area shows a disproportionately large response to one unimodal stimuli as compared to another and an intermediate response to bimodal stimuli (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-3" hwp:rel-id="ref-1">Beauchamp, 2005</xref>). Therefore, although outcomes may be affected by small variations in either unisensory response (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-4" hwp:rel-id="ref-1">Beauchamp, 2005</xref>), using the ‘minimum statistic’ provides a useful compromise while also ensuring that the response to unimodal audio and visual stimuli were extracted from the signal.</p><p hwp:id="p-22">To examine the effects of congruency across the brain, we collapsed both congruent conditions (<italic toggle="yes">CongN, CongF</italic>) and compared the response to the collapsed incongruent conditions (<italic toggle="yes">IncongNF-FV, IncongFF-NV).</italic> Note that in this case, it is not straightforward to examine the effects of emotional expression on congruency, as the incongruent conditions cannot be categorized as fearful or neutral.</p></sec><sec id="s2h" hwp:id="sec-11"><title hwp:id="title-13">Region of interest analysis</title><p hwp:id="p-23">To examine the response to the experimental conditions in regions selective for faces and voice, we obtained regions of interest (ROI) at the single-subject level from independent localiser sessions. Face-selective regions of interest (ROI) were identified from the face localizer by the conjunction (<italic toggle="yes">neutral faces</italic> &gt; <italic toggle="yes">scrambled faces</italic>) ∩ (<italic toggle="yes">neutral faces</italic> &gt; <italic toggle="yes">objects</italic>), thresholded at P &lt; 0.001 uncorrected. Based on coordinates from previous studies (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Davies-Thompson and Andrews, 2012</xref>; <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Rossion et al., 2012</xref>), we identified the bilateral fusiform face area (FFA; left: n = 23; right: n = 24), bilateral occipital face area (OFA; left: n = 19; right: n = 20), and the posterior superior temporal sulcus (pSTS; left: n = 11; right: n = 20) in individual subjects. Voice-selective ROIs were identified from the voice localizer by the conjunction (<italic toggle="yes">neutral voices</italic> &gt; <italic toggle="yes">scrambled voices</italic>) ∩ (<italic toggle="yes">neutral voices</italic> &gt; <italic toggle="yes">objects</italic>), thresholded at P &lt; 0.001 uncorrected. Based on the reported coordinates from previous studies (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Belin et al., 2002</xref>), we identified the bilateral temporal voice area (TVA) in middle temporal gyrus (left: n = 16; right: n = 19), bilateral anterior temporal lobe (ATL; left: n = 12; right: n = 8), and left anterior superior temporal sulcus (aSTS; n = 14) in individual subjects.</p><p hwp:id="p-24">Given the strict criteria used to identify these regions, some subjects did not have a complete set of regions of interest. In order to obtain data from these regions, which may be subthreshold, we created a 10mm sphere mask around the peak coordinate at the group level and extracted data from individual subjects who did not show this region at the individual subject level. This allowed us to obtain data from all 24 subjects for all regions of interest (<xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>). (Note that analysis showed the same pattern of response for each ROI, regardless of whether regions were defined only at the individual subject level (partial sample) or whether regions were defined from the group level coordinates (whole sample)). The beta values to each of the experiment conditions within the face-and voice-selective regions of interest were then entered into repeated-measures ANOVA to determine significant differences in the response to each stimulus condition.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-2"><p hwp:id="p-25">MNI coordinates of the face-and voice-selective region of interest analysis. Also shown are the number of subjects each region was identified at the individual subject level. Note, however, that data was collected for all subjects for each region (see Methods).</p></caption><graphic xlink:href="197426_tbl1" position="float" orientation="portrait" hwp:id="graphic-2"/></table-wrap></sec><sec id="s2i" hwp:id="sec-12"><title hwp:id="title-14">Dynamic Causal Modeling</title><p hwp:id="p-26">We use Dynamic Causal modeling (DCM; (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Friston et al., 2003</xref>) to investigate cortical interaction between face and voice-selective regions, and how this gives rise to multisensory responses. In DCM changes in the neuronal states of a set of regions in a network are modeled using a bilinear state equation and a biophysically validated model of how these neuronal dynamics affect the bold response observed in fMRI. The bilinear state equation consists of three main parameters. These parameters are the endogenous interactions between regions (A), modulation of these connections by a stimulus or task (B), and the exogenous input to the network (C).</p><p hwp:id="p-27">DCM analysis was implemented using SPM8 toolbox (Wellcome Department of Imaging Neuroscience, London, UK), running in Matlab R2012b (The Math Works, Natick, MA, USA). We defined bilinear models with mean-centered inputs with endogenous connections (A) between the regions of interest. Unimodal faces, unimodal voices, and bimodal face-voice stimuli served as the driving inputs to the regions (C). To investigate how these intrinsic dynamics interact with facial expression, emotional expression (neutral, fearful) was entered as a modulator of connectivity (B) between regions, as well as a direct input (C).</p><p hwp:id="p-28">Next, as we were interested in how the intrinsic connections are modulated as a function of emotional expression, each model was repeated for alternative points at which emotional expression might influence the intrinsic connections. In the case of models with bi-directional intrinsic connections (A), modulations (B) were assumed to be bidirectional as well.</p><p hwp:id="p-29">To decide which model best explained our results, we used random-effects Bayesian model selection (BMS; (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Stephan et al., 2009</xref>)).</p></sec><sec id="s2j" hwp:id="sec-13"><title hwp:id="title-15">Statistical analysis</title><p hwp:id="p-30">For each face-selective region (OFA, FFA, pSTS), beta values from the face localiser from individual subjects were entered as the outcome variable in a repeated measures ANOVA with Hemisphere (left, right), and Condition (neutral faces, emotive faces, objects, scrambled neutral faces) as factors, and Subject as a random effect. To explore the basis of significant main effects and interactions revealed by the ANOVA, two-tailed paired-samples t-tests were then used for post hoc comparisons (Bonferroni corrected for multiple comparisons). The same method was used to test for differences in the response to the voice localiser within the face-selective regions, with Hemisphere (left, right) and Condition (neutral voices, emotive voices, objects, scrambled neutral voices, scrambled emotive voices) as factors, and Subject as a random effect. Identical statistical analyses were conducted for voice-selective regions (TVA, ALT, aSTS), but without Hemisphere as a factor for left aSTS due to no right hemisphere equivalent.</p><p hwp:id="p-31">For the multisensory experiment, beta values from face-selective and voice-selective regions were entered as the outcome variable in a repeated-measures ANOVA with Hemisphere (left, right), Emotion (neutral, fearful), and Modality (bimodal congruent, unimodal faces, unimodal voices) as factors, and Subject as a random effect. To explore the basis of interactions and main effects, two-tailed paired-samples t-tests were then used for planned comparisons to compare responses across-modality within each region (Bonferroni corrected for multiple comparisons). Finally, to test for congruency effects in face-selective and voice-selective regions, beta values were entered into independent repeated-measures ANOVAs, with Region (face-selective: 6 regions; voice-selective: 5 regions) and Congruency (congruent, incongruent) as factors, and Subject as a random effect, followed by paired-samples t-tests (Bonferroni corrected).</p></sec></sec><sec id="s3" hwp:id="sec-14"><title hwp:id="title-16">RESULTS</title><sec id="s3a" hwp:id="sec-15"><title hwp:id="title-17">Crossmodal selectivity in functional localisers</title><p hwp:id="p-32">To determine whether face-selective regions responded to voices, and voice-selective regions responded to faces, we extracted the beta values to all stimuli from the face and voice localisers in all face and voice regions of interest. To specifically test for crossmodal selectivity, beta values to the non-preferred stimuli – the response to the face localiser in voice selective regions, and the response to the voice localiser in face selective regions - were entered into repeated measures ANOVAs (hemisphere, condition), followed by post hoc paired-samples t-tests to determine whether: (1) the response to faces was greater than to scrambled faces and objects in voice-selective regions, and (2) the response to voices was greater than scrambled voices and objects in face-selective regions.</p><p hwp:id="p-33"><xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref> shows the response to the face and voice conditions in face-selective regions of interest. Independent beta values extracted from the voice localizer experiment only revealed positive beta values (higher than baseline) in the pSTS regions, while other face selective regions (FFA and OFA) showed a pattern of deactivation for auditory categories. A 2 (hemisphere) x 5 (conditions) ANOVA for the voice localiser in the FFA revealed a significant effect of Condition (F(4,92) = 7.24, P &lt; 0.001) but no effect of Hemisphere (F(1,23) = 3.99, P = 0.06) or Interaction (F(4,92) = 2.14, P = 0.08). In both the left and the right FFA, the effect of condition was caused by a negative response to objects as compared to both neutral voices (left: t(23) = 2.33, p &lt; 0.05; right: t(23) = 3.00, p &lt; 0.01) and emotive voices (left: t(23) = 4.24, p &lt; 0.001; right: t(23) = 4.84, p &lt; 0.001); there was also a greater negative response to scrambled emotive voices than to emotive voices in the left FFA (t(23) = 2.32, P &lt; 0.05). The same pattern was observed in the OFA: there was an effect of Condition (F(4,92) = 17.79, P &lt; 0.001) but no effect of Hemisphere (F(1,23) = 0.80, P = 0.38) or Interaction (F(4,92) = 1.20, P = 0.32). This was caused by a negative response to objects as compared to both neutral voices (left: t(23) = 4.99, p &lt; 0.001; right: t(23) = 2.65, p &lt; 0.05) and emotive voices (left: t(23) = 7.36, p &lt; 0.001; right: t(23) = 4.89, p &lt; 0.001). In sum, neither the FFA nor OFA responded significantly to human voices. In the pSTS, there was an effect of Condition (F(4,92) = 13.85, p &lt; 0.001) and an Interaction (F(4,92) = 6.15, p &lt; 0.001), but no effect of Hemisphere (F(1,23) = 2.22, p = 0.15). In contrast to the FFA and OFA, the pSTS showed a positive response to both neutral faces &gt; scrambled neutral faces (left: t(23) = 2.90, p &lt; 0.01; right: t(23) = 4.93, p &lt; 0.001) and emotive faces &gt; scrambled emotive faces (left: t(23) = 2.72, p &lt; 0.05; right: t(23) = 4.28, p &lt; 0.001). The right pSTS also responded more to emotive voices &gt; objects (t(23) = 2.59, p &lt; 0.05). These data suggest cross-modal selectivity to human voices in the left and right pSTS only.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-3"><p hwp:id="p-34">Face-responsive regions identified in the face localizer. Top: Bilateral fusiform face area (FFA), occipital face area (OFA), and a region in the posterior superior temporal sulcus (pSTS) responded more to faces than to scrambled faces and objects. Contrast: (faces &gt; scrambled faces) ∩ (faces &gt; objects). Bottom: The response of these face-selective regions to stimuli presented in the face and voice localisers. Neither the left nor right FFA nor OFA responded significantly to neutral or emotive voices. In contrast, both left and right pSTS responded more to neutral and emotive voices as compared to their scrambled counterparts (neutral voices &gt; scrambled neutral voices; emotive voices &gt; scrambled emotive voices).</p></caption><graphic xlink:href="197426_fig2" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-35"><xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> shows the response to the face and voice conditions in voice-selective regions of interest. A 2 (hemisphere) x 4 (conditions) ANOVA for the face localiser in the TVA revealed no effect of Condition (F(3,69) = 0.40, p = 0.76) or Hemisphere (F(1,23) = 1.93, p = 0.18), but a significant Interaction (F(3,69) = 3.02, p &lt; 0.05). However, there were no significant differences in the response to neutral or emotive faces as compared to scrambled faces and objects (P’s &gt; 0.17). In ATL, there was no effect of Condition (F(3,69) = 2.03, p = 0.12), Hemisphere (F(1,23) = 0.08, p = 0.79), or Interaction (F(3,69) = 0.34, p = 0.80). Finally, a 1 x 5 (conditions) ANOVA for the left aSTS showed no effect of Condition (F(3,69) = 0.40, p = 0.75). These data suggest no cross-modal selectivity to faces in voice-selective regions.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-4"><p hwp:id="p-36">Voice-responsive regions identified in the voice localizer. Top: Bilateral temporal voice area (TVA), anterior temporal lobe (ATL), and a region in the anterior superior temporal sulcus (aSTS) responded more to voices than to scrambled voices and objects. Contrast: (voices &gt; scrambled voices) ∩ (voices &gt; object sounds). Bottom: The response of these voice-selective regions to stimuli presented in the face and voice localisers. Neither the left nor right TVA, ATL, nor left aSTS, responded significantly to neutral or emotive faces.</p></caption><graphic xlink:href="197426_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s3b" hwp:id="sec-16"><title hwp:id="title-18">Effects of bimodal versus unimodal stimuli</title><sec id="s3b1" hwp:id="sec-17"><title hwp:id="title-19">Whole brain analysis</title><p hwp:id="p-37"><xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref> shows a region in the right posterior superior temporal sulcus (rpSTS) that responded more to bimodal conditions as compared to unimodal conditions for fearful stimuli in the whole brain analysis (P &lt; 0.05, resel corrected for multiple comparisons). This region overlapped with the face-selective pSTS identified in the independent localizer (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref>). No other regions responded significantly more to bimodal than to either unimodal conditions for either fearful or neutral stimuli.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-5"><p hwp:id="p-38">Whole brain analysis. A region in the right superior temporal sulcus (rpSTS) responded more to bimodal than to unimodal fearful stimuli. Contrast: [bimodal congruent fear &gt; unimodal fearful face] ∩ [bimodal congruent fear &gt; unimodal fearful voice]. The whole-brain map is displayed at P &lt; 0.05 resel corrected.</p></caption><graphic xlink:href="197426_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s3b2" hwp:id="sec-18"><title hwp:id="title-20">Face-selective regions</title><p hwp:id="p-39"><xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref> shows the response across subjects in face-selective regions of interest (as defined by the conjunction (neutral faces &gt; scrambled faces) ∩ (neutral faces &gt; objects)) to the different conditions in the independent multisensory experiment. Bilateral FFA and bilateral OFA showed similar responses. For both the OFA and FFA, 2 (Hemisphere) x 2 (Emotion: neutral, fearful) x 3 (Modality: face, voice, bimodal) ANOVAs showed effects of Emotion (P’s &lt; 0.05) and Modality (P’s &lt; 0.001), but no effects of Hemisphere (P &gt; 0.10). For all four regions (left FFA, right FFA, left OFA, right OFA), the effect of Modality was due to significantly greater responses to faces and congruent bimodal stimuli as compared to voices for neutral (<italic toggle="yes">neutral faces</italic> &gt; <italic toggle="yes">neutral voices</italic>; <italic toggle="yes">CongN</italic> &gt; <italic toggle="yes">neutral voices</italic>) and fearful stimuli (<italic toggle="yes">fearful faces</italic> &gt; <italic toggle="yes">fearful voices</italic>; <italic toggle="yes">CongF</italic> &gt; <italic toggle="yes">fearful voices</italic>) (P’s &lt; 0.001), but no difference in the response to unimodal faces and congruent bimodal stimuli (<italic toggle="yes">CongN</italic> = <italic toggle="yes">neutral faces</italic>; <italic toggle="yes">CongF</italic> = <italic toggle="yes">fearful faces</italic>) (P’s &gt; 0.07). The effect of Emotion was due to a greater response to <italic toggle="yes">bimodal fearful &gt; bimodal neutral</italic> stimuli in the right hemisphere of the OFA (P = 0.014), and in both the left and right hemisphere of the FFA (P’s &lt; 0.01). Both regions showed a significant interaction between Hemisphere x Modality (P’s &lt; 0.05); in the OFA, this was caused by a greater response in the right &gt; left OFA to bimodal neutral and fearful stimuli (P’s &lt; 0.05); in the FFA, this was caused by a greater response to neutral voices in the left &gt; right hemisphere (P &lt; 0.05).</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-6"><p hwp:id="p-40">The response to unimodal faces, unimodal voices, and bimodal faces and voices in face-selective regions of interest. The right pSTS responded significantly more to bimodal stimuli (cong) than to unimodal faces and unimodal voices in the fearful conditions. No other face-selective regions responded more to bimodal &gt; unimodal stimuli. <sup>*</sup>P &lt; 0.05, <sup>*</sup><sup>*</sup>P &lt; 0.01, <sup>*</sup><sup>*</sup><sup>*</sup>P &lt; 0.001.</p></caption><graphic xlink:href="197426_fig5" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-41">The pSTS showed a different response. A 2 x 2 x 3 ANOVA showed an effect of Hemisphere (F(1,23) = 15.52, P &lt; 0.001), and Modality (F(2,46) = 6.13, P &lt; 0.005), but no effect of Emotion (F(1,23) = 1.55, P = 0.23). However, there was an interaction between Emotion x Modality (F(2,46) = 3.81, P &lt; 0.05) and Hemisphere x Emotion x Modality (F(2,46) = 3.71, P &lt; 0.05). These interactions were caused by differences between the response to bimodal stimuli across emotions and hemispheres. For fearful stimuli, the right pSTS responded significantly more to bimodal congruent stimuli than to both unimodal faces (<italic toggle="yes">CongF</italic> &gt; <italic toggle="yes">fearful faces</italic>: t(23) = 3.39, p &lt; 0.005) and unimodal voices (<italic toggle="yes">CongF</italic> &gt; <italic toggle="yes">fearful voices</italic>: t(23) = 3.61, p &lt; 0.001), but no difference in the response between fearful faces and fearful voices (t(23) = 1.45, p = 0.16). Contrarily, for neutral stimuli, there was no difference in the response between any conditions (P’s &gt; 0.24). In the left hemisphere, the pSTS showed a significantly greater response to congruent fearful stimuli as compared to fearful voices (<italic toggle="yes">CongF</italic> &gt; <italic toggle="yes">fearful voices</italic>: t(23) = 2.74, p &lt; 0.05). There were no other significant differences for either fearful conditions (P’s &gt; 0.11) or neutral stimuli conditions (P’s &gt; 0.13).</p></sec><sec id="s3b3" hwp:id="sec-19"><title hwp:id="title-21">Voice-selective regions</title><p hwp:id="p-42"><xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref> shows the response across subjects in voice-selective regions of interest (as defined by the conjunction (voices &gt; scrambled voices) ∩ (voices &gt; object sounds)) to the different conditions in the independent multisensory experiment.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-7"><p hwp:id="p-43">The response to unimodal faces, unimodal voices, and bimodal faces and voices in voice-selective regions of interest. No regions responded more to bimodal (cong) &gt; unimodal faces and unimodal voices. <sup>*</sup>P &lt; 0.05, <sup>*</sup><sup>*</sup>P &lt; 0.01, <sup>*</sup><sup>*</sup><sup>*</sup>P &lt; 0.001.</p></caption><graphic xlink:href="197426_fig6" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-44">2 (Hemisphere) x 2 (Emotion: neutral, fearful) x 3 (Modality: face, voice, bimodal) ANOVAs for the TVA and ATL, showed main effects of Modality (P’s &lt; 0.001), but no effects of Emotion (P’s &gt; 0.71). For all four regions (left TVA, right TVA, left ATL, right ATL), the effect of Modality was due to significantly greater responses to voices and congruent bimodal stimuli as compared to faces for neutral (<italic toggle="yes">neutral voices</italic> &gt; <italic toggle="yes">neutral faces</italic>; <italic toggle="yes">CongN</italic> &gt; <italic toggle="yes">neutral faces</italic>) and fearful stimuli (<italic toggle="yes">fearful voices</italic> &gt; <italic toggle="yes">fearful faces</italic>; <italic toggle="yes">CongF</italic> &gt; <italic toggle="yes">fearful faces</italic>) (P’s &lt; 0.05), but no difference in the response to unimodal voices and congruent bimodal stimuli (<italic toggle="yes">CongN</italic> = <italic toggle="yes">neutral voices</italic>; <italic toggle="yes">CongF</italic> = <italic toggle="yes">fearful voices</italic>) (P’s &gt; 0.18). In TVA, there was also a significant effect of Hemisphere (F(1,23) = 51.95, P &lt; 0.001) and an interaction between Hemisphere x Modality (F(2,46) = 14.63, P &lt; 0.001). This was caused by a greater response in the right &gt; left TVA for all corresponding conditions (i.e. neutral voices in right TVA &gt; neutral voices in left TVA) (P’s &lt; 0.001). No such differences were observed in ATL.</p><p hwp:id="p-45">A 2 x 3 ANOVA for the left aSTS showed an effect of Emotion (F(1,23) = 6.82, P &lt; 0.05) and Modality (F(2,46) = 21.63, P &lt; 0.001), but no interaction between Emotion x Modality (F(2,46) = 0.27, P = 0.76). For neutral stimuli, there was a greater response to voices and congruent bimodal stimuli as compared to faces (P’s &lt; 0.001), with no difference in the response between faces and congruent bimodal stimuli (<italic toggle="yes">CongN</italic> = <italic toggle="yes">neutral voices</italic>: t(23) = 1.91, p = 0.07). For fearful stimuli, there was a greater response to voices and congruent bimodal stimuli than to faces (P’s &lt; 0.001); however, there was a significantly greater response to fearful voices as compared to congruent bimodal stimuli (<italic toggle="yes">fearful voices &gt; CongF</italic>: t(23) = 2.14, p &lt; 0.05), showing that this region responds more to fearful voices when presented in isolation as compared to when they were presented with congruent facial stimuli.</p></sec></sec><sec id="s3c" hwp:id="sec-20"><title hwp:id="title-22">Congruency effects</title><p hwp:id="p-46">To identify regions that are involved in processing multisensory stimuli as a function of congruency, we examined the response to <italic toggle="yes">congruent &gt; incongruent</italic> stimuli [<italic toggle="yes">(congruent neutral + congruent fearful) – (both incongruent conditions)]</italic> as well as the reverse contrast to show regions responding more to <italic toggle="yes">incongruent &gt; congruent</italic> stimuli [(<italic toggle="yes">both incongruent conditions</italic>) - (<italic toggle="yes">congruent neutral</italic> + <italic toggle="yes">congruent fearful</italic>)].</p><p hwp:id="p-47"><xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7A</xref> shows several regions that responded more to <italic toggle="yes">incongruent &gt; congruent</italic> stimuli in a whole brain analysis at a liberal statistical threshold (P &lt; 0.05, resel corrected for multiple comparisons). These include regions in bilateral middle frontal gyrus (lMFG, rMFG), bilateral superior parietal cortex (lSupPar, rSupPar), the left middle temporal gyrus, a portion of the left frontal pole (lFront Pole), and a portion of the right pSTS (<xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>). <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Figure 8</xref> shows that this portion of rpSTS (green) partially overlaps with the face-selective rpSTS (yellow), and to a much lesser extent, the area in rpSTS showing a multimodal effect for fearful stimuli in the group analysis (red). However, with the exception of a trend in rSTS, none of these regions responded significantly more to <italic toggle="yes">bimodal &gt; unimodal</italic> stimuli. In contrast, no regions responded more to congruent than to incongruent stimuli – even at the more liberal (P &lt; 0.001, uncorrected) threshold.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-8"><p hwp:id="p-48">Congruency effects. A). Regions showing congruency effects in the whole-brain analysis. Several regions responded more to <italic toggle="yes">incongruent &gt; congruent</italic> stimuli, including bilateral middle frontal gyrus (lMFG, rMFG), bilateral superior parietal cortex (lSupPar, rSupPar), a portion of the left frontal pole (lFrontPole), and a portion of the rpSTS (indicated by a white circle). Aside from a trend in rpSTS, none of these regions responded more to bimodal &gt; unimodal stimuli. No regions responded more to <italic toggle="yes">congruent &gt; incongruent</italic> stimuli. Statistical maps thresholded at P &lt; 0.05 resel corrected. * P &lt; 0.05. B) The response to congruent and incongruent stimuli in the face-selective and voice-selective regions of interest, as well as the region in the group analysis which showed a greater response to bimodal &gt; unimodal stimuli (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>). There was a greater response to <italic toggle="yes">incongruent &gt; congruent</italic> stimuli in the voice-selective right TVA, the face-selective rpSTS, and the region that responded more to <italic toggle="yes">bimodal &gt; unimodal</italic> stimuli in the group analysis (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4</xref>).</p></caption><graphic xlink:href="197426_fig7" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-9"><p hwp:id="p-49">Regions of overlap in the rpSTS. Shown are the face-selective rpSTS (yellow), the region that responded to <italic toggle="yes">bimodal &gt; unimodal</italic> fearful stimuli in the group analysis (red; see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4</xref>), and the region in that responded more to <italic toggle="yes">incongruent &gt; incongruent</italic> stimuli in the group analysis (green; <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7</xref>). Statistical maps thresholded at P &lt; 0.05 resel corrected.</p></caption><graphic xlink:href="197426_fig8" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-10"><p hwp:id="p-50">Regions showing congruency effects in the whole-brain analysis (P &lt; 0.05, resel corrected).</p></caption><graphic xlink:href="197426_tbl2" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap><p hwp:id="p-51">Next, we examined the response to congruent and incongruent stimuli in the face- and voice-selective regions of interest (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7B</xref>). A 2x6 ANOVA (congruency, region) for the face-selective regions showed no effect of Congruency (F(1,23) = 1.75, P = 0.20), but significant effects of Region (F(5,115) = 6.05, P &lt; 0.001) and an interaction between Congruency x Region (F(5,115) = 3.61, P &lt; 0.005). Paired-samples t-tests showed this was due to a greater response to <italic toggle="yes">incongruent &gt; congruent</italic> stimuli in the rpSTS (t(24) = 2.84, p &lt; 0.05). There were no other significant differences (P’s &gt; 0.20). A 2x5 ANOVA for the voice-selective regions also showed no effect of Congruency (F(1,23) = 2.29, P = 0.14), but significant effects of Region (F(4,92) = 7.04, P &lt; 0.001) and an interaction between Congruency x Region (F(4,92) = 3.48, P &lt; 0.05). This was due to a greater response to incongruent &gt; congruent stimuli in the voice-selective right TVA (t(24) = 2.84, p &lt; 0.05); there were no other significant differences (P’s &gt; 0.20). Finally, a paired-samples t-test in the portion of rpSTS which showed a multimodal effect in the whole brain analysis, revealed a greater response to incongruent &gt; congruent stimuli (t(24) = 2.11, p &lt; 0.05).</p></sec><sec id="s3d" hwp:id="sec-21"><title hwp:id="title-23">Dynamic Causal Modeling</title><p hwp:id="p-52">We used Dynamic Causal modeling (DCM; (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Friston et al., 2003</xref>) to investigate the cortical interactions between the face-selective right FFA (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2</xref>), the voice-selective right TVA (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3</xref>) and the portion of right posterior STS (rpSTS) showing a multisensory response (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Figure 4</xref>) to understand the network dynamic of how emotional faces and voices are integrated. The FFA and TVA were chosen based on previous studies suggesting these regions are involved in individuating faces (<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Rotshtein et al., 2005</xref>) and voices (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Belin and Zatorre, 2003</xref>) respectively. Only right hemisphere regions were included in the analysis due to the multisensory response being observed in the rpSTS. The emotional expression condition was entered as a modulator of connectivity (B) between unisensory (FFA, TVA) and multisensory (rpSTS) regions, as well as a direct input to the rpSTS (C).</p><p hwp:id="p-53">Since we were specifically interested in how the rpSTS integrates information from the FFA and TVA, and putative connections between FFA and TVA, we created 8 plausible models containing different ways in which these nodes may be intrinsically connected. We were specifically interested in whether reciprocal connections exist between the face-selective FFA and voice-selective TVA such has been previously suggested by studies on identity recognition (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-2" hwp:rel-id="ref-63">von Kriegstein and Giraud, 2006</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-3" hwp:rel-id="ref-6">Blank et al., 2011</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Blank et al., 2014</xref>), and how these regions are connected to the rpSTS a region that this study and others (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Ethofer et al., 2006</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">Kreifelts et al., 2009</xref>; <xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Watson et al., 2014b</xref>) have implicated as being involved in the integration of face and voice emotion information. Crucially, we examined the connections between the face-selective and voice-selective regions of interest identified in independent localiser scans, and the portion of rpSTS which showed a greater response to bimodal &gt; unimodal faces and voices in the whole brain analysis (see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Figure 4</xref>).</p><p hwp:id="p-54">Each model was repeated for three alternative points at which emotional expression might influence the intrinsic connections: 1) directly modulating the response of the rpSTS; 2) modulating the connections between the unisensory FFA/TVA and the multisensory rpSTS; 3) directly modulating the response of the rpSTS as well as the connections between the unisensory FFA/TVA and the multisensory rpSTS. This resulted in a total of 24 models (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure 9A</xref>).</p><fig id="fig9" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;197426v1/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9.</label><caption hwp:id="caption-11"><p hwp:id="p-55">DCM analysis. <bold>A</bold>) Models entered into the DCM analysis. The model space was defined by variations of either feed-forward or feed-backward endogenous connections between the three regions (8 model types; rows), the type of modulation imposed onto the endogenous connections (columns) by expression. The resulting model space consisted of 24 models in total, which were entered into a RFX BMS analysis. <bold>B</bold>) Model exceedance probability obtained using RFX BMS across all participants, showed a strong preference for model 9. <bold>C</bold>) The winning model consisted of information entering the right FFA (yellow) and right TVA (blue), and significant feed-forward effective connectivity with the right pSTS (red). Emotional expression modulated the network response of the endogenous connections between the unimodal regions (right FFA and right TVA) and the multisensory right pSTS (as also shown in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Figure 4</xref>).</p></caption><graphic xlink:href="197426_fig9" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><p hwp:id="p-56"><xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Figure 9B</xref> shows the model exceedance probability obtained using RFX BMS across all participants for each of the 24 models in our models space. The highly winning model is model 9, with an exceedance probability of 0.9783 and expected posterior probability of 0.3151. The second highest exceedance probability was below 0.006. The winning model (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Figure 9C</xref>) consisted of forward intrinsic connections from the FFA and TVA towards the rpSTS, with a modulatory effect of emotional expression of the face and/or voice on the network response of the endogenous connections between the unimodal regions (FFA and TVA) and the multisensory rpSTS (as shown in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2</xref>).</p></sec></sec><sec id="s4" hwp:id="sec-22"><title hwp:id="title-24">DISCUSSION</title><p hwp:id="p-57">This study set out to specifically test at which stage of the face and voice processing hierarchy these signals are integrated, and how emotional expressions influences this integration. We also assessed whether crossmodal responses could be observed in face or voice selective regions (preferential response to face in voice selective region and reversely). Finally, we explored how information flows between face-selective, voice-selective and integrative regions.</p><p hwp:id="p-58">Influential models of identity recognition propose that integration of information across the different sensory modalilites is performed at an independent stage where information from both modalities converge (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Bruce and Young, 1986</xref>; <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Burton et al., 1990</xref>). However, more recent animal (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Ghazanfar et al., 2005</xref>; <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">Perrodin et al., 2014</xref>, <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">2015</xref>) and human (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-3" hwp:rel-id="ref-64">Von Kriegstein et al., 2005</xref>; <xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-3" hwp:rel-id="ref-63">von Kriegstein and Giraud, 2006</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-4" hwp:rel-id="ref-6">Blank et al., 2011</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">Joassin et al., 2011</xref>) studies suggested that multisensory integration of faces and voices may occur at earlier stages of processing, including in regions typically thought to be uniquely face or voice selective. For instance, in humans, studies have suggested that integration of face and voice identity information occurs via reciprocal connections between unisensory regions (for a review, see (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Blank et al., 2014</xref>), with a portion of the fusiform cortex and the voice-responsive middle/anterior STS showing increased functional connectivity during a speaker recognition task as compared to a content-related task (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-4" hwp:rel-id="ref-64">Von Kriegstein et al., 2005</xref>), while the extent of the functional connectivity between these regions correlates with voice recognition abilities across subjects (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Blank et al., 2015</xref>).</p><p hwp:id="p-59">In our study, we found strong evidence that multisensory integration between face and voice signals of emotional expressions is highly selective to the right posterior STS (rpSTS) and arise after unimodal computation in unimodal face and voice selective regions. First, we observed that a greater response to bimodal fearful stimuli as compared to unimodal stimuli in the whole-brain analysis was unique to the rpSTS. This region also activated differentially when faces and voices were presented in a congruent or incongruent fashion. This region overlapped with the face selective rpSTS patch (as defined in an independent face localiser). Importantly, we found that the greater response to bimodal than unimodal stimuli in the rpSTS was specific to fearful faces. Finally, we found that the multisensory rpSTS receives uni-directional information from the face-selective FFA, and voice selective TVA, with emotional expression affecting the strength of the connections.</p><p hwp:id="p-60">Using independent localisers to identify face and voice-selective regions of interest, we examined the response in these regions to their non-preferred stimulus. The FFA and OFA regions of the face selective network (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2</xref>) and the TVA, ATL, and aSTS regions of the voice selective network (see <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3</xref>) did not show any sign of crossmodal responses (faces in voice selective region or voices in face-selective regions) or of multisensory integration (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figures 5</xref>-<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">6</xref>). However, we observed significant responses to voices in bilateral pSTS as defined from the functional face localiser. Further, the face-selective rpSTS responded more to bimodal than unimodal stimuli, and responded more to incongruent than congruent stimuli. Together, our results provide strong support for a unique role of the rpSTS in multisensory integration of face-voice emotional stimuli.</p><p hwp:id="p-61">Previous studies suggested that a portion of the fusiform gyrus plays a role in multisensory integration of faces and voices. For example, the fusiform shows a greater response to bimodal as compared to unimodal stimuli (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-3" hwp:rel-id="ref-36">Joassin et al., 2011</xref>) as well as an increased neural response following audio-visual learning of speaker recognition (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">von Kriegstein et al., 2008</xref>). However, we found no evidence of multisensory integration in this region – neither in the whole brain group analysis, nor in the face-selective FFA identified in an independent localiser. Interestingly, studies suggesting a role of the fusiform gyrus in multisensory integration of faces and voices manipulate the identity pairing of face and voice stimuli (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-2" hwp:rel-id="ref-65">von Kriegstein et al., 2008</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-4" hwp:rel-id="ref-36">Joassin et al., 2011</xref>), whilst the current study manipulated the emotion pairing. It is therefore possible that the FFA might play a multisensory role for identity discrimination while the rpSTS would integrate facial and vocal signals for emotional content. This is in line with models of face recognition proposing distinct pathways for processing identity and emotional expressions (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">Bruce and Young, 1986</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Burton et al., 1999</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Haxby et al., 2000</xref>), as well as neuroimaging studies showing the FFA is sensitive to changes in identity but invariant to changes in emotional expressions (<xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">Winston et al., 2004</xref>), while the rpSTS is sensitive to emotional expression (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Narumoto et al., 2001</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-2" hwp:rel-id="ref-71">Winston et al., 2004</xref>) but not to changes in identity (<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Davies-Thompson et al., 2009</xref>). However, other studies have shown significant interactions between identity and emotional expression processing; for example the response in the FFA and rpSTS varies depending on whether participants are asked to attend to the identity or the emotional expression of the face stimuli (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Gorno-Tempini et al., 2001</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Ganel et al., 2005</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Fox et al., 2009</xref>). Such task effects raises the interesting idea that the differences observed between our study showing multisensory integration of emotive face and voice stimuli in rpSTS, and other studies showing multisensory integration of identity face and voice information in the fusiform gyrus (i.e. (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-3" hwp:rel-id="ref-65">von Kriegstein et al., 2008</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-5" hwp:rel-id="ref-36">Joassin et al., 2011</xref>)), could be task-dependent constraints (emotion versus identity).</p><p hwp:id="p-62">Neuropsychological studies have shown that individuals with face recognition deficits (prosopagnosia) typically remain able to identify individuals from voices (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Liu et al., 2014</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Liu et al., 2015</xref>), while individuals with voice recognition deficits (phonagnosia) have intact face recognition (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Garrido et al., 2009</xref>). In line with these studies, with the exception of the rpSTS, we found no significant crossmodal responses, or evidence of multisensory integration, in face-selective (OFA, FFA) or voice-selective (TVA, ATL, aSTS) regions; specifically, we observed no significant responses to voices in face-selective regions that, following damage, can result in prosopagnosia. Interestingly, damage to the rpSTS can result in deficits in facial expressions with intact facial identity (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Fox et al., 2011</xref>), but can also leave vocal identity and vocal expression recognition intact (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Jiahui et al., 2017</xref>). This suggests that while right rpSTS may be involved in multisensory integration of faces and voices, it is somewhat independent of unimodal voice processing.</p><p hwp:id="p-63">We also examined the response to congruent versus incongruent bimodal stimuli. Behavioral studies have demonstrated that enhanced performance in emotion categorization tasks for congruent emotional stimuli, but impaired performance for incongruent stimuli (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-2" hwp:rel-id="ref-43">Massaro and Egan, 1996</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">De Gelder and Vroomen, 2000</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-3" hwp:rel-id="ref-18">Collignon et al., 2008</xref>). We reasoned that, if a region is involved in the integration of faces and voices then one would expect this region to be sensitive to congruency effects. We found that the rpSTS was indeed sensitive to congruency, both at the level of the region of interest analysis, as well as at the level of the whole-brain analysis, providing further support for the rpSTS to be involved in the multisensory integration of faces and voices of emotional stimuli. None of the other regions of the face or voice selective network showed sensitivity to the congruency of the stimuli. However, in addition to the rpSTS, several other regions responded more to incongruent than congruent stimuli, including the bilateral middle frontal gyrus, bilateral superior parietal cortex, and a portion of the left frontal pole (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Figure 7</xref>). These regions are similar to those observed in previous studies suggesting a cingulate-fronto-parietal network involved in conflict monitoring (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Carter et al., 1998</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Botvinick et al., 2004</xref>; <xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">Weissman et al., 2004</xref>; <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Ochsner et al., 2009</xref>; <xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">Wittfoth et al., 2009</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Müller et al., 2011</xref>). However, unlike the rpSTS, none of these regions responded significantly more to congruent bimodal than to unimodal stimuli for either fearful or neutral stimuli, or showed face or voice selectivity in the independent localisers. Both of these methods have been used previously to identify regions of the brain involved in multisensory integration; however, the little overlap between regions responding more to bimodal than to unimodal stimuli, and regions showing congruency effects, suggest that these manipulations isolate distinct networks and processes (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Morís Fernández et al., 2017</xref>).</p><p hwp:id="p-64">Using DCM, we explored how face and voice signals flow between face-selective, voice selective and heteromodal rpSTS regions, and how this information transfer is modulated by the emotional content of the stimuli. More specifically, we wanted to test if the integration of emotion across faces and voices relies on direct reciprocal interaction between FFA and TVA regions as previously suggested for the integration of identity information (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-4" hwp:rel-id="ref-63">von Kriegstein and Giraud, 2006</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-5" hwp:rel-id="ref-6">Blank et al., 2011</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">Blank et al., 2014</xref>), or alternatively if face and voice signal is transferred and then integrated in a distinct convergence region in the pSTS (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">Ethofer et al., 2006</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-3" hwp:rel-id="ref-38">Kreifelts et al., 2009</xref>; <xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-2" hwp:rel-id="ref-68">Watson et al., 2014b</xref>). We found strong evidence for the model receiving uni-directional information from the face-selective FFA and voice-selective TVA. Further, the winning model included fearful expressions affecting the strength of the connections between the unisensory regions (FFA, TVA) and the rpSTS. These results provide support for a hierarchical architecture for the integration of emotional signals from faces and voices, with unisensory face and voice selective regions projecting to rpSTS where those information are integrated.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-25">Acknowledgements</title><p hwp:id="p-65">This study was supported in part by the Center for Mind and Brain Science (CIMeC), University of Trento and by a European Research Council starting grant attributed to OC (MADVIS: ERC StG 337573).</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-26">Reference</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2 xref-ref-1-3 xref-ref-1-4"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Beauchamp MS"><surname>Beauchamp</surname> <given-names>MS</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-2">See me, hear me, touch me: multisensory integration in lateral occipital-temporal cortex</article-title>. <source hwp:id="source-1">Current opinion in neurobiology</source> <volume>15</volume>:<fpage>145</fpage>-<lpage>153</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zatorre RJ"><surname>Zatorre</surname> <given-names>RJ</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-3">Adaptation to speaker's voice in right anterior temporal lobe</article-title>. <source hwp:id="source-2">Neuroreport</source> <volume>14</volume>:<fpage>2105</fpage>–<lpage>2109</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zatorre RJ"><surname>Zatorre</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ahad P"><surname>Ahad</surname> <given-names>P</given-names></string-name> (<year>2002</year>) <article-title hwp:id="article-title-4">Human temporal-lobe response to vocal sounds</article-title>. <source hwp:id="source-3">Cognitive Brain Research</source> <volume>13</volume>:<fpage>17</fpage>-<lpage>26</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fillion-Bilodeau S"><surname>Fillion-Bilodeau</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gosselin F"><surname>Gosselin</surname> <given-names>F</given-names></string-name> (<year>2008</year>) <article-title hwp:id="article-title-5">The Montreal Affective Voices: a validated set of nonverbal affect bursts for research on auditory affective processing</article-title>. <source hwp:id="source-4">Behavior research methods</source> <volume>40</volume>:<fpage>531</fpage>-<lpage>539</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zatorre RJ"><surname>Zatorre</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lafaille P"><surname>Lafaille</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ahad P"><surname>Ahad</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pike B"><surname>Pike</surname> <given-names>B</given-names></string-name> (<year>2000</year>) <article-title hwp:id="article-title-6">Voice-selective areas in human auditory cortex</article-title>. <source hwp:id="source-5">Nature</source> <volume>403</volume>:<fpage>309</fpage>-<lpage>312</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2 xref-ref-6-3 xref-ref-6-4 xref-ref-6-5"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Blank H"><surname>Blank</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anwander A"><surname>Anwander</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="von Kriegstein K"><surname>von Kriegstein</surname> <given-names>K</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-7">Direct structural connections between voice-and face-recognition areas</article-title>. <source hwp:id="source-6">The Journal of Neuroscience</source> <volume>31</volume>:<fpage>12906</fpage>-<lpage>12915</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Blank H"><surname>Blank</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wieland N"><surname>Wieland</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="von Kriegstein K"><surname>von Kriegstein</surname> <given-names>K</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-8">Person recognition and the brain: merging evidence from patients and healthy individuals</article-title>. <source hwp:id="source-7">Neuroscience &amp; Biobehavioral Reviews</source> <volume>47</volume>:<fpage>717</fpage>-<lpage>734</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Blank H"><surname>Blank</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kiebel SJ"><surname>Kiebel</surname> <given-names>SJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="von Kriegstein K"><surname>von Kriegstein</surname> <given-names>K</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-9">How the human brain exchanges information across sensory modalities to recognize other people</article-title>. <source hwp:id="source-8">Hum Brain Mapp</source> <volume>36</volume>:<fpage>324</fpage>-<lpage>339</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Bonda E"><surname>Bonda</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Petrides M"><surname>Petrides</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ostry D"><surname>Ostry</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Evans A"><surname>Evans</surname> <given-names>A</given-names></string-name> (<year>1996</year>) <article-title hwp:id="article-title-10">Specific involvement of human parietal systems and the amygdala in the perception of biological motion</article-title>. <source hwp:id="source-9">J Neurosci</source> <volume>16</volume>:<fpage>3737</fpage>-<lpage>3744</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cohen JD"><surname>Cohen</surname> <given-names>JD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carter CS"><surname>Carter</surname> <given-names>CS</given-names></string-name> (<year>2004</year>) <article-title hwp:id="article-title-11">Conflict monitoring and anterior cingulate cortex: an update</article-title>. <source hwp:id="source-10">Trends in cognitive sciences</source> <volume>8</volume>:<fpage>539</fpage>-<lpage>546</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Bruce V"><surname>Bruce</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Young A"><surname>Young</surname> <given-names>A</given-names></string-name> (<year>1986</year>) <source hwp:id="source-11">Understanding face recognition. British Journal of Psychology</source> <volume>77</volume>:<fpage>305</fpage>-<lpage>327</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Burton A"><surname>Burton</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bruce V"><surname>Bruce</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnston R"><surname>Johnston</surname> <given-names>R</given-names></string-name> (<year>1990</year>) <article-title hwp:id="article-title-12">Understanding face recognition with an interactive activation model</article-title>. <source hwp:id="source-12">Brit J Psychol</source> <volume>81</volume>:<fpage>361</fpage>-<lpage>380</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Burton A"><surname>Burton</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bruce V"><surname>Bruce</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hancock P"><surname>Hancock</surname> <given-names>P</given-names></string-name> (<year>1999</year>) <article-title hwp:id="article-title-13">From pixels to people: a model of familiar face recognition</article-title>. <source hwp:id="source-13">Cognitive Sci</source> <volume>23</volume>:<fpage>1</fpage>-<lpage>31</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Calvert GA"><surname>Calvert</surname> <given-names>GA</given-names></string-name> (<year>2001</year>) <article-title hwp:id="article-title-14">Crossmodal processing in the human brain: insights from functional neuroimaging studies</article-title>. <source hwp:id="source-14">Cerebral cortex</source> <volume>11</volume>:<fpage>1110</fpage>-<lpage>1123</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Campanella S"><surname>Campanella</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-15">Integrating face and voice in person perception</article-title>. <source hwp:id="source-15">Trends in cognitive sciences</source> <volume>11</volume>:<fpage>535</fpage>-<lpage>543</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Carter CS"><surname>Carter</surname> <given-names>CS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Braver TS"><surname>Braver</surname> <given-names>TS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barch DM"><surname>Barch</surname> <given-names>DM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Noll D"><surname>Noll</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cohen JD"><surname>Cohen</surname> <given-names>JD</given-names></string-name> (<year>1998</year>) <article-title hwp:id="article-title-16">Anterior cingulate cortex, error detection, and the online monitoring of performance</article-title>. <source hwp:id="source-16">Science</source> <volume>280</volume>:<fpage>747</fpage>-<lpage>749</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Castelli F"><surname>Castelli</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Happé F"><surname>Happé</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frith U"><surname>Frith</surname> <given-names>U</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frith C"><surname>Frith</surname> <given-names>C</given-names></string-name> (<year>2000</year>) <article-title hwp:id="article-title-17">Movement and mind: a functional imaging study of perception and interpretation of complex intentional movement patterns</article-title>. <source hwp:id="source-17">Neuroimage</source> <volume>12</volume>:<fpage>314</fpage>-<lpage>325</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2 xref-ref-18-3"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Collignon O"><surname>Collignon</surname> <given-names>O</given-names></string-name>, <string-name name-style="western" hwp:sortable="Girard S"><surname>Girard</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gosselin F"><surname>Gosselin</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Roy S"><surname>Roy</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Saint-Amour D"><surname>Saint-Amour</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lassonde M"><surname>Lassonde</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lepore F"><surname>Lepore</surname> <given-names>F</given-names></string-name> (<year>2008</year>) <article-title hwp:id="article-title-18">Audio-visual integration of emotion expression</article-title>. <source hwp:id="source-18">Brain research</source> <volume>1242</volume>:<fpage>126</fpage>-<lpage>135</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="other" citation-type="journal" ref:id="197426v1.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Thompson J Davies"><given-names>Davies</given-names> <surname>Thompson J</surname></string-name>, <string-name name-style="western" hwp:sortable="Andrews TJ"><surname>Andrews</surname> <given-names>TJ</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-19">Intra-and inter-hemispheric connectivity between face-selective regions in the human brain</article-title>. <source hwp:id="source-19">J Neurophysiol</source>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Thompson J Davies"><given-names>Davies</given-names> <surname>Thompson J</surname></string-name>, <string-name name-style="western" hwp:sortable="Gouws A"><surname>Gouws</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Andrews TJ"><surname>Andrews</surname> <given-names>TJ</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-20">An image-dependent representation of familiar and unfamiliar faces in the human ventral stream</article-title>. <source hwp:id="source-20">Neuropsychologia</source> <volume>47</volume>:<fpage>1627</fpage>-<lpage>1635</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="De Gelder B"><surname>De Gelder</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vroomen J"><surname>Vroomen</surname> <given-names>J</given-names></string-name> (<year>2000</year>) <article-title hwp:id="article-title-21">The perception of emotions by ear and by eye</article-title>. <source hwp:id="source-21">Cognition &amp; Emotion</source> <volume>14</volume>:<fpage>289</fpage>-<lpage>311</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Ethofer T"><surname>Ethofer</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pourtois G"><surname>Pourtois</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wildgruber D"><surname>Wildgruber</surname> <given-names>D</given-names></string-name> (<year>2006</year>) <article-title hwp:id="article-title-22">Investigating audiovisual integration of emotional signals in the human brain</article-title>. <source hwp:id="source-22">Progress in brain research</source> <volume>156</volume>:<fpage>345</fpage>-<lpage>361</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Ethofer T"><surname>Ethofer</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bretscher J"><surname>Bretscher</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wiethoff S"><surname>Wiethoff</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bisch J"><surname>Bisch</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schlipf S"><surname>Schlipf</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wildgruber D"><surname>Wildgruber</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kreifelts B"><surname>Kreifelts</surname> <given-names>B</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-23">Functional responses and structural connections of cortical areas for processing faces and voices in the superior temporal sulcus</article-title>. <source hwp:id="source-23">Neuroimage</source> <volume>76</volume>:<fpage>45</fpage>-<lpage>56</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Föcker J"><surname>Föcker</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gondan M"><surname>Gondan</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Röder B"><surname>Röder</surname> <given-names>B</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-24">Preattentive processing of audio-visual emotional signals</article-title>. <source hwp:id="source-24">Acta psychologica</source> <volume>137</volume>:<fpage>36</fpage>-<lpage>47</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Fox CJ"><surname>Fox</surname> <given-names>CJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moon S"><surname>Moon</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iaria G"><surname>Iaria</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barton JJS"><surname>Barton</surname> <given-names>JJS</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-25">The correlates of subjective perception of identity and expression in the face network: An fMRI adaptation study</article-title>. <source hwp:id="source-25">Neuroimage</source> <volume>44</volume>:<fpage>569</fpage>-<lpage>580</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Fox CJ"><surname>Fox</surname> <given-names>CJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hanif HM"><surname>Hanif</surname> <given-names>HM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iaria G"><surname>Iaria</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duchaine BC"><surname>Duchaine</surname> <given-names>BC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barton JJS"><surname>Barton</surname> <given-names>JJS</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-26">Perceptual and anatomic patterns of selective deficits in facial identity and expression processing</article-title>. <source hwp:id="source-26">Neuropsychologia</source> <volume>49</volume>:<fpage>3188</fpage>-<lpage>3200</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Friston KJ"><surname>Friston</surname> <given-names>KJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harrison L"><surname>Harrison</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Penny W"><surname>Penny</surname> <given-names>W</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-27">Dynamic causal modelling</article-title>. <source hwp:id="source-27">Neuroimage</source> <volume>19</volume>:<fpage>1273</fpage>-<lpage>1302</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Ganel T"><surname>Ganel</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Valyear KF"><surname>Valyear</surname> <given-names>KF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gottstein Y Goshen"><given-names>Goshen</given-names> <surname>Gottstein Y</surname></string-name>, <string-name name-style="western" hwp:sortable="Goodale MA"><surname>Goodale</surname> <given-names>MA</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-28">The involvement of the " fusiform face area" in processing facial expression</article-title>. <source hwp:id="source-28">Neuropsychologia</source> <volume>43</volume>:<fpage>1645</fpage>-<lpage>1654</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Garrido L"><surname>Garrido</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eisner F"><surname>Eisner</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGettigan C"><surname>McGettigan</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stewart L"><surname>Stewart</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sauter D"><surname>Sauter</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hanley JR"><surname>Hanley</surname> <given-names>JR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schweinberger SR"><surname>Schweinberger</surname> <given-names>SR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Warren JD"><surname>Warren</surname> <given-names>JD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duchaine B"><surname>Duchaine</surname> <given-names>B</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-29">Developmental phonagnosia: a selective deficit of vocal identity recognition</article-title>. <source hwp:id="source-29">Neuropsychologia</source> <volume>47</volume>:<fpage>123</fpage>-<lpage>131</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Ghazanfar AA"><surname>Ghazanfar</surname> <given-names>AA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maier JX"><surname>Maier</surname> <given-names>JX</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoffman KL"><surname>Hoffman</surname> <given-names>KL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Logothetis NK"><surname>Logothetis</surname> <given-names>NK</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-30">Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title>. <source hwp:id="source-30">Journal of Neuroscience</source> <volume>25</volume>:<fpage>5004</fpage>-<lpage>5012</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Gorno Tempini ML"><surname>Gorno Tempini</surname> <given-names>ML</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pradelli S"><surname>Pradelli</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Serafini M"><surname>Serafini</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pagnoni G"><surname>Pagnoni</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baraldi P"><surname>Baraldi</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Porro C"><surname>Porro</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nicoletti R"><surname>Nicoletti</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Umità C"><surname>Umità</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichelli P"><surname>Nichelli</surname> <given-names>P</given-names></string-name> (<year>2001</year>) <article-title hwp:id="article-title-31">Explicit and incidental facial expression processing: an fMRI study</article-title>. <source hwp:id="source-31">Neuroimage</source> <volume>14</volume>:<fpage>465</fpage>-<lpage>473</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Grossman ED"><surname>Grossman</surname> <given-names>ED</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blake R"><surname>Blake</surname> <given-names>R</given-names></string-name> (<year>2002</year>) <article-title hwp:id="article-title-32">Brain areas active during visual perception of biological motion</article-title>. <source hwp:id="source-32">Neuron</source> <volume>35</volume>:<fpage>1167</fpage>-<lpage>1175</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Haxby JV"><surname>Haxby</surname> <given-names>JV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoffman EA"><surname>Hoffman</surname> <given-names>EA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gobbini MI"><surname>Gobbini</surname> <given-names>MI</given-names></string-name> (<year>2000</year>) <article-title hwp:id="article-title-33">The distributed human neural system for face perception</article-title>. <source hwp:id="source-33">Trends Cogn Sci</source> <volume>4</volume>:<fpage>223</fpage>-<lpage>233</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Hölig C"><surname>Hölig</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Föcker J"><surname>Föcker</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Best A"><surname>Best</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Röder B"><surname>Röder</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="chel Bü"><given-names>Bü</given-names> <surname>chel</surname></string-name> C (<year>2017</year>) <article-title hwp:id="article-title-34">Activation in the angular gyrus and in the pSTS is modulated by face primes during voice recognition</article-title>. <source hwp:id="source-34">Hum Brain Mapp</source> <volume>38</volume>:<fpage>2553</fpage>-<lpage>2565</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="other" citation-type="journal" ref:id="197426v1.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Jiahui G"><surname>Jiahui</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garrido L"><surname>Garrido</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu RR"><surname>Liu</surname> <given-names>RR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Susilo T"><surname>Susilo</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barton JJ"><surname>Barton</surname> <given-names>JJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duchaine B"><surname>Duchaine</surname> <given-names>B</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-35">Normal voice processing after posterior superior temporal sulcus lesion</article-title>. <source hwp:id="source-35">Neuropsychologia</source>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2 xref-ref-36-3 xref-ref-36-4 xref-ref-36-5"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Joassin F"><surname>Joassin</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pesenti M"><surname>Pesenti</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maurage P"><surname>Maurage</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Verreckt E"><surname>Verreckt</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bruyer R"><surname>Bruyer</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Campanella S"><surname>Campanella</surname> <given-names>S</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-36">Cross-modal interactions between human faces and voices involved in person recognition</article-title>. <source hwp:id="source-36">Cortex</source> <volume>47</volume>:<fpage>367</fpage>-<lpage>376</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Kanwisher N"><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="McDermott J"><surname>McDermott</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chun MM"><surname>Chun</surname> <given-names>MM</given-names></string-name> (<year>1997</year>) <article-title hwp:id="article-title-37">The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source hwp:id="source-37">Journal of Neuroscience</source> <volume>17</volume>:<fpage>4302</fpage>-<lpage>4311</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2 xref-ref-38-3"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Kreifelts B"><surname>Kreifelts</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ethofer T"><surname>Ethofer</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shiozawa T"><surname>Shiozawa</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grodd W"><surname>Grodd</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wildgruber D"><surname>Wildgruber</surname> <given-names>D</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-38">Cerebral representation of non-verbal emotional perception: fMRI reveals audiovisual integration area between voice-and face-sensitive regions in the superior temporal sulcus</article-title>. <source hwp:id="source-38">Neuropsychologia</source> <volume>47</volume>:<fpage>3059</fpage>-<lpage>3066</lpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Langner O"><surname>Langner</surname> <given-names>O</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dotsch R"><surname>Dotsch</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bijlstra G"><surname>Bijlstra</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wigboldus DH"><surname>Wigboldus</surname> <given-names>DH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawk ST"><surname>Hawk</surname> <given-names>ST</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Knippenberg A"><surname>van Knippenberg</surname> <given-names>A</given-names></string-name> (<year>2010</year>) <article-title hwp:id="article-title-39">Presentation and validation of the Radboud Faces Database</article-title>. <source hwp:id="source-39">Cognition and emotion</source> <volume>24</volume>:<fpage>1377</fpage>-<lpage>1388</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Laurienti PJ"><surname>Laurienti</surname> <given-names>PJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perrault TJ"><surname>Perrault</surname> <given-names>TJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stanford TR"><surname>Stanford</surname> <given-names>TR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wallace MT"><surname>Wallace</surname> <given-names>MT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stein BE"><surname>Stein</surname> <given-names>BE</given-names></string-name> (<year>2005</year>) <source hwp:id="source-40">On the use of superadditivity as a metric for characterizing multisensory integration in functional neuroimaging studies. Experimental Brain Research</source> <volume>166</volume>:<fpage>289</fpage>-<lpage>297</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="other" citation-type="journal" ref:id="197426v1.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Liu RR"><surname>Liu</surname> <given-names>RR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pancaroglu R"><surname>Pancaroglu</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hills CS"><surname>Hills</surname> <given-names>CS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duchaine B"><surname>Duchaine</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barton JJ"><surname>Barton</surname> <given-names>JJ</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-40">Voice recognition in face-blind patients</article-title>. <source hwp:id="source-41">Cerebral Cortex:bhu240</source>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Liu RR"><surname>Liu</surname> <given-names>RR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Corrow SL"><surname>Corrow</surname> <given-names>SL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pancaroglu R"><surname>Pancaroglu</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duchaine B"><surname>Duchaine</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barton JJ"><surname>Barton</surname> <given-names>JJ</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-41">The processing of voice identity in developmental prosopagnosia</article-title>. <source hwp:id="source-42">Cortex</source> <volume>71</volume>:<fpage>390</fpage>-<lpage>397</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1 xref-ref-43-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Massaro DW"><surname>Massaro</surname> <given-names>DW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Egan PB"><surname>Egan</surname> <given-names>PB</given-names></string-name> (<year>1996</year>) <source hwp:id="source-43">Perceiving affect from the voice and the face. Psychonomic Bulletin &amp; Review</source> <volume>3</volume>:<fpage>215</fpage>-<lpage>221</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="other" citation-type="journal" ref:id="197426v1.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Morís Fernández L"><surname>Morís Fernández</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Macaluso E"><surname>Macaluso</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Soto-Faraco S"><surname>Soto-Faraco</surname> <given-names>S</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-42">Audiovisual integration as conflict resolution: The conflict of the McGurk illusion</article-title>. <source hwp:id="source-44">Hum Brain Mapp</source>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Müller VI"><surname>Müller</surname> <given-names>VI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Habel U"><surname>Habel</surname> <given-names>U</given-names></string-name>, <string-name name-style="western" hwp:sortable="Derntl B"><surname>Derntl</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schneider F"><surname>Schneider</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zilles K"><surname>Zilles</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Turetsky BI"><surname>Turetsky</surname> <given-names>BI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eickhoff SB"><surname>Eickhoff</surname> <given-names>SB</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-43">Incongruence effects in crossmodal emotional integration</article-title>. <source hwp:id="source-45">Neuroimage</source> <volume>54</volume>:<fpage>2257</fpage>-<lpage>2266</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Narumoto J"><surname>Narumoto</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Okada T"><surname>Okada</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sadato N"><surname>Sadato</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fukui K"><surname>Fukui</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yonekura Y"><surname>Yonekura</surname> <given-names>Y</given-names></string-name> (<year>2001</year>) <article-title hwp:id="article-title-44">Attention to emotion modulates fMRI activity in human right superior temporal sulcus</article-title>. <source hwp:id="source-46">Cognitive Brain Research</source> <volume>12</volume>:<fpage>225</fpage>-<lpage>231</lpage>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Ochsner KN"><surname>Ochsner</surname> <given-names>KN</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hughes B"><surname>Hughes</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Robertson ER"><surname>Robertson</surname> <given-names>ER</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cooper JC"><surname>Cooper</surname> <given-names>JC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gabrieli JD"><surname>Gabrieli</surname> <given-names>JD</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-45">Neural systems supporting the control of affective and cognitive conflicts</article-title>. <source hwp:id="source-47">Journal of cognitive neuroscience</source> <volume>21</volume>:<fpage>1841</fpage>-<lpage>1854</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Peelen MV"><surname>Peelen</surname> <given-names>MV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Atkinson AP"><surname>Atkinson</surname> <given-names>AP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vuilleumier P"><surname>Vuilleumier</surname> <given-names>P</given-names></string-name> (<year>2010</year>) <article-title hwp:id="article-title-46">Supramodal representations of perceived emotions in the human brain</article-title>. <source hwp:id="source-48">The Journal of neuroscience</source> <volume>30</volume>:<fpage>10127</fpage>-<lpage>10134</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Perrault TJ"><surname>Perrault</surname> <given-names>TJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vaughan JW"><surname>Vaughan</surname> <given-names>JW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stein BE"><surname>Stein</surname> <given-names>BE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wallace MT"><surname>Wallace</surname> <given-names>MT</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-47">Neuron-specific response characteristics predict the magnitude of multisensory integration</article-title>. <source hwp:id="source-49">J Neurophysiol</source> <volume>90</volume>:<fpage>4022</fpage>-<lpage>4026</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Perrault TJ"><surname>Perrault</surname> <given-names>TJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vaughan JW"><surname>Vaughan</surname> <given-names>JW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stein BE"><surname>Stein</surname> <given-names>BE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wallace MT"><surname>Wallace</surname> <given-names>MT</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-48">Superior colliculus neurons use distinct operational modes in the integration of multisensory stimuli</article-title>. <source hwp:id="source-50">J Neurophysiol</source> <volume>93</volume>:<fpage>2575</fpage>-<lpage>2586</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Perrodin C"><surname>Perrodin</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Logothetis NK"><surname>Logothetis</surname> <given-names>NK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Petkov CI"><surname>Petkov</surname> <given-names>CI</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-49">Auditory and visual modulation of temporal lobe neurons in voice sensitive and association cortices</article-title>. <source hwp:id="source-51">Journal of Neuroscience</source> <volume>34</volume>:<fpage>2524</fpage>-<lpage>2537</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Perrodin C"><surname>Perrodin</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Logothetis NK"><surname>Logothetis</surname> <given-names>NK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Petkov CI"><surname>Petkov</surname> <given-names>CI</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-50">Natural asynchronies in audiovisual communication signals regulate neuronal multisensory interactions in voice-sensitive cortex</article-title>. <source hwp:id="source-52">PNAS</source> <volume>112</volume>:<fpage>273</fpage>-<lpage>278</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Pye A"><surname>Pye</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bestelmeyer PE"><surname>Bestelmeyer</surname> <given-names>PE</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-51">Evidence for a supra-modal representation of emotion from cross-modal adaptation</article-title>. <source hwp:id="source-53">Cognition</source> <volume>134</volume>:<fpage>245</fpage>-<lpage>251</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Romanski LM"><surname>Romanski</surname> <given-names>LM</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-52">Representation and integration of auditory and visual stimuli in the primate ventral lateral prefrontal cortex</article-title>. <source hwp:id="source-54">Cerebral Cortex</source> <volume>17</volume>:<fpage>i61</fpage>-<lpage>i69</lpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Rossion B"><surname>Rossion</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hanseeuw B"><surname>Hanseeuw</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dricot L"><surname>Dricot</surname> <given-names>L</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-53">Defining face perception areas in the human brain: A large-scale factorial fMRI face localizer analysis</article-title>. <source hwp:id="source-55">Brain and Cognition</source> <volume>79</volume>:<fpage>138</fpage>-<lpage>157</lpage>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Rotshtein P"><surname>Rotshtein</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henson RNA"><surname>Henson</surname> <given-names>RNA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Treves A"><surname>Treves</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Driver J"><surname>Driver</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-54">Morphing Marilyn into Maggie dissociates physical and identity face representations in the brain</article-title>. <source hwp:id="source-56">Nat Neurosci</source> <volume>8</volume>:<fpage>107</fpage>-<lpage>113</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Sadr J"><surname>Sadr</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sinha P"><surname>Sinha</surname> <given-names>P</given-names></string-name> (<year>2004</year>) <article-title hwp:id="article-title-55">Object recognition and random image structure evolution</article-title>. <source hwp:id="source-57">Cognitive Science</source> <volume>28</volume>:<fpage>259</fpage>-<lpage>287</lpage>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Schultz RT"><surname>Schultz</surname> <given-names>RT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grelotti DJ"><surname>Grelotti</surname> <given-names>DJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Klin A"><surname>Klin</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kleinman J"><surname>Kleinman</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Van der Gaag C"><surname>Van der Gaag</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marois R"><surname>Marois</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Skudlarski P"><surname>Skudlarski</surname> <given-names>P</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-56">The role of the fusiform face area in social cognition: implications for the pathobiology of autism</article-title>. <source hwp:id="source-58">Philos Trans R Soc Lond B Biol Sci</source> <volume>358</volume>:<fpage>415</fpage>-<lpage>427</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Simon D"><surname>Simon</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Craig KD"><surname>Craig</surname> <given-names>KD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gosselin F"><surname>Gosselin</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rainville P"><surname>Rainville</surname> <given-names>P</given-names></string-name> (<year>2008</year>) <article-title hwp:id="article-title-57">Recognition and discrimination of prototypical dynamic expressions of pain and emotions</article-title>. <source hwp:id="source-59">PAIN®</source> <volume>135</volume>:<fpage>55</fpage>-<lpage>64</lpage>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Stanford TR"><surname>Stanford</surname> <given-names>TR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Quessy S"><surname>Quessy</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stein BE"><surname>Stein</surname> <given-names>BE</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-58">Evaluating the operations underlying multisensory integration in the cat superior colliculus</article-title>. <source hwp:id="source-60">The Journal of Neuroscience</source> <volume>25</volume>:<fpage>6499</fpage>-<lpage>6508</lpage>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Stephan KE"><surname>Stephan</surname> <given-names>KE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Penny WD"><surname>Penny</surname> <given-names>WD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daunizeau J"><surname>Daunizeau</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moran RJ"><surname>Moran</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Friston KJ"><surname>Friston</surname> <given-names>KJ</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-59">Bayesian model selection for group studies</article-title>. <source hwp:id="source-61">Neuroimage</source> <volume>46</volume>:<fpage>1004</fpage>-<lpage>1017</lpage>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Van Atteveldt N"><surname>Van Atteveldt</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Formisano E"><surname>Formisano</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goebel R"><surname>Goebel</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blomert L"><surname>Blomert</surname> <given-names>L</given-names></string-name> (<year>2004</year>) <article-title hwp:id="article-title-60">Integration of letters and speech sounds in the human brain</article-title>. <source hwp:id="source-62">Neuron</source> <volume>43</volume>:<fpage>271</fpage>-<lpage>282</lpage>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1 xref-ref-63-2 xref-ref-63-3 xref-ref-63-4"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="von Kriegstein K"><surname>von Kriegstein</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Giraud AL"><surname>Giraud</surname> <given-names>AL</given-names></string-name> (<year>2006</year>) <article-title hwp:id="article-title-61">Implicit multisensory associations influence voice recognition</article-title>. <source hwp:id="source-63">PLoS Biol</source> <volume>4</volume>:<fpage>e326</fpage>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1 xref-ref-64-2 xref-ref-64-3 xref-ref-64-4"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Von Kriegstein K"><surname>Von Kriegstein</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kleinschmidt A"><surname>Kleinschmidt</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sterzer P"><surname>Sterzer</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Giraud A L"><surname>Giraud</surname> <given-names>A L</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-62">Interaction of face and voice areas during speaker recognition</article-title>. <source hwp:id="source-64">Journal of Cognitive Neuroscience</source> <volume>17</volume>:<fpage>367</fpage>-<lpage>376</lpage>.</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1 xref-ref-65-2 xref-ref-65-3"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="von Kriegstein K"><surname>von Kriegstein</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dogan Ö"><surname>Dogan</surname> <given-names>Ö</given-names></string-name>, <string-name name-style="western" hwp:sortable="ter M Grü"><given-names>Grü</given-names> <surname>ter M</surname></string-name>, <string-name name-style="western" hwp:sortable="Giraud A L"><surname>Giraud</surname> <given-names>A L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kell CA"><surname>Kell</surname> <given-names>CA</given-names></string-name>, <string-name name-style="western" hwp:sortable="ter T Grü"><given-names>Grü</given-names> <surname>ter T</surname></string-name>, <string-name name-style="western" hwp:sortable="Kleinschmidt A"><surname>Kleinschmidt</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kiebel SJ"><surname>Kiebel</surname> <given-names>SJ</given-names></string-name> (<year>2008</year>) <article-title hwp:id="article-title-63">Simulation of talking faces in the human brain improves auditory speech recognition</article-title>. <source hwp:id="source-65">PNAS</source> <volume>105</volume>:<fpage>6747</fpage>-<lpage>6752</lpage>.</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1 xref-ref-66-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Vroomen J"><surname>Vroomen</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Driver J"><surname>Driver</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="De Gelder B"><surname>De Gelder</surname> <given-names>B</given-names></string-name> (<year>2001</year>) <article-title hwp:id="article-title-64">Is cross-modal integration of emotional expressions independent of attentional resources?</article-title> <source hwp:id="source-66">Cognitive, Affective, &amp; Behavioral Neuroscience</source> <volume>1</volume>:<fpage>382</fpage>-<lpage>387</lpage>.</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1 xref-ref-67-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Watson R"><surname>Watson</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Latinus M"><surname>Latinus</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Charest I"><surname>Charest</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crabbe F"><surname>Crabbe</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name> (<year>2014a</year>) <article-title hwp:id="article-title-65">People selectivity, audiovisual integration and heteromodality in the superior temporal sulcus</article-title>. <source hwp:id="source-67">Cortex</source> <volume>50</volume>:<fpage>125</fpage>-<lpage>136</lpage>.</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1 xref-ref-68-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Watson R"><surname>Watson</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Latinus M"><surname>Latinus</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Noguchi T"><surname>Noguchi</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garrod O"><surname>Garrod</surname> <given-names>O</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crabbe F"><surname>Crabbe</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belin P"><surname>Belin</surname> <given-names>P</given-names></string-name> (<year>2014b</year>) <article-title hwp:id="article-title-66">Crossmodal adaptation in right posterior superior temporal sulcus during face–voice emotional integration</article-title>. <source hwp:id="source-68">The Journal of Neuroscience</source> <volume>34</volume>:<fpage>6813</fpage>-<lpage>6821</lpage>.</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Weissman D"><surname>Weissman</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Warner L"><surname>Warner</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Woldorff M"><surname>Woldorff</surname> <given-names>M</given-names></string-name> (<year>2004</year>) <article-title hwp:id="article-title-67">The neural mechanisms for minimizing cross-modal distraction</article-title>. <source hwp:id="source-69">The Journal of Neuroscience</source> <volume>24</volume>:<fpage>10941</fpage>-<lpage>10949</lpage>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Willenbockel V"><surname>Willenbockel</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sadr J"><surname>Sadr</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fiset D"><surname>Fiset</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Horne GO"><surname>Horne</surname> <given-names>GO</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gosselin F"><surname>Gosselin</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tanaka JW"><surname>Tanaka</surname> <given-names>JW</given-names></string-name> (<year>2010</year>) <article-title hwp:id="article-title-68">Controlling low-level image properties: the SHINE toolbox</article-title>. <source hwp:id="source-70">Behavior research methods</source> <volume>42</volume>:<fpage>671</fpage>-<lpage>684</lpage>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1 xref-ref-71-2"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Winston JS"><surname>Winston</surname> <given-names>JS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henson Rna"><surname>Henson</surname> <given-names>Rna</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fine-Goulden MR"><surname>Fine-Goulden</surname> <given-names>MR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name> (<year>2004</year>) <article-title hwp:id="article-title-69">fMRI Adaptation Reveals Dissociable Neural Representations of Identity and Expression in Face Perception</article-title>. <source hwp:id="source-71">J Neurophysiol</source> <volume>92</volume>:<fpage>1830</fpage>-<lpage>1839</lpage>.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><citation publication-type="other" citation-type="journal" ref:id="197426v1.72" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Wittfoth M"><surname>Wittfoth</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schröder C"><surname>Schröder</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schardt DM"><surname>Schardt</surname> <given-names>DM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dengler R"><surname>Dengler</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heinze H-J"><surname>Heinze</surname> <given-names>H-J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kotz SA"><surname>Kotz</surname> <given-names>SA</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-70">On emotional conflict: interference resolution of happy and angry prosody reveals valence specific effects</article-title>. <source hwp:id="source-72">Cerebral Cortex:bhp106</source>.</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><citation publication-type="journal" citation-type="journal" ref:id="197426v1.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Worsley KJ"><surname>Worsley</surname> <given-names>KJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cao J"><surname>Cao</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paus T"><surname>Paus</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Petrides M"><surname>Petrides</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Evans AC"><surname>Evans</surname> <given-names>AC</given-names></string-name> (<year>1998</year>) <article-title hwp:id="article-title-71">Applications of random field theory to functional connectivity</article-title>. <source hwp:id="source-73">Hum Brain Mapp</source> <volume>6</volume>:<fpage>364</fpage>-<lpage>367</lpage>.</citation></ref></ref-list></back></article>
