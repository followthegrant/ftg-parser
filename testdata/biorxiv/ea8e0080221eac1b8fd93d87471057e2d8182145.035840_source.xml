<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/035840</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;035840</article-id><article-id pub-id-type="other" hwp:sub-type="slug">035840</article-id><article-id pub-id-type="other" hwp:sub-type="tag">035840</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">The divisible load balance problem with shared cost and its application to phylogenetic inference</article-title></title-group><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Scholl Constantin"><surname>Scholl</surname><given-names>Constantin</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Kobert Kassian"><surname>Kobert</surname><given-names>Kassian</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">‡</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Flouri Tomáš"><surname>Flouri</surname><given-names>Tomáš</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">*</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">‡</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Stamatakis Alexandros"><surname>Stamatakis</surname><given-names>Alexandros</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">*</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">‡</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>*</label><institution hwp:id="institution-1">Karlsruhe Institute of Technology, Institute for Theoretical Informatics</institution>, <country>Germany</country>, Email: <email hwp:id="email-1">constantin@scholl.im</email>, <email hwp:id="email-2">alexandros.stamatakis@kit.edu</email></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3"><label>‡</label><institution hwp:id="institution-2">Heidelberg Institute for Theoretical Studies</institution>, <country>Germany</country>, <institution hwp:id="institution-3">Karlsruhe Institute of Technology, Institute for Theoretical Informatics</institution>, <country>Germany</country>, Email: <email hwp:id="email-3">alexandros.stamatakis@h-its.org</email>, <email hwp:id="email-4">tomas.flouri@h-its.org</email>, <email hwp:id="email-5">kassian.kobert@h-its.org</email></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2016"><year>2016</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2016-01-02T15:57:12-08:00">
    <day>2</day><month>1</month><year>2016</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2016-02-12T08:21:20-08:00">
    <day>12</day><month>2</month><year>2016</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2016-01-02T16:05:30-08:00">
    <day>2</day><month>1</month><year>2016</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2016-02-12T08:40:26-08:00">
    <day>12</day><month>2</month><year>2016</year>
  </pub-date><elocation-id>035840</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2016-01-02"><day>02</day><month>1</month><year>2016</year></date>
<date date-type="accepted" hwp:start="2016-02-12"><day>12</day><month>2</month><year>2016</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2016</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="035840.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/035840v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="035840.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/035840v3/035840v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/035840v3/035840v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Motivated by load balance issues in parallel calculations of the phylogenetic likelihood function, we recently introduced an approximation algorithm for efficiently distributing partitioned alignment data to a given number of CPUs. The goal is to balance the accumulated number of sites per CPU, and, at the same time, to minimize the maximum number of unique partitions per CPU. The approximation algorithm assumes that likelihood calculations on individual alignment sites have identical runtimes and that likelihood calculation times on distinct sites are entirely independent from each other. However, a recently introduced optimization of the phylogenetic likelihood function, the so-called site repeats technique, violates both aforementioned assumptions. To this end, we modify our data distribution algorithm and explore 72 distinct heuristic strategies that take into account the additional restrictions induced by site repeats, to yield a ‘good’ parallel load balance.</p><p hwp:id="p-3">Our best heuristic strategy yields a reduction in required arithmetic operations that ranges between 2% and 92% with an average of 62% for all test datasets using 2, 4, 8, 16, 32, and 64 CPUs compared to the original site-repeat-agnostic data distribution algorithm.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">bin packing</kwd><kwd hwp:id="kwd-2">site repeats</kwd><kwd hwp:id="kwd-3">load balancing</kwd><kwd hwp:id="kwd-4">data distribution</kwd><kwd hwp:id="kwd-5">phylogenetic likelihood</kwd><kwd hwp:id="kwd-6">phylogenomic analysis</kwd></kwd-group><counts><page-count count="15"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><label>I.</label><title hwp:id="title-3">INTRODUCTION</title><p hwp:id="p-4">Maximizing the efficiency of parallel codes, by distributing the data in such a way as to optimize load balance, represents one of the major challenges in high performance computing.</p><p hwp:id="p-5">Here, we address a specific data distribution challenge, which, to the best of our knowledge, has not been addressed before. Our work is motivated by parallel phylo-genetic likelihood computations, that is, reconstruction of evolutionary histories based on molecular sequence data (see [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>] for an overview). In phylogenetics, we are given a <italic toggle="yes">multiple sequence alignment</italic> (MSA) whose columns are nowadays typically subdivided into distinct partitions (e.g., genes or other disjoint subsets of the data). The columns of a MSA represent the sites and the rows represent the taxa (species) of the MSA for which we intend to infer evolutionary histories. All characters in a given column of the MSA are assumed to share a common evolutionary history. Given the MSA and a partitioning scheme, we can calculate the likelihood on a given candidate tree. In partitioned analyses, individual partitions of a MSA, have a separate set of likelihood model parameters, that is, they are assumed to evolve at different rates. This reflects the distinct evolutionary pressures that, for instance, different genes or parts thereof experience in the course of evolution. Partitioned analyses are the standard use-case for current likelihood-based analyses of empirical data with tools such as RAxaML [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>] or MrBayes [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]. In current tools for likelihood-based phylogenetic inference, likelihood calculations on candidate trees typically account for over 85% of total runtime [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]. Thus, the <italic toggle="yes">phylogenetic likelihood function</italic> (PLF) is <italic toggle="yes">the</italic> target function for parallelization. The reasons for load-imbalance in partitioned parallel likelihood calculations are explained in detail in [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>].</p><p hwp:id="p-6">Initially, we formally state the original data distribution problem. Then, we explain why so-called <italic toggle="yes">site repeats</italic> (SR), that is, sub-columns of the MSA that are identical within a given subtree, complicate the matter.</p><p hwp:id="p-7">In the standard case (i.e., without site repeats), we are given a list of <italic toggle="yes">k</italic> partitions and <italic toggle="yes">c</italic> CPUs. Each partition has a computation cost that is linear to the number of sites/columns it comprises. Analogously to the classic bin packing problem, we try to optimally assign the partitions (items) to the CPUs (bins). The first key difference is that the number of CPUs <italic toggle="yes">c</italic> is given, and that we intend to balance the data among all CPUs. In other words, we want to minimize the maximal per-CPU load. The second key difference is that partitions <italic toggle="yes">are</italic> divisible, since a partition consists of (originally independent) sites/subelements. Thus, for improving load balance, we can split partitions into disjoint sets of sites that are allocated to distinct CPUs.</p><p hwp:id="p-8">The computational cost per partition has two components: All partitions have an identical constant base cost α. If we split a partition among two or more CPUs, each subset of that partition incurs this base cost α for the CPU it is assigned to. In phylogenomics, a is the cost for calculating the transition probability matrix <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) for a given time <italic toggle="yes">t</italic>. This probability matrix is an integral part of the statistical model of evolution. Since partitions evolve under distinct models, we need to compute <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) for each partition separately. However, all MSA sites that belong to the same partition have identical model parameters. Thus, α incurs only once per partition and per CPU. Alternatively, we could also parallelize the <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) calculations and then broadcast the <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) values to all CPUs. However, based on previous computational experiments, the <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) calculations are excessively fine-grained and frequent, to allow for efficient parallelization. Therefore, we compute <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) (α) redundantly at several CPUs if the sites of a partition have been split up and allocated to more than one CPU.</p><p hwp:id="p-9">The second component of the per-partition PLF calculation is the variable cost <italic toggle="yes">φ</italic>. For standard likelihood implementations <italic toggle="yes">φ</italic> is linear in the number of sites per partition. This is because the same amount of arithmetic operations is required to compute individual per-site likelihoods. Since alignment sites are assumed to evolve independently in the likelihood model, calculations on a single site of a partition can be performed independently of and concurrently to all other sites. Therefore, we can easily distribute the sites of a single partition to several CPUs.</p><p hwp:id="p-10">Thus, based on the prolegomena, to maximize parallel efficiency, we need to minimize redundant calculations of <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) (α) by only splitting partitions when necessary, while distributing sites evenly among CPUs.</p><p hwp:id="p-11">Our initial work [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>] focused on addressing this data distribution problem for representative PLF implementations in tools such as PLL [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>] and RAxML [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>]. As mentioned before, the <italic toggle="yes">original data distribution algorithm</italic> (ODDA [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref>]) assumes that (i) all sites have the same per-site computation cost and (ii) these per-site computation costs do <italic toggle="yes">not</italic> depend on <italic toggle="yes">how</italic> sites of a single partition are split up among CPUs. For example, if a partition with 100 sites is split up among two CPUs and each CPU shall be allocated 50 sites, any split of those 100 sites into 50 sites per CPU will exhibit the same computational cost.</p><p hwp:id="p-12">Techniques for accelerating PLF calculations such as <italic toggle="yes">Subtree Equality Vectors</italic> (SEV) [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>], [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>] or <italic toggle="yes">Site Repeats</italic> (SR) [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>], [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>] complicate this matter. If we use these techniques in the PLF, the data distribution problem becomes more complex, since both previous assumptions (same cost per site and site independence) regarding per-site computation costs are violated. These techniques take repeating MSA site patterns in subtrees of the phylogeny into account to reduce the amount of per-site PLF computations. A comparison between the PLF functions in the <italic toggle="yes">Phyloge-netic Likelihood Library</italic> [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>] (derived from RAxML [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-3" hwp:rel-id="ref-2">2</xref>]) and a rudimentary implementation that uses SRs shows speedups that range between a factor of 2 up to a factor of 10 [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>]. Note that, SR-based techniques can also yield memory savings by more than 50% depending on the input MSA (see e.g., [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>]). We describe the SR technique more thoroughly in Section III. Essentially, SR-based optimizations reduce the computation cost <italic toggle="yes">φ</italic> by detecting and re-using identical intermediate PLF results <italic toggle="yes">among</italic> sites. Thus, distinct sites now have varying computation costs. Furthermore, if we assign two sites that can share a large fraction of intermediate results to different CPUs, the accumulated computation cost <italic toggle="yes">φ</italic> will increase. Note that, communicating intermediate shared site computation results between CPUs is not a viable solution, since the computation to communication ratio is unfavorable. Analogously to the argument for recomputating α (<italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>)) we can not communicate such intermediate results because of the extremely fine-grained nature of the PLF operations.</p><p hwp:id="p-13">This has implications for distributing data of SR-based PLF implementations. While we can still split partitions arbitrarily among CPUs, we will have to sacrifice some savings that stem from the shared computations. Thus, as for a, we will need to conduct some redundant computations for sites that belong to the same partition and that share some results if these sites are allocated to distinct CPUs. To leverage the substantial computational savings of SR-based PLF implementations, we present an appropriately adapted data distribution algorithm here. Our goal is to split as few partitions among as few CPUs as possible. At the same time we intend to maximize the amount of shared computations between sites for each partition that had to be split. This minimizes the variable per-partition cost <italic toggle="yes">φ</italic> and the accumulated number of base costs a for calculating <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>). Finally, by reducing the overall accumulated computation cost of the data distribution, we can minimize parallel runtimes.</p><p hwp:id="p-14">Given that the problem ODDA aims to solve was shown to be NP-hard, we suspect that the current problem, that has one additional optimality condition, is NP-hard as well. However, a more thorough study of the theoretical properties of our problem is outside the scope of this paper. For this reason, we first assess whether taking into account among-site dependencies for the data distribution algorithm affects parallel efficiency, or not. As we show in Section VI-E, disregarding among-site dependencies <italic toggle="yes">can</italic> decrease parallel efficiency by up to one order of magnitude. In Section V we therefore present and assess several ad hoc heuristics for improving load balance in parallel SR-based PLF computations.</p><p hwp:id="p-15">Note that, parallel PLF implementations now form part of several widely-used tools (e.g., ExaML, RAxML, MrBayes, ExaBayes, PhyML) and the results presented here are generally applicable to all of these tools. At present, only ExaML and RAxML offer basic SR-based PLF implementations that merely exploit a fraction of the SRs that are present for accelerating calculations (see [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>]). However, we expect that most PLF-based tools will adopt the SR technique [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref>], because of the substantial runtime <italic toggle="yes">and</italic> memory savings that can be achieved.</p><p hwp:id="p-16">The remainder of this paper is organized as follows. In Section II, we survey related work on similar scheduling problems from the area of bin packing with items that can share space. In the subsequent Section III we describe the aspects of the SR technique that are relevant for the work we present here at an abstract level. In Section IV we discuss the properties of the cost factors <italic toggle="yes">α</italic> and <italic toggle="yes">φ</italic>. We also define a theoretical <italic toggle="yes">lower bound</italic> that we use as baseline to design and assess our heuristics. Our main contribution is presented in Sections V and VI. Section V introduces polynomial-time heuristics which yield data distributions that are only 5.75% worse than the lower bound. Finally, in Section VI, we present our experimental setup and the performance gains achieved by our heuristics compared to the original data distribution algorithm (ODDA) on both, simulated, and empirical MSAs.</p></sec><sec id="s2" hwp:id="sec-2"><label>II.</label><title hwp:id="title-4">RELATED WORK</title><p hwp:id="p-17">Since we are not aware of any related work in phyloge-nomics, we briefly review related work on similar scheduling problems.</p><p hwp:id="p-18">The so-called <italic toggle="yes">many-constrained bi-objective bin packing</italic> task is one such related problem [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>]. Here, items need to be distributed into bins <italic toggle="yes">and</italic> the items are subject to various constraints. The constraints reduce to a cost function between items: The cost of an item can depend on another in a positive, neutral, or negative way if they are in the same bin. In the beginning all items are considered to be located in separate bins. The objective is to minimize the accumulated cost <italic toggle="yes">and</italic> number of bins, by grouping items together into bins.</p><p hwp:id="p-19">The Virtual machine (VM) packing problem is also related [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>], [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>]. The goal is to pack a given amount of VMs (items) into a minimal amount of servers (bins). Each VM has a certain set of memory pages. These memory pages can overlap with those of other VMs, therefore, increasing the number of VMs that fit onto a server.</p><p hwp:id="p-20">In contrast to our problem, the items in both scheduling problems are not divisible and the number of bins is not given as input. In fact, the objective function for these related problems is to minimize the number of bins.</p></sec><sec id="s3" hwp:id="sec-3"><label>III.</label><title hwp:id="title-5">SITE REPEATS</title><p hwp:id="p-21">Repeated PLF evaluations can account for over 85% of total runtime in maximum likelihood (ML) based tree searches and Bayesian phylogenetic inferences (BI) [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">4</xref>]. One simple observation is that two identical sites that evolve under the same evolutionary model (forming part of the same MSA partition) always yield the same likelihood score. Thus, a common method to avoid these globally redundant calculations is to compress identical MSA sites in a pre-processing step. However, such identical calculations also appear locally at the subtree level. That is, MSA columns might be partially identical for a subset of taxa (species/rows) defined by a subtree (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref> for an example). While there have been previous attempts to exploit this property [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>], Kobert <italic toggle="yes">et al.</italic> recently introduced an efficient algorithm [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-3" hwp:rel-id="ref-12">12</xref>] for detecting <italic toggle="yes">site repeats</italic> (SR). The PLF optimization relies on recognizing repeating DNA patterns at different column indices, <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>, of the MSA (on the same partition), defined by a subtree of the phylogeny whose PLF is being computed. As a consequence, the amount of SR-based savings depends on the actual tree topology.</p><p hwp:id="p-22">At an abstract level, PLF calculations conduct post-order tree traversals to update the so-called <italic toggle="yes">conditional likelihood vectors</italic> (CLVs) that have as many entries as the input MSA has columns. Given a fast method to identify repeating subtree site patterns, we can omit all PLF calculations of column indices <italic toggle="yes">p</italic> &gt; <italic toggle="yes">q</italic> for identical subtree site patterns, if <italic toggle="yes">q</italic> is the first occurrence of the specific pattern.</p><p hwp:id="p-23">In <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref> we only calculate the CLVs at node <italic toggle="yes">v</italic> for column index <italic toggle="yes">p</italic> = 1 and omit the PLF calculations for column indices <italic toggle="yes">q</italic> = 2 and <italic toggle="yes">q</italic> = 5. Without further optimization we can omit a total of 5 out of 15 CLV calculations in this example. An important metric for distributing the data is the number of SRs a specific column contributes to. Henceforth, we will call this metric the <italic toggle="yes">site repeat count</italic> (SRC). In our example site 1 has a SRC of 1, while site 2 has a SRC of 3.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-24">Sites 1, 2, and 5 form a SR at node <italic toggle="yes">v</italic> as they share the same subtree site pattern <monospace>GA</monospace>. Another repeating pattern is present at sites 3 and 4 (<monospace>CG</monospace>) for the same node. Note that, node <italic toggle="yes">w</italic> also induces a subtree with pattern <monospace>CG</monospace> at the tips at site 1. However, since branch lengths can be and typically are different from those of the subtree induced by node <italic toggle="yes">v</italic>, the conditional likelihoods may differ as well. But, there is another SR at sites 2 and 5 for node <italic toggle="yes">w</italic> (<monospace>TC</monospace>), and hence the conditional likelihood is the same for those two sites. Finally, sites 2 and 5 are a SR for node <italic toggle="yes">u</italic> (<monospace>GATC</monospace>).</p></caption><graphic xlink:href="035840_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-25">Kobert <italic toggle="yes">et al.</italic> show that a—at a technical level— not fully optimized SR-based PLF implementation consistently outperforms one of the most efficient available PLF implementations (including AVX intrinsics) for distinct real-world application scenarios and tree topologies. Respective speedups range between a factor of 2 up to 10 [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-4" hwp:rel-id="ref-12">12</xref>]. Because the work on SRs is very recent, we are not aware of any phylogenomic data distribution algorithm that takes into account the additional constraints induced by SR-based PLF implementations.</p></sec><sec id="s4" hwp:id="sec-4"><label>IV.</label><title hwp:id="title-6">PRELIMINARIES</title><p hwp:id="p-26">Assume a MSA of <italic toggle="yes">m</italic> sequences (taxa) and <italic toggle="yes">n</italic> sites <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="035840_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>, and a partition scheme of <italic toggle="yes">k</italic> disjoint partitions <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="035840_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> such that <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="035840_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula>. Our task is to distribute these partitions among <italic toggle="yes">c</italic> CPUs, such that the maximum computational cost (<italic toggle="yes">load</italic>) among CPUs is minimized. Note that, we are allowed to assign distinct sites of a partition to different CPUs. Let <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="035840_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> be the mapping of sites to CPUs and partitions. Further, let
<disp-formula id="ueqn1" hwp:id="disp-formula-1"><alternatives hwp:id="alternatives-5"><graphic xlink:href="035840_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives></disp-formula>
be the set of partitions assigned to CPU <italic toggle="yes">j</italic>. The total computational cost at a CPU <italic toggle="yes">j</italic> is then
<disp-formula id="ueqn2" hwp:id="disp-formula-2"><alternatives hwp:id="alternatives-6"><graphic xlink:href="035840_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives></disp-formula>
where the mapping <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="035840_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> is the SR-based PLF-C (computational cost) for a specific subset of sites from a partition. Note that, for simplicity, in the rest of the text we will treat (site) partitions as lists instead of sets. Hence, we introduce two operations \ and ∪ for adding and removing sites from partition lists. Operation <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="035840_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> removes sites with indices <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="035840_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> from a partition list <italic toggle="yes">p</italic>, and <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="035840_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> appends sites <italic toggle="yes">s</italic><sub>1</sub>, <italic toggle="yes">s</italic><sub>2</sub>,…, <italic toggle="yes">s<sub>m</sub></italic> at the end of list <italic toggle="yes">p</italic>. A site can be removed in <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="035840_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> time, and adding <italic toggle="yes">m</italic> sites requires <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="035840_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula> time.</p><p hwp:id="p-27">The mapping λ can be implemented as a lookup table of <italic toggle="yes">c</italic> CPUs. Each element of this lookup table is a again a lookup table for each of the <italic toggle="yes">k</italic> partitions. Note that, all algorithms we present assume that the number of sites <italic toggle="yes">n</italic> is larger than the number of CPUs <italic toggle="yes">c</italic>.</p><sec id="s4a" hwp:id="sec-5"><label>A.</label><title hwp:id="title-7">Computational costs α and φ</title><p hwp:id="p-28">The first simple observation is, that the computational cost for each CPU depends on the cost components α and <italic toggle="yes">φ</italic>. However, both quantities can only be measured in abstract terms. The exact runtimes depend on the hardware architecture, data type, and tree topology. Therefore, the objective is to minimize both factors, without being able to directly compare them. We will show that the count of additional α values, which we call <italic toggle="yes">extra</italic><sub>α</sub>, created by splitting partitions, does not vary substantially for the heuristics we tested in Section VI-E. Thus, we focus on minimizing the cost <italic toggle="yes">φ</italic>, which corresponds to the number of arithmetic operations required to evaluate the PLF on a given set of sites for a given tree topology. We call this cost the <italic toggle="yes">PLF count of operations</italic> (PLF-C).</p></sec><sec id="s4b" hwp:id="sec-6"><label>B.</label><title hwp:id="title-8">Lower bound <italic toggle="yes">L</italic></title><p hwp:id="p-29">The second observation is, that we can calculate a simple theoretical lower bound for the optimal accumulated per-CPU PLF-C. We use this bound, to steer our heuristics <italic toggle="yes">and</italic> to assess the performance of our heuristics.</p><p hwp:id="p-30">Assume that all partitions are assigned to a single CPU. We can then calculate the PLF-C for this CPU. Simply dividing this single CPU PLF-C by the number of CPUs <italic toggle="yes">c</italic> yields a natural lower bound <italic toggle="yes">L</italic>. Since no partitions have been split up on the single CPU assignment, the respective SR-induced savings are maximal. Thus, for any assignment of the partitions to two or more CPUs, which may not be able to retain all site repeats, the per-CPU PLF-C will be ≥ <italic toggle="yes">L</italic>. Consider the example MSA in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1</xref> with only two sites and assume that we want to assign the data to two CPUs. The lower bound is <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="035840_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> PLF-C. However, the optimum data distribution will assign one site to each CPU. Thus, the optimized PLF can not skip the SR <monospace>GA</monospace> anymore, resulting in a per-CPU PLF-C of 3 &gt; 2.5 for both CPUs.</p></sec></sec><sec id="s5" hwp:id="sec-7"><label>V.</label><title hwp:id="title-9">DESCRIPTION OF HEURISTIC COMPONENTS</title><p hwp:id="p-31">The heuristics we present here consist of three phases: Data pre-processing (phase I), initial distribution of sites to CPUs (phase II), and site reshuffling strategies (phase III). The data pre-processing phase prepares the MSA data for the subsequent initial distribution phase. We present two algorithms for computing such an initial partition data to CPU distribution. Finally, we employ three different site reshuffling strategies to further decrease the PLF-C.</p><p hwp:id="p-32">The individual components designed for these three phases can be flexibly combined in a plethora of ways and orders to rapidly assemble and test heuristic data distribution algorithms. We assessed a total of 72 heuristic strategies using the 7 core components (2 for pre-processing, 2 for the initial assignment, and 3 for the reshuffling phase) for implementing the three distinct distribution phases. For a complete list of all 72 strategies, see the on-line supplement<sup><xref ref-type="fn" rid="fn1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">1</xref></sup> on <monospace>github</monospace>. Here, we only present a performance assessment of 32—including the two best-performing ones— out of 72 heuristics. Presenting all 72 heuristics would merely require describing additional variants of our components.</p><p hwp:id="p-33">In addition, we describe the two heuristics that performed best, in more detail. All 32 heuristics we tested are enumerated on the <italic toggle="yes">x</italic>-axis of <xref ref-type="fig" rid="fig12" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 12</xref>.</p><fig id="fig12" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG12</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig12</object-id><label>Figure 12.</label><caption hwp:id="caption-2"><p hwp:id="p-34">Comparison of all 32 heuristic component combinations, where, cut = cut approach; grdy = greedy approach; lxi = lexicographic sorting of MSA; adj-lmt = adjust limit reshuffling strategy; low-src = low SRC reshuffling strategy; red-max = reduce max reshuffling strategy. The upper part presents the average PLF-C to <italic toggle="yes">L</italic> percentage of the heuristics over all MSAs and numbers of CPUs. The lower part presents the average <italic toggle="yes">extra<sub>α</sub></italic> for the heuristics over all MSAs and numbers of CPUs.</p></caption><graphic xlink:href="035840_fig12" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><sec id="s5a" hwp:id="sec-8"><label>A.</label><title hwp:id="title-10">Phase I heuristics: Data pre-processing</title><p hwp:id="p-35">A commonly used method to avoid redundant calculations in the PLF is to remove all duplicate sites from the MSA and only keep the unique sites. Since identical sites yield the same per-site likelihood if they evolve under the same model, the overall likelihood can be computed by assigning respective weights to unique sites.</p><p hwp:id="p-36">As a second pre-processing step we sort the remaining, unique sites lexicographically. Through an empirical analysis we have observed that lexicographic sorting has the effect that sites sharing repeats are more likely to reside next to each other than for a random site order. <xref ref-type="fig" rid="fig10" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 10</xref> depicts the sorted versus unsorted PLF-C between all pairs of consecutive sites, accumulated over all sites, all 1200 inferred trees, and all 8 MSAs we used (see Section VI-B). Sorting leads to better results for the overall data distribution algorithm (see Section VI-E). The lexicographic sort requires <inline-formula hwp:id="inline-formula-12"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="035840_inline12.gif" hwp:id="inline-graphic-12"/></alternatives></inline-formula> time and <inline-formula hwp:id="inline-formula-13"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="035840_inline13.gif" hwp:id="inline-graphic-13"/></alternatives></inline-formula> space.</p><fig id="fig10" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig10</object-id><label>Figure 10.</label><caption hwp:id="caption-3"><p hwp:id="p-37">Lexicographically sorted versus unsorted PLF-C between all pairs of consecutive sites, accumulated over all sites, all 1200 trees, and all 8 MSAs.</p></caption><graphic xlink:href="035840_fig10" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s5b" hwp:id="sec-9"><label>B.</label><title hwp:id="title-11">Phase II heuristics: Initial distribution</title><p hwp:id="p-38">The initial distribution of partitions to CPUs is a two-step process. First, we <italic toggle="yes">pre-fill</italic> the CPUs with entire partitions, that is, without splitting any partition until a filling limit is reached for at least one CPU. Then, we distribute and potentially split up the remaining partitions via the <italic toggle="yes">greedy</italic> or the <italic toggle="yes">cut</italic> approach (see below). Both approaches try to balance the PLF-C among CPUs.</p><p hwp:id="p-39">In the pre-filling step, we intend to assign as many entire (without splitting them up) partitions as possible to the CPUs, for minimizing the per-CPU a cost. Initially, the <italic toggle="yes">lower bound L</italic> (see Section IV-B) is used as a computing capacity limit for each CPU. The reshuffling strategies (phase III, see Section V-C) might adjust this limit later-on. The procedure for assigning entire partitions to CPUs is analogous to phases 1 and 2 of the ODDA [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-3" hwp:rel-id="ref-6">6</xref>]: We first sort partitions by increasing order of computational cost (PLF-C). Starting with the partition that has the smallest PLF-C, we assign partitions to CPUs in a cyclic manner until adding a partition exceeds the capacity limit (<italic toggle="yes">L</italic>) of a CPU. When the procedure stops, the still unassigned (remaining) partitions have indices <inline-formula hwp:id="inline-formula-14"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="035840_inline14.gif" hwp:id="inline-graphic-14"/></alternatives></inline-formula>. In a final step we sort the CPUs by their accumulated PLF-C in ascending order. <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Figures 9a</xref> and <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-2" hwp:rel-id="F12">9b</xref> illustrate the pre-filling process; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 2</xref> contains the PREFILL algorithm. The asymptotic runtime complexity for pre-filling is <inline-formula hwp:id="inline-formula-15"><alternatives hwp:id="alternatives-17"><inline-graphic xlink:href="035840_inline15.gif" hwp:id="inline-graphic-15"/></alternatives></inline-formula>, as sorting the <italic toggle="yes">k</italic> partitions requires time <inline-formula hwp:id="inline-formula-16"><alternatives hwp:id="alternatives-18"><inline-graphic xlink:href="035840_inline16.gif" hwp:id="inline-graphic-16"/></alternatives></inline-formula>, and calculating the PLF-C for all partitions requires <inline-formula hwp:id="inline-formula-17"><alternatives hwp:id="alternatives-19"><inline-graphic xlink:href="035840_inline17.gif" hwp:id="inline-graphic-17"/></alternatives></inline-formula> time.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-4"><p hwp:id="p-40">The PREFILL function requires the following input parameters: a list <italic toggle="yes">P</italic> of <italic toggle="yes">k</italic> partitions sorted in ascending order by their PLF-C, the number of CPUs <italic toggle="yes">c</italic>, and the lower bound <italic toggle="yes">L</italic>.</p></caption><graphic xlink:href="035840_fig2" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-41">1) <italic toggle="yes">Greedy approach:</italic> The greedy approach is one of two alternatives for distributing the remaining partitions among the pre-filled CPUs. It tries to find the best fit CPU for each individual site. The site fit to a CPU is defined as the PLF-C increase that is induced by assigning the site to the CPU. The lower this PLF-C increase is for a specific CPU, the better the site fits. The underlying idea is, that by increasing the fit of sites from remaining partitions that need to be split up we can reduce the overall PLF-C for the data distribution.</p><p hwp:id="p-42">However, we need to first assign at least some sites of the remaining partitions to CPUs to be able to compute a meaningful site fit. We achieve this via a virtual assignment of remaining partitions to CPUs (please see <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Figures 9c</xref>-<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-4" hwp:rel-id="F12">9f</xref> before reading on, since the subsequent description will be easier to follow). Initially, we calculate the average per-site PLF-C cost μ for each remaining partition. Then, we use μ as a proxy for the PLF-C to generate a virtual assignment of the remaining partitions to the CPUs. This virtual assignment ignores SR effects and simply distributes all sites from all remaining partitions to the CPUs based on μ. Thereby, we can approximately balance the per-CPU PLF-C. Then, for each CPU that has at least one site from one of the remaining partitions, we select the site in the middle of the subset of sites of the remaining partition (the median site) that has been assigned to this CPU as representative site. This representative site from the virtual assignment is then assigned to the respective CPU and becomes a real site assignment. As a result, at most two sites from at most two of the remaining partitions are assigned to each CPU. Sites of at most two distinct remaining partitions are assigned to each CPU, since we virtually assign the remaining partitions one-by-one in as large as possible monolithic blocks to CPUs. Keep in mind that each of the remaining partitions exceeds the capacity limit of every CPU because of the way we designed the prefilling component.</p><p hwp:id="p-43"><xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-5" hwp:rel-id="F12">Figures 9c</xref>-<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-6" hwp:rel-id="F12">9e</xref> illustrate and <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 3</xref> presents the pseudocode for the virtual assignment step of GREEDY. In the pseudocode <italic toggle="yes">z</italic> is the site index pointing to the first site of the current partition, while <italic toggle="yes">z</italic>’ is the length of the current partition. The variable s in line 9 is the median site. Since we remove the median site s from <italic toggle="yes">p<sub>i</sub></italic> we also need to decrement <italic toggle="yes">z</italic> for the next partition (line 13). If the algorithm virtually assigns two partitions to a CPU, <italic toggle="yes">vcost</italic> represents the computational cost that is required for the first partition that was assigned to this CPU and defines how much computing capacity is left for the second partition on this CPU.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-5"><p hwp:id="p-44">The GREEDY function requires the following input parameters: The list <italic toggle="yes">P</italic> of remaining partitions sorted in ascending order by their PLF-C, where index <italic toggle="yes">r</italic> denotes the first and index <italic toggle="yes">k</italic> the last unassigned partition; the distribution function λ and the <italic toggle="yes">cost</italic> for the <italic toggle="yes">c</italic> CPUs; the lower bound <italic toggle="yes">L</italic>. Here we show the virtual assignment step (step 1) of the algorithm.</p></caption><graphic xlink:href="035840_fig3" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-45">Once we have computed this initial assignment of median sites from the remaining partitions to CPUs, the second step of GREEDY is straight-forward. We simply iterate over the yet unassigned sites from the remaining partitions and assign each site to the CPU with the best PLF-C fit. If adding a remaining site to this CPU leads to exceeding the capacity limit <italic toggle="yes">L</italic>, we assign the site to the CPU with the second-best PLF-C fit etc., provided that adding the site will not violate its capacity limit. As a consequence we generally assign the site to a CPU that already has some sites of the same partition in order to minimize the a cost. Only, if no CPU below the capacity limit has sites of the same partition, but there are other CPUs below the capacity limit, we will assign the site arbitrarily to one of the latter. Finally, if <italic toggle="yes">all</italic> CPUs exceed <italic toggle="yes">L</italic>, we assign the site to the CPU with the best fit. <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 4</xref> presents the pseudocode for the second step of GREEDY. Note that, the second step of Greedy will only terminate once all sites of all remaining partitions have been assigned to a CPU.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-6"><p hwp:id="p-46">Step 2 of the GREEDY function.</p></caption><graphic xlink:href="035840_fig4" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-47">The total asymptotic runtime of step 1 is <inline-formula hwp:id="inline-formula-18"><alternatives hwp:id="alternatives-20"><inline-graphic xlink:href="035840_inline18.gif" hwp:id="inline-graphic-18"/></alternatives></inline-formula>. The runtime is dominated by calculating μ in line 5. For all sites in the <italic toggle="yes">r – k</italic> remaining partitions we have to compute φ, which requires <inline-formula hwp:id="inline-formula-19"><alternatives hwp:id="alternatives-21"><inline-graphic xlink:href="035840_inline19.gif" hwp:id="inline-graphic-19"/></alternatives></inline-formula> time per site.</p><p hwp:id="p-48">The runtime of step 2 is <inline-formula hwp:id="inline-formula-20"><alternatives hwp:id="alternatives-22"><inline-graphic xlink:href="035840_inline20.gif" hwp:id="inline-graphic-20"/></alternatives></inline-formula>. For each site of the <italic toggle="yes">k – r</italic> remaining partitions we need to calculate the <italic toggle="yes">scost</italic> for each CPU. The <italic toggle="yes">scost</italic> in line 25 can be computed in <inline-formula hwp:id="inline-formula-21"><alternatives hwp:id="alternatives-23"><inline-graphic xlink:href="035840_inline21.gif" hwp:id="inline-graphic-21"/></alternatives></inline-formula> by keeping the site repeats data structure in memory and using the algorithms from [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref>], [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>]. We use this observation in all subsequent runtime analyses.</p><p hwp:id="p-49"><italic toggle="yes">2) Cut approach:</italic> As an alternative to GREEDY, we implemented the CUT approach for distributing the remaining partitions to pre-filled CPUs. Here, we cut contiguous regions from the remaining partitions and assign them to CPUs. This has the benefit of incurring <italic toggle="yes">exactly</italic> the same accumulated a cost as the ODDA, which has been proven to exceed the optimal solution by at most one a per CPU [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-4" hwp:rel-id="ref-6">6</xref>]. We illustrate the cut approach in <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-7" hwp:rel-id="F12">Figures 9g</xref> and <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-12-8" hwp:rel-id="F12">9h</xref>.</p><p hwp:id="p-50">After the pre-filling phase, the remaining partitions and CPUs are still sorted by increasing PLF-C. Starting with these sorted lists, we iterate over the CPUs from the least filled to the most filled one (i.e., from largest to smallest available capacity). For each CPU we select a region from at most two of the remaining partitions. The size of the region is computed as follows for every CPU to be filled: we divide the free capacity of the particular CPU by the accumulated free capacity of all CPUs that have not reached their capacity limit yet and multiply this by the total PLF-C of all remaining and yet unassigned partitions. The underlying idea is to distribute the additional arithmetic operations we will need to conduct due to partitions that have already been split up as evenly as possible over all CPUs. To then actually assign such a cut region to the CPU, the region must exhaust or exceed the capacity limit of the current CPU. If this is not the case, we increase the size of the region until it exhausts the capacity limit of the current CPU. This restriction that CPU capacity limits must at least be exhausted is required for the reshuffling strategies (phase III, see V-C).</p><p hwp:id="p-51">The pseudocode for CUT is provided in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 5</xref>. While δ returns the free capacity for a given CPU <italic toggle="yes">j</italic>, the array Δ contains the accumulated free capacity of all CPUs with an index ≥ <italic toggle="yes">j</italic> for a given <italic toggle="yes">j</italic>. We use <italic toggle="yes">z</italic> as site index again and <italic toggle="yes">rcost</italic> is the total PLF-C of all remaining partitions. The value <italic toggle="yes">l</italic> denotes the target PLF-C capacity of the current CPU <italic toggle="yes">j</italic> which defines the size of the region that shall be assigned to CPU <italic toggle="yes">j</italic>. Note that, CUT terminates when all sites from all remaining partitions have been assigned to CPUs.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-7"><p hwp:id="p-52">The CUT function requires the following input parameters: The list <italic toggle="yes">P</italic> of remaining partitions sorted in ascending order by their PLF-C, where index <italic toggle="yes">r</italic> denotes the first and index <italic toggle="yes">k</italic> the last unassigned partition; the distribution function λ and the <italic toggle="yes">cost</italic> for the <italic toggle="yes">c</italic> number of CPUs; the lower bound <italic toggle="yes">L</italic>.</p></caption><graphic xlink:href="035840_fig5" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-53">The total asymptotic runtime of CUT is <inline-formula hwp:id="inline-formula-22"><alternatives hwp:id="alternatives-24"><inline-graphic xlink:href="035840_inline22.gif" hwp:id="inline-graphic-22"/></alternatives></inline-formula>. For line 9 we can initialize two arrays: One, computing <italic toggle="yes">φ</italic> in the order of sites in the remaining partitions and the other computing <italic toggle="yes">φ</italic> in the reverse order of sites. Thus, line 9 requires constant time and the initialization can be computed in <inline-formula hwp:id="inline-formula-23"><alternatives hwp:id="alternatives-25"><inline-graphic xlink:href="035840_inline23.gif" hwp:id="inline-graphic-23"/></alternatives></inline-formula>. By definition we will split up at most 2(<italic toggle="yes">k – r</italic>) partitions. These partitions are disjoint. Therefore, line 12 will be executed once for each site of the partitions that will be split and thus requires <inline-formula hwp:id="inline-formula-24"><alternatives hwp:id="alternatives-26"><inline-graphic xlink:href="035840_inline24.gif" hwp:id="inline-graphic-24"/></alternatives></inline-formula> time.</p></sec><sec id="s5c" hwp:id="sec-10"><label>C.</label><title hwp:id="title-12">Phase III heuristics: Reshuffling strategies</title><p hwp:id="p-54">After the initial distribution (e.g., computed with GREEDY or CUT), we can employ up to three site reshuffling strategies to further improve the PLF-C: Adjust limit, reduce max, and low SRC.</p><p hwp:id="p-55">1) <italic toggle="yes">Adjust limit:</italic> The initial distribution provides a good estimate of how many partitions need to be split up and how the overall PLF-C for the distribution increases due to these splits. We use this knowledge to adjust the capacity limit of the CPUs: Instead of the lower bound <italic toggle="yes">L</italic> we now use the average CPU load from the initial distribution as capacity limit. We then simply re-run the pre-filling and the GREEDY or CUT approach with this updated capacity limit.</p><p hwp:id="p-56">2) <italic toggle="yes">Reduce max:</italic> The bottleneck in parallel PLF computations is always the CPU with the highest PLF-C since it determines the overall runtime. Therefore, we attempt to reduce the load of the CPU with the highest PLF-C.</p><p hwp:id="p-57">This reshuffling strategy uses the worst-case PLF-C for one site. This is a MSA-specific value that depends on the number of taxa <italic toggle="yes">m</italic>. The <italic toggle="yes">worst-case site PLF-C</italic> simply has a SRC of zero.</p><p hwp:id="p-58">First, we select all partitions that have been split up and have been assigned to the most loaded CPU. Then, we determine all candidate CPUs that also have a part of those partitions. Out of those candidate CPUs we then select only those with a PLF-C that is smaller than the mean per-CPU PLF-C. Using the worst-case site PLF-C, we estimate how many sites can be moved from the most loaded CPU to the least loaded CPUs, such that they do not exceed the current capacity limit. We then move the sites with the lowest SRC away from the most loaded CPU. Thereby, we only lose a small number of SRs for the sites that will remain on the most loaded CPU. This <italic toggle="yes">reduce max</italic> operation can be repeated for an arbitrary number of times. We choose to repeat this process as many times as there are CPUs to achieve an acceptable trade-off between overall PLF-C improvement and the run-time for this heuristic component. Evidently, one could also design an explicit convergence criterion to determine when additional applications of reduce max are not worthwhile any more.</p><p hwp:id="p-59">We present the pseudocode for reduce max in <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 6</xref>. Line 1 assumes that <italic toggle="yes">P</italic> is the original sorted list of all partitions from the pre-filling phase (Section V-B). The constant <italic toggle="yes">wcost</italic> is the <italic toggle="yes">worst case PLF-C for a site. P’</italic> is the list of split up partitions assigned to one of the most loaded CPU <italic toggle="yes">j</italic>.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-8"><p hwp:id="p-60">The REDMAX function requires the following input parameters: a list <italic toggle="yes">P</italic> of <italic toggle="yes">k</italic> partitions, the distribution function λ, and the <italic toggle="yes">cost</italic> for <italic toggle="yes">c</italic> CPUs.</p></caption><graphic xlink:href="035840_fig6" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-61">The total asymptotic runtime for <italic toggle="yes">reduce max</italic> is <inline-formula hwp:id="inline-formula-25"><alternatives hwp:id="alternatives-27"><inline-graphic xlink:href="035840_inline25.gif" hwp:id="inline-graphic-25"/></alternatives></inline-formula>. The first part is defined by the calculation of <italic toggle="yes">L</italic>. The second part is defined by sorting the sites in <italic toggle="yes">p</italic>’ by their SRC.</p><p hwp:id="p-62">3) <italic toggle="yes">Low SRC:</italic> One goal of our distribution strategies is to maximize the site repeats in order to decrease the overall PLF-C. The <italic toggle="yes">low SRC</italic> strategy intends to reshuffle sites that exhibit a low SRC on their current CPU. For reshuffling, we select 20% (see below for a rationale for this setting) of the sites from each split up partition that have the lowest SRC under the current data distribution. We then provide this selection of sites as input to step 2 of GREEDY (see Section V-B1) to reassign them to the CPUs.</p><p hwp:id="p-63">The actual percentage of sites selected for reshuffling is a tuning parameter. It should not be too high, since we want to determine how well the sites fit into a comparatively large, fixed site distribution. It should not be too low either, since this will decrease the overall PLF-C improvement of the reshuffling. We empirically determined that a threshold setting of 20% yields good performance.</p><p hwp:id="p-64"><xref ref-type="fig" rid="fig7" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure 7</xref> presents the pseudocode. Line 1 assumes that <italic toggle="yes">P</italic> is the original sorted list of all partitions as obtained in the pre-filling step (Section V-B). <italic toggle="yes">P</italic>’ is the list of split partitions of CPU <italic toggle="yes">j</italic>.</p><p hwp:id="p-65">The total asymptotic runtime of <italic toggle="yes">low SRC</italic> is <inline-formula hwp:id="inline-formula-26"><alternatives hwp:id="alternatives-28"><inline-graphic xlink:href="035840_inline26.gif" hwp:id="inline-graphic-26"/></alternatives></inline-formula>. For each CPU we iterate over all split partitions, which are at most <italic toggle="yes">k</italic> and sort them in <inline-formula hwp:id="inline-formula-27"><alternatives hwp:id="alternatives-29"><inline-graphic xlink:href="035840_inline27.gif" hwp:id="inline-graphic-27"/></alternatives></inline-formula>. Line 9 will reassign at most <italic toggle="yes">n</italic> sites, where each site requires <inline-formula hwp:id="inline-formula-28"><alternatives hwp:id="alternatives-30"><inline-graphic xlink:href="035840_inline28.gif" hwp:id="inline-graphic-28"/></alternatives></inline-formula>. As before Greedy phase 2 requires <inline-formula hwp:id="inline-formula-29"><alternatives hwp:id="alternatives-31"><inline-graphic xlink:href="035840_inline29.gif" hwp:id="inline-graphic-29"/></alternatives></inline-formula> time.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-9"><p hwp:id="p-66">LOWSRC takes the following input parameters: a list <italic toggle="yes">P</italic> of <italic toggle="yes">k</italic> partitions, the distribution function λ, the <italic toggle="yes">cost</italic>, and the number of CPUs <italic toggle="yes">c</italic>.</p></caption><graphic xlink:href="035840_fig7" position="float" orientation="portrait" hwp:id="graphic-11"/></fig></sec></sec><sec id="s6" hwp:id="sec-11"><label>VI.</label><title hwp:id="title-13">EXPERIMENTAL SETUP AND RESULTS</title><sec id="s6a" hwp:id="sec-12"><title hwp:id="title-14">A. Software</title><p hwp:id="p-67">Given a MSA and a partition scheme, we employed RAxML [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-4" hwp:rel-id="ref-2">2</xref>] to infer and sample phylogenetic trees (for details see Section VI-C). Using Ruby and the Ruby Gem Newick-Ruby<sup><xref ref-type="fn" rid="fn2" hwp:id="xref-fn-2-1" hwp:rel-id="fn-2">2</xref></sup> for parsing trees, we initially characterized the problem and then implemented the heuristics. To visualize and post-process the results we used appropriate <monospace>R</monospace> packages.</p></sec><sec id="s6b" hwp:id="sec-13"><label>B.</label><title hwp:id="title-15">Test datasets</title><p hwp:id="p-68">We used five simulated and three empirical nucleotide MSAs for benchmarking the heuristics. To simplify the experiments we removed all identical sites from the MSAs prior to running the heuristics such that all MSAs only contained unique sites. The MSA are available for download at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.cormconscho/phylo_scheduling/ttee/master/" ext-link-type="uri" xlink:href="https://github.cormconscho/phylo_scheduling/ttee/master/" hwp:id="ext-link-2">https://github.cormconscho/phylo_scheduling/ttee/master/</ext-link> datasets and their properties are summarized in <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table I.</xref></p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table I</label><caption hwp:id="caption-10"><p hwp:id="p-69">MSA PROPERTIES.</p></caption><graphic xlink:href="035840_tbl1" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap></sec><sec id="s6c" hwp:id="sec-14"><label>C.</label><title hwp:id="title-16">Characterizing the problem</title><p hwp:id="p-70">Initially, we assessed to which degree the actual tree shape influences the PLF-C differences between SR-based and SR-agnostic PLF implementations. We started 100 maximum likelihood (ML) tree searches from randomized stepwise addition order parsimony starting trees and another 100 ML searches on random starting trees for every test MSA using RAxML. We then calculated the PLF-C ratios between SR-based and SR-agnostic PLF implementations for each tree and MSA. The average SR-induced PLF-C savings for ML trees inferred on random and parsimony starting trees are almost identical. However, ML trees inferred from random starting trees show a slightly higher variance in savings than ML trees inferred on parsimony starting trees. This variance is more pronounced for our empirical MSAs. <xref ref-type="fig" rid="fig11" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Figure 11</xref> shows a representative boxplot for MSA 3.</p><fig id="fig11" position="float" orientation="portrait" fig-type="figure" hwp:id="F10" hwp:rev-id="xref-fig-10-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG11</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">fig11</object-id><label>Figure 11.</label><caption hwp:id="caption-11"><p hwp:id="p-71">PLF-C savings when employing the SR technique for MSA 3</p></caption><graphic xlink:href="035840_fig11" position="float" orientation="portrait" hwp:id="graphic-13"/></fig><p hwp:id="p-72">As shown by Kobert <italic toggle="yes">et al.</italic> [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-5" hwp:rel-id="ref-12">12</xref>], the savings are substantial. The average PLF-C saving over all MSA and all trees we generated is 61.68%, with a minimum of 48.92% and a maximum of 91.14%.</p><p hwp:id="p-73">Next, we evaluated possible factors influencing the SR-induced savings based on several MSA characteristics. We did not find a correlation of savings with the likelihood score of a tree for a given MSA. That is, a tree with a better likelihood score does not necessarily have a lower PLF-C. Also, the PLF-C does not appear to correlate with the number of taxa. However, the number of sites per par-tition <italic toggle="yes">is</italic> positively correlated with PLF-C savings. Also, the placement of the virtual root, that is used to conduct the post-order tree traversal for calculating the CLVs (see Section III), has an impact on the PLF-C. Our initial experiments revealed that placing the virtual root using the so-called midpoint rooting technique, that reduces the height of the thereby <italic toggle="yes">rooted</italic> binary tree, increases PLF-C savings.</p><p hwp:id="p-74">Because of this observation, all of our heuristics use a midpoint rooting on the given input tree for calculating the PLF-C of respective data distributions.</p></sec><sec id="s6d" hwp:id="sec-15"><label>D.</label><title hwp:id="title-17">Ground-truth comparison</title><p hwp:id="p-75">To analyze the performance of our heuristics we also compared them with the exact, optimal solution. We therefore generated and evaluated all possible data distributions exhaustively. Since there are n<sup>c</sup> possibilities for assigning sites to CPUs, this was only feasible for small problem instances. The optimal solution is the data distribution that minimizes the PLF-C of the most loaded CPU. From each of the 8 MSAs in <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table I</xref> we assembled 4 small MSAs by sampling MSA sites from the beginning of each partition. These groundtruth MSAs contained 4 partitions with 3 – 5 sites that were assigned to 2 CPUs and 2 partitions with 4-6 sites that were assigned to 3 CPUs. As one may expect, the exact solution is generally better than our heuristic solution. However, for 11 out of the 32 MSAs one of the heuristics was <italic toggle="yes">on par</italic> with the exact solution. In the worst case, the PLF-C difference between the best heuristic and the optimal solution for the most loaded CPU was 9.34%.</p><p hwp:id="p-76">The optimal solution was also the <italic toggle="yes">perfect</italic> solution in 1 out of 32 experiments. We have a perfect solution when the PLF-C of each CPU equals the lower bound <italic toggle="yes">L</italic> (see Section IV-B). For instance, this is the case if the data distribution does not need to split up any partition such that sites which share SRs are not distributed over different CPUs (see Section III).</p><p hwp:id="p-77">Keep in mind that, due to the small input sizes, the above results are mainly useful for verifying that our heuristics are not ‘far off’. Also note that, the groundtruth calculates the optimal solution with respect to <italic toggle="yes">φ</italic> (i.e., the PLF-C) but does not strive to minimize the α cost.</p></sec><sec id="s6e" hwp:id="sec-16"><label>E.</label><title hwp:id="title-18">Performance ofheuristics</title><p hwp:id="p-78">To assess the performance of our heuristics, we calculated data distributions for all MSAs. For each MSA we arbitrarily selected one of the ML trees inferred on a parsimony starting tree with RAxML and computed distributions for 2, 4, 8, 16, 32, and 64 CPUs. We then averaged the results for each heuristic over all MSAs and CPU counts. We assess the resulting heuristic data distributions by comparing the respective PLF-C of the most loaded CPU with the lower bound PLF-C L (see Section IV-B) and the resulting number <italic toggle="yes">extra<sub>α</sub></italic> of additional <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) calculations due to splitting up partitions (where <inline-formula hwp:id="inline-formula-30"><alternatives hwp:id="alternatives-32"><inline-graphic xlink:href="035840_inline30.gif" hwp:id="inline-graphic-30"/></alternatives></inline-formula>, see Section IV for notation <italic toggle="yes">c</italic>, <italic toggle="yes">P<sub>j</sub></italic>, and <italic toggle="yes">k</italic>) with the lowest <italic toggle="yes">extra<sub>α</sub></italic> among all heuristic distributions. The heuristic that yielded the lowest PLF-C over all MSAs comprises the following components: a lexicographic MSA sort, then CUT, followed by the ADJUSTLIMIT, LOWSRC, and REDMAX reshuffling strategies. On average, the PLF-C of this heuristic is only 5.75% higher than the rather conservative theoretical lower bound <italic toggle="yes">L</italic>. Out of the 32 combinations of our heuristic components that we list in this paper (see Section V) it ranks 5<italic toggle="yes">th</italic> with respect to <italic toggle="yes">extra<sub>α</sub></italic>. The <italic toggle="yes">extra<sub>α</sub></italic> is only 4.69% higher than that of the best result which we deem acceptable based on our experience with developing ExaML and ExaBayes.</p><p hwp:id="p-79">The best combination using the GREEDY component, also uses lexicographically sorted MSAs and <italic toggle="yes">all</italic> available reshuffling strategies in the same order as above. Its PLF-C is on average 6.27% worse than <italic toggle="yes">L</italic> but the <italic toggle="yes">extra<sub>α</sub></italic> is 42.83% higher than that of the best result which might be rather prohibitive for real-world parallel PLF implementations.</p><p hwp:id="p-80">We also need to answer the question if a dedicated data distribution strategy for SR-based parallel PLF implementations is indeed necessary. To this end, we repeated the above experiments with the original SR-agnostic data distribution algorithm (ODDA). The data distribution proposed by ODDA requires on average 176.21% more PLF-C than <italic toggle="yes">L</italic>. In comparison to ODDA, the best performing heuristic requires on average 61.71% less PLF-C with a minimum of 1.99% and a maximum of 92.31%. The <italic toggle="yes">extra<sub>α</sub></italic> of the best heuristic is surprisingly 0.81% lower than that of the ODDA. It might appear counter-intuitive, that some heuristics yield a lower <italic toggle="yes">extra<sub>α</sub></italic> than the ODDA. However, our reshuffling strategies can strive to reduce <italic toggle="yes">extra<sub>α</sub></italic> by placing entire small partitions with a large SRC onto the same CPU. Also, running ODDA on lexicographically sorted MSAs does not significantly change the results. It merely improves the PLF-C to 175.9% with respect to the lower bound <italic toggle="yes">L</italic>. Thus, a dedicated SR-aware data distribution heuristic <italic toggle="yes">is</italic> required since it can reduce PLF-C values and hence parallel runtimes by 60% on average.</p><p hwp:id="p-81">The results of these performance analyses are summarized in <xref ref-type="fig" rid="fig12" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 12</xref>. The Figure displays the key heuristic performance metrics averaged over all MSAs. It also demonstrates that <italic toggle="yes">extra<sub>α</sub></italic> is generally low and does not vary substantially among heuristics. <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Figure 8</xref> presents a representative overview of the heuristic performances for dataset 2.</p><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F11" hwp:rev-id="xref-fig-11-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-12"><p hwp:id="p-82">Comparison of the 2 best performing heuristic component combinations and the ODDA for dataset 2 for all 6 CPU scenarios. The y-axis shows the percentage that the most loaded CPU is larger than the lower bound L in terms of PLF-C. The red dot represents the average over all CPUs scenarios. In this case the heuristic using CUT requires on average 1 – (106.5/276.93) = 61.54% less PLF-C than the ODDA.</p></caption><graphic xlink:href="035840_fig8" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><fig id="fig9" position="float" orientation="portrait" fig-type="figure" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3 xref-fig-12-4 xref-fig-12-5 xref-fig-12-6 xref-fig-12-7 xref-fig-12-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9.</label><caption hwp:id="caption-13"><p hwp:id="p-83">Illustrations.</p></caption><graphic xlink:href="035840_fig9" position="float" orientation="portrait" hwp:id="graphic-15"/></fig><p hwp:id="p-84">In <xref ref-type="fig" rid="fig13" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Figure 13</xref> we provide an example data distribution for MSA 1 on 4 CPUs for our two best heuristics as well as for ODDA.</p><fig id="fig13" position="float" orientation="portrait" fig-type="figure" hwp:id="F13" hwp:rev-id="xref-fig-13-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;035840v3/FIG13</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">fig13</object-id><label>Figure 13.</label><caption hwp:id="caption-14"><p hwp:id="p-85">MSA 1 distributed across 4 CPUs. The upper part of the graph presents the data distribution in terms of PLF-C, the lower part in terms of sites. The red horizontal bar indicates the lower bound <italic toggle="yes">L</italic>. In this example the <italic toggle="yes">greedy approach</italic> in conjunction with a lexicographic MSA sort, the <italic toggle="yes">low SRC</italic> and <italic toggle="yes">reduce max</italic> reshuffling strategies performs best. The <italic toggle="yes">extra<sub>α</sub></italic> value shows that one more redundant <italic toggle="yes">P</italic>(<italic toggle="yes">t</italic>) calculation is required than for the data distribution of the <italic toggle="yes">cut approach.</italic> The ODDA balances the partitions in terms of sites, but this deteriorates PLF-C values for SR-based PLF implementations.</p></caption><graphic xlink:href="035840_fig13" position="float" orientation="portrait" hwp:id="graphic-16"/></fig></sec></sec><sec id="s7" hwp:id="sec-17"><label>VII.</label><title hwp:id="title-19">CONCLUSION AND FUTURE WORK</title><p hwp:id="p-86">In this paper we present the, to our knowledge, first data distribution problem statement for parallel PLF implementations that rely on the site repeats technique for reducing the number of PLF calculations on partitioned phylogenomic MSAs.</p><p hwp:id="p-87">Whether the data distribution problem is NP-hard remains an open question. Here, we assume no polynomial algorithm exists, and we provide a lower bound <italic toggle="yes">L</italic> for the optimal data distribution scheme and explore a large number of heuristic data distribution algorithms using a component-based framework. All components and heuristics as well as the MSAs are freely available via <monospace>github</monospace>.</p><p hwp:id="p-88">We initially characterize and quantify some aspects of SR-based calculations. Then, we present the two best performing heuristics that generate data distribution schemes with a PLF-C that is on average only 6% worse than the PLF-C of our conservative lower bound <italic toggle="yes">L</italic>. More importantly, we show that designing SR-aware data distribution algorithms <italic toggle="yes">does matter</italic>, since the standard SR-agnostic ODDA approach yields an average PLF-C for the most loaded CPU that is more than twice as high as the PLF-C attained by our heuristics. Thus, SR-aware data distribution heuristics can reduce runtimes for parallel phylogenetic analyses by 60%.</p><p hwp:id="p-89">We are planning on integrating the heuristics into real-world phylogenetic inference tools. We are currently developing a revised version of the PLL [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">7</xref>] that will include a full SR implementation and a Message Passing Interface (MPI) parallelization. Hence, we will also integrate a variant of the heuristics presented here. Note that, these heuristics were assessed on fixed trees with fixed (midpoint) rootings. While this covers application scenarios where trees remain fixed and only statistical model parameters on the tree are being optimized (e.g., divergence time estimates or tests for positive selection) this is not practical for tree searches where the tree topologies and virtual root positions constantly change. To this end, we will need to test appropriate heuristics that yield “good” data distributions for a larger set of trees and virtual rootings. Evidently, there is a tradeoff between slightly increased PLF-C costs and frequent data re-distributions. While the asymptotic complexities of our heuristics might appear to be large, we expect them to be amortized by the substantially higher empirical cost of likelihood-based tree evaluations.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-20">ACKNOWLEDGMENT</title><p hwp:id="p-90">Part of this work was financially supported by the Klaus Tschira Foundation.</p></ack><fn-group hwp:id="fn-group-1"><fn id="fn1" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1"><label>1</label><p hwp:id="p-91"><ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/conscho/phylo_scheduling" ext-link-type="uri" xlink:href="https://github.com/conscho/phylo_scheduling" hwp:id="ext-link-3">https://github.com/conscho/phylo_scheduling</ext-link></p></fn></fn-group><fn-group hwp:id="fn-group-2"><fn id="fn2" hwp:id="fn-2" hwp:rev-id="xref-fn-2-1"><label>2</label><p hwp:id="p-92"><ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/jhbadger/Newick-ruby" ext-link-type="uri" xlink:href="https://github.com/jhbadger/Newick-ruby" hwp:id="ext-link-4">https://github.com/jhbadger/Newick-ruby</ext-link></p></fn></fn-group><ref-list hwp:id="ref-list-1"><title hwp:id="title-21">REFERENCES</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Yang Z"><given-names>Z</given-names>. <surname>Yang</surname></string-name>, <article-title hwp:id="article-title-2">Computational Molecular Evolution</article-title>. <source hwp:id="source-1">Oxford Series in Ecology and Evolution</source>, <year>2006</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2 xref-ref-2-3 xref-ref-2-4"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Stamatakis A"><given-names>A</given-names>. <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-3">RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies</article-title>,” <source hwp:id="source-2">Bioinformatics</source>, vol. <volume>30</volume>, no. <issue>9</issue>, pp. <fpage>1312</fpage>–<lpage>1313</lpage>, <year>2014</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Huelsenbeck J. P."><given-names>J. P.</given-names> <surname>Huelsenbeck</surname></string-name>, <string-name name-style="western" hwp:sortable="Ronquist F."><given-names>F.</given-names> <surname>Ronquist</surname></string-name>, and others, “<article-title hwp:id="article-title-4">MrBayes: Bayesian inference of phylogenetic trees</article-title>,” <source hwp:id="source-3">Bioinformatics</source>, vol. <volume>17</volume>, no. <issue>8</issue>, pp. <fpage>754</fpage>–<lpage>755</lpage>, <year>2001</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Alachiotis N."><given-names>N.</given-names> <surname>Alachiotis</surname></string-name> and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-5">A generic and versatile architecture for inference of evolutionary trees under maximum likelihood</article-title>,” in <source hwp:id="source-4">Signals, Systems and Computers (ASILOMAR), 2010 Conference Record of the Forty Fourth Asilomar Conference on.</source> IEEE, <year>2010</year>, pp. <fpage>829</fpage>–<lpage>835</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Zhang J."><given-names>J.</given-names> <surname>Zhang</surname></string-name> and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-6">The multi-processor scheduling problem in phylogenetics</article-title>,” in <source hwp:id="source-5">Parallel and Distributed Processing Symposium Workshops PhD Forum (IPDPSW), 2012 IEEE 26th International</source>, May <year>2012</year>, pp. <fpage>691</fpage>–<lpage>698</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2 xref-ref-6-3 xref-ref-6-4"><label>[6]</label><citation publication-type="book" citation-type="book" ref:id="035840v3.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Kobert K."><given-names>K.</given-names> <surname>Kobert</surname></string-name>, <string-name name-style="western" hwp:sortable="Flouri T."><given-names>T.</given-names> <surname>Flouri</surname></string-name>, <string-name name-style="western" hwp:sortable="Aberer A."><given-names>A.</given-names> <surname>Aberer</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, <chapter-title>“The Divisible Load Balance Problem and Its Application to Phylo-genetic Inference,”</chapter-title> in <source hwp:id="source-6">Algorithms in Bioinformatics</source>. <publisher-name>Springer</publisher-name> <publisher-loc>Berlin Heidelberg</publisher-loc>, <year>2014</year>, pp. <fpage>204</fpage>–<lpage>216</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Flouri T"><given-names>T</given-names>. <surname>Flouri</surname></string-name>, <string-name name-style="western" hwp:sortable="Izquierdo-Carrasco F."><given-names>F.</given-names> <surname>Izquierdo-Carrasco</surname></string-name>, <string-name name-style="western" hwp:sortable="Darriba D."><given-names>D.</given-names> <surname>Darriba</surname></string-name>, <string-name name-style="western" hwp:sortable="Aberer A.J."><given-names>A.J.</given-names> <surname>Aberer</surname></string-name>, <string-name name-style="western" hwp:sortable="Nguyen L.-T."><given-names>L.-T.</given-names> <surname>Nguyen</surname></string-name>, <string-name name-style="western" hwp:sortable="Minh B.Q."><given-names>B.Q.</given-names> <surname>Minh</surname></string-name>, <string-name name-style="western" hwp:sortable="Haeseler A. V."><given-names>A. V.</given-names> <surname>Haeseler</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-7">The Phylogenetic Likelihood Library</article-title>,” <source hwp:id="source-7">Systematic Biology</source>, vol. <volume>64</volume>, no. <issue>2</issue>, pp. <fpage>356</fpage>–<lpage>362</lpage>, <year>2015</year>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Stamatakis A"><given-names>A</given-names>. <surname>Stamatakis</surname></string-name>, <string-name name-style="western" hwp:sortable="Ludwig T."><given-names>T.</given-names> <surname>Ludwig</surname></string-name>, <string-name name-style="western" hwp:sortable="Meier H."><given-names>H.</given-names> <surname>Meier</surname></string-name>, and <string-name name-style="western" hwp:sortable="Wolf M."><given-names>M.</given-names> <surname>Wolf</surname></string-name>, “<article-title hwp:id="article-title-8">Accelerating Parallel Maximum Likelihood-Based Phylogenetic Tree Calculations Using Subtree Equality Vectors</article-title>,” in <source hwp:id="source-8">Supercomputing, ACM/IEEE 2002 Conference</source>, <year>2002</year>, pp. <fpage>40</fpage>–<lpage>40</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Izquierdo-Carrasco F"><given-names>F</given-names>. <surname>Izquierdo-Carrasco</surname></string-name>, <string-name name-style="western" hwp:sortable="Smith S. A."><given-names>S. A.</given-names> <surname>Smith</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-9">Algorithms, data structures, and numerics for likelihood-based phylogenetic inference of huge trees</article-title>,” <source hwp:id="source-9">BMC Bioinformatics</source>, vol. <volume>12</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>14</lpage>, <year>2011</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="book" citation-type="book" ref:id="035840v3.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Christou M."><given-names>M.</given-names> <surname>Christou</surname></string-name>, <string-name name-style="western" hwp:sortable="Crochemore M."><given-names>M.</given-names> <surname>Crochemore</surname></string-name>, <string-name name-style="western" hwp:sortable="Flouri T."><given-names>T.</given-names> <surname>Flouri</surname></string-name>, <string-name name-style="western" hwp:sortable="Iliopoulos C.S."><given-names>C.S.</given-names> <surname>Iliopoulos</surname></string-name>, <string-name name-style="western" hwp:sortable="Janouek J."><given-names>J.</given-names> <surname>Janouek</surname></string-name>, <string-name name-style="western" hwp:sortable="Melichar B."><given-names>B.</given-names> <surname>Melichar</surname></string-name>, and <string-name name-style="western" hwp:sortable="Pissis S. P."><given-names>S. P.</given-names> <surname>Pissis</surname></string-name>, “<chapter-title>Computing All Subtree Repeats in Ordered Ranked Trees</chapter-title>,” in <source hwp:id="source-10">Proceedings of the 18th International Conference on String Processing and Information Retrieval, ser.</source> SPIRE’11. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>, <year>2011</year>, pp. <fpage>338</fpage>–<lpage>343</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><label>[11]</label><citation publication-type="book" citation-type="book" ref:id="035840v3.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Flouri T."><given-names>T.</given-names> <surname>Flouri</surname></string-name>, <string-name name-style="western" hwp:sortable="Kobert K."><given-names>K.</given-names> <surname>Kobert</surname></string-name>, <string-name name-style="western" hwp:sortable="Pissis S.P."><given-names>S.P.</given-names> <surname>Pissis</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, <chapter-title>“An Optimal Algorithm for Computing All Subtree Repeats in Trees,”</chapter-title> in <source hwp:id="source-11">Combinatorial Algorithms</source>. <publisher-name>Springer</publisher-name> <publisher-loc>Berlin Heidelberg</publisher-loc>, <year>2013</year>, no. <issue>8288</issue>, pp. <fpage>269</fpage>–<lpage>282</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2 xref-ref-12-3 xref-ref-12-4 xref-ref-12-5"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Kobert K."><given-names>K.</given-names> <surname>Kobert</surname></string-name>, <string-name name-style="western" hwp:sortable="Flouri T."><given-names>T.</given-names> <surname>Flouri</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-10">Efficient detection of repeating sites to accelerate phylogenetic likelihood calculations</article-title>,” <source hwp:id="source-12">bioRxiv</source>, <year>2015</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="book" citation-type="book" ref:id="035840v3.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Sathe M."><given-names>M.</given-names> <surname>Sathe</surname></string-name>, <string-name name-style="western" hwp:sortable="Schenk O."><given-names>O.</given-names> <surname>Schenk</surname></string-name>, and <string-name name-style="western" hwp:sortable="Burkhart H."><given-names>H.</given-names> <surname>Burkhart</surname></string-name>, “<chapter-title>Solving Bi-objective Many-Constraint Bin Packing Problems in Automobile Sheet Metal Forming Processes</chapter-title>,” in <source hwp:id="source-13">Evolutionary Multi-Criterion Optimization</source>, <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Ehrgott M."><given-names>M.</given-names> <surname>Ehrgott</surname></string-name>, <string-name name-style="western" hwp:sortable="Fonseca C. M."><given-names>C. M.</given-names> <surname>Fonseca</surname></string-name>, <string-name name-style="western" hwp:sortable="Gandibleux X."><given-names>X.</given-names> <surname>Gandibleux</surname></string-name>, <string-name name-style="western" hwp:sortable="Hao J.-K."><given-names>J.-K.</given-names> <surname>Hao</surname></string-name>, and <string-name name-style="western" hwp:sortable="Sevaux M."><given-names>M.</given-names> <surname>Sevaux</surname></string-name></person-group>, Eds. <publisher-name>Springer</publisher-name> <publisher-loc>Berlin Heidelberg</publisher-loc>, <year>2009</year>, no. <issue>5467</issue>, pp. <fpage>246</fpage>–<lpage>260</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="book" citation-type="book" ref:id="035840v3.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Wood T."><given-names>T.</given-names> <surname>Wood</surname></string-name>, <string-name name-style="western" hwp:sortable="Tarasuk-Levin G."><given-names>G.</given-names> <surname>Tarasuk-Levin</surname></string-name>, <string-name name-style="western" hwp:sortable="Shenoy P."><given-names>P.</given-names> <surname>Shenoy</surname></string-name>, <string-name name-style="western" hwp:sortable="Desnoyers P."><given-names>P.</given-names> <surname>Desnoyers</surname></string-name>, <string-name name-style="western" hwp:sortable="Cec-chet E."><given-names>E.</given-names> <surname>Cec-chet</surname></string-name>, and <string-name name-style="western" hwp:sortable="Corner M.D."><given-names>M.D.</given-names> <surname>Corner</surname></string-name>, <chapter-title>“Memory Buddies: Exploiting Page Sharing for Smart Colocation in Virtualized Data Centers,”</chapter-title> in <source hwp:id="source-14">Proceedings of the 2009 ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments</source>, ser. VEE ’09. <publisher-loc>New York, NY, USA</publisher-loc>: ACM, <year>2009</year>, pp. <fpage>31</fpage>–<lpage>40</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Sindelar M."><given-names>M.</given-names> <surname>Sindelar</surname></string-name>, <string-name name-style="western" hwp:sortable="Sitaraman R.K."><given-names>R.K.</given-names> <surname>Sitaraman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Shenoy P."><given-names>P.</given-names> <surname>Shenoy</surname></string-name>, “<article-title hwp:id="article-title-11">Sharing-aware Algorithms for Virtual Machine Colocation</article-title>,” in <source hwp:id="source-15">Proceedings of the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures</source>, ser. SPAA ’11. New York, NY, USA: ACM, <year>2011</year>, pp. <fpage>367</fpage>–<lpage>378</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="035840v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Flouri T."><given-names>T.</given-names> <surname>Flouri</surname></string-name>, <string-name name-style="western" hwp:sortable="Kobert K."><given-names>K.</given-names> <surname>Kobert</surname></string-name>, <string-name name-style="western" hwp:sortable="Pissis S. P."><given-names>S. P.</given-names> <surname>Pissis</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stamatakis A."><given-names>A.</given-names> <surname>Stamatakis</surname></string-name>, “<article-title hwp:id="article-title-12">An optimal algorithm for computing all subtree repeats in trees</article-title>,” <source hwp:id="source-16">Philosophical Transactions ofthe Royal Society of London A: Mathematical, Physical and Engineering Sciences</source>, vol. <volume>372</volume>, no. <issue>2016</issue>, p. <fpage>20130140</fpage>, <year>2014</year>.</citation></ref></ref-list><app-group hwp:id="app-group-1"><app id="app1" hwp:id="app-1"><title hwp:id="title-22">APPENDIX</title></app></app-group></back></article>
