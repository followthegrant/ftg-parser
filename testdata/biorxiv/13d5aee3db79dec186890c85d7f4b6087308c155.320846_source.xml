<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/320846</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;320846</article-id><article-id pub-id-type="other" hwp:sub-type="slug">320846</article-id><article-id pub-id-type="other" hwp:sub-type="tag">320846</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Deep Bidirectional Recurrent Neural Networks as End-To-End Models for Smoking Status Extraction from Clinical Notes in Spanish</article-title></title-group><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6940-9734</contrib-id><name name-style="western" hwp:sortable="Esteban Santiago"><surname>Esteban</surname><given-names>Santiago</given-names></name><degrees>MD MPH</degrees><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-6940-9734"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Tablado Manuel Rodríguez"><surname>Tablado</surname><given-names>Manuel Rodríguez</given-names></name><degrees>MD</degrees><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Peper Francisco E."><surname>Peper</surname><given-names>Francisco E.</given-names></name><degrees>MD</degrees><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Terrasa Sergio A."><surname>Terrasa</surname><given-names>Sergio A.</given-names></name><degrees>MD MSc</degrees><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Kopitowski Karin S."><surname>Kopitowski</surname><given-names>Karin S.</given-names></name><degrees>MD</degrees><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5"><label>1</label><institution hwp:id="institution-1">Family and Community Medicine Division, Hospital Italiano de Buenos Aires</institution>, Buenos Aires, <country>Argentina</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label><institution hwp:id="institution-2">Research Department, Instituto Universitario Hospital Italiano</institution>, Buenos Aires, <country>Argentina</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Public Health Department, Instituto Universitario Hospital Italiano</institution>, Buenos Aires, <country>Argentina</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-05-13T21:02:57-07:00">
    <day>13</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-08-03T11:51:09-07:00">
    <day>3</day><month>8</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-05-13T21:08:59-07:00">
    <day>13</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-08-03T11:58:08-07:00">
    <day>3</day><month>8</month><year>2018</year>
  </pub-date><elocation-id>320846</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-05-11"><day>11</day><month>5</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-08-03"><day>03</day><month>8</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-08-03"><day>03</day><month>8</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></p></license></permissions><self-uri xlink:href="320846.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/320846v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="320846.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/320846v3/320846v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/320846v3/320846v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><sec hwp:id="sec-1"><title hwp:id="title-2">Introduction</title><p hwp:id="p-2">Although natural language processing (NLP) tools have been available in English for quite some time, this is not the case for many other languages, particularly for texts from specific contexts such as clinical texts. This poses a challenge for tasks such as classifying text in languages other than English. In the absence of basic NLP tools, the development of statistical models that include manually designed variables that capture the semantic information of the documents is a potential solution. However, this process is expensive and slow. Deep recurrent neural networks (RNNs) have been proposed as “end-to-end” models that learn both variables and parameters jointly, thus avoiding manual feature engineering and saving development time.</p></sec><sec hwp:id="sec-2"><title hwp:id="title-3">Methods</title><p hwp:id="p-3">We compared the performance of two strategies for labeling clinical notes of an electronic medical record in Spanish according to the patient’s smoking status (current smoker, current non-smoker, text without information on tobacco): 1. A traditional approach using two classifiers (a multilayer perceptron (MLP) and a support vector machine (SVM)) together with a ‘bag-of-words’ text representation that involves intensive manual development of features and, 2. an ’end-to-end’ model which uses a Short-Long-Term Memory bidirectional deep RNN with GloVe word embeddings. The classifiers were trained in the training set (n = 11775 clinical texts) and were evaluated in the test set (n = 2943) by means of macro-averaged recall, precision and F1 score.</p></sec><sec hwp:id="sec-3"><title hwp:id="title-4">Results</title><p hwp:id="p-4">The RNN scored high values of all three metrics in the test set (sensitivity [95% CI]: 0.965 [0.96, 0.97], PPV: 0.963 [0.96, 0.97], F1 score: 0.964 [0.96, 0.97]). It also showed to be slightly superior to the MLP (difference in recall: 0.009 [95% CI: -0.0007, 0.017], precision: 0.007 [95% CI: -0.0015, 0.019] and F1 score: 0.009 [95% CI: 0.0018, 0.016]); comparing the RNN with the SVM, the latter has a better performance in general (recall difference [95% CI]: -0.007 [-0.016, 0.0018], precision: -0.009 [-0.018, 0.00015] and score F1: -0.008 [-0.014, -0.0017]). In both cases only the confidence interval for the F1 score difference excludes zero. In turn, the RNN consumed 80% less overall development time.</p></sec><sec hwp:id="sec-4"><title hwp:id="title-5">Conclusion</title><p hwp:id="p-5">In our work, the deep bidirectional RNN as end-to-end model, reached similar levels of performance in the classification of clinical texts in Spanish that models with a great manual engineering of variables, although in less than 20% of the development time. This makes them an important tool to streamline text processing in languages where the development of NLP tools has not progressed as much as in English. Areas such as research or public health management could clearly benefit from ’end-to-end’ models that facilitate the exploitation of already available data sources, such as electronic clinical records.</p></sec></abstract><counts><page-count count="18"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-5"><title hwp:id="title-6">Introduction</title><p hwp:id="p-6">Globally, tobacco is one of the main risk factors for premature death<sup>[<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>]</sup> being responsible for 11.5% of the worldwide annual deaths (6.5 million)<sup>[<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>]</sup>. It is also one of the five main risk factors of disability<sup>[<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>]</sup>. In 2015, the global prevalence of daily tobacco use was estimated at 25% (uncertainty interval (UI) 24.2-25.7) in men and 5.4% (UI 5.1-5.7) in women with a marked decrease since 1990<sup>[<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-3" hwp:rel-id="ref-2">2</xref>]</sup>. However, despite these general trends, there is a high level of heterogeneity among countries and this decline was not as marked in countries with low and middle socio-demographic levels, especially in women<sup>[<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-4" hwp:rel-id="ref-2">2</xref>]</sup>. This is evident in that of the 933 million daily smokers, 80% reside in such countries<sup>[<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]</sup>. However, despite its importance, clinical and public health research on risk factors such as smoking is often restricted, among other things, by the scarcity of trained human resources and economic resources<sup>[<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>, <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>]</sup>.</p><p hwp:id="p-7">In this context, electronic health records (EHR) are proposed as data sources that facilitate clinical research, especially taking into account the increasing adoption rate in both developed and developing countries<sup>[<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>]</sup>.</p><p hwp:id="p-8">Information regarding tobacco consumption is usually recorded in the EHRs in different formats, both as structured problems but also as free text, usually complementing each other<sup>[<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>]</sup>. However, there is a clear tendency to register a large part of the relevant information on smoking in the free text due to, among other reasons, the lack of flexibility of the more structured recording systems<sup>[<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]</sup>. This is why many authors have explored methods based on natural language processing (NLP) for the extraction of information regarding the patients’ smoking status from clinical notes<sup>[<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>–<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>]</sup>.</p><p hwp:id="p-9">The task of classifying texts by assigning them a predefined category or label (for example: smoker, non-smoker, ex-smoker) has traditionally been solved using techniques based on hard-coded rules and techniques based on statistical or supervised machine-learning<sup>[<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>]</sup>, which have progressively dominated NLP tasks in the last 30 years.</p><p hwp:id="p-10">One of the most disseminated and also simplest approaches is the bag-of-words model<sup>[<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]</sup>. In this model each gram (term or token) or n-gram (concatenation of n tokens) of the corpus (collection of documents) is treated as a feature and a value is assigned to each feature based on the weighted or unweighted counts of the terms in each document. Thus, each document is represented by a feature vector of these counts. In turn, all the feature vectors compose a document-term-matrix, where each observation (document) is a row and each feature, a column. Once this matrix is created, a classifier algorithm is iteratively trained to minimize a cost function in order to find the best separation plane between the categories and predict de document’s label. The performance of this method can be improved by developing variables that better capture the linguistic structure of the text. Many NLP tools have been designed to create such features like constituency<sup>[<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>]</sup> and dependency parsing<sup>[<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>]</sup>, negation detection<sup>[<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>, <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]</sup>, part-of-speech (POS) tagging<sup>[<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>]</sup> or named-entity recognition<sup>[<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>]</sup> tools.</p><p hwp:id="p-11">However, these tools are, for the most part, language-specific and, in many cases, context-specific. This means that, in order to apply them in a language or context different from the one they were developed in, it is necessary to retrain (re-learn the parameters of the model in a new dataset) or even redesign them completely. Clinical notes from EHRs are clearly different from usual texts: they contain specific jargon, incomplete or grammatically incorrect sentences and a large number of abbreviations and acronyms. This implies that, in order to develop the beforementioned NLP tools for specific languages and contexts, it is necessary to develop a training corpus, usually annotated by linguistics experts (“gold corpus”), which is an expensive and slow process that in many cases ends up being prohibitive in terms of resource expenditure. This aspect is rather important considering that most of the research and tools in NLP have been developed in English and most of the developing countries, where it would be essential to promote research on smoking, do not have English as their primary language and therefore, these tools are not available in an “off-the-shelf” fashion.</p><p hwp:id="p-12">A potentially more accessible solution is the extension of the models described so far by developing simple linguistic models and manually engineering features specific to the classification task. This is usually done by rigorous exploration of the text and with specific knowledge of the classification problem itself. Examples of this would be the identification of key topic words, the development of specific tools to detect key term negations and modifiers. This, although feasible, requires the investment of many hours in the process of manual feature engineering and feature selection. This approach, nevertheless, suffers from the fact that the models are not usually exportable to other classification problems.</p><sec id="s1a" hwp:id="sec-6"><title hwp:id="title-7">End-to-End models for text classification</title><p hwp:id="p-13">The models described so far are usually implemented in two phases, where the features and the parameters of the models are developed separately and by different actors: the features are manually engineered by humans, the parameters are estimated from the data by the classifier. The philosophy behind ‘End-to-End’ models seeks that both, features and parameters are learned jointly and by the same actor: the algorithm. In recent years ‘End-to-End’ models have been excelling at tasks such as image classification<sup>[<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>]</sup>, machine translation<sup>[<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]</sup>, autonomous driving<sup>[<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>]</sup> and speech recognition<sup>[<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>]</sup>. Most of these examples have used deep neural networks but the ‘End-to-End’ philosophy exceeds this type of algorithms. Basically, End-to-End models remove intermediate steps by freeing the human from the task of manually engineering features and can potentially help find patterns in the data that would be difficult for a human to discover. In the specific case of NLP, they can avoid having to develop many of the most widely used tools (parsers, POS taggers, etc.). This is particularly advantageous for languages or contexts for which prebuilt tools are not available or don’t perform very well.</p></sec><sec id="s1b" hwp:id="sec-7"><title hwp:id="title-8">Bidirectional Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) as End-to-End models for text classification</title><p hwp:id="p-14">Recurrent neural networks are a type of artificial neural network architecture that allows to process series of data sequentially. Basically, the hidden state of the previous neuron (<italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic>−1</sub>) is part of the input of the neuron <italic toggle="yes">t</italic><sub>0</sub> along with the input at <italic toggle="yes">t</italic> (<italic toggle="yes">x</italic><sub><italic toggle="yes">t</italic></sub>) (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">fig. 1</xref>). Thus, <italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic></sub> depends on <italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic>-1</sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">t</italic></sub> (<italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic></sub> = <italic toggle="yes">f</italic>(<italic toggle="yes">W</italic><sup><italic toggle="yes">xh</italic></sup><italic toggle="yes">x</italic><sub><italic toggle="yes">t</italic></sub> + <italic toggle="yes">W</italic><sup><italic toggle="yes">hh</italic></sup><italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic>−1</sub> + <italic toggle="yes">b</italic><sup><italic toggle="yes">h</italic></sup>)), where <italic toggle="yes">f</italic> is usually a non-linear activation function (sigmoid, tanh, ReLU) and <italic toggle="yes">b</italic> is the bias term. Unlike in feed-forward neural networks, the weights (<italic toggle="yes">W</italic><sup><italic toggle="yes">xh</italic></sup>, <italic toggle="yes">W</italic><sup><italic toggle="yes">hh</italic></sup>, <italic toggle="yes">W</italic><sup><italic toggle="yes">hy</italic></sup>) are shared for each time step <italic toggle="yes">t</italic>. This sharing of weights is what gives RNN the ability to ‘remember’ the hidden states of previous neurons and allows modeling of data sequences by capturing sequential patterns. Nevertheless, in practice, standard RNN are not capable of handling long-term dependencies because of the vanishing and exploding gradients problem initially detailed by Bengio et al<sup>[<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>]</sup>. Modifications to the structure of basic RNNs (Long-Short Term Memory<sup>[<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>]</sup> units; Gated Recurrent Units<sup>[<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]</sup>) have allowed these algorithms to learn even more distant dependencies within the series without suffering from the problems detailed above. Being able to represent long term dependencies makes them natural candidates for the classification of texts, since they can represent key aspects of linguistic structures that resembles human text processing<sup>[<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>]</sup>. In addition to this, bdRNNs (bidirectional RNNs)<sup>[<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>]</sup> allows for the processing of the texts such that <italic toggle="yes">h</italic><sub><italic toggle="yes">t</italic></sub> is also influenced by data further down the sequence. Also, stacking of such bidirectional RNNs in layers enables the network to learn hierarchical more abstract representations (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">fig. 2</xref>). Moreover, the representation of words by means of pretrained word embeddings<sup>[<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>–<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>]</sup> further increases the representational capacity of these models.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><title hwp:id="title-9">An unrolled recurrent neural network</title><p hwp:id="p-15">For simplicity the biases are not depicted.</p><p hwp:id="p-16"><italic toggle="yes">x</italic>: input; <italic toggle="yes">h</italic>: hidden state; <italic toggle="yes">t</italic>: time step; <italic toggle="yes">y</italic>: output; <italic toggle="yes">w</italic>: weights.</p></caption><graphic xlink:href="320846_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><title hwp:id="title-10">An unrolled bidirectional long-short-term-memory recurrent neural network</title><p hwp:id="p-17"><italic toggle="yes">LSTM</italic>: long-short-term memory; <italic toggle="yes">t</italic>: time step; <italic toggle="yes">h</italic>: hidden layer; <italic toggle="yes">x</italic>: input; <italic toggle="yes">y</italic>: output.</p></caption><graphic xlink:href="320846_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec></sec><sec id="s2" hwp:id="sec-8"><title hwp:id="title-11">Background</title><p hwp:id="p-18">In recent years, End-to-End models implemented by means of artificial neural networks (ANN) have become more relevant for NLP tasks in general <sup>[<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>–<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>]</sup> as well as in specific domains, like clinical narratives. In 2016, Jagannatha et al. <sup>[<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>]</sup> compared the performance of a bidirectional RNN vs conditional random fields (CRF)<sup>[<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>]</sup> for detecting medical events in notes from an EHR. They found improvements in accuracy, recall and F1 score vs. the base system. Other applications of RNNs in medical texts have explored their performance in tasks such as deidentification of clinical notes<sup>[<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">33</xref>]</sup>, information extraction<sup>[<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>, <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>]</sup>, named entity recognition<sup>[<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>]</sup>, relation extraction<sup>[<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>]</sup> and text classification<sup>[<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>]</sup> with superior results to the reference models. However, we find did not find any articles that assessed the capacity of these models to classify clinical notes in Spanish.</p><sec id="s2a" hwp:id="sec-9"><title hwp:id="title-12">Aim</title><p hwp:id="p-19">The main objective of our research is the comparison of two strategies for the classification of clinical notes in Spanish according to the patient’s smoking status: 1. a traditional approach involving task specific knowledge and heavy manual feature engineering and, 2. An ‘End-to-End’ approach. For the first strategy we used a bag of words representation, developed a task specific parser and trained two classifiers: a multi-layer perceptron (MLP) and a support vector machine (SVM). For the End-to-End strategy we implemented a deep bidirectional RNN using word embeddings as input.</p></sec><sec id="s2b" hwp:id="sec-10"><title hwp:id="title-13">Dataset</title><p hwp:id="p-20">Clinical notes were obtained from the EHR database at Hospital Italiano de Buenos Aires, Buenos Aires, Argentina. After taking a random sample of 6000 notes out of all the clinical notes from the ambulatory, emergency room and inpatient setting, we established that only 3.65% (95% CI 3.14 - 4.1) contained some information related to smoking. In order to optimize the manual labeling process given the low proportion of notes with information on tobacco use, we defined a filter based on four stems (‘tab’, ‘cig’, ‘fum’, ‘tbq’) to achieve a sample that had a greater proportion of notes with information on tobacco. We evaluated this filter on another set of 6000 notes sampled at random (which were also manually labeled), resulting in a negative predictive value of 0.99948 (95% CI 0.9985 - 0.9999) and a positive predictive value of 0.83077 (95% CI 0.7796 - 0.8743). Using this filter, we selected a sample of notes that contained any of the four stems from a random sample of 3054 patients 18 years of age or older, between 2005 and 2015. This yielded a total of 14718 clinical notes. The notes were labeled by three domain experts (family doctors) as ‘current smoker’, ‘current non-smoker’ and ‘clinical note without information on tobacco’ (inter-annotator agreement: kappa 0.97, 95% CI 0.9 - 1). The label ‘previous smoker’ was avoided since we found that in many cases these ex-smokers were described as non-smokers in the notes and that patients were almost never described as ‘never smokers’. So, the ‘current non-smoker’ category includes ‘never smokers’ and ‘ex-smokers’. <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref> shows the characteristics of the dataset.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-3"><p hwp:id="p-21">Dataset characteristics.</p></caption><graphic xlink:href="320846_tbl1" position="float" orientation="portrait" hwp:id="graphic-3"/></table-wrap></sec></sec><sec id="s3" hwp:id="sec-11"><title hwp:id="title-14">Methods</title><p hwp:id="p-22">Once the filtered sample of 14718 clinical notes was obtained, it was randomly split into a training set (11775 notes, 80%) and a test set (2943 notes, 20%). The test set was set aside and only used for the evaluation of the final models. The training set was used to train all three models, using 10-fold cross-validation for the hyperparameter tuning and feature selection processes. The final performance of the classifiers in the test set was assessed using per class and macro-averaged precision (positive predictive value), recall (sensitivity) and F1-score. All analyses were performed in R 3.4.1<sup>[<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>]</sup> using the dplyr, ggplot, tidyr, text2vec, keras, tidytext, tensorflow and caret packages. All models were trained through the Keras API<sup>[<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>]</sup> with TensorFlow<sup>[<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>]</sup> as backend. <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> shows an overview of the models’ development and validation process.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-4"><title hwp:id="title-15">Overview of the models’ development and validation process</title><p hwp:id="p-23">EtE: End-to-End model; MLP: Multi-layer perceptron; SVM: Support Vector Machine; CV: Cross-validation.</p></caption><graphic xlink:href="320846_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><sec id="s3a" hwp:id="sec-12"><title hwp:id="title-16">Manually developed models</title><p hwp:id="p-24">The development of the models was divided into two stages: 1. Development of a task specific parser and manual feature engineering, 2. Classifier training</p><sec id="s3a1" hwp:id="sec-13"><label>1.</label><title hwp:id="title-17">Task specific parser development (feature engineering and selection)</title><p hwp:id="p-25">In order to incorporate task and context specific knowledge into the feature engineering process, we developed a task specific parser that allowed us to detect key smoking related terms, negations of those terms, positive and negative modifiers of those terms and modifier negation terms. Also, we detected who the subject associated with key smoking terms was.</p><p hwp:id="p-26">Twelve key terms related to smoking were defined. The key term negation detection module was developed using regular expressions. This module detects negation terms 25 characters before and 10 after the key terms, unless some punctuation sign is detected such as commas, periods, question marks, semicolons and parentheses. Positive and negative key term modifiers were detected from a predefined list. Using a logic similar to that of the negation module, these modifiers were only detected if they were in the proximity of the key terms. Then the detection of negations of these modifiers was added. Finally, a list of terms that would detect other subjects related to the central terms (mother, father, brother, spouse, etc.) was developed. <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref> shows an example of the parser applied to a sentences containing information on smoking status.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-5"><title hwp:id="title-18">Example of the task specific parser (Spanish and English)</title><p hwp:id="p-27"><italic toggle="yes">Subj:</italic> subject; <italic toggle="yes">negat.:</italic> negation; <italic toggle="yes">neg.mod.:</italic> negative modifier</p></caption><graphic xlink:href="320846_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-28">Once this parser was defined, the documents were segmented into sentences and only those that had any of the key smoking related terms were kept. A table was constructed in which each row was a sentence and each column contained the counts of each of the terms detected by the parser per sentence. Thus, initially, 9021 features were defined. This number was later reduced to 561 after dropping those with a spearman correlation coefficient greater than 0.85 and those that had an entropy-based importance of zero. Completing this process required 280 hours.</p></sec><sec id="s3a2" hwp:id="sec-14"><label>2.</label><title hwp:id="title-19">Classifier training</title><p hwp:id="p-29">For both classifiers, hyperparameters were evaluated using a grid search approach and 10-fold cross validation.</p><sec id="s3a2a" hwp:id="sec-15"><title hwp:id="title-20">Multi-layer perceptron</title><p hwp:id="p-30">The final classifier resulted in a multi-layer perceptron (feed-forward fully connected artificial neural network) with three hidden dense layers (483, 377 and 358 hidden units respectively) and a Softmax layer as the output layer (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">figure 5</xref>). It was optimized for 150 epochs in batches of 128 observations by mini-batch gradient descent with momentum of 0.9, learning rate of 0.2 and learning rate decay of 0.004. We used Rectified Linear Units (ReLU) as activation functions between layers and categorical cross entropy as the objective function. All hyperparameters were tuned using 10-fold cross-validation. L2 regularization and dropout<sup>[<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]</sup> were evaluated to avoid overfitting, but they did not have a significant impact on the model’s performance in the cross-validation set so they were not incorporated into the final model. The total training time of the classifier was 40 hours.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-6"><title hwp:id="title-21">Multi-layer perceptron (feed forward neural network)</title><p hwp:id="p-31">CS: Current smoker; CNS: Current non-smoker; NIT: no information on tobacco; ReLU: Rectified linear unit; x: input.</p></caption><graphic xlink:href="320846_fig5" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec><sec id="s3a2b" hwp:id="sec-16"><title hwp:id="title-22">Support vector machine</title><p hwp:id="p-32">As an alternative to the MLP we trained a support vector machine<sup>[<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>]</sup> on the same 561 features dataset as the MLP. We tested three different kernels: lineal, polynomial and radial. Finally, the radial basis function was selected with hyperparameters cost=8 and gamma=0.57. Total training time was 16 hours.</p></sec></sec><sec id="s3a3" hwp:id="sec-17"><title hwp:id="title-23">End-to-End model</title><sec id="s3a3a" hwp:id="sec-18"><label>1.</label><title hwp:id="title-24">Classifier training</title><p hwp:id="p-33">We trained a RNN at the document level with a word embedding layer, three hidden bidirectional LSTM layers (100, 100, 100 hidden units per layer per direction), three hidden dense layers (100, 100, 50 hidden units) and a Softmax layer to output the probabilities for each category (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">fig. 6</xref>). We used recurrent dropout<sup>[<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref>]</sup> (dropout probability = 0.2) to avoid overfitting in the LSTM layers and regular dropout (dropout probability = 0.2) in the dense layers. The activation function between the LSTM layers is tanh and ReLU between the dense layers. As optimization algorithm we used RMSprop<sup>[<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref>]</sup>. We also used categorical cross entropy as the objective function. All hyperparameters were tuned using 10-fold cross validation. The total time of development of the model was 56 hours. The first 10 hours were used for the training and exploration of the word embeddings; the last 46 were assigned to training the network.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-7"><title hwp:id="title-25">Final End-to-End model</title><p hwp:id="p-34"><italic toggle="yes">h</italic>: hidden layer; <italic toggle="yes">bdLSTM</italic>: bidirectional long-short-term-memory layer; <italic toggle="yes">HU</italic>: Hidden units; <italic toggle="yes">Dim</italic>: dimensions; <italic toggle="yes">Dense</italic>: dense fully connected layer; <italic toggle="yes">p</italic>: probability of retention; <italic toggle="yes">AF</italic>: activation function; <italic toggle="yes">ReLU</italic>: rectified linear units; <italic toggle="yes">OU</italic>: output units; <italic toggle="yes">OF</italic>: output function, CS: Current smoker; CNS: Current non-smoker; NIT: No information on tobacco.</p></caption><graphic xlink:href="320846_fig6" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s3a3b" hwp:id="sec-19"><title hwp:id="title-26">GloVe Word embeddings</title><p hwp:id="p-35">In order to initialize the embedding layer of the RNN, we pretrained word embeddings from a corpus formed by the training set documents plus the 12,000 documents of the samples described in the <italic toggle="yes">‘Dataset’</italic> section, using the model described by Pennington et. al<sup>[<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">37</xref>]</sup>. A window of 10 words was selected. Of the 36,558 unique words, those that appeared less than 5 times throughout the corpus were discarded. The remaining 9,483 tokens were trained to obtain 200 dimensional vectors.</p></sec></sec></sec></sec><sec id="s4" hwp:id="sec-20"><title hwp:id="title-27">Results</title><p hwp:id="p-36"><xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref> and <xref rid="fig7a" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">figure 7a</xref> show the performance of the three classifiers on the test set. The RNN was slightly superior to the MLP in all three macro-averaged metrics (difference in recall: 0.009 [95%CI: -0.0007, 0.017], precision: 0.007 [95% CI: -0.0015, 0.019] and F1-score: 0.009 [95% CI: 0.0018, 0.016]), although the 95% confidence interval only excludes zero for the difference in F1 score (<xref rid="fig7b" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">figure 7b</xref>). When comparing the performance of the bdRNN (bidirectional RNN) to the SVM, the SVM performs better in general (difference in recall: -0.007 [95%CI: -0.016, 0.0018], precision: -0.009 [95% CI: -0.018, 0.00015] and F1-score: -0.008 [95% CI: -0.014, -0.0017]), where also the F1 score difference confidence interval excludes zero.</p><fig id="fig7a" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG7A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7a</object-id><label>Figure 7a.</label><caption hwp:id="caption-8"><p hwp:id="p-37">Performance of the classifiers on the test set.</p></caption><graphic xlink:href="320846_fig7a" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><fig id="fig7b" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/FIG7B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig7b</object-id><label>Figure 7b.</label><caption hwp:id="caption-9"><title hwp:id="title-28">Difference in macro-averaged metrics between classifiers</title><p hwp:id="p-38">Macro-averaged F1-score, precision and recall with 95% confidence intervals.</p><p hwp:id="p-39">bdRNN: bidirectional recurrent neural network; SVMrbf: support vector machine w/radial basis function; MLP: multi-layer perceptron.</p></caption><graphic xlink:href="320846_fig7b" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;320846v3/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-10"><p hwp:id="p-40">Performance of all three classifiers on the test set.</p></caption><graphic xlink:href="320846_tbl2" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap></sec><sec id="s5" hwp:id="sec-21"><title hwp:id="title-29">Discussion</title><p hwp:id="p-41">We developed a set of classifiers that detect smoking status from clinical notes is Spanish using a bag-of-word model with heavy manual feature engineering and a sequential End-to-End model by means of a bidirectional deep LSTM RNN. The End-to-End model performed similarly to methods that involve a great investment of time and task specific knowledge for the manual development of features. To the best of our knowledge, this is the first paper that evaluates the performance of these models in clinical notes in Spanish.</p><p hwp:id="p-42">This capability of the RNN to represent the information contained in the documents without any manual development of features is explained by several of its characteristics. Bidirectional sequential processing allows the incorporation of previous and subsequent information to each word and is very similar to the way in which humans understand texts. On the other hand, the different layers of the RNN allow a hierarchical representation of the information, learning more abstract representations of the texts in each layer. This, added to the representation of the words by word embeddings that capture the semantic relationships between words, places this type of models as an excellent choice of End-to-End models.</p><p hwp:id="p-43">Not having to engineer features manually facilitates texts analysis in languages or contexts for which NLP tools are not available. In addition, according to our experience, it saves time (in our case, the development of the RNN required less than 20% of the time it took to develop the manual models) and allows the analyst to focus on the architecture of the network rather than on the exploration of the notes and extraction of variables. This in turn makes the workflow easily exportable to other classification problems, in various contexts and languages unlike the process of manual variable development. In the particular context of public health management based on EHR data, it is even more valuable since it allows to potentially design, in a short time, precise classifiers for creating electronic phenotypes than can be used in policy evaluation or research. This, nevertheless, comes at the price of spending time in hyperparameter tuning and designing the structure of the network, which is usually a trial and error process.</p><p hwp:id="p-44">Our work has several limitations. Among them, we use a set of bag-of-words models as baseline models since it is part of our usual workflow, however other alternatives such as conditional random fields or other state-of-the-art models could be excellent baseline models as well, as proposed by other authors<sup>[<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-3" hwp:rel-id="ref-33">33</xref>, <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">42</xref>, <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">44</xref>, <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">45</xref>, <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">47</xref>, <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-2" hwp:rel-id="ref-48">48</xref>]</sup>. Also, we only evaluated the models in a classification problem with only a few classification categories. However, other teams have had very good results even with more categories and fewer observations<sup>[<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-3" hwp:rel-id="ref-42">42</xref>, <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-3" hwp:rel-id="ref-44">44</xref>]</sup>.</p></sec><sec id="s6" hwp:id="sec-22"><title hwp:id="title-30">Conclusion</title><p hwp:id="p-45">We have shown that deep bidirectional LSTM RNNs as End-to-End models achieve similar same levels of performance in the classification of clinical texts in Spanish, as models with heavy manual feature engineering albeit in less than 20% of the development time. This makes them an important tool for streamlining text processing in languages in which the development of NLP tools has not advanced as much as in English. Areas like research or public health management could clearly benefit from ‘End-to-End’ models that facilitate the exploitation of already available data sources such as EHRs.</p></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-31">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><source hwp:id="source-1">Global, regional, and national comparative risk assessment of 79 behavioural, environmental and occupational, and metabolic risks or clusters of risks, 1990-2015: a systematic analysis for the Global Burden of Disease Study 2015</source>. <publisher-loc>Lancet</publisher-loc>, <year>2016</year>. <volume>388</volume>(<issue>10053</issue>): p. <fpage>1659</fpage>–<lpage>1724</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2 xref-ref-2-3 xref-ref-2-4"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Reitsma M.B."><surname>Reitsma</surname>, <given-names>M.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fullman N."><given-names>N.</given-names> <surname>Fullman</surname></string-name>, <string-name name-style="western" hwp:sortable="Ng M."><given-names>M.</given-names> <surname>Ng</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-2">Smoking prevalence and attributable disease burden in 195 countries and territories, 1990&amp;–2015: a systematic analysis from the Global Burden of Disease Study 2015</article-title>. <source hwp:id="source-2">The Lancet</source>. <volume>389</volume>(<issue>10082</issue>): p. <fpage>1885</fpage>–<lpage>1906</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>3.</label><citation publication-type="website" citation-type="web" ref:id="320846v3.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><collab hwp:id="collab-1">WHO</collab>. <source hwp:id="source-3">Tobacco fact sheet</source>. <year>2016</year> [cited <date-in-citation>2018 03/03/2018</date-in-citation>]; Available from: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.who.int/mediacentre/factsheets/fs339/en/" ext-link-type="uri" xlink:href="http://www.who.int/mediacentre/factsheets/fs339/en/" hwp:id="ext-link-2">http://www.who.int/mediacentre/factsheets/fs339/en/</ext-link>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><article-title hwp:id="article-title-3">Strengthening clinical research capacity in low and middle-income countries</article-title>. <source hwp:id="source-4">Workshop report</source>., <string-name name-style="western" hwp:sortable="Sciences T.A.o.M."><given-names>T.A.o.M.</given-names> <surname>Sciences</surname></string-name>, Editor. <year>2017</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Käser M."><surname>Käser</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maure C."><given-names>C.</given-names> <surname>Maure</surname></string-name>, <string-name name-style="western" hwp:sortable="Halpaap B.M.M."><given-names>B.M.M.</given-names> <surname>Halpaap</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-4">Research Capacity Strengthening in Low and Middle Income Countries – An Evaluation of the WHO/TDR Career Development Fellowship Programme</article-title>. <source hwp:id="source-5">PLoS Neglected Tropical Diseases</source>, <year>2016</year>. <volume>10</volume>(<issue>5</issue>): p. <fpage>e0004631</fpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>6.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Stone C.P."><surname>Stone</surname>, <given-names>C.P.</given-names></string-name>, <source hwp:id="source-6">A Glimpse at EHR Implementation Around the World: The Lessons the US Can Learn</source>. <year>2014</year>, <publisher-name>The Health Institute for E-Health Policy</publisher-name>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Wang Y."><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen E.S."><given-names>E.S.</given-names> <surname>Chen</surname></string-name>, <string-name name-style="western" hwp:sortable="Pakhomov S."><given-names>S.</given-names> <surname>Pakhomov</surname></string-name>, <string-name name-style="western" hwp:sortable="Lindemann E."><given-names>E.</given-names> <surname>Lindemann</surname></string-name>, and <string-name name-style="western" hwp:sortable="Melton G.B."><given-names>G.B.</given-names> <surname>Melton</surname></string-name>, <article-title hwp:id="article-title-5">Investigating Longitudinal Tobacco Use Information from Social History and Clinical Notes in the Electronic Health Record</article-title>. <source hwp:id="source-7">AMIA Annu Symp Proc</source>, <year>2016</year>. <volume>2016</volume>: p. <fpage>1209</fpage>–<lpage>1218</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Chen E.S."><surname>Chen</surname>, <given-names>E.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carter E.W."><given-names>E.W.</given-names> <surname>Carter</surname></string-name>, <string-name name-style="western" hwp:sortable="Sarkar I.N."><given-names>I.N.</given-names> <surname>Sarkar</surname></string-name>, <string-name name-style="western" hwp:sortable="Winden T.J."><given-names>T.J.</given-names> <surname>Winden</surname></string-name>, and <string-name name-style="western" hwp:sortable="Melton G.B."><given-names>G.B.</given-names> <surname>Melton</surname></string-name>, <article-title hwp:id="article-title-6">Examining the use, contents, and quality of free-text tobacco use documentation in the Electronic Health Record</article-title>. <source hwp:id="source-8">AMIA Annu Symp Proc</source>, <year>2014</year>. <volume>2014</volume>: p. <fpage>366</fpage>–<lpage>74</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Wu C.Y."><surname>Wu</surname>, <given-names>C.Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chang C.K."><given-names>C.K.</given-names> <surname>Chang</surname></string-name>, <string-name name-style="western" hwp:sortable="Robson D."><given-names>D.</given-names> <surname>Robson</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-7">Evaluation of smoking status identification using electronic health records and open-text information in a large mental health case register</article-title>. <source hwp:id="source-9">PLoS One</source>, <year>2013</year>. <volume>8</volume>(<issue>9</issue>): p. <fpage>e74262</fpage>.</citation></ref><ref id="c10" hwp:id="ref-10"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Wicentowski R."><surname>Wicentowski</surname>, <given-names>R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sydes M.R."><given-names>M.R.</given-names> <surname>Sydes</surname></string-name>, <article-title hwp:id="article-title-8">Using implicit information to identify smoking status in smoke-blind medical discharge summaries</article-title>. <source hwp:id="source-10">J Am Med Inform Assoc</source>, <year>2008</year>. <volume>15</volume>(<issue>1</issue>): p. <fpage>29</fpage>–<lpage>31</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Uzuner O."><surname>Uzuner</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goldstein I."><given-names>I.</given-names> <surname>Goldstein</surname></string-name>, <string-name name-style="western" hwp:sortable="Luo Y."><given-names>Y.</given-names> <surname>Luo</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kohane I."><given-names>I.</given-names> <surname>Kohane</surname></string-name>, <article-title hwp:id="article-title-9">Identifying patient smoking status from medical discharge records</article-title>. <source hwp:id="source-11">J Am Med Inform Assoc</source>, <year>2008</year>. <volume>15</volume>(<issue>1</issue>): p. <fpage>14</fpage>–<lpage>24</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Solberg L.I."><surname>Solberg</surname>, <given-names>L.I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Flottemesch T.J."><given-names>T.J.</given-names> <surname>Flottemesch</surname></string-name>, <string-name name-style="western" hwp:sortable="Foldes S.S."><given-names>S.S.</given-names> <surname>Foldes</surname></string-name>, <string-name name-style="western" hwp:sortable="Molitor B.A."><given-names>B.A.</given-names> <surname>Molitor</surname></string-name>, <string-name name-style="western" hwp:sortable="Walker P.F."><given-names>P.F.</given-names> <surname>Walker</surname></string-name>, and <string-name name-style="western" hwp:sortable="Crain A.L."><given-names>A.L.</given-names> <surname>Crain</surname></string-name>, <article-title hwp:id="article-title-10">Tobacco-use prevalence in special populations taking advantage of electronic medical records</article-title>. <source hwp:id="source-12">Am J Prev Med</source>, <year>2008</year>. <volume>35</volume>(<issue>6 Suppl</issue>): p. <fpage>S501</fpage>–<lpage>7</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Savova G.K."><surname>Savova</surname>, <given-names>G.K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ogren P.V."><given-names>P.V.</given-names> <surname>Ogren</surname></string-name>, <string-name name-style="western" hwp:sortable="Duffy P.H."><given-names>P.H.</given-names> <surname>Duffy</surname></string-name>, <string-name name-style="western" hwp:sortable="Buntrock J.D."><given-names>J.D.</given-names> <surname>Buntrock</surname></string-name>, and <string-name name-style="western" hwp:sortable="Chute C.G."><given-names>C.G.</given-names> <surname>Chute</surname></string-name>, <article-title hwp:id="article-title-11">Mayo clinic NLP system for patient smoking status identification</article-title>. <source hwp:id="source-13">J Am Med Inform Assoc</source>, <year>2008</year>. <volume>15</volume>(<issue>1</issue>): p. <fpage>25</fpage>–<lpage>8</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="McCormick P.J."><surname>McCormick</surname>, <given-names>P.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Elhadad N."><given-names>N.</given-names> <surname>Elhadad</surname></string-name>, and <string-name name-style="western" hwp:sortable="Stetson P.D."><given-names>P.D.</given-names> <surname>Stetson</surname></string-name>, <article-title hwp:id="article-title-12">Use of semantic features to classify patient smoking status</article-title>. <source hwp:id="source-14">AMIA Annu Symp Proc</source>, <volume>2008</volume>: p. <fpage>450</fpage>–<lpage>4</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Heinze D.T."><surname>Heinze</surname>, <given-names>D.T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morsch M.L."><given-names>M.L.</given-names> <surname>Morsch</surname></string-name>, <string-name name-style="western" hwp:sortable="Potter B.C."><given-names>B.C.</given-names> <surname>Potter</surname></string-name>, and <string-name name-style="western" hwp:sortable="Sheffer R.E."><given-names>R.E.</given-names> <surname>Sheffer</surname></string-name>, <article-title hwp:id="article-title-13">Jr., Medical i2b2 NLP smoking challenge: the A-Life system architecture and methodology</article-title>. <source hwp:id="source-15">J Am Med Inform Assoc</source>, <year>2008</year>. <volume>15</volume>(<issue>1</issue>): p. <fpage>40</fpage>–<lpage>3</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Cohen A.M."><surname>Cohen</surname>, <given-names>A.M.</given-names></string-name>, <article-title hwp:id="article-title-14">Five-way smoking status classification using text hot-spot identification and error-correcting output codes</article-title>. <source hwp:id="source-16">J Am Med Inform Assoc</source>, <year>2008</year>. <volume>15</volume>(<issue>1</issue>): p. <fpage>32</fpage>–<lpage>5</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Clark C."><surname>Clark</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Good K."><given-names>K.</given-names> <surname>Good</surname></string-name>, <string-name name-style="western" hwp:sortable="Jezierny L."><given-names>L.</given-names> <surname>Jezierny</surname></string-name>, <string-name name-style="western" hwp:sortable="Macpherson M."><given-names>M.</given-names> <surname>Macpherson</surname></string-name>, <string-name name-style="western" hwp:sortable="Wilson B."><given-names>B.</given-names> <surname>Wilson</surname></string-name>, and <string-name name-style="western" hwp:sortable="Chajewska U."><given-names>U.</given-names> <surname>Chajewska</surname></string-name>, <article-title hwp:id="article-title-15">Identifying smokers with a medical extraction system</article-title>. <source hwp:id="source-17">J Am Med Inform Assoc</source>, <year>2008</year>. <volume>15</volume>(<issue>1</issue>): p. <fpage>36</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>18.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Manning CD P.R."><surname>Manning CD</surname>, <given-names>P.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schütze H"><surname>Schütze</surname> <given-names>H</given-names></string-name>, <source hwp:id="source-18">Text classification &amp; Naive Bayes, in Introduction to Information Retrieval</source>. <volume>2008</volume>, <publisher-name>Cambridge University Press</publisher-name>: <publisher-loc>Cambridge, England</publisher-loc>. p. <fpage>253</fpage>–<lpage>289</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Salton G."><surname>Salton</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wong A."><given-names>A.</given-names> <surname>Wong</surname></string-name>, and <string-name name-style="western" hwp:sortable="Yang C.S."><given-names>C.S.</given-names> <surname>Yang</surname></string-name>, <article-title hwp:id="article-title-16">A vector space model for automatic indexing</article-title>. <source hwp:id="source-19">Commun. ACM</source>, <year>1975</year>. <volume>18</volume>(<issue>11</issue>): p. <fpage>613</fpage>–<lpage>620</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Cohen R."><surname>Cohen</surname>, <given-names>R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Elhadad M."><given-names>M.</given-names> <surname>Elhadad</surname></string-name>, <article-title hwp:id="article-title-17">Syntactic dependency parsers for biomedical-NLP</article-title>. <source hwp:id="source-20">AMIA Annu Symp Proc</source>, <year>2012</year>. <volume>2012</volume>: p. <fpage>121</fpage>–<lpage>8</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>21.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Chen D."><surname>Chen</surname>, <given-names>D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Manning C."><given-names>C.</given-names> <surname>Manning</surname></string-name>. <article-title hwp:id="article-title-18">A Fast and Accurate Dependency Parser using Neural Networks</article-title>. <year>2014</year>. <source hwp:id="source-21">Association for Computational Linguistics</source>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Mehrabi S."><surname>Mehrabi</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krishnan A."><given-names>A.</given-names> <surname>Krishnan</surname></string-name>, <string-name name-style="western" hwp:sortable="Sohn S."><given-names>S.</given-names> <surname>Sohn</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-19">DEEPEN: A negation detection system for clinical text incorporating dependency relation into NegEx</article-title>. <source hwp:id="source-22">J Biomed Inform</source>, <year>2015</year>. <volume>54</volume>: p. <fpage>213</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Chapman W.W."><surname>Chapman</surname>, <given-names>W.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hillert D."><given-names>D.</given-names> <surname>Hillert</surname></string-name>, <string-name name-style="western" hwp:sortable="Velupillai S."><given-names>S.</given-names> <surname>Velupillai</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-20">Extending the NegEx lexicon for multiple languages</article-title>. <source hwp:id="source-23">Stud Health Technol Inform</source>, <year>2013</year>. <volume>192</volume>: p. <fpage>677</fpage>–<lpage>81</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Toutanova K."><surname>Toutanova</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Klein D."><given-names>D.</given-names> <surname>Klein</surname></string-name>, <string-name name-style="western" hwp:sortable="Manning C.D."><given-names>C.D.</given-names> <surname>Manning</surname></string-name>, and <string-name name-style="western" hwp:sortable="Singer Y."><given-names>Y.</given-names> <surname>Singer</surname></string-name>, <article-title hwp:id="article-title-21">Feature-rich part-of-speech tagging with a cyclic dependency network, in Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</article-title> - Volume <volume>1</volume>. <year>2003</year>, <source hwp:id="source-24">Association for Computational Linguistics: Edmonton, Canada</source>. p. <fpage>173</fpage>–<lpage>180</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>25.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Wei Q."><surname>Wei</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen T."><given-names>T.</given-names> <surname>Chen</surname></string-name>, <string-name name-style="western" hwp:sortable="Xu R."><given-names>R.</given-names> <surname>Xu</surname></string-name>, <string-name name-style="western" hwp:sortable="He Y."><given-names>Y.</given-names> <surname>He</surname></string-name>, and <string-name name-style="western" hwp:sortable="Gui L."><given-names>L.</given-names> <surname>Gui</surname></string-name>, <chapter-title>Disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks</chapter-title>.<source hwp:id="source-25">Database</source> (<publisher-loc>Oxford</publisher-loc>), <volume>2016</volume>. <year>2016</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>26.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Alex Krizhevsky S."><surname>Alex Krizhevsky</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ilya Hinton"><surname>Ilya</surname>, <given-names>Hinton</given-names></string-name>, <string-name name-style="western" hwp:sortable="Geoffrey E"><surname>Geoffrey</surname> <given-names>E</given-names></string-name>, <source hwp:id="source-26">ImageNet Classification with Deep Convolutional Neural Networks, in Advances in Neural Information Processing Systems 25</source>, <string-name name-style="western" hwp:sortable="Weinberger F.P.a.C.J.C.B.a.L.B.a.K.Q."><given-names>F.P.a.C.J.C.B.a.L.B.a.K.Q.</given-names> <surname>Weinberger</surname></string-name>, Editor. <year>2012</year>, <publisher-name>Curran Associates, Inc</publisher-name>. p. <fpage>1097</fpage>–<lpage>1105</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>27.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Yonghui Wu M.S."><surname>Yonghui Wu</surname>, <given-names>M.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen Zhifeng"><given-names>Zhifeng</given-names> <surname>Chen</surname></string-name>, <string-name name-style="western" hwp:sortable="Quoc V. Le"><surname>Quoc</surname> <given-names>V. Le</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norouzi Mohammad"><given-names>Mohammad</given-names> <surname>Norouzi</surname></string-name>, <string-name name-style="western" hwp:sortable="Macherey Wolfgang"><given-names>Wolfgang</given-names> <surname>Macherey</surname></string-name>, <source hwp:id="source-27">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</source>. <publisher-name>CoRR</publisher-name>, <year>2016</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>28.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Mariusz Bojarski D.D.T."><surname>Mariusz Bojarski</surname>, <given-names>D.D.T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dworakowski Daniel"><given-names>Daniel</given-names> <surname>Dworakowski</surname></string-name>, <source hwp:id="source-28">End to End Learning for Self-Driving Cars</source>. <publisher-name>CoRR</publisher-name>, <year>2016</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>29.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Wayne Xiong J.D."><surname>Wayne Xiong</surname>, <given-names>J.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang Xuedong"><given-names>Xuedong</given-names> <surname>Huang</surname></string-name> and <source hwp:id="source-29">The Microsoft 2016 Conversational Speech Recognition System</source>. <publisher-name>CoRR</publisher-name>, <year>2016</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simard P."><given-names>P.</given-names> <surname>Simard</surname></string-name>, and <string-name name-style="western" hwp:sortable="Frasconi P."><given-names>P.</given-names> <surname>Frasconi</surname></string-name>, <article-title hwp:id="article-title-22">Learning long-term dependencies with gradient descent is difficult</article-title>. <source hwp:id="source-30">IEEE Trans Neural Netw</source>, <year>1994</year>. <volume>5</volume>(<issue>2</issue>): p. <fpage>157</fpage>–<lpage>66</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Hochreiter S.S."><surname>Hochreiter</surname>, <given-names>S.S.</given-names></string-name>, <article-title hwp:id="article-title-23">Jürgen Long Short-Term Memory</article-title>. <source hwp:id="source-31">Neural Computation</source>, <year>1997</year>. <volume>9</volume>(<issue>8</issue>): p. <fpage>1735</fpage>–<lpage>1780</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>32.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.32" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Junyoung Chung C.G."><surname>Junyoung Chung</surname>, <given-names>C.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cho KyungHyun"><given-names>KyungHyun</given-names> <surname>Cho</surname></string-name>, <string-name name-style="western" hwp:sortable="Bengio Yoshua"><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <article-title hwp:id="article-title-24">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</article-title>. <year>2014</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2 xref-ref-33-3"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Dernoncourt F."><surname>Dernoncourt</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee J.Y."><given-names>J.Y.</given-names> <surname>Lee</surname></string-name>, <string-name name-style="western" hwp:sortable="Uzuner O."><given-names>O.</given-names> <surname>Uzuner</surname></string-name>, and <string-name name-style="western" hwp:sortable="Szolovits P."><given-names>P.</given-names> <surname>Szolovits</surname></string-name>, <article-title hwp:id="article-title-25">De-identification of patient notes with recurrent neural networks</article-title>. <source hwp:id="source-32">J Am Med Inform Assoc</source>, <year>2017</year>. <volume>24</volume>(<issue>3</issue>): p. <fpage>596</fpage>–<lpage>606</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Schuster M."><surname>Schuster</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Paliwal K.K."><given-names>K.K.</given-names> <surname>Paliwal</surname></string-name>, <article-title hwp:id="article-title-26">Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing</article-title>, <year>1997</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>35.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Mikolov T S.I."><surname>Mikolov T</surname>, <given-names>S.I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen K"><surname>Chen</surname> <given-names>K</given-names></string-name>. <article-title hwp:id="article-title-27">Distributed representations of words and phrases and their compositionality</article-title>. <source hwp:id="source-33">in Advances in Neural Information Processing Systems</source>. <year>2013</year>.</citation></ref><ref id="c36" hwp:id="ref-36"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Collobert R W.J."><surname>Collobert R</surname>, <given-names>W.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bottou L"><surname>Bottou</surname> <given-names>L</given-names></string-name>, <article-title hwp:id="article-title-28">Natural language processing (almost) from scratch</article-title>. <source hwp:id="source-34">Journal of Machine Learning Research</source>, <year>2011</year>. <volume>12</volume>: p. <fpage>44</fpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2"><label>37.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Pennington J S.R."><surname>Pennington J</surname>, <given-names>S.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Manning CD"><surname>Manning</surname> <given-names>CD</given-names></string-name>. <article-title hwp:id="article-title-29">. GloVe: Global vectors for word representation</article-title>. <source hwp:id="source-35">in Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</source>. <year>2014</year>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>38.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Mikolov T K.a.M."><surname>Mikolov T</surname>, <given-names>K.a.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burget L"><surname>Burget</surname> <given-names>L</given-names></string-name>, <article-title hwp:id="article-title-30">Recurrent neural network based language model, in Interspeech</article-title>. <year>2013</year>.</citation></ref><ref id="c39" hwp:id="ref-39"><label>39.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Socher R P.A."><surname>Socher R</surname>, <given-names>P.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu JY"><surname>Wu</surname> <given-names>JY</given-names></string-name>. <article-title hwp:id="article-title-31">Recursive deep models for semantic compositionality over a sentiment treebank</article-title>. <source hwp:id="source-36">in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</source>. <year>2013</year>.</citation></ref><ref id="c40" hwp:id="ref-40"><label>40.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Y K."><surname>Y</surname>, <given-names>K.</given-names></string-name>, <article-title hwp:id="article-title-32">Convolutional neural networks for sentence classification., in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</article-title>. <year>2014</year>. p. <fpage>1746</fpage>–<lpage>51</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Lee JY D.F."><surname>Lee JY</surname>, <given-names>D.F.</given-names></string-name>, <article-title hwp:id="article-title-33">Sequential short-text classification with recurrent and convolutional neural networks, in Human Language Technologies 2016: The Conference of the North American Chapter of the Association for Computational Linguistics</article-title>. <year>2016</year>. p. <fpage>515</fpage>–<lpage>20</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2 xref-ref-42-3"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Jagannatha A.N."><surname>Jagannatha</surname>, <given-names>A.N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yu H."><given-names>H.</given-names> <surname>Yu</surname></string-name>, <article-title hwp:id="article-title-34">Bidirectional RNN for Medical Event Detection in Electronic Health Records</article-title>. <source hwp:id="source-37">Proc Conf</source>, <year>2016</year>. <volume>2016</volume>: p. <fpage>473</fpage>–<lpage>482</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>43.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.43" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Lafferty J.D.M."><surname>Lafferty</surname>, <given-names>J.D.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pereira Andrew"><given-names>Andrew</given-names>; <surname>Pereira</surname></string-name>, <string-name name-style="western" hwp:sortable="Fernando CN"><surname>Fernando</surname> <given-names>CN</given-names></string-name>. <chapter-title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</chapter-title>. <source hwp:id="source-38">in Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01. 2001</source>. <publisher-loc>San Francisco, CA, USA</publisher-loc>: <publisher-name>Morgan Kaufmann Publishers Inc</publisher-name>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2 xref-ref-44-3"><label>44.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Gao S."><surname>Gao</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Young M.T."><given-names>M.T.</given-names> <surname>Young</surname></string-name>, <string-name name-style="western" hwp:sortable="Qiu J.X."><given-names>J.X.</given-names> <surname>Qiu</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-35">Hierarchical attention networks for information extraction from cancer pathology reports</article-title>. <source hwp:id="source-39">J Am Med Inform Assoc</source>, <year>2017</year>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Zheng W."><surname>Zheng</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin H."><given-names>H.</given-names> <surname>Lin</surname></string-name>, <string-name name-style="western" hwp:sortable="Luo L."><given-names>L.</given-names> <surname>Luo</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-36">An attention-based effective neural model for drug-drug interactions extraction</article-title>. <source hwp:id="source-40">BMC Bioinformatics</source>, <year>2017</year>. <volume>18</volume>(<issue>1</issue>): p. <fpage>445</fpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Liu Z."><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yang M."><given-names>M.</given-names> <surname>Yang</surname></string-name>, <string-name name-style="western" hwp:sortable="Wang X."><given-names>X.</given-names> <surname>Wang</surname></string-name>, <etal>et al.</etal>, <article-title hwp:id="article-title-37">Entity recognition from clinical texts via recurrent neural network</article-title>. <source hwp:id="source-41">BMC Med Inform Decis Mak</source>, <year>2017</year>. <volume>17</volume>(<issue>Suppl 2</issue>): p. <fpage>67</fpage>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Luo Y."><surname>Luo</surname>, <given-names>Y.</given-names></string-name>, <article-title hwp:id="article-title-38">Recurrent neural networks for classifying relations in clinical notes</article-title>. <source hwp:id="source-42">J Biomed Inform</source>, <year>2017</year>. <volume>72</volume>: p. <fpage>85</fpage>–<lpage>95</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1 xref-ref-48-2"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Weng W.H."><surname>Weng</surname>, <given-names>W.H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wagholikar K.B."><given-names>K.B.</given-names> <surname>Wagholikar</surname></string-name>, <string-name name-style="western" hwp:sortable="McCray A.T."><given-names>A.T.</given-names> <surname>McCray</surname></string-name>, <string-name name-style="western" hwp:sortable="Szolovits P."><given-names>P.</given-names> <surname>Szolovits</surname></string-name>, and <string-name name-style="western" hwp:sortable="Chueh H.C."><given-names>H.C.</given-names> <surname>Chueh</surname></string-name>, <article-title hwp:id="article-title-39">Medical subdomain classification of clinical notes using a machine learning-based natural language processing approach</article-title>. <source hwp:id="source-43">BMC Med Inform Decis Mak</source>, <year>2017</year>. <volume>17</volume>(<issue>1</issue>): p. <fpage>155</fpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>49.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Team R.C."><surname>Team</surname>, <given-names>R.C.</given-names></string-name>, <source hwp:id="source-44">R: A language and environment for statistical computing</source>. <year>2017</year>, <publisher-name>R Foundation for Statistical Computing</publisher-name>: <publisher-loc>Vienna, Austria</publisher-loc>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>50.</label><citation publication-type="website" citation-type="web" ref:id="320846v3.50" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Chollet F. Keras"><surname>Chollet</surname>, <given-names>F. Keras</given-names></string-name>. <year>2015</year>; Available from: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://keras.io" ext-link-type="uri" xlink:href="https://keras.io" hwp:id="ext-link-3">https://keras.io</ext-link>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>51.</label><citation publication-type="book" citation-type="book" ref:id="320846v3.51" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Abadi M."><surname>Abadi</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barham P."><given-names>P.</given-names> <surname>Barham</surname></string-name>, <string-name name-style="western" hwp:sortable="Chen J."><given-names>J.</given-names> <surname>Chen</surname></string-name>, <etal>et al.</etal>, <source hwp:id="source-45">TensorFlow: a system for large-scale machine learning, in Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation</source>. <year>2016</year>, <publisher-name>USENIX Association</publisher-name>: <publisher-loc>Savannah, GA, USA</publisher-loc>. p. <fpage>265</fpage>–<lpage>283</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Nitish Srivastava G.H."><surname>Nitish Srivastava</surname>, <given-names>G.H.</given-names></string-name>, <article-title hwp:id="article-title-40">Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: A Simple Way to Prevent Neural Networks from Overfitting</article-title>. <source hwp:id="source-46">Journal of Machine Learning Research</source>, <year>2014</year>. <volume>15</volume>: p. <fpage>1929</fpage>–<lpage>1958</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="320846v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Cortes C."><surname>Cortes</surname>, <given-names>C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Vapnik V."><given-names>V.</given-names> <surname>Vapnik</surname></string-name>, <article-title hwp:id="article-title-41">Support-vector networks</article-title>. <source hwp:id="source-47">Machine Learning</source>, <year>1995</year>. <volume>20</volume>(<issue>3</issue>): p. <fpage>273</fpage>–<lpage>297</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="other" citation-type="journal" ref:id="320846v3.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Yarin Gal Z.G."><surname>Yarin Gal</surname>, <given-names>Z.G.</given-names></string-name>, <article-title hwp:id="article-title-42">A Theoretically Grounded Applicationof Dropout in Recurrent Neural Networks, in Neural Information Processing Systems</article-title>. <year>2016</year>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>55.</label><citation publication-type="website" citation-type="web" ref:id="320846v3.55" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Tijmen Tieleman G.H."><surname>Tijmen Tieleman</surname>, <given-names>G.H.</given-names></string-name> <article-title hwp:id="article-title-43">Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude</article-title>. <source hwp:id="source-48">Neural Networks for Machine Learning 2012</source> [cited <date-in-citation>2018 8/3/2018</date-in-citation>]; Available from: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" ext-link-type="uri" xlink:href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" hwp:id="ext-link-4">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</ext-link>.</citation></ref></ref-list></back></article>
