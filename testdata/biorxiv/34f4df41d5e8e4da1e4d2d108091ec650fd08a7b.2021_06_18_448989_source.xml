<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.06.18.448989</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.06.18.448989</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.06.18.448989</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.06.18.448989</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.06.18.448989</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">The functional specialization of visual cortex emerges from training parallel pathways with self-supervised predictive learning</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label><email hwp:id="email-1">patrick.mineault@gmail.com</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2439-7252</contrib-id><name name-style="western" hwp:sortable="Bakhtiari Shahab"><surname>Bakhtiari</surname><given-names>Shahab</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-2439-7252"/></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Mineault Patrick"><surname>Mineault</surname><given-names>Patrick</given-names></name><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Lillicrap Tim"><surname>Lillicrap</surname><given-names>Tim</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Pack Christopher C."><surname>Pack</surname><given-names>Christopher C.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Richards Blake A."><surname>Richards</surname><given-names>Blake A.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Mila &amp; McGill University</institution>, <email hwp:id="email-2">bakhtias@mila.quebec</email></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">DeepMind</institution>, <email hwp:id="email-3">timothylillicrap@google.com</email></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">McGill University</institution>, <email hwp:id="email-4">christopher.pack@mcgill.ca</email></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">CIFAR, Mila &amp; McGill University</institution>, <email hwp:id="email-5">blake.richards@mila.quebec</email></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-06-18T10:06:12-07:00">
    <day>18</day><month>6</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-10-26T17:51:13-07:00">
    <day>26</day><month>10</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-06-18T10:11:47-07:00">
    <day>18</day><month>6</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-10-26T17:56:51-07:00">
    <day>26</day><month>10</month><year>2021</year>
  </pub-date><elocation-id>2021.06.18.448989</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-06-18"><day>18</day><month>6</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-10-26"><day>26</day><month>10</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-10-26"><day>26</day><month>10</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="448989.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.06.18.448989v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="448989.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.06.18.448989v3/2021.06.18.448989v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.06.18.448989v3/2021.06.18.448989v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">The visual system of mammals is comprised of parallel, hierarchical specialized pathways. Different pathways are specialized in so far as they use representations that are more suitable for supporting specific downstream behaviours. In particular, the clearest example is the specialization of the ventral (“what”) and dorsal (“where”) pathways of the visual cortex. These two pathways support behaviours related to visual recognition and movement, respectively. To-date, deep neural networks have mostly been used as models of the ventral, recognition pathway. However, it is unknown whether both pathways can be modelled with a single deep ANN. Here, we ask whether a single model with a single loss function can capture the properties of both the ventral and the dorsal pathways. We explore this question using data from mice, who like other mammals, have specialized pathways that appear to support recognition and movement behaviours. We show that when we train a deep neural network architecture with two parallel pathways using a self-supervised predictive loss function, we can outperform other models in fitting mouse visual cortex. Moreover, we can model both the dorsal and ventral pathways. These results demonstrate that a self-supervised predictive learning approach applied to parallel pathway architectures can account for some of the functional specialization seen in mammalian visual systems.</p></abstract><counts><page-count count="24"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-3">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-1"><p hwp:id="p-4">Revisions include new supplementary results and some changes in the acknowledgements</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-4">Introduction</title><p hwp:id="p-5">In the mammalian visual cortex information is processed in a hierarchical manner using two specialized pathways [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>, <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]: the ventral, or “where” pathway, and the dorsal, or “what” pathway. These two pathways are specialized for visual recognition and movement, respectively [<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>, <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>, <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>, <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref>, <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>]. For example, damage to the ventral pathway may impair object recognition, whereas damage to the dorsal pathway may impair motion perception [<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">64</xref>].</p><p hwp:id="p-6">Deep artificial neural networks (ANNs) trained in a supervised manner on object categorization have been successful at matching the representations of the ventral visual stream [<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">62</xref>, <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. They have been shown to develop representations that map onto the ventral hierarchy, and which can be used to predict [<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-2" hwp:rel-id="ref-62">62</xref>] and control [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>, <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>] neural activity in the ventral pathway. However, when we look at the other principal visual pathway in the mammalian brain, i.e. the dorsal pathway, the situation is different. Very few studies have examined the ability of deep ANNs to develop representations that match the dorsal hierarchy (though see the following fMRI study: [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>]). Moreover, to the best of our knowledge, no studies have demonstrated both ventral-like and dorsal-like representations in a single network.</p><p hwp:id="p-7">This lack of deep ANN models that capture both ventral and dorsal pathways leads naturally to an important question: under what training conditions would specialized ventral-like and dorsal-like pathways emerge in a deep ANN? Would a second loss function be required to obtain matches to dorsal pathways, or is there a single loss function that could induce both types of representations?</p><p hwp:id="p-8">One promising set of candidates are predictive self-supervised loss functions [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>, <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>, <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>]. Recent work has shown that self-supervised learning can produce similar results to supervised learning for the ventral pathway [<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">63</xref>, <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>, <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>]. Moreover, there is a large body of work showing that mammals possess predictive processing mechanisms in their cortex [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>, <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>, <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>, <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>, <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>], including in the dorsal pathway [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>, <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>], which suggests that a predictive form of self-supervised learning could potentially lead to the emergence of both ventral and dorsal-like representations.</p><p hwp:id="p-9">Addressing this question requires recordings from different ventral and dorsal visual areas in the brain. Here, we explore these issues using publicly available data from the Allen Brain Observatory [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>], which provides recordings from a large number of areas in mouse visual cortex. We examine the ability of a self-supervised predictive loss function (contrastive predictive coding [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">45</xref>, <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>]) to induce representations that match mouse visual cortex. When we train a network with a single pathway, we find that it possesses more ventral-like representations. However, when we train a network with two parallel pathways we find that the predictive loss function induces distinct representations that map onto the ventral/dorsal division. This allows the network to better support both object categorization and motion recognition downstream tasks via the respective specialized pathways. In contrast, supervised training with an action categorization task only leads to matches to the ventral pathway, and not the dorsal pathway.</p><p hwp:id="p-10">Altogether, this work demonstrates that the two specialized pathways of visual cortex can be modelled with the same ANN if a self-supervised predictive loss function is applied to an architecture with parallel pathways. This suggests that self-supervised predictive loss functions may hold great promise for explaining the functional properties of mammalian visual cortex.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-5">Background and related work</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-6">Self-supervised ANN models of the ventral visual stream</title><p hwp:id="p-11">Recently, [<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-2" hwp:rel-id="ref-63">63</xref>] and [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">34</xref>] showed that the representations learned by self-supervised models trained on static images produce good matches to the ventral pathway. Our work builds on this by exploring the potential for self-supervised learning to also explain the dorsal pathway.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-7">ANN models of the mouse visual system</title><p hwp:id="p-12">Mice have become a common animal model in visual neuroscience due to the sophisticated array of experimental tools [<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>, <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]. As such, [<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>] compared the responses of different areas in mouse visual cortex with the representations of a VGG16 trained on ImageNet. In this paper, we show that self-supervised learning can produce better fits to both ventral and dorsal areas than supervised learning.</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-8">ANN models of the dorsal pathway</title><p hwp:id="p-13">The MotionNet ANN model [<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>], is a feedforward ANN trained to predict the motion direction of segments of natural images, which was inspired by the role of the dorsal pathway in motion perception. In [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>], they show that learning both ventral and dorsal-like representations in a single ANN with two pathways is possible if one forces the two pathways to process the phase and amplitude of a complex decomposition of the stimuli separately. In [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref>], they show that training a deep ANN on supervised action recognition can induce some match to dorsal pathway fMRI recordings. In this study, we show that with prediction as the learning objective and an architecture that has two parallel pathways, both ventral-like and dorsal-like representations can be learned.</p></sec></sec><sec id="s3" hwp:id="sec-6"><label>3</label><title hwp:id="title-9">Methods</title><sec id="s3a" hwp:id="sec-7"><label>3.1</label><title hwp:id="title-10">Datasets</title><p hwp:id="p-14">We use the Allen Brain Observatory open 2-photon calcium imaging dataset for the experiments in this study. We select subsets of the dataset based on brain area, recording depth, and visual stimuli used. Recordings from five areas of mouse visual cortex are used (VISp, VISlm, VISal, VISpm, VISam; <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1a</xref>). We only exclude one area from our analyses (VISrl) because it is a multi-sensory area, and visual stimuli alone do not drive it well [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>]. We use recordings from cortical depths of 175-250<italic toggle="yes">μm</italic> (which corresponds to cortical layers 2-3) as these are the recordings with the largest number of neurons. When selecting visual stimuli, we use recordings elicited by the presentation of natural movies, because unlike static images, movies can elicit clear responses in both ventral and dorsal areas [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>]. Thus, we only use those parts of the dataset in which natural movies (30 seconds long) were presented as visual stimuli. More details about the dataset can be found in [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">9</xref>]. For training the deep ANNs, we use the UCF101 dataset [<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref>] (see <xref ref-type="sec" rid="s7a4" hwp:id="xref-sec-33-1" hwp:rel-id="sec-33">section A.4</xref> of supplementary materials).</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11 xref-fig-1-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-15">(a) The schematic of mouse visual cortex. (b) Representational similarity analysis between the visual areas included in our analysis. (c) The anatomical hierarchy score of the visual areas adopted from [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]. (d) Ventral and dorsal scores of the visual areas. Areas are sorted from the most ventral (VISlm - left) to the most dorsal (VISam - right) areas.</p></caption><graphic xlink:href="448989v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s3b" hwp:id="sec-8" hwp:rev-id="xref-sec-8-1 xref-sec-8-2"><label>3.2</label><title hwp:id="title-11">Analysis techniques</title><sec id="s3b1" hwp:id="sec-9"><title hwp:id="title-12">Representational Similarity Analysis (RSA)</title><p hwp:id="p-16">To measure the representational similarities between real brains and ANNs we use the RSA method. RSA has been commonly used in both the neuroscience [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>, <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>] and deep learning literature [<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>, <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>]. The details of RSA can be found in those citations, but we will summarize it briefly here. First, we create response matrices <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="448989v3_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> for every brain area and every layer of our ANNs, where <italic toggle="yes">N</italic> is the number of neurons and <italic toggle="yes">M</italic> is the number of video blocks, with each block comprising 15 frames of the video. Element <italic toggle="yes">ij</italic> of the response matrix represents the response of the <italic toggle="yes">i<sup>th</sup></italic> neuron to the <italic toggle="yes">j<sup>th</sup></italic> video sequence. We then use Pearson correlation to calculate the similarity of every pair of columns (<italic toggle="yes">e.g. k<sup>th</sup></italic> and <italic toggle="yes">l<sup>th</sup></italic> columns) in matrix <italic toggle="yes">R</italic>, and form the <italic toggle="yes">M</italic> by <italic toggle="yes">M</italic> Representation Similarity Matrix <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="448989v3_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>, in which every element (<italic toggle="yes">kl</italic>) quantifies the similarity of the responses to the <italic toggle="yes">k<sup>th</sup></italic> and <italic toggle="yes">l<sup>th</sup></italic> videos blocks. Thus, the <italic toggle="yes">RSM</italic>s describe the representation space in a network, be it a brain area or an ANN layer. Given two <italic toggle="yes">RSM</italic>s, we then use Kendall’s τ between the vectorized <italic toggle="yes">RSM</italic>s to quantify the similarity between the two representations. It is important to note that measurement noise can potentially induce bias in the RSM estimations, but since the variance of measurement noise is not expected to be different across different video blocks, using Kendall’s <italic toggle="yes">τ</italic> rank correlation should cancel the bias in our RSM estimations (see [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>] for more details). As an additional sanity check, we use RSA to compare the representations between mice. If RSA is identifying salient representational geometries, then the <italic toggle="yes">RSM</italic>s between different areas should be lower than the RSMs for the same areas. Indeed, as shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1b</xref>, the representational similarity is highest for the same regions across animals (i.e. the diagonal values are larger). These diagonal values also represent the noise ceiling for these areas (see <xref ref-type="sec" rid="s7a1" hwp:id="xref-sec-30-1" hwp:rel-id="sec-30">section A.1</xref>). Throughout the paper (with the exception of <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1b</xref>), <italic toggle="yes">RSM</italic> similarities are reported as the percentage of the noise ceiling.</p></sec><sec id="s3b2" hwp:id="sec-10"><title hwp:id="title-13">Identification of brain regions in the hierarchy</title><p hwp:id="p-17">The hierarchical organization of mouse visual areas can be inferred based on the anatomical and functional properties of each brain region. We adopted an approximation of hierarchical indices as reported in [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">23</xref>]. The relative hierarchical placing of the visual areas included in this study are shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1c</xref>.</p></sec><sec id="s3b3" hwp:id="sec-11"><title hwp:id="title-14">Identification of brain regions into ventral and dorsal streams</title><p hwp:id="p-18">We group the brain regions into two sets, i.e. ventral and dorsal areas. Although the ventral/dorsal specialization of visual pathways in mice is not as clear and well understood as it is in primates, many anatomical and physiological studies do suggest that mice also possess such specialized pathways [<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>, <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-2" hwp:rel-id="ref-58">58</xref>, <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>, <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>]. According to previous anatomical and physiological studies [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref>, <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-3" hwp:rel-id="ref-58">58</xref>], VISlm and VISam can be considered as the most ventral and dorsal areas of mouse visual cortex, respectively. We then use the similarity of representations between other areas and VISlm and VISam to estimate a ventral score <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="448989v3_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> and a dorsal score <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="448989v3_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> for each area. <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="448989v3_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> is the representational similarity between area <italic toggle="yes">A</italic> and VISam, and <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="448989v3_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> is the representational similarity between area <italic toggle="yes">A</italic> and VISlm. The D-score and V-score values for the five visual areas are plotted in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1d</xref>. Based on the D-score and V-score values, we grouped VISlm, VISp, and VISpm as more ventral and VISal, and VISam as more dorsal in our analysis. This grouping is in keeping with a recent study that showed VISam and VISal are the first and second most responsive areas to motion stimuli: an important characteristic of dorsal areas [<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref>].</p></sec></sec><sec id="s3c" hwp:id="sec-12" hwp:rev-id="xref-sec-12-1 xref-sec-12-2 xref-sec-12-3 xref-sec-12-4 xref-sec-12-5 xref-sec-12-6 xref-sec-12-7"><label>3.3</label><title hwp:id="title-15">Model</title><sec id="s3c1" hwp:id="sec-13"><title hwp:id="title-16">Self-supervised predictive learning</title><p hwp:id="p-19">We used Contrastive Predictive Coding (CPC) for self-supervised learning, which was developed for modeling sequential data, including video datasets [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">45</xref>]. The loss function relies on predicting the future latent representations of a video sequence, given its present and past representations. See <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure S1</xref> for a schematic of the model, and A.2 for more details regarding the CPC loss function.</p></sec><sec id="s3c2" hwp:id="sec-14"><title hwp:id="title-17">ANN Architecture</title><p hwp:id="p-20">All the ANN backbones used in our study are variants of the ResNet architecture, similar to the ones used in [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>]. Our ResNet architectures have either one or two pathways. The one-pathway ResNet (ResNet-1p) is a regular 3D ResNet (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2a</xref>). The two-pathway ResNet (ResNet-2p) is composed of two parallel ResNet branches, which split after a single convolutional layer, and merge after their final layers through concatenating their outputs along the channel dimension (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3a</xref>). Both pathways of ResNet-2p receive a copy of the first convolutional layer output, and each has 10 Res-blocks. To keep the total number of output channels the same in both architectures, each pathway in ResNet-2p has half the number of channels of the single pathway in ResNet-1p. For the ANNs trained on object categorization, we use ResNet-18 pretrained on the ImageNet database [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>]. Our 3D ResNet architectures are summarized in <xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table S1</xref>.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-21">Representational Similarity Analysis between all the visual areas and the ANN trained with CPC. (a) The schematic of the ANN architecture with one pathway (ResNet-1p) used as the backbone of CPC. (b) Representational similarity between all the layers of the ANN with one pathways (trained with CPC) and the ventral (top: VISlm, VISp, VISpm) and the dorsal (bottom: VISal, VISam) areas. (c) The maximum representational similarity values between the ANN and the ventral and dorsal areas. (d) Hierarchy index of the ventral areas based on their fit to the ANN. Error bars represent bootstrapped standard deviation.</p></caption><graphic xlink:href="448989v3_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9 xref-fig-3-10 xref-fig-3-11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-22">Representational Similarity Analysis between all the visual areas and the ANN trained with CPC. (a) The schematic of the ANN architecture with two pathways (ResNet-2p) used as the backbone of CPC. (b) Representational similarity between all the layers of the ANN with two pathways (trained with CPC) and the ventral (top: VISlm, VISp, VISpm) and the dorsal (bottom: VISal, VISam) areas. (c) The maximum representational similarity values between the ANN and the ventral and dorsal areas. CPC (1p) and CPC (2p) are ResNet-1p and ResNet-2p, respectively, both trained with CPC loss function.(d) Hierarchy index of the ventral (left; in red) and dorsal (right; in blue) areas based on their fit to the ANN. Error bars represent bootstrapped standard deviation.</p></caption><graphic xlink:href="448989v3_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec><sec id="s3c3" hwp:id="sec-15"><title hwp:id="title-18">Baseline models</title><p hwp:id="p-23">In terms of alignment with brain representations of video sequences, we compare the CPC trained ANNs with four other models: (1) a simple model based on Gabor filters, (2) a randomized deep ANN, (3) a ResNet-18 trained on ImageNet, and (4) 3D ResNets trained on action recognition in a supervised manner. See <xref ref-type="sec" rid="s7a3" hwp:id="xref-sec-32-1" hwp:rel-id="sec-32">section A.3</xref> of supplementary materials for more details.</p></sec><sec id="s3c4" hwp:id="sec-16"><title hwp:id="title-19">Training</title><p hwp:id="p-24">We use the backpropagation algorithm and Adam optimizer. CPC is trained with a batch size of 40 samples, a learning rate of 10<sup>−3</sup>, and 100 epochs. Supervised action recognition is trained with a batch size of 256 samples, a learning rate of 5 × 10<sup>−4</sup>, and 300 epochs. All the models are implemented with PyTorch 1.7 and trained on RTX8000 NVIDIA GPUs.</p></sec><sec id="s3c5" hwp:id="sec-17"><title hwp:id="title-20">Downstream tasks</title><p hwp:id="p-25">In addition to comparisons with dorsal and ventral representations in mouse brain, we examine the two pathways of our trained ResNet-2p on two downstream tasks: object categorization and motion discrimination, which are supported in the real brain by the ventral and dorsal pathways, respectively. See <xref ref-type="sec" rid="s7a5" hwp:id="xref-sec-34-1" hwp:rel-id="sec-34">section A.5</xref> of supplementary materials for more details.</p></sec></sec></sec><sec id="s4" hwp:id="sec-18"><label>4</label><title hwp:id="title-21">Results</title><sec id="s4a" hwp:id="sec-19"><label>4.1</label><title hwp:id="title-22">CPC with a single pathway architecture produces better matches to mouse ventral stream</title><p hwp:id="p-26">We first compare the representations learned with CPC on a single pathway architecture (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2a</xref>) with the three ventral areas of mouse visual cortex (VISlm, VISp, and VISpm; based on D-scores/V-scores in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Figure 1d</xref>). In <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2b</xref> (top), the similarity of RSMs between different layers of the ANN and these three areas are shown. Comparing the maximum representational similarities between models in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2c</xref>, we can see that, for the three ventral areas, CPC shows a higher maximum similarity compared to the other models. Compared to the baseline models (untrained ANN and Gabors), the ANN trained with object categorization has higher representational similarity to VISlm and VISp, the two most ventral areas (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Figure 1d</xref>), which is consistent with the suggested role of ventral areas in form and shape representation [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>, <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">17</xref>]. This is in contrast with a previous study [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>] that could not find any significant difference between ANNs trained on object categorization and untrained ANNs in modeling mouse visual cortex. There are several sources of variability that could explain this contradiction (e.g. architecture, datasets, etc.), but an important possibility is that the two studies used different visual stimuli, namely, natural videos in our study vs static natural images in [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>]. Natural videos are better suited for eliciting sufficiently strong responses that can distinguish between the representations of the object categorization-trained and randomly initialized ANNs [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-5" hwp:rel-id="ref-9">9</xref>].</p><p hwp:id="p-27">As noted, and as can be seen in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Figure 2b</xref>, the maximum similarity happens in different hierarchical levels of the CPC-trained ANN for each area. We quantify the hierarchy index for every area by dividing the layer number with the highest similarity by the maximum number of layers. Based on this measure, the areas at the very top and bottom of the visual hierarchy have a hierarchy index of 1 and 0, respectively. The hierarchy index for the three areas are shown in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Figure 2d</xref>. In accordance with the anatomical and functional data (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Figure 1c</xref>), VISp has a lower hierarchical index (0.41 ± 0.15) than VISlm (0.61 ± 0.08) and VISpm (0.55 ± 0.08). It should be noted, that despite the fact that anatomical/functional hierarchy indices position VISpm higher in the hierarchy than VISlm, our model-based measure of hierarchy places the two areas at around a similar level, with VISlm having a marginally higher value than VISpm. We speculate that this may be a result of the fact that VISpm has noisier representations, and therefore a lower noise ceiling (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1b</xref>). This may be due to VISpm actually being more multi-modal than purely visual.</p></sec><sec id="s4b" hwp:id="sec-20"><label>4.2</label><title hwp:id="title-23">CPC on a single pathway does not match mouse dorsal stream</title><p hwp:id="p-28">We compare the similarity of RSMs between ResNet-1p trained with CPC (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2a</xref>) and the dorsal areas (VISam and VISal; based on D-scores/V-scores in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Figure 1d</xref>). As shown in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Figure 2b</xref> (bottom), the ANN trained with CPC just passes the pixel-level representations in its early layers (the first gray circle in the plots), and then the similarity values continuously decrease for deeper layers. Notably, for the most dorsal area (VISam), the maximum similarity of RSMs does not go above the untrained model (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Figure 2c</xref>). For VISal, even though CPC shows some improvement compared to an untrained ANN, the performance is much lower than for the ventral areas, as shown in the previous section (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2c</xref>). These findings show that the representations learned by CPC and the ResNet-1p architecture are more ventral-like, and do not easily explain dorsal area representations (see <xref ref-type="sec" rid="s7e" hwp:id="xref-sec-39-1" hwp:rel-id="sec-39">supplementary section E</xref>). Similarly, an object categorization trained ANN has a very low similarity with the dorsal areas (even lower than the untrained ANN for VISam) indicating that object categorization cannot be considered as an appropriate loss function for the dorsal pathway, which is consistent with our current understanding in neuroscience.</p></sec><sec id="s4c" hwp:id="sec-21"><label>4.3</label><title hwp:id="title-24">An architecture with parallel pathways trained with CPC can model both ventral and dorsal areas</title><p hwp:id="p-29">CPC’s tendency to learn ventral-like representations, as seen in the previous section, could be due to a limitation of the backbone architecture. Predicting the next frame of a video with a contrastive loss requires learning invariances to every kind of transformation (or augmentation) that could happen from one frame to the next. For example, representing objects’ shapes requires invariance to objects’ motion, and representing objects’ motion requires invariance to objects’ shape. As suggested in [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref>], these two types of invariances often underlie the distinction between the dorsal and ventral representations in the brain. Therefore, one possibility is that ventral and dorsal-like representations compete for resources in the network, and ultimately, the ventral-like representations win out, possibly because they provide a greater overall reduction in loss. If this hypothesis is true, then a network with two separate pathways may be able to reduce the competition by assigning one pathway to be more ventral-like, and one more dorsal-like.</p><p hwp:id="p-30">To test this hypothesis, we use the simplest extension of ResNet-1p: ResNet-2p, a ResNet architecture that is composed of two identical, parallel ResNets which split after the first layer and merge after their final layers (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3a</xref>; see also <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-1" hwp:rel-id="sec-12">section 3.3</xref>). Each pathway of ResNet-2p has half the number of channels of ResNet-1p, keeping the total number of channels per layer equal between the two architectures. We choose this ResNet-2p architecture as it shares all of the features of ResNet-1p, but with two separate pathways that could potentially be assigned specialized functions. We then check whether ResNet-2p can learn both ventral- and dorsal-like representations. We compare the representations of all the layers of the two pathways in ResNet-2p (blue and red in the schematic in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3a</xref>) with all the visual areas. The results show that the representations along one of the pathways (the blue pathway in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3a-b</xref>, top) are more ventral-like, and the representations along the other pathway (the red pathway in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3a-b</xref>, bottom) are more dorsal-like. Therefore, the two pathways together can model both ventral and dorsal areas of mouse visual cortex. Compared to an untrained ANN, both ResNet-1p and ResNet-2p achieve high RSM similarity values for ventral areas (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 3c</xref>), with ResNet-2p showing a slight decrease in VISlm and VISpm compared to ResNet-1p. However, unlike ResNet-1p, which fails to model the dorsal areas, ResNet-2p outperforms ResNet-1p and the untrained ANN for area VISam by a wide margin (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Fig. 3c</xref>). For VISal, maximum RSM similarity values for ResNet-1p and ResNet-2p are around the same level (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Fig. 3c</xref>), though the similarity values of the red pathway representations do not drop as much throughout the network as the blue pathway representations do, indicating the general similarity of the red pathway to the dorsal areas. Examination of the RSM similarity between the two pathways shows that the representational geometries are quite different (<xref ref-type="fig" rid="figS2" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. S2</xref>). Moreover, when we compare the RSMs of the two pathways with those from ResNet-1p, we can see that ResNet-1p has representations that are a better match to the ventral-like pathway from ResNet-2p (<xref ref-type="fig" rid="figS3" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Fig. S3</xref>). This supports the idea that, in the single pathway model, there is a competition between the two forms of representation that favours ventral-like functions, and which leads to specialized functions in the two-pathway architecture. Using architectures with more than two parallel pathways also does not improve representational similarities (<xref ref-type="fig" rid="figS5" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Fig. S5</xref>).</p><p hwp:id="p-31">The hierarchy index values for the ventral and dorsal areas are shown in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Figure 3d</xref>. The hierarchy index for every area is calculated based on the model pathway that aligns best with that area in terms of representation similarity (blue pathway for VISlm, VISp, VISpm, and red pathway for VISam, VISal). The hierarchy indices are shown separately for the ventral and dorsal pathways. Similar to the results with ResNet-1p for the ventral pathway, VISp has a lower hierarchy index (0.25 ± 0) than VISlm (0.31 ± 0.05) and VISpm (0.29 ± 0.07). For the dorsal pathway, VISam has a larger hierarchy index than VISal (VISam: 0.82 ± 0.16 vs. VISal: 0.19 ± 0.15) which is also consistent with the anatomical/functional hierarchy index (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Fig. 1c</xref>).</p><p hwp:id="p-32">In terms of the predictive loss function, the performances of ResNet-1p and ResNet-2p are not significantly different (top-3 accuracy for ResNet-1p: 94.64 (0.68) and ResNet-2p: 93.472 (0.83)). However, it is worth noting that ResNet-2p has, in total, fewer parameters than ResNet-1p (ResNet-1p: 435k vs ResNet-2p: 285k). Therefore, considering the lower capacity of ResNet-2p and its similar predictive performance to ResNet-1p, we can conclude that the inductive bias of parallel architecture could help in predictive processing.</p></sec><sec id="s4d" hwp:id="sec-22" hwp:rev-id="xref-sec-22-1 xref-sec-22-2"><label>4.4</label><title hwp:id="title-25">Supervised learning of action recognition with parallel-pathways is not sufficient for dorsal match</title><p hwp:id="p-33">The inductive bias of an architecture with parallel pathways combined with the spatiotemporal dynamics of the video data could be enough to trigger the emergence of both ventral and dorsal-like representations, as defined in the previous section, regardless of the loss function used. To understand the role of the loss function, we trained the same ResNet-1p and ResNet-2p backbones with a supervised action classification loss on the UCF101 dataset (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4a</xref>). Based on the representation similarity plots in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4b</xref>, both blue and red pathways seem to learn similar representations that are more ventral-like (also see <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig. S2</xref>). The maximum RSM similarity values in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4c</xref> show that both ResNet-1p and ResNet-2p reach similarity values higher than the untrained ANN for ventral-like areas, but for the dorsal areas, and specifically VISam, neither architecture achieves good representational similarity. The low performance of the action classification models here could not be due to the low spatial resolution of the training videos as our comparisons with ANNs pretrianed with higher spatial resolution videos also show similar results (see <xref ref-type="sec" rid="s7f" hwp:id="xref-sec-40-1" hwp:rel-id="sec-40">supplementary section F</xref>).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-34">Representational Similarity Analysis between all the visual areas and the ANN trained with supervised action recognition loss function. (a) Representational similarity between all the layers of the ANN with two pathways (trained with action recognition objective) and the ventral-like (top: VISlm, VISp, VISpm) and the dorsal-like (bottom: VISal, VISam) areas. (b) The maximum representational similarity values between the ANN and the ventral and dorsal areas. AR (1p) and AR (2p) are ResNet-1p and ResNet-2p, respectively, both trained with action recognition loss function. (c) Hierarchy index of the ventral (left; in red) and dorsal (right; in blue) areas based on their fit to the ANN. Error bars represent bootstrapped standard deviation.</p></caption><graphic xlink:href="448989v3_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-35">The hierarchy indices calculated using the ANN trained with an action classification objective (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4d</xref>), reproduce the CPC results (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-10" hwp:rel-id="F3">Fig. 3d</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-11" hwp:rel-id="F2">2d</xref>) for ventral-like areas, and roughly match the anatomical/functional hierarchy index. However, the action classification model fails to predict the hierarchical organization of the dorsal areas, which is to be expected given the model’s poor alignment with these areas. Overall, these findings demonstrate the importance of the CPC loss function for learning both ventral and dorsal-like representations across the ResNet-2p architecture.</p></sec><sec id="s4e" hwp:id="sec-23"><label>4.5</label><title hwp:id="title-26">Functional specialization of the ventral and dorsal-like pathways in a CPC trained model</title><p hwp:id="p-36">The ventral pathway is responsible for object and scene-based tasks [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>], while the dorsal pathway is responsible for motion-based tasks [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>, <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>, <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>, <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-2" hwp:rel-id="ref-54">54</xref>]. Based on our knowledge of the functional specialization of the two pathways in the real brain, we run a linear evaluation on the two pathways of ResNet-2p trained with CPC on two downstream tasks: (1) object categorization (CIFAR10 dataset) and (2) motion discrimination with random dot kinematograms (RDKs) (see <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 5a</xref>). In vision neuroscience, RDKs have been commonly used to characterise motion representation in the dorsal pathway [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]. We use this stimulus to evaluate the CPC-trained ResNet-2p (the red pathway in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-11" hwp:rel-id="F3">Fig. 3</xref>) on motion direction discrimination (four directions: up, down, left, right). <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5b</xref> shows the results for the two pathways, as well as for an untrained ResNet. As expected based on the comparisons with mouse visual areas, the ventral-like pathway is better in object categorization, while the dorsal-like pathway outperforms the ventral-like pathway in motion discrimination. Therefore, in addition to their better fits to neural data, the two pathways are indeed more ventral-like and dorsal-like according to their ability to support downstream tasks.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-37">Random dots motion discrimination task. (a) Schematics of the RDK task. (b) The performance of the two pathways and an untrained ResNet on the object categorization (CIFAR-10) and motion discrimination (RDK) tasks. The black dashed lines show the chance levels for the two tasks. (c) The accuracy of the dorsal-like and the ventral-like paths for different levels of the random dots coherence. (d) The accuracy of the two pathways for different number of frames of the random dots stimulus.</p></caption><graphic xlink:href="448989v3_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-38">As previously stated, decreasing the dots coherence (increasing spatial noise) makes the randomdots task more difficult. In real brains, dorsal areas can average out noise and extract motion signals by integrating motion over both time and space [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>, <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref>]. It is thought that this is key to the importance of dorsal areas for RDK tasks. We measured ResNet-2p ventral-like and dorsallike pathway performances for different coherence levels of random-dots (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig. 5c</xref>). Ventral-like performance was dramatically reduced when the coherence was lowered from 100% to 50%. In contrast, the decrease in dorsal-like pathway performance was much smaller, demonstrating the dorsal-like pathway’s ability to spatially integrate motion, which is consistent with our expectations of dorsal areas. Indeed, the dorsal-like pathway can still achieve just under 40% accuracy even with 20% coherence. We also measured ventral-like and dorsal-like pathway performances for different numbers of frames of the random-dots stimuli at 50% coherence (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig. 5d</xref>). The dorsal-like pathway’s performance improved significantly when the number of frames was increased, illustrating the ability of the dorsal-like pathway to integrate motion information over time. On the other hand, increasing the number of frames did not increase ventral-like pathway accuracy, but rather reduced it to around chance level. This shows that unlike the dorsal-like pathway, the ventral-like pathway of ResNet-2p does not integrate motion information over time.</p></sec></sec><sec id="s5" hwp:id="sec-24"><label>5</label><title hwp:id="title-27">Discussion</title><p hwp:id="p-39">In this paper, we showed that self-supervised learning with CPC can produce representations that are more analogous to mouse visual cortex than either simple models or ANNs trained in a supervised manner. Furthermore, we showed that CPC applied to an architecture that has two parallel pathways can model both the ventral and dorsal areas of mouse visual cortex. The downstream tasks of object recognition and motion discrimination also support the ventral-like and dorsal-like representations of the two pathways in the model. Our experiments with supervised training on action classification indicated that the two-pathway architecture and video dataset are necessary but not sufficient for learning both types of representations, showing an interaction between the self-supervised objective function and the architecture. This finding shows that self-supervised predictive learning is a required component of our model for obtaining both ventral- and dorsal-like representations. Our observation that supervised action classification cannot generate dorsal representations is in contradiction with a previous fMRI study [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">21</xref>]. The different results of the two studies boil down to the different data modalities used in the two studies: two-photon calcium imaging in our study and fMRI in [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-4" hwp:rel-id="ref-21">21</xref>]. Capturing the responses of the dorsal areas elicited by natural videos with high temporal dynamics requires neuronal recordings with high temporal sampling rate. Therefore, we hypothesize that some aspects of dorsal representations of movement would not be reflected in the fMRI data, which could bias the fMRI-based analysis toward more static and lower temporal frequency features of the stimulus and neuronal responses (e.g. very slow-varying features; [<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref>]).</p><p hwp:id="p-40">Even though our results demonstrated the importance of self-supervised predictive learning for generating ventral- and dorsal-like representations, it is important to note that our conclusions are limited to the supervised loss functions that we included in our comparisons (i.e. object categorization and action recognition). We acknowledge that a different combination of more ecologically relevant supervised loss functions might be sufficient for learning ventral and dorsal representations in ANNs.</p><p hwp:id="p-41">Learning representations of input data (images, videos, etc.) that are invariant to certain data augmen-tations (e.g. rotation, cropping, etc.) is a common goal of modern self-supervised learning methods. In models such as SimCLR [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>] and BYOL [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>], the augmentations are engineered for learning the most appropriate representations for downstream tasks. In CPC, however, the augmentations are inherent to the data being used. As noted above, predicting the next frame in a movie requires two different invariances: (1) invariance to motion, but selective for shape, and (2) invariance to shape, but selective for motion. Our results suggest that these two types of invariances are mutually exclusive, which can explain both the need for two separate pathways to get good matches to both ventral and dorsal areas and the inverse relationship between ventral-likeness and dorsal-likeness of the learned representation (see <xref ref-type="sec" rid="s7d" hwp:id="xref-sec-38-1" hwp:rel-id="sec-38">section D</xref> and <xref ref-type="fig" rid="figS4" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Fig. S4</xref> in supplementary materials). Thus, our results suggest that the functional specialization observed in the mammalian brain may be a natural consequence of a predictive objective applied to an architecture with two distinct pathways.</p></sec><sec id="s6" hwp:id="sec-25" hwp:rev-id="xref-sec-25-1"><label>6</label><title hwp:id="title-28">Limitations</title><p hwp:id="p-42">One limitation of this work is the lack of comparisons with ANNs that are trained with other predictive loss functions, such as PredNet [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">38</xref>]. Other self-supervised video-based learning models (for example, see [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]) that do not optimize a predictive loss function could also be compared with CPC in terms of matching the representations of mouse visual cortex. Another limitation concerns the backbone architectures that we used in this study. Different parameters of the architectures (such as the number of residual blocks, the number of layers before the split and after the merger of the two pathways in in ResNet-2p, etc.) could be searched more thoroughly to determine the optimal setting for modeling mouse visual cortex. Furthermore, training ResNet-2p with CPC on a synthetic video dataset in which the motion and shape contents of the videos can be controlled could demonstrate more directly that the two pathways of ResNet-2p learn motion-invariant shape selectivity and shape-invariant motion selectivity, respectively.</p></sec></body><back><sec hwp:id="sec-26"><title hwp:id="title-29">Broader Impact</title><p hwp:id="p-43">ANNs can serve as a framework for understanding brains [<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>], as demonstrated here. This understanding would be based on finding the loss functions, architecture, and learning rules that best capture brain representations. Technologies that directly or indirectly interface with the brain, such as brain machine interfaces, can benefit from an <italic toggle="yes">in silico</italic> model of the brain. ANNs, if being used as such models, can facilitate designing and optimizing these technologies. However, the downside is that the ANN models of the brain are prone to the same limitations encountered by other ANNs, such as adversarial attack or implicit bias. These limitations can potentially leak into the applications in which these models would be used with human subjects.</p></sec><ack hwp:id="ack-1"><title hwp:id="title-30">Acknowledgments and Disclosure of Funding</title><p hwp:id="p-44">We thank Iris Jianghong Shi, Michael Buice, Stefan Mihalas, Eric Shea-Brown, and Bryan Tripp for helpful discussions. We also thank Pouya Bashivan and Alex Hernandez-Garcia for their suggestions on the manuscript. This work was supported by a NSERC (Discovery Grant: RGPIN-2020-05105; Discovery Accelerator Supplement: RGPAS-2020-00031), Healthy Brains, Healthy Lives (New Investigator Award: 2b-NISU-8; Innovative Ideas Grant: 1c-II-15), and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship). CCP was funded by a CIHR grant (MOP-115178). This work was also funded by the Canada First Research Excellence Fund (CFREF Competition 2, 2015-2016) awarded to the Healthy Brains, Healthy Lives initiative at McGill University, through the Helmholtz International BigBrain Analytics and Learning Laboratory (HIBALL).</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-31">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Bashivan Pouya"><given-names>Pouya</given-names> <surname>Bashivan</surname></string-name>, <string-name name-style="western" hwp:sortable="Kar Kohitij"><given-names>Kohitij</given-names> <surname>Kar</surname></string-name>, and <string-name name-style="western" hwp:sortable="DiCarlo James J"><given-names>James J</given-names> <surname>DiCarlo</surname></string-name>. <article-title hwp:id="article-title-2">Neural population control via deep image synthesis</article-title>. <source hwp:id="source-1">Science</source>, <volume>364</volume>(<issue>6439</issue>), <year>2019</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Braun Doris I"><given-names>Doris I</given-names> <surname>Braun</surname></string-name>, <string-name name-style="western" hwp:sortable="Boman Duane K"><given-names>Duane K</given-names> <surname>Boman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hotson John R"><given-names>John R</given-names> <surname>Hotson</surname></string-name>. <article-title hwp:id="article-title-3">Anticipatory smooth eye movements and predictive pursuit after unilateral lesions in human brain</article-title>. <source hwp:id="source-2">Experimental brain research</source>, <volume>110</volume>(<issue>1</issue>):<fpage>111</fpage>–<lpage>116</lpage>, <year>1996</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Britten Kenneth H"><given-names>Kenneth H</given-names> <surname>Britten</surname></string-name>, <string-name name-style="western" hwp:sortable="Newsome William T"><given-names>William T</given-names> <surname>Newsome</surname></string-name>, <string-name name-style="western" hwp:sortable="Shadlen Michael N"><given-names>Michael N</given-names> <surname>Shadlen</surname></string-name>, <string-name name-style="western" hwp:sortable="Celebrini Simona"><given-names>Simona</given-names> <surname>Celebrini</surname></string-name>, <string-name name-style="western" hwp:sortable="Movshon J Anthony"><given-names>J Anthony</given-names> <surname>Movshon</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-4">A relationship between behavioral choice and the visual responses of neurons in macaque mt</article-title>. <source hwp:id="source-3">Visual neuroscience</source>, <volume>13</volume>(<issue>1</issue>):<fpage>87</fpage>–<lpage>100</lpage>, <year>1996</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Britten Kenneth H"><given-names>Kenneth H</given-names> <surname>Britten</surname></string-name>, <string-name name-style="western" hwp:sortable="Shadlen Michael N"><given-names>Michael N</given-names> <surname>Shadlen</surname></string-name>, <string-name name-style="western" hwp:sortable="Newsome William T"><given-names>William T</given-names> <surname>Newsome</surname></string-name>, and <string-name name-style="western" hwp:sortable="Movshon J Anthony"><given-names>J Anthony</given-names> <surname>Movshon</surname></string-name>. <article-title hwp:id="article-title-5">The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title>. <source hwp:id="source-4">Journal of Neuroscience</source>, <volume>12</volume>(<issue>12</issue>):<fpage>4745</fpage>–<lpage>4765</lpage>, <year>1992</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2"><label>[5]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Cadena Santiago A"><given-names>Santiago A</given-names> <surname>Cadena</surname></string-name>, <string-name name-style="western" hwp:sortable="Sinz Fabian H"><given-names>Fabian H</given-names> <surname>Sinz</surname></string-name>, <string-name name-style="western" hwp:sortable="Muhammad Taliah"><given-names>Taliah</given-names> <surname>Muhammad</surname></string-name>, <string-name name-style="western" hwp:sortable="Froudarakis Emmanouil"><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name name-style="western" hwp:sortable="Cobos Erick"><given-names>Erick</given-names> <surname>Cobos</surname></string-name>, <string-name name-style="western" hwp:sortable="Walker Edgar Y"><given-names>Edgar Y</given-names> <surname>Walker</surname></string-name>, <string-name name-style="western" hwp:sortable="Reimer Jake"><given-names>Jake</given-names> <surname>Reimer</surname></string-name>, <string-name name-style="western" hwp:sortable="Bethge Matthias"><given-names>Matthias</given-names> <surname>Bethge</surname></string-name>, <string-name name-style="western" hwp:sortable="Tolias Andreas"><given-names>Andreas</given-names> <surname>Tolias</surname></string-name>, and <string-name name-style="western" hwp:sortable="Ecker Alexander S"><given-names>Alexander S</given-names> <surname>Ecker</surname></string-name>. <article-title hwp:id="article-title-6">How well do deep neural networks trained on object recognition characterize the mouse visual system?</article-title> <year>2019</year>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Cadieu Charles F"><given-names>Charles F</given-names> <surname>Cadieu</surname></string-name> and <string-name name-style="western" hwp:sortable="Olshausen Bruno A"><given-names>Bruno A</given-names> <surname>Olshausen</surname></string-name>. <article-title hwp:id="article-title-7">Learning intermediate-level representations of form and motion from natural movies</article-title>. <source hwp:id="source-5">Neural computation</source>, <volume>24</volume>(<issue>4</issue>):<fpage>827</fpage>–<lpage>866</lpage>, <year>2012</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><label>[7]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.18.448989v3.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Chen Ting"><given-names>Ting</given-names> <surname>Chen</surname></string-name>, <string-name name-style="western" hwp:sortable="Kornblith Simon"><given-names>Simon</given-names> <surname>Kornblith</surname></string-name>, <string-name name-style="western" hwp:sortable="Norouzi Mohammad"><given-names>Mohammad</given-names> <surname>Norouzi</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hinton Geoffrey"><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>. <article-title hwp:id="article-title-8">A simple framework for contrastive learning of visual representations</article-title>. In <conf-name>International conference on machine learning</conf-name>, pages <fpage>1597</fpage>–<lpage>1607</lpage>. PMLR, <year>2020</year>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Clark Andy"><given-names>Andy</given-names> <surname>Clark</surname></string-name>. <article-title hwp:id="article-title-9">Whatever next? predictive brains, situated agents, and the future of cognitive science</article-title>. <source hwp:id="source-6">Behavioral and brain sciences</source>, <volume>36</volume>(<issue>3</issue>):<fpage>181</fpage>–<lpage>204</lpage>, <year>2013</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4 xref-ref-9-5"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="de Vries Saskia EJ"><given-names>Saskia EJ</given-names> <surname>de Vries</surname></string-name>, <string-name name-style="western" hwp:sortable="Lecoq Jerome A"><given-names>Jerome A</given-names> <surname>Lecoq</surname></string-name>, <string-name name-style="western" hwp:sortable="Buice Michael A"><given-names>Michael A</given-names> <surname>Buice</surname></string-name>, <string-name name-style="western" hwp:sortable="Groblewski Peter A"><given-names>Peter A</given-names> <surname>Groblewski</surname></string-name>, <string-name name-style="western" hwp:sortable="Ocker Gabriel K"><given-names>Gabriel K</given-names> <surname>Ocker</surname></string-name>, <string-name name-style="western" hwp:sortable="Oliver Michael"><given-names>Michael</given-names> <surname>Oliver</surname></string-name>, <string-name name-style="western" hwp:sortable="Feng David"><given-names>David</given-names> <surname>Feng</surname></string-name>, <string-name name-style="western" hwp:sortable="Cain Nicholas"><given-names>Nicholas</given-names> <surname>Cain</surname></string-name>, <string-name name-style="western" hwp:sortable="Ledochowitsch Peter"><given-names>Peter</given-names> <surname>Ledochowitsch</surname></string-name>, <string-name name-style="western" hwp:sortable="Millman Daniel"><given-names>Daniel</given-names> <surname>Millman</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-10">A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title>. <source hwp:id="source-7">Nature neuroscience</source>, <volume>23</volume>(<issue>1</issue>):<fpage>138</fpage>–<lpage>151</lpage>, <year>2020</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Denys Katrien"><given-names>Katrien</given-names> <surname>Denys</surname></string-name>, <string-name name-style="western" hwp:sortable="Vanduffel Wim"><given-names>Wim</given-names> <surname>Vanduffel</surname></string-name>, <string-name name-style="western" hwp:sortable="Fize Denis"><given-names>Denis</given-names> <surname>Fize</surname></string-name>, <string-name name-style="western" hwp:sortable="Nelissen Koen"><given-names>Koen</given-names> <surname>Nelissen</surname></string-name>, <string-name name-style="western" hwp:sortable="Peuskens Hendrik"><given-names>Hendrik</given-names> <surname>Peuskens</surname></string-name>, <string-name name-style="western" hwp:sortable="Van Essen David"><given-names>David</given-names> <surname>Van Essen</surname></string-name>, and <string-name name-style="western" hwp:sortable="Orban Guy A"><given-names>Guy A</given-names> <surname>Orban</surname></string-name>. <article-title hwp:id="article-title-11">The processing of visual shape in the cerebral cortex of human and nonhuman primates: a functional magnetic resonance imaging study</article-title>. <source hwp:id="source-8">Journal of Neuroscience</source>, <volume>24</volume>(<issue>10</issue>):<fpage>2551</fpage>–<lpage>2565</lpage>, <year>2004</year>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Devereux Barry J"><given-names>Barry J</given-names> <surname>Devereux</surname></string-name>, <string-name name-style="western" hwp:sortable="Clarke Alex"><given-names>Alex</given-names> <surname>Clarke</surname></string-name>, <string-name name-style="western" hwp:sortable="Marouchos Andreas"><given-names>Andreas</given-names> <surname>Marouchos</surname></string-name>, and <string-name name-style="western" hwp:sortable="Tyler Lorraine K"><given-names>Lorraine K</given-names> <surname>Tyler</surname></string-name>. <article-title hwp:id="article-title-12">Representational similarity analysis reveals commonalities and differences in the semantic processing of words and objects</article-title>. <source hwp:id="source-9">Journal of Neuroscience</source>, <volume>33</volume>(<issue>48</issue>):<fpage>18906</fpage>–<lpage>18916</lpage>, <year>2013</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Diedrichsen Jörn"><given-names>Jörn</given-names> <surname>Diedrichsen</surname></string-name>, <string-name name-style="western" hwp:sortable="Berlot Eva"><given-names>Eva</given-names> <surname>Berlot</surname></string-name>, <string-name name-style="western" hwp:sortable="Mur Marieke"><given-names>Marieke</given-names> <surname>Mur</surname></string-name>, <string-name name-style="western" hwp:sortable="Schütt Heiko H"><given-names>Heiko H</given-names> <surname>Schütt</surname></string-name>, <string-name name-style="western" hwp:sortable="Shahbazi Mahdiyar"><given-names>Mahdiyar</given-names> <surname>Shahbazi</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kriegeskorte Nikolaus"><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <article-title hwp:id="article-title-13">Comparing representational geometries using whitened unbiased-distance-matrix similarity</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">2007.02789</pub-id>, <year>2020</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>[13]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.18.448989v3.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Feichtenhofer Christoph"><given-names>Christoph</given-names> <surname>Feichtenhofer</surname></string-name>, <string-name name-style="western" hwp:sortable="Fan Haoqi"><given-names>Haoqi</given-names> <surname>Fan</surname></string-name>, <string-name name-style="western" hwp:sortable="Malik Jitendra"><given-names>Jitendra</given-names> <surname>Malik</surname></string-name>, and <string-name name-style="western" hwp:sortable="He Kaiming"><given-names>Kaiming</given-names> <surname>He</surname></string-name>. <article-title hwp:id="article-title-14">Slowfast networks for video recognition</article-title>. In <conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision</conf-name>, pages <fpage>6202</fpage>–<lpage>6211</lpage>, <year>2019</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Feichtenhofer Christoph"><given-names>Christoph</given-names> <surname>Feichtenhofer</surname></string-name>, <string-name name-style="western" hwp:sortable="Fan Haoqi"><given-names>Haoqi</given-names> <surname>Fan</surname></string-name>, <string-name name-style="western" hwp:sortable="Xiong Bo"><given-names>Bo</given-names> <surname>Xiong</surname></string-name>, <string-name name-style="western" hwp:sortable="Girshick Ross"><given-names>Ross</given-names> <surname>Girshick</surname></string-name>, and <string-name name-style="western" hwp:sortable="He Kaiming"><given-names>Kaiming</given-names> <surname>He</surname></string-name>. <article-title hwp:id="article-title-15">A large-scale study on unsupervised spatiotemporal representation learning</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">2104.14558</pub-id>, <year>2021</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.18.448989v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Felleman Daniel J"><given-names>Daniel J</given-names> <surname>Felleman</surname></string-name> and <string-name name-style="western" hwp:sortable="Van Essen David C"><given-names>David C</given-names> <surname>Van Essen</surname></string-name>. <chapter-title>Distributed hierarchical processing in the primate cerebral cortex</chapter-title>. <source hwp:id="source-10">Cerebral cortex</source> (<publisher-loc>New York, NY</publisher-loc>: <year>1991</year>), <volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>, 1991.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Fiser Aris"><given-names>Aris</given-names> <surname>Fiser</surname></string-name>, <string-name name-style="western" hwp:sortable="Mahringer David"><given-names>David</given-names> <surname>Mahringer</surname></string-name>, <string-name name-style="western" hwp:sortable="Oyibo Hassana K"><given-names>Hassana K</given-names> <surname>Oyibo</surname></string-name>, <string-name name-style="western" hwp:sortable="Petersen Anders V"><given-names>Anders V</given-names> <surname>Petersen</surname></string-name>, <string-name name-style="western" hwp:sortable="Leinweber Marcus"><given-names>Marcus</given-names> <surname>Leinweber</surname></string-name>, and <string-name name-style="western" hwp:sortable="Keller Georg B"><given-names>Georg B</given-names> <surname>Keller</surname></string-name>. <article-title hwp:id="article-title-16">Experience-dependent spatial expectations in mouse visual cortex</article-title>. <source hwp:id="source-11">Nature neuroscience</source>, <volume>19</volume>(<issue>12</issue>):<fpage>1658</fpage>–<lpage>1664</lpage>, <year>2016</year>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Froudarakis Emmanouil"><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name name-style="western" hwp:sortable="Fahey Paul G"><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name name-style="western" hwp:sortable="Reimer Jacob"><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name name-style="western" hwp:sortable="Smirnakis Stelios M"><given-names>Stelios M</given-names> <surname>Smirnakis</surname></string-name>, <string-name name-style="western" hwp:sortable="Tehovnik Edward J"><given-names>Edward J</given-names> <surname>Tehovnik</surname></string-name>, and <string-name name-style="western" hwp:sortable="Tolias Andreas S"><given-names>Andreas S</given-names> <surname>Tolias</surname></string-name>. <article-title hwp:id="article-title-17">The visual cortex in context</article-title>. <source hwp:id="source-12">Annual review of vision science</source>, <volume>5</volume>:<fpage>317</fpage>–<lpage>339</lpage>, <year>2019</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Gillon Colleen J"><given-names>Colleen J</given-names> <surname>Gillon</surname></string-name>, <string-name name-style="western" hwp:sortable="Pina Jason E"><given-names>Jason E</given-names> <surname>Pina</surname></string-name>, <string-name name-style="western" hwp:sortable="Lecoq Jérôme A"><given-names>Jérôme A</given-names> <surname>Lecoq</surname></string-name>, <string-name name-style="western" hwp:sortable="Ahmed Ruweida"><given-names>Ruweida</given-names> <surname>Ahmed</surname></string-name>, <string-name name-style="western" hwp:sortable="Billeh Yazan"><given-names>Yazan</given-names> <surname>Billeh</surname></string-name>, <string-name name-style="western" hwp:sortable="Caldejon Shiella"><given-names>Shiella</given-names> <surname>Caldejon</surname></string-name>, <string-name name-style="western" hwp:sortable="Groblewski Peter"><given-names>Peter</given-names> <surname>Groblewski</surname></string-name>, <string-name name-style="western" hwp:sortable="Henley Tim M"><given-names>Tim M</given-names> <surname>Henley</surname></string-name>, <string-name name-style="western" hwp:sortable="Lee Eric"><given-names>Eric</given-names> <surname>Lee</surname></string-name>, <string-name name-style="western" hwp:sortable="Luviano Jennifer"><given-names>Jennifer</given-names> <surname>Luviano</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-18">Learning from unexpected events in the neocortical microcircuit</article-title>. <source hwp:id="source-13">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Goodale Melvyn A"><given-names>Melvyn A</given-names> <surname>Goodale</surname></string-name> and <string-name name-style="western" hwp:sortable="Milner A David"><given-names>A David</given-names> <surname>Milner</surname></string-name>. <article-title hwp:id="article-title-19">Separate visual pathways for perception and action</article-title>. <source hwp:id="source-14">Trends in neurosciences</source>, <volume>15</volume>(<issue>1</issue>):<fpage>20</fpage>–<lpage>25</lpage>, <year>1992</year>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Grill Jean-Bastien"><given-names>Jean-Bastien</given-names> <surname>Grill</surname></string-name>, <string-name name-style="western" hwp:sortable="Strub Florian"><given-names>Florian</given-names> <surname>Strub</surname></string-name>, <string-name name-style="western" hwp:sortable="Altché Florent"><given-names>Florent</given-names> <surname>Altché</surname></string-name>, <string-name name-style="western" hwp:sortable="Tallec Corentin"><given-names>Corentin</given-names> <surname>Tallec</surname></string-name>, <string-name name-style="western" hwp:sortable="Richemond Pierre H"><given-names>Pierre H</given-names> <surname>Richemond</surname></string-name>, <string-name name-style="western" hwp:sortable="Buchatskaya Elena"><given-names>Elena</given-names> <surname>Buchatskaya</surname></string-name>, <string-name name-style="western" hwp:sortable="Doersch Carl"><given-names>Carl</given-names> <surname>Doersch</surname></string-name>, <string-name name-style="western" hwp:sortable="Pires Bernardo Avila"><given-names>Bernardo Avila</given-names> <surname>Pires</surname></string-name>, <string-name name-style="western" hwp:sortable="Guo Zhaohan Daniel"><given-names>Zhaohan Daniel</given-names> <surname>Guo</surname></string-name>, <string-name name-style="western" hwp:sortable="Azar Mohammad Gheshlaghi"><given-names>Mohammad Gheshlaghi</given-names> <surname>Azar</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-20">Bootstrap your own latent: A new approach to self-supervised learning</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">2006.07733</pub-id>, <year>2020</year>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3 xref-ref-21-4"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Güçlü Umut"><given-names>Umut</given-names> <surname>Güçlü</surname></string-name> and <string-name name-style="western" hwp:sortable="van Gerven Marcel AJ"><given-names>Marcel AJ</given-names> <surname>van Gerven</surname></string-name>. <article-title hwp:id="article-title-21">Increasingly complex representations of natural movies across the dorsal stream are shared between subjects</article-title>. <source hwp:id="source-15">NeuroImage</source>, <volume>145</volume>:<fpage>329</fpage>–<lpage>336</lpage>, <year>2017</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>[22]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.18.448989v3.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Han Tengda"><given-names>Tengda</given-names> <surname>Han</surname></string-name>, <string-name name-style="western" hwp:sortable="Xie Weidi"><given-names>Weidi</given-names> <surname>Xie</surname></string-name>, and <string-name name-style="western" hwp:sortable="Zisserman Andrew"><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <article-title hwp:id="article-title-22">Video representation learning by dense predictive coding</article-title>. In <conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</conf-name>, pages <fpage>0</fpage>–<lpage>0</lpage>, <year>2019</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><label>[23]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Harris Julie A"><given-names>Julie A</given-names> <surname>Harris</surname></string-name>, <string-name name-style="western" hwp:sortable="Mihalas Stefan"><given-names>Stefan</given-names> <surname>Mihalas</surname></string-name>, <string-name name-style="western" hwp:sortable="Hirokawa Karla E"><given-names>Karla E</given-names> <surname>Hirokawa</surname></string-name>, <string-name name-style="western" hwp:sortable="Whitesell Jennifer D"><given-names>Jennifer D</given-names> <surname>Whitesell</surname></string-name>, <string-name name-style="western" hwp:sortable="Choi Hannah"><given-names>Hannah</given-names> <surname>Choi</surname></string-name>, <string-name name-style="western" hwp:sortable="Bernard Amy"><given-names>Amy</given-names> <surname>Bernard</surname></string-name>, <string-name name-style="western" hwp:sortable="Bohn Phillip"><given-names>Phillip</given-names> <surname>Bohn</surname></string-name>, <string-name name-style="western" hwp:sortable="Caldejon Shiella"><given-names>Shiella</given-names> <surname>Caldejon</surname></string-name>, <string-name name-style="western" hwp:sortable="Casal Linzy"><given-names>Linzy</given-names> <surname>Casal</surname></string-name>, <string-name name-style="western" hwp:sortable="Cho Andrew"><given-names>Andrew</given-names> <surname>Cho</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-23">Hierarchical organization of cortical and thalamic connectivity</article-title>. <source hwp:id="source-16">Nature</source>, <volume>575</volume>(<issue>7781</issue>):<fpage>195</fpage>–<lpage>202</lpage>, <year>2019</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Haxby James V"><given-names>James V</given-names> <surname>Haxby</surname></string-name>, <string-name name-style="western" hwp:sortable="Grady Cheryl L"><given-names>Cheryl L</given-names> <surname>Grady</surname></string-name>, <string-name name-style="western" hwp:sortable="Horwitz Barry"><given-names>Barry</given-names> <surname>Horwitz</surname></string-name>, <string-name name-style="western" hwp:sortable="Ungerleider Leslie G"><given-names>Leslie G</given-names> <surname>Ungerleider</surname></string-name>, <string-name name-style="western" hwp:sortable="Mishkin Mortimer"><given-names>Mortimer</given-names> <surname>Mishkin</surname></string-name>, <string-name name-style="western" hwp:sortable="Carson Richard E"><given-names>Richard E</given-names> <surname>Carson</surname></string-name>, <string-name name-style="western" hwp:sortable="Herscovitch Peter"><given-names>Peter</given-names> <surname>Herscovitch</surname></string-name>, <string-name name-style="western" hwp:sortable="Schapiro Mark B"><given-names>Mark B</given-names> <surname>Schapiro</surname></string-name>, and <string-name name-style="western" hwp:sortable="Rapoport Stanley I"><given-names>Stanley I</given-names> <surname>Rapoport</surname></string-name>. <article-title hwp:id="article-title-24">Dissociation of object and spatial visual processing pathways in human extrastriate cortex</article-title>. <source hwp:id="source-17">Proceedings of the National Academy of Sciences</source>, <volume>88</volume>(<issue>5</issue>):<fpage>1621</fpage>–<lpage>1625</lpage>, <year>1991</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>[25]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.18.448989v3.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="He Kaiming"><given-names>Kaiming</given-names> <surname>He</surname></string-name>, <string-name name-style="western" hwp:sortable="Zhang Xiangyu"><given-names>Xiangyu</given-names> <surname>Zhang</surname></string-name>, <string-name name-style="western" hwp:sortable="Ren Shaoqing"><given-names>Shaoqing</given-names> <surname>Ren</surname></string-name>, and <string-name name-style="western" hwp:sortable="Sun Jian"><given-names>Jian</given-names> <surname>Sun</surname></string-name>. <article-title hwp:id="article-title-25">Deep residual learning for image recognition</article-title>. In <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>, pages <fpage>770</fpage>–<lpage>778</lpage>, <year>2016</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>[26]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Higgins Irina"><given-names>Irina</given-names> <surname>Higgins</surname></string-name>, <string-name name-style="western" hwp:sortable="Chang Le"><given-names>Le</given-names> <surname>Chang</surname></string-name>, <string-name name-style="western" hwp:sortable="Langston Victoria"><given-names>Victoria</given-names> <surname>Langston</surname></string-name>, <string-name name-style="western" hwp:sortable="Hassabis Demis"><given-names>Demis</given-names> <surname>Hassabis</surname></string-name>, <string-name name-style="western" hwp:sortable="Summerfield Christopher"><given-names>Christopher</given-names> <surname>Summerfield</surname></string-name>, <string-name name-style="western" hwp:sortable="Tsao Doris"><given-names>Doris</given-names> <surname>Tsao</surname></string-name>, and <string-name name-style="western" hwp:sortable="Botvinick Matthew"><given-names>Matthew</given-names> <surname>Botvinick</surname></string-name>. <article-title hwp:id="article-title-26">Unsupervised deep learning identifies semantic disentanglement in single inferotemporal neurons</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">2006.14304</pub-id>, <year>2020</year>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Huberman Andrew D"><given-names>Andrew D</given-names> <surname>Huberman</surname></string-name> and <string-name name-style="western" hwp:sortable="Niell Cristopher M"><given-names>Cristopher M</given-names> <surname>Niell</surname></string-name>. <article-title hwp:id="article-title-27">What can mice tell us about how vision works?</article-title> <source hwp:id="source-18">Trends in neurosciences</source>, <volume>34</volume>(<issue>9</issue>):<fpage>464</fpage>–<lpage>473</lpage>, <year>2011</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>[28]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="James Thomas W"><given-names>Thomas W</given-names> <surname>James</surname></string-name>, <string-name name-style="western" hwp:sortable="Culham Jody"><given-names>Jody</given-names> <surname>Culham</surname></string-name>, <string-name name-style="western" hwp:sortable="Keith Humphrey G"><given-names>G</given-names> <surname>Keith Humphrey</surname></string-name>, <string-name name-style="western" hwp:sortable="Milner A David"><given-names>A David</given-names> <surname>Milner</surname></string-name>, and <string-name name-style="western" hwp:sortable="Goodale Melvyn A"><given-names>Melvyn A</given-names> <surname>Goodale</surname></string-name>. <article-title hwp:id="article-title-28">Ventral occipital lesions impair object recognition but not object-directed grasping: an fmri study</article-title>. <source hwp:id="source-19">Brain</source>, <volume>126</volume>(<issue>11</issue>):<fpage>2463</fpage>–<lpage>2475</lpage>, <year>2003</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Jones Judson P"><given-names>Judson P</given-names> <surname>Jones</surname></string-name> and <string-name name-style="western" hwp:sortable="Palmer Larry A"><given-names>Larry A</given-names> <surname>Palmer</surname></string-name>. <article-title hwp:id="article-title-29">An evaluation of the two-dimensional gabor filter model of simple receptive fields in cat striate cortex</article-title>. <source hwp:id="source-20">Journal of neurophysiology</source>, <volume>58</volume>(<issue>6</issue>):<fpage>1233</fpage>–<lpage>1258</lpage>, <year>1987</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Keller Georg B"><given-names>Georg B</given-names> <surname>Keller</surname></string-name>, <string-name name-style="western" hwp:sortable="Bonhoeffer Tobias"><given-names>Tobias</given-names> <surname>Bonhoeffer</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hübener Mark"><given-names>Mark</given-names> <surname>Hübener</surname></string-name>. <article-title hwp:id="article-title-30">Sensorimotor mismatch signals in primary visual cortex of the behaving mouse</article-title>. <source hwp:id="source-21">Neuron</source>, <volume>74</volume>(<issue>5</issue>):<fpage>809</fpage>–<lpage>815</lpage>, <year>2012</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Keller Georg B"><given-names>Georg B</given-names> <surname>Keller</surname></string-name> and <string-name name-style="western" hwp:sortable="Mrsic-Flogel Thomas D"><given-names>Thomas D</given-names> <surname>Mrsic-Flogel</surname></string-name>. <article-title hwp:id="article-title-31">Predictive processing: a canonical cortical computation</article-title>. <source hwp:id="source-22">Neuron</source>, <volume>100</volume>(<issue>2</issue>):<fpage>424</fpage>–<lpage>435</lpage>, <year>2018</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Khaligh-Razavi Seyed-Mahdi"><given-names>Seyed-Mahdi</given-names> <surname>Khaligh-Razavi</surname></string-name> and <string-name name-style="western" hwp:sortable="Kriegeskorte Nikolaus"><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <article-title hwp:id="article-title-32">Deep supervised, but not unsupervised, models may explain it cortical representation</article-title>. <source hwp:id="source-23">PLoS computational biology</source>, <volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>, <year>2014</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Kietzmann Tim C"><given-names>Tim C</given-names> <surname>Kietzmann</surname></string-name>, <string-name name-style="western" hwp:sortable="Spoerer Courtney J"><given-names>Courtney J</given-names> <surname>Spoerer</surname></string-name>, <string-name name-style="western" hwp:sortable="Sörensen Lynn KA"><given-names>Lynn KA</given-names> <surname>Sörensen</surname></string-name>, <string-name name-style="western" hwp:sortable="Cichy Radoslaw M"><given-names>Radoslaw M</given-names> <surname>Cichy</surname></string-name>, <string-name name-style="western" hwp:sortable="Hauk Olaf"><given-names>Olaf</given-names> <surname>Hauk</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kriegeskorte Nikolaus"><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <article-title hwp:id="article-title-33">Recurrence is required to capture the representational dynamics of the human visual system</article-title>. <source hwp:id="source-24">Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>43</issue>):<fpage>21854</fpage>–<lpage>21863</lpage>, <year>2019</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2"><label>[34]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Konkle Talia"><given-names>Talia</given-names> <surname>Konkle</surname></string-name> and <string-name name-style="western" hwp:sortable="Alvarez George"><given-names>George</given-names> <surname>Alvarez</surname></string-name>. <article-title hwp:id="article-title-34">Deepnets do not need category supervision to predict visual system responses to objects</article-title>. <source hwp:id="source-25">Journal of Vision</source>, <volume>20</volume>(<issue>11</issue>):<fpage>498</fpage>–<lpage>498</lpage>, <year>2020</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Kowler Eileen"><given-names>Eileen</given-names> <surname>Kowler</surname></string-name>, <string-name name-style="western" hwp:sortable="Rubinstein Jason F"><given-names>Jason F</given-names> <surname>Rubinstein</surname></string-name>, <string-name name-style="western" hwp:sortable="Santos Elio M"><given-names>Elio M</given-names> <surname>Santos</surname></string-name>, and <string-name name-style="western" hwp:sortable="Wang Jie"><given-names>Jie</given-names> <surname>Wang</surname></string-name>. <article-title hwp:id="article-title-35">Predictive smooth pursuit eye movements</article-title>. <source hwp:id="source-26">Annual review of vision science</source>, <volume>5</volume>:<fpage>223</fpage>–<lpage>246</lpage>, <year>2019</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>[36]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Krizhevsky Alex"><given-names>Alex</given-names> <surname>Krizhevsky</surname></string-name>, <string-name name-style="western" hwp:sortable="Hinton Geoffrey"><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-36">Learning multiple layers of features from tiny images</article-title>. <year>2009</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>[37]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Laramée Marie-Eve"><given-names>Marie-Eve</given-names> <surname>Laramée</surname></string-name> and <string-name name-style="western" hwp:sortable="Boire Denis"><given-names>Denis</given-names> <surname>Boire</surname></string-name>. <article-title hwp:id="article-title-37">Visual cortical areas of the mouse: comparison of parcellation and network structure with primates</article-title>. <source hwp:id="source-27">Frontiers in neural circuits</source>, <volume>8</volume>:<fpage>149</fpage>, <year>2015</year>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2"><label>[38]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Lotter William"><given-names>William</given-names> <surname>Lotter</surname></string-name>, <string-name name-style="western" hwp:sortable="Kreiman Gabriel"><given-names>Gabriel</given-names> <surname>Kreiman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Cox David"><given-names>David</given-names> <surname>Cox</surname></string-name>. <article-title hwp:id="article-title-38">Deep predictive coding networks for video prediction and unsupervised learning</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">1605.08104</pub-id>, <year>2016</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>[39]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Marshel James H"><given-names>James H</given-names> <surname>Marshel</surname></string-name>, <string-name name-style="western" hwp:sortable="Garrett Marina E"><given-names>Marina E</given-names> <surname>Garrett</surname></string-name>, <string-name name-style="western" hwp:sortable="Nauhaus Ian"><given-names>Ian</given-names> <surname>Nauhaus</surname></string-name>, and <string-name name-style="western" hwp:sortable="Callaway Edward M"><given-names>Edward M</given-names> <surname>Callaway</surname></string-name>. <article-title hwp:id="article-title-39">Functional specialization of seven mouse visual cortical areas</article-title>. <source hwp:id="source-28">Neuron</source>, <volume>72</volume>(<issue>6</issue>):<fpage>1040</fpage>–<lpage>1054</lpage>, <year>2011</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>[40]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="McDonald AJ"><given-names>AJ</given-names> <surname>McDonald</surname></string-name> and <string-name name-style="western" hwp:sortable="Mascagni F"><given-names>F</given-names> <surname>Mascagni</surname></string-name>. <article-title hwp:id="article-title-40">Cortico-cortical and cortico-amygdaloid projections of the rat occipital cortex: a phaseolus vulgaris leucoagglutinin study</article-title>. <source hwp:id="source-29">Neuroscience</source>, <volume>71</volume>(<issue>1</issue>):<fpage>37</fpage>–<lpage>54</lpage>, <year>1996</year>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>[41]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Mehrer Johannes"><given-names>Johannes</given-names> <surname>Mehrer</surname></string-name>, <string-name name-style="western" hwp:sortable="Spoerer Courtney J"><given-names>Courtney J</given-names> <surname>Spoerer</surname></string-name>, <string-name name-style="western" hwp:sortable="Kriegeskorte Nikolaus"><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kietzmann Tim C"><given-names>Tim C</given-names> <surname>Kietzmann</surname></string-name>. <article-title hwp:id="article-title-41">Individual differences among deep neural network models</article-title>. <source hwp:id="source-30">Nature communications</source>, <volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>12</lpage>, <year>2020</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>[42]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Mishkin Mortimer"><given-names>Mortimer</given-names> <surname>Mishkin</surname></string-name>, <string-name name-style="western" hwp:sortable="Ungerleider Leslie G"><given-names>Leslie G</given-names> <surname>Ungerleider</surname></string-name>, and <string-name name-style="western" hwp:sortable="Macko Kathleen A"><given-names>Kathleen A</given-names> <surname>Macko</surname></string-name>. <article-title hwp:id="article-title-42">Object vision and spatial vision: two cortical pathways</article-title>. <source hwp:id="source-31">Trends in neurosciences</source>, <volume>6</volume>:<fpage>414</fpage>–<lpage>417</lpage>, <year>1983</year>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>[43]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Nguyen Thao"><given-names>Thao</given-names> <surname>Nguyen</surname></string-name>, <string-name name-style="western" hwp:sortable="Raghu Maithra"><given-names>Maithra</given-names> <surname>Raghu</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kornblith Simon"><given-names>Simon</given-names> <surname>Kornblith</surname></string-name>. <article-title hwp:id="article-title-43">Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">2010.15327</pub-id>, <year>2020</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>[44]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Niell Cristopher M"><given-names>Cristopher M</given-names> <surname>Niell</surname></string-name> and <string-name name-style="western" hwp:sortable="Scanziani Massimo"><given-names>Massimo</given-names> <surname>Scanziani</surname></string-name>. <article-title hwp:id="article-title-44">How cortical circuits implement cortical computations: Mouse visual cortex as a model</article-title>. <source hwp:id="source-32">Annual Review of Neuroscience</source>, <volume>44</volume>, <year>2021</year>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3"><label>[45]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="van den Oord Aaron"><given-names>Aaron</given-names> <surname>van den Oord</surname></string-name>, <string-name name-style="western" hwp:sortable="Li Yazhe"><given-names>Yazhe</given-names> <surname>Li</surname></string-name>, and <string-name name-style="western" hwp:sortable="Vinyals Oriol"><given-names>Oriol</given-names> <surname>Vinyals</surname></string-name>. <article-title hwp:id="article-title-45">Representation learning with contrastive predictive coding</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">1807.03748</pub-id>, <year>2018</year>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>[46]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Pack Christopher C"><given-names>Christopher C</given-names> <surname>Pack</surname></string-name> and <string-name name-style="western" hwp:sortable="Born Richard T"><given-names>Richard T</given-names> <surname>Born</surname></string-name>. <article-title hwp:id="article-title-46">Temporal dynamics of a neural solution to the aperture problem in visual area mt of macaque brain</article-title>. <source hwp:id="source-33">Nature</source>, <volume>409</volume>(<issue>6823</issue>):<fpage>1040</fpage>–<lpage>1042</lpage>, <year>2001</year>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>[47]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Pasternak Tatiana"><given-names>Tatiana</given-names> <surname>Pasternak</surname></string-name>, <string-name name-style="western" hwp:sortable="Horn Kris M"><given-names>Kris M</given-names> <surname>Horn</surname></string-name>, and <string-name name-style="western" hwp:sortable="Maunsell John HR"><given-names>John HR</given-names> <surname>Maunsell</surname></string-name>. <article-title hwp:id="article-title-47">Deficits in speed discrimination following lesions of the lateral suprasylvian cortex in the cat</article-title>. <source hwp:id="source-34">Visual neuroscience</source>, <volume>3</volume>(<issue>4</issue>):<fpage>365</fpage>–<lpage>375</lpage>, <year>1989</year>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>[48]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Pasternak Tatiana"><given-names>Tatiana</given-names> <surname>Pasternak</surname></string-name> and <string-name name-style="western" hwp:sortable="Merigan William H"><given-names>William H</given-names> <surname>Merigan</surname></string-name>. <article-title hwp:id="article-title-48">Motion perception following lesions of the superior temporal sulcus in the monkey</article-title>. <source hwp:id="source-35">Cerebral Cortex</source>, <volume>4</volume>(<issue>3</issue>):<fpage>247</fpage>–<lpage>259</lpage>, <year>1994</year>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>[49]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Ponce Carlos R"><given-names>Carlos R</given-names> <surname>Ponce</surname></string-name>, <string-name name-style="western" hwp:sortable="Xiao Will"><given-names>Will</given-names> <surname>Xiao</surname></string-name>, <string-name name-style="western" hwp:sortable="Schade Peter F"><given-names>Peter F</given-names> <surname>Schade</surname></string-name>, <string-name name-style="western" hwp:sortable="Hartmann Till S"><given-names>Till S</given-names> <surname>Hartmann</surname></string-name>, <string-name name-style="western" hwp:sortable="Kreiman Gabriel"><given-names>Gabriel</given-names> <surname>Kreiman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Livingstone Margaret S"><given-names>Margaret S</given-names> <surname>Livingstone</surname></string-name>. <article-title hwp:id="article-title-49">Evolving images for visual neurons using a deep generative network reveals coding principles and neuronal preferences</article-title>. <source hwp:id="source-36">Cell</source>, <volume>177</volume>(<issue>4</issue>):<fpage>999</fpage>–<lpage>1009</lpage>, <year>2019</year>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>[50]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Richards Blake A"><given-names>Blake A</given-names> <surname>Richards</surname></string-name>, <string-name name-style="western" hwp:sortable="Lillicrap Timothy P"><given-names>Timothy P</given-names> <surname>Lillicrap</surname></string-name>, <string-name name-style="western" hwp:sortable="Beaudoin Philippe"><given-names>Philippe</given-names> <surname>Beaudoin</surname></string-name>, <string-name name-style="western" hwp:sortable="Bengio Yoshua"><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <string-name name-style="western" hwp:sortable="Bogacz Rafal"><given-names>Rafal</given-names> <surname>Bogacz</surname></string-name>, <string-name name-style="western" hwp:sortable="Christensen Amelia"><given-names>Amelia</given-names> <surname>Christensen</surname></string-name>, <string-name name-style="western" hwp:sortable="Clopath Claudia"><given-names>Claudia</given-names> <surname>Clopath</surname></string-name>, <string-name name-style="western" hwp:sortable="Costa Rui Ponte"><given-names>Rui Ponte</given-names> <surname>Costa</surname></string-name>, <string-name name-style="western" hwp:sortable="de Berker Archy"><given-names>Archy</given-names> <surname>de Berker</surname></string-name>, <string-name name-style="western" hwp:sortable="Ganguli Surya"><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-50">A deep learning framework for neuroscience</article-title>. <source hwp:id="source-37">Nature neuroscience</source>, <volume>22</volume>(<issue>11</issue>):<fpage>1761</fpage>–<lpage>1770</lpage>, <year>2019</year>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>[51]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Rideaux Reuben"><given-names>Reuben</given-names> <surname>Rideaux</surname></string-name> and <string-name name-style="western" hwp:sortable="Welchman Andrew E"><given-names>Andrew E</given-names> <surname>Welchman</surname></string-name>. <article-title hwp:id="article-title-51">But still it moves: static image statistics underlie how we see motion</article-title>. <source hwp:id="source-38">Journal of Neuroscience</source>, <volume>40</volume>(<issue>12</issue>):<fpage>2538</fpage>–<lpage>2552</lpage>, <year>2020</year>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>[52]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Schiller Peter H"><given-names>Peter H</given-names> <surname>Schiller</surname></string-name> and <string-name name-style="western" hwp:sortable="Logothetis Nikos K"><given-names>Nikos K</given-names> <surname>Logothetis</surname></string-name>. <article-title hwp:id="article-title-52">The color-opponent and broad-band channels of the primate visual system</article-title>. <source hwp:id="source-39">Trends in neurosciences</source>, <volume>13</volume>(<issue>10</issue>):<fpage>392</fpage>–<lpage>398</lpage>, <year>1990</year>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>[53]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Shi Jianghong"><given-names>Jianghong</given-names> <surname>Shi</surname></string-name>, <string-name name-style="western" hwp:sortable="Shea-Brown Eric"><given-names>Eric</given-names> <surname>Shea-Brown</surname></string-name>, and <string-name name-style="western" hwp:sortable="Buice Michael A"><given-names>Michael A</given-names> <surname>Buice</surname></string-name>. <article-title hwp:id="article-title-53">Comparison against task driven artificial neural networks reveals functional organization of mouse visual cortex</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">1911.07986</pub-id>, <year>2019</year>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1 xref-ref-54-2"><label>[54]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Sit Kevin K"><given-names>Kevin K</given-names> <surname>Sit</surname></string-name> and <string-name name-style="western" hwp:sortable="Goard Michael J"><given-names>Michael J</given-names> <surname>Goard</surname></string-name>. <article-title hwp:id="article-title-54">Distributed and retinotopically asymmetric processing of coherent motion in mouse visual cortex</article-title>. <source hwp:id="source-40">Nature communications</source>, <volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>, <year>2020</year>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>[55]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Smyth Darragh"><given-names>Darragh</given-names> <surname>Smyth</surname></string-name>, <string-name name-style="western" hwp:sortable="Willmore Ben"><given-names>Ben</given-names> <surname>Willmore</surname></string-name>, <string-name name-style="western" hwp:sortable="Baker Gary E"><given-names>Gary E</given-names> <surname>Baker</surname></string-name>, <string-name name-style="western" hwp:sortable="Thompson Ian D"><given-names>Ian D</given-names> <surname>Thompson</surname></string-name>, and <string-name name-style="western" hwp:sortable="Tolhurst David J"><given-names>David J</given-names> <surname>Tolhurst</surname></string-name>. <article-title hwp:id="article-title-55">The receptive-field organization of simple cells in primary visual cortex of ferrets under natural scene stimulation</article-title>. <source hwp:id="source-41">Journal of Neuroscience</source>, <volume>23</volume>(<issue>11</issue>):<fpage>4746</fpage>–<lpage>4759</lpage>, <year>2003</year>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1 xref-ref-56-2"><label>[56]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Soomro Khurram"><given-names>Khurram</given-names> <surname>Soomro</surname></string-name>, <string-name name-style="western" hwp:sortable="Zamir Amir Roshan"><given-names>Amir Roshan</given-names> <surname>Zamir</surname></string-name>, and <string-name name-style="western" hwp:sortable="Shah Mubarak"><given-names>Mubarak</given-names> <surname>Shah</surname></string-name>. <article-title hwp:id="article-title-56">Ucf101: A dataset of 101 human actions classes from videos in the wild</article-title>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">1212.0402</pub-id>, <year>2012</year>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><label>[57]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.18.448989v3.57" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Storrs Katherine R"><given-names>Katherine R</given-names> <surname>Storrs</surname></string-name>, <string-name name-style="western" hwp:sortable="Kietzmann Tim C"><given-names>Tim C</given-names> <surname>Kietzmann</surname></string-name>, <string-name name-style="western" hwp:sortable="Walther Alexander"><given-names>Alexander</given-names> <surname>Walther</surname></string-name>, <string-name name-style="western" hwp:sortable="Mehrer Johannes"><given-names>Johannes</given-names> <surname>Mehrer</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kriegeskorte Nikolaus"><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <article-title hwp:id="article-title-57">Diverse deep neural networks all predict human it well, after training and fitting</article-title>. <source hwp:id="source-42">bioRxiv</source>, <year>2020</year>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1 xref-ref-58-2 xref-ref-58-3"><label>[58]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Wang Quanxin"><given-names>Quanxin</given-names> <surname>Wang</surname></string-name>, <string-name name-style="western" hwp:sortable="Gao Enquan"><given-names>Enquan</given-names> <surname>Gao</surname></string-name>, and <string-name name-style="western" hwp:sortable="Burkhalter Andreas"><given-names>Andreas</given-names> <surname>Burkhalter</surname></string-name>. <article-title hwp:id="article-title-58">Gateways of ventral and dorsal streams in mouse visual cortex</article-title>. <source hwp:id="source-43">Journal of Neuroscience</source>, <volume>31</volume>(<issue>5</issue>):<fpage>1905</fpage>–<lpage>1918</lpage>, <year>2011</year>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>[59]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Wang Quanxin"><given-names>Quanxin</given-names> <surname>Wang</surname></string-name>, <string-name name-style="western" hwp:sortable="Sporns Olaf"><given-names>Olaf</given-names> <surname>Sporns</surname></string-name>, and <string-name name-style="western" hwp:sortable="Burkhalter Andreas"><given-names>Andreas</given-names> <surname>Burkhalter</surname></string-name>. <article-title hwp:id="article-title-59">Network analysis of corticocortical connections reveals ventral and dorsal processing streams in mouse visual cortex</article-title>. <source hwp:id="source-44">Journal of Neuroscience</source>, <volume>32</volume>(<issue>13</issue>):<fpage>4386</fpage>–<lpage>4399</lpage>, <year>2012</year>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>[60]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Watamaniuk Scott NJ"><given-names>Scott NJ</given-names> <surname>Watamaniuk</surname></string-name> and <string-name name-style="western" hwp:sortable="Sekuler Robert"><given-names>Robert</given-names> <surname>Sekuler</surname></string-name>. <article-title hwp:id="article-title-60">Temporal and spatial integration in dynamic randomdot stimuli</article-title>. <source hwp:id="source-45">Vision research</source>, <volume>32</volume>(<issue>12</issue>):<fpage>2341</fpage>–<lpage>2347</lpage>, <year>1992</year>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><label>[61]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Wiskott Laurenz"><given-names>Laurenz</given-names> <surname>Wiskott</surname></string-name> and <string-name name-style="western" hwp:sortable="Sejnowski Terrence J"><given-names>Terrence J</given-names> <surname>Sejnowski</surname></string-name>. <article-title hwp:id="article-title-61">Slow feature analysis: Unsupervised learning of invariances</article-title>. <source hwp:id="source-46">Neural computation</source>, <volume>14</volume>(<issue>4</issue>):<fpage>715</fpage>–<lpage>770</lpage>, <year>2002</year>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1 xref-ref-62-2"><label>[62]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Yamins Daniel LK"><given-names>Daniel LK</given-names> <surname>Yamins</surname></string-name>, <string-name name-style="western" hwp:sortable="Hong Ha"><given-names>Ha</given-names> <surname>Hong</surname></string-name>, <string-name name-style="western" hwp:sortable="Cadieu Charles F"><given-names>Charles F</given-names> <surname>Cadieu</surname></string-name>, <string-name name-style="western" hwp:sortable="Solomon Ethan A"><given-names>Ethan A</given-names> <surname>Solomon</surname></string-name>, <string-name name-style="western" hwp:sortable="Seibert Darren"><given-names>Darren</given-names> <surname>Seibert</surname></string-name>, and <string-name name-style="western" hwp:sortable="DiCarlo James J"><given-names>James J</given-names> <surname>DiCarlo</surname></string-name>. <article-title hwp:id="article-title-62">Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source hwp:id="source-47">Proceedings of the national academy of sciences</source>, <volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>, <year>2014</year>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1 xref-ref-63-2"><label>[63]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.63" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Zhuang Chengxu"><given-names>Chengxu</given-names> <surname>Zhuang</surname></string-name>, <string-name name-style="western" hwp:sortable="Yan Siming"><given-names>Siming</given-names> <surname>Yan</surname></string-name>, <string-name name-style="western" hwp:sortable="Nayebi Aran"><given-names>Aran</given-names> <surname>Nayebi</surname></string-name>, <string-name name-style="western" hwp:sortable="Schrimpf Martin"><given-names>Martin</given-names> <surname>Schrimpf</surname></string-name>, <string-name name-style="western" hwp:sortable="Frank Michael C"><given-names>Michael C</given-names> <surname>Frank</surname></string-name>, <string-name name-style="western" hwp:sortable="DiCarlo James J"><given-names>James J</given-names> <surname>DiCarlo</surname></string-name>, and <string-name name-style="western" hwp:sortable="Yamins Daniel LK"><given-names>Daniel LK</given-names> <surname>Yamins</surname></string-name>. <article-title hwp:id="article-title-63">Unsupervised neural network models of the ventral visual stream</article-title>. <source hwp:id="source-48">Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>3</issue>), <year>2021</year>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><label>[64]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.18.448989v3.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Zihl Josef"><given-names>Josef</given-names> <surname>Zihl</surname></string-name>, <string-name name-style="western" hwp:sortable="Von Cramon D"><given-names>D</given-names> <surname>Von Cramon</surname></string-name>, and <string-name name-style="western" hwp:sortable="Mai Norbert"><given-names>Norbert</given-names> <surname>Mai</surname></string-name>. <article-title hwp:id="article-title-64">Selective disturbance of movement vision after bilateral brain damage</article-title>. <source hwp:id="source-49">Brain</source>, <volume>106</volume>(<issue>2</issue>):<fpage>313</fpage>–<lpage>340</lpage>, <year>1983</year>.</citation></ref></ref-list><sec hwp:id="sec-27"><title hwp:id="title-32">Checklist</title><p hwp:id="p-45">The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default <bold>[TODO]</bold> to [Yes], [No], or [N/A]. You are strongly encouraged to include a <bold>justification to your answer</bold>, either by referencing the appropriate section of your paper or providing a brief inline description. For example:
<list list-type="bullet" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-46">Did you include the license to the code and datasets? [Yes] See Section <bold>??</bold>.</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-47">Did you include the license to the code and datasets? [No] The code and the data are proprietary.</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-48">Did you include the license to the code and datasets? [N/A]</p></list-item></list></p><p hwp:id="p-49">Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.
<list list-type="order" hwp:id="list-2"><list-item hwp:id="list-item-4"><p hwp:id="p-50">For all authors…</p><list list-type="alpha-lower" hwp:id="list-3"><list-item hwp:id="list-item-5"><p hwp:id="p-51">Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes]</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-52">Did you describe the limitations of your work? [Yes] See <xref ref-type="sec" rid="s6" hwp:id="xref-sec-25-1" hwp:rel-id="sec-25">section 6</xref></p></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-53">Did you discuss any potential negative societal impacts of your work? [Yes]</p></list-item><list-item hwp:id="list-item-8"><p hwp:id="p-54">Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</p></list-item></list></list-item><list-item hwp:id="list-item-9"><p hwp:id="p-55">If you are including theoretical results…</p><list list-type="alpha-lower" hwp:id="list-4"><list-item hwp:id="list-item-10"><p hwp:id="p-56">Did you state the full set of assumptions of all theoretical results? [N/A]</p></list-item><list-item hwp:id="list-item-11"><p hwp:id="p-57">Did you include complete proofs of all theoretical results? [N/A]</p></list-item></list></list-item><list-item hwp:id="list-item-12"><p hwp:id="p-58">If you ran experiments…</p><list list-type="alpha-lower" hwp:id="list-5"><list-item hwp:id="list-item-13"><p hwp:id="p-59">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See the supplementary materials</p></list-item><list-item hwp:id="list-item-14"><p hwp:id="p-60">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-2" hwp:rel-id="sec-12">section 3.3</xref> and <xref ref-type="sec" rid="s7a" hwp:id="xref-sec-29-1" hwp:rel-id="sec-29">section A</xref> of supplementary materials</p></list-item><list-item hwp:id="list-item-15"><p hwp:id="p-61">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] All the values reported in the figures have error bars. The error bars represent the standard deviations estimated with bootstrapping on the neurons.</p></list-item><list-item hwp:id="list-item-16"><p hwp:id="p-62">Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-3" hwp:rel-id="sec-12">section 3.3</xref></p></list-item></list></list-item><list-item hwp:id="list-item-17"><p hwp:id="p-63">If you are using existing assets (e.g., code, data, models) or curating/releasing new assets…</p><list list-type="alpha-lower" hwp:id="list-6"><list-item hwp:id="list-item-18"><p hwp:id="p-64">If your work uses existing assets, did you cite the creators? [Yes] See <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-4" hwp:rel-id="sec-12">section 3.3</xref></p></list-item><list-item hwp:id="list-item-19"><p hwp:id="p-65">Did you mention the license of the assets? [N/A]</p></list-item><list-item hwp:id="list-item-20"><p hwp:id="p-66">Did you include any new assets either in the supplemental material or as a URL? [Yes] We included the codes and the checkpoint files for the pretrained models.</p></list-item><list-item hwp:id="list-item-21"><p hwp:id="p-67">Did you discuss whether and how consent was obtained from people whose data you’re using/curating? N/A]</p></list-item><list-item hwp:id="list-item-22"><p hwp:id="p-68">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]</p></list-item></list></list-item><list-item hwp:id="list-item-23"><p hwp:id="p-69">If you used crowdsourcing or conducted research with human subjects…</p><list list-type="alpha-lower" hwp:id="list-7"><list-item hwp:id="list-item-24"><p hwp:id="p-70">Did you include the full text of instructions given to participants and screenshots, if applicable? N/A]</p></list-item><list-item hwp:id="list-item-25"><p hwp:id="p-71">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]</p></list-item><list-item hwp:id="list-item-26"><p hwp:id="p-72">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</p></list-item></list></list-item></list></p></sec><sec id="s7" hwp:id="sec-28"><title hwp:id="title-33">Supplementary Material</title><fig id="figS1" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1:</label><caption hwp:id="caption-6"><p hwp:id="p-73">The schematic of Contrastive Predictive Coding model with (a) ResNet-1p, and (b) ResNet-2p backbone architectures. The present and past frames of the video (<italic toggle="yes">x<sub>t</sub>, x</italic><sub><italic toggle="yes">t</italic>−1</sub>,…) are given as input to the 3D ResNets. The output of the ResNets at each time point (<italic toggle="yes">z<sub>t</sub>, z</italic><sub><italic toggle="yes">t</italic>−1</sub>,…) are then passed to a recurrent neural network (RNN) which generates a context variable at time <italic toggle="yes">t</italic> (<italic toggle="yes">c<sub>t</sub></italic>). The context variable, <italic toggle="yes">c<sub>t</sub></italic>, is then used to predict the latent state of the next frame <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="448989v3_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> via a single layer MLP (<italic toggle="yes">f</italic>). The predicted next frame along with the latent representation of the correct next frame (positive sample, <italic toggle="yes">z</italic><sub><italic toggle="yes">t</italic>+1</sub>) and incorrect samples (negative examples, not shown here) are then fed to the loss function (see <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">equation 1</xref>).</p></caption><graphic xlink:href="448989v3_figS1" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><sec id="s7a" hwp:id="sec-29" hwp:rev-id="xref-sec-29-1"><label>A</label><title hwp:id="title-34">Supplementary methods</title><sec id="s7a1" hwp:id="sec-30" hwp:rev-id="xref-sec-30-1"><label>A.1</label><title hwp:id="title-35">RSA noise ceiling</title><p hwp:id="p-74">To estimate the noise ceiling of <italic toggle="yes">RSM</italic> similarity (the maximum <italic toggle="yes">RSM</italic> similarity we can reach for a given area), we randomly split the neuronal responses into two groups, and calculate two different <italic toggle="yes">RSM</italic>s based on each split. We then calculate the similarity between the <italic toggle="yes">RSM</italic> of the splits (as explained in <xref ref-type="sec" rid="s3b" hwp:id="xref-sec-8-1" hwp:rel-id="sec-8">section 3.2</xref>), and the similarity value gives us an estimate of the best possible match (i.e. the noise ceiling) for the recordings of that brain area. This process is repeated 100 times to also obtain a measure of variability for the estimated noise ceiling. The diagonal values of the matrix in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-12" hwp:rel-id="F1">Figure 1b</xref> shows the average noise ceiling values for every area.</p></sec><sec id="s7a2" hwp:id="sec-31"><label>A.2</label><title hwp:id="title-36">Contrastive predictive coding</title><p hwp:id="p-75">As noted in <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-5" hwp:rel-id="sec-12">section 3.3</xref>, the CPC loss function relies on predicting the future latent representations of a video sequence, given its present and past representations (<xref ref-type="fig" rid="figS1" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Fig. S1</xref>). Specifically, a block of <italic toggle="yes">N</italic> frames (<inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="448989v3_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula>; <italic toggle="yes">W, H</italic> are the spatial dimensions, <italic toggle="yes">C</italic> is the number of input channels) are passed through a backbone architecture, here a 3D convolutional neural network (CNN), and the CNN output (<inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="448989v3_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula>; <italic toggle="yes">W</italic>′, <italic toggle="yes">H</italic>′ are the spatial dimensions, <italic toggle="yes">D</italic> is the number of output channels) is given to a recurrent neural network (RNN). The RNN aggregates the latent variables of S blocks of frames (<italic toggle="yes">z</italic><sub><italic toggle="yes">t</italic>–<italic toggle="yes">S</italic>+1</sub>,…, <italic toggle="yes">z<sub>t</sub></italic>; here <italic toggle="yes">S</italic> = 5), and generates the context variable <italic toggle="yes">c<sub>t</sub></italic> which is then used to predict the future <italic toggle="yes">T</italic> steps of the latent variables (<inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="448989v3_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula>; here <italic toggle="yes">T</italic> = 3). Importantly, the prediction is done in the latent space (<italic toggle="yes">i.e</italic>. the CNN output), and not in the pixel space. Specifically, CPC optimizes the following contrastive loss function:
<disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1"><alternatives hwp:id="alternatives-11"><graphic xlink:href="448989v3_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives></disp-formula>
where, <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> denote the <italic toggle="yes">i<sup>th</sup></italic> and <italic toggle="yes">j<sup>th</sup></italic> time points. Minimizing the above contrastive loss function maximizes the similarity of the predicted latent variable <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="448989v3_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> and the true future latent state (<italic toggle="yes">z<sub>i</sub></italic>; AKA positive pairs), and minimizes its similarity with incorrect latent states (<italic toggle="yes">z<sub>j</sub></italic> for <italic toggle="yes">j</italic> ≠ <italic toggle="yes">i</italic>; AKA negative pairs).</p></sec><sec id="s7a3" hwp:id="sec-32" hwp:rev-id="xref-sec-32-1"><label>A.3</label><title hwp:id="title-37">Other models</title><p hwp:id="p-76">Two models are used as baselines: 3D Gabors, that were shown to be an acceptable model of the primary visual cortex in cats and primates [<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref>, <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>], and an untrained, randomly initialized 3D ResNet. The Gabor model comprised of 3D Gabor filters, spanning 8 motion directions (equally spaced between 0 and 315 degrees), 3 phases (−1, 0, 1), and 5 spatial scales. The randomly initialized model has its weights selected by matching the mean and standard deviation of synaptic weights in the trained models. These two baseline models are expected to be surpassed, in terms of alignment with brain representations, by any trained ANN that is using a relevant loss function for the visual system (meaning that the loss function captures something relevant about how either evolution or learning shape the brain). In addition to the baseline models, we also compare the CPC trained ANNs with ANNs that are trained with two other supervised loss functions: supervised object categorization (ImageNet dataset) and supervised action recognition (UCF101 dataset). We consider the ANNs trained with object categorization simply because they have been the standard in studies of the ventral pathway. We also use supervised action recognition (with UCF101 video dataset) in order to ensure that we have a fair comparison with CPC, which is being trained with dynamic videos. Importantly, both CPC and supervised action recognition use the UCF101 dataset, which makes the loss function the only difference between these two models. In order to show that the spatiotemporal dynamics of the video data is important, we also compared CPC with SimCLR, a self-supervised learning model trained on static images [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>]. All the models are compared based on their representation alignment with different areas of mouse visual cortex (see <xref ref-type="sec" rid="s3b" hwp:id="xref-sec-8-2" hwp:rel-id="sec-8">section 3.2</xref>), and by using downstream tasks (see <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-6" hwp:rel-id="sec-12">section 3.3</xref>).</p></sec><sec id="s7a4" hwp:id="sec-33" hwp:rev-id="xref-sec-33-1"><label>A.4</label><title hwp:id="title-38">Datasets</title><p hwp:id="p-77">For training the deep ANNs, we use the UCF101 dataset. UCF101 is a dataset of 13320 short video segments from 101 action categories collected from YouTube [<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-2" hwp:rel-id="ref-56">56</xref>]. We use this dataset for both self-supervised and supervised training of the ANNs. In the case of supervised training, the ANNs are trained to categorize actions in the videos. However, the UCF101 videos are not used for comparing representations between brain areas and the ANNs. For this, we use the same videos that the Allen Brain Observatory presented to the mice. Both datasets (UCF101 and Allen Brain Observatory videos) were normalized (with mean and standard deviation calculated across each dataset) and downsampled to 64 × 64 to account for the low spatial resolution of mouse retina.</p></sec><sec id="s7a5" hwp:id="sec-34" hwp:rev-id="xref-sec-34-1 xref-sec-34-2"><label>A.5</label><title hwp:id="title-39">Linear evaluation tasks</title><p hwp:id="p-78">As noted in <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-12-7" hwp:rel-id="sec-12">section 3.3</xref>, we examine the two pathways of our trained ResNet-2p on two downstream tasks: object categorization and motion discrimination, which are supported in the real brain by the ventral and dorsal pathways, respectively. For each downstream task, the weights of the trained pathways are frozen and a linear classifier is trained on the final convolutional layer of each pathway. We use the CIFAR-10 dataset for the object categorization task [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>]. For the motion discrimination task, we use random-dot kinetograms (dot density = 2.5%, dot size = 2 <italic toggle="yes">pixels</italic>, speed = 5 <italic toggle="yes">pixels/frame</italic>). In every sample of the stimulus, a portion of dots move coherently in one of four directions (up, down, right, left), and the rest move completely randomly. The linear classifier is trained to detect the principal direction of motion. The models are evaluated on 3 motion coherence levels (20%, 50%, 100%) and 6 stimulus durations (5, 10, 15, 20, 30, 40 frames).</p></sec><sec id="s7a6" hwp:id="sec-35"><label>A.6</label><title hwp:id="title-40">Code availability</title><p hwp:id="p-79">The codes repository and the pretrained models are available at: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://ventral-dorsal-model.netlify.app/" ext-link-type="uri" xlink:href="https://ventral-dorsal-model.netlify.app/" hwp:id="ext-link-2">https://ventral-dorsal-model.netlify.app/</ext-link>.</p><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Table S1:</label><caption hwp:id="caption-7"><p hwp:id="p-80">An instantiation of the ResNet-1p and ResNet-2p architectures. The dimensions of kernels are shown by <italic toggle="yes">T</italic> × <italic toggle="yes">S</italic><sup>2</sup>, <italic toggle="yes">C</italic> for temporal, spatial, and channel dimensions. The parameters of the two pathways of ResNet-2p are shown separately in red and blue colors. Output sizes are also denoted by <italic toggle="yes">T</italic> × <italic toggle="yes">S</italic><sup>2</sup> for temporal and spatial dimensions. The concatenate layer is only included in ResNet-2p where the two pathway outputs are concatenated along the channel dimensions. The sequence of res3 and res4 residual blocks repeats 4 times. In total, there are 10 residual blocks in both ResNet-1p and ResNet-2p.</p></caption><graphic xlink:href="448989v3_tblS1" position="float" orientation="portrait" hwp:id="graphic-8"/></table-wrap></sec></sec><sec id="s7b" hwp:id="sec-36"><label>B</label><title hwp:id="title-41">Representational similarity between the two pathways of ResNet-2p</title><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Figure S2:</label><caption hwp:id="caption-8"><p hwp:id="p-81">Representational similarity between and within the two pathways of ResNet-2p trained with CPC (top) and action recognition (bottom) loss functions.</p></caption><graphic xlink:href="448989v3_figS2" position="float" orientation="portrait" hwp:id="graphic-9"/></fig></sec><sec id="s7c" hwp:id="sec-37"><label>C</label><title hwp:id="title-42">Representational similarity between the two pathways of ResNet-2p and one pathway of ResNet-1p</title><fig id="figS3" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Figure S3:</label><caption hwp:id="caption-9"><p hwp:id="p-82">Representational similarity between ResNet-1p (columns in the figure) and ResNet-2p (rows in the figure) trained with CPC loss function. The blue and the red pathways in ResNet-2p schematic represent the ventral- and the dorsal-like pathways, respectively. As can be seen in the figure, the lower part of the rectangle has higher values than the upper part showing that ResNet-1p has more similar representations to the ventral-like pathway (blue in ResNet-2p schematic) than the dorsal-like pathway (red in ResNet-2p schematic).</p></caption><graphic xlink:href="448989v3_figS3" position="float" orientation="portrait" hwp:id="graphic-10"/></fig></sec><sec id="s7d" hwp:id="sec-38" hwp:rev-id="xref-sec-38-1"><label>D</label><title hwp:id="title-43">Representational similarity during training</title><p hwp:id="p-83">To gain an insight into the development of the dorsal- and ventral-like pathways, we quantify the similarity of each pathway in ResNet-1p and ResNet-2p to dorsal and ventral areas during training with CPC. For that purpose, we use the maximum representation similarity with VISam and VISlm (the most dorsal and ventral areas, respectively) as the dorsal-score and ventral-score, respectively. The scores are normalized between 0 and 1 and shown in <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure S2</xref>, for both ResNet-1p (left) and ResNet-2p (right). The results suggest that there is a competition between dorsal- and ventral-like representations in the one-pathway architecture. An increase in ventral score leads to a decrease in the dorsal score in ResNet-1p, which is reflected in the correlation value of <italic toggle="yes">r</italic> = −0:54 between the two scores. However, the second pathway in the ResNet-2p model decreases the anti-correlation between the two scores to <italic toggle="yes">r</italic> = −0:18. Indeed, by assigning the two representations to separate pathways in ResNet-2p, the two representations become partially independent. This result suggests that a two-pathway architecture can learn both ventral- and dorsal-like representations because it can partially decouple these two competing forms of representation.</p><p hwp:id="p-84">For 10 random initialization seeds, we do get the dorsal/ventral split in 7 seeds (average D-score = 0.714, average V-score = 0.663). In 1 seed, both pathways are more dorsal-like (D-score = 0.688, V-score = 0.504) and in 2 seeds, more ventral-like (average D-score = 0.620, average V-score = 0.664). Also, the seeds with better dorsal/ventral splits across the two pathways reach higher average predictive performance at the end of training (top-3 accuracy with dorsal/ventral split: 93.88 (1.02), without dorsal/ventral: 91.80 (1.17)).</p><fig id="figS4" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figS4</object-id><label>Figure S4:</label><caption hwp:id="caption-10"><p hwp:id="p-85">Ventral (red) and dorsal (blue) scores during training for the architectures with one (left) and two (right) pathways. <italic toggle="yes">r</italic> shows the correlation coefficient between the changes in dorsal and ventral scores during training.</p></caption><graphic xlink:href="448989v3_figS4" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><fig id="figS5" position="float" orientation="portrait" fig-type="figure" hwp:id="F10" hwp:rev-id="xref-fig-10-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">figS5</object-id><label>Figure S5:</label><caption hwp:id="caption-11"><p hwp:id="p-86">Maximum representational similarity for ResNet-1p, ResNet-2p, and ResNet-4p trained with CPC.For all the areas of mouse visual cortex, increasing the number of pathways from 2 to 4 decreased the representational similarity.</p></caption><graphic xlink:href="448989v3_figS5" position="float" orientation="portrait" hwp:id="graphic-12"/></fig></sec><sec id="s7e" hwp:id="sec-39" hwp:rev-id="xref-sec-39-1"><label>E</label><title hwp:id="title-44">Dorsal-like specialization in the one-pathway model</title><p hwp:id="p-87">Although the one-pathway architecture trained with CPC showed more similar representations with the ventral pathway in mouse visual cortex (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-12" hwp:rel-id="F2">Fig. 2</xref>), we can still ask explicitly if there is any learned dorsal-like specialization in ResNet-1p. To address this question, we take two approaches: (1) we measure the performance of ResNet-1p on the two downstream tasks, CIFAR-10 and RDK, as explained in <xref ref-type="sec" rid="s7a5" hwp:id="xref-sec-34-2" hwp:rel-id="sec-34">section A.5</xref>, and (2) predicting the responses of the artificial neurons in ResNet-1p based on the responses of the neurons of the most ventral (VISlm) and the most dorsal (VISam) areas of mouse visual cortex. For (2), particularly, we take every artificial neuron in ResNet-1p, and fit a linear regression model to the neurons’ responses. We use 10-fold cross-validation to evaluate the linear fits. Neurons are labeled as dorsal-like if the dorsal neurons from VISam are better predictors of that artificial neuron’s responses than VISlm neurons.</p><p hwp:id="p-88">The results of the two approaches are shown in <xref ref-type="table" rid="tblS2" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table S2</xref> below. The linear evaluation results show that ResNet-1p is better than both ventral-like and dorsal-like pathways of ResNet-2p in object categorization, supporting the observation that ResNet-1p representations are more ventral-like. For the motion discrimination task, ResNet-1p is better than the ventral-like pathway of ResNet-2p, but worse than the dorsal-like pathway of ResNet-2p. This observation implies that ResNet-1p has some dorsal-like specialisation, but separating the two pathways in the architecture enhances dorsal-like specialisation in one of the pathways.</p><p hwp:id="p-89">Consistent with the results of the downstream tasks, the linear regression approach also show that around 39% of ResNet-1p neurons are dorsal-like. When compared to the percentage of dorsal-like neurons in each pathway of ResNet-2p, we can see that using an architecture with parallel pathways enhances and isolates ventral and dorsal specializations in each pathway: the dorsal-like pathway has a significantly higher number of dorsal-like neurons compared to the ventral-like pathway and ResNet-1p.</p><table-wrap id="tblS2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/TBLS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tblS2</object-id><label>Table S2:</label><caption hwp:id="caption-12"><p hwp:id="p-90">The percentage of dorsal-like neurons and the performance on downstream tasks (CIFAR-10 object categorization and RDK motion discrimination) for ResNet-1p and each pathway of ResNet-2p</p></caption><graphic xlink:href="448989v3_tblS2" position="float" orientation="portrait" hwp:id="graphic-13"/></table-wrap></sec><sec id="s7f" hwp:id="sec-40" hwp:rev-id="xref-sec-40-1"><label>F</label><title hwp:id="title-45">Comparisons with pretrained high resolution action recognition models</title><p hwp:id="p-91">The ANNs trained on supervised action classification, similar to the CPC-trained models, were trained with low spatial resolution videos (64 × 64; UCF101 dataset) to account for the low spatial resolution of mouse retina. The low resolution of the video data might affect the quality of the learned representations, especially in supervised learning. Therefore, in this section, we compare CPC with an ANN pretrained with higher resolution action classification datasets (112 × 112; Kinetics-400 dataset). We also compare the models with the SlowFast network: a two-stream architecture pretrained on action classification. However, unlike the two-pathway architecture used in our study, the two pathways of the SlowFast network are not identical. By choosing different stride sizes, kernel sizes, and input sampling rates, one pathway (the Slow pathway) was designed for learning representations of more static information, while the other pathway (the Fast pathway) was designed for capturing rapidly changing motion (see [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref>] for details).</p><p hwp:id="p-92">The results of these comparisons are shown in <xref ref-type="fig" rid="figS6" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Figure S6</xref>. First, we can see that the pretrained action classification model (AR (high res)) shows a lower representational similarity to all the brain areas compared to CPC, consistent with the results presented in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Figure 4</xref>. Therefore, the lower performance of the supervised models, as reported in <xref ref-type="sec" rid="s4d" hwp:id="xref-sec-22-1" hwp:rel-id="sec-22">section 4.4</xref>, cannot be explained by the low resolution of the input videos that we used in the training.</p><p hwp:id="p-93">Second, the SlowFast network performs at the same level as the action classification model for the two most ventral areas (VISlm and VISp) and the most dorsal area (VISam). Therefore, training an ANN with two parallel pathways on supervised action classification does not lead to learning ventral- and dorsal-like representations, consistent with the results in <xref ref-type="sec" rid="s4d" hwp:id="xref-sec-22-2" hwp:rel-id="sec-22">section 4.4</xref> and <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Figure 4</xref>. However, the SlowFast network shows the highest representational similarity with two of the visual areas: VISpm (ventral) and VISal (dorsal). We can attribute the superior performance of the SlowFast network, in the case of these two areas, to the inductive biases used in this model. In particular, the temporal sampling rate of the input videos for training the Fast pathway of the SlowFast network was higher than the input sampling rate we used for training our models. While, in this study, we focused on the role of loss functions and number of pathways in the architecture, these results highlight the importance of other architectural hyperparameters in modeling different visual areas. Importantly, these results suggest that different visual areas may be best modeled with different architectural and input hyperparameters.</p><p hwp:id="p-94">The pretrained action recognition model and the SlowFast network were both trained on Kinetics-400 which is a much larger dataset compared to UCF101 that we used in this study (650k samples in Kinetics-400 vs 13k samples in UCF101). Therefore, we should bear in mind that the training dataset is a confounding factor here when comparing the pretrainied models with CPC.</p><fig id="figS6" position="float" orientation="portrait" fig-type="figure" hwp:id="F11" hwp:rev-id="xref-fig-11-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.18.448989v3/FIGS6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figS6</object-id><label>Figure S6:</label><caption hwp:id="caption-13"><p hwp:id="p-95">Maximum representational similarity for models trained on supervised action classification with high resolution videos (AR (high res)), the SlowFast network, and ResNet-1p and ResNet-2p trained with CPC.</p></caption><graphic xlink:href="448989v3_figS6" position="float" orientation="portrait" hwp:id="graphic-14"/></fig></sec></sec></back></article>
