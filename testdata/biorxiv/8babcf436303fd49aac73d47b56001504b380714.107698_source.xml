<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/107698</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;107698v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;107698</article-id><article-id pub-id-type="other" hwp:sub-type="slug">107698</article-id><article-id pub-id-type="other" hwp:sub-type="tag">107698</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Model-free reinforcement learning operates over information stored in working-memory to drive human choices</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Silva Carolina Feher da"><surname>Silva</surname><given-names>Carolina Feher da</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9635-7826</contrib-id><name name-style="western" hwp:sortable="Yao Yuan-Wei"><surname>Yao</surname><given-names>Yuan-Wei</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-9635-7826"/></contrib><contrib contrib-type="author" hwp:id="contrib-3"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0260-2772</contrib-id><name name-style="western" hwp:sortable="Hare Todd A."><surname>Hare</surname><given-names>Todd A.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-0260-2772"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1">
<label>1</label><institution hwp:id="institution-1">Department of General Physics, Institute of Physics, University of São Paulo</institution></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1">
<label>2</label><institution hwp:id="institution-2">State Key Laboratory of Cognitive Neuroscience and Learning and IDG/McGovern Institute for Brain Research, Beijing Normal University</institution></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1">
<label>3</label><institution hwp:id="institution-3">Laboratory for Social and Neural Systems Research, Department of Economics, University of Zurich</institution>,</aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1">
<label>4</label><institution hwp:id="institution-4">Zurich Center for Neuroscience, University of Zurich and ETH</institution></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-02-11T19:21:14-08:00">
    <day>11</day><month>2</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-02-11T19:21:14-08:00">
    <day>11</day><month>2</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-02-11T19:45:16-08:00">
    <day>11</day><month>2</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-02-11T19:45:16-08:00">
    <day>11</day><month>2</month><year>2017</year>
  </pub-date><elocation-id>107698</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-02-10"><day>10</day><month>2</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-02-11"><day>11</day><month>2</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="107698.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/107698v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="107698.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/107698v1/107698v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/107698v1/107698v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Model-free learning creates stimulus-response associations, but are there limits to the types of stimuli it can operate over? Most experiments on reward-learning have used discrete sensory stimuli, but there is no algorithmic reason to restrict model-free learning to external stimuli, and theories suggest that model-free processes may operate over highly abstract concepts and goals. Our study aimed to determine whether model-free learning can operate over environmental states defined by information held in working memory. We compared the data from human participants in two conditions that presented learning cues either simultaneously or as a temporal sequence that required working memory. There was a significant influence of model-free learning in the working memory condition. Moreover, both groups showed greater model-free effects than simulated model-based agents. Thus, we show that model-free learning processes operate not just in parallel, but also in cooperation with canonical executive functions such as working memory to support behavior.</p></abstract><counts><page-count count="24"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-2">Introduction</title><p hwp:id="p-3">Reinforcement learning theory and the computational algorithms associated with it have been extremely influential in the behavioral, biological, and computer sciences. Reinforcement learning theory describes how an agent learns by interacting with its environment [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>]. In a typical reinforcement learning paradigm, the agent selects an action and the environment responds by presenting rewards and taking the agent to the next situation, or state. A reinforcement learning algorithm determines how the agent changes its action selection strategy as a result of experience, with the goal of maximizing future rewards. Depending on how algorithms accomplish this goal, they are classified as model-free or model-based [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">1</xref>]. Model-based algorithms acquire beliefs about how the environment generates outcomes in response to their actions and select actions according to their predicted consequences. By contrast, model-free algorithms generate a propensity to perform, in each state of the world, actions that were more rewarding in previous visits to that environmental state. Model-free reinforcement learning algorithms are of considerable interest to behavioral and biological scientists, in part because they offer a compelling account of the phasic activity of dopamine neurons, but also more generally can explain many observed patterns of behavior in human and non-human animals [<xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>, <xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>, <xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>, <xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>, <xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>, <xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>].</p><p hwp:id="p-4">A key concept in reinforcement learning theory is the environmental state. Typically, empirical tests of reinforcement learning algorithms use discrete sensory stimuli to define environmental states. However, there is no theoretical or algorithmic constraint to define the states of the environment exclusively by sensory stimuli. State definitions may also include the agent’s internal stimuli, such as its memory of past events, thirst or hunger level, or even subjective characteristics such as happiness or sadness [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-3" hwp:rel-id="ref-1">1</xref>]. Thus, model-free reinforcement learning might operate over a wide variety of both external and internal factors.</p><p hwp:id="p-5">Indeed, recent work suggests that model-free learning algorithms can support a large set of cognitive processes and behaviors beyond the formation of habitual response associations with discrete sensory stimuli [<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>, <xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>, <xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>]. For instance, it has been proposed that the model-free system can perform the action of selecting a goal for goal-directed planning [<xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>] or conversely that a model-based decision can trigger a habitual action sequence [<xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>, <xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>, <xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>, <xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>]. Model-free algorithms have also been suggested to gate working memory [<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>]. However, many of these important theoretical proposals about model-free algorithms have not been directly tested empirically.</p><p hwp:id="p-6">Here, we determine the ability of model-free reinforcement learning algorithms to operate over states defined by information held in working memory, an internal state. Specifically, we use an experimental paradigm and computational modeling framework designed to dissociate model-free from model-based influences on behavior [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>] to test if temporally separated sequences of individually uninformative cues can drive model-free learning and behavior. If an agent can store the elements of a temporal sequence in its memory to form a unique and predictive cue and use the memorized information as the state definition, then, theoretically, it can use model-free algorithms to learn the associations between a specific sequence of <italic toggle="yes">individually uninformative cues</italic> and action outcomes [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>].</p><p hwp:id="p-7">Our approach has several important facets. First, we use an experimental paradigm that allows us to determine not only if our participants learn from information in working memory, but also whether that learning is supported by model-based or model-free algorithms. Second, the cues in our temporal sequences are individually uninformative; in other words, any single cue in isolation provides no information about which response is correct. It is well-known that model-free algorithms can shift response associations to the earliest occurring predictor of the correct response in a temporal sequence of informative cues and can integrate predictive information across individual cues. Neither of these mechanisms is possible in our paradigm because the individual cues themselves contain no information about the previous or subsequent cues or which response is best.</p><p hwp:id="p-8">Temporal pattern learning is a fundamental and early developing human cognitive ability. It allows people to form predictions about what will happen from what has happened and select their actions accordingly. Humans can learn patterns both explicitly and implicitly in the absence of specific instructions or conscious awareness [<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]. Moreover, they can do so as early as two months of age [<xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>]. In fact, people identify patterns even when, in reality, no pattern exists [<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>]. These empirical results together with the theoretical potential for model-free learning to operate over internal stimuli suggest that temporal pattern learning could be supported by model-free processes. However, to date, studies of reinforcement learning and decision making have focused primarily on tasks in which the relevant stimuli are presented simultaneously just prior to or at the time of decision-making, or on implicit motor sequence learning, wherein participants learn a sequence of movements automatically, without full awareness (for instance, <xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>, <xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>, <xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>, <xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>, <xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>). Thus, the degree to which model-free processes do in fact operate over temporal sequences or any other information stored in working memory has not yet been directly tested and compared with model-free learning from traditionally employed external, static environmental cues.</p><p hwp:id="p-9">Here, we directly test whether model-free processes can access and learn from information stored in working memory. We adapted a decision-making paradigm originally developed by Daw et al. [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref>] that can behaviorally dissociate the influence of model-free and model-based learning on choice. The task was performed by two groups of human participants either in a simultaneous condition (i.e. static and external), wherein visual stimuli were presented simultaneously, or in a sequential condition, wherein the same visual stimuli were presented as a temporal sequence that required working memory processing. We also simulated a series of experiments in which artificial model-based agents whose behavioral processes we determined were compared to the human participants. Our analysis indicates that our temporal sequences, and consequently information stored in working memory, can trigger model-free learning. Moreover, we found no evidence that the degree to which model-free learning influenced behavior differed between conditions in which environmental states were defined by external sensory stimuli compared to those defined by internal representations stored in working memory. Our findings support the theoretical proposition that model-free learning can act on stimuli internally represented in working memory as well as on external ones.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-3">Results</title><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-4">Determining model-free and model-based influences on choice behavior</title><p hwp:id="p-10">Forty-one young human participants completed a behavioral task adapted from Daw et al. [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">17</xref>]. In our task, participants began each trial in a randomly selected initial state represented by one of four possible sequences of two symbols: AA, AB, BA, or BB (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>). At this initial state, participants chose one of two possible actions: going left or going right. They were then taken to one of two possible final states, the blue state or the pink state. If they had gone left, they were taken with 0.8 probability to the final state given by the rule AA → blue, AB → pink, BA → pink, BB → blue or with 0.2 probability to the other final state. If they had gone right, they were taken with 0.8 probability to the final state <italic toggle="yes">not</italic> given by the previous rule or with 0.2 probability to the other final state. The common (most probable) transitions between the initial and final states are shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>. To predict the final state accurately, participants had to know both elements of the sequence. If they knew only one, the final state might have been either blue or pink with 0.5 probability and they would not be able to perform above chance. This feature is key and separates our work from others in which each element of a sequence is predictive on its own.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;107698v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-11">Timelines of events in a trial. The two symbols that represent the initial state are presented simultaneously in the simultaneous condition (left) and separately as a temporal sequence in the sequential condition (right). In this example, AB is the initial state. The simultaneous condition participant goes to the pink final state and receives a reward (signaled by the green symbol). The sequential condition participant goes to the blue final state and does not receive a reward (signaled by the black X symbol).</p></caption><graphic xlink:href="107698_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;107698v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-12">Common state transitions in the behavioral task’s model. These graphics highlight the uninformative nature of each single element (i.e. A or B symbols) in the simultaneous or sequential cues. Knowledge of only the first or final element of the combined cue provides no indication of how likely the right and left responses are to lead to a specific state.</p></caption><graphic xlink:href="107698_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-13">One of the final states delivered a monetary reward with 0.7 probability and the other with 0.3 probability. The optimal strategy was to always select the action that led with 0.8 probability to the final state with 0.7 reward probability. Initially, participants were instructed to learn the common transitions between the initial and final states in the absence of rewards. They were told that each final state might be rewarded with different probabilities, but not what the probabilities were nor that they were fixed. The task comprised 250 trials and participants received the total reward they obtained at the end.</p><p hwp:id="p-14">Twenty-one participants were randomly allocated to a simultaneous condition and twenty to a sequential condition (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref>). In the simultaneous condition, both symbols that represented the initial state were displayed simultaneously on the screen. In the sequential condition, each symbol was displayed consecutively by itself, as a temporal sequence. The specific objective of this study was to determine if participants in the sequential condition could use states represented in working memory to learn the task in a model-free way or if their learning was necessarily model-based. The simultaneous condition is already known to support model-free learning as well as model-based learning [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-4" hwp:rel-id="ref-17">17</xref>, <xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>, <xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>, <xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>, <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>]. We thus sought to determine the difference between the standard simultaneous and working-memory dependent sequential conditions.</p><p hwp:id="p-15">The two-stage task we used can differentiate between model-free and model-based learning because algorithms that implement them make different predictions about how a reward received in a trial impacts a participant’s choices in subsequent trials. The SARSA (λ = 1) model-free algorithm learns this task by strengthening or weakening associations between initial states and initial-state actions depending on whether the action is followed by a reward or not [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-4" hwp:rel-id="ref-1">1</xref>]. Therefore, it simply predicts that an initial-state action that resulted in a reward is more likely to be repeated in the next trial with the same initial state [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-5" hwp:rel-id="ref-17">17</xref>]. On the other hand, the model-based algorithm considered in this study uses an internal model of the task’s structure to determine the initial-state choice that will most likely result in a reward [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-6" hwp:rel-id="ref-17">17</xref>]. To this end, it considers which final state, pink or blue, was most frequently rewarded in recent trials and selects the initial-state action, left or right, that will most likely lead there. Therefore, the model-free algorithm predicts that the participant will choose the mostly frequently rewarded <italic toggle="yes">action</italic> in past trials with the same initial state, while the model-based algorithm predicts that the participant will choose the action with the highest probability of leading to the mostly frequently rewarded <italic toggle="yes">final state</italic> in past trials, regardless of their initial states.</p><p hwp:id="p-16">The model-free and model-based algorithms thus generate different predictions about the <italic toggle="yes">stay probability</italic>, which is the probability that in the next trial with the same initial state the participant will stay with their previous choice and take the same initial-state action. For instance, if in a given trial whose initial state was AA the participant chose left, and in the next trial with AA as the initial state the participant also chose left, this was considered a stay. The model-free prediction is that the stay probability will increase if the previous trial with the same initial state was rewarded and decrease if it was not. The model-based prediction, on the other hand, depends on the transition structure of the task and how the estimated reward probabilities of the two final states have changed since the previous trial with the same initial state (see Methods for a detailed description of how model-based predictions were calculated).</p><p hwp:id="p-17">We simulated model-free and model-based agents performing this task for comparison with the behavior of human participants in each condition. In all cases, we analyzed the data using Bayesian hierarchical logistic regression analyses. The correspondence between theoretical predictions of the model-free and model-based algorithms and choices of the simulated agents are shown in the top row of <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>. The correspondence between theoretical predictions of the model-free and model-based algorithms and choices of the human participants in each experimental condition are shown in the bottom row of <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3</xref>.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;107698v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-18">Stay probabilities for simulated agents and human participants as a function of mode-free and model-based predictions. The bar graphs show the choice probabilities derived from the logistic regressions as a function of model-free (separated along the <italic toggle="yes">x</italic>-axis) and model-based predictions (indicated by the color of the bars). The four panels demonstrate the behavior of <bold>A</bold>) model-free simulations (<italic toggle="yes">N</italic> = 10,000), <bold>B)</bold> model-based simulations (<italic toggle="yes">N</italic> = 10,000), <bold>C)</bold> human participants in the simultaneous condition (<italic toggle="yes">N</italic> = 21) and <bold>D</bold>) human participants in the sequential condition (<italic toggle="yes">N</italic> = 20). Error bars on the data from human participants represent the 95% highest density interval.</p></caption><graphic xlink:href="107698_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-19">In addition to examining the stay choice probabilities, we directly tested the degree to which the human participants’ and simulated agents’ choices were influenced by model-based and model-free signals. The coefficients of these logistic regression analyses are shown in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>. The positive value of the intercept <italic toggle="yes">β</italic><sub>0</sub> indicates that the stay probabilities tended to be above 0.5, i.e., simulated agents and human participants were more likely to repeat their previous choice than switch to the other choice in the next trial with the same initial state. This is also visible in the stay probabilities shown in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3</xref>. The coefficients for the regressions on the simulated agents’ choices show the expected pattern with the model-free and model-based coefficients primarily determining behavior for the model-free and model-based agents, respectively (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4A</xref>) and differing substantially between agent types (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4C</xref>).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7 xref-fig-4-8 xref-fig-4-9 xref-fig-4-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;107698v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-20">The relative effects of model-free and model-based learning on choice behavior. A) The regression coefficients from a logistic regression on stay vs switch choices for the model-free (blue; <italic toggle="yes">N</italic> = 10,000) and the model-based simulations (purple; <italic toggle="yes">N</italic> = 10,000). B) The difference between model-based and model-free simulation coefficients (i.e. red minus blue from panel A). <bold>C</bold>) Logistic regression coefficients from the same model used for panel A, but here estimated on choices from the simultaneous (blue; <italic toggle="yes">N</italic> = 21) and sequential condition (purple; <italic toggle="yes">N</italic> = 20) participants. D) The difference between sequential and simultaneous condition participants’ coefficients (i.e. red minus blue from panel C). In all panels, <italic toggle="yes">β</italic><sub>0</sub> is the logistic regression’s intercept, <italic toggle="yes">β</italic><sub><italic toggle="yes">mb</italic></sub> is the model-based coefficient, <italic toggle="yes">β</italic><sub><italic toggle="yes">mf</italic></sub> is the model-free coefficient, and <italic toggle="yes">β</italic><sub><italic toggle="yes">mb</italic>×<italic toggle="yes">mf</italic></sub> is the coefficient of the interaction between the model-based and model-free effects. Error bars on the data from human participants represent the 95% highest density interval.</p></caption><graphic xlink:href="107698_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-21">In the human participants, behavior was influenced by both model-based and model-free processes regardless of whether the states were defined by external sensory cues or internal working-memory representations. The model-based and model-free coefficients, <italic toggle="yes">β</italic><sub><italic toggle="yes">mb</italic></sub> and <italic toggle="yes">β</italic><sub><italic toggle="yes">mf</italic></sub>, were positive for both the simultaneous and sequential conditions with 0.999 posterior probability (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4B</xref>). The model-based coefficient was 0.32 (95% highest density interval [0:18; 0:47]) for the simultaneous condition and 0.25 (95% HDI [0:10; 0:40]) for the sequential condition, and the model-free coefficient was 0.41 (95% HDI [0:30; 0:54]) for the simultaneous condition and 0.45 (95% HDI [0:33; 0:58]) for the sequential condition. The differences between the sequential and simultaneous conditions were –0:07 (95% HDI [–0:28; 0:14]) for the model-based coefficient and 0.04 (95% HDI [–0:13; 0:21]) for the model-free coefficient. The posterior probability that the model-free coefficient is smaller in the sequential group than in the simultaneous group is 0.32, and the posterior probability that the model-based coefficient is greater in the sequential group than in the simultaneous group is 0.24 (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Figure 4D</xref>). Thus, we find no evidence that sequentially presented, working-memory-dependent state cues shift the balance of model-based and model-free effects on choice behavior compared to traditional, static, external cues.</p><p hwp:id="p-22">The model-based predictions in our two-stage decision task differ from those reported in previous work using similar tasks. In the version of the two-stage task used by Daw et al. [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-7" hwp:rel-id="ref-17">17</xref>], the model-based prediction is that the heights of the two orange bars should be (nearly) equal to one another and that the heights of the two green bars should be (nearly) equal as well (note, that the precise prediction depends on the exact parameterization of the model). However, in our task the model-based prediction includes a reward effect. Consequently, we find that the stay probabilities in the model-based agent simulations are influenced by the outcome of the most recent trial with the same initial state as can be seen in the differences in magnitude between the two inner, orange bars as well as the two outer, green bars in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3B</xref>. Specifically, if the previous matching-state trial was rewarded, then the stay probabilities are greater than if it was not rewarded. This reward effect in the model-based choices is similar to a model-free effect, but not identical because the model-based value updating procedure incorporates the transition probabilities while the model-free algorithm does not. However, the reward-effect does lead to model-free-like choice patterns in the data that result in a small, but significant model-free coefficient in the logistic regressionson model-based agents’ choices (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Figure 4A</xref>). Therefore, small model-free-like patterns in the stay probabilities do not necessarily indicate the influence of model-free learning, because we know the model-based agents do not use this learning algorithm. We directly address the potential for spurious effects mimicking the influence of a model-free algorithm in our human participants in the working-memory-dependent sequential condition in the following paragraphs.</p></sec><sec id="s2b" hwp:id="sec-4"><label>2.2</label><title hwp:id="title-5">Direct comparisons between human participants and simulated model-based agents</title><p hwp:id="p-23">Our goal was to test the hypothesis that working-memory-dependent, temporal patterns can be learned through a model-free process in humans. Given that our model-based simulations showed a reward effect that shared some properties with a model-free learning process, we sought to determine if the same results obtained by the human participants in the sequential condition, including the estimated model-free effect, could have been generated through the use of a model-based algorithm alone. Despite the fact that we find no evidence for differences in behavior between participants in the sequential and simultaneous conditions, this additional test is important because as <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figures 3</xref> and <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-7" hwp:rel-id="F4">4</xref> show, the model-based simulated agents exhibited behavior that mimicked a model-free effect even though they operated solely on the basis of a model-based algorithm by design.</p><p hwp:id="p-24">This raises the question, could purely model-based agents exhibit a model-free effect as large as the participants in the sequential condition? To this end, we fitted the model-based algorithm to the sequential condition results using a Bayesian hierarchical model. We then simulated 10,000 experiments in which we first created behavior for 20 simulated model-based agents (replacing the 20 sequential condition human participants) and then combined those data with that from the 21 human participants in the simultaneous condition and estimated the same hierarchical logistic regression on stay/switch choices described above and summarized in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-8" hwp:rel-id="F4">Figure 4</xref>. These 10,000 regressions give us a measure of what the coefficients in the sequential condition participants would be if they were purely model-based.</p><p hwp:id="p-25">We found that while the simulated purely-model-based (PMB) agents showed a level of model-based influence comparable to participants in the sequential condition, the degree of model-free influence in PMB agents was substantially lower. The mean value of the model-based coefficient, <italic toggle="yes">β</italic><sub><italic toggle="yes">mb</italic></sub>, was 0.23 (95% HDI [0:05; 0:49]), which, as expected, is very close to the mean value of 0.25 from the sequential participants’ behavior. Likewise, the mean difference across simulations between the PMB agents and the participants in the simultaneous condition for <italic toggle="yes">β</italic><sub><italic toggle="yes">mb</italic></sub> was –0:09 (95% HDI [–0:28; 0:15]), similar to the –0:07 value for the difference between the sequential and simultaneous conditions in humans. In contrast, the mean value of the model-free coefficient, <italic toggle="yes">β</italic><sub><italic toggle="yes">mf</italic></sub>, in the simulated agents was 0.15 (95% HDI [0:07; 0:23]), and it was smaller than 0.45, the mean value obtained for the sequential condition, in more than 99.9% (in fact all 10,000) of the simulated experiments. Furthermore, the mean difference between the PMB agents and the participants in the simultaneous condition for <italic toggle="yes">β</italic><sub><italic toggle="yes">mf</italic></sub> (i.e. the difference corresponding to <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-9" hwp:rel-id="F4">Figure 4D</xref>) was –0:26 (95% HDI [–0:34; –0:18]), and more than 99.9% (in fact all 10,000) of simulated experiments yielded a difference for <italic toggle="yes">β</italic><sub><italic toggle="yes">mf</italic></sub> smaller than 0.04, the observed difference between human participants in the sequential and simultaneous conditions. In summary, the model-free coefficient observed in the sequential condition is three times the size one would expect to see from a purely model-based agent, which strongly suggests that the observed effect is due to a true model-free influence and not mimicked by the reward-effect.</p></sec></sec><sec id="s3" hwp:id="sec-5"><label>3</label><title hwp:id="title-6">Discussion</title><p hwp:id="p-26">In this study, we empirically tested the hypothesis that human participants can develop model-free associations between temporal sequences of stimuli stored in working memory and a motor response. To that end, we developed a behavioral task based on a previous decision-making paradigm that can determine the model-free and model-based influences on choice [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-8" hwp:rel-id="ref-17">17</xref>]. The participants in the simultaneous condition performed this task with the two visual symbols presented together simultaneously and those in the sequential condition performed it with the same two visual symbols presented as a temporal sequence that had to be held in working memory. The model-free effect estimated for the sequential condition was similar to the one estimated for the simultaneous condition and higher than that predicted by a purely model-based algorithm. Our results suggest that both model-based and model-free learning influenced the participants’ choices whether they saw the entire set of stimuli at once or saw each stimulus by itself at separate times. Our study thus provides experimental support to proposed model-free algorithms of temporal pattern learning [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>] and the view that model-free learning and habituation can be triggered by external or internal stimuli [<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>, <xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>, <xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref>]</p><p hwp:id="p-27">A key element of our experimental paradigm is that the individual symbols within each temporal sequence convey no information about the best response in isolation. This fact rules out the possibility that the sequential condition’s model-free effect is due to an association between a single symbol in the sequence and a response rather than one between the entire sequence and a response. Each sequence element is completely uninformative by itself: it cannot predict reward delivery above chance. Therefore, the task cannot be learned by simple stimulus-response associations with individual symbols in the temporal sequence.</p><p hwp:id="p-28">Model-free learning processes support habit formation, and thus our results suggest that stimuli stored in working memory can trigger habitual responses. To the best of our knowledge, no study has yet tested for habituation to temporal sequences directly, using procedures such as contingency degradation or outcome devaluation. Although two-stage choice tasks similar to the one we use here have been reported to share construct validity with outcome devaluation measures of habitual responding [<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>], direct tests of outcome devaluation and contingency degradation following temporal sequence learning are still needed. If such additional tests show positive evidence for habituation, this would indicate that habits can be triggered by internally generated stimuli as well as by external ones. Conversely, if no evidence is found for habituation to temporal sequences, this would indicate that model-free learning processes can use internal stimuli, but do not necessarily produce habits. Experimental evidence already suggests that habits are not exclusively learned in a model-free way [<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]; it may also be true that habituation involves additional mechanisms beyond the model-free caching of state-action-reward contingencies. Our study also raises the question of which neural systems are commonly versus distinctly recruited in order to learn from stimuli represented in working memory (e.g. temporal sequences) compared to purely external stimuli in a reinforcement learning task. While numerous studies have investigated the neural systems mediating reinforcement learning over externally presented stimuli (see <xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref> for a review), to date, only a single study has investigated brain activity involved in temporal pattern learning using fMRI [<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref>]. However, the sequence of events in that study was random, and any pattern that occurred was spurious. Moreover, participants were required to respond to the stimuli instead of predicting them, and might thus be implicitly learning a motor sequence. It remains to be determined what brain regions support explicit learning from temporal sequences, or other stimuli held in working memory, and to what degree these systems overlap with those shown to underlie learning from external environmental cues. In conclusion, we have presented experimental evidence that temporal pattern learning, and consequently learning from internal stimuli held in working memory, can be model-free.</p><p hwp:id="p-29">Our study has helped delineate the contexts that support model-free learning—a subject of current debate. Temporal pattern learning is a fundamental aspect of human cognition and model-free learning and habit formation are subjects of immediate relevance for research on typical learning as well as for the study of neuropsychiatric disorders ranging from addiction, obsessive-compulsive disorder, and Tourette syndrome to anxiety disorders and major depression [<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>]. It is thus important to continue investigating temporal pattern learning, including whether the model-free learning of temporal sequences produces outcome-insensitive, habitual responses and how such learning is implemented in the brain.</p></sec><sec id="s4" hwp:id="sec-6"><label>4</label><title hwp:id="title-7">Methods</title><sec id="s4a" hwp:id="sec-7"><label>4.1</label><title hwp:id="title-8">Participants</title><p hwp:id="p-30">Forty-one healthy young adults participated in the experiment, 21 (13 female) randomly assigned by a random number generator to the simultaneous condition and 20 (13 female) to the sequential condition. The inclusion criterion was speaking English and no participants were excluded from the analysis. The sample size was chosen by the precision for research planning method [<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>, <xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>], by comparing the estimated differences between participant groups in the logistic regression analysis with those between model-free and model-based simulated agents.</p><p hwp:id="p-31">The experiment was conducted in accordance with the Zurich Cantonal Ethics Commission’s norms for conducting research with human participants, and all participants gave written informed consent.</p></sec><sec id="s4b" hwp:id="sec-8"><label>4.2</label><title hwp:id="title-9">Task</title><p hwp:id="p-32">The task’s state transition model defines four possible initial states, which were randomly selected with uniform distribution in each trial and represented by four different stimuli, each composed of two symbols: AA, AB, BA, or BB. At the initial state, two actions were available to the participant: pressing the left or the right arrow keys. By pressing one of the keys, the participant was taken to a final state, which might be either the blue state or the pink state. If the left arrow key was pressed, the participant was taken to the final state given by the rule AA → blue, AB → pink, BA → pink, BB → blue with 0.8 probability or to the other state with 0.2 probability; if the right arrow key was pressed, the participant was taken to the final state not given by the previous rule with 0.8 probability or to the other state with 0.2 probability. There was no choice of action at the final state, but participants were required to make a button press to potentially earn the reward. Each final state was rewarded according to an associated probability, which was 0.7 for one state and 0.3 for the other. The highest reward probability was associated with the blue state for half of the participants and to the pink state for the other half. Participants were told that each final state might be rewarded with different probabilities, but not what the probabilities were nor that they were fixed.</p><p hwp:id="p-33">In contrast with our task design, in which the final states’ reward probabilities were fixed, in the original task design proposed by Daw et al. [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-9" hwp:rel-id="ref-17">17</xref>] the reward probabilities slowly drifted over time, because those authors were interested in the trade-off between model-based and model-free mechanisms, which is assumed to happen on the basis of their relative uncertainties. In this study we were interested instead in testing if model-free learning of temporal patterns is possible and keeping the task environment stable helps making the model-free associations stronger and more likely to influence choice [<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>,<xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>].</p><p hwp:id="p-34">Participants were initially instructed to learn the common transitions between the initial and the final states in the absence of reward. Participants then performed the task defined by the model above in the simultaneous or sequential condition. Half of the participants were randomly allocated to the simultaneous condition and the other half to the sequential condition (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure1</xref>). In the simultaneous condition, both symbols that define the initial state were displayed simultaneously on the screen for 3 seconds. In the sequential condition, each symbol is an element of a sequence and each element was presented for 1 second, but never conjointly, and with a 1-second delay (blank screen) in between. Two triangles pointing left and right then appeared and the participant was given 2 seconds to make a decision about whether to press the left or the right arrow keys; if they did not press any keys, the word SLOW was displayed for 1 second, and the trial was aborted and omitted from analysis. A blue or pink rectangle appeared immediately afterward, indicating the final state. The participant then pressed the up-arrow key and, if the final state was rewarded, a green dollar sign appeared on the screen for 2 seconds; otherwise, a black X appeared for 2 seconds. The task comprised 250 trials, with a break every 50 trials, and participants received the total reward they obtained by the end of the task (0.18 CHF per reward).</p></sec><sec id="s4c" hwp:id="sec-9"><label>4.3</label><title hwp:id="title-10">Model-free algorithm</title><p hwp:id="p-35">The SARSA model-free algorithm with replacing eligibility traces [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-5" hwp:rel-id="ref-1">1</xref>, <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-10" hwp:rel-id="ref-17">17</xref>] was used to simulate model-free learning agents. For each action <italic toggle="yes">a</italic> and state <italic toggle="yes">s</italic>, it estimated the value <italic toggle="yes">Q</italic>(<italic toggle="yes">s,a</italic>) of performing that action in that state. The task’s initial states <italic toggle="yes">s</italic><sub><italic toggle="yes">i</italic></sub> were AA, AB, BA, and BB, and the actions <italic toggle="yes">a</italic><sub><italic toggle="yes">i</italic></sub> available at the initial states were <italic toggle="yes">left</italic> and <italic toggle="yes">right</italic>. The final states were <italic toggle="yes">pink</italic> and <italic toggle="yes">blue</italic>, and the only action <italic toggle="yes">a</italic><sub><italic toggle="yes">f</italic></sub> available at those states was <italic toggle="yes">up</italic>. The initial value of <italic toggle="yes">Q</italic>(<italic toggle="yes">s,a</italic>) for every state and action was 0.5. In each trial <italic toggle="yes">t</italic>, the simulated agent at the initial state <italic toggle="yes">s</italic><sub><italic toggle="yes">i</italic></sub> chose <italic toggle="yes">left</italic> as its initial-state action with probability <italic toggle="yes">p</italic><sub><italic toggle="yes">left</italic></sub> and <italic toggle="yes">right</italic> with probability 1 – <italic toggle="yes">p</italic><sub><italic toggle="yes">left</italic></sub>, according to the following equation:</p><disp-formula id="eqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="107698_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula><p hwp:id="p-36">Where <italic toggle="yes">β</italic> &gt; 0 is an inverse temperature parameter that determines the algorithm’s propensity to choose the option with the highest estimated value. After the final state <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub> was observed and a reward <italic toggle="yes">r</italic> ∈ {0, 1} was received, state-action values were updated according to the following equations:</p><disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="107698_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives>
</disp-formula><disp-formula id="eqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="107698_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula><p hwp:id="p-37">where 0 ≤ <italic toggle="yes">α</italic><sub>1</sub>, <italic toggle="yes">α</italic><sub>2</sub>, λ ≤ 1 are parameters: <italic toggle="yes">α</italic><sub>1</sub> is the initial learning rate, <italic toggle="yes">α</italic><sub>2</sub> is the final learning rate, and λ is the eligibility trace [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-6" hwp:rel-id="ref-1">1</xref>, <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-11" hwp:rel-id="ref-17">17</xref>].</p><p hwp:id="p-38">In the special case where λ = 1, the update of initial state-action values becomes</p><disp-formula id="eqn4" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="107698_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula><p hwp:id="p-39">that is, the estimated values of choosing <italic toggle="yes">left</italic> and <italic toggle="yes">right</italic> in each initial state are updated independently of the final state’s estimated value. Thus, SARSA (λ = 1) ignores the identity of the final state when making initial-state decisions, and an initial-state action that resulted in a reward will necessarily lead to a higher stay probability when the respective initial state recurs. This is true even if the action will probably lead to the final state with the lowest value.</p></sec><sec id="s4d" hwp:id="sec-10"><label>4.4</label><title hwp:id="title-11">Model-based algorithm</title><p hwp:id="p-40">In simulations of model-based agents [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-12" hwp:rel-id="ref-17">17</xref>], values were assigned to initial-state actions and to final states. The value <italic toggle="yes">V</italic> of a final state <italic toggle="yes">s</italic> ∈ {<italic toggle="yes">pink, blue</italic>} in the first trial <italic toggle="yes">t</italic> = 1 was <italic toggle="yes">V</italic> (<italic toggle="yes">s</italic>, 1) = 0.5. An initial-state choice <italic toggle="yes">c</italic> ∈ {<italic toggle="yes">left, right</italic>} in trial <italic toggle="yes">t</italic> had a value <italic toggle="yes">V</italic> given by</p><disp-formula id="eqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="107698_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula><p hwp:id="p-41">where Pr(<italic toggle="yes">c</italic> → <italic toggle="yes">s</italic>) is the probability that choosing <italic toggle="yes">c</italic> will lead to the final state <italic toggle="yes">s</italic>, which might be 0.8 or 0.2 according to the task’s transition model. The value of an initial-state choice can thus be understood as the expected value of the final state the agent will go to after making that choice. If <italic toggle="yes">V</italic> (<italic toggle="yes">left, t</italic>) &gt; <italic toggle="yes">V</italic> (<italic toggle="yes">right, t</italic>), the agent was more likely to choose left and vice-versa.</p><p hwp:id="p-42">In each trial <italic toggle="yes">t</italic>, the agent’s initial state action was <italic toggle="yes">left</italic> with probability <italic toggle="yes">p</italic><sub><italic toggle="yes">left</italic></sub> and <italic toggle="yes">right</italic> with probability 1 − <italic toggle="yes">p</italic><sub><italic toggle="yes">left</italic></sub>, given by</p><disp-formula id="eqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="107698_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula><p hwp:id="p-43">where <italic toggle="yes">β</italic> is an inverse temperature parameter. After the agent made its initial-state choice and went to a final state <italic toggle="yes">s</italic>, that final state’s value was updated according to the following equation:</p><disp-formula id="eqn7" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="107698_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula><p hwp:id="p-44">where <italic toggle="yes">r</italic>(<italic toggle="yes">t</italic>) ∈ {0, 1} indicates if the agent received a reward and 0 ≤ <italic toggle="yes">α</italic> ≤ 1 is a learning-rate parameter of the model. The value of a final state is thus the moving average of the rewards received in that state.</p><sec id="s4d1" hwp:id="sec-11"><label>4.4.1</label><title hwp:id="title-12">Model-based predictions</title><p hwp:id="p-45">Our method of determining model-based predictions for the stay probability was different from the method used by Daw et al. [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-13" hwp:rel-id="ref-17">17</xref>]. In that study, there was only one initial state and the model-based and model-free algorithms predicted how the stay probability would change from one trial to the next. The present study’s task, on the other hand, had four initial states and the model-free algorithm made predictions about how rewards would affect the participant’s choices from one trial to the next trial <italic toggle="yes">with the same initial state</italic>, which is not necessarily the next trial. We therefore had to devise an alternative method of calculating the model-based predictions.</p><p hwp:id="p-46">Our method relies directly on how the model-based algorithm estimates the reward probabilities of the initial-state choices, which either increase or decrease from one trial to the next with the same initial state depending on what happened, and was therefore learned about reward probabilities, in the intervening trials. If the participant’s initial-state choice in a trial <italic toggle="yes">t</italic><sub>1</sub> was <italic toggle="yes">left</italic>, for instance, the model-based prediction was that in a future trial <italic toggle="yes">t</italic><sub>2</sub> with the same initial state the stay probability should increase if <italic toggle="yes">V</italic> (<italic toggle="yes">left</italic>,<italic toggle="yes">t</italic><sub>2</sub>) − <italic toggle="yes">V</italic> (<italic toggle="yes">right</italic>; <italic toggle="yes">t</italic><sub>2</sub>) &gt; <italic toggle="yes">V</italic> (<italic toggle="yes">left</italic>, <italic toggle="yes">t</italic><sub>1</sub>) − <italic toggle="yes">V</italic>(<italic toggle="yes">right</italic>, <italic toggle="yes">t</italic><sub>1</sub>) and decrease otherwise. The model-based predictions depended on the parameter <italic toggle="yes">α</italic>. The data analysis results were obtained by setting <italic toggle="yes">α</italic> = 0.4, as this was the mean value that Daw et al. [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-14" hwp:rel-id="ref-17">17</xref>] found in their experiment by fitting to their experimental data an expanded reinforcement learning model that combines model-based and model-free learning. For comparison, we tried other values for <italic toggle="yes">α</italic>, but the analysis results did not vary significantly.</p></sec></sec><sec id="s4e" hwp:id="sec-12"><label>4.5</label><title hwp:id="title-13">Data analysis by logistic regression</title><p hwp:id="p-47">For each human participant or simulated agent, we calculated the stay probability as a function of model-free and model-based predictions. In each trial, if the human participant or simulated agent chose an action that was the same as that chosen in the previous trial with the same initial state, this was considered a stay. The four initial-state choices following the first occurrence of an initial state were not analyzed. The remaining initial-state choices were coded as the random variable <italic toggle="yes">y</italic> and classified as a stay (<italic toggle="yes">y</italic> = 1) or not a stay (<italic toggle="yes">y</italic> = 0).</p><p hwp:id="p-48">We then analyzed the resulting data using a hierarchical logistic regression model whose parameters were estimated through Bayesian computational methods. The dependent variable was <italic toggle="yes">p</italic><sub>stay</sub>, the stay probability for a given trial, and the independent variables were <italic toggle="yes">x</italic><sub><italic toggle="yes">mf</italic></sub>, which indicated what the model-free algorithm predicted about <italic toggle="yes">p</italic><sub>stay</sub> (+1 if it predicted an increase-1 if it predicted a decrease), <italic toggle="yes">x</italic><sub><italic toggle="yes">mb</italic></sub>, which indicated what the model-based algorithm predicted about <italic toggle="yes">p</italic><sub>stay</sub> (+1 if it predicted an increase, −1 if it predicted a decrease), and the interaction between the two. Thus, for each participant, we determined a four-dimensional vector <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="107698_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> whose components were the <italic toggle="yes">β</italic> coefficients of the following equation:</p><disp-formula id="eqn8" hwp:id="disp-formula-8">
<alternatives hwp:id="alternatives-9"><graphic xlink:href="107698_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives></disp-formula><p hwp:id="p-49">The distribution of <italic toggle="yes">y</italic> was Bernoulli(<italic toggle="yes">p</italic><sub>stay</sub>). The distribution of the <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="107698_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> vectors was <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="107698_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> if the participant was in the simultaneous condition and <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="107698_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> if the participant was in the sequential condition; in other words, the group means for each <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="107698_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> were allowed to vary independently. The parameters of the <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="107698_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> distribution were given vague prior distributions based on preliminary analyses—the <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="107698_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> vectors’ components were given a <italic toggle="yes">N</italic>(<italic toggle="yes">μ</italic> = 0, σ<sup>2</sup> = 25) prior, and the <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="107698_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> vector’s components were given a Half-normal(0, 25) prior. Other vague prior distributions for the model parameters were tested and the results did not change significantly.</p><p hwp:id="p-50">To obtain parameter estimates from the model’s posterior distribution, we coded the model into the Stan modeling language version 2.14.0 [<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>, <xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>] and used the PyStan Python package [<xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>] to obtain 100,000 samples of the joint posterior distribution from four chains of length 50,000 (warmup 25,000). Convergence of the chains was indicated by <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-17"><inline-graphic xlink:href="107698_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> for all parameters. The minimum effective sample size for the parameters of interest <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-18"><inline-graphic xlink:href="107698_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula>, and <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-19"><inline-graphic xlink:href="107698_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> was 31785.</p></sec><sec id="s4f" hwp:id="sec-13"><label>4.6</label><title hwp:id="title-14">Fitting of the algorithms to experimental data</title><p hwp:id="p-51">For comparison with the participant data, we fitted the SARSA model-free algorithm and the model-based algorithm to the experimental data and generated replicated data using the fitted parameters. The parameters were obtained by fitting both algorithms to all participants (to generate <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figures 3</xref> and <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-10" hwp:rel-id="F4">4</xref>) and the model-based algorithm to the participants in the sequential condition (to perform the simulated experiments). To that end, we used a Bayesian hierarchical model, which allowed us to pool data from all participants to improve individual parameter estimates.</p><p hwp:id="p-52">The parameters of the model-based algorithm for the <italic toggle="yes">i</italic>th participant were <italic toggle="yes">α</italic><sup><italic toggle="yes">i</italic></sup> and <italic toggle="yes">β</italic><sup><italic toggle="yes">i</italic></sup>. They were given a Beta(<italic toggle="yes">a</italic><sub>α</sub>, <italic toggle="yes">b</italic><sub>α</sub>) and ln <inline-formula hwp:id="inline-formula-12"><alternatives hwp:id="alternatives-20"><inline-graphic xlink:href="107698_inline12.gif" hwp:id="inline-graphic-12"/></alternatives></inline-formula> prior distributions respectively. The hyperparameters <italic toggle="yes">a</italic><sub>α</sub> and <italic toggle="yes">b</italic><sub>α</sub> were themselves given a noninformative Half-normal(0, 10<sup>4</sup>) prior and the hyperparametersand <italic toggle="yes">μ</italic><sub><italic toggle="yes">β</italic></sub> and σ<sup>2</sup><sub><italic toggle="yes">β</italic></sub> were given a non-informative <italic toggle="yes">N</italic>(0, 10<sup>4</sup>) and Half-normal(0, 10<sup>4</sup>) priors respectively. The parameters of the model-free algorithm for the <italic toggle="yes">i</italic>th participant were <italic toggle="yes">α</italic><sub>1</sub><sup><italic toggle="yes">i</italic></sup>, <italic toggle="yes">α</italic><sub>2</sub><sup><italic toggle="yes">i</italic></sup>, ⋋<sup><italic toggle="yes">i</italic></sup>, and <italic toggle="yes">β</italic><sup><italic toggle="yes">i</italic></sup>. They were given a Beta(<italic toggle="yes">a</italic><sub>α1</sub>, <italic toggle="yes">b</italic><sub>α1</sub>), Beta(<italic toggle="yes">a</italic><sub>α2</sub>; <italic toggle="yes">b</italic><sub>α1</sub>), Beta(<italic toggle="yes">a</italic><sub>λ</sub>; <italic toggle="yes">b</italic><sub>λ</sub>) and ln <inline-formula hwp:id="inline-formula-13"><alternatives hwp:id="alternatives-21"><inline-graphic xlink:href="107698_inline14.gif" hwp:id="inline-graphic-13"/></alternatives></inline-formula> prior distributions respectively. The hyperparameters <italic toggle="yes">a</italic><sub>α1</sub>, <italic toggle="yes">a</italic><sub>α2</sub>, <italic toggle="yes">a</italic><sub>λ</sub>, <italic toggle="yes">b</italic><sub>α1</sub>, <italic toggle="yes">b</italic><sub>α2</sub>, and <italic toggle="yes">b</italic><sub>λ</sub> were themselves given a noninformative Half-normal(0, 10<sup>4</sup>) prior and the hyperparameters <italic toggle="yes">μ</italic><sub><italic toggle="yes">β</italic></sub> and σ<sup>2</sup><sub>β</sub> were given a non-informative <inline-formula hwp:id="inline-formula-14"><alternatives hwp:id="alternatives-22"><inline-graphic xlink:href="107698_inline15.gif" hwp:id="inline-graphic-14"/></alternatives></inline-formula> and Half-normal(0, 10<sup>4</sup>) priors respectively. We then coded the models into the Stan modeling language version 2.14.0 [<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-2" hwp:rel-id="ref-39">39</xref>, <xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">40</xref>] and used the PyStan Python package [<xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-2" hwp:rel-id="ref-41">41</xref>] to obtain 50,000 samples of the joint posterior distribution from one chain of length 60,000 (warmup10,000). Convergence of the chains was indicated by <inline-formula hwp:id="inline-formula-15"><alternatives hwp:id="alternatives-23"><inline-graphic xlink:href="107698_inline17.gif" hwp:id="inline-graphic-15"/></alternatives></inline-formula> for all parameters. The minimum effective sample size was 1481 for all hyperparameters. The results were used to generate <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figures 2</xref> and <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3</xref>.</p></sec><sec id="s4g" hwp:id="sec-14"><label>4.7</label><title hwp:id="title-15">Simulated experiments</title><p hwp:id="p-53">Given that this study’s aim was to determine if working memory-dependent temporal pattern learning is necessarily model-based or can be model-free, we sought to determine if the results obtained for the sequential condition could have been generated by the model-based algorithm. To this end, we simulated 10,000 experiments wherein, in each simulated experiment, the 21 participants in the simultaneous condition were compared to a different group of 20 simulated purely-model-based agents (as replacements for the 20 human participants in the sequential condition).</p><p hwp:id="p-54">The model-based algorithm was first fitted to the sequential condition results using the Bayesian hierarchical method described above to obtain 200,000 samples of the posterior distribution from four chains of length 60,000 (warmup 10,000). Convergence of the chains was indicated by <inline-formula hwp:id="inline-formula-16"><alternatives hwp:id="alternatives-24"><inline-graphic xlink:href="107698_inline18.gif" hwp:id="inline-graphic-16"/></alternatives></inline-formula> for all parameters. The minimum effective sample size was 16467 for all hyperparameters. For each simulated experiment, a point was randomly selected from the posterior distribution of hyperparameters (<italic toggle="yes">a</italic><sub><italic toggle="yes">α</italic></sub>, <italic toggle="yes">b</italic><sub><italic toggle="yes">α</italic></sub>, <italic toggle="yes">μ</italic><sub><italic toggle="yes">β</italic></sub>, σ<sub><italic toggle="yes">β</italic></sub>) and 20 sets of algorithm parameters (<italic toggle="yes">α</italic>, <italic toggle="yes">β</italic>) were randomly generated using the selected values, i.e. <italic toggle="yes">α</italic> ~ Beta(<italic toggle="yes">a</italic><sub><italic toggle="yes">α</italic></sub>, <italic toggle="yes">b</italic><sub><italic toggle="yes">α</italic></sub>), <italic toggle="yes">β</italic> ~ ln <inline-formula hwp:id="inline-formula-17"><alternatives hwp:id="alternatives-25"><inline-graphic xlink:href="107698_inline16.gif" hwp:id="inline-graphic-17"/></alternatives></inline-formula>. For each (<italic toggle="yes">α</italic>, <italic toggle="yes">β</italic>) parameter set, the model-based algorithm was run for 250 trials of the experimental task to generate results for a simulated purely-model-based agent. These simulated agents were then compared with the actual participants in the simultaneous condition using the same logistic regression analysis described above, except that, for computational efficiency, only 600 samples from one chain of 800 samples (warmup 200) was obtained from the posterior distribution.</p><p hwp:id="p-55">The entire analysis procedure was replicated several times with differing parameter values and prior distributions to ensure that the results and conclusions remained the same under a wide set of assumptions. In all cases, the results were nearly identical and supported the same conclusions.</p></sec><sec id="s4h" hwp:id="sec-15"><label>4.8</label><title hwp:id="title-16">Code and data availability</title><p hwp:id="p-56">All the computer code and behavioral data used in this study are available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/carolfs/mf_wm" ext-link-type="uri" xlink:href="https://github.com/carolfs/mf_wm" hwp:id="ext-link-2">https://github.com/carolfs/mf_wm</ext-link></p></sec></sec></body><back><ack hwp:id="ack-1"><label>5</label><title hwp:id="title-17">Acknowledgements</title><p hwp:id="p-57">This work was supported by the São Paulo Research Foundation – FAPESP (grant number 2013/10694-0) and the start-up research funds from the University of Zurich. Y.Y.’s involvement was supported by the China Scholarship Council.</p><sec hwp:id="sec-16"><label>6</label><title hwp:id="title-18">Author contributions</title><p hwp:id="p-58">C.F.S. and T.A.H. designed the study; C.F.S. and Y.Y. conducted the behavioral experiment; C.F.S. performed the simulations and analyzed the data with input from T.A.H.; C.F.S., Y.Y., and T.A.H. wrote the manuscript.</p></sec><sec sec-type="COI-statement" hwp:id="sec-17"><label>7</label><title hwp:id="title-19">Competing financial interests</title><p hwp:id="p-59">The authors declare no competing financial interests.</p></sec></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-20">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2 xref-ref-1-3 xref-ref-1-4 xref-ref-1-5 xref-ref-1-6"><label>[1]</label><citation publication-type="other" citation-type="journal" ref:id="107698v1.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Sutton Richard S."><given-names>Richard S.</given-names> <surname>Sutton</surname></string-name> and <string-name name-style="western" hwp:sortable="Barto Andrew G."><given-names>Andrew G.</given-names> <surname>Barto</surname>.</string-name> <article-title hwp:id="article-title-2">Reinforcement Learning: An Introduction</article-title>. <source hwp:id="source-1">A Bradford Book, first edition</source>, <year>1998</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Schultz W."><surname>Schultz</surname> <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> <string-name name-style="western" hwp:sortable="Montague P. R."><surname>Montague</surname>. <given-names>P. R.</given-names></string-name> <article-title hwp:id="article-title-3">A Neural Substrate of Prediction and Reward</article-title>. <source hwp:id="source-2">Science</source>, <volume>275</volume> (<issue>5306</issue>): <fpage>1593</fpage>–<lpage>1599</lpage>, mar <year>1997</year>. ISSN <issn>0036-8075</issn>. doi: <pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.sciencemag.org/cgi/doi/10.1126/science.275.5306.1593" ext-link-type="uri" xlink:href="http://www.sciencemag.org/cgi/doi/10.1126/science.275.5306.1593" hwp:id="ext-link-3">http://www.sciencemag.org/cgi/doi/10.1126/science.275.5306.1593</ext-link>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Fiorillo Christopher D"><surname>Fiorillo</surname>, <given-names>Christopher D</given-names></string-name> <string-name name-style="western" hwp:sortable="Tobler Philippe N"><surname>Tobler</surname>, <given-names>Philippe N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wolfram Schultz"><surname>Wolfram</surname> <given-names>Schultz</given-names></string-name>. <article-title hwp:id="article-title-4">Discrete coding of reward probability and uncertainty by dopamine neurons</article-title>. <source hwp:id="source-3">Science (New York, N.Y.)</source>, <volume>299</volume> (<issue>5614</issue>): <fpage>1898</fpage>–<lpage>902</lpage>, mar <year>2003</year>. ISSN <issn>1095-9203</issn>. <pub-id pub-id-type="doi">doi: 10.1126/science.1077349</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.ncbi.nlm.nih.gov/pubmed/12649484" ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/12649484" hwp:id="ext-link-4">http://www.ncbi.nlm.nih.gov/pubmed/12649484</ext-link>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Yael Niv"><surname>Yael</surname> <given-names>Niv</given-names></string-name>. <article-title hwp:id="article-title-5">Reinforcement learning in the brain</article-title>. <source hwp:id="source-4">Journal of Mathematical Psychology</source>, <volume>53</volume> (<issue>3</issue>): <fpage>139</fpage>–<lpage>154</lpage>, jun <year>2009</year>. ISSN 00222496. <pub-id pub-id-type="doi">doi: 10.1016/j.jmp.2008.12.005</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/S0022249608001181" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0022249608001181" hwp:id="ext-link-5">http://linkinghub.elsevier.com/retrieve/pii/S0022249608001181</ext-link>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Glimcher P. W."><surname>Glimcher</surname>. <given-names>P. W.</given-names></string-name> <article-title hwp:id="article-title-6">Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis</article-title>. <source hwp:id="source-5">Proceedings of the National Academy of Sciences</source>, <volume>108</volume> (Supplement_<issue>3</issue>): <fpage>15647</fpage>–<lpage>15654</lpage>, sep <year>2011</year>. ISSN <issn>0027-8424</issn>. <pub-id pub-id-type="doi">doi: 10.1073/pnas.1014269108</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pnas.org/cgi/doi/10.1073/pnas.1014269108" ext-link-type="uri" xlink:href="http://www.pnas.org/cgi/doi/10.1073/pnas.1014269108" hwp:id="ext-link-6">http://www.pnas.org/cgi/doi/10.1073/pnas.1014269108</ext-link>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Daeyeol Lee"><surname>Daeyeol</surname> <given-names>Lee</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hyojung Seo"><surname>Hyojung</surname> <given-names>Seo</given-names></string-name>, <string-name name-style="western" hwp:sortable="Min Whan Jung"><surname>Min Whan</surname> <given-names>Jung</given-names></string-name>. <article-title hwp:id="article-title-7">Neural Basis of Reinforcement Learning and Decision Making</article-title>. <source hwp:id="source-6">Annual Review of Neuroscience</source>, <volume>35</volume> (<issue>1</issue>): <fpage>287</fpage>–<lpage>308</lpage>, jul <year>2012</year>. ISSN <issn>0147-006X</issn>. <pub-id pub-id-type="doi">doi: 10.1146/annurev-neuro-062111-150512</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-062111-150512" ext-link-type="uri" xlink:href="http://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-062111-150512" hwp:id="ext-link-7">http://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-062111-150512</ext-link>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Dolan Ray J."><surname>Dolan</surname> <given-names>Ray J.</given-names></string-name> <string-name name-style="western" hwp:sortable="Peter Dayan"><surname>Peter</surname> <given-names>Dayan</given-names></string-name>. <article-title hwp:id="article-title-8">Goals and Habits in the Brain</article-title>. <source hwp:id="source-7">Neuron</source>, <volume>80</volume> (<issue>2</issue>): <fpage>312</fpage>–<lpage>325</lpage>, oct <year>2013</year>. ISSN 08966273. <pub-id pub-id-type="doi">doi: 10.1016/j.neuron.2013.09.007</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/S0896627313008052" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627313008052" hwp:id="ext-link-8">http://linkinghub.elsevier.com/retrieve/pii/S0896627313008052.</ext-link>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Graybiel Ann M."><surname>Graybiel</surname>. <given-names>Ann M.</given-names></string-name> <article-title hwp:id="article-title-9">Habits, Rituals, and the Evaluative Brain</article-title>. <source hwp:id="source-8">Annual Review of Neuroscience</source>, <volume>31</volume> (<issue>1</issue>): <fpage>359</fpage>–<lpage>387</lpage>, jul <year>2008</year>. ISSN <issn>0147-006X</issn>. <pub-id pub-id-type="doi">doi: 10.1146/annurev.neuro.29.051605.112851</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.annualreviews.org/doi/10.1146/annurev.neuro.29.051605.112851" ext-link-type="uri" xlink:href="http://www.annualreviews.org/doi/10.1146/annurev.neuro.29.051605.112851" hwp:id="ext-link-9">http://www.annualreviews.org/doi/10.1146/annurev.neuro.29.051605.112851</ext-link>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Peter Dayan"><surname>Peter</surname> <given-names>Dayan</given-names></string-name>. <article-title hwp:id="article-title-10">How to set the switches on this thing</article-title>. <source hwp:id="source-9">Current Opinion in Neurobiology</source>, <volume>22</volume> (<issue>6</issue>): <fpage>1068</fpage>–<lpage>1074</lpage>, dec <year>2012</year>. ISSN 09594388. <pub-id pub-id-type="doi">doi: 10.1016/j.conb.2012.05.011</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/S0959438812000992" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0959438812000992" hwp:id="ext-link-10">http://linkinghub.elsevier.com/retrieve/pii/S0959438812000992</ext-link>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Smith Kyle S"><surname>Smith</surname> <given-names>Kyle S</given-names></string-name>. <string-name name-style="western" hwp:sortable="Graybiel Ann M"><surname>Graybiel</surname>. <given-names>Ann M</given-names></string-name>. <article-title hwp:id="article-title-11">Investigating habits: strategies, technologies and models</article-title>. <source hwp:id="source-10">Frontiers in Behavioral Neuroscience</source>, <volume>8</volume>, <year>2014</year>. ISSN <issn>1662-5153</issn>. <pub-id pub-id-type="doi">doi: 10.3389/fnbeh.2014.00039</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://journal.frontiersin.org/article/10.3389/fnbeh.2014.00039/abstract" ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnbeh.2014.00039/abstract" hwp:id="ext-link-11">http://journal.frontiersin.org/article/10.3389/fnbeh.2014.00039/abstract</ext-link>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Cushman Fiery"><surname>Cushman</surname> <given-names>Fiery</given-names></string-name> <string-name name-style="western" hwp:sortable="Morris Adam"><surname>Morris</surname>. <given-names>Adam</given-names></string-name> <article-title hwp:id="article-title-12">Habitual control of goal selection in humans</article-title>. <source hwp:id="source-11">Proceedings of the National Academy of Sciences</source>, <volume>112</volume> (<issue>45</issue>): <fpage>13817</fpage>–<lpage>13822</lpage>, nov <year>2015</year>. ISSN <issn>0027-8424</issn>. <pub-id pub-id-type="doi">doi: 10.1073/pnas.1506367112</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pnas.org/lookup/doi/10.1073/pnas.1506367112" ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/doi/10.1073/pnas.1506367112" hwp:id="ext-link-12">http://www.pnas.org/lookup/doi/10.1073/pnas.1506367112</ext-link>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Aarts Henk"><surname>Aarts</surname> <given-names>Henk</given-names></string-name> <string-name name-style="western" hwp:sortable="Dijksterhuis Ap"><surname>Dijksterhuis</surname>. <given-names>Ap</given-names></string-name> <article-title hwp:id="article-title-13">Habits as knowledge structures: Automaticity in goal directed behavior</article-title>. <source hwp:id="source-12">Journal of Personality and Social Psychology</source>, <volume>78</volume> (<issue>1</issue>): <fpage>53</fpage>–<lpage>63</lpage>, <year>2000</year>. ISSN <issn>1939-1315</issn>. <pub-id pub-id-type="doi">doi: 10.1037/0022-3514.78.1.53</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.78.1.53" ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.78.1.53" hwp:id="ext-link-13">http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.78.1.53</ext-link>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Dezfouli Amir"><surname>Dezfouli</surname> <given-names>Amir</given-names></string-name> <string-name name-style="western" hwp:sortable="Balleine Bernard W."><surname>Balleine</surname>. <given-names>Bernard W.</given-names></string-name> <article-title hwp:id="article-title-14">Habits, action sequences and reinforcement learning</article-title>. <source hwp:id="source-13">European Journal of Neuroscience</source>, <volume>35</volume> (<issue>7</issue>): <fpage>1036</fpage>–<lpage>1051</lpage>, apr <year>2012</year>. ISSN 0953816X. <pub-id pub-id-type="doi">doi: 10.1111/j.1460-9568.2012.08050.x</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.wiley.com/10.1111/j.1460-9568.2012.08050.x" ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1111/j.1460-9568.2012.08050.x" hwp:id="ext-link-14">http://doi.wiley.com/10.1111/j.1460-9568.2012.08050.x</ext-link>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Dezfouli Amir"><surname>Dezfouli</surname> <given-names>Amir</given-names></string-name> <string-name name-style="western" hwp:sortable="Balleine Bernard W."><surname>Balleine</surname>. <given-names>Bernard W.</given-names></string-name> <article-title hwp:id="article-title-15">Actions, Action Sequences and Habits: Evidence That Goal–Directed and Habitual Action Control Are Hierarchically Organized</article-title>. <source hwp:id="source-14">PLoS Computational Biology</source>, <volume>9</volume> (<issue>12</issue>):<fpage>e1003364</fpage>, dec <year>2013</year>. ISSN 1553–7358. <pub-id pub-id-type="doi">doi: 10.1371/journal.pcbi.1003364</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.plos.org/10.1371/journal.pcbi.1003364" ext-link-type="uri" xlink:href="http://dx.plos.org/10.1371/journal.pcbi.1003364" hwp:id="ext-link-15">http://dx.plos.org/10.1371/journal.pcbi.1003364</ext-link>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Dezfouli A"><surname>Dezfouli</surname> <given-names>A</given-names></string-name>. <string-name name-style="western" hwp:sortable="Lingawi N. W."><surname>Lingawi</surname>, <given-names>N. W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Balleine B. W"><surname>Balleine</surname>. <given-names>B. W</given-names></string-name>. <article-title hwp:id="article-title-16">Habits as action sequences: hierarchical action control and changes in outcome value</article-title>. <source hwp:id="source-15">Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>369</volume>(<issue>1655</issue>):<fpage>20130482</fpage>–<lpage>20130482</lpage>, sep <year>2014</year>. ISSN 0962–8436. <pub-id pub-id-type="doi">doi: 10.1098/rstb.2013.0482</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0482" ext-link-type="uri" xlink:href="http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0482" hwp:id="ext-link-16">http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0482</ext-link>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="O’Reilly Randall C">.<surname>O’Reilly</surname> <given-names>Randall C</given-names></string-name> and <string-name name-style="western" hwp:sortable="Frank Michael J."><surname>Frank</surname> <given-names>Michael J.</given-names></string-name> <article-title hwp:id="article-title-17">Making Working Memory Work: A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia</article-title>. <source hwp:id="source-16">Neural Computation</source>, <volume>18</volume> (<issue>2</issue>):<fpage>283</fpage>–<lpage>328</lpage>, feb <year>2006</year>. ISSN 0899–7667. <pub-id pub-id-type="doi">doi: 10.1162/089976606775093909</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.mitpressjournals.org/doi/abs/10.1162/089976606775093909" ext-link-type="uri" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/089976606775093909" hwp:id="ext-link-17">http://www.mitpressjournals.org/doi/abs/10.1162/089976606775093909</ext-link>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3 xref-ref-17-4 xref-ref-17-5 xref-ref-17-6 xref-ref-17-7 xref-ref-17-8 xref-ref-17-9 xref-ref-17-10 xref-ref-17-11 xref-ref-17-12 xref-ref-17-13 xref-ref-17-14"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Nathaniel D. Daw"><surname>Nathaniel</surname> <given-names>D. Daw</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman Samuel J."><surname>Gershman</surname> <given-names>Samuel J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour Ben"><surname>Seymour</surname> <given-names>Ben</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peter Dayan"><surname>Peter</surname> <given-names>Dayan</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Raymond J. Dolan"><surname>Raymond</surname> <given-names>J. Dolan</given-names></string-name>. <article-title hwp:id="article-title-18">Model–Based Influences on Humans’ Choices and Striatal Prediction Errors</article-title>. <source hwp:id="source-17">Neuron</source>, <volume>69</volume> (<issue>6</issue>):<fpage>1204</fpage>–<lpage>1215</lpage>, mar <year>2011</year>. ISSN 08966273. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.cell.com/neuron/abstract/S0896-6273(11)00125-5" ext-link-type="uri" xlink:href="http://www.cell.com/neuron/abstract/S0896-6273(11)00125-5" hwp:id="ext-link-18">http://www.cell.com/neuron/abstract/S0896-6273(11)00125-5</ext-link><ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3077926{&amp;}tool=pmcentrez{&amp;}rendertype=abstract" ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3077926{&amp;}tool=pmcentrez{&amp;}rendertype=abstract" hwp:id="ext-link-19">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3077926{&amp;}tool=pmcentrez{&amp;}rendertype=abstract</ext-link> <ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255" hwp:id="ext-link-20">http://www.cell.com/neuron/abstract/S0896-6273(11)00125-5</ext-link><ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3077926{&amp;}tool=pmcentrez{&amp;}rendertype=abstract" ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3077926{&amp;}tool=pmcentrez{&amp;}rendertype=abstract" hwp:id="ext-link-21">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3077926{&amp;}tool=pmcentrez{&amp;}rendertype=abstract</ext-link><ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255" hwp:id="ext-link-22">http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255</ext-link>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><label>[18]</label><citation publication-type="book" citation-type="book" ref:id="107698v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Michael T Todd"><surname>Michael</surname> <given-names>T Todd</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yael Niv"><surname>Yael</surname> <given-names>Niv</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Jonathan D Cohen"><surname>Jonathan</surname> <given-names>D Cohen</given-names></string-name>. <chapter-title>Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement. In D Koller, D Schuurmans, Y Bengio, and L Bottou, editors</chapter-title>, <source hwp:id="source-18">Advances in Neural Information Processing Systems</source> <volume>21</volume>, pages <fpage>1689</fpage>–<lpage>1696</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2009</year>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://papers.nips.cc/paper/3508-learning-to-use-working-memory-in-partially-observable-environments-through-dopaminergic-pdf" ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/3508-learning-to-use-working-memory-in-partially-observable-environments-through-dopaminergic-pdf" hwp:id="ext-link-23">http://papers.nips.cc/paper/3508-learning-to-use-working-memory-in-partially-observable-environments-through-dopaminergic-pdf</ext-link>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Arthur S. Reber"><surname>Arthur</surname> <given-names>S. Reber</given-names></string-name>. <article-title hwp:id="article-title-19">Implicit learning and tacit knowledge</article-title>. <source hwp:id="source-19">Journal of Experimental Psychology: General</source>, <volume>118</volume>(<issue>3</issue>):<fpage>219</fpage>–<lpage>235</lpage>, <year>1989</year>. ISSN <issn>1939-2222</issn>. <pub-id pub-id-type="doi">doi: 10.1037/0096-3445.118.3.219</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.118.3.219" ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.118.3.219" hwp:id="ext-link-24">http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.118.3.219</ext-link>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Canfield Richard L."><surname>Canfield</surname> <given-names>Richard L.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Haith Marshall M."><surname>Haith</surname> <given-names>Marshall M.</given-names></string-name>. <article-title hwp:id="article-title-20">Young infants’ visual expectations for symmetric and asymmetric stimulus sequences</article-title>. <source hwp:id="source-20">Developmental Psychology</source>, <volume>27</volume>(<issue>2</issue>):<fpage>198</fpage>–<lpage>208</lpage>, <year>1991</year>. ISSN <issn>0012-1649</issn>. doi: <pub-id pub-id-type="doi">10.1037/0012-1649.27.2.198</pub-id>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://cat.inist.fr/?aModele=afficheN{&amp;}cpsidt=19452330http://doi.apa.org/getdoi.cfm?doi=10.1037/0012-1649.27.2.198" ext-link-type="uri" xlink:href="http://cat.inist.fr/?aModele=afficheN{&amp;}cpsidt=19452330http://doi.apa.org/getdoi.cfm?doi=10.1037/0012-1649.27.2.198" hwp:id="ext-link-25">http://cat.inist.fr/?aModele=afficheN{&amp;}cpsidt=19452330http://doi.apa.org/getdoi.cfm?doi=10.1037/0012-1649.27.2.198</ext-link>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Huettel Scott A."><surname>Huettel</surname> <given-names>Scott A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mack Peter B."><surname>Mack</surname>, <given-names>Peter B.</given-names></string-name> and <string-name name-style="western" hwp:sortable="McCarthy Gregory"><surname>McCarthy</surname> <given-names>Gregory</given-names></string-name>. <article-title hwp:id="article-title-21">Perceiving patterns in random series: dynamic processing of sequence in prefrontal cortex</article-title>. <source hwp:id="source-21">Nature Neuroscience</source>, apr <year>2002</year>. ISSN 10976256. <pub-id pub-id-type="doi">doi: 10.1038/nn841</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.nature.com/doifinder/10.1038/nn841" ext-link-type="uri" xlink:href="http://www.nature.com/doifinder/10.1038/nn841" hwp:id="ext-link-26">http://www.nature.com/doifinder/10.1038/nn841</ext-link>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>[22]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Asher Cohen"><surname>Asher</surname> <given-names>Cohen</given-names></string-name>, <string-name name-style="western" hwp:sortable="Richard I. Ivry"><surname>Richard</surname> <given-names>I. Ivry</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Steven W. Keele"><surname>Steven</surname> <given-names>W. Keele</given-names></string-name>. <article-title hwp:id="article-title-22">Attention and structure in sequence learning</article-title>. <source hwp:id="source-22">Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>16</volume>(<issue>1</issue>):<fpage>17</fpage>–<lpage>30</lpage>, <year>1990</year>. ISSN <issn>1939-1285</issn>. <pub-id pub-id-type="doi">doi: 10.1037/0278-7393.16.1.17</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.16.1.17" ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.16.1.17" hwp:id="ext-link-27">http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.16.1.17</ext-link>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Cleeremans Axel"><surname>Cleeremans</surname> <given-names>Axel</given-names></string-name> and <string-name name-style="western" hwp:sortable="McClelland James L."><surname>McClelland</surname>. <given-names>James L.</given-names></string-name> <article-title hwp:id="article-title-23">Learning the structure of event sequences</article-title>. <source hwp:id="source-23">Journal of Experimental Psychology: General</source>, <volume>120</volume>(<issue>3</issue>):<fpage>235</fpage>–<lpage>253</lpage>, <year>1991</year>. ISSN <issn>1939-2222</issn>. <pub-id pub-id-type="doi">doi: 10.1037/0096-3445.120.3.235</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.120.3.235" ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.120.3.235" hwp:id="ext-link-28">http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.120.3.235</ext-link>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Jenkins I H"><surname>Jenkins</surname>, <given-names>I H</given-names></string-name> <string-name name-style="western" hwp:sortable="Brooks D J"><surname>Brooks</surname>, <given-names>D J</given-names></string-name> <string-name name-style="western" hwp:sortable="Nixon P D"><surname>Nixon</surname>, <given-names>P D</given-names></string-name> <string-name name-style="western" hwp:sortable="Frackowiak R S"><surname>Frackowiak</surname>, <given-names>R S</given-names></string-name> and <string-name name-style="western" hwp:sortable="Passingham R E"><surname>Passingham</surname> <given-names>R E</given-names></string-name>. <article-title hwp:id="article-title-24">Motor sequence learning: a study with positron emission tomography</article-title>. <source hwp:id="source-24">The Journal of neuroscience : the official journal of the Society for Neuroscience</source>, <volume>14</volume>(<issue>6</issue>):<fpage>3775</fpage>–<lpage>90</lpage>, jun <year>1994</year>. ISSN <issn>0270-6474</issn>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.ncbi.nlm.nih.gov/pubmed/8207487" ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/8207487" hwp:id="ext-link-29">http://www.ncbi.nlm.nih.gov/pubmed/8207487</ext-link>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Vakil Eli"><surname>Vakil</surname> <given-names>Eli</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kahan Shimon"><surname>Kahan</surname> <given-names>Shimon</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huberman Moshe"><surname>Huberman</surname> <given-names>Moshe</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Osimani Alicia"><surname>Osimani</surname> <given-names>Alicia</given-names></string-name>. <article-title hwp:id="article-title-25">Motor and non–motor sequence learning in patients with basal ganglia lesions: The case of serial reaction time (SRT)</article-title>.<source hwp:id="source-25">Neuropsychologia</source>, <volume>38</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>, <year>2000</year>. ISSN 00283932. doi: <pub-id pub-id-type="doi">10.1016/S0028-3932(99)00058-5</pub-id>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Lehericy S."><surname>Lehericy</surname> <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benali H."><surname>Benali</surname> <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Van de Moortele P.-F."><surname>Van de Moortele</surname> <given-names>P.-F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pelegrini-Issac M."><surname>Pelegrini-Issac</surname> <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Waechter T."><surname>Waechter</surname> <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ugurbil K."><surname>Ugurbil</surname> <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Doyon J."><surname>Doyon</surname> <given-names>J.</given-names></string-name>. <article-title hwp:id="article-title-26">Distinct basal ganglia territories are engaged in early and advanced motor sequence learning</article-title>. <source hwp:id="source-26">Proceedings of the National Academy of Sciences</source>, <volume>102</volume>(<issue>35</issue>):<fpage>12566</fpage>–<lpage>12571</lpage>, aug <year>2005</year>. ISSN <issn>0027-8424</issn>. <pub-id pub-id-type="doi">doi: 10.1073/pnas.0502762102</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pnas.org/cgi/doi/10.1073/pnas.0502762102" ext-link-type="uri" xlink:href="http://www.pnas.org/cgi/doi/10.1073/pnas.0502762102" hwp:id="ext-link-30">http://www.pnas.org/cgi/doi/10.1073/pnas.0502762102</ext-link>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="confproc" citation-type="confproc" ref:id="107698v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Otto A. R."><surname>Otto</surname> <given-names>A. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Raio C. M."><surname>Raio</surname> <given-names>C. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chiang A."><surname>Chiang</surname> <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Phelps E. A."><surname>Phelps</surname> <given-names>E. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname> <given-names>N. D.</given-names></string-name>. <article-title hwp:id="article-title-27">Working-memory capacity protects model-based learning from stress</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name>, <volume>110</volume>(<issue>52</issue>):<fpage>20941</fpage>–<lpage>20946</lpage>, dec <year>2013</year>. ISSN <issn>0027-8424</issn>. <pub-id pub-id-type="doi">doi: 10.1073/pnas.1312011110</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pnas.org/cgi/doi/10.1073/pnas.1312011110" ext-link-type="uri" xlink:href="http://www.pnas.org/cgi/doi/10.1073/pnas.1312011110" hwp:id="ext-link-31">http://www.pnas.org/cgi/doi/10.1073/pnas.1312011110</ext-link>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>[28]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Ross Otto A."><surname>Ross Otto</surname> <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman Samuel J."><surname>Gershman</surname> <given-names>Samuel J.</given-names></string-name>,<string-name name-style="western" hwp:sortable="Markman Arthur B."><surname>Markman</surname> <given-names>Arthur B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Nathaniel D. Daw"><surname>Nathaniel</surname><given-names>D. Daw</given-names></string-name>. <article-title hwp:id="article-title-28">The Curse of Planning</article-title>. <source hwp:id="source-27">Psychological Science</source>, <volume>24</volume>(<issue>5</issue>):<fpage>751</fpage>–<lpage>761</lpage>, may <year>2013</year>. ISSN <issn>0956-7976</issn>.<pub-id pub-id-type="doi">doi: 10.1177/ 0956797612463080</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://journals.sagepub.com/doi/10.1177/0956797612463080" ext-link-type="uri" xlink:href="http://journals.sagepub.com/doi/10.1177/0956797612463080" hwp:id="ext-link-32">http://journals.sagepub.com/doi/10.1177/0956797612463080</ext-link>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Ross Otto A."><surname>Ross Otto</surname> <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Skatova Anya"><surname>Skatova</surname> <given-names>Anya</given-names></string-name>, <string-name name-style="western" hwp:sortable="Madlon-Kay Seth"><surname>Madlon-Kay</surname> <given-names>Seth</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Nathaniel D. Daw"><surname>Nathaniel</surname> <given-names>D. Daw</given-names></string-name>. <article-title hwp:id="article-title-29">Cognitive Control Predicts Use of Model-based Reinforcement Learning</article-title>. <source hwp:id="source-28">Journal of Cognitive Neuroscience</source>, <volume>27</volume> (<issue>2</issue>):<fpage>319</fpage>–<lpage>333</lpage>, feb <year>2015</year>. ISSN <issn>0898-929X</issn>. <pub-id pub-id-type="doi">doi: 10.1162/jocn_a_00709</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.mitpressjournals.org/doi/abs/10.1162/jocn{_}a{_}00709" ext-link-type="uri" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/jocn{_}a{_}00709" hwp:id="ext-link-33">http://www.mitpressjournals.org/doi/abs/10.1162/jocn{_}a{_}00709</ext-link>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Decker J. H."><surname>Decker</surname> <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Otto A. R."><surname>Otto</surname> <given-names>A. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname> <given-names>N. D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hartley C. A."><surname>Hartley</surname> <given-names>C. A.</given-names></string-name>. <article-title hwp:id="article-title-30">From Creatures of Habit to Goal-Directed Learners: Tracking the Developmental Emergence of Model-Based Reinforcement Learning</article-title>. <source hwp:id="source-29">Psychological Science</source>, <volume>27</volume>(<issue>6</issue>):<fpage>848</fpage>–<lpage>858</lpage>, jun <year>2016</year>. ISSN <issn>0956-7976</issn>. <pub-id pub-id-type="doi">doi: 10.1177/0956797616639301</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://pss.sagepub.com/lookup/doi/10.1177/0956797616639301" ext-link-type="uri" xlink:href="http://pss.sagepub.com/lookup/doi/10.1177/0956797616639301" hwp:id="ext-link-34">http://pss.sagepub.com/lookup/doi/10.1177/0956797616639301</ext-link>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Friedel Eva"><given-names>Eva</given-names> <surname>Friedel</surname></string-name>, <string-name name-style="western" hwp:sortable="Koch Stefan P."><given-names>Stefan P.</given-names> <surname>Koch</surname></string-name>, <string-name name-style="western" hwp:sortable="Wendt Jean"><given-names>Jean</given-names> <surname>Wendt</surname></string-name>, <string-name name-style="western" hwp:sortable="Heinz Andreas"><given-names>Andreas</given-names> <surname>Heinz</surname></string-name>, <string-name name-style="western" hwp:sortable="Deserno Lorenz"><given-names>Lorenz</given-names> <surname>Deserno</surname></string-name> and <string-name name-style="western" hwp:sortable="Schlagenhauf Florian"><given-names>Florian</given-names> <surname>Schlagenhauf</surname></string-name>. <article-title hwp:id="article-title-31">Devaluation and sequential decisions: linking goal–directed and modelbased behavior</article-title>. <source hwp:id="source-30">Frontiers in Human Neuroscience</source>, <day>8</day>, aug <year>2014</year>. ISSN <issn>1662-5161</issn>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2014.00587</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://journal.frontiersin.org/article/10.3389/fnhum.2014.00587/abstract" ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnhum.2014.00587/abstract" hwp:id="ext-link-35">http://journal.frontiersin.org/article/10.3389/fnhum.2014.00587/abstract</ext-link>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Gershman Samuel J."><given-names>Samuel J.</given-names> <surname>Gershman</surname></string-name>, <string-name name-style="western" hwp:sortable="Markman Arthur B."><given-names>Arthur B.</given-names> <surname>Markman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Otto A. Ross"><given-names>A. Ross</given-names> <surname>Otto</surname></string-name>. <article-title hwp:id="article-title-32">Retrospective revaluation in sequential decision making: A tale of two systems</article-title>. <source hwp:id="source-31">Journal of Experimental Psychology: General</source>, <volume>143</volume>(<issue>1</issue>):<fpage>182</fpage>–<lpage>194</lpage>, <year>2014</year>. ISSN <issn>1939-2222</issn>. doi: <pub-id pub-id-type="doi">10.1037/a0030844</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.apa.org/getdoi.cfm?doi=10.1037/a0030844" ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/a0030844" hwp:id="ext-link-36">http://doi.apa.org/getdoi.cfm?doi=10.1037/a0030844</ext-link>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="O’Doherty John P."><given-names>John P.</given-names> <surname>O’Doherty</surname></string-name>, <string-name name-style="western" hwp:sortable="Cockburn Jeffrey"><given-names>Jeffrey</given-names> <surname>Cockburn</surname></string-name>, and <string-name name-style="western" hwp:sortable="Pauli Wolfgang M."><given-names>Wolfgang M.</given-names> <surname>Pauli</surname></string-name>. <source hwp:id="source-32">Learning, Reward, and Decision Making. Annual Review of Psychology</source>, <volume>68</volume>(<issue>1</issue>):<fpage>73</fpage>–<lpage>100</lpage>, jan <year>2017</year>. ISSN <issn>0066-4308</issn>. doi:<pub-id pub-id-type="doi">10.1146/annurev-psych-010416-044216</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.annualreviews.org/doi/10.1146/annurev-psych-010416-044216" ext-link-type="uri" xlink:href="http://www.annualreviews.org/doi/10.1146/annurev-psych-010416-044216" hwp:id="ext-link-37">http://www.annualreviews.org/doi/10.1146/annurev-psych-010416-044216</ext-link>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>[34]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Read Montague P."><given-names>P.</given-names> <surname>Read Montague</surname></string-name>, <string-name name-style="western" hwp:sortable="Dolan Raymond J."><given-names>Raymond J.</given-names> <surname>Dolan</surname></string-name>, <string-name name-style="western" hwp:sortable="Friston Karl J."><given-names>Karl J.</given-names> <surname>Friston</surname></string-name>, and <string-name name-style="western" hwp:sortable="Dayan Peter"><given-names>Peter</given-names> <surname>Dayan</surname></string-name>. <source hwp:id="source-33">Computational psychiatry. Trends in Cognitive Sciences</source>, <volume>16</volume>(<issue>1</issue>):<fpage>72</fpage>–<lpage>80</lpage>, jan <year>2012</year>. ISSN 13646613. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2011.11.018</pub-id>. URL<ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/S1364661311002518" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S1364661311002518" hwp:id="ext-link-38">http://linkinghub.elsevier.com/retrieve/pii/S1364661311002518</ext-link>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="book" citation-type="book" ref:id="107698v1.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Cumming G."><given-names>G.</given-names> <surname>Cumming</surname></string-name>. <source hwp:id="source-34">Precision for Planning. In Understanding The New Statistics, chapter 13</source>, pages <fpage>355</fpage>–<lpage>380</lpage>. <publisher-name>Routledge</publisher-name>, <publisher-loc>New York, London</publisher-loc>, <edition>1 edition</edition>, <year>2012</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>[36]</label><citation publication-type="book" citation-type="book" ref:id="107698v1.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Kruschke J. K."><given-names>J. K.</given-names> <surname>Kruschke</surname></string-name>. <source hwp:id="source-35">Goals, Power, and Sample Size. In Doing Bayesian Data Analysis, chapter 13</source>, pages <fpage>359</fpage>–<lpage>398</lpage>. <publisher-name>Academic Press</publisher-name>, <publisher-loc>London</publisher-loc>, <edition>2 edition</edition>, <year>2015</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>[37]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="D Daw Nathaniel"><given-names>Nathaniel</given-names> <surname>D Daw</surname></string-name>, <string-name name-style="western" hwp:sortable="Niv Yael"><given-names>Yael</given-names> <surname>Niv</surname></string-name>, and <string-name name-style="western" hwp:sortable="Dayan Peter"><given-names>Peter</given-names> <surname>Dayan</surname></string-name>. <article-title hwp:id="article-title-33">Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source hwp:id="source-36">Nature neuroscience</source>, <volume>8</volume>(<issue>12</issue>): <fpage>1704</fpage>–<lpage>11</lpage>, dec <year>2005</year>. ISSN 10976256. doi:<pub-id pub-id-type="doi">10.1038/nn1560</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10" ext-link-type="uri" xlink:href="http://dx.doi.org/10" hwp:id="ext-link-39">http://dx.doi.org/10</ext-link>. <ext-link l:rel="related" l:ref-type="uri" l:ref="1038/nn1560http://www.ncbi.nlm.nih.gov/pubmed/16286932" ext-link-type="uri" xlink:href="1038/nn1560http://www.ncbi.nlm.nih.gov/pubmed/16286932" hwp:id="ext-link-40">1038/nn1560http://www.ncbi.nlm.nih.gov/pubmed/16286932</ext-link>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>[38]</label><citation publication-type="book" citation-type="book" ref:id="107698v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Daw Nathaniel D."><given-names>Nathaniel D.</given-names> <surname>Daw</surname></string-name> and <string-name name-style="western" hwp:sortable="O’Doherty John P."><given-names>John P.</given-names> <surname>O’Doherty</surname></string-name>. <source hwp:id="source-37">Multiple Systems for Value Learning.</source> In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Glimcher Paul W."><given-names>Paul W.</given-names> <surname>Glimcher</surname></string-name> and <string-name name-style="western" hwp:sortable="Fehr Ernst"><given-names>Ernst</given-names> <surname>Fehr</surname></string-name></person-group>, editors, <italic toggle="yes">Neuroeconomics</italic>, chapter 21, pages <fpage>393</fpage>–<lpage>410</lpage>. <publisher-name>Elsevier</publisher-name>, <edition>second edition</edition>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1016/B978-0-12-416008-8.00021-8</pub-id>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://linkinghub.elsevier.com/retrieve/pii/B9780124160088000218" ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/B9780124160088000218" hwp:id="ext-link-41">http://linkinghub.elsevier.com/retrieve/pii/B9780124160088000218</ext-link>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1 xref-ref-39-2"><label>[39]</label><citation publication-type="journal" citation-type="journal" ref:id="107698v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Carpenter Bob"><given-names>Bob</given-names> <surname>Carpenter</surname></string-name>, <string-name name-style="western" hwp:sortable="Gelman Andrew"><given-names>Andrew</given-names> <surname>Gelman</surname></string-name>, <string-name name-style="western" hwp:sortable="Hoffman Matthew D."><given-names>Matthew D.</given-names> <surname>Hoffman</surname></string-name>, <string-name name-style="western" hwp:sortable="Lee Daniel"><given-names>Daniel</given-names> <surname>Lee</surname></string-name>, <string-name name-style="western" hwp:sortable="Goodrich Ben"><given-names>Ben</given-names> <surname>Goodrich</surname></string-name>, <string-name name-style="western" hwp:sortable="Betancourt Michael"><given-names>Michael</given-names> <surname>Betancourt</surname></string-name>, <string-name name-style="western" hwp:sortable="Brubaker Marcus"><given-names>Marcus</given-names> <surname>Brubaker</surname></string-name>, <string-name name-style="western" hwp:sortable="Guo Jiqiang"><given-names>Jiqiang</given-names> <surname>Guo</surname></string-name>, <string-name name-style="western" hwp:sortable="Li Peter"><given-names>Peter</given-names> <surname>Li</surname></string-name> and <string-name name-style="western" hwp:sortable="Riddell Allen"><given-names>Allen</given-names> <surname>Riddell</surname></string-name>. <article-title hwp:id="article-title-34">Stan : A Probabilistic Programming Language</article-title>. <source hwp:id="source-38">Journal of Statistical Software</source>, <volume>76</volume>(<issue>1</issue>), <year>2017</year>. ISSN <fpage>1548</fpage>–<lpage>7660</lpage>. doi: <pub-id pub-id-type="doi">10.18637/jss.v076.i01.</pub-id> URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.jstatsoft.org/v76/i01/" ext-link-type="uri" xlink:href="http://www.jstatsoft.org/v76/i01/" hwp:id="ext-link-42">http://www.jstatsoft.org/v76/i01/</ext-link>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2"><label>[40]</label><citation publication-type="other" citation-type="journal" ref:id="107698v1.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><collab hwp:id="collab-1">Stan Development Team</collab>. <source hwp:id="source-39">Stan Modeling Language Users Guide and Reference Manual</source>, Version 2.14. <volume>0</volume>, <fpage>2016</fpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1 xref-ref-41-2"><label>[41]</label><citation publication-type="other" citation-type="journal" ref:id="107698v1.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><collab hwp:id="collab-2">Stan Development Team</collab>. <source hwp:id="source-40">PyStan: the Python interface to Stan</source>, <year>2016</year>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://mc-stan.org" ext-link-type="uri" xlink:href="http://mc-stan.org" hwp:id="ext-link-43">http://mc-stan.org</ext-link>.</citation></ref></ref-list></back></article>
