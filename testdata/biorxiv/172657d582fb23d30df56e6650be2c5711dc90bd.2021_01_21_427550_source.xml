<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.01.21.427550</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.01.21.427550</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.01.21.427550</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.01.21.427550</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.01.21.427550</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Confirmatory Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Hearing loss is associated with delayed neural responses to continuous speech</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1 xref-corresp-1-2"><label>*</label><bold>Corresponding authors:</bold> Marlies Gillis (<email hwp:id="email-1">marlies.gillis@kuleuven.be</email>), Tom Francart (<email hwp:id="email-2">tom.francart@kuleuven.be</email>)</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3967-2950</contrib-id><name name-style="western" hwp:sortable="Gillis Marlies"><surname>Gillis</surname><given-names>Marlies</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3967-2950"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Decruy Lien"><surname>Decruy</surname><given-names>Lien</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Vanthornhout Jonas"><surname>Vanthornhout</surname><given-names>Jonas</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9734-4261</contrib-id><name name-style="western" hwp:sortable="Francart Tom"><surname>Francart</surname><given-names>Tom</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-2" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-9734-4261"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">KU Leuven, Department of Neurosciences, ExpORL</institution>, 3000 Leuven, <country>Belgium</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Institute for Systems Research, University of Maryland</institution>, College Park, MD 20740, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-01-21T05:33:13-08:00">
    <day>21</day><month>1</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-11-17T02:51:14-08:00">
    <day>17</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-01-21T05:36:22-08:00">
    <day>21</day><month>1</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-11-17T03:00:09-08:00">
    <day>17</day><month>11</month><year>2021</year>
  </pub-date><elocation-id>2021.01.21.427550</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-01-21"><day>21</day><month>1</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-11-16"><day>16</day><month>11</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-11-17"><day>17</day><month>11</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license hwp:id="license-1"><p hwp:id="p-1">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="427550.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.01.21.427550v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="427550.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.01.21.427550v3/2021.01.21.427550v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.01.21.427550v3/2021.01.21.427550v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">We investigated the impact of hearing loss on the neural processing of speech. Using a forward modeling approach, we compared the neural responses to continuous speech of 14 adults with sensorineural hearing loss with those of age-matched normal-hearing peers.</p><p hwp:id="p-3">Compared to their normal-hearing peers, hearing-impaired listeners had increased neural tracking and delayed neural responses to continuous speech in quiet. The latency also increased with the degree of hearing loss. As speech understanding decreased, neural tracking decreased in both populations; however, a significantly different trend was observed for the latency of the neural responses. For normal-hearing listeners, the latency increased with increasing background noise level. However, for hearing-impaired listeners, this increase was not observed.</p><p hwp:id="p-4">Our results support the idea that the neural response latency indicates the efficiency of neural speech processing. Hearing-impaired listeners process speech in silence less efficiently than normal-hearing listeners. Our results suggest that this reduction in neural speech processing efficiency is a gradual effect which occurs as hearing deteriorates. Moreover, the efficiency of neural speech processing in hearing-impaired listeners is already at its lowest level when listening to speech in quiet, while normal-hearing listeners show a further decrease in efficiency when the noise level increases.</p><p hwp:id="p-5">From our results, it is apparent that sound amplification does not solve hearing loss. Even when listing to speech in silence at a comfortable loudness, hearing-impaired listeners process speech less efficiently.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Key words</title><kwd hwp:id="kwd-1">neural tracking</kwd><kwd hwp:id="kwd-2">hearing loss</kwd><kwd hwp:id="kwd-3">speech</kwd><kwd hwp:id="kwd-4">EEG</kwd></kwd-group><counts><page-count count="36"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-6">The authors have declared no competing interest.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-4">Introduction</title><p hwp:id="p-7">It is widely known that hearing loss alters the brain (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Eggermont, 2017</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Peelle and Wingfield, 2016</xref>). To study the functional neural changes, several studies focussed on cortical auditory evoked potentials (CAEP) using electroencephalography (EEG). CAEPs reflect the cortical responses evoked by repetitions of simple sounds such as syllables, tone pips, or clicks. These neural responses reflect sound detection and/or discrimination by the brain. The CAEP-response is characterized by a first positive peak (P1) around 50 ms, a first negative peak (N1) around 100 ms and a later positive peak (P2) around 180 ms (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Burkard et al., 2007</xref>). <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Harkrider et al. (2009)</xref> and <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Campbell and Sharma (2013)</xref> reported increased P2-latencies in hearing impaired listeners(HI listeners) compared to normal hearing listeners (NH listeners). Interestingly, <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">Campbell and Sharma (2013)</xref> reported that P2-latency was also correlated with the person’s speech perception ability in noise: a poorer perception of speech in noise is associated with a longer P2-latency. Although changes in latency are often not reported, in most studies HI listeners showed increased amplitudes compared to NH listeners (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Tremblay et al., 2003</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Harkrider et al., 2006</xref>; <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Bertoli et al., 2011</xref>; <xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Alain, 2014</xref>; <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Maamor and Billings, 2017</xref>) while <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Billings et al. (2015)</xref> and <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Koerner and Zhang (2018)</xref> did not observe differences between these two populations. However, other studies attributed differences in neural response to the audibility of the stimulus: although presented at the same sound intensity, the sound might be less audible for HI listeners than for NH listeners or differently, when the sound is presented at a higher intensity for HI listeners, the neural differences between the two populations might be an effect of stimulus intensity rather than the impact of the hearing loss (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Oates et al., 2002</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Van Dun et al., 2016</xref>; <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">McClannahan et al., 2019</xref>).</p><p hwp:id="p-8">No consensus has been reached on the impact of hearing loss on the P1-N1-P2-complex. This may be due to the complexity of hearing-loss-related research: age and hearing loss are difficult to disentangle as HI listeners are often older. Moreover, when using simple sound stimuli, attention, motivation and task instructions might confound the study results and explain the variability between the findings of above-mentioned studies. In this study, participants listen to continuous speech. Continuous speech is closer to real-life situations than simple sounds or clicks. As understanding speech requires more in-depth neural processing of the stimulus, we believe that the use of continuous speech as a stimulus is essential to investigate the neural changes underlying speech understanding deficits in HI listeners.</p><p hwp:id="p-9">When listening to continuous speech, neural responses time-lock to specific speech characteristics. This phenomenon is called neural tracking, i.e. the brain tracks specific characteristics of the speech (for a review, see, e.g., <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Brodbeck and Simon, 2020</xref>). Neural tracking can be derived from a forward or backward modeling approach. Most of the neural tracking studies focus on a measure of neural tracking derived from a backward modeling approach: a linear model fitted to reconstruct the acoustic representation, e.g., the envelope, from the measured EEG responses. This approach results in a reconstruction accuracy representing how well the acoustic representation is reconstructed from the EEG responses. With the forward modeling approach, a linear model is fitted to predict the EEG responses from the acoustic representation. Such a forward modeling approach yields a prediction accuracy, i.e., to what extent the predicted EEG responses correlate with the actual EEG responses, and a temporal response function (TRF). Such a TRF characterizes the way the brain responds to speech, as a function of measurement electrode and time delay, and therefore, gives unique temporal and spatial information.</p><p hwp:id="p-10">Often neural tracking is studied with acoustic properties of the speech, like the envelope or spectrogram, and by using a backward modeling approach (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Aiken and Picton, 2008</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Ding and Simon, 2012b</xref>). Interestingly, in NH listeners, neural tracking increases when the speech is better understood (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Ding and Simon, 2012a</xref>;<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Horton et al., 2014</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">O’Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Das et al., 2016</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Etard and Reichenbach, 2019</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Iotzov and Parra,2019</xref>; <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Lesenfants et al., 2019</xref>). A limited number of studies has been conducted to study the effect of hearing loss on neural tracking of continuous speech. These studies investigated the effect of hearing loss in a two-talker scenario: an attended speaker and an ignored one (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Petersen et al., 2017</xref>;<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Mirkovic et al., 2019</xref>; <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Presacco et al., 2019</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Decruy et al., 2020</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Fuglsang et al., 2020</xref>). In all these studies, both NH listeners and HI listeners showed a higher neural tracking of the attended speech stream than that of the ignored speech stream. <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">Petersen et al. (2017)</xref> reported that adults with a higher degree of hearing loss showed a higher neural tracking of the ignored speech and no change in the attended stream, suggesting that they experience more difficulties inhibiting irrelevant information. Although <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Mirkovic et al. (2019)</xref> and <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-2" hwp:rel-id="ref-53">Presacco et al. (2019)</xref> did not report a neural difference between the two populations, <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">Decruy et al. (2020</xref>) and <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">Fuglsang et al. (2020)</xref> observed, in contrast to <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-3" hwp:rel-id="ref-51">Petersen et al. (2017)</xref>, an enhanced neural tracking in HI listeners for the attended-speech compared to their normal-hearing peers. This enhancement can indicate a compensation mechanism: HI listeners need to compensate for the degraded auditory input and therefore show enhanced neural tracking.</p><p hwp:id="p-11">By using a forward modeling approach, we aimed to gain more insight into the aforementioned compensation mechanism. An essential factor to consider is the neurophysiological changes that occur as hearing declines. Previous research suggests that due to hearing loss, the brain changes in structure and functionality (for a review, see: <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Cardin, 2016</xref>). These changes in structure and/or function are covered by the concept of neural plasticity, i.e., the ability of the nervous system to change its activity in response to intrinsic or extrinsic stimuli by reorganizing its structure, functions, or connections as defined by <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Mateos-Aparicio and Rodríguez-Moreno (2019)</xref>. We aim to evaluate the functional changes by looking at neural tracking and the latency of the neural responses to continuous speech. Subsequently, we investigate whether these neural responses to speech evoke different activation patterns at the sensor level. If so, this indicates that the underlying neural activity is different and therefore indicates structural changes of the brain.</p><p hwp:id="p-12">The difficulties of researching HI listeners are twofold. First, most HI listeners are older, and ageing also has an impact on brain responses (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">Tremblay et al., 2003</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Harkrider et al., 2006</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Burkard et al., 2007</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Harkrider et al., 2009</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Decruy et al., 2019</xref>; <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Presacco et al., 2016</xref>). Therefore, it is important to compare HI listeners to age-matched normal-hearing peers. Second, audibility of the stimulus must be taken into account: sound presented at the same intensity can be less audible for HI listeners than for NH listeners.</p><p hwp:id="p-13">Previous studies which reported the differences between HI listeners and NH listeners, focused on differences in neural tracking, i.e. reconstruction accuracies or prediction accuracies. Firstly, we studied the effect of hearing loss on prediction accuracy, which should align with previous literature: HI listeners show increased prediction accuracies compared to NH listeners (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-3" hwp:rel-id="ref-28">Fuglsang et al., 2020</xref>). Subsequently, we investigated whether the neural responses to continuous speech (e.g., latency and topography) differed between HI listeners and NH listeners. By examining characteristics of the neural responses to continuous speech, we aimed to better understand the compensation mechanism, and its associated functional as structural changes, on which HI listeners rely.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-5">Materials and Methods</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-6">Participants</title><p hwp:id="p-14">We used a dataset containing EEG of 14 HI listeners (8 female; average age ±std = 58±20) with sensorineural hearing loss and 14 aged-matched normal-hearing peers (13 female; average age ±std = 51±15). The data were collected in a previous study by <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-3" hwp:rel-id="ref-20">Decruy et al. (2020)</xref> (medical ethics committee of the University Hospital of Leuven approved the experiment (S57102); all participants signed an informed consent form). Inclusion criteria were: (1) having Dutch as a mother tongue, (2) having symmetrical hearing (i.e., determined based upon the criteria derived from the AMCLASS algorithm of <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Margolis and Saly (2008)</xref>) and (3) absence of medical conditions and learning disorders. A cognitive screening, the Montreal Cognitive Assessment (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Nasreddine, 2004</xref>), was performed for to ensure the absence of cognitive impairment. Hearing thresholds were determined using pure tone audiometry (125 to 8000 Hz). Normal hearing was defined for all participants where the hearing threshold did not exceed 30 dB HL for frequencies 125 to 4000 Hz (as defined by <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">Decruy et al. (2019)</xref>; average of hearing thresholds within this frequency range in the stimulated ear is denoted as the pure-tone average (PTA)). The hearing thresholds and PTA are shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref> (NH listeners: average PTA ± std= 13.27 ± 5.60 dB HL, HI listeners: average PTA ± std= 44.46 ± 10.54 dB HL).</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><title hwp:id="title-7">Participant details</title><p hwp:id="p-15">Panel A: The hearing thresholds for the stimulated ear. The dashed lines represent the individual hearing thresholds. The full orange or green line annotates the median value for respectively NH listeners and HI listeners. The error bars indicate the range between the 25th and 75th quantile. Panel B: PTA as a function of age for NH listeners (dot, orange) and HI listeners (triangle green). The histograms across the horizontal and vertical axes show the distribution of PTA and age across the participants.</p></caption><graphic xlink:href="427550v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-8">Experimental Procedures</title><sec id="s2b1" hwp:id="sec-5"><title hwp:id="title-9">Behavioural Experiment: Flemish Matrix sentence test</title><p hwp:id="p-16">The Matrix sentence test was performed to determine the participant’s Speech Reception Threshold (SRT) in speech weighted noise (SWN). These Matrix sentences have a standard grammatical structure, consisting of a name, a verb, a numeral, a colour and an object (<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Luts et al., 2014</xref>). The SRT represents the signal-to-noise ratio (SNR) at which 50% of the presented words are recalled correctly. These sentences were presented at a fixed intensity for the NH listeners (55 dB SPL; A-weighted) and an adjusted intensity for the HI listeners to assure a comfortable level and good audibility (more details in the section Stimuli presentation). To determine the SRT, we used an adaptive procedure that adjusted the background noise level based on the correctly recalled words of the Matrix sentence. We opted to adjust the noise level rather than the speech level to assure that the participant’s speech intelligibility was affected by the noise rather than a decrease in audibility.</p></sec><sec id="s2b2" hwp:id="sec-6"><title hwp:id="title-10">EEG Experiment</title><sec id="s2b2a" hwp:id="sec-7"><title hwp:id="title-11">Data acquisition</title><p hwp:id="p-17">A BioSemi ActiveTwo system (Amsterdam, Netherlands) was used to measure EEG signals during stimuli presentation. This system uses 64 Ag/AgCl electrodes placed according to the 10-20 system (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Oostenveld and Praamstra, 2001</xref>). The EEG signals were measured with a sampling frequency of 8192 Hz. All recordings were carried out in a soundproof booth with Faraday cage at ExpORL (Dept. Neurosciences, KU Leuven).</p></sec><sec id="s2b2b" hwp:id="sec-8"><title hwp:id="title-12">Stimuli presentation</title><p hwp:id="p-18">The speech stimuli were presented monaurally through ER-3A insert phones (Etymotic Research Inc, IL, USA) using the software platform APEX (Dept. Neurosciences, KU Leuven) (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Francart et al., 2008</xref>). The stimuli were presented to the right ear unless the participant preferred the left ear (n = 3; 1 NH; 2 HI; ear preference was determined with a Flemish, modified version of the laterality preference inventory of <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Coren (1993)</xref>). All stimuli were set to the same root mean square level and were calibrated.</p><p hwp:id="p-19">For NH listeners, the speech stimuli’ intensity was fixed at 55 dB SPL (A-weighted). To ensure audible stimuli for HI listeners, the stimuli were linearly amplified based on the participant’s hearing thresholds according to the National Acoustics Laboratory-Revised Profound (NAL) algorithm (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Byrne et al., 2001</xref>). To ensure a comfortable level, the overall level was adjusted on a subject-specific basis in addition to the linear amplification so that the stimulus was minimally effortful and comfortable to listen to (mean±std = 57.9 ±6.4 dB SPL; A-weighted). <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-4" hwp:rel-id="ref-20">Decruy et al. (2020)</xref> determined the comfortable loudness level by varying the overall intensity of a story in quiet until participants indicated on a scale that it was comfortable, intelligible, and minimally effortful to understand. The individual presentation levels are reported by <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-5" hwp:rel-id="ref-20">Decruy et al. (2020)</xref>. In the continuation of this manuscript, we use the term <italic toggle="yes">sound intensity</italic> to refer to the SPL levels.</p><p hwp:id="p-20">In summary, we compensated for the hearing loss by two types of amplification: (1) we provided frequency-specific amplification of the stimulus based upon their hearing threshold, and (2) we adjusted the overall sound intensity to a comfortable loudness. Without this amplification, listeners with hearing loss perceive a degraded version of the speech and do not have access to the high-frequency information which is crucial for speech intelligibility. Hence, they might allocate more effort to listen the stimulus. Listening to degraded speech (e.g. <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">Mirkovic et al., 2019</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Verschueren et al., 2021</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Kraus et al., 2020</xref>) or spending more effort to listen to the stimulus (e.g. <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Dimitrijevic et al., 2019</xref>) affects the neural responses to continuous speech. By providing amplification, we aimed to make the peripheral activation levels as similar as possible in both groups, which motivates our choice to model the neural responses to the non-amplified speech features for HI listeners.</p><p hwp:id="p-21">During the EEG recording, 2 Dutch stories were presented: (1) “Milan”, a 12-minute long story narrated by Stijn Vranken (male) presented in quiet and (2) “De Wilde Zwanen” narrated by Katrien Devos (female) presented in 5 different levels of background speech-weighted noise (each lasted around 2 minutes; SNR conditions were shuffled randomly across the 5 different parts). The duration of silences was limited to 200 ms.</p><p hwp:id="p-22">The levels of background noise for the second story depended on the participant’s speech-in-noise performance. As the SRT depends on the presented speech material, we used the self-assessed Békesy procedure to derive the SRT of the story (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-3" hwp:rel-id="ref-19">Decruy et al., 2019</xref>). This procedure uses the relation between the subjective rating of the Matrix sentences and the SRT of the Matrix sentences to estimate the story SRT based upon its subjective rating (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Decruy et al., 2018</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-4" hwp:rel-id="ref-19">2019</xref>). The noise conditions were calculated on the participant’s story adjusted SRT, namely: SRT - 3 dB, SRT, SRT + 3 dB, SRT + 6 dB and a condition without noise, which approximate speech understanding levels of 20%, 50%, 80%, 95% and 100%. This way the level of speech understanding was kept constant rather than the amount of background noise across the different participants. After each condition, the participant was asked to rate how much they understood on a scale from 0 to 100% to obtain the participant’s subjective rating of speech understanding (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">Decruy et al., 2018</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-5" hwp:rel-id="ref-19">2019</xref>) (question: <italic toggle="yes">What percentage of the words did you understand correctly?</italic>; subjective ratings are visualized in <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-8-1" hwp:rel-id="F8">supplementary Figure S.1</xref>).</p></sec></sec></sec><sec id="s2c" hwp:id="sec-9"><title hwp:id="title-13">Signal Processing</title><sec id="s2c1" hwp:id="sec-10"><title hwp:id="title-14">Processing of the EEG signals</title><p hwp:id="p-23">The EEG recording with a sampling frequency of 8192 Hz was downsampled to 256 Hz to decrease processing time. To remove artefacts of eye blinks, we applied multi-channel Wiener filtering to the EEG data to remove artefacts of eye blinks (<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Somers et al., 2018</xref>). Then we referenced the EEG data to the common-average and filtered the data between 0.5 and 25 Hz using a zero-phase Chebyshev filter (Type II with an attenuation of 80 dB at 10% outside the passband; acausal Chebyshev filtering returns similar outputs as causal Least Squares filtering (order = 500, passband weight = 100, stopband weight = 1, stopband frequency at 10% outside the passband frequency), i.e., similar accuracies (Pearson r &gt; 0.999, p &lt; 0.0001), peak latencies (Pearson r &gt; 0.98, p &lt; 0.0001) and TRF patterns (Pearson r &gt; 0.998)). Additional downsampling to 128 Hz was performed.</p></sec><sec id="s2c2" hwp:id="sec-11"><title hwp:id="title-15">Extraction of the speech features</title><p hwp:id="p-24">In this study, we used 2 speech features: spectrogram and acoustical onsets. Both speech features are continuous features which represent the acoustical properties of the speech stimulus.</p><p hwp:id="p-25">To create the spectrogram, the speech stimulus (without amplification) was low-pass filtered below 4000 Hz (zero-phase low-pass FIR filter with a hamming window of 159 samples) because the ER-3A insert phones also low-pass filter at this frequency. A spectrogram representation was obtained using the Gammatone Filterbank Toolkit 1.0 (<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Heeris, 2014</xref>) (centre frequencies between 70 and 4000 Hz with 256 filter channels and an integration window of 0.01 second). This toolkit calculates a spectrogram representation based on a series of gammatone filters inspired by the structure of the human auditory system (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Slaney, 1998</xref>). The resulting 256 filter outputs were averaged into 8 frequency bands (each containing 32 outputs). Additionally, each frequency band was downsampled to the same sampling frequency as the processed EEG, namely 128 Hz. The NAL filtering introduced a delay of 5.333 ms, which was compensated for by padding the non-delayed speech representation with zeros for the delay’s duration at the stimulus’s beginning (filter delay is constant across frequency). The acoustical onsets were calculated as a half-wave rectification, i.e., negative values were set to zero, of the spectrogram’s derivative.</p></sec><sec id="s2c3" hwp:id="sec-12"><title hwp:id="title-16">Prediction accuracies, temporal response function &amp; peak picking method</title><p hwp:id="p-26">In this study, we focused on a linear forward modeling approach that predicts the EEG based on a linear combination of speech features of the presented speech. This forward modeling approach results in 2 outcomes: (a) a TRF and (b) a prediction accuracy. (a) A TRF is a linear approximation of the brain’s impulse response. It is a signal over time that describes how the brain responds to the speech features. (b) TRFs can be used to predict the EEG by convolving it with the speech features. The predicted EEG is then correlated with the actual EEG to obtain a prediction accuracy. Prediction accuracy is considered a measure of neural tracking: the higher the prediction accuracy, the better the brain tracks the stimulus. A schematic overview of this forward modeling approach is given in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-27">Schematic overview of the forward modeling approach. A participant listens to a stimulus (whether or not presented in noise). From the clean stimulus (i.e., without the noise), we extracted two speech features: the spectrogram (A) and the acoustic onsets (B). For visualization purposes, only one of the eight frequency bands is visualized for these features. The forward modeling approach results in a TRF estimate for each EEG channel and each band of both speech features: one TRF for each channel and each spectrogram band (C) or acoustic onsets band (D). Subsequently, these TRFs are convolved with the stimulus features to predict the EEG signals (for visualization purposes, only one EEG channel is visualized). Neural tracking is then calculated as the correlation between the predicted and measured EEG signal (i.e., a prediction accuracy for each EEG channel).</p></caption><graphic xlink:href="427550v3_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-28">(a) To estimate TRFs, we used the Eelbrain toolbox (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Brodbeck, 2020</xref>). The toolbox estimates TRFs using the boosting algorithm by <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">David et al. (2007)</xref> (using a fixed step size of 0.005 after Euclidian normalization of the predictor and EEG data; early stopping based on the ℓ<sub>2</sub>-norm by minimizing the Euclidian distance between the actual and predicted EEG data on the validation partition; kernel basis of 50 ms; parameters were kept constant across all participants). We used 4-fold cross-validation (4 equally long folds; 3 folds used for training, 1 for validation) and an integration window between 0 and 700 ms. The estimated TRFs, averaged across folds and frequency bands, were used to determine the peak latencies.</p><p hwp:id="p-29">(b) To calculate the prediction accuracy, the TRF is applied to left-out EEG to allow a fair comparison between models with a different number of speech features. We used the boosting algorithm with a testing fold. This implies a 4-fold cross-validation with 2 folds for training, 1 fold for validation and 1 fold for testing, which is left-out during training and validation. Each estimated TRF was used to predict the EEG of the left-out testing fold. The predicted EEG of all left-out segments are correlated, using Pearson correlation, with the actual EEG to obtain a prediction accuracy per EEG-electrode. The prediction accuracies were averaged acros sEEG-electrodes and denoted as neural tracking. Similarly, as <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-6" hwp:rel-id="ref-20">Decruy et al. (2020)</xref>, we calculated neural tracking of the second story, presented in different level of background noise, using the TRFs estimated on the story in quiet.</p><p hwp:id="p-30">In summary, to obtain the neural tracking, i.e., prediction accuracy averaged across channels, for the story presented in silence (~13 min data), we used the forward model estimated with testing partition to obtain an unbiased estimate of neural tracking. For the different story parts presented in noise (each ~2 min data), we did not have enough data to estimate a forward model with a testing partition. Therefore, we used the estimated TRF without testing partition of the story in silence to calculate the neural tracking at the different noise levels.</p><p hwp:id="p-31">To investigate the spatial and temporal characteristics of the neural responses, we examined the estimated TRF without a testing partition. This way, we used all the data to estimate the TRF, which leads to a better characterization of neural responses to the two speech features.</p><p hwp:id="p-32">We did not observe a relation between hearing status and the latencies for the different frequency bands (no significant differences except for amplitude differences, similar to (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Brodbeck et al., 2018</xref>)). Moreover, we observed that the peak latency for HI listeners is delayed for each frequency band compared to NH listeners. However, for some frequency bands, the pattern of the TRF is less prominent, which is also observed in other studies that investigate the neural response to the spectrogram representation (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Di Liberto et al., 2015</xref>). As we did not observe an interesting pattern across frequency bands, the TRFs were averaged across the different frequency bands.</p><p hwp:id="p-33">From the TRF, we aimed to identify the amplitude and latency of 3 peaks: P1, N1 and P2. As the EEG data contains 64 different channels, 64 different TRFs were estimated, which made peak picking more complex. To investigate the neural changes associated with the development of a hearing loss, we evaluated the neural responses to speech in two ways: by investigating the topographies and the neural response latency. We expected to see differences in topographies between NH listeners and HI listeners, hence, setting one channel selection to determine the peak latency can bias the result. Therefore, we applied principal component analysis (PCA), a dimensionality reduction method: this method allows identifying the relevant channels on a subject-specific basis. The PCA-method results in (a) signals in component space and (b) corresponding spatial filters which describe the linear combinations of EEG channels to obtain these components. In our analysis, the first component was used. Adding more components up to 4 did not change the findings of this study. In addition to the time course of the component, we also investigated the corresponding spatial filter. As the sign of this spatial filter is arbitrary, we forced the average of occipital and parietal channels (P9, P7, PO7, O1, Oz, O2, PO8, P8, Iz, P10) to be negative by multiplying the spatial filter with −1 when needed. This way, we assured that the PCA component had the same polarity across all participants. The PCA-method was applied to the data per story for each participant.</p><p hwp:id="p-34">To identify the different peaks, we performed a z-score normalization of the TRF in component space and determined the maximal or minimal amplitude for positive and negative peaks in different time regions (P1: 30 to 110 ms, N1: 70 to 210 ms, P2: 110 to 270 ms), respectively. The overlap of these time regions is not an issue as we identified either the maximal or minimal amplitude to determine the peak latency of a positive or negative peak, respectively. To only identify prominent peaks, a peak was discarded from the analysis if the amplitude of the normalized TRF was smaller than the threshold of 1.</p></sec><sec id="s2c4" hwp:id="sec-13"><title hwp:id="title-17">Statistical analysis</title><p hwp:id="p-35">We used the R software package (version 3.6.3) (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">R Core Team, 2020</xref>) and the Buildmer toolbox, which allows identifying the best linear mixed model (LMM) or linear model (LM) given a series of predictors and all their possible interactions based on the likelihood-ratio test (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Voeten, 2020</xref>). Depending on the analysis, we used the following predictors: (a) hearing status (NH or HI) or the PTA depending on whether we were interested in the group effect or the effect of the degree of hearing loss, (b) age and (c) peak type (4 levels: P1 and N1 for acoustic onsets, N1 and P2 for spectrogram). To observe an effect of model choice on prediction accuracy, we also included the predictor (d) model type (Spectrogram, Acoustic onsets, Acoustic onsets + Spectrogram) in the statistical analysis. The analysis over different noise conditions also included the predictor (e) speech understanding. For the analyses with regard to peak latency, all continuous predictors were z-scored to minimize effects due to differences in scale. A matching factor indicated the participants belonging to the same age-matched pair. We included a nested random effect: participant nested inside match, as each match contained a pair of participants, and each participant had multiple dependent observations. The models’ assumptions were checked with a visual inspection of the residual plots to assure homoscedasticity and normality. The models’ outcomes were reported with the regression coefficient (<italic toggle="yes">β</italic>) with standard error (SE), t-ratio and p-value per fixed effect. If significant interaction effects were found or if we aimed to identify differences between levels of a factor, additional Holm-adjusted post-hoc tests were performed by looking at the estimated marginal means, linear trends or pairwise comparisons of these estimates, implemented by the Emmeans toolbox (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Lenth, 2020</xref>). In some instances, to gain more insight into interaction terms, we estimated one predictor’s marginal mean or trend given another continuous predictor. In such cases, we made the latter z-scored predictor discrete to the levels −1 and 1. All post-hoc tests were corrected for multiple comparisons by applying the Holm–Bonferroni method. A significance level of <italic toggle="yes">α</italic> = 0.05 was used.</p><p hwp:id="p-36">To compare differences in spatial filters or topographies of the peaks between the 2 groups, we used a related cluster-based permutation test proposed by <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Maris and Oostenveld (2007)</xref> to determine whether the topography differs between NH listeners and HI listeners, using the Eelbrain implementation (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Brodbeck, 2020</xref>). For these related cluster-based permutation tests the age-matching was preserved. For instance to test whether the topography differed between HI listeners and NH listeners, only the peak topographies of the age-matched participants were considered if both participants showed a prominent peak. A significance level of <italic toggle="yes">α</italic> = 0.05 was used.</p></sec></sec></sec><sec id="s3" hwp:id="sec-14"><title hwp:id="title-18">Results</title><p hwp:id="p-37">First, we evaluated the effect of hearing loss on the prediction accuracy averaged across channels, further denoted as <italic toggle="yes">neural tracking</italic>. Second, we evaluated the effect of hearing loss on the pattern of the TRF. More specifically, we quantified the effect of hearing loss on the neural response peak latencies and their corresponding peak topographies.</p><sec id="s3a" hwp:id="sec-15"><title hwp:id="title-19">Neural differences when listening to speech in quiet</title><sec id="s3a1" hwp:id="sec-16"><title hwp:id="title-20">Differences in neural tracking</title><p hwp:id="p-38">We identified the speech feature(s) that resulted in the highest neural tracking: acoustic onsets, spectrogram or a combination of both speech features. As shown in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> and verified by the statistical analysis, the highest neural tracking was obtained with a combination of both speech features (analysis using LMM: <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>). Additionally, HI listeners showed higher neural tracking compared to the group of NH listeners (on average 0.012 higher; SE = 0.0054, df = 26, t-ratio = 2.2715, p = 0.0316). Age did not have a significant effect on the neural tracking of speech. A Holm-adjusted pairwise comparison confirmed that the highest neural tracking was obtained with a combination of both speech features which was higher compared to the model using just the acoustic onsets (on average the prediction accuracy of the combined model is 0.003 higher; SE = 0.000651; df = 54, t-ratio = 4.521, p = 0.0001) and higher compared to the model using the spectrogram variable (on average the prediction accuracy of the combined model is 0.005 higher; SE = 0.000651; df = 54; t-ratio = 7.230; p &lt; 0.0001). Therefore, in the continuation of this study, we investigated the corresponding TRFs of the model which combines acoustic onsets with the spectrogram.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-39">Neural tracking (Pearson’s r) as a function of different combinations of speech features (‘spectrogram’, ‘acoustic onsets’ and ‘acoustic onsets + spectrogram’, respectively) for both NH listeners (left; orange) and HI listeners (right; green).</p></caption><graphic xlink:href="427550v3_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-4"><p hwp:id="p-40">Linear mixed model: the effect of hearing status and model type on neural tracking. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), degrees of freedom (df), t-Ratios and p-values are reported per fixed effect term. Participant nested in match was included as a random effect.</p><p hwp:id="p-41">Formula: neural tracking ~ 1 + hearing status + model type + (1 | match/participant)</p></caption><graphic xlink:href="427550v3_tbl1" position="float" orientation="portrait" hwp:id="graphic-4"/></table-wrap></sec></sec><sec id="s3b" hwp:id="sec-17"><title hwp:id="title-21">Differences in neural response latencies</title><sec id="s3b1" hwp:id="sec-18"><title hwp:id="title-22">Difference between NH listeners and HI listeners</title><p hwp:id="p-42">In <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4.A</xref>, we visualized the normalized first PCA component of the TRFs for both groups and speech features. The TRFs of HI listeners show delayed neural responses to speech compared to those of NH listeners. Additionally, the average TRF for each speech feature show 2 prominent peaks: P1-peak of acoustic onsets (<italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>) and N1-peak of acoustic onsets (<italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>) for the acoustic onsets, N1-peak of spectrogram (<italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>) and P2-peak of spectrogram (<italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub>) for the spectrogram. Subsequently, we extracted the peak latency of each peak in these TRFs and analyzed whether these peak latencies depended on the considered peak (<italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>, <italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub>; also referred to as peak type), hearing status (NH listeners or HI listeners) and z-scored age. Using the Buildmer toolbox, we identified that the best LMM to predict the peak latency included main effects of peak type, hearing status and age, and an interaction term between age and peak type (<xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-5"><p hwp:id="p-43">An overview of the neural responses of HI listeners (HI; striped line; green, triangle) and NH listeners (NH; orange, dot). Panel A: normalized first PCA component of the TRFs when listening to a story in quiet for both speech features evaluated for both HI listeners and NH listeners. The thick line represents the average TRF over participants. The shaded area indicates the standard error of this average TRF. Panel B: The peak latency in function of the degree of hearing loss (PTA) derived from the neural responses when listening to a story in quiet.</p></caption><graphic xlink:href="427550v3_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-6"><p hwp:id="p-44">Results of the linear mixed model in order to assess peak type, hearing status and age (z-scored) on the peak latency of <italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>, <italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub>. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), degrees of freedom (df), t-Ratios and p-values are reported per fixed effect term. Participant nested in match was included as a random effect.</p><p hwp:id="p-45">Formula: latency ~ 1 + peak + hearing status + age + peak:age + (1 | match/participant)</p></caption><graphic xlink:href="427550v3_tbl2" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap><p hwp:id="p-46">HI listeners showed later peak latencies (an increase of 23 ms, SE = 6.7665, df = 24.0459, t-ratio = 3.3822, p = 0.0025). Depending on which peak is considered, the effect of age on the peak latency changes. In <xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2</xref>, the interaction between age and peak type is shown relative to the age effect for peak <italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>. However, this relative effect does not give insight into the actual trend in the data. Therefore, we performed post-hoc testing to estimate the marginal trends of age for each peak type. This way, we observed that only <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>-latency significantly decreases with increasing age (estimate of marginal trend: −14.41, SE = 4.93, df = 68.8, t-ratio = −2.926, p = 0.0186) while no significant trend was observed for the other peak latencies.</p><p hwp:id="p-47">To obtain the TRFs in component space, we applied PCA on the 64 TRFs (one TRF per EEG channel) and used the first PCA component. This method also returns a corresponding spatial filter of this component. We did not observe a significant difference between the spatial filters of HI listeners and NH listeners, which was the case for both speech features (spatial filters are visualized in <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure S.2</xref>).</p><p hwp:id="p-48">When the peak latencies were determined on the TRF in component space, we extracted the topography of the TRF in sensor space at that latency. Although the spatial filters did not significantly differ between NH listeners and HI listeners, the corresponding peak topographies might. Indeed, HI listeners showed a significantly different topography for <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub> compared to NH listeners (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>). HI listeners showed a more prominent central negativity and a higher occipital positivity which was slightly left-lateralized. However, for the other peak topographies, no significant difference was observed.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-7"><p hwp:id="p-49">Visualization of the topographies of the peaks in the TRFs in sensor space for both speech features, spectrogram and acoustic onsets, and for NH listeners and HI listeners. The channels which drive the significant difference in the <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>-topography, as identified by the cluster-permutation test, are encircled.</p></caption><graphic xlink:href="427550v3_fig5" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s3b2" hwp:id="sec-19"><title hwp:id="title-23">Effect of the degree of hearing loss</title><p hwp:id="p-50">As significant differences in peak latencies were observed between NH listeners and HI listeners, we hypothesized that a higher degree of hearing loss is associated with increased latency of the peaks. Similarly as above, we identified the LMM which explains the variance in the latency. However, instead of using the factor hearing status, we used the continuous variable describing the degree of hearing loss. We justify this approach because the degree of hearing loss (represented by the PTA) is rather continuously distributed across the participants (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref>).</p><p hwp:id="p-51">We observed that the peak latency was affected by the peak type, degree of hearing loss (i.e. z-scored PTA value) and age (z-scored) and all its possible interaction terms (<xref ref-type="table" rid="tbl3" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref>). To investigate how the degree of hearing loss affects the peak latency given the peak type and given the age, we performed a post-hoc test to estimate its marginal trend. The Holm-adjusted estimates of the marginal trend showed that the trend of increasing latency with increasing degree of hearing loss is only significant for older adults for the peak latency of <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub> (estimate of trend = 21.0, SE = 6.44, df = 61.0, t-ratio = 3.260, p = 0.0146) and <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub> (estimate of this trend = 20.4, SE = 6.38, df = 60.5, t-ratio = 3.192, p = 0.0157). However, this trend did not significantly differ between the different peaks nor between younger and older adults.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3:</label><caption hwp:id="caption-8"><p hwp:id="p-52">Results of the linear mixed model in order to assess the effects of degree of hearing loss (PTA) and age (z-scored) on the peak latency of <italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>, <italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub>. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), degrees of freedom (df), t-Ratios and p-values are reported per fixed effect term. Participant nested in the matching factor was included as a random nested effect.</p><p hwp:id="p-53">Formula: latency ~ 1 + peak + PTA + age + peak:age + peak:PTA + peak:PTA:age + (1 | match/participant)</p></caption><graphic xlink:href="427550v3_tbl3" position="float" orientation="portrait" hwp:id="graphic-8"/></table-wrap><p hwp:id="p-54"><xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1</xref> shows that age is not evenly distributed. Therefore, the age effects in the analysis above might be biased towards three younger age-matched pairs. Hence, we replicated the analysis without these three pairs. Using this subset of the data, the best LMM to explain the peak latency did not contain the three-way interaction between age, degree of hearing loss and peak type, nor the interaction between peak type and degree of hearing loss. However, an interaction between age and peak type was still observed. More specifically, how age affects the peak latency of P1ao is significantly different from the age trend seen for the P2s latency (<xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-6-1" hwp:rel-id="T6">Table S.1</xref>). To gain more insight into this interaction between age and peak type, we performed post hoc tests to estimate the marginal trends of age for each peak type. These Holm-adjusted estimates of the effect of age on the peak latency were not significant for any of the peak latencies. This analysis indicates that the observed age effect is not robust: firstly, it is biased by these three age-matched younger pairs, and secondly, although the age trends for the different peaks differ, the post hoc test showed that the marginal trends do not reach significance. Therefore, this effect was not visualized in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>.</p><p hwp:id="p-55">However, it is reassuring to observe that even when taking a subset of the dataset, we observe that the peak latency is affected by the degree of hearing loss (estimate = 12.895, SE = 3.518, df = 17.872, t-ratio = 3.665, p = 0.002). This observed trend was independent of age and peak type.</p><p hwp:id="p-56">To verify whether the effect of the degree of hearing loss on the peak latency is robust, we repeated the above analysis using only HI listeners. Although this reduced the statistical power, we observed a significant effect of degree of hearing loss on the <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>-latency: HI listeners with a more severe hearing loss showed an increased latency (analysis using LM and scaled predictors; <xref ref-type="table" rid="tblS2" hwp:id="xref-table-wrap-7-1" hwp:rel-id="T7">Table S.2</xref>; estimate = 34.89, SE = 12.20, t-ratio = 2.860, p = 0.0188). We did not observe a significant effect of hearing loss or age on the peak latency for the other peak types.</p></sec></sec><sec id="s3c" hwp:id="sec-20"><title hwp:id="title-24">Neural differences when speech understanding decreases</title><sec id="s3c1" hwp:id="sec-21"><title hwp:id="title-25">Differences in neural tracking</title><p hwp:id="p-57">The effect of increased neural tracking for HI listeners was robust over different levels of background noise (estimate = 0.0154, SE = 0.0046, df = 25.1566, t-ratio = 3.3594, p = 0.0025); <xref ref-type="table" rid="tbl4" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Table 4</xref>; <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref>). Additionally, higher neural tracking was observed with increasing age (estimate = 3e-04, SE = 1e-04, df = 24.9812, t-ratio = 2.2452, p = 0.0339); <xref ref-type="table" rid="tbl4" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">Table 4</xref>; <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6</xref>) and with increasing speech understanding (estimate = 2e-04, SE = 0, df = 115.3178, t-ratio = 5.6547, p &lt; 0.001; <xref ref-type="table" rid="tbl4" hwp:id="xref-table-wrap-4-3" hwp:rel-id="T4">Table 4</xref>; <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6</xref>). No significant interaction effect was observed between hearing status and speech understanding.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-9"><p hwp:id="p-58">Neural tracking (Pearson’s r) as a function of speech understanding. For visualisation purposes, the effect of age was discretized with 2 levels: the average age of participants younger than 50 years (level young; 31 years; pink) and average age of participants older than 50 years (level old; 68 years; purple) for NH listeners and HI listeners. The level of 50 years was set by visual inspection of the age distribution (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1</xref>).</p></caption><graphic xlink:href="427550v3_fig6" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><table-wrap id="tbl4" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2 xref-table-wrap-4-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBL4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tbl4</object-id><label>Table 4:</label><caption hwp:id="caption-10"><p hwp:id="p-59">Linear mixed model: the effect of hearing status and speech understanding on neural tracking. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), degrees of freedom (df), t-Ratios and p-values are reported per fixed effect term. Participant nested in match was included as a random effect. Formula: neural tracking ~ 1 + hearing status + age + speech understanding + (1 | match/participant)</p></caption><graphic xlink:href="427550v3_tbl4" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap></sec></sec><sec id="s3d" hwp:id="sec-22"><title hwp:id="title-26">Differences in neural response latencies</title><sec id="s3d1" hwp:id="sec-23"><title hwp:id="title-27">Effect of the degree of hearing loss</title><p hwp:id="p-60">Subsequently, we analysed how the neural response latency is affected by speech understanding, age and the degree of hearing loss for the second story presented in different levels of background noise. From the above analysis using speech in silence, we observed that the degree of hearing loss influences the peak latencies. Therefore, we here immediately consider the effect of hearing loss as a continuous variable, i.e. degree of hearing loss, instead of a categorical predictor, i.e. hearing status.</p><p hwp:id="p-61">Using Buildmer, we identified the best LMM to explain the second story’s peak latencies, which was presented in different levels of background noise. This model shows that the peak latency of the neural responses depends on the peak type, age and degree of hearing loss (<xref ref-type="table" rid="tbl5" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Table 5</xref>; <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7.B</xref>). Moreover, we observed interactions between speech understanding and degree of hearing loss, between peak type and speech understanding, between degree of hearing loss and age, between peak type and age and speech understanding and age. These interactions show that the effect of age and speech understanding on peak latency depended on the peak type. However, by looking at the estimates of these age effects, we observe that these are not consistent across the different peak types. Furthermore, from the analysis using speech in silence, we observed that the age effect is not a robust effect. Indeed, post-hoc tests showed that the marginal trends of age on the peak latency did not show a significant effect for any of the peak types. Therefore, this effect is not visualized in <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7.B</xref>.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-11"><p hwp:id="p-62">An overview of the neural responses of HI listeners (HI; green, triangle) and NH listeners (NH; orange, dot) when listening to speech in silence and speech presented in noise. Panel A: normalized first PCA component of the TRFs when listening to a story part in quiet (full line) and a story part in noise (striped line) for both speech features evaluated for both HI listeners and NH listeners for visualization purpose only the lowest SNR is visualized, namely SRT - 3 dB). The shaded area indicates the standard error of the average TRF which is represented by the full line. Panel B: The corresponding peak latencies in function of speech understanding. The effect of degree of hearing loss was made discrete at 2 levels, the average hearing thresholds of all NH listeners (level NH listeners: 13 dB HL) and subjects with hearing loss (level HI listeners: 44 dB HL) and is represented by the regression lines with confidence intervals (shaded area).</p></caption><graphic xlink:href="427550v3_fig7" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><table-wrap id="tbl5" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBL5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tbl5</object-id><label>Table 5:</label><caption hwp:id="caption-12"><p hwp:id="p-63">Results of the linear mixed model in order to assess the effects of degree of hearing loss (z-scored), speech understanding (z-scored) and age (z-scored) on the peak latency of <italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>, <italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub>. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), degrees of freedom (df), t-Ratios and p-values are reported per fixed effect term. Participant nested in the matching factor was included as a random nested effect.</p><p hwp:id="p-64">Formula: latency ~ 1 + peak + SI + degree of hearing loss + speech understanding:degree of hearing loss + peak:speech understanding + age +degree of hearing loss:age + peak:age + speech understanding:age + (1 | match/participant)</p></caption><graphic xlink:href="427550v3_tbl5" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap><p hwp:id="p-65">Interestingly, a significant interaction effect between speech understanding and degree of hearing loss was found (estimate = 4.441, SE = 1.071, df = 396.822, t-ratio = 4.148, p &lt; .001). Since there is no three-way interaction between peak type, speech understanding, and the degree of hearing loss, this indicates that interaction between the degree of hearing loss and speech understanding is similar for all peak types. The effect of speech understanding on the peak latency has a negative estimate, which implies that with increasing speech understanding, the latency decreases. This trend is consistent for all four peak types. The interaction term’s estimate is positive and indicates that as the degree of hearing loss increases, the effect of speech understanding on the peak latency becomes less negative and slightly flattens out, which is also visible in <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7</xref>.</p><p hwp:id="p-66">We performed a post-hoc test to estimate the marginal trend of speech understanding for each peak type and hearing loss to gain more insight into this effect. Indeed, as observed in <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Figure 7</xref>: with increasing speech understanding, latency decreases for NH listeners in all peak types (P1ao: estimate = −6.902, SE = 2.50, df=386, t-ratio = −2.760, p-value = 0.0303; N1ao: estimate = −11.388, SE = 2.14, df = 385, t-ratio = −5.310, p-value &lt; 0.0001; N1s: estimate = −8.050, SE = 2.32, df = 385, t-ratio = −3.469, p-value = 0.0035; P2s: estimate = −15.605, SE = 2.23, df = 384, t-ratio = −6.991, p-value &lt; 0.0001), while this effect for HI listeners is only observed for the P2s-latency (estimate = −6.722, SE = 2.44, df = 393, t-ratio = −2.756, p-value = 0.0303) and not for other peak types.</p><p hwp:id="p-67">The effect of the degree of hearing loss on the peak amplitude was not consistent for all peaks and therefore not elaborated upon in this manuscript.</p></sec></sec></sec><sec id="s4" hwp:id="sec-24"><title hwp:id="title-28">Discussion</title><p hwp:id="p-68">We compared the neural responses to continuous speech of adults with a sensorineural hearing loss with those of age-matched normal-hearing peers. We found that HI listeners show higher neural tracking and increased peak latencies in their neural responses. Across noise conditions, NH listeners showed increased latencies as speech understanding decreased. However, for adults with hearing loss, this increase in latency was not observed.</p><sec id="s4a" hwp:id="sec-25"><title hwp:id="title-29">Higher neural tracking of speech in hearing-impaired listeners</title><p hwp:id="p-69">By evaluating neural tracking, we concluded that (1) higher neural tracking is observed for a combination of the spectrogram and acoustic onsets compared to the speech features individually and (2) HI listeners show enhanced neural tracking compared to normal-hearing peers.</p><p hwp:id="p-70">The combination of the two speech features results in higher neural tracking than the individual features alone. Therefore, it suggests that the acoustic onsets describe different neural response characteristics than the spectrogram and vice versa. Following <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Hamilton et al. (2018)</xref> and <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Brodbeck et al. (2020)</xref>, we hypothesize that both speech features allow a differentiation between sustained activity, i.e. ongoing sounds, represented by the spectrogram, and transient activity, i.e. onset of the sound, represented by acoustic onsets.</p><p hwp:id="p-71">Using a forward instead of a backward modeling approach, we observed enhanced neural tracking in HI listeners. Although the conclusions of a forward and backward model are expected to converge, it is reassuring to observe that the conclusions align even though we used different speech features and a different modeling approach. Accordingly, using the same dataset but a different modeling approach, our results agree with those of <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-7" hwp:rel-id="ref-20">Decruy et al. (2020)</xref>. Also <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-4" hwp:rel-id="ref-28">Fuglsang et al. (2020)</xref> reported that HI listeners have higher neural tracking than NH listeners of the attended speaker. Nevertheless, <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-3" hwp:rel-id="ref-53">Presacco et al. (2019)</xref> did not find a difference in neural tracking between the two populations. However, in their study, the populations were not closely age-matched, while ageing is known to increase neural tracking (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">Presacco et al., 2016</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-6" hwp:rel-id="ref-19">Decruy et al., 2019</xref>).</p><p hwp:id="p-72">Like previous literature, we also observed that neural tracking decreases with decreasing speech understanding (<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-2" hwp:rel-id="ref-59">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">Lesenfants et al., 2019</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-8" hwp:rel-id="ref-20">Decruy et al., 2020</xref>). Even when the speech is presented with background noise, HI listeners showed enhanced neural tracking of speech. This suggests evidence for a compensation mechanism: higher neural tracking indicates more neural activity to compensate for the degraded auditory input (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Eggermont, 2017</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-5" hwp:rel-id="ref-28">Fuglsang et al.,2020</xref>). Although neural tracking of speech was enhanced in HI listeners, the effect of speech understanding on the neural tracking was similar for both populations. We also observed an effect of age: older adults showed higher neural tracking compared to younger adults. Although this converges with the results of ageing studies (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-3" hwp:rel-id="ref-52">Presacco et al., 2016</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-7" hwp:rel-id="ref-19">Decruy et al., 2019</xref>), we are hesitant to interpret this effect, as in subsequent analyses, we inferred that the age effects are biased towards the three younger age-matched pairs (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1</xref>).</p></sec><sec id="s4b" hwp:id="sec-26"><title hwp:id="title-30">Hearing-impaired listeners process speech less efficiently</title><p hwp:id="p-73">HI listeners showed significantly increased latencies compared to their age-matched normal-hearing peers when they listened to a story presented in quiet (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4.A</xref>). Additionally, the delay in neural responses increased with a higher degree of hearing loss (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4.B</xref>).</p><p hwp:id="p-74">Investigating the CAEP-response to syllables, <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-3" hwp:rel-id="ref-13">Campbell and Sharma (2013)</xref> and <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Bidelman et al. (2019b)</xref> have reported an increased P2 latency with worse speech perception in noise (QuickSIN scores) but not with the degree of hearing loss. However, in both studies, the speech was presented at the same intensity to both HI listeners and NH listeners. <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">McClannahan et al. (2019)</xref> remarked that differences in the audibility of the stimulus might explain the differences in neural response latency: at the same intensity, HI listeners will have a worse perception of the speech compared to NH listeners. Indeed, <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-2" hwp:rel-id="ref-60">Verschueren et al. (2021)</xref> concluded that when the audibility of the stimulus decreases, the latency of the neural response to continuous speech increases. However, this is only true at intensities where the speech audibility affects the listening effort and speech understanding. For NH listeners, the latency reaches a plateau at a comfortable loudness (intensities of 60 dB or higher). Here, we amplified the stimulus based on the participants’ hearing thresholds and presented the speech at a subject-specific intensity to assure comfortable listening for HI listeners. Since we compared the latency of the neural responses at a comfortable loudness, i.e. when the latency has reached a plateau, we can assume that the increased latencies for HI listeners are due to the impact of the hearing loss rather than the impact of a different speech intensity.</p><p hwp:id="p-75">Even though the sound was amplified, HI listeners showed increased latencies. Therefore, we hypothesize that there are some intrinsic differences in neural speech processing between HI listeners and NH listeners.A possible explanation may be that HI listeners process speech less efficiently, as proposed by <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Bidelman et al. (2019a)</xref>. Using functional connectivity analysis, <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Bidelman et al. (2019a)</xref> showed that HI listeners have higher eccentricity networks, i.e. brain areas communicate in a chain-like fashion rather than star-like. Higher eccentricity suggests (a) more long-range neural signalling, i.e. more extended neural communication pathways, and (b) less efficient information exchange among different brain regions. (a) More extended neural communication pathways may reflect a form of compensation in which additional brain regions are recruited to understand the degraded auditory input. This is supported by increased frontal activation in HI listeners in the neural responses to simple sounds (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-4" hwp:rel-id="ref-13">Campbell and Sharma, 2013</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">Bidelman et al., 2019b</xref>). Similarly, using continuous speech rather than simple repeated sounds, we showed that HI listeners have a significantly different <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub> peak topography. Such a peak topography reflects the sensor response to the activity of underlying neural sources, which respond in a time-locked fashion to the speech material. Therefore, a significant difference in topography suggests the recruitment of additional or different underlying neural sources (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5</xref>). (b) When more or different brain regions are involved to process the speech, it causes longer communication pathways in the brain and therefore decreases the neural speech processing efficiency (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">Bidelman et al., 2019b</xref>). Here, we propose the neural response latency as a marker for the efficiency of neural processing of continuous, natural speech: less efficient speech processing is reflected by increased neural response latency as information exchange is hampered due to more involved brain regions and longer communication pathways.</p><p hwp:id="p-76">In summary, even though the sound was amplified, HI listeners rely on a compensation mechanism: more brain regions are involved to understand the speech, which decreases the efficiency of the neural speech processing and increases the neural response latency. The efficiency of neural speech processing decreases as the severity of the hearing loss increases. Therefore, we hypothesize that this compensation mechanism is a gradual effect which occurs as the hearing deteriorates.</p><p hwp:id="p-77">When the speech understanding decreases, NH listeners showed a prominent increase in latency, while this was less prominent for HI listeners. In several studies, it has been shown that NH listeners show an increased neural response latency with increasing task demand due to lower stimulus intensity, increasing background noise or stimulus vocoding. This is the case for neural processing of continuous speech (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-4" hwp:rel-id="ref-45">Mirkovic et al., 2019</xref>;<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-3" hwp:rel-id="ref-60">Verschueren et al., 2021</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">Kraus et al., 2020</xref>) as well as simple sounds (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">Billings et al., 2015</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-2" hwp:rel-id="ref-58">Van Dun et al., 2016</xref>; <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">Maamor and Billings, 2017</xref>; <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-3" hwp:rel-id="ref-44">McClannahan et al., 2019</xref>).</p><p hwp:id="p-78">However, we did not observe this increase in latency with increasing task demand in adults with a hearing loss (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Figure 7.B</xref>). We can integrate these findings with the results of <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-3" hwp:rel-id="ref-4">Bidelman et al. (2019a)</xref>. Although <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">Bidelman et al. (2019b)</xref> did not report an increase in P2-latency when the stimulus was noise-degraded, in the same data, <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-4" hwp:rel-id="ref-4">Bidelman et al. (2019a)</xref> reported that when the stimulus was noise-degraded, NH listeners showed more long-range neural signalling, whereas this was not seen for HI listeners (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-5" hwp:rel-id="ref-4">Bidelman et al., 2019a</xref>). Our data support the latter finding: in NH listeners the neural response latency increases as the speech understanding decreases due to increasing level of background noise, while this was less prominent for HI listeners. Following the above reasoning: as the stimulus is noise-degraded, the task demand increases. Therefore, more processing time is required to attend the speech stream and ignore the noise, which decreases the efficiency of neural auditory processing, i.e. the response latency is a marker for the efficiency of neural processing of speech. However, this effect is only observed for NH listeners.</p><p hwp:id="p-79">For HI listeners, this is not the case: when background noise increases, processing efficiency does not decrease, i.e. no increase in latency, but remains elevated. This aligns with the finding of <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-6" hwp:rel-id="ref-4">Bidelman et al. (2019a)</xref>: they reported that HI listeners did not show more long-range neural signalling when the stimulus was noise-degraded. Assuming that longer latencies are a marker for less efficient neural processing and thus the number of recruited brain regions: our results suggest that HI listeners recruit already a large number of brain regions to understand speech in quiet. Their neural response latency does not increase with an increasing amount of background noise, indicating that no additional brain regions could be required to compensate for the increased demand.</p><p hwp:id="p-80">Our findings explain why <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-5" hwp:rel-id="ref-45">Mirkovic et al. (2019)</xref> did not find a difference in latency between NH listeners and HI listeners as they presented only two noise conditions. As the noise level increases, the difference in latency between the two populations becomes smaller, which reduces the likelihood of a statistical difference between the two populations.</p><p hwp:id="p-81">A caveat of the current study is that the age distribution across the different age-matched pairs is not uniform. Therefore, we are hesitant to interpret the current age effects (or interactions with age) in the current dataset as these might be biased towards the younger-age matched pairs. We suggest that future research should investigate the effect of hearing loss on the neural responses by exploring a dataset with age-matched pairs, which are uniformly distributed across the age range. This way, the effect of age and hearing loss on the neural responses to continuous speech can be disentangled. We have to note, however, that finding young HI listeners is challenging. Moreover, the aetiology of hearing loss should also be taken into account as it might affect how the brain responds to continuous speech.</p><p hwp:id="p-82">Finally, we would like to highlight the difference in the trend of neural tracking and neural response latency. As speech understanding decreases, neural tracking decreases for both NH and HI listeners while the neural response latency remains constant (HI) or increases (NH). This difference in trend suggests that both measures capture different underlying neural processes for speech comprehension.</p></sec></sec><sec id="s5" hwp:id="sec-27"><title hwp:id="title-31">Conclusion</title><p hwp:id="p-83">In this study, we compared the neural responses to continuous speech of adults with a sensorineural hearing loss with those of age-matched normal-hearing peers. HI listeners showed increased peak latencies of their neural responses. Interestingly, the latency increases as the degree of hearing loss increases. Across noise conditions, latency generally increases as the listening conditions become more difficult. However, for HI listeners, this increase in latency is not observed. We here suggest latency as a marker for the efficiency of neural processing to understand continuous, natural speech.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-32">Acknowledgements</title><p hwp:id="p-84">The authors would like to thank all members of the ExpORL ISIFIT team for their weekly guidance. Furthermore, we would like to thank Christian Brodbeck. He helped us with applying the Eelbrain toolbox, which accelerated this research project.</p></ack><sec hwp:id="sec-28"><title hwp:id="title-33">Conflict of Interest</title><p hwp:id="p-85">none</p></sec><sec hwp:id="sec-29"><title hwp:id="title-34">Grants</title><p hwp:id="p-86">The presented study received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Tom Francart; grant agreement No. 637424). Research of Marlies Gillis (PhD grant: SB 1SA0620N) and Jonas Vanthornout (postdoctoral grant: 1290821N) was funded by the Research Foundation Flanders (FWO).</p></sec><sec hwp:id="sec-30"><title hwp:id="title-35">Contributions</title><p hwp:id="p-87">M.G., L.D., J.V., and T.F. designed and performed research. M.G. and L.D. analysed data. M.G., L.D., J.V., and T.F. wrote the paper.</p></sec><sec hwp:id="sec-31"><title hwp:id="title-36">Data availability statement</title><p hwp:id="p-88">The data that support the findings of this study can be made available upon request, in so far as this is in agreement with privacy and ethical regulations.</p></sec><glossary hwp:id="glossary-1"><title hwp:id="title-37">Acronyms</title><def-list hwp:id="def-list-1"><def-item hwp:id="def-item-1"><term hwp:id="term-1"><italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub></term><def hwp:id="def-1"><p hwp:id="p-89">N1-peak of spectrogram. 14, 15, 17, 18, 21, 25, 34</p></def></def-item><def-item hwp:id="def-item-2"><term hwp:id="term-2"><italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub></term><def hwp:id="def-2"><p hwp:id="p-90">N1-peak of acoustic onsets. 14, 15, 17, 18, 21, 34</p></def></def-item><def-item hwp:id="def-item-3"><term hwp:id="term-3"><italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub></term><def hwp:id="def-3"><p hwp:id="p-91">P1-peak of acoustic onsets. 14, 18, 21, 34</p></def></def-item><def-item hwp:id="def-item-4"><term hwp:id="term-4"><italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub></term><def hwp:id="def-4"><p hwp:id="p-92">P2-peak of spectrogram. 14, 18, 21, 34</p></def></def-item><def-item hwp:id="def-item-5"><term hwp:id="term-5"><italic toggle="yes">β</italic></term><def hwp:id="def-5"><p hwp:id="p-93">regression coefficient. 11</p></def></def-item><def-item hwp:id="def-item-6"><term hwp:id="term-6">CAEP</term><def hwp:id="def-6"><p hwp:id="p-94">cortical auditory evoked potentials. 2, 24</p></def></def-item><def-item hwp:id="def-item-7"><term hwp:id="term-7">EEG</term><def hwp:id="def-7"><p hwp:id="p-95">electroencephalography. 2, 4, 6-10</p></def></def-item><def-item hwp:id="def-item-8"><term hwp:id="term-8">HI</term><def hwp:id="def-8"><p hwp:id="p-96">listeners hearing impaired listeners. 2-7, 10, 12-17, 19, 20, 22-26, 34</p></def></def-item><def-item hwp:id="def-item-9"><term hwp:id="term-9">LM</term><def hwp:id="def-9"><p hwp:id="p-97">linear model. 11, 17</p></def></def-item><def-item hwp:id="def-item-10"><term hwp:id="term-10">LMM</term><def hwp:id="def-10"><p hwp:id="p-98">linear mixed model. 11, 12, 15, 20</p></def></def-item><def-item hwp:id="def-item-11"><term hwp:id="term-11">N1</term><def hwp:id="def-11"><p hwp:id="p-99">first negative peak. 2, 10, 11</p></def></def-item><def-item hwp:id="def-item-12"><term hwp:id="term-12">NH listeners</term><def hwp:id="def-12"><p hwp:id="p-100">normal hearing listeners. 2-6, 10, 12-17, 19, 20, 22-26</p></def></def-item><def-item hwp:id="def-item-13"><term hwp:id="term-13">P1</term><def hwp:id="def-13"><p hwp:id="p-101">first positive peak. 2, 10, 11</p></def></def-item><def-item hwp:id="def-item-14"><term hwp:id="term-14">P2</term><def hwp:id="def-14"><p hwp:id="p-102">later positive peak. 2, 10, 11, 24, 25</p></def></def-item><def-item hwp:id="def-item-15"><term hwp:id="term-15">PCA</term><def hwp:id="def-15"><p hwp:id="p-103">principal component analysis. 10, 11</p></def></def-item><def-item hwp:id="def-item-16"><term hwp:id="term-16">PTA</term><def hwp:id="def-16"><p hwp:id="p-104">pure-tone average. 4, 5, 11, 15, 16</p></def></def-item><def-item hwp:id="def-item-17"><term hwp:id="term-17">SE</term><def hwp:id="def-17"><p hwp:id="p-105">standard error. 11</p></def></def-item><def-item hwp:id="def-item-18"><term hwp:id="term-18">SNR</term><def hwp:id="def-18"><p hwp:id="p-106">signal-to-noise ratio. 6, 7</p></def></def-item><def-item hwp:id="def-item-19"><term hwp:id="term-19">SRT</term><def hwp:id="def-19"><p hwp:id="p-107">Speech Reception Threshold. 5-7</p></def></def-item><def-item hwp:id="def-item-20"><term hwp:id="term-20">SWN</term><def hwp:id="def-20"><p hwp:id="p-108">speech weighted noise. 5</p></def></def-item><def-item hwp:id="def-item-21"><term hwp:id="term-21">TRF</term><def hwp:id="def-21"><p hwp:id="p-109">temporal response function. 3, 8-12, 14-17, 22</p><p hwp:id="p-110">[title=Abbreviations]</p></def></def-item></def-list></glossary><ref-list hwp:id="ref-list-1"><title hwp:id="title-38">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Aiken S. J."><surname>Aiken</surname>, <given-names>S. J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Picton T. W."><surname>Picton</surname>, <given-names>T. W.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-2">Human cortical responses to the speech envelope</article-title>. <source hwp:id="source-1">Ear and hearing</source>, <volume>29</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>157</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Alain C."><surname>Alain</surname>, <given-names>C.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-3">Effects of age-related hearing loss and background noise on neuromagnetic activity from auditory cortex</article-title>. <source hwp:id="source-2">Frontiers in systems neuroscience</source>, <volume>8</volume>:<fpage>8</fpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bertoli S."><surname>Bertoli</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Probst R."><surname>Probst</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bodmer D."><surname>Bodmer</surname>, <given-names>D.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-4">Late auditory evoked potentials in elderly long-term hearing-aid users with unilateral or bilateral fittings</article-title>. <source hwp:id="source-3">Hearing research</source>, <volume>280</volume>(<issue>1-2</issue>):<fpage>58</fpage>–<lpage>69</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2 xref-ref-4-3 xref-ref-4-4 xref-ref-4-5 xref-ref-4-6"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bidelman G. M."><surname>Bidelman</surname>, <given-names>G. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mahmud M. S."><surname>Mahmud</surname>, <given-names>M. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yeasin M."><surname>Yeasin</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shen D."><surname>Shen</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arnott S. R."><surname>Arnott</surname>, <given-names>S. R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Alain C."><surname>Alain</surname>, <given-names>C.</given-names></string-name> (<year>2019a</year>). <article-title hwp:id="article-title-5">Age-related hearing loss increases full-brain connectivity while reversing directed signaling within the dorsal–ventral pathway for speech</article-title>. <source hwp:id="source-4">Brain Structure and Function</source>, <volume>224</volume>(<issue>8</issue>):<fpage>2661</fpage>–<lpage>2676</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Bidelman G. M."><surname>Bidelman</surname>, <given-names>G. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Price C. N."><surname>Price</surname>, <given-names>C. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shen D."><surname>Shen</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arnott S. R."><surname>Arnott</surname>, <given-names>S. R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Alain C."><surname>Alain</surname>, <given-names>C.</given-names></string-name> (<year>2019b</year>). <article-title hwp:id="article-title-6">Afferent-efferent connectivity between auditory brainstem and cortex accounts for poorer speech-in-noise comprehension in older adults</article-title>. <source hwp:id="source-5">Hearing research</source>, <volume>382</volume>:<fpage>107795</fpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Billings C. J."><surname>Billings</surname>, <given-names>C. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Penman T. M."><surname>Penman</surname>, <given-names>T. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McMillan G. P."><surname>McMillan</surname>, <given-names>G. P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Ellis E."><surname>Ellis</surname>, <given-names>E.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-7">Electrophysiology and perception of speech in noise in older listeners: effects of hearing impairment &amp; age</article-title>. <source hwp:id="source-6">Ear and hearing</source>, <volume>36</volume>(<issue>6</issue>):<fpage>710</fpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="website" citation-type="web" ref:id="2021.01.21.427550v3.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Brodbeck C."><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-8">Eelbrain 0.32</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.5281/zenodo.3923991" ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.3923991" hwp:id="ext-link-1">http://doi.org/10.5281/zenodo.3923991</ext-link>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Brodbeck C."><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hong L. E."><surname>Hong</surname>, <given-names>L. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-9">Rapid transformation from auditory to linguistic representations of continuous speech</article-title>. <source hwp:id="source-7">Current Biology</source>, <volume>28</volume>(<issue>24</issue>):<fpage>3976</fpage>–<lpage>3983</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="other" citation-type="journal" ref:id="2021.01.21.427550v3.9" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Brodbeck C."><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jiao A."><surname>Jiao</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hong L. E."><surname>Hong</surname>, <given-names>L. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-10">Neural speech restoration at the cocktail party: Auditory cortex recovers masked speech of both attended and ignored speakers</article-title>. <source hwp:id="source-8">bioRxiv</source>, page <fpage>866749</fpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="other" citation-type="journal" ref:id="2021.01.21.427550v3.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Brodbeck C."><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-11">Continuous speech processing</article-title>. <source hwp:id="source-9">Current Opinion in Physiology</source>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><citation publication-type="book" citation-type="book" ref:id="2021.01.21.427550v3.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Burkard R. F."><surname>Burkard</surname>, <given-names>R. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eggermont J. J."><surname>Eggermont</surname>, <given-names>J. J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Don M."><surname>Don</surname>, <given-names>M.</given-names></string-name> (<year>2007</year>). <source hwp:id="source-10">Auditory evoked potentials: basic principles and clinical application</source>. <publisher-name>Lippincott Williams &amp; Wilkins</publisher-name>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Byrne D."><surname>Byrne</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dillon H."><surname>Dillon</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ching T."><surname>Ching</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Katsch R."><surname>Katsch</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Keidser G."><surname>Keidser</surname>, <given-names>G.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-12">Nal-nl1 procedure for fitting nonlinear hearing aids: characteristics and comparisons with other procedures</article-title>. <source hwp:id="source-11">Journal of the American academy of audiology</source>, <volume>12</volume>(<issue>1</issue>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2 xref-ref-13-3 xref-ref-13-4"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Campbell J."><surname>Campbell</surname>, <given-names>J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sharma A."><surname>Sharma</surname>, <given-names>A.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-13">Compensatory changes in cortical resource allocation in adults with hearing loss</article-title>. <source hwp:id="source-12">Frontiers in systems neuroscience</source>, <volume>7</volume>:<fpage>71</fpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Cardin V."><surname>Cardin</surname>, <given-names>V.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-14">Effects of aging and adult-onset hearing loss on cortical auditory regions</article-title>. <source hwp:id="source-13">Frontiers in Neuroscience</source>, <volume>10</volume>:<fpage>199</fpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Coren S."><surname>Coren</surname>, <given-names>S.</given-names></string-name> (<year>1993</year>). <article-title hwp:id="article-title-15">The lateral preference inventory for measurement of handedness, footedness, eyedness, and earedness: Norms for young adults</article-title>. <source hwp:id="source-14">Bulletin of the Psychonomic Society</source>, <volume>31</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>3</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Das N."><surname>Das</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Biesmans W."><surname>Biesmans</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bertrand A."><surname>Bertrand</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-16">The effect of head-related filtering and ear-specific decoding bias on auditory attention detection</article-title>. <source hwp:id="source-15">Journal of neural engineering</source>, <volume>13</volume>(<issue>5</issue>):<fpage>056014</fpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="David S. V."><surname>David</surname>, <given-names>S. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mesgarani N."><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Shamma S. A."><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-17">Estimating sparse spectro-temporal receptive fields with natural stimuli</article-title>. <source hwp:id="source-16">Network: Computation in neural systems</source>, <volume>18</volume>(<issue>3</issue>):<fpage>191</fpage>–<lpage>212</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Decruy L."><surname>Decruy</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Das N."><surname>Das</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Verschueren E."><surname>Verschueren</surname>, <given-names>E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-18">The self-assessed Békesy procedure: validation of a method to measure intelligibility of connected discourse</article-title>. <source hwp:id="source-17">Trends in hearing</source>, <volume>22</volume>:<fpage>2331216518802702</fpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2 xref-ref-19-3 xref-ref-19-4 xref-ref-19-5 xref-ref-19-6 xref-ref-19-7"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Decruy L."><surname>Decruy</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vanthornhout J."><surname>Vanthornhout</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-19">Evidence for enhanced neural tracking of the speech envelope underlying age-related speech-in-noise difficulties</article-title>. <source hwp:id="source-18">Journal of neurophysiology</source>, <volume>122</volume>(<issue>2</issue>):<fpage>601</fpage>–<lpage>615</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2 xref-ref-20-3 xref-ref-20-4 xref-ref-20-5 xref-ref-20-6 xref-ref-20-7 xref-ref-20-8"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Decruy L."><surname>Decruy</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vanthornhout J."><surname>Vanthornhout</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-20">Hearing impairment is associated with enhanced neural tracking of the speech envelope</article-title>. <source hwp:id="source-19">Hearing Research</source>, page <fpage>107961</fpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Di Liberto G. M."><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Sullivan J. A."><surname>O’Sullivan</surname>, <given-names>J. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Lalor E. C."><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-21">Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title>. <source hwp:id="source-20">Current Biology</source>, <volume>25</volume>(<issue>19</issue>):<fpage>2457</fpage>–<lpage>2465</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Dimitrijevic A."><surname>Dimitrijevic</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith M. L."><surname>Smith</surname>, <given-names>M. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kadis D. S."><surname>Kadis</surname>, <given-names>D. S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Moore D. R."><surname>Moore</surname>, <given-names>D. R.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-22">Neural indices of listening effort in noisy environments</article-title>. <source hwp:id="source-21">Scientific Reports</source>, <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Ding N."><surname>Ding</surname>, <given-names>N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> (<year>2012a</year>). <article-title hwp:id="article-title-23">Emergence of neural encoding of auditory objects while listening to competing speakers</article-title>. <source hwp:id="source-22">Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>29</issue>):<fpage>11854</fpage>–<lpage>11859</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Ding N."><surname>Ding</surname>, <given-names>N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> (<year>2012b</year>). <article-title hwp:id="article-title-24">Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title>. <source hwp:id="source-23">Journal of neurophysiology</source>, <volume>107</volume>(<issue>1</issue>):<fpage>78</fpage>–<lpage>89</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Eggermont J. J."><surname>Eggermont</surname>, <given-names>J. J.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-25">Acquired hearing loss and brain plasticity</article-title>. <source hwp:id="source-24">Hearing Research</source>, <volume>343</volume>:<fpage>176</fpage>–<lpage>190</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Etard O."><surname>Etard</surname>, <given-names>O.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Reichenbach T."><surname>Reichenbach</surname>, <given-names>T.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-26">Neural speech tracking in the theta and in the delta frequency band differentially encode clarity and comprehension of speech in noise</article-title>. <source hwp:id="source-25">Journal of Neuroscience</source>, <volume>39</volume>(<issue>29</issue>):<fpage>5750</fpage>–<lpage>5759</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Wieringen A."><surname>van Wieringen</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wouters J."><surname>Wouters</surname>, <given-names>J.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-27">Apex 3: a multi-purpose test platform for auditory psychophysical experiments</article-title>. <source hwp:id="source-26">Journal of neuroscience methods</source>, <volume>172</volume>(<issue>2</issue>):<fpage>283</fpage>–<lpage>293</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2 xref-ref-28-3 xref-ref-28-4 xref-ref-28-5"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Fuglsang S. A."><surname>Fuglsang</surname>, <given-names>S. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Märcher-Rørsted J."><surname>Märcher-Rørsted</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dau T."><surname>Dau</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hjortkjær J."><surname>Hjortkjær</surname>, <given-names>J.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-28">Effects of sensorineural hearing loss on cortical synchronization to competing speech during selective attention</article-title>. <source hwp:id="source-27">Journal of Neuroscience</source>, <volume>40</volume>(<issue>12</issue>):<fpage>2562</fpage>–<lpage>2572</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Hamilton L. S."><surname>Hamilton</surname>, <given-names>L. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Edwards E."><surname>Edwards</surname>, <given-names>E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Chang E. F."><surname>Chang</surname>, <given-names>E. F.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-29">A spatial map of onset and sustained responses to speech in the human superior temporal gyrus</article-title>. <source hwp:id="source-28">Current Biology</source>, <volume>28</volume>(<issue>12</issue>):<fpage>1860</fpage>–<lpage>1871</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Harkrider A. W."><surname>Harkrider</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Plyler P. N."><surname>Plyler</surname>, <given-names>P. N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hedrick M. S."><surname>Hedrick</surname>, <given-names>M. S.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-30">Effects of hearing loss and spectral shaping on identification and neural response patterns of stop-consonant stimuli</article-title>. <source hwp:id="source-29">The Journal of the Acoustical Society of America</source>, <volume>120</volume>(<issue>2</issue>):<fpage>915</fpage>–<lpage>925</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Harkrider A. W."><surname>Harkrider</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Plyler P. N."><surname>Plyler</surname>, <given-names>P. N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hedrick M. S."><surname>Hedrick</surname>, <given-names>M. S.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-31">Effects of hearing loss and spectral shaping on identification and neural response patterns of stop-consonant stimuli in young adults</article-title>. <source hwp:id="source-30">Ear and hearing</source>, <volume>30</volume>(<issue>1</issue>):<fpage>31</fpage>–<lpage>42</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="website" citation-type="web" ref:id="2021.01.21.427550v3.32" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Heeris J."><surname>Heeris</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-32">Gammatone filterbank toolkit 1.0</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/detly/gammatone" ext-link-type="uri" xlink:href="https://github.com/detly/gammatone" hwp:id="ext-link-2">https://github.com/detly/gammatone</ext-link>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Horton C."><surname>Horton</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Srinivasan R."><surname>Srinivasan</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="D’Zmura M."><surname>D’Zmura</surname>, <given-names>M.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-33">Envelope responses in single-trial eeg indicate attended speaker in a ‘cocktail party’</article-title>. <source hwp:id="source-31">Journal of neural engineering</source>, <volume>11</volume>(<issue>4</issue>):<fpage>046015</fpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Iotzov I."><surname>Iotzov</surname>, <given-names>I.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Parra L. C."><surname>Parra</surname>, <given-names>L. C.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-34">EEG can predict speech intelligibility</article-title>. <source hwp:id="source-32">Journal of Neural Engineering</source>, <volume>16</volume>(<issue>3</issue>):<fpage>036008</fpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Koerner T. K."><surname>Koerner</surname>, <given-names>T. K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhang Y."><surname>Zhang</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-35">Differential effects of hearing impairment and age on electrophysiological and behavioral measures of speech in noise</article-title>. <source hwp:id="source-33">Hearing research</source>, <volume>370</volume>:<fpage>130</fpage>–<lpage>142</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2"><citation publication-type="other" citation-type="journal" ref:id="2021.01.21.427550v3.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Kraus F."><surname>Kraus</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tune S."><surname>Tune</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ruhe A."><surname>Ruhe</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J."><surname>Obleser</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Woestmann M."><surname>Woestmann</surname>, <given-names>M.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-36">Unilateral acoustic degradation delays attentional separation of competing speech</article-title>. <source hwp:id="source-34">bioRxiv</source>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="other" citation-type="journal" ref:id="2021.01.21.427550v3.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Lenth R."><surname>Lenth</surname>, <given-names>R.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-37">emmeans: Estimated Marginal Means, aka Least-Squares Means</article-title>. <source hwp:id="source-35">R package version 1.4.8</source>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Lesenfants D."><surname>Lesenfants</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vanthornhout J."><surname>Vanthornhout</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Verschueren E."><surname>Verschueren</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Decruy L."><surname>Decruy</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-38">Predicting individual speech intelligibility from the cortical tracking of acoustic-and phonetic-level speech representations</article-title>. <source hwp:id="source-36">Hearing research</source>, <volume>380</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="other" citation-type="journal" ref:id="2021.01.21.427550v3.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Luts H."><surname>Luts</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jansen S."><surname>Jansen</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dreschler W."><surname>Dreschler</surname>, <given-names>W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wouters J."><surname>Wouters</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-39">Development and normative data for the Flemish/Dutch matrix test</article-title>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Maamor N."><surname>Maamor</surname>, <given-names>N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Billings C. J."><surname>Billings</surname>, <given-names>C. J.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-40">Cortical signal-in-noise coding varies by noise type, signal-to-noise ratio, age, and hearing status</article-title>. <source hwp:id="source-37">Neuroscience letters</source>, <volume>636</volume>:<fpage>258</fpage>–<lpage>264</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Margolis R. H."><surname>Margolis</surname>, <given-names>R. H.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Saly G. L."><surname>Saly</surname>, <given-names>G. L.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-41">Asymmetric hearing loss: definition, validation, and prevalence</article-title>. <source hwp:id="source-38">Otology &amp; Neurotology</source>, <volume>29</volume>(<issue>4</issue>):<fpage>422</fpage>–<lpage>431</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Maris E."><surname>Maris</surname>, <given-names>E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Oostenveld R."><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-42">Nonparametric statistical testing of eeg-and meg-data</article-title>. <source hwp:id="source-39">Journal of neuroscience methods</source>, <volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>–<lpage>190</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Mateos-Aparicio P."><surname>Mateos-Aparicio</surname>, <given-names>P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rodríguez-Moreno A."><surname>Rodríguez-Moreno</surname>, <given-names>A.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-43">The impact of studying brain plasticity</article-title>. <source hwp:id="source-40">Frontiers in cellular neuroscience</source>, <volume>13</volume>:<fpage>66</fpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2 xref-ref-44-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="McClannahan K. S."><surname>McClannahan</surname>, <given-names>K. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Backer K. C."><surname>Backer</surname>, <given-names>K. C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Tremblay K. L."><surname>Tremblay</surname>, <given-names>K. L.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-44">Auditory evoked responses in older adults with normal hearing, untreated, and treated age-related hearing loss</article-title>. <source hwp:id="source-41">Ear and hearing</source>, <volume>40</volume>(<issue>5</issue>):<fpage>1106</fpage>–<lpage>1116</lpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3 xref-ref-45-4 xref-ref-45-5"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Mirkovic B."><surname>Mirkovic</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Debener S."><surname>Debener</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schmidt J."><surname>Schmidt</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jaeger M."><surname>Jaeger</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Neher T."><surname>Neher</surname>, <given-names>T.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-45">Effects of directional sound processing and listener’s motivation on eeg responses to continuous noisy speech: Do normal-hearing and aided hearing-impaired listeners differ?</article-title> <source hwp:id="source-42">Hearing Research</source>, <volume>377</volume>:<fpage>260</fpage>–<lpage>270</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="book" citation-type="book" ref:id="2021.01.21.427550v3.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Nasreddine Z."><surname>Nasreddine</surname>, <given-names>Z.</given-names></string-name> (<year>2004</year>). <chapter-title>Montreal cognitive assessment (MoCA)</chapter-title>. <source hwp:id="source-43">École des sciences de la réadaptation, Sciences de la santé</source>, <publisher-name>Université d’Ottawa</publisher-name>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Oates P. A."><surname>Oates</surname>, <given-names>P. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kurtzberg D."><surname>Kurtzberg</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Stapells D. R."><surname>Stapells</surname>, <given-names>D. R.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-46">Effects of sensorineural hearing loss on cortical event-related potential and behavioral measures of speech-sound processing</article-title>. <source hwp:id="source-44">Ear and hearing</source>, <volume>23</volume>(<issue>5</issue>):<fpage>399</fpage>–<lpage>415</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Oostenveld R."><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Praamstra P."><surname>Praamstra</surname>, <given-names>P.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-47">The five percent electrode system for high-resolution eeg and erp measurements</article-title>. <source hwp:id="source-45">Clinical neurophysiology</source>, <volume>112</volume>(<issue>4</issue>):<fpage>713</fpage>–<lpage>719</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="O’Sullivan J. A."><surname>O’Sullivan</surname>, <given-names>J. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Power A. J."><surname>Power</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mesgarani N."><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rajaram S."><surname>Rajaram</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foxe J. J."><surname>Foxe</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shinn-Cunningham B. G."><surname>Shinn-Cunningham</surname>, <given-names>B. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Slaney M."><surname>Slaney</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shamma S. A."><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Lalor E. C."><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-48">Attentional selection in a cocktail party environment can be decoded from single-trial eeg</article-title>. <source hwp:id="source-46">Cerebral cortex</source>, <volume>25</volume>(<issue>7</issue>):<fpage>1697</fpage>–<lpage>1706</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Peelle J. E."><surname>Peelle</surname>, <given-names>J. E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wingfield A."><surname>Wingfield</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-49">The neural consequences of age-related hearing loss</article-title>. <source hwp:id="source-47">Trends in neurosciences</source>, <volume>39</volume>(<issue>7</issue>):<fpage>486</fpage>–<lpage>497</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2 xref-ref-51-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Petersen E. B."><surname>Petersen</surname>, <given-names>E. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wöstmann M."><surname>Wöstmann</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J."><surname>Obleser</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Lunner T."><surname>Lunner</surname>, <given-names>T.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-50">Neural tracking of attended versus ignored speech is differentially affected by hearing loss</article-title>. <source hwp:id="source-48">Journal of neurophysiology</source>, <volume>117</volume>(<issue>1</issue>):<fpage>18</fpage>–<lpage>27</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2 xref-ref-52-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Presacco A."><surname>Presacco</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Anderson S."><surname>Anderson</surname>, <given-names>S.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-51">Evidence of degraded representation of speech in noise, in the aging midbrain and cortex</article-title>. <source hwp:id="source-49">Journal of neurophysiology</source>, <volume>116</volume>(<issue>5</issue>):<fpage>2346</fpage>–<lpage>2355</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1 xref-ref-53-2 xref-ref-53-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Presacco A."><surname>Presacco</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Anderson S."><surname>Anderson</surname>, <given-names>S.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-52">Speech-in-noise representation in the aging midbrain and cortex: Effects of hearing loss</article-title>. <source hwp:id="source-50">PloS one</source>, <volume>14</volume>(<issue>3</issue>):<fpage>e0213899</fpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="book" citation-type="book" ref:id="2021.01.21.427550v3.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><collab hwp:id="collab-1">R Core Team</collab> (<year>2020</year>). <source hwp:id="source-51">R: A Language and Environment for Statistical Computing</source>. <publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.55" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Slaney M."><surname>Slaney</surname>, <given-names>M.</given-names></string-name> (<year>1998</year>). <article-title hwp:id="article-title-53">Auditory toolbox</article-title>. <source hwp:id="source-52">Interval Research Corporation, Tech. Rep</source>, <volume>10</volume>(<issue>1998</issue>).</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Somers B."><surname>Somers</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bertrand A."><surname>Bertrand</surname>, <given-names>A.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-54">A generic eeg artifact removal algorithm based on the multi-channel wiener filter</article-title>. <source hwp:id="source-53">Journal of neural engineering</source>, <volume>15</volume>(<issue>3</issue>):<fpage>036007</fpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Tremblay K. L."><surname>Tremblay</surname>, <given-names>K. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Piskosz M."><surname>Piskosz</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Souza P."><surname>Souza</surname>, <given-names>P.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-55">Effects of age and age-related hearing loss on the neural representation of speech cues</article-title>. <source hwp:id="source-54">Clinical Neurophysiology</source>, <volume>114</volume>(<issue>7</issue>):<fpage>1332</fpage>–<lpage>1343</lpage>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1 xref-ref-58-2"><citation publication-type="book" citation-type="book" ref:id="2021.01.21.427550v3.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Van Dun B."><surname>Van Dun</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kania A."><surname>Kania</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dillon H."><surname>Dillon</surname>, <given-names>H.</given-names></string-name> (<year>2016</year>). <chapter-title>Cortical auditory evoked potentials in (un) aided normal-hearing and hearing-impaired adults</chapter-title>. In <source hwp:id="source-55">Seminars in hearing</source>, volume <volume>37</volume>, page <fpage>9</fpage>. <publisher-name>Thieme Medical Publishers</publisher-name>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1 xref-ref-59-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Vanthornhout J."><surname>Vanthornhout</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Decruy L."><surname>Decruy</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wouters J."><surname>Wouters</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon J. Z."><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-56">Speech intelligibility predicted from neural entrainment of the speech envelope</article-title>. <source hwp:id="source-56">Journal of the Association for Research in Otolaryngology</source>, <volume>19</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>191</lpage>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1 xref-ref-60-2 xref-ref-60-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.01.21.427550v3.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Verschueren E."><surname>Verschueren</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vanthornhout J."><surname>Vanthornhout</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Francart T."><surname>Francart</surname>, <given-names>T.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-57">The effect of stimulus intensity on neural envelope tracking</article-title>. <source hwp:id="source-57">Hearing Research</source>, <volume>403</volume>:<fpage>108175</fpage>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><citation publication-type="other" citation-type="journal" ref:id="2021.01.21.427550v3.61" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Voeten C. C."><surname>Voeten</surname>, <given-names>C. C.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-58">buildmer: Stepwise Elimination and Term Reordering for Mixed-Effects Regression</article-title>. <source hwp:id="source-58">R package version 1.6</source>.</citation></ref></ref-list><sec hwp:id="sec-32"><title hwp:id="title-39">Supplementary Material</title><p hwp:id="p-111"><bold>Participant Details: subjective rating for each SNR condition</bold></p><fig id="figS1" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S.1:</label><caption hwp:id="caption-13"><p hwp:id="p-112">Subjective rating for each SNR condition for hearing-impaired listeners (green) and normal hearing listeners (orange).</p></caption><graphic xlink:href="427550v3_figS1" position="float" orientation="portrait" hwp:id="graphic-13"/></fig></sec><sec hwp:id="sec-33"><title hwp:id="title-40">Spatial Filters</title><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Figure S.2:</label><caption hwp:id="caption-14"><p hwp:id="p-113">Spatial filters for acoustic onsets and spectrogram TRF, evaluated for both NH listeners and HI listeners.</p></caption><graphic xlink:href="427550v3_figS2" position="float" orientation="portrait" hwp:id="graphic-14"/></fig></sec><sec hwp:id="sec-34"><title hwp:id="title-41">Supplementary Statistical Material</title><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T6" hwp:rev-id="xref-table-wrap-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Table S.1:</label><caption hwp:id="caption-15"><p hwp:id="p-114">Results of the linear mixed model in order to assess the effects of degree of hearing loss (PTA) and age on the peak latency of <italic toggle="yes">P</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>, <italic toggle="yes">N</italic>1<sub><italic toggle="yes">S</italic></sub>, <italic toggle="yes">P</italic>2<sub><italic toggle="yes">S</italic></sub>. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), degrees of freedom (df), t-Ratios and p-values are reported per fixed effect term. Participant nested in the matching factor was included as a random nested effect.</p><p hwp:id="p-115">Formula: latency ~ 1 + peak + PTA +age + peak:age + peak:PTA + peak:PTA:age + (1 | match/participant)</p></caption><graphic xlink:href="427550v3_tblS1" position="float" orientation="portrait" hwp:id="graphic-15"/></table-wrap><table-wrap id="tblS2" orientation="portrait" position="float" hwp:id="T7" hwp:rev-id="xref-table-wrap-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.01.21.427550v3/TBLS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tblS2</object-id><label>Table S.2:</label><caption hwp:id="caption-16"><p hwp:id="p-116">Results of the linear model in order to assess the effect of degree of hearing loss (z-scored) on the peak latency of <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub> when only HI listeners are takening into account. Estimates of the regression coefficients (<italic toggle="yes">β</italic>), standard errors (SE), t-Ratios and p-values are reported per fixed effect term.</p><p hwp:id="p-117">Formula: <italic toggle="yes">N</italic>1<sub><italic toggle="yes">AO</italic></sub>-latency ~ 1 + PTA</p><p hwp:id="p-118"><italic toggle="yes"><inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="427550v3_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>, F = 8.178 on 1 and 9 df, p = 0.01879</italic></p></caption><graphic xlink:href="427550v3_tblS2" position="float" orientation="portrait" hwp:id="graphic-16"/></table-wrap></sec></back></article>
