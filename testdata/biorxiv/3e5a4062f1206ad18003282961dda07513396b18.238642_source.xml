<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/238642</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;238642v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;238642</article-id><article-id pub-id-type="other" hwp:sub-type="slug">238642</article-id><article-id pub-id-type="other" hwp:sub-type="tag">238642</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">A cortical active-suppression mechanism facilitates the distraction-invariant selective processing of concurrent speech</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1">Author correspondence: Lorenz Fiedler &amp; Jonas Obleser Department of Psychology, University of Lubeck Maria-Goeppert StraBe 9a 23562 Lubeck, Germany <email hwp:id="email-1">lorenz.fiedler@uni-luebeck.de</email>; <email hwp:id="email-2">jonas.obleser@uni-luebeck.de</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Fiedler Lorenz"><surname>Fiedler</surname><given-names>Lorenz</given-names></name></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Wöstmann Malte"><surname>Wöstmann</surname><given-names>Malte</given-names></name></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Herbst Sophie K."><surname>Herbst</surname><given-names>Sophie K.</given-names></name></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Obleser Jonas"><surname>Obleser</surname><given-names>Jonas</given-names></name></contrib><aff id="a1" hwp:id="aff-1"><label>1</label><institution hwp:id="institution-1">Department of Psychology, University of Lubeck, Lubeck</institution>, <country>Germany</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-12-22T08:24:13-08:00">
    <day>22</day><month>12</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-03-12T11:03:14-07:00">
    <day>12</day><month>3</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-12-22T08:31:53-08:00">
    <day>22</day><month>12</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-03-12T11:15:50-07:00">
    <day>12</day><month>3</month><year>2018</year>
  </pub-date><elocation-id>238642</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-12-22"><day>22</day><month>12</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2018-03-12"><day>12</day><month>3</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-03-12"><day>12</day><month>3</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license hwp:id="license-1"><p hwp:id="p-1">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="238642.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/238642v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="238642.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/238642v2/238642v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/238642v2/238642v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Listening in noisy environments requires selective neural processing of the incoming sound mixture. This selectivity has been attributed to auditory cortex regions, where neural activity closely matches the attended acoustic signal. Here, we show how such selective neural processing unfolds over time in components of the neural auditory response, and how the active suppression of ignored-talker signals is brought about by sources in a fronto-parietal cortical network. We recorded and source-localized the electroencephalogram of 18 subjects who listened to one of two simultaneously presented and spatially non-separable audiobooks, while the signal-to-noise ratio (SNR) between the two talkers continuously varied between −6 and +6 dB. While the earliest (i.e., PS0-like) modelled response emerging from temporal cortex depended almost exclusively on acoustics (i.e., the momentary attended-to-ignored-talker level), the ensuing N100- and P200-like model responses reflected an increasingly distraction-invariant, attention-biased selectivity. Importantly, a late parietal response component and enhanced phase coherence in dorsal anterior cingulate cortex (dACC) suggest active suppression of the ignored talker signal. Thus, listening not only involves amplification of attended signals in auditory cortex, but it hinges additionally on active suppression of ignored signals through the fronto-parietal attention network.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">auditory cortex</kwd><kwd hwp:id="kwd-2">attention</kwd><kwd hwp:id="kwd-3">EEG</kwd><kwd hwp:id="kwd-4">parietal cortex</kwd><kwd hwp:id="kwd-5">SNR</kwd></kwd-group><counts><page-count count="36"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">Introduction</title><p hwp:id="p-3">Human listeners understand speech even in the presence of distracting sound sources (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Cherry, 1953</xref>). An emerging question is, how competing acoustic events capture bottom-up attention due to their saliency (e.g., by being louder than the background), and how top-down attention shapes neural responses in order to overcome these adverse listening conditions (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Kaya and Elhilali, 2017</xref>).</p><p hwp:id="p-4">In recent years, the use of encoding and decoding models (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Paninski et al., 2007</xref>) to investigate the neural responses to continuous speech has opened new paths to study the neural implementation of auditory attention (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Lalor et al., 2009</xref>). It is by now well-established that the auditory cortical system selectively responds to the temporal envelope of attended vs. ignored speech (Magnetoencephalography: <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Ding and Simon, 2012a</xref>; Electroencephalography: <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Power et al., 2012</xref>, <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Fiedler et al., 2017</xref>). Accordingly, auditory cortical responses allow for a reconstruction of the spectrogram of speech and to detect the attended speaker (e.g., Zion and Chang, 2012; <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Zion Golumbic et al., 2013</xref>). The neural response to broad-band continuous speech can be obtained from EEG by estimating the (delayed) covariance of the temporal speech envelope and the EEG, which results in a linear model of the cortical response; a temporal response function (TRF; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Lalor et al., 2009</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Crosse et al., 2016</xref>). Analogous to the event-related potential (ERP), the components of the TRF can be interpreted as reflecting a sequence of processing stages where later components reflect higher order processes within the hierarchy of the auditory system (Davis and Johnsrude, 2003; <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Picton et al., 2013</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Di Liberto et al., 2015</xref>).</p><p hwp:id="p-5">Also, adverse listening conditions are known to attenuate the neural tracking of attended speech. Manipulations have included temporal fine structure (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Ding et al., 2014</xref>), rhythmicity (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Kayser et al., 2015</xref>), reverberation (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Fuglsang et al., 2017</xref>) or signal-to-noise ratio (SNR; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Kong et al., 2014</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">Ding and Simon, 2012b</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Giordano et al., 2017</xref>). Not least, neural selection of speech appears weakened in people with hearing loss (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Petersen et al., 2016</xref>). Nevertheless, the selective neural processing of speech compensates for adverse conditions to some degree by filtering the acoustic signal to obtain a more robust neural representation of the attended talker.</p><p hwp:id="p-6">As of yet, the selective auditory processing of concurrent speech has been mainly attributed to auditory cortex regions. However, the additional involvement of supra-modal, fronto-parietal attention networks is plausible (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Woolgar et al., 2016</xref>; <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Pamper and Chait 2017</xref>). It is still under debate, however, whether the increased neural response to attended auditory input is achieved only by neural gain (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Willmore et al., 2014</xref>) or by active suppression of the ignored acoustic input as well. A neural-gain mechanism would result only in a relative attenuation of the neural response to the ignored talker, while active suppression would yield an additional, counteracting neural response to ignored as opposed to attended speech (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Horton et al., 2013</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Kong et al., 2015</xref>; <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Wostmann et al., 2017</xref>).</p><p hwp:id="p-7">Here, we use a listening scenario in which two concurrent talkers underwent continuous SNR variation. Our results demonstrate differential effects of bottom-up acoustics vs. top-down attentional set on earlier vs. later model response components, respectively. These findings reveal that not only auditory cortex regions are involved in the selective neural processing of concurrent speech, but that a fronto-parietal attention network contributes to selective neural processing through active suppression of the ignored talker.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Methods</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Participants</title><p hwp:id="p-8">Eighteen native speakers of German (9 females) were invited from the participant database of the Department of Psychology, University of LUbeck, Germany. We recruited participants who were between 23 and 68 years old (mean: 49, SD: 17), to allow valid conclusions from such a challenging listening scenario to middle-aged and older adults. All reported normal hearing and no histories of neurological disorders. Incomplete data due to recording hardware failure was obtained in four more, initially invited participants. All participants gave informed consent and received payment of 8 €/hour. The study was approved by the local ethics committee of the University of LUbeck.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Experimental Design</title><p hwp:id="p-9">The goal of this study was to investigate the selective neural processing of one of two talkers under a continuously varying signal-to-noise ratio (SNR). Here, the signal is a to-be-attended talker and the noise is a to-be-ignored talker. Our study was conducted in a within subject 2 by 3 design (attention by SNR (three levels)).</p><p hwp:id="p-10">The identical mixture of the attended and ignored talker was presented on both ears, resulting in a concurrent listening scenario without any spatial cue (i.e. diotic, <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1A</xref>), such that the only cues available for talker segregation consisted in the spectra-temporal features of the talkers, such as pitch, formants, and amplitude modulation. The SNR was stochastically varied between three levels of −6, 0 and +6 dB (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1B</xref>; see <italic toggle="yes">Stimuli</italic> below). The particular dB range was chosen to create a challenging but at the same time solvable listening task. Even if an SNR of −6 dB is rare in real-life listening scenarios (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Smeds et al., 2015</xref>), the neural tracking of attended speech has been reported as intact at SNRs as low as−6 dB (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Ding and Simon, 2013</xref>). However, speech perception (number of words repeated correctly) of normal hearing subjects starts to suffer around an SNR &lt; 0 dB and the speech­ reception threshold (i.e. 50% correct) usually lies between -5 and O dB (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Pichora-Fuller et al., 1995</xref>, <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Bentler et al., 2004</xref>).</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-7">Stimuli</title><p hwp:id="p-11">We selected two audiobooks read by native German speakers, one female (Elke Heidenreich, ‘Nero Corleone kehrt zuruck’, read by Elke Heidenreich) and one male (Yuval Noah Harari, ‘Eine kurze Geschichte der Menschheit’, read by Jurgen Holdorf). The following steps of stimulus preparation were done using custom code written in MATLAB (<italic toggle="yes">Mathwoks Inc.</italic>). Sequences of silence longer than 500 ms were truncated to 500 ms in order to avoid long periods of silence (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">O’Sullivan et al., 2014</xref>). The first hour of each audiobook was selected for further preparation. The first 30 minutes of each audiobook served as the to-be-attended and the rest served as the to-be-ignored speech, such that all subjects could attend both stories from the beginning and attended (and ignored) both the male and the female voice the same amount ohime.</p><p hwp:id="p-12">The SNR was modulated symmetrically around 0 dB. An SNR ofO dB refers to concurrent talker signals with a matched long-term root-mean-square (rms) amplitude as used previously in numerous studies (e.g. <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">Power et al., 2012</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">O’Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Mirkovic et al., 2015</xref>). Coming from an SNR of 0dB, the SNR was either increased to +6 dB by raising the sound pressure level (SPL) of the to-be­ attended talker by 6 dB or decreased to −6 dB by raising the SPL of the to-be-ignored talker by 6 dB (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1B</xref>). As building blocks for SNR modulation, we created a sample of plateaus (i.e., constant SNR of−6, 0or +6 dB) and ramps (i.e., transition between plateaus). The length of plateaus was uniformly distributed between 5 and 9 seconds in discrete steps of one second. The ramps were linear interpolations between SNRs with the length uniformly distributed between 1 and 5 seconds in discrete steps of one second. The length distributions of plateaus and ramps were kept uniform within each talker and within their assignments as being attended or ignored. We concatenated plateaus via ramps such that a 0 dB plateau was either followed by a +6 dB or a −6 dB, whereas a +6 dB and a −6 dB plateau were always followed by a 0 dB plateau via a respective ramp. The randomly varying SNR time courses for every subject individually in order to avoid systematic overlap between the SNR modulation and the audiobooks. Stimulus material was cut into twelve blocks, which resulted in an average block length offive minutes. Sound files were created with a sampling rate of 44.1 kHz and a 16-bit resolution. The experiment was implemented in the software <italic toggle="yes">Presentation (Neurobehavioural Systems</italic>). Stimuli were presented via headphones (Sennheiser HD25).</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;238642v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><title hwp:id="title-8">Experimental design, forward model, and classification accuracy.</title><p hwp:id="p-13"><bold>A)</bold> Two mixed talkers (female &amp; male) were presented on both ears without spatial segregation (diotic). <bold>B)</bold> The signal-to-noise ratio (SNR) between attended (signal) and ignored (noise) talker was varied between −6, 0 and +6 dB. Length of ramps and plateaus were drawn from uniform distributions. Trials were extracted by cutting the data in the middle of the ramps (i.e. at −3 dB or +3 dB). <bold>C</bold>) Temporal response functions (TRF) to the attended and ignored talker were extracted by a forward (encoding) regression model based on the assumption that the measured EEG signal is the superposition (convolution) of the envelope onsets (of the attended and ignored talkers) and the TRFs, respectively. TRFs reflect the neural response evoked by a single envelope onset. <bold>D)</bold> Accuracy in classification of the attended talker averaged across subjects, obtained by prediction of EEG signals by a forward model (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Fiedler et al., 2017</xref>) at single EEG channels and single voxels in source space, respectively. Highlighted channels of topographic maps indicate that classification accuracy on the group level was significantly above chance level (chance9s% = 60%). Source maps show voxels exceeding an average classification accuracy of 65%.</p></caption><graphic xlink:href="238642_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-9">Task</title><p hwp:id="p-14">The twelve blocks were presented such that subjects were instructed to attend to the female or to the male talker in an alternating fashion. After instruction before each block (i.e. attend to female or attend to male), subjects were asked to start the stimulus presentation by a button press, which enabled the subjects to take a break between blocks. During listening, subjects were asked to fixate a cross presented on the screen to reduce eye movement.</p><p hwp:id="p-15">Every other block, the story picked up at the point it ended two blocks before. After each block, subjects were asked to rate the difficulty of maintaining attention on a continuous color bar ranging from red (difficult) to green (easy) by a mouse click. For later analysis, the continuous color bar was discretized into ten segments (1 = hard, 10 = easy). Subsequently, participants were asked to answer four multiple-choice questions concerning the content of the to-be-attended audiobook. The average rating of difficulty was neither significantly correlated with the number of questions correctly answered (Pearson’s r = 0.1, p = 0.7), nor with participants’ age (Pearson’s r = −0.1, p = 0.5). Furthermore, we found no significant correlation of the number of correctly answered questions with age (Pearson’s r = −0.1, p = 0.65).</p></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-10">Data acquisition and preprocessing</title><p hwp:id="p-16">EEG was recorded with 64 electrodes <italic toggle="yes">Acticap</italic> (<italic toggle="yes">Easy Cap</italic>) connected to an <italic toggle="yes">ActiChamp</italic> (<italic toggle="yes">Brain Products</italic>) amplifier. EEG signals were recorded with the software <italic toggle="yes">Brain Recorder</italic> (<italic toggle="yes">Brain Products</italic>) at a sampling rate of 1 kHz. Impedances were kept below 10 kO. Electrode TP9 (left mastoid) served as reference during recording.</p><p hwp:id="p-17">The EEG data were pre-processed in <italic toggle="yes">MATLAB (201 la)</italic> using both the <italic toggle="yes">Fieldtrip-toolbox</italic> (version: 20170321; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Oostenveld et al., 2011</xref>) and custom written code. The EEG data were re-referenced to the average of the electrodes TP9 and TP10 (left and right mastoids) and resampled to <italic toggle="yes">f</italic><sub><italic toggle="yes">s</italic></sub> = 125 Hz. The continuous EEG data were highpass-filtered at <italic toggle="yes">f</italic><sub><italic toggle="yes">c</italic></sub>= 1Hz and lowpass-filtered at <italic toggle="yes">f</italic><sub><italic toggle="yes">c</italic></sub>= 30 Hz (two-pass Hamming window FIR, filter order: 3f<sub>s</sub>/f<sub>c</sub>).</p><p hwp:id="p-18">From the continuous EEG data, we extracted the parts during which the twelve blocks of audiobooks were presented (see above). For every subject, we applied independent component analysis (ICA; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Makeig et al., 2004</xref>) on the concatenated data of the twelve blocks in order to reject components that were clearly related to eye movements, eye blinks, muscle artifacts as well as heartbeat. On average, 26 of 62 components (SD: 7.3) were rejected.</p><p hwp:id="p-19">For further analysis, we lowpass-filtered the data again at f<sub>c</sub>= 10 Hz (two-pass Hamming window FIR, filter order: <italic toggle="yes">3f<sub>s</sub>/f<sub>c</sub></italic>), which assured that the amplitudes at all frequencies up to 8 Hz were not reduced. Previously, neural activity phase-locked to the envelope was only found up to a frequency of approximately 8 Hz (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-2" hwp:rel-id="ref-61">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">Ding et al., 2014</xref>). We could confirm this finding by incrementally raising the cutoff frequency, which didn’t change the morphology of the TRFs (see below) but only decreased the prediction accuracy due to the interference of non-phase-locked neural activity and external noise.</p></sec><sec id="s2f" hwp:id="sec-8"><title hwp:id="title-11">Extraction of envelope onsets</title><p hwp:id="p-20">A temporal representation of the syllable onsets, further called envelope onsets, was extracted from the presented speech signals (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-3" hwp:rel-id="ref-15">Fiedler et al., 2017</xref>). Those representations later served as regressors to model neural responses to the talkers (see below). First, we extracted an auditory spectrogram containing 128 spectrally resolved sub-band envelopes of the speech signals logarithmically spaced between approximately 90 and 4000 Hz using the <italic toggle="yes">NSL</italic> toolbox (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Chi et al., 2005</xref>). Second, the auditory spectrogram was summed up across frequencies, which resulted in broad-band temporal envelopes of the audiobooks. Taking the derivative of the envelope and zeroing all values smaller than zero (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Hertrich et al., 2012</xref>) returned the envelope onsets, which only contain positive values at time periods of an increasing envelope, as can be found at syllable onsets (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1C</xref>). Using the envelope onsets as regressor does not imply that we only modeled the encoding of syllable onsets. Every syllable onset is followed by a peak in the speech envelope (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1B</xref>), which is then again followed by an offset and the next onset and so forth, resulting in a high autocorrelation between those features. Nevertheless, onsets are the earliest feature that could possibly evoke a neural response (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">Picton, 2013</xref>). The latency of modelled responses to envelope onsets (compared to envelopes) was found to be most similar to conventional ERPs (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-4" hwp:rel-id="ref-15">Fiedler et al., 2017</xref>, supplemental material).</p></sec><sec id="s2g" hwp:id="sec-9"><title hwp:id="title-12">Estimation of temporal response functions</title><p hwp:id="p-21">We applied an established method to estimate a linear forward (encoding) model (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-3" hwp:rel-id="ref-33">Lalor et al., 2009</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Crosse et al., 2016</xref>). The model contains temporal response functions (TRF), which are estimations of the neural response to a certain continuously varying stimulus feature. In our case, this stimulus feature is the envelope onsets (see above) of both, the attended and the ignored talker. Based on the assumption that every sample in the EEG signal <italic toggle="yes">r(t)</italic> is the superposition of neural responses to past onsets and thus can be expressed for one talker by a convolution operation:
<disp-formula id="eqn1" hwp:id="disp-formula-1"><alternatives hwp:id="alternatives-1"><graphic xlink:href="238642_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives><label>[]</label></disp-formula>
where <italic toggle="yes">s(t)</italic> is the envelope onsets, TRF is the temporal response function that describes the relationship betweens and rover a range of time lags <italic toggle="yes">τ</italic> (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1C</xref>). The TRF contains a weight for each time lag r. We investigated time lags in the range from -100 to 500 ms. In order to obtain the weights of the TRF to both talkers contained in the matrix <italic toggle="yes">G</italic><sub><italic toggle="yes">TRF</italic></sub>, ridge regression (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Hoerl and Kennard, 1970</xref>) was applied, which can be expressed in the linear algebraic form:
<disp-formula id="eqn2" hwp:id="disp-formula-2"><alternatives hwp:id="alternatives-2"><graphic xlink:href="238642_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives><label>[]</label></disp-formula>
where <italic toggle="yes">S</italic> is matrix containing the onset envelopes of both the attended and ignored talker and its sample-wise time lagged replications, <italic toggle="yes">R</italic> contains the measured EEG signal, λ is the ridge parameter for regularization, the scalar m is the mean of the trace of STS (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Biesmans et al., 2016</xref>) and / is the identity matrix. The optimal ridge parameter λ was estimated according to Fiedler et al. (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-5" hwp:rel-id="ref-15">2017</xref>) and was set to λ = 10<sup>2</sup>.</p><p hwp:id="p-22">TRFs were estimated on a trial-by-trial basis, where trial refers to a part (e.g. a plateau of +6 dB) of certain length cut from the continuous stimulus and the respective EEG data. For the subsequent analysis, we subdivided the data in two ways: First, to get a general estimate of the model’s ability to dissociate between attended and ignored talkers, we cut the data into one-minute trials, resulting in trial lengths comparable to previous studies (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">O’Sullivan et al., 2014</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">Mirkovic et al., 2015</xref>; <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Biesmans et al., 2016</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-6" hwp:rel-id="ref-15">Fiedler et al., 2017</xref>). This resulted in 60 trials per subject. Second, we cut the data based on the applied SNR modulation, which resulted in three groups of trials: −6 dB, 0 dB and +6 dB. To use the entire recording, the data were cut at the time points where ramps of the SNR time courses either crossed −3 dB or+ 3 dB (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1B</xref>). This resulted in 180 trials of O dB and 90 trials of −6 and +6 dB, respectively. The average length of those trials was 10 seconds (i.e. average length of a plateau (7 seconds) and average length of two halves of a ramp (2×1.5 seconds)). In order to balance the number of trials across SNRs, 90 trials from O dB were randomly drawn from the 180 trials of every subject.</p></sec><sec id="s2h" hwp:id="sec-10"><title hwp:id="title-13">Forward model classification accuracy</title><p hwp:id="p-23">Besides the statistical analysis of the TRFs (see below), we evaluated the TRFs regarding their ability to detect the attended talker expressed in classification accuracy. In order to obtain classification accuracy, we followed the forward method of predicting two EEG signals and comparing those to the measured EEG signal, as described in detail by in Fiedler et al. (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-7" hwp:rel-id="ref-15">2017</xref>). We used the data cut into trials of one-minute length, independent of the applied SNR modulation (see above). This resulted in 60 epochs per subject, which we trained TRFs on. In a leave-one-out fashion, we predicted to be expected EEG signals of a single trial contained in <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="238642_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> following the equation:
<disp-formula id="eqn3" hwp:id="disp-formula-3"><alternatives hwp:id="alternatives-4"><graphic xlink:href="238642_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives></disp-formula>
where <italic toggle="yes">S</italic> is the matrix containing the onset envelopes and G<sub>TRF</sub> is the matrix containing the TRFs. Two different EEG signals were predicted per trial, the first representing the one and the second the other talker being attended. To obtain a classification decision of which talker was attended, we compared the Pearson correlations from both predicted EEG signals with the measured EEG signal and chose the one that produced the stronger correlation (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-8" hwp:rel-id="ref-15">Fiedler et al., 2017</xref>). Per subject, 60 decisions were made.</p><p hwp:id="p-24">Classification accuracy was defined as the percentage of trials in which the to-be-attended talker was detected correctly as such by the model prediction. Since this is a forward model approach, classification accuracy is obtained at every single EEG channel (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">Crosse et al., 2016</xref>). Likewise, classification accuracy was obtained at the source level at every single voxel.</p></sec><sec id="s2i" hwp:id="sec-11"><title hwp:id="title-14">Statistical analysis on temporal response functions</title><p hwp:id="p-25">To extract significant spatio-temporal deflections in the TRFs at an SNR of O dB, we applied a two­ level statistical analysis (two-level cluster-test; e.g. <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Obleser et al., 2012</xref>). At the single-subject level, we used independent sample t-tests to test the TRF to the attended, the ignored as well as the attended-ignored difference against zero. Resulting t-values were transformed to z-scores. Since the weights obtained from Eq. 2 are arbitrary (i.e., depend on A), we decided to show these normalized (i.e. z-scored) TRFs. These z-scores have the advantage of expressing the deviation from zero relative to the standard deviation ofTRFs across trials (as a measure of how consistent TRFs are across the 90 trials of each subject under a certain SNR). At the group level, the deflection of z-scores from zero was tested by a cluster-based permutation one-sample t-test (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Maris and Oostenveld, 2007</xref>), which clusters t-values of adjacent time lags in time-electrode space (with a minimum of 4 neighboring EEG channels). The extracted cluster is compared to 4,000 clusters drawn randomly from the data by permuting condition labels. The resulting cluster p-value reflects the relative number of Monte Carlo iterations in which the summed t-statistic of the observed cluster is exceeded.</p><p hwp:id="p-26">In a second step, the identical cluster-based permutation test was applied to obtain significant differences between the extreme SNRs−6 vs.+6 dB in the TRF based on the attended and the ignored signal, as well as on the attended-ignored difference.</p><p hwp:id="p-27">For illustration of the neural responses, we averaged single-subject z-scores obtained from the first level test across channels of interest. Channels of interest were defined as the channels being part of both significant clusters found in the attended-ignored difference between TRFs under a balanced SNR of 0 dB (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2C</xref>). The 95%-confidence-bands were obtained by bootstrapping (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Efron, 1979</xref>) across the averaged responses of all subjects, using 4,000 iterations.</p></sec><sec id="s2j" hwp:id="sec-12"><title hwp:id="title-15">Extraction of individual amplitudes and instantaneous phase</title><p hwp:id="p-28">In order to disentangle amplitude- and latency-effects, we treated the TRF as a band-limited signal and extracted the amplitude and instantaneous phase of the prominent components (P1<sub>TRF</sub>, N1<sub>TRF</sub> and P2<sub>TRF</sub>) from the single-trial TRFs in every subject, only averaged across channels of interest.</p><p hwp:id="p-29">Amplitude was defined as the maximum or minimum within a certain time interval (P1<sub>TRF</sub>: 0-100 ms; N1<sub>TRF</sub>: 100-200 ms; P2<sub>TRF</sub>: 200-300 ms) of the subject-and SNR-specific TRF. This individual extraction of amplitudes compensated for the observed latency shifts of components.</p><p hwp:id="p-30">The instantaneous phase was extracted from the TRF averaged across channels of interest. We extracted the instant phase angle under every SNR by keeping the latency fixed at the individual peaks of the TRF to the attended talker under +6 dB, which is the condition with weakest masking by the ignored talker and thus the best proxy of the TRF peaks. Here, the instantaneous phase is an appropriate measure, since the time-locked response to continuous speech is band-limited below 8 Hz (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-3" hwp:rel-id="ref-61">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">Ding et al., 2014</xref>) and the TRF was found to pass through three successive and comparably low frequent components. Thus, we avoided to split up the EEG signal into frequency bands of arbitrary edges, but rather looked at the response phase as a sequential process going through several stages (i.e. components). The instantaneous phase cp(T) was extracted from the complex analytic signal TRF.(T) of the z-scored TRFs of the attended and ignored talker:
<disp-formula id="eqn4" hwp:id="disp-formula-4"><alternatives hwp:id="alternatives-5"><graphic xlink:href="238642_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives></disp-formula>
where <italic toggle="yes">TRF</italic> is the Hilberttransform of the TRF.</p></sec><sec id="s2k" hwp:id="sec-13"><title hwp:id="title-16">TRF phase coherence</title><p hwp:id="p-31">As in ERPs, a reduced amplitude in the average TRF can either originate from reduced amplitude or reduced phase coherence between individual trials or subjects. In order to assure that the observed effects do not result from differences in phase coherence between trials alone, we calculated the phase coherence for every single subject and SNR. The phase coherence was calculated by obtaining the TRFs’ analytic signal of every single trial, setting the magnitude of the complex phasor to one, adding up all single trial complex phasors within each SNR and dividing by the number of trials (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Lachaux et al., 1999</xref>).</p><p hwp:id="p-32">Note that here trials refer to epochs of constant condition (i.e. plateaus, see above). A single trial TRF can be interpreted as the average response to envelope onsets within this epoch. Thus, the TRF phase coherence is not measuring the consistency between neural responses to each single syllable onset within an epoch, but between the single-trial TRFs across epochs. To avoid confusion, we did not use the term inter-trial phase coherence, even if the underlying calculations are similar (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">Makeig et al., 2004</xref>). Analogous to the TRFs, we tested the difference between SNRs against zero in a cluster­ based permutation one-sample t-test (see above).</p></sec><sec id="s2l" hwp:id="sec-14"><title hwp:id="title-17">Source localization</title><p hwp:id="p-33">To further trace the origin of effects observed in sensor space, we applied LCMV-beamforming (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Drongelen et al., 1994</xref>; <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Van Veen et al., 1997</xref>) to obtain source-activity time courses in single voxels of the brain. Using a standard template brain from Fieldtrip/SPM (Montreal Neurological Institute) together with the <italic toggle="yes">Acticap</italic> electrode layout, leadfields were calculated with a grid resolution of 10 mm. Individual LCMV-filter weights were obtained using 5% regularization. The continuous time­ domain EEG data were projected to source space, resulting in three source activity time courses (X­ Y-Z) per voxel. In order to obtain a single time course in each voxel, the direction of highest variance was determined by principal component analysis and used for further analysis. All further processing steps in source space were done analogously to sensor space EEG data.</p></sec><sec id="s2m" hwp:id="sec-15"><title hwp:id="title-18">Statistical analysis</title><p hwp:id="p-34">Statistical analysis was performed according to respective data type and its underlying distribution. We performed cluster-based permutation tests correcting for multiple comparisons on the sensor level (see above). Confidence intervals (95%) were calculated by bootstrapping the mean across the z-scored TRFs of the single subjects (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">Efron, 1979</xref>). The amplitude peak differences were tested using a two-sided t-test. Arithmetic and circular statistical analysis on phase angles were performed using the toolbox <italic toggle="yes">circstat2012a</italic> (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Berens, 2009</xref>), including the Hotelling paired-samples test (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Zar, 1999</xref>). Since we observed that pre-requisites for a Hotelling paired-sample test weren’t always met, we performed a non-parametric permutation test by shuffling the labels (20,000 permutations) of the to-be-tested data and obtaining a Hotelling paired-sample test statistic for every permutation. Reported p-values (p<sub>perm</sub>) refer to the relative number of permutations that the test returned a higher F statistic than the empirical data. Confidence slices (i.e. circular confidence intervals; 95%) were calculated by bootstrapping the circular mean across phase angles of the single subjects (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-3" hwp:rel-id="ref-13">Efron, 1979</xref>).</p></sec><sec id="s2n" hwp:id="sec-16"><title hwp:id="title-19">Results</title><p hwp:id="p-35">We asked participants to listen to one of two simultaneously presented audiobooks under varying signal-to-noise ratio. After each of twelve five-minute blocks, subjects were asked to rate the difficulty of listening to the to-be-attended talker on a color bar ranging from red (difficult= 1) to green (easy= 10). The average difficulty ratings strongly varied between subjects (mean: 5.2, SD: 2.2, range: 2.3-8.9). No difference in difficulty ratings for listening to the female versus male talker was found (paired t-test, t<sub>17</sub>= 1.17, p = 0.26). With respect to successful attending, participants were asked to answer four multiple-choice questions on the content of the to-be-attended audiobook after each five-minute block. The percentage of correctly answered questions was far above chance (25%) for all participants (mean: 81%, SD: 9%, range: 60-96%). All participants were thus able to follow the to­ be-attended talker.</p></sec><sec id="s2o" hwp:id="sec-17"><title hwp:id="title-20">Classification accuracy</title><p hwp:id="p-36">To obtain a general estimate of which EEG channels and which voxels show signatures of selective neural processing, we detected the attended talker by forward prediction of EEG signals (see methods). We obtained highest classification accuracy of approximately 80% at fronto-central channels, slightly lateralized towards temporal channels (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig. 1D</xref>). The source localization revealed that high classification accuracy was mainly driven by temporal channels, where we found classification accuracy within single voxels of up to 78% (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1D</xref>).</p></sec><sec id="s2p" hwp:id="sec-18"><title hwp:id="title-21">Attention-modulated neural responses to concurrent speech</title><p hwp:id="p-37">Next, we assessed in greater detail the unfolding of attentional selection of to-be-attended speech in time. To this end, we assessed the most prominent response components and their modulation by attention independent of our SNR manipulation (i.e., we estimated the TRFs from the balanced SNR trials of 0 dB). We inspected both the TRFs to the attended and ignored talker individually (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2A</xref>&amp;B), as well as the difference between the TRFs to the attended and ignored talker (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3C</xref>) to examine signatures of selective neural processing.</p><p hwp:id="p-38">Three prominent components (P1<sub>TRF</sub>, N1<sub>TRF</sub>, P2<sub>TRF</sub>; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2A</xref>) were identifiable with notable consistency across individual subjects. The latter two components were absent in the TRF to the ignored talker and thus indicated selective neural processing. All three components (P1<sub>TRF</sub>, N1<sub>TRF</sub>, P2<sub>TRF</sub>) mainly localized to superior and inferior temporal regions (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2A</xref>). Note that the source localizations of the two latter components (N1<sub>TRF</sub>, P2<sub>TRF</sub>) compared well to the sources of enhanced classification accuracy between attended and un-attended talkers (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Fig. 1D</xref>).</p><p hwp:id="p-39">First, an early positive component (termed P1<sub>TRF</sub>) appeared in the TRFs to the attended (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig. 2A</xref>, 24-88 ms, p = 2×10<sup>−4</sup>) and ignored (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig. 2B</xref>, 24-112 ms, p = 2×10<sup>−4</sup>) talkers, but without any attention­ related difference (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2</xref>(). Latency, polarity, and topography of this component compared well to a P1 as found in auditory evoked potentials (AEPs).</p><p hwp:id="p-40">Second, a later negative deflection (termed N1<sub>TRF</sub>) was only present in the TRF to the attended talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Fig. 2A</xref>; 112-176 ms, p = 5×10<sup>−4</sup>). This component was significantly increased in magnitude (i.e., more negative) for the attended versus the ignored talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2C</xref>, 80-176 ms, p = 5×10<sup>−4</sup>). Noteworthy, the significant attentional modulation of this component (attended-ignored) started already at a time lag of 80 ms, when both the TRF to the attended and to the ignored talkers were still in positive deflection.</p><p hwp:id="p-41">Third, a positive deflection between 200 and 300 ms (termed P2<sub>TRF</sub>; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2A</xref>, 216-304 ms, p = 5×10<sup>−4</sup>), was again only present in the TRF to the attended talker. This component mainly drove the significant difference between the responses to the attended and ignored talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-11" hwp:rel-id="F2">Fig. 2C</xref>, p = 2×1o-4). Interestingly, in the same time interval, a comparably long negative deflection was found in the TRF to the ignored talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-12" hwp:rel-id="F2">Fig. 2B</xref>, 248-424 ms, p = 2×10<sup>−4</sup>). While at earlier stages, TRFs to the attended and the ignored talker had the same polarity (P1<sub>TRF</sub>), at the stage of the P2<sub>TRF</sub> we see an anti-polar relationship. Effectively, this also enhanced the late, attended-ignored difference in the P2<sub>TRF</sub> time range (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-13" hwp:rel-id="F2">Fig. 2C</xref>).</p><p hwp:id="p-42">Lastly, a late negative deflection of the response to the attended talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-14" hwp:rel-id="F2">Fig. 2A</xref>, 360-384 ms, p = 0.03) was found, but no equivalent cluster occurred in the difference between the TRFs to the attended and ignored talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-15" hwp:rel-id="F2">Fig. 2C</xref>). Hence, this cluster was excluded from further inspections.</p></sec><sec id="s2q" hwp:id="sec-19"><title hwp:id="title-22">Sustained phase-locking of TRFs for attended speech</title><p hwp:id="p-43">To further investigate how attention modulates the TRF components, we inspected TRF phase coherence (1-8 Hz) across individual trials. TRF phase coherence to both the attended and the ignored talker peaked at around 100 ms (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-16" hwp:rel-id="F2">Fig. 2D</xref>), before decaying back to baseline at around 300 ms. This decay of phase coherence, however, was more pronounced in the TRF of the ignored talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-17" hwp:rel-id="F2">Fig. 2D</xref>, p = 0.01 at left fronto-central EEG channels). On the source level, this attention-related difference in TRF phase coherence was strongest in the left anterior temporal lobe, where we also found maximal classification accuracy (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Fig. 1D</xref>).</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12 xref-fig-2-13 xref-fig-2-14 xref-fig-2-15 xref-fig-2-16 xref-fig-2-17 xref-fig-2-18 xref-fig-2-19 xref-fig-2-20 xref-fig-2-21 xref-fig-2-22"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;238642v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><title hwp:id="title-23">Temporalresponse functions (TRF) to continuous speech of concurrent talkers under balanced SNR (O dB).</title><p hwp:id="p-44">Z-5cores of TRFs depict average across subjects. Z-scores were obtained by testing the distribution ofsingle-trial TRF weightings against zero at single EEG channels. TRFs shown are averaged across channels of interest. Confidence bands (95%) were obtained by bootstrapping. Black horizontal lines indicate time ranges ofsignificant difference from zero obtained from a cluster­ based permutation test at the group level. Topographic maps show z-scores of clusters averaged across the cluster time range. Highlighted channels are part of the significant clusters. Source localizations show the 35% most strongly contributing voxels. <bold>A)</bold> Responses to the attended talker clearly show a cascade of three components (Pl rnF-N1 TRF-P2<sub>TRF</sub>), <bold>B)</bold> Responses to the ignored talker only show a PhRF, whereas the NhRF and P2<sub>TRF</sub> are suppressed. <bold>C)</bold> Significant differences between neural responses to the attended and ignored talker are present in the Nl <sub>TRF</sub>-and P2rRF-timerange. <bold>D)</bold> TRF phase coherence (1-8 Hz) shows sustained phase coherence in the TRFs to the attended vs. ignored talker.</p></caption><graphic xlink:href="238642_fig2" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec><sec id="s2r" hwp:id="sec-20"><title hwp:id="title-24">The most detrimental signal-to-noise ratio reveals a signature of active suppression</title><p hwp:id="p-45">Next, we analyzed the impact of a varying SNR on the TRFs identified in response to SNR = 0 dB. To this end, we contrasted the TRFs of the two extreme conditions, −6 vs. +6 dB (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3</xref>).</p><p hwp:id="p-46">The TRFs to the attended talker (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3A</xref>) showed significant SNR differences during two time intervals, first at around 100 ms (72-136 ms, p = 2×10<sup>−4</sup>) and second around 200 ms (176-232 ms, p = 2×10<sup>−4</sup> These differences occurred in the transition between components (P<sub>TRF</sub> to N<sub>TRF</sub>, and N<sub>TRF</sub> to P2<sub>TRF</sub>). This was consistent with the visual impression of the TRFs being similar in morphology, yet delayed under an SNR of−6 dB compared to +6dB.</p><p hwp:id="p-47">The TRFs to the ignored talker (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3B</xref>) also showed such an SNR-related delay, captured by a negative cluster (96-104 ms, p = 0.04). Importantly, two later additional components appeared under an SNR of−6 dB compared to +6 dB selectively for ignored speech: the first (160-176 ms, p = 0.02) localized to temporal regions, and the second localized to parietal regions (240-280 ms, p = 0.004). This parietal localization clearly differentiated this detrimental-SNR, ignored-speech component from all others.</p><p hwp:id="p-48">Exploratory inspection of TRF phase coherence (1-8 Hz; <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3D</xref>) across SNRs gave further evidence for a superimposed neural mechanism being involved in the suppression of the ignored talker. The</p><p hwp:id="p-49">TRF to the ignored talker exhibited significantly enhanced phase coherence under an SNR of−6 dB (vs. +6 dB), again in the time range of the late P2<sub>TRF</sub> and again at parietal EEG channels (232-248, p = 0.005). No such change was observable in the TRF to the attended talker (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Fig. 3D</xref> inset). In source space, the enhanced phase coherence localized to the dorsal anterior cingulate (dACC), spreading into parietal regions. This is further, albeit exploratory evidence for an additional neural mechanism originating from non-auditory, supra-modal regions in the suppression of the ignored talker.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9 xref-fig-3-10 xref-fig-3-11 xref-fig-3-12 xref-fig-3-13 xref-fig-3-14"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;238642v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><title hwp:id="title-25">Temporal response functions (TRF) to continuous speech for signal-to-noise ratios of −6 vs. +6 dB.</title><p hwp:id="p-50">Z-scores of TRFs depict average across subjects. Z-scores were obtained by testing distribution of single-trial TRF weightings against zero at single EEG channels. TRFs shown are averaged across channels of interest. Confidence bands (95%) were obtained by bootstrapping. Black horizontal lines indicate time ranges of significant difference between −6 vs. +6 dB obtained from a cluster-based permutation test at the group level. Topographic maps show z-scores of clusters averaged across the cluster time range. Highlighted channels are part of the significant clusters. Source localizations show the 35% most strongly contributing voxels. <bold>A)</bold> Responses to the attended talker are delayed under an SNR of−6 dB compared to +6 dB. <bold>B)</bold> Under an SNR of−6 dB, a late component appeared, which was localized in parietal and frontal regions. <bold>C)</bold> The components of the difference between the responses to the attended and ignored talker are differently affected by a varying signal-to-noise ratio. Note that TRF under O dB is only shown in C for better overview. <bold>D)</bold> TRF phase coherence (1-8 Hz) of TRFs to the attended (inset) and ignored talker under the different SNRs.</p></caption><graphic xlink:href="238642_fig3" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s2s" hwp:id="sec-21"><title hwp:id="title-26">Unfolding of a distraction-invariant representation of the attended talker: TRF magnitude</title><p hwp:id="p-51">A central question was how the SNR (i.e., bottom-up acoustic conditions) affects the difference in attending versus ignoring speech (i.e., the top-down attentional set), which is shown in detail in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3C</xref>. Crucially, in order to account for observed latency shifts in the TRFs, we here also inspected the amplitude differences (attended-ignored) at individual participants’ peaks of the TRF components (shown in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4A &amp;B</xref>).</p><p hwp:id="p-52">During the early time interval of the P1<sub>TRF</sub>, the TRF difference (attended-ignored) indicated that a higher relative sound intensity evoked a more positive P1<sub>TRF</sub> amplitude, independent of being attended or ignored (16-72 ms, p = 2×10<sup>−4</sup>). The P1<sub>TRF</sub> peak amplitude difference (attended vs. ignored, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4B</xref>) showed no significant difference from zero under an SNR of 0 dB (one-sample t-test, t<sub>17</sub> = −0.01, p = 0.99), whereas under an SNR of −6 dB, all but two subjects showed a negative) difference (t<sub>17</sub> = −6.2, p = 1xl0<sup>-5</sup> and all subjects showed a positive difference under an SNR of +6 dB (t<sub>17</sub> = 5.8, p = 2xl0<sup>-5</sup> This linear trend was reflected in the differences between SNRs (−6 vs. 0 dB: t<sub>17</sub> = 6.0, p = 1.5xl0<sup>-5</sup>, 0 vs. +6 dB: t<sub>17</sub> = 5.8, p = 2xl0<sup>-5</sup>, - 6 dB vs. +6 dB: t<sub>17</sub>= 7.3, p = 1.3xl0<sup>−6</sup>). The contrast of the three SNRs centered around zero, which indicates that the P1<sub>TRF</sub> amplitude is purely driven by the varying SNR, with the absence of any attention-related influence.</p><p hwp:id="p-53">The N1<sub>TRF</sub> was more negative to the attended vs. ignored talker under all SNRs. Critically, the TRF difference (attended-ignored) of this attentional modulation of the TRF further increased (i.e., larger negativity in the neural response; <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Fig. 3C</xref>, 104-144 ms, p = 5xl0<sup>-4</sup>) under a more favorable SNR (−6 vs. +6 dB). The N1<sub>TRF</sub> peak amplitude difference (attended vs. ignored, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4B</xref>) turned out to be generally affected by attention, since across all SNRs, a more negative TRF (negative offset) could be detected in the response to the attended talker, resulting in significant differences from zero under all SNRs (one-sample t-test, −6 dB: t<sub>17</sub>= −3.6, p = 0.002; 0 dB: t<sub>17</sub> = -8.1, p = 3xl0<sup>-7</sup>; +6 dB: t<sub>17</sub>= 8.8, p = .10<sup>−9</sup>). Interestingly, the negativity of the N1<sub>TRF</sub> increased with a more favorable SNR (negative slope across SNRs in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4B</xref>), which was reflected in a significant difference between SNRs (paired sample t-test, −6 dB vs. 0 dB: t<sub>17</sub> = −2.5, p = 0.02; 0 dB vs. +6 dB: t<sub>17</sub>= −4.4, p = 4×10<sup>−4</sup>,−6 dB vs. +6 dB, t<sub>17</sub>= - 4.7, p = 2×10<sup>−4</sup>). Thus, the N1<sub>TRF</sub> peak amplitude was always more negative in the TRF to the attended talker, which indicates a consistent signature of selective neural processing. However, the selectivity of the N<sub>TRF</sub> amplitude was prone to degradation by more adverse SNRs.</p><p hwp:id="p-54">Lastly, the magnitude of the TRF difference (attended-ignored) in the P2<sub>TRF</sub> interval was remarkably constant across SNRs (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Fig. 3C</xref>). However, the delay (for −6 vs. 6 dB) and the additional component in the response to the ignored talker at an adverse SNR of−6 dB (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-10" hwp:rel-id="F3">Fig. 3B</xref>) might indicate an additional mechanism being involved during that comparably late interval of the responses to the concurrent talkers. Finally, the P2<sub>TRF</sub> peak amplitude difference (attended-ignored, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig. 4B</xref>) showed an increased response to the attended talker under all SNRs (one-sample t-tests, −6 dB: t<sub>17</sub> = 4.8, p = 2×10<sup>−4</sup>; 0 dB: t<sub>17</sub> = 7.6, p = 8×10<sup>−1</sup>; +6 dB: t<sub>17</sub>= 8.8, p = 10<sup>−1</sup>). In contrast to the N1<sub>TRF</sub> amplitudes, the P2<sub>TRF</sub> amplitudes were not modulated by SNR (paired sample t-test, −6 dB vs. 0 dB: t<sub>17</sub> = 0.3, p = 0.77; 0 dB vs. +6 dB: t<sub>17</sub> = 0.8, p = 0.4, −6 dB vs. +6 dB, t<sub>17</sub>= 0.95, p = 0.35). This indicates the P2<sub>TRF</sub> amplitude to be robust against a varying SNR and solely driven by attention.</p><p hwp:id="p-55">In sum, whether a talker was attended or ignored did not affect early TRF components (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig. 4B</xref>; P<sub>TRF</sub>) but only the later components (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Fig. 4B</xref>; N<sub>TRF</sub> &amp; P2<sub>TRF</sub>), In contrast, the impact of SNR was large for early (P<sub>TRF</sub> &amp; N<sub>TRF</sub>) but absent for late neural response components (P2<sub>TRF</sub>), Thus, the peak amplitudes of the TRFs indicate that the neural representation of concurrent speech within the first</p><p hwp:id="p-56">∽400 ms becomes gradually more biased towards the attended talker and becomes gradually more SNR-invariant.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7 xref-fig-4-8 xref-fig-4-9 xref-fig-4-10 xref-fig-4-11 xref-fig-4-12 xref-fig-4-13"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;238642v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><title hwp:id="title-27">Effects of attentional set and SNR on response components.</title><p hwp:id="p-57"><bold>A)</bold> The attended­ ignored amplitude difference of the TRFs for the three SNR levels (adopted from <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-11" hwp:rel-id="F3">Fig. 3C</xref>) shows three major components (P1<sub>TRF</sub>, NhRF &amp; P2<sub>TRF</sub>). <bold>B)</bold> Individual peak amplitude difference. Within each panel, the slope across the three bars indicates the effect of increasing SNR, whereas an offset indicates the influence of attending (versus ignoring). <bold>C)</bold> Individual differences in phase angles (attended-ignored) for TRF components PhRF, NhRF &amp; P2rRF under different SNRs (color coded). Dots show phase angle differences for individual participants, colored lines and shades indicate the circular mean phase angle difference and the 95%-confidence­ slices obtained by bootstrap across subjects. <bold>D)</bold> The schematic illustration depicts the successively decreasing impact of SNR (orange) and the increasing impact of attention (green) on the TRF components (P1<sub>TRF</sub>, N1<sub>TRF</sub> &amp; P2<sub>TRF</sub>). The labels <italic toggle="yes">amplitude</italic> and <italic toggle="yes">phase</italic> describe the measures affected by attention and SNR, respectively.</p></caption><graphic xlink:href="238642_fig4" position="float" orientation="portrait" hwp:id="graphic-8"/></fig></sec><sec id="s2t" hwp:id="sec-22"><title hwp:id="title-28">Unfolding of a distraction-invariant representation of the attended talker: TRF phase</title><p hwp:id="p-58">In the section above we controlled for latency shifts ofTRF components in order to investigate effects of SNR and attention on TRF amplitude. Here, to also investigate latency-specific effects of attention and SNR on neural response components, we determined the individual instantaneous phase of the TRFs (see <italic toggle="yes">Methods).</italic> Specifically, we investigated the SNR-dependent phase difference (attended­ ignored) by extracting phase angles at the time lags of the three prominent TRF components for every single subject. <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-8" hwp:rel-id="F4">Fig. 4C</xref> shows the attended-ignored phase difference of the three prominent components (P<sub>TRF</sub>, N1<sub>TRF</sub>, P2<sub>TRF</sub>) under the three different SNRs (−6, 0, +6 dB). Analogous to the analysis of the components’ amplitude, we can assume that a phase difference under an SNR of 0 dB is purely attention-related, whereas the change of the phase difference across SNR levels is due to the varying acoustics.</p><p hwp:id="p-59">Interestingly, in the P1<sub>TRF</sub> we found a phase difference (attended-ignored) under an SNR of 0 dB (Hotelling paired-sample test, mean: 0.34 rad, F<sub>2,16</sub>= 7.3, p<sub>perm</sub> = 0.002). Since the sound intensity was balanced, this delay indicates that the response to the attended talker is leading already at the early stage of the P1<sub>TRF</sub>. This phase difference was also modulated by SNR. Under the favorable SNR of +6 dB, this early phase difference (attended-ignored) further increased (Hotelling paired-sample test, mean: 0.72 rad, F<sub>2,16</sub> = 59.2, p<sub>perm</sub> = 4xl0<sup>−6</sup>), whereas under the adverse SNR of −6 dB, it diminishes (Hotelling paired-sample test, mean: −0.01 rad, F<sub>2,16</sub> = 0.016, p<sub>perm</sub> = 0.99). Contrasting the phase difference under an SNR of−6 dB against +6 dB confirmed a significant increase of phase difference due to a more favorable SNR (Hotelling paired sample test, mean: −0.72 rad, F<sub>2.16</sub> = 12.1, p<sub>perm</sub> = 5.5×l0<sup>−4</sup>). The early attended-ignored phase difference in P1<sub>TRF</sub> indicates an early attentional selection.</p><p hwp:id="p-60">In the N<sub>TRF</sub>, an even stronger phase difference (attended-ignored) was found. Under the balanced SNR of 0 dB, we found a significant phase difference (attended-ignored) under the balanced SNR of 0 dB (Hotelling paired-sample test, mean: 1.1 rad, F<sub>2,16</sub> = 554.3, p<sub>perm</sub> = 3.8×10<sup>−6</sup>, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-9" hwp:rel-id="F4">Fig. 4C</xref>, center). Comparable to the P1<sub>TRF</sub>, the phase difference was modulated by SNR. Under the favorable SNR of +6 dB, a further increase of the phase difference (attended-ignored) was present (Hotelling paired­ sample test, mean: 1.2 rad, F<sub>2,16</sub> = 27.2, p<sub>perm</sub> = 0.029). Under the adverse SNR of −6 dB, the phase difference (attended-ignored) was not significant (Hotelling paired-sample test, mean: 0.39 rad, F<sub>2,16</sub> = 66.8, p<sub>perm</sub> = 0.44), but the confidence slice indicated an evolving phase difference even under the adverse SNR of−6 dB. Contrasting the phase difference under an SNR of−6 dB against +6 dB revealed a significant increase of phase difference due to a more favorable SNR (Hotelling paired-sample test, mean:-0.94 rad, F<sub>2,16</sub>= 4.3, p = 0.03). Note that even though we found a phase difference in the N1<sub>TRF</sub>, this phase difference was not exceeding n/2 (i.e. 90°). Thus, we cannot speak of a anti-phasic relationship at this stage.</p><p hwp:id="p-61">Strikingly, the later P2<sub>TRF</sub> showed an attended-ignored phase difference under all SNRs (Hotelling paired-sample test, −6 dB: mean: −2.8 rad, F<sub>2,16</sub>= 62.5, p<sub>perm</sub> = 4×1o−6 0dB: mean: −2.7 rad, F<sub>2,16</sub>= 37.6, p<sub>perm</sub> = 4×10<sup>−6</sup> −6 dB: mean: −2.5 rad, F<sub>2,16</sub> = 42.7, p<sub>perm</sub> = 5×10<sup>−5</sup>, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-10" hwp:rel-id="F4">Fig. 4C</xref>, right). In contrast to the preceding components, the phase difference under an SNR of −6 dB against +6 dB revealed no significant increase of phase difference due to a more favorable SNR (Hotelling paired-sample test, mean:-0.13 rad, F2,15= 0.64, p<sub>perm</sub> = 0.55). Comparable to the amplitude differences, the almost anti­ phasic relationship between TRFs to the attended and ignored talker (which reflects in phase angles of the attended-ignored difference close to ±n) present under all SNRs yet again indicates an SNR­ invariant selective neural processing of the concurrent talkers in the P2<sub>TRF</sub>.</p></sec></sec><sec id="s3" hwp:id="sec-23"><title hwp:id="title-29">Discussion</title><p hwp:id="p-62">In the present study, human listeners attended to one of two concurrent talkers under continuously varying signal-to-noise ratio (SNR). Forward modeling revealed neural responses to the temporal envelopes of individual talkers and their modulation by both, top-down attentional set, and bottom­ up SNR. The model response yielded a clear succession of P1-N1-P2-like components, localized in auditory temporal regions, with attention-classification accuracies up to 80%. While a distinction between different SNR levels occurred for earlier components (Pl and Nl), separation between attended and ignored talkers unfolded later in time (Nl and P2), establishing an SNR-invariant representation of attended speech. Critically, only under the most adverse SNR, the TRF showed a distinct late parietal response component and increased coherence, which localized to dorsal anterior cingulate (dACC). This component likely reflects active suppression of irrelevant, to-be­ ignored acoustic input in a fronto-parietal attention network.</p><sec id="s3a" hwp:id="sec-24"><title hwp:id="title-30">Neural responses reflect unfolding of a distraction-invariant representation of attended speech</title><p hwp:id="p-63">In accordance with previous studies on neural tracking of auditory stimuli in EEG (e.g. <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-3" hwp:rel-id="ref-49">Power et al., 2012</xref>), three prominent components from the modeled TRFs to the attended talker were selected for further investigation (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-18" hwp:rel-id="F2">Fig. 2A</xref>; P1<sub>TRF</sub>, N1<sub>TRF</sub> and P2<sub>TRF</sub>). Akin to the more classically studied auditory evoked potential (AEP), we interpret the TRF as a sequence of components reflecting consecutive stages of (selective) neural processing along the auditory pathway (for review see: <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-3" hwp:rel-id="ref-46">Picton, 2013</xref>).</p><p hwp:id="p-64">The P1<sub>TRF</sub> peak amplitude was strongly dominated by the saliency of the talkers (i.e. the SNR variation). This agrees with the supposed role of the Pl, described as reflecting mostly bottom-up processing (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Herrmann et al., 2013</xref>). At this relatively early stage of auditory processing, the relevant spectra-temporal features of the acoustic input might be extracted. Attentional modulations at this component have been described in ERPs (Giuliano et al. 2014; Woldorff and Hillyard) but not in the response to continuous speech (cf. <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-3" hwp:rel-id="ref-10">Ding and Simon, 2012b</xref>). However, mechanisms of auditory selective attention were found already at the brainstem level (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Slee and David, 2015</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Forte et al., 2017</xref>). In the present data, the only signature of selective processing during the P1<sub>TRF</sub>was the slight forward-shift in phase for the P1<sub>TRF</sub> to the attended compared to the ignored talker (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-11" hwp:rel-id="F4">Fig 4C</xref>), which most likely results from the more negative N<sub>TRF</sub> to the attended talker. This early phase shift is suggesting that as soon as relevant features of the attended talker are identified at the stage of the P<sub>TRF</sub>, the N<sub>TRF</sub> is evoked (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-19" hwp:rel-id="F2">Fig 2A</xref>), whereas no N<sub>TRF</sub> is elicited by irrelevant features leading to a longer sustain (and phase shift) of the P1<sub>TRF</sub> (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-20" hwp:rel-id="F2">Fig 2B</xref>), which, in line with Chait et al. (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">2010</xref>), also leads to a relatively earlyTRF difference (attended-ignored) emerging at 80 ms (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-21" hwp:rel-id="F2">Fig 2C</xref>).</p><p hwp:id="p-65">The N1<sub>TRF</sub> was strongly selective towards the attended talker, both in terms of magnitude and phase differences between the modeled response to the attended and the ignored talker. In AEPs, comparable effects have been observed (e.g. Hillyard et al., 1973; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Nataanen et al., 1981</xref>). The Nl can be regarded as the pivotal stage of attentional selection, decisive upon the ‘perceptual fate’ of concurrent speech signals. This negative-going deflection of the modeled response function in the 100-150 ms time window is the most robustly replicated TRF component (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-4" hwp:rel-id="ref-10">Ding &amp; Simon, 2012a</xref>, <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-5" hwp:rel-id="ref-10">Ding &amp; Simon, 2012b</xref>). Here, however, we demonstrate that this attentional selectivity of the N1<sub>TRF</sub> is not SNR-invariant, but benefits from better SNR.</p><p hwp:id="p-66">The modulation of magnitude, phase and likely generators of the ensuing P2<sub>TRF</sub> component demonstrate how such a robust neural representation of attended speech might be brought about: The P2<sub>TRF</sub>was found to be strongly selective towards the attended talker. In contrast to the N1<sub>TRF</sub>, the strength of this selectivity was not affected by adverse SNRs (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-12" hwp:rel-id="F4">Fig. 4B, C</xref>). A robust representation of the attended signal at the P2<sub>TRF</sub> is in line with previous findings: Fuglsang et al. (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">2017</xref>) exposed participants to a cocktail-party scenario of varying reverberation and found the P2<sub>TRF</sub> most robust. Puvvada et al. (<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">2017</xref>) showed that from early to late time lags of the neural response an increasing dissociation between attended and unattended talkers can be observed. Di Liberto et al. (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">2015</xref>) also suggested the P2<sub>TRF</sub> to reflect an enhanced, post-categorical stage of speech processing along the auditory pathway. Taken together, those findings suggest that the P2<sub>TRF</sub> reflects a neural stage at which the representation of the attended signal has been largely isolated from distracting sources.</p><p hwp:id="p-67">We argue here that the unfolding anti-phasic TRF for attended vs. ignored speech not only indicates amplification of relevant but also active suppression of irrelevant acoustic input (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-13" hwp:rel-id="F4">Fig. 4C</xref>). In the P2<sub>TRF</sub>, the attentional selection not only attenuates the response to the ignored talker (which would result in an amplitude but not a phase difference), but rather responds in an anti-polar fashion. Phase­ related sensitivity and attention effects have been found both in the visual and in the auditory domain (<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Spaak et al., 2014</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Henry and Obleser, 2012</xref>). Here we show that an attention-related,anti-phasic relationship reflects active suppression of the ignored talker (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-22" hwp:rel-id="F2">Fig. 2A,B</xref>), which is in line with previous findings about the neural tracking of speech (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Horton et al., 2013</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">Kong et al., 2015</xref>). However, here we showed that this anti-phasic relationship is not omnipresent but evolves later during the unfolding of the neural response.</p><p hwp:id="p-68">In general, it is likely that further decreases in SNR beyond −6 dB will prevent the neural extraction (∽P1<sub>TRF</sub>) and amplification (∽N1<sub>TRF</sub>) of the relevant features of the attended talker due to extensive masking by the ignored talker. Ding and Simon (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">2013</xref>) estimated such a breakdown of the neural tracking of attended speech in noise to occur between −6 and -9 dB in MEG. Critically, the present data show that this selectivity might stay intact down to an SNR of −6 dB due to the support by additional supra-modal mechanisms, as will be discussed in the next section.</p></sec><sec id="s3b" hwp:id="sec-25"><title hwp:id="title-31">Late distractor suppression by a non-auditory, supra-modal attention network</title><p hwp:id="p-69">Under the adverse SNR of−6 dB compared to +6 dB, our analysis revealed an enhanced response to the ignored talker in the P2<sub>TRF</sub>time range consisting of a positive and a negative component (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-12" hwp:rel-id="F3">Fig. 3B</xref>). The latter is in anti-phase to the P2<sub>TRF</sub>in the TRF to the attended talker. Together with thelate increase in phase coherence (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-13" hwp:rel-id="F3">Fig. 3D</xref>), we interpret this additional component as a signature of active suppression of the ignored talker emerging from non-auditory, supra-modal regions, which are part of the fronto-parietal attention or global-demand network (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">Woolgar et al. 2016</xref>).</p><p hwp:id="p-70">Under the assumption that such active suppression is costly to the cognitive system, it has been suggested that it is only deployed if necessary (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Chait et al., 2010</xref>). Neural signatures for active suppression of irrelevant signals during late (∽200 ms) AEPs have been examined before (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Melara et al., 2002</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-3" hwp:rel-id="ref-4">Chait et al., 2010</xref>). Pamper and Chait (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-2" hwp:rel-id="ref-48">2017</xref>) related enhanced centre-parietal activity in the theta band (4-7 Hz) to enhanced top-down control. Parietal activity <bold>in</bold> the theta-band was also found to be inversely related to the delta-band auditory entrainment <bold>in</bold> superior temporal gyrus (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Keitel et al., 2017</xref>). None of these studies, however, had reported these signatures of suppression involves the dorsal anterior cingulate (dACC), a key region in adaptive control of effortful listening (e.g., Vaden et al., 2016; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Erb et al., 2013</xref>) and showing here increased TRF phase coherence for hard-to-ignore speech (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-14" hwp:rel-id="F3">Fig. 3D</xref>).</p></sec></sec><sec id="s4" hwp:id="sec-26"><title hwp:id="title-32">Conclusions</title><p hwp:id="p-71">The present data show how components of the unfolding temporal response function as identified in a forward encoding model reflect distinct neural stages of attentional filtering. These stages contain the initial, attention-independent encoding of acoustic signals; the extraction and amplification of relevant features; and lastly a robust, purely attention-driven response to attended acoustic signals. A phase-locked, active-suppression response to ignored acoustic signals originates from attentional networks accompanied by an increased phase coherence originating from dACC. In sum, with a design closer to real-life listening scenarios, our study provides insight into how selective neural processing of attended speech unfolds and is upheld not only by auditory cortices.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-33">Acknowledgments</title><p hwp:id="p-72">Research was supported by the European Research Council (ERC-CoG-2014 646696 to JO) and the Oticon Foundation (NEURO-CHAT).</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-34">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Bentler RA"><surname>Bentler</surname> <given-names>RA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palmer C"><surname>Palmer</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dittberner AB"><surname>Dittberner</surname> <given-names>AB</given-names></string-name>. <year>2004</year>. <article-title hwp:id="article-title-2">Hearing-in-Noise: Comparison of Listeners with Normal and (Aided) Impaired Hearing</article-title>. <source hwp:id="source-1">J Am Acad Audiol</source>. <volume>15</volume>:<fpage>216</fpage>–<lpage>225</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Berens P"><surname>Berens</surname> <given-names>P</given-names></string-name>. <year>2009</year>. <article-title hwp:id="article-title-3">CircStat: A MATLAB Toolbox for Circular Statistics</article-title>. <source hwp:id="source-2">J Stat Softw</source>. <fpage>31</fpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Biesmans W"><surname>Biesmans</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Das N"><surname>Das</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Francart T"><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bertrand A"><surname>Bertrand</surname> <given-names>A</given-names></string-name>. <year>2016</year>. <article-title hwp:id="article-title-4">Auditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario</article-title>. <source hwp:id="source-3">{IEEE} Trans Neural Syst Rehabil Eng</source>. <fpage>4320</fpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2 xref-ref-4-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cheveigne A De"><surname>Cheveigne</surname> <given-names>A</given-names> <suffix>De</suffix></string-name>, <string-name name-style="western" hwp:sortable="Poeppel D"><surname>Poeppel</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>. <year>2010</year>. <article-title hwp:id="article-title-5">Neuropsychologia Neural dynamics of attending and ignoring in human auditory cortex</article-title>. <source hwp:id="source-4">Neuropsychologia</source>. <volume>48</volume>:<fpage>3262</fpage>–<lpage>3271</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Cherry EC"><surname>Cherry</surname> <given-names>EC</given-names></string-name>. <year>1953</year>. <article-title hwp:id="article-title-6">Some Experiments on the Recognition of Speech, with One and with Two Ears</article-title>. <source hwp:id="source-5">J Acoust Soc Am</source>. <volume>25</volume>:<fpage>975</fpage>–<lpage>979</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Chi T"><surname>Chi</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ru P"><surname>Ru</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shamma SA"><surname>Shamma</surname> <given-names>SA</given-names></string-name>. <year>2005</year>. <article-title hwp:id="article-title-7">Multiresolution Spectrotemporal Analysis of Complex Sounds</article-title>. <source hwp:id="source-6">J Acoust Soc Am</source>. <volume>118</volume>:<fpage>887</fpage>–<lpage>906</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Crosse MJ"><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Di Liberto GM"><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bednar A"><surname>Bednar</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lalor EC"><surname>Lalor</surname> <given-names>EC</given-names></string-name>. <year>2016</year>. <article-title hwp:id="article-title-8">The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title>. <source hwp:id="source-7">Front Hum Neurosci</source>. <volume>10</volume>:<fpage>604</fpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Di Liberto GM"><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Sullivan JA"><surname>O’Sullivan</surname> <given-names>JA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lalor EC"><surname>Lalor</surname> <given-names>EC</given-names></string-name>. <year>2015</year>. <article-title hwp:id="article-title-9">Low-frequency cortical entrainment to speech reflects phoneme­ level processing</article-title>. <source hwp:id="source-8">Curr Biol</source>. <volume>25</volume>:<fpage>2457</fpage>–<lpage>2465</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chatterjee M"><surname>Chatterjee</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>. <year>2014</year>. <article-title hwp:id="article-title-10">Robust cortical entrainment to the speech envelope relies on the spectra-temporal fine structure</article-title>. <source hwp:id="source-9">Neuroimage</source>. <volume>88</volume>:<fpage>41</fpage>–<lpage>46</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2 xref-ref-10-3 xref-ref-10-4 xref-ref-10-5"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>. <year>2012</year>. <article-title hwp:id="article-title-11">Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title>. <source hwp:id="source-10">J Neurophysiol</source>. <volume>107</volume>:<fpage>78</fpage>–<lpage>89</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>. <year>2013</year>. <article-title hwp:id="article-title-12">Adaptive temporal encoding leads to a background-insensitive cortical representation of speech</article-title>. <source hwp:id="source-11">J Neurosci</source>. <volume>33</volume>:<fpage>5728</fpage>–<lpage>5735</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Drongelen W Van"><surname>Drongelen</surname> <given-names>W</given-names> <suffix>Van</suffix></string-name>, <string-name name-style="western" hwp:sortable="Yuchtman M"><surname>Yuchtman</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Veen BD Van"><surname>Veen</surname> <given-names>BD</given-names> <suffix>Van</suffix></string-name>, <string-name name-style="western" hwp:sortable="Huffelen AC Van"><surname>Huffelen</surname> <given-names>AC</given-names> <suffix>Van</suffix></string-name>. <year>1994</year>. <article-title hwp:id="article-title-13">A Spatial Filtering Technique to Detect and Localize Multiple Sources in the Brain</article-title>. <source hwp:id="source-12">Brain Topogr</source>. <volume>9</volume>:<fpage>39</fpage>–<lpage>49</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2 xref-ref-13-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Efron B"><surname>Efron</surname> <given-names>B</given-names></string-name>. <year>1979</year>. <article-title hwp:id="article-title-14">Bootstrap methods: Another look at the jackknife</article-title>. <source hwp:id="source-13">Ann Stat</source>. <volume>7</volume>:<fpage>1</fpage>–<lpage>26</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Erb J"><surname>Erb</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eisner F"><surname>Eisner</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>. <year>2013</year>. <source hwp:id="source-14">The Brain Dynamics of Rapid Perceptual Adaptation to Adverse Listening Conditions</source>. <volume>33</volume>:<fpage>10688</fpage>–<lpage>10697</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2 xref-ref-15-3 xref-ref-15-4 xref-ref-15-5 xref-ref-15-6 xref-ref-15-7 xref-ref-15-8"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Fiedler L"><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wostmann M"><surname>Wostmann</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Graversen C"><surname>Graversen</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brandmeyer A"><surname>Brandmeyer</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lunner T"><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-15">Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech</article-title>. <source hwp:id="source-15">J Neural Eng</source>. <fpage>14</fpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Forte AE"><surname>Forte</surname> <given-names>AE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Etard O"><surname>Etard</surname> <given-names>O</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reichenbach T"><surname>Reichenbach</surname> <given-names>T</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-16">The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention</article-title>. <source hwp:id="source-16">Elife</source>. <fpage>1</fpage>–<lpage>12</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Fuglsang SA"><surname>Fuglsang</surname> <given-names>SA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dau T"><surname>Dau</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hjortkjcer J"><surname>Hjortkjcer</surname> <given-names>J</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-17">Neurolmage Noise-robust cortical tracking of attended speech in real­ world acoustic scenes</article-title>. <source hwp:id="source-17">Neuroimage</source>. <volume>156</volume>:<fpage>435</fpage>–<lpage>444</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Giordano BL"><surname>Giordano</surname> <given-names>BL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ince RAA"><surname>Ince</surname> <given-names>RAA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross J"><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schyns PG"><surname>Schyns</surname> <given-names>PG</given-names></string-name>, <string-name name-style="western" hwp:sortable="Panzeri S"><surname>Panzeri</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-18">Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title>. <source hwp:id="source-18">Elife</source>. <volume>6</volume>:<fpage>1</fpage>–<lpage>27</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Giuliano RJ"><surname>Giuliano</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Karns CM"><surname>Karns</surname> <given-names>CM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Neville HJ"><surname>Neville</surname> <given-names>HJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hillyard SA"><surname>Hillyard</surname> <given-names>SA</given-names></string-name>. <year>2015</year>. <article-title hwp:id="article-title-19">NIH Public Access</article-title>. <source hwp:id="source-19">J Cogn Neurosci</source>. <volume>26</volume>:<fpage>2682</fpage>–<lpage>2690</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>. <year>2012</year>. <article-title hwp:id="article-title-20">Frequency modulation entrains slow neural oscillations and optimizes human listening behavior</article-title>. <source hwp:id="source-20">PNAS</source>. <volume>109</volume>:<fpage>20095</fpage>–<lpage>20100</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>. <year>2013</year>. <article-title hwp:id="article-title-21">Frequency-specific adaptation in human auditory cortex depends on the spectral variance in the acoustic stimulation</article-title>. <source hwp:id="source-21">J Neurophysiol</source>. <volume>109</volume>:<fpage>2086</fpage>–<lpage>2096</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Hertrich I"><surname>Hertrich</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dietrich S"><surname>Dietrich</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Trouvain J"><surname>Trouvain</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moos A"><surname>Moos</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ackermann H"><surname>Ackermann</surname> <given-names>H</given-names></string-name>. <year>2012</year>. <article-title hwp:id="article-title-22">Magnetic brain activity phase-locked to the envelope, the syllable onsets, and the fundamental frequency of a perceived speech signal</article-title>. <source hwp:id="source-22">Psychophysiology</source>. <volume>49</volume>:<fpage>322</fpage>–<lpage>334</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Hoerl AE"><surname>Hoerl</surname> <given-names>AE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kennard RW"><surname>Kennard</surname> <given-names>RW</given-names></string-name>. <year>1970</year>. <article-title hwp:id="article-title-23">Ridge Regression: Biased Estimation for Nonorthogonal Problems</article-title>. <source hwp:id="source-23">Technometrics</source>. <volume>12</volume>:<fpage>55</fpage>–<lpage>67</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Horton C"><surname>Horton</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Srinivasan R"><surname>Srinivasan</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zmura MD"><surname>Zmura</surname> <given-names>MD</given-names></string-name>. <year>2014</year>. <article-title hwp:id="article-title-24">Envelope responses in single-trial EEG indicate attended speaker in a “cocktail party.”</article-title> <source hwp:id="source-24">J Neural Eng</source>. <volume>11</volume>:<fpage>12</fpage>pp.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Horton C"><surname>Horton</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zmura MD"><surname>Zmura</surname> <given-names>MD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Srinivasan R"><surname>Srinivasan</surname> <given-names>R</given-names></string-name>. <year>2013</year>. <article-title hwp:id="article-title-25">Suppression of competing speech through entrainment of cortical oscillations</article-title>. <source hwp:id="source-25">J Neurophysiol</source>. <volume>109</volume>:<fpage>3082</fpage>–<lpage>3093</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Kaya EM"><surname>Kaya</surname> <given-names>EM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Elhilali M"><surname>Elhilali</surname> <given-names>M</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-26">Modelling auditory attention</article-title>. <source hwp:id="source-26">Phil Trans R Soc B</source>. <fpage>372</fpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Kayser SJ"><surname>Kayser</surname> <given-names>SJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ince RAA"><surname>Ince</surname> <given-names>RAA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross J"><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name>. <year>2015</year>. <article-title hwp:id="article-title-27">Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha</article-title>. <source hwp:id="source-27">J Neurosci</source>. <volume>35</volume>:<fpage>14691</fpage>–<lpage>14701</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Keitel A"><surname>Keitel</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ince RAA"><surname>Ince</surname> <given-names>RAA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross J"><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-28">Neurolmage Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</article-title>. <source hwp:id="source-28">Neuroimage</source>. <volume>147</volume>:<fpage>32</fpage>–<lpage>42</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Kong Y-Y"><surname>Kong</surname> <given-names>Y-Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Somarowthu A"><surname>Somarowthu</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>. <year>2015</year>. <article-title hwp:id="article-title-29">Effects of Spectral Degradation on Attentional Modulation of Cortical Auditory Responses to Continuous Speech</article-title>. <source hwp:id="source-29">J Assoc Res Otolaryngol</source>. <volume>16</volume>:<fpage>783</fpage>–<lpage>796</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Kong YY"><surname>Kong</surname> <given-names>YY</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mullangi A"><surname>Mullangi</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>. <year>2014</year>. <article-title hwp:id="article-title-30">Differential modulation of auditory responses to attended and unattended speech in different listening conditions</article-title>. <source hwp:id="source-30">Hear Res</source>. <volume>316</volume>:<fpage>73</fpage>–<lpage>81</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Lachaux J"><surname>Lachaux</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rodriguez E"><surname>Rodriguez</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martinerie J"><surname>Martinerie</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varela FJ"><surname>Varela</surname> <given-names>FJ</given-names></string-name>. <year>1999</year>. <article-title hwp:id="article-title-31">Measuring Phase Synchrony in Brain Signals</article-title>. <source hwp:id="source-31">Hum Brain Mapp</source>. <volume>8</volume>:<fpage>194</fpage>–<lpage>208</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Karmas G"><surname>Karmas</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mehta AD"><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ulbert I"><surname>Ulbert</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name>. <year>2008</year>. <article-title hwp:id="article-title-32">Entrainment of Neuronal Attentional Selection</article-title>. <source hwp:id="source-32">Science</source> (80-). <volume>320</volume>:<fpage>110</fpage>–<lpage>113</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2 xref-ref-33-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Lalor EC"><surname>Lalor</surname> <given-names>EC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Power AJ"><surname>Power</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reilly RB"><surname>Reilly</surname> <given-names>RB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foxe JJ"><surname>Foxe</surname> <given-names>JJ</given-names></string-name>. <year>2009</year>. <article-title hwp:id="article-title-33">Resolving Precise Temporal Processing Properties of the Auditory System Using Continuous Stimuli</article-title>. <source hwp:id="source-33">J Neurophysiol</source>. <volume>102</volume>:<fpage>349</fpage>–<lpage>359</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Makeig S"><surname>Makeig</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Debener S"><surname>Debener</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Onton J"><surname>Onton</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Delorme A"><surname>Delorme</surname> <given-names>A</given-names></string-name>. <year>2004</year>. <article-title hwp:id="article-title-34">Mining event-related brain dynamics</article-title>. <source hwp:id="source-34">Trends Cogn Sci</source>. <volume>8</volume>:<fpage>204</fpage>–<lpage>210</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Maris E"><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oostenveld R"><surname>Oostenveld</surname> <given-names>R</given-names></string-name>. <year>2007</year>. <article-title hwp:id="article-title-35">Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source hwp:id="source-35">J Neurosci Methods</source>. <volume>164</volume>:<fpage>177</fpage>–<lpage>190</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Melara RD"><surname>Melara</surname> <given-names>RD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rao A"><surname>Rao</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tong Y"><surname>Tong</surname> <given-names>Y</given-names></string-name>. <year>2002</year>. <article-title hwp:id="article-title-36">The duality of selection: Excitatory and inhibitory processes in auditory selective attention</article-title>. <source hwp:id="source-36">J Exp Psychol</source>. <volume>28</volume>:<fpage>279</fpage>–<lpage>306</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Mesgarani N"><surname>Mesgarani</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chang EF"><surname>Chang</surname> <given-names>EF</given-names></string-name>. <year>2012</year>. <article-title hwp:id="article-title-37">Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source hwp:id="source-37">Nature</source>. <volume>485</volume>:<fpage>233</fpage>–<lpage>236</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Mirkovic B"><surname>Mirkovic</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Debener S"><surname>Debener</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jaeger M"><surname>Jaeger</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vos M De"><surname>Vos</surname> <given-names>M</given-names> <suffix>De</suffix></string-name>. <year>2015</year>. <article-title hwp:id="article-title-38">Decoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications</article-title>. <source hwp:id="source-38">J Neural Eng</source>. <volume>12</volume>:<fpage>46007</fpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Naatanen R"><surname>Naatanen</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gaillard AWK"><surname>Gaillard</surname> <given-names>AWK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varey CA"><surname>Varey</surname> <given-names>CA</given-names></string-name>. <year>1981</year>. <article-title hwp:id="article-title-39">Attention effects on auditory EPs as a function of inter-stimulus interval</article-title>. <source hwp:id="source-39">Biol Psychol</source>. <volume>13</volume>:<fpage>173</fpage>–<lpage>187</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="O’Sullivan JA"><surname>O’Sullivan</surname> <given-names>JA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Power AJ"><surname>Power</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mesgarani N"><surname>Mesgarani</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rajaram S"><surname>Rajaram</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foxe JJ"><surname>Foxe</surname> <given-names>JJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shinn-Cunningham BG"><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></string-name>, <string-name name-style="western" hwp:sortable="Slaney M"><surname>Slaney</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shamma SA"><surname>Shamma</surname> <given-names>SA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lalor EC"><surname>Lalor</surname> <given-names>EC</given-names></string-name>. <year>2014</year>. <article-title hwp:id="article-title-40">Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG</article-title>. <source hwp:id="source-40">Cereb Cortex</source>. <volume>25</volume>:<fpage>1697</fpage>–<lpage>1706</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wostmann M"><surname>Wostmann</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hellbernd N"><surname>Hellbernd</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilsch A"><surname>Wilsch</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maess B"><surname>Maess</surname> <given-names>B</given-names></string-name>. <year>2012</year>. <article-title hwp:id="article-title-41">Adverse Listening Conditions and Memory Load Drive a Common Alpha Oscillatory Network</article-title>. <source hwp:id="source-41">J Neurosci</source>. <volume>32</volume>:<fpage>12376</fpage>–<lpage>12383</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Oostenveld R"><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fries P"><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maris E"><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schoffelen J"><surname>Schoffelen</surname> <given-names>J</given-names></string-name>. <year>2011</year>. <article-title hwp:id="article-title-42">FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source hwp:id="source-42">Comput IntelI Neurosci</source>. <fpage>2011</fpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="book" citation-type="book" ref:id="238642v2.43" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Paninski L"><surname>Paninski</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pillow J"><surname>Pillow</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lewi J"><surname>Lewi</surname> <given-names>J</given-names></string-name>. <year>2007</year>. <chapter-title>Statistical models for neural encoding, decoding, and optimal stimulus design</chapter-title>. In: <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Cisek P"><surname>Cisek</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Drew T"><surname>Drew</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kalaska JF"><surname>Kalaska</surname> <given-names>JF</given-names></string-name></person-group>, editors. <source hwp:id="source-43">Computational Neuroscience: Theoretical Insights into Brain Function. Progress in Brain Research</source>. <publisher-name>Elsevier</publisher-name>. p. <fpage>493</fpage>–<lpage>507</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Petersen EB"><surname>Petersen</surname> <given-names>EB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wostmann M"><surname>Wostmann</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lunner T"><surname>Lunner</surname> <given-names>T</given-names></string-name>. <year>2016</year>. <article-title hwp:id="article-title-43">Neural tracking of attended versus ignored speech is differentially affected by hearing loss</article-title>. <source hwp:id="source-44">J Neurophysiol</source>. <volume>117</volume>:<fpage>18</fpage>–<lpage>27</lpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Pichora-Fuller MK"><surname>Pichora-Fuller</surname> <given-names>MK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schneider BA"><surname>Schneider</surname> <given-names>BA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daneman M"><surname>Daneman</surname> <given-names>M</given-names></string-name>. <year>1995</year>. <article-title hwp:id="article-title-44">How young and old listen to and remember speech in noise</article-title>. <source hwp:id="source-45">J Acoust Soc Am</source>. <volume>91</volume>:<fpage>593</fpage>–<lpage>608</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2 xref-ref-46-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Picton T"><surname>Picton</surname> <given-names>T</given-names></string-name>. <year>2013</year>. <article-title hwp:id="article-title-45">Hearing in Time: Evoked Potential Studies ofTemporal Processing</article-title>. <source hwp:id="source-46">Ear Hear</source>. <volume>34</volume>:<fpage>385</fpage>–<lpage>401</lpage>.</citation></ref><ref id="c47" hwp:id="ref-47"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hillyard SA"><surname>Hillyard</surname> <given-names>SA</given-names></string-name>. <year>1974</year>. <article-title hwp:id="article-title-46">Human auditory evoked potentials. ii: effects of attention</article-title>. <source hwp:id="source-47">Electroencephalogr Clin Neurophysiol</source>. <volume>36</volume>:<fpage>191</fpage>–<lpage>199</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1 xref-ref-48-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Pamper U"><surname>Pamper</surname> <given-names>U</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-47">The impact of visual gaze direction on auditory object tracking</article-title>. <source hwp:id="source-48">Sci Rep</source>. <volume>7</volume>:<fpage>1</fpage>–<lpage>16</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2 xref-ref-49-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Power AJ"><surname>Power</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foxe JJ"><surname>Foxe</surname> <given-names>JJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Forde EJ"><surname>Forde</surname> <given-names>EJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reilly RB"><surname>Reilly</surname> <given-names>RB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lalor EC"><surname>Lalor</surname> <given-names>EC</given-names></string-name>. <year>2012</year>. <article-title hwp:id="article-title-48">At what time is the cocktail party? A late locus of selective attention to natural speech</article-title>. <source hwp:id="source-49">Eur J Neurosci</source>. <volume>35</volume>:<fpage>1497</fpage>–<lpage>1503</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Puvvada KC"><surname>Puvvada</surname> <given-names>KC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon XJZ"><surname>Simon</surname> <given-names>XJZ</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-49">Cortical Representations of Speech in a Multitalker Auditory Scene</article-title>. <source hwp:id="source-50">J Neurosci</source>. <volume>37</volume>:<fpage>9189</fpage>–<lpage>9196</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Slee SJ"><surname>Slee</surname> <given-names>SJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="David SV"><surname>David</surname> <given-names>SV</given-names></string-name>. <year>2015</year>. <article-title hwp:id="article-title-50">Rapid Task-Related Plasticity of Spectrotemporal Receptive Fields in the Auditory Midbrain</article-title>. <source hwp:id="source-51">J Neurosci</source>. <volume>35</volume>:<fpage>13090</fpage>–<lpage>13102</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Smeds K"><surname>Smeds</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wolters F"><surname>Wolters</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rung M"><surname>Rung</surname> <given-names>M</given-names></string-name>. <year>2015</year>. <article-title hwp:id="article-title-51">Estimation of Signal-to-Noise Ratios in Realistic Sound Scenarios</article-title>. <source hwp:id="source-52">J Am Acad Audiol</source>. <volume>196</volume>:<fpage>183</fpage>–<lpage>196</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Spaak E"><surname>Spaak</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lange FP De"><surname>Lange</surname> <given-names>FP</given-names> <suffix>De</suffix></string-name>, <string-name name-style="western" hwp:sortable="Jensen O"><surname>Jensen</surname> <given-names>O</given-names></string-name>. <year>2014</year>. <article-title hwp:id="article-title-52">Local Entrainment of Alpha Oscillations by Visual Stimuli Causes Cyclic Modulation of Perception</article-title>. <source hwp:id="source-53">J Neurosci</source>. <volume>34</volume>:<fpage>3536</fpage>–<lpage>3544</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Veen BO Van"><surname>Veen</surname> <given-names>BO</given-names> <suffix>Van</suffix></string-name>, <string-name name-style="western" hwp:sortable="Drongelen W Van"><surname>Drongelen</surname> <given-names>W</given-names> <suffix>Van</suffix></string-name>, <string-name name-style="western" hwp:sortable="Yuchtman M"><surname>Yuchtman</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Suzuki A"><surname>Suzuki</surname> <given-names>A</given-names></string-name>. <year>1997</year>. <article-title hwp:id="article-title-53">Localization of Brain Electrical Activity via Linearly Constrained Minimum Variance Spatial Filtering</article-title>. <source hwp:id="source-54">IEEE Trans Biomed Eng</source>. <volume>44</volume>:<fpage>867</fpage>–<lpage>880</lpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Willmore BOB"><surname>Willmore</surname> <given-names>BOB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cooke JE"><surname>Cooke</surname> <given-names>JE</given-names></string-name>, <string-name name-style="western" hwp:sortable="King AJ"><surname>King</surname> <given-names>AJ</given-names></string-name>. <year>2014</year>. <article-title hwp:id="article-title-54">Hearing in noisy environments: noise invariance and contrast gain control</article-title>. <source hwp:id="source-55">J Physiol</source>. <volume>16</volume>:<fpage>3371</fpage>–<lpage>3381</lpage>.</citation></ref><ref id="c56" hwp:id="ref-56"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Woldorff MG"><surname>Woldorff</surname> <given-names>MG</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hillyard SA"><surname>Hillyard</surname> <given-names>SA</given-names></string-name>. <year>1991</year>. <article-title hwp:id="article-title-55">Modulation of early auditory processing during selective listening to rapidly presented tones*</article-title>. <source hwp:id="source-56">Electroencephalogr Clin Neurophysiol</source>. <volume>79</volume>:<fpage>170</fpage>–<lpage>191</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Woolgar A"><surname>Woolgar</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jackson J"><surname>Jackson</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duncan J"><surname>Duncan</surname> <given-names>J</given-names></string-name>. <year>2016</year>. <article-title hwp:id="article-title-56">Coding of Visual, Auditory, Rule, and Response Information in the Brain: 10 Years of Multivoxel Pattern Analysis</article-title>. <source hwp:id="source-57">J Cogn Neurosci</source>. <volume>28</volume>:<fpage>1433</fpage>–<lpage>1454</lpage>.</citation></ref><ref id="c58" hwp:id="ref-58"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.58" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Wostmann M"><surname>Wostmann</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fiedler L"><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>. <year>2016</year>. <article-title hwp:id="article-title-57">Tracking the signal, cracking the code: Speech and speech comprehension in non-invasive human electrophysiology</article-title>. <source hwp:id="source-58">Languange, Cogn Neurosci</source>. <fpage>1</fpage>–<lpage>15</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Wostmann M"><surname>Wostmann</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lim S"><surname>Lim</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name>. <year>2017</year>. <article-title hwp:id="article-title-58">The Human Neural Alpha Response to Speech is a Proxy of Attentional Control</article-title>. <source hwp:id="source-59">Cereb Cortex</source>. <volume>27</volume>:<fpage>3307</fpage>–<lpage>3317</lpage>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><citation publication-type="book" citation-type="book" ref:id="238642v2.60" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Zar JH"><surname>Zar</surname> <given-names>JH</given-names></string-name>. <year>1999</year>. <chapter-title>Biostatistical Analysis</chapter-title>. <edition>4th ed.</edition> <publisher-loc>Michigan</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1 xref-ref-61-2 xref-ref-61-3"><citation publication-type="journal" citation-type="journal" ref:id="238642v2.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Zion Golumbic EM"><surname>Zion Golumbic</surname> <given-names>EM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bickel S"><surname>Bickel</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schevon CA"><surname>Schevon</surname> <given-names>CA</given-names></string-name>, <string-name name-style="western" hwp:sortable="McKhann GM"><surname>McKhann</surname> <given-names>GM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goodman RR"><surname>Goodman</surname> <given-names>RR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Emerson R"><surname>Emerson</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mehta AD"><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poeppel D"><surname>Poeppel</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name>. <year>2013</year>. <article-title hwp:id="article-title-59">Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party.”</article-title> <source hwp:id="source-60">Neuron</source>. <volume>77</volume>:<fpage>980</fpage>–<lpage>991</lpage>.</citation></ref></ref-list></back></article>
