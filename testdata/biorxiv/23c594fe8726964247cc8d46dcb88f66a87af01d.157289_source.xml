<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/157289</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;157289</article-id><article-id pub-id-type="other" hwp:sub-type="slug">157289</article-id><article-id pub-id-type="other" hwp:sub-type="tag">157289</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Neural classifiers with limited connectivity and recurrent readouts</article-title></title-group><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Kushnir Lyudmila"><surname>Kushnir</surname><given-names>Lyudmila</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3035-6652</contrib-id><name name-style="western" hwp:sortable="Fusi Stefano"><surname>Fusi</surname><given-names>Stefano</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3035-6652"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label>GNT - LNC, Departement d'etudes cognitives, Ecole normale superieure, INSERM, <institution hwp:id="institution-1">PSL Research University</institution>, 75005 Paris, <country>France</country>
</aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label>Center for Theoretical Neuroscience, <institution hwp:id="institution-2">College of Physicians and Surgeons, Columbia University</institution>
</aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution>
</aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Kavli Institute for Brain Sciences, Columbia University</institution>
</aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-06-29T08:18:04-07:00">
    <day>29</day><month>6</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-12-14T14:27:12-08:00">
    <day>14</day><month>12</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-06-29T08:43:06-07:00">
    <day>29</day><month>6</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-12-14T14:33:44-08:00">
    <day>14</day><month>12</month><year>2017</year>
  </pub-date><elocation-id>157289</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-06-28"><day>28</day><month>6</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2017-12-14"><day>14</day><month>12</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-12-14"><day>14</day><month>12</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by/4.0/</ext-link></p></license></permissions><self-uri xlink:href="157289.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/157289v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="157289.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/157289v3/157289v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/157289v3/157289v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">For many neural network models in which neurons are trained to classify inputs like perceptrons, the number of inputs that can be classified is limited by the connectivity of each neuron, even when the total number of neurons is very large. This poses the problem of how the biological brain can take advantage of its huge number of neurons given that the connectivity is sparse. One solution is to combine multiple perceptrons together, as in committee machines. The number of classifiable random patterns would then grow linearly with the number of perceptrons, even when each perceptron has limited connectivity. However, the problem is moved to the downstream readout neurons, which would need a number of connections that is as large as the number of perceptrons. Here we propose a different approach in which the readout is implemented by connecting multiple perceptrons in a recurrent attractor neural network. We prove analytically that the number of classifiable random patterns can grow unboundedly with the number of perceptrons, even when the connectivity of each perceptron remains finite. Most importantly, both the recurrent connectivity and the connectivity of downstream readouts also remain finite. Our study shows that feed-forward neural classifiers with numerous long range afferent connections can be replaced by recurrent networks with sparse long range connectivity without sacrificing the classification performance. Our strategy could be used to design more general scalable network architectures with limited connectivity, which resemble more closely the brain neural circuits which are dominated by recurrent connectivity.</p></abstract><counts><page-count count="49"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-2">Significance statement</title><p hwp:id="p-3">The mammalian brain has a huge number of neurons but the connectivity is rather sparse. This observation seems to contrast with the theoretical studies showing that for many neural network models the performance scales with the number of connections per neuron and not with the total number of neurons. To solve this dilemma, we propose a model in which a recurrent network reads out multiple neural classifiers. Its performance scales with the total number of neurons even when each neuron of the network has limited connectivity. Our study reveals an important role of recurrent connections in neural systems like the hippocampus, in which the computational limitations due to sparse long range feed-forward connectivity might be compensated by local recurrent connections.</p></sec><sec id="s2" hwp:id="sec-2" hwp:rev-id="xref-sec-2-1"><label>2</label><title hwp:id="title-3">Introduction</title><p hwp:id="p-4">The performance of a neural circuit is often evaluated by determining the number of input-output functions that can be implemented, or equivalently by the number of inputs that can be classified correctly by the neural circuit. Theoretical studies on perceptrons [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>] and recurrent neural circuits (see e.g. [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>]) have shown that typically the performance of a neural circuit scales with the number of synaptic connections that individual neurons receive, and not with the total number of synapses, or with the total number of neurons (see e.g. [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]). This is clearly a problem in the biological brain in which the connectivity is sparse, especially when long range connections are considered [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]. One striking example is the mammalian hippocampus [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>]. A typical pyramidal neuron in rodent CA3 receives only 50 synapses from the upstream area[<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>], the dentate gyrus (DG), which contains around 10<sup>6</sup> neurons. Not only the connectivity is sparse but also the neural activity[<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>, <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>].</p><p hwp:id="p-5">One possible way to overcome the limitations of sparse connectivity is to adopt the strategy of “committee machines” [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>], which are basically populations of classifiers. Each classifier is weak, as a perceptron with limited connectivity, but the output is generated by reading out a large number of weak classifiers and by combining them together using a majority vote or some more sophisticated strategies. The final classification performance is significantly better than the one of each individual classifier, provided that the errors of the individual classifiers are sufficiently independent. The term committee machines goes back to 1960-s [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>], but they have also been a focus of more recent studies (see e.g. [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>], [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>]) and basically they are all based on strategies that in machine learning are known as <italic toggle="yes">ensemble methods</italic> or <italic toggle="yes">hypothesis boosting</italic> [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>, <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>], strategies that are often adopted also in statistics [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>, <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>, <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>, <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>]. Some of the examples include stacking [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>], bagging [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>], arcing [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>] and adaboost [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>, <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>].</p><p hwp:id="p-6">One class of committee machines is implemented using populations of neurons, each essentially behaving as a neural classifier, like a perceptron [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>, <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>, <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>, <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]. Classifiers with limited connectivity are weak classifiers. It is possible to compute the classification capacity when each neural classifier has sparse connectivity [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>]. The connections between the <italic toggle="yes">N</italic> input neurons and the <italic toggle="yes">M</italic> &lt; <italic toggle="yes">N</italic> neural classifiers are assumed to be non-overlapping (<italic toggle="yes">N/M</italic> connections per “perceptron”) and plastic. The final response of the committee machine is obtained by majority vote of the <italic toggle="yes">M</italic> neural classifiers, which can be easily implemented by introducing a readout neuron that is connected to all the neural classifiers with equal weights. The maximum number of correctly classified inputs is proportional to <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="157289_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> whereas each neural classifier would not go beyond <italic toggle="yes">N/M</italic> inputs. This is a favorable scaling and it is similar to the one obtained in other committee machines. However, one has to keep in mind that in these implementations the neural classifiers have sparse connectivity, but the readout neuron performing the majority vote should have a number of connections that scales with <italic toggle="yes">N</italic>.</p><p hwp:id="p-7">Here we propose a network architecture that overcomes the restrictions imposed by the limited connectivity, as in the committee machines, but it replaces the readout neuron that has extensive connectivity with a more biologically plausible recurrent network in which all the neurons have a number of connections that remains finite when the number of classifiable patterns grows unboundedly. More specifically we show that the number of random inputs that can be correctly classified scales linearly with the number of input neurons <italic toggle="yes">N</italic>, even when the number of connections per neural classifier <italic toggle="yes">C<sub>F</sub></italic> does not increase with <italic toggle="yes">N</italic>. The number of neural classifiers <italic toggle="yes">M</italic> is assumed to be proportional to <italic toggle="yes">N</italic>.</p><p hwp:id="p-8">Interestingly, under certain conditions the recurrent scheme has larger classification capacity than the majority vote scheme. This happens for sparse input representations, the regime that is relevant for the mammalian hippocampus and that we investigate in detail.</p></sec><sec id="s3" hwp:id="sec-3" hwp:rev-id="xref-sec-3-1 xref-sec-3-2"><label>3</label><title hwp:id="title-4">Methods</title><sec id="s3a" hwp:id="sec-4" hwp:rev-id="xref-sec-4-1"><label>3.1</label><title hwp:id="title-5">Fully connected readout</title><p hwp:id="p-9">In this section we derive the classification capacity of a single fully connected linear threshold readout, or <italic toggle="yes">perceptron</italic> (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1a</xref>) achieved with a simple learning rule that we employ throughout this work. We assume that the input patterns and labels are random and uncorrelated, meaning that the activity of each input unit as well as the label for each pattern is chosen independently, which makes calculations analytically tractable. We use a simple Hebbian-like learning rule, that is not optimal, and thus leads to a lower capacity than Cover’s 2<italic toggle="yes">N</italic> result [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>]. However, the scaling of the maximal number of learned input patterns <italic toggle="yes">P</italic><sub>max</sub> with the number of input units <italic toggle="yes">N</italic> is still linear, as is shown below.</p><fig id="fig1" position="float" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-10">Architectures of the three network classifiers considered in the study, and their scaling properties. <bold>a</bold>. Fully connected readout, considered in <xref ref-type="sec" rid="s3a" hwp:id="xref-sec-4-1" hwp:rel-id="sec-4">section 3.1.</xref> The capacity of this classifier grows linearly with the number of input units <italic toggle="yes">N</italic>, however the number of afferent connections <italic toggle="yes">C<sub>F</sub></italic> grows as quickly as <italic toggle="yes">N</italic>. <bold>b</bold>. Committee machine of partially connected perceptrons (<xref ref-type="sec" rid="s3b" hwp:id="xref-sec-7-1" hwp:rel-id="sec-7">section 3.2</xref>). The collective decision is made using a majority vote. Even though the number of connections per perceptron can be kept constant as the number of input neurons <italic toggle="yes">N</italic> increases, the number of readouts <italic toggle="yes">M</italic> has to grow with <italic toggle="yes">N</italic> in order to match the performance scaling of (a). The majority vote strategy requires another downstream readout, whose connectivity grows with <italic toggle="yes">M</italic> and hence with <italic toggle="yes">N</italic>. <bold>c.</bold> The recurrent readout that we propose in <xref ref-type="sec" rid="s3c" hwp:id="xref-sec-10-1" hwp:rel-id="sec-10">section 3.3</xref>. As <italic toggle="yes">N</italic> → ∞, the number of feedforward connections per perceptron <italic toggle="yes">C<sub>F</sub></italic>, the number of recurrent connections per perceptron <italic toggle="yes">C<sub>R</sub></italic>, as well as the number of connections of the downstream readout stay constant when <italic toggle="yes">N</italic> increases.</p></caption><graphic xlink:href="157289_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><sec id="s3a1" hwp:id="sec-5"><label>3.1.1</label><title hwp:id="title-6">Input statistics</title><p hwp:id="p-11">We assume that pairs (<italic toggle="yes">ξ<sup>μ</sup></italic>, <italic toggle="yes">η<sup>μ</sup></italic>) of a pattern <italic toggle="yes">ξ<sup>μ</sup></italic> and a label <italic toggle="yes">η<sup>μ</sup></italic> are drawn from a random ensemble of <italic toggle="yes">P</italic> pairs (pattern, label). The pattern components <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="157289_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> on all <italic toggle="yes">N</italic> input units and labels <italic toggle="yes">η<sup>μ</sup></italic> are random independent variables. We assume that each component <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="157289_inline2a.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> (<italic toggle="yes">i</italic> = 1… <italic toggle="yes">N</italic> is the unit index and <italic toggle="yes">μ</italic> = 1… <italic toggle="yes">P</italic> is the pattern index) is activated to 1 with probability <italic toggle="yes">f</italic> called <italic toggle="yes">coding level</italic> and otherwise is 0, and that label <italic toggle="yes">η<sup>μ</sup></italic> takes one of the two values: <italic toggle="yes">η<sup>μ</sup></italic> = +1 with probability <italic toggle="yes">y</italic>, called the output sparseness, and <italic toggle="yes">η<sup>μ</sup></italic> = −1 otherwise:
<disp-formula id="eqn31" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="157289_eqn3-1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives></disp-formula>
</p></sec><sec id="s3a2" hwp:id="sec-6"><label>3.1.2</label><title hwp:id="title-7">Learning rule and the synaptic current</title><p hwp:id="p-12">The linear threshold readout, or perceptron, classifies its inputs based on the sign of the weighted sum of the input components. This sum is sometimes called <italic toggle="yes">synaptic current</italic>, as it is viewed as modeling the synaptic current into a biological neuron
<disp-formula id="ueqn1" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="157289_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives></disp-formula></p><p hwp:id="p-13">We say that the network has learned the association between <italic toggle="yes">P</italic> input patterns <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="157289_inline3.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> and <italic toggle="yes">P</italic> labels <italic toggle="yes">η<sup>μ</sup></italic> if for any pattern <italic toggle="yes">μ</italic>
<disp-formula id="ueqn2" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="157289_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives></disp-formula>
Where <italic toggle="yes">θ</italic> is the threshold, that we further assume to be equal to zero.</p><p hwp:id="p-14">Training the network means finding the set of weights w<sub>i</sub> that satisfies the above expression for all <italic toggle="yes">P</italic> patterns.</p><p hwp:id="p-15">The Hebb-like learning rule, which we use to train the weights {<italic toggle="yes">w<sub>i</sub></italic>} of the classifier is:
<disp-formula id="eqn32" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-8"><graphic xlink:href="157289_eqn3-2.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives></disp-formula></p><p hwp:id="p-16">In the case when patterns are equally likely to belong to either class <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="157289_inline4.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula>, the learning rule simplifies to:
<disp-formula id="ueqn3" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-10"><graphic xlink:href="157289_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives></disp-formula>
</p><p hwp:id="p-17">Here and in all that follows we set the threshold <italic toggle="yes">θ</italic> to zero.</p><p hwp:id="p-18">After training, the synaptic current in response to a test pattern <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="157289_inline5.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> is
<disp-formula id="eqn33" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-12"><graphic xlink:href="157289_eqn3-3.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives></disp-formula></p><p hwp:id="p-19">If <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="157289_inline5d.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> together with its label <italic toggle="yes">η<sup>v</sup></italic> was part of the training set, we can split the sum over patterns into the contribution from the presented pattern <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="157289_inline5a.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> and the contribution from other learned patterns as follows
<disp-formula id="eqn34" hwp:id="disp-formula-7" hwp:rev-id="xref-disp-formula-7-1">
<alternatives hwp:id="alternatives-15"><graphic xlink:href="157289_eqn3-4.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives></disp-formula>
</p><p hwp:id="p-20">Here we used <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="157289_inline6.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> takes value 0 or 1.</p><p hwp:id="p-21">We denote the number of active input units for the pattern <italic toggle="yes">ν</italic> by <italic toggle="yes">n<sup>ν</sup></italic>
<disp-formula id="eqn35" hwp:id="disp-formula-8">
<alternatives hwp:id="alternatives-17"><graphic xlink:href="157289_eqn3-5.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives></disp-formula>
</p><p hwp:id="p-22">The value of <italic toggle="yes">n<sup>ν</sup></italic> is in <italic toggle="yes">binomial distribution</italic> of <italic toggle="yes">N</italic> trials with probability <italic toggle="yes">f</italic>, <bold>B</bold>(<italic toggle="yes">N</italic>, <italic toggle="yes">f</italic>). Its expected value is determined by the number of input units <italic toggle="yes">N</italic> and the coding level <italic toggle="yes">f</italic>
<disp-formula id="eqn36" hwp:id="disp-formula-9">
<alternatives hwp:id="alternatives-18"><graphic xlink:href="157289_eqn3-6.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives></disp-formula>
(here and throughout this text the angular brackets denote the mean over the realizations of the input patterns).</p><p hwp:id="p-23">We replace the sum in the square brackets of (<xref ref-type="disp-formula" rid="eqn34" hwp:id="xref-disp-formula-7-1" hwp:rel-id="disp-formula-7">3.4</xref>) by <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-19"><inline-graphic xlink:href="157289_inline7.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula> where we have introduced a <italic toggle="yes">noise random variable z<sup>ν</sup></italic> with zero mean and unit variance. The coefficient is concluded from the fact that each individual term <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-20"><inline-graphic xlink:href="157289_inline8.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> has variance
<disp-formula id="eqn37" hwp:id="disp-formula-10">
<alternatives hwp:id="alternatives-21"><graphic xlink:href="157289_eqn3-7.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives></disp-formula>
and the fact that the <inline-formula hwp:id="inline-formula-12"><alternatives hwp:id="alternatives-22"><inline-graphic xlink:href="157289_inline2b.gif" hwp:id="inline-graphic-12"/></alternatives></inline-formula> variables are mutually independent. By the central limit theorem the noise variable <italic toggle="yes">z<sup>ν</sup></italic> can be approximated as Gaussian in the limit <italic toggle="yes">P</italic> → ∞ with finite <italic toggle="yes">f</italic> and <italic toggle="yes">n<sup>ν</sup></italic>.</p><p hwp:id="p-24">In terms of <italic toggle="yes">z<sup>ν</sup></italic> and <italic toggle="yes">n<sup>ν</sup></italic> the synaptic current is written as
<disp-formula id="eqn38" hwp:id="disp-formula-11">
<alternatives hwp:id="alternatives-23"><graphic xlink:href="157289_eqn3-8.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives></disp-formula>
</p><p hwp:id="p-25">If a pattern belongs to either class with equal probability <inline-formula hwp:id="inline-formula-13"><alternatives hwp:id="alternatives-24"><inline-graphic xlink:href="157289_inline4a.gif" hwp:id="inline-graphic-13"/></alternatives></inline-formula>, this expression simplifies to
<disp-formula id="eqn39" hwp:id="disp-formula-12" hwp:rev-id="xref-disp-formula-12-1">
<alternatives hwp:id="alternatives-25"><graphic xlink:href="157289_eqn3-9.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives></disp-formula>
</p><p hwp:id="p-26">Note that the first term is the one that reflects the correct classification of the input pattern, and the second one represents the noise caused by the interference from other patterns that were learned by the perceptron. The important parameter is the ratio of the two, which is proportional to <inline-formula hwp:id="inline-formula-14"><alternatives hwp:id="alternatives-26"><inline-graphic xlink:href="157289_inline9.gif" hwp:id="inline-graphic-14"/></alternatives></inline-formula></p></sec></sec><sec id="s3b" hwp:id="sec-7" hwp:rev-id="xref-sec-7-1"><label>3.2</label><title hwp:id="title-8">Committee machine</title><p hwp:id="p-27">We now turn to deriving the classification capacity of a committee machine, the network shown on the <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1b</xref>, where each out of <italic toggle="yes">M</italic> perceptrons receives feedforward connections from <italic toggle="yes">C<sub>F</sub></italic> input units. The connectivity <italic toggle="yes">C<sub>F</sub></italic> does not scale when the number of input units <italic toggle="yes">N</italic> increases.</p><p hwp:id="p-28">The final decision is the majority vote of the classifiers. In other words, if classification is accurate
<disp-formula id="ueqn4" hwp:id="disp-formula-13">
<alternatives hwp:id="alternatives-27"><graphic xlink:href="157289_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-14"/></alternatives></disp-formula>
</p><p hwp:id="p-29">Here <italic toggle="yes">i</italic> ∊ <italic toggle="yes">I<sub>k</sub></italic> stands for all the input units (there are <italic toggle="yes">C<sub>F</sub></italic> of them) that are connected to the readout <italic toggle="yes">k</italic>, and <inline-formula hwp:id="inline-formula-15"><alternatives hwp:id="alternatives-28"><inline-graphic xlink:href="157289_inline10.gif" hwp:id="inline-graphic-15"/></alternatives></inline-formula> is the strength of the connection from the input unit <italic toggle="yes">i</italic> to the readout <italic toggle="yes">k</italic> (for the learning rule we consider <inline-formula hwp:id="inline-formula-16"><alternatives hwp:id="alternatives-29"><inline-graphic xlink:href="157289_inline10a.gif" hwp:id="inline-graphic-16"/></alternatives></inline-formula> does not depend on <italic toggle="yes">k</italic>).
The synaptic current into the readout unit <italic toggle="yes">k</italic> when pattern <inline-formula hwp:id="inline-formula-17"><alternatives hwp:id="alternatives-30"><inline-graphic xlink:href="157289_inline5b.gif" hwp:id="inline-graphic-17"/></alternatives></inline-formula> is presented is determined by
<disp-formula id="eqn310" hwp:id="disp-formula-14" hwp:rev-id="xref-disp-formula-14-1 xref-disp-formula-14-2 xref-disp-formula-14-3 xref-disp-formula-14-4 xref-disp-formula-14-5 xref-disp-formula-14-6 xref-disp-formula-14-7 xref-disp-formula-14-8 xref-disp-formula-14-9 xref-disp-formula-14-10 xref-disp-formula-14-11">
<alternatives hwp:id="alternatives-31"><graphic xlink:href="157289_eqn3-10.gif" position="float" orientation="portrait" hwp:id="graphic-15"/></alternatives></disp-formula>
</p><p hwp:id="p-30">The number of active inputs connected to the perceptron <italic toggle="yes">k</italic>, <inline-formula hwp:id="inline-formula-18"><alternatives hwp:id="alternatives-32"><inline-graphic xlink:href="157289_inline11.gif" hwp:id="inline-graphic-18"/></alternatives></inline-formula> is drawn from the binomial distribution <bold>B</bold>(<italic toggle="yes">C<sub>F</sub></italic>, <italic toggle="yes">f</italic>) of now <italic toggle="yes">C<sub>F</sub></italic> trials with the success rate <italic toggle="yes">f</italic>: and its expectation value is
<disp-formula id="eqn311" hwp:id="disp-formula-15" hwp:rev-id="xref-disp-formula-15-1">
<alternatives hwp:id="alternatives-33"><graphic xlink:href="157289_eqn3-11.gif" position="float" orientation="portrait" hwp:id="graphic-16"/></alternatives></disp-formula></p><p hwp:id="p-31">Since the number of connections per readout <italic toggle="yes">C<sub>F</sub></italic> stays constant as the number of patterns <italic toggle="yes">P</italic> and the size of the network (<italic toggle="yes">N</italic> and <italic toggle="yes">M</italic>) grow, the probability of a single perceptron to classify a pattern correctly approaches the chance level. Indeed, in contrast to the fully connected perceptron, the number of active inputs <inline-formula hwp:id="inline-formula-19"><alternatives hwp:id="alternatives-34"><inline-graphic xlink:href="157289_inline11a.gif" hwp:id="inline-graphic-19"/></alternatives></inline-formula> per readout neuron does not change with the size of the network (see (<xref ref-type="disp-formula" rid="eqn311" hwp:id="xref-disp-formula-15-1" hwp:rel-id="disp-formula-15">3.11</xref>)). Hence, the first term of the expression (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-1" hwp:rel-id="disp-formula-14">3.10</xref>) decreases in the absolute value as the number of patterns <italic toggle="yes">P</italic> grows, while the typical value of the second term stays the same. However, there is always a slight tendency towards the correct answer <inline-formula hwp:id="inline-formula-20"><alternatives hwp:id="alternatives-35"><inline-graphic xlink:href="157289_inline12.gif" hwp:id="inline-graphic-20"/></alternatives></inline-formula>, that can be utilized by having a growing number of sparsely connected classifiers that take a collective decision by majority vote. This scheme is known by the name of committee machine and has been shown to largely exceed the performance of a single classifier.</p><p hwp:id="p-32">It is important to note that in order for the capacity of a committee machine to keep increasing as new classifiers (committee members) are added, the responses of different classifiers should stay sufficiently independent from each other. In the case of limited connectivity, which we consider here, the correlations automatically become smaller and smaller as we increase the number of input units. This happens because the probability of a typical pair of readout neurons to have a common input unit, and thus correlated responses, decreases. In order for the correlations not to be a limiting factor of the classification capacity, we need to increase the number of input units linearly with the number of perceptrons. If one introduces some other mechanism of reducing the correlations between the responses of the classifiers with common input units (like making different perceptrons learn different sets of patterns), a sublinear scaling of the number of input units <italic toggle="yes">N</italic> with the number of perceptrons <italic toggle="yes">M</italic> might be sufficient.</p><sec id="s3b1" hwp:id="sec-8"><label>3.2.1</label><title hwp:id="title-9">Non-overlapping case</title><p hwp:id="p-33">The majority vote of <italic toggle="yes">M</italic> linear threshold classifiers is given by the <italic toggle="yes">average vote</italic>
<disp-formula id="eqn312" hwp:id="disp-formula-16" hwp:rev-id="xref-disp-formula-16-1 xref-disp-formula-16-2">
<alternatives hwp:id="alternatives-36"><graphic xlink:href="157289_eqn3-12.gif" position="float" orientation="portrait" hwp:id="graphic-17"/></alternatives></disp-formula>
where <inline-formula hwp:id="inline-formula-21"><alternatives hwp:id="alternatives-37"><inline-graphic xlink:href="157289_inline13.gif" hwp:id="inline-graphic-21"/></alternatives></inline-formula> is given in (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-2" hwp:rel-id="disp-formula-14">3.10</xref>). Positive <italic toggle="yes">r<sup>ν</sup>η<sup>ν</sup></italic> means that the pattern ν is classified correctly.</p><p hwp:id="p-34">The expectation value of <italic toggle="yes">r<sup>ν</sup></italic> follows from (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-3" hwp:rel-id="disp-formula-14">3.10</xref>) after integrating over the noise variable <inline-formula hwp:id="inline-formula-22"><alternatives hwp:id="alternatives-38"><inline-graphic xlink:href="157289_inline14.gif" hwp:id="inline-graphic-22"/></alternatives></inline-formula>, which is approximated to be normally distributed. We make an assumption <inline-formula hwp:id="inline-formula-23"><alternatives hwp:id="alternatives-39"><inline-graphic xlink:href="157289_inline15.gif" hwp:id="inline-graphic-23"/></alternatives></inline-formula>, which is justified for a large number of patterns, and that allows us to use the approximation of the error function for small arguments to get
<disp-formula id="eqn313" hwp:id="disp-formula-17" hwp:rev-id="xref-disp-formula-17-1 xref-disp-formula-17-2 xref-disp-formula-17-3 xref-disp-formula-17-4 xref-disp-formula-17-5">
<alternatives hwp:id="alternatives-40"><graphic xlink:href="157289_eqn3-13.gif" position="float" orientation="portrait" hwp:id="graphic-18"/></alternatives></disp-formula></p><p hwp:id="p-35">The expectation value <inline-formula hwp:id="inline-formula-24"><alternatives hwp:id="alternatives-41"><inline-graphic xlink:href="157289_inline16.gif" hwp:id="inline-graphic-24"/></alternatives></inline-formula> is computed over the binomial distribution <bold>B</bold>(<italic toggle="yes">C<sub>F</sub></italic>, <italic toggle="yes">f</italic>)
<disp-formula id="eqn314" hwp:id="disp-formula-18">
<alternatives hwp:id="alternatives-42"><graphic xlink:href="157289_eqn3-14.gif" position="float" orientation="portrait" hwp:id="graphic-19"/></alternatives></disp-formula>
</p><p hwp:id="p-36">In the dense regime, <italic toggle="yes">C<sub>F</sub>f</italic> ≫ 1, it can be approximated by
<disp-formula id="eqn315" hwp:id="disp-formula-19">
<alternatives hwp:id="alternatives-43"><graphic xlink:href="157289_eqn3-15.gif" position="float" orientation="portrait" hwp:id="graphic-20"/></alternatives></disp-formula>
and in the extremely sparse case, when <italic toggle="yes">C<sub>F</sub>f</italic> ≪ 1 and only <inline-formula hwp:id="inline-formula-25"><alternatives hwp:id="alternatives-44"><inline-graphic xlink:href="157289_inline11b.gif" hwp:id="inline-graphic-25"/></alternatives></inline-formula> = {0, 1} are encountered substantially often, by
<disp-formula id="eqn316" hwp:id="disp-formula-20" hwp:rev-id="xref-disp-formula-20-1">
<alternatives hwp:id="alternatives-45"><graphic xlink:href="157289_eqn3-16.gif" position="float" orientation="portrait" hwp:id="graphic-21"/></alternatives></disp-formula>
</p><p hwp:id="p-37">To proceed with deriving the classification capacity, let us start with independent classifiers first. The independence of the responses can be achieved either by forcing the connections to be non-overlapping, or by assuming an additional mechanism that, for example, causes different classifiers to update their incoming connections in response to different subsets of the input patterns.</p><p hwp:id="p-38">In this case <italic toggle="yes">r<sup>ν</sup></italic> can be though of as drawn from a Gaussian distribution with the mean given by (<xref ref-type="disp-formula" rid="eqn313" hwp:id="xref-disp-formula-17-1" hwp:rel-id="disp-formula-17">3.13</xref>) and the variance
<disp-formula id="eqn317" hwp:id="disp-formula-21">
<alternatives hwp:id="alternatives-46"><graphic xlink:href="157289_eqn3-17.gif" position="float" orientation="portrait" hwp:id="graphic-22"/></alternatives></disp-formula>
</p><p hwp:id="p-39">The gaussian assumption is justified by the law of large numbers.</p><p hwp:id="p-40">Here and from now on we ignore the contributions of the subleading order, &#x01d4de;(<italic toggle="yes">P</italic><sup>−1</sup>) in this case. The probability <italic toggle="yes">p</italic><sub>Correct</sub> to classify a pattern correctly (<italic toggle="yes">r<sup>ν</sup>η<sup>ν</sup></italic> &gt; 0) can then be easily computed. Fixing <italic toggle="yes">tolerated error rate</italic> ε and requiring <italic toggle="yes">p</italic><sub>correct</sub> &gt; 1 – ε leads to the expression for the maximal number of input patterns that can be classified with the accuracy 1 – ε.
<disp-formula id="eqn318" hwp:id="disp-formula-22">
<alternatives hwp:id="alternatives-47"><graphic xlink:href="157289_eqn3-18.gif" position="float" orientation="portrait" hwp:id="graphic-23"/></alternatives></disp-formula>
</p><p hwp:id="p-41">Here <inline-formula hwp:id="inline-formula-26"><alternatives hwp:id="alternatives-48"><inline-graphic xlink:href="157289_inline17.gif" hwp:id="inline-graphic-26"/></alternatives></inline-formula> denotes the average over binomial distribution, <italic toggle="yes">n</italic> ∼ <bold>B</bold>(<italic toggle="yes">C<sub>F</sub>, f</italic>).</p><p hwp:id="p-42">This result only holds for the case of non-overlapping connections or in the presence of a decorre-lation mechanism. In the following section we generalize it to random connectivity.</p></sec><sec id="s3b2" hwp:id="sec-9" hwp:rev-id="xref-sec-9-1"><label>3.2.2</label><title hwp:id="title-10">Correction to classification capacity due to overlap in the connections</title><p hwp:id="p-43">To derive an analogous expression for the overlapping case without a decorrelation mechanism we need to compute the variance
<disp-formula id="eqn319" hwp:id="disp-formula-23">
<alternatives hwp:id="alternatives-49"><graphic xlink:href="157289_eqn3-19.gif" position="float" orientation="portrait" hwp:id="graphic-24"/></alternatives></disp-formula>
of the average vote <italic toggle="yes">r<sup>ν</sup></italic>, defined by (<xref ref-type="disp-formula" rid="eqn312" hwp:id="xref-disp-formula-16-1" hwp:rel-id="disp-formula-16">3.12</xref>), taking into account the correlations of individual votes <inline-formula hwp:id="inline-formula-27"><alternatives hwp:id="alternatives-50"><inline-graphic xlink:href="157289_inline18.gif" hwp:id="inline-graphic-27"/></alternatives></inline-formula>.</p><p hwp:id="p-44">We start by splitting the covariance into diagonal and non-diagonal contributions:
<disp-formula id="eqn320" hwp:id="disp-formula-24" hwp:rev-id="xref-disp-formula-24-1 xref-disp-formula-24-2">
<alternatives hwp:id="alternatives-51"><graphic xlink:href="157289_eqn3-20.gif" position="float" orientation="portrait" hwp:id="graphic-25"/></alternatives></disp-formula></p><p hwp:id="p-45">We assume that <italic toggle="yes">M</italic> and <italic toggle="yes">N</italic> scale linearly with <italic toggle="yes">P</italic> and <italic toggle="yes">M</italic>, <italic toggle="yes">N</italic>, <italic toggle="yes">P</italic> → ∞. The leading terms are thus of the order <inline-formula hwp:id="inline-formula-28"><alternatives hwp:id="alternatives-52"><inline-graphic xlink:href="157289_inline19.gif" hwp:id="inline-graphic-28"/></alternatives></inline-formula> and we ignore all the subleading contributions.</p><p hwp:id="p-46">When the classifiers <italic toggle="yes">k</italic> and <italic toggle="yes">l</italic> share input units, the correlation between their responses is positive and is closely related to the correlation of the input currents <inline-formula hwp:id="inline-formula-29"><alternatives hwp:id="alternatives-53"><inline-graphic xlink:href="157289_inline20.gif" hwp:id="inline-graphic-29"/></alternatives></inline-formula> (see (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-4" hwp:rel-id="disp-formula-14">3.10</xref>)).</p><p hwp:id="p-47">Let <inline-formula hwp:id="inline-formula-30"><alternatives hwp:id="alternatives-54"><inline-graphic xlink:href="157289_inline21.gif" hwp:id="inline-graphic-30"/></alternatives></inline-formula> be the number of input units that are connected to both the classifier <italic toggle="yes">k</italic> and the classifier <italic toggle="yes">l</italic> and are active in the pattern <inline-formula hwp:id="inline-formula-31"><alternatives hwp:id="alternatives-55"><inline-graphic xlink:href="157289_inline5c.gif" hwp:id="inline-graphic-31"/></alternatives></inline-formula>. For a large number of input units <italic toggle="yes">N</italic> and finite connectivity Cf we can assume that <inline-formula hwp:id="inline-formula-32"><alternatives hwp:id="alternatives-56"><inline-graphic xlink:href="157289_inline21a.gif" hwp:id="inline-graphic-32"/></alternatives></inline-formula> can be either 0 or 1, but not more. The probability of <inline-formula hwp:id="inline-formula-33"><alternatives hwp:id="alternatives-57"><inline-graphic xlink:href="157289_inline21b.gif" hwp:id="inline-graphic-33"/></alternatives></inline-formula> being 1 is given by
<disp-formula id="ueqn5" hwp:id="disp-formula-25">
<alternatives hwp:id="alternatives-58"><graphic xlink:href="157289_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-26"/></alternatives></disp-formula>
</p><p hwp:id="p-48">The number of active units that are connected to only one of the two classifiers are denoted by <inline-formula hwp:id="inline-formula-34"><alternatives hwp:id="alternatives-59"><inline-graphic xlink:href="157289_inline22.gif" hwp:id="inline-graphic-34"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-35"><alternatives hwp:id="alternatives-60"><inline-graphic xlink:href="157289_inline23.gif" hwp:id="inline-graphic-35"/></alternatives></inline-formula> respectively. In the current approximation both of them can be assumed to be distributed according to a binomial distribution <bold>B</bold>(<italic toggle="yes">C<sub>F</sub></italic>, <italic toggle="yes">f</italic>).</p><p hwp:id="p-49">Then, the currents can be written as (see (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-5" hwp:rel-id="disp-formula-14">3.10</xref>)):
<disp-formula id="eqn321" hwp:id="disp-formula-26" hwp:rev-id="xref-disp-formula-26-1">
<alternatives hwp:id="alternatives-61"><graphic xlink:href="157289_eqn3-21.gif" position="float" orientation="portrait" hwp:id="graphic-27"/></alternatives></disp-formula></p><p hwp:id="p-50">Where <inline-formula hwp:id="inline-formula-36"><alternatives hwp:id="alternatives-62"><inline-graphic xlink:href="157289_inline24.gif" hwp:id="inline-graphic-36"/></alternatives></inline-formula> are all independent gaussian variables with zero mean and unit variance.</p><p hwp:id="p-51">To compute the covariance
<disp-formula id="eqn322" hwp:id="disp-formula-27" hwp:rev-id="xref-disp-formula-27-1 xref-disp-formula-27-2">
<alternatives hwp:id="alternatives-63"><graphic xlink:href="157289_eqn3-22.gif" position="float" orientation="portrait" hwp:id="graphic-28"/></alternatives></disp-formula>
we start by integrating over the variables <inline-formula hwp:id="inline-formula-37"><alternatives hwp:id="alternatives-64"><inline-graphic xlink:href="157289_inline25.gif" hwp:id="inline-graphic-37"/></alternatives></inline-formula> to get
<disp-formula id="eqn323" hwp:id="disp-formula-28">
<alternatives hwp:id="alternatives-65"><graphic xlink:href="157289_eqn3-23.gif" position="float" orientation="portrait" hwp:id="graphic-29"/></alternatives></disp-formula>
</p><p hwp:id="p-52">Then, (<xref ref-type="disp-formula" rid="eqn322" hwp:id="xref-disp-formula-27-1" hwp:rel-id="disp-formula-27">3.22</xref>) can be evaluated using the table integral <sup><xref ref-type="fn" rid="fn1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">1</xref></sup>
<disp-formula id="eqn324" hwp:id="disp-formula-29">
<alternatives hwp:id="alternatives-66"><graphic xlink:href="157289_eqn3-24.gif" position="float" orientation="portrait" hwp:id="graphic-30"/></alternatives></disp-formula>
</p><p hwp:id="p-53">In the leading order we get:
<disp-formula id="eqn325" hwp:id="disp-formula-30" hwp:rev-id="xref-disp-formula-30-1 xref-disp-formula-30-2 xref-disp-formula-30-3 xref-disp-formula-30-4 xref-disp-formula-30-5 xref-disp-formula-30-6 xref-disp-formula-30-7 xref-disp-formula-30-8">
<alternatives hwp:id="alternatives-67"><graphic xlink:href="157289_eqn3-25.gif" position="float" orientation="portrait" hwp:id="graphic-31"/></alternatives></disp-formula>
</p><p hwp:id="p-54">In the dense regime (<italic toggle="yes">C<sub>F</sub>f</italic> ≫ 1) the expression for <italic toggle="yes">φC<sub>F,f</sub></italic> in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-1" hwp:rel-id="disp-formula-30">3.25</xref>) can be approximated as
<disp-formula id="eqn326" hwp:id="disp-formula-31" hwp:rev-id="xref-disp-formula-31-1 xref-disp-formula-31-2">
<alternatives hwp:id="alternatives-68"><graphic xlink:href="157289_eqn3-26.gif" position="float" orientation="portrait" hwp:id="graphic-32"/></alternatives></disp-formula>
which leads
<disp-formula id="eqn327" hwp:id="disp-formula-32">
<alternatives hwp:id="alternatives-69"><graphic xlink:href="157289_eqn3-27.gif" position="float" orientation="portrait" hwp:id="graphic-33"/></alternatives></disp-formula></p><p hwp:id="p-55">While in the sparse approximation (<italic toggle="yes">C<sub>F</sub>f</italic> ≪ 1),
<disp-formula id="eqn328" hwp:id="disp-formula-33" hwp:rev-id="xref-disp-formula-33-1 xref-disp-formula-33-2">
<alternatives hwp:id="alternatives-70"><graphic xlink:href="157289_eqn3-28.gif" position="float" orientation="portrait" hwp:id="graphic-34"/></alternatives></disp-formula>
and
<disp-formula id="eqn329" hwp:id="disp-formula-34">
<alternatives hwp:id="alternatives-71"><graphic xlink:href="157289_eqn3-29.gif" position="float" orientation="portrait" hwp:id="graphic-35"/></alternatives></disp-formula></p><p hwp:id="p-56">Plugging this result into (<xref ref-type="disp-formula" rid="eqn320" hwp:id="xref-disp-formula-24-1" hwp:rel-id="disp-formula-24">3.20</xref>), we get for the variance of the majority vote <italic toggle="yes">r<sup>ν</sup></italic> in the overlapping case
<disp-formula id="ueqn5a" hwp:id="disp-formula-35">
<alternatives hwp:id="alternatives-72"><graphic xlink:href="157289_ueqn5a.gif" position="float" orientation="portrait" hwp:id="graphic-36"/></alternatives></disp-formula>
which together with (<xref ref-type="disp-formula" rid="eqn313" hwp:id="xref-disp-formula-17-2" hwp:rel-id="disp-formula-17">3.13</xref>) leads for the maximal number of input patterns that the committee machine can learn to classify with the accuracy 1 – ε
<disp-formula id="eqn330" hwp:id="disp-formula-36" hwp:rev-id="xref-disp-formula-36-1 xref-disp-formula-36-2">
<alternatives hwp:id="alternatives-73"><graphic xlink:href="157289_eqn3-30.gif" position="float" orientation="portrait" hwp:id="graphic-37"/></alternatives></disp-formula>
</p><p hwp:id="p-57">Here <italic toggle="yes">φC<sub>F,f</sub></italic> is given in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-2" hwp:rel-id="disp-formula-30">3.25</xref>) and is approximated by (<xref ref-type="disp-formula" rid="eqn326" hwp:id="xref-disp-formula-31-1" hwp:rel-id="disp-formula-31">3.26</xref>) or (<xref ref-type="disp-formula" rid="eqn328" hwp:id="xref-disp-formula-33-1" hwp:rel-id="disp-formula-33">3.28</xref>).</p><p hwp:id="p-58">If both the number of input units <italic toggle="yes">N</italic> and the number of classifiers <italic toggle="yes">M</italic> increase in proportion to each other, the capacity <italic toggle="yes">P</italic> increases linearly with <italic toggle="yes">N</italic> (or <italic toggle="yes">M</italic>).</p><p hwp:id="p-59">In the case of dense representations, <italic toggle="yes">C<sub>F</sub>f</italic> ≫ 1 the last expression simplifies to
<disp-formula id="eqn331" hwp:id="disp-formula-37">
<alternatives hwp:id="alternatives-74"><graphic xlink:href="157289_eqn3-31.gif" position="float" orientation="portrait" hwp:id="graphic-38"/></alternatives></disp-formula>
and in the ultra-sparse limit, <italic toggle="yes">C<sub>F</sub> f</italic> ≪ 1
<disp-formula id="eqn332" hwp:id="disp-formula-38">
<alternatives hwp:id="alternatives-75"><graphic xlink:href="157289_eqn3-32.gif" position="float" orientation="portrait" hwp:id="graphic-39"/></alternatives></disp-formula>
</p></sec></sec><sec id="s3c" hwp:id="sec-10" hwp:rev-id="xref-sec-10-1"><label>3.3</label><title hwp:id="title-11">Committee machine with recurrent connections</title><p hwp:id="p-60">The majority rule scenario already overcomes the limitations of the connectivity of a single perceptron, but this is not the final answer to constructing a classifier with limited connectivity. The reason is that we still need to implement the majority rule and bring the classification signal to the level of a single unit. The naive way to do it would require another final readout that would have to sample the entire population of <italic toggle="yes">M</italic> intermediate layer perceptrons. Since <italic toggle="yes">M</italic> has to scale linearly with the number of learned patterns <italic toggle="yes">P</italic>, the connectivity of the final readout would also have to scale linearly with <italic toggle="yes">P</italic> (see (<xref ref-type="disp-formula" rid="eqn330" hwp:id="xref-disp-formula-36-1" hwp:rel-id="disp-formula-36">3.30</xref>)) and would exceed any predetermined limit for sufficiently large number of learned patterns.</p><p hwp:id="p-61">To implement the majority vote of the intermediate perceptrons while keeping the connectivity of any unit in the network limited, we introduce the recurrent connectivity in the layer of perceptrons. Our goal is to have two attractor states of the intermediate layer dynamics, that correspond to the two classes. The feedforward input through the connections {<inline-formula hwp:id="inline-formula-38"><alternatives hwp:id="alternatives-76"><inline-graphic xlink:href="157289_inline10b.gif" hwp:id="inline-graphic-38"/></alternatives></inline-formula>} trained in the same way as before, will be slightly biased in the positive direction for one class of the input patterns and in the negative for the other. This slight bias determines which attractor state the network will choose. It is essential that the attractors are far away and do not become closer when the number of learned patterns <italic toggle="yes">P</italic> increases implies that the final readout will be able to discriminate between these states, and thus indicate the class of the presented pattern, even if its connectivity does not scale with <italic toggle="yes">P</italic>. It turns out that for binary classification it is enough to have random recurrent connectivity with sufficiently large but not increasing with <italic toggle="yes">P</italic> number of connections per unit. The weights of these recurrent connection do not have to be tuned (no learning required for recurrent connections).</p><p hwp:id="p-62">We compute the probability of the network of recurrently connected readouts to go to the correct attractor (the one assigned to the class of the input pattern presented) as a function of the number of input units <italic toggle="yes">N</italic>, number of perceptrons <italic toggle="yes">M</italic> and various parameters of the network.</p><sec id="s3c1" hwp:id="sec-11"><label>3.3.1</label><title hwp:id="title-12">Network topology</title><p hwp:id="p-63">The recurrent readout network shown on the right of <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1c</xref> consists of the input layer (green), the intermediate layer of perceptrons (orange) and the final readout unit (purple).</p><p hwp:id="p-64">As before, the <italic toggle="yes">input layer</italic> of <italic toggle="yes">N</italic> neurons is presented with a random and uncorrelated pattern <inline-formula hwp:id="inline-formula-39"><alternatives hwp:id="alternatives-77"><inline-graphic xlink:href="157289_inline26.gif" hwp:id="inline-graphic-39"/></alternatives></inline-formula> from a set of <italic toggle="yes">P</italic> patterns <inline-formula hwp:id="inline-formula-40"><alternatives hwp:id="alternatives-78"><inline-graphic xlink:href="157289_inline27.gif" hwp:id="inline-graphic-40"/></alternatives></inline-formula> that the network has learned to classify.</p><p hwp:id="p-65">The layer of perceptrons we now call <italic toggle="yes">intermediate layer.</italic> It consists of <italic toggle="yes">M</italic> linear threshold readouts, each of which is connected to a randomly chosen <italic toggle="yes">C<sub>F</sub></italic> out of <italic toggle="yes">N</italic> input units. Hence, the <italic toggle="yes">feedforward connectivity C<sub>F</sub></italic> is the number of feedforward inputs that each perceptron receives. The <italic toggle="yes">C<sub>F</sub></italic> is an important parameter in the problem as it determines the classification capacity of a perceptron consid-ered in isolation. The intermediate layer is recurrently connected. For the case of binary classification, the probability that two units are connected is the same for each pair. The recurrent connections are not plastic and can be chosen to be all of equal strength <italic toggle="yes">α</italic>.</p><p hwp:id="p-66">The <italic toggle="yes">final layer</italic> consists of a single readout unit that is connected to a randomly chosen subset of <italic toggle="yes">C</italic> perceptrons in the intermediate layer, with the strength of all connections taken equal.</p><p hwp:id="p-67">The recurrent connectivity matrix <italic toggle="yes">J<sub>kl</sub></italic>, <italic toggle="yes">k, l</italic> ∊ [1… <italic toggle="yes">M</italic>] is assumed to be symmetric
<disp-formula id="eqn333" hwp:id="disp-formula-39">
<alternatives hwp:id="alternatives-79"><graphic xlink:href="157289_eqn3-33.gif" position="float" orientation="portrait" hwp:id="graphic-40"/></alternatives></disp-formula>
</p><p hwp:id="p-68">Let <italic toggle="yes">C<sub>R</sub></italic> be the number of recurrent connections per unit
<disp-formula id="eqn334" hwp:id="disp-formula-40">
<alternatives hwp:id="alternatives-80"><graphic xlink:href="157289_eqn3-34.gif" position="float" orientation="portrait" hwp:id="graphic-41"/></alternatives></disp-formula>
</p><p hwp:id="p-69">We will keep the connectivity parameters <italic toggle="yes">C<sub>F</sub></italic>, <italic toggle="yes">C<sub>R</sub></italic> and <italic toggle="yes">C</italic> and coding level <italic toggle="yes">f</italic> at fixed constant values, while sending the number of input units <italic toggle="yes">N</italic>, the number of intermediate perceptrons <italic toggle="yes">M</italic> and the number of patterns <italic toggle="yes">P</italic> to infinity
<disp-formula id="eqn335" hwp:id="disp-formula-41">
<alternatives hwp:id="alternatives-81"><graphic xlink:href="157289_eqn3-35.gif" position="float" orientation="portrait" hwp:id="graphic-42"/></alternatives></disp-formula>
</p><p hwp:id="p-70">We want to recover the linear scaling of the maximal number of patterns <italic toggle="yes">P</italic><sub>max</sub> that the network can learn to classify with the number of input units <italic toggle="yes">N</italic>, which is known to hold for the fully connected perceptron [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref>].</p></sec><sec id="s3c2" hwp:id="sec-12"><label>3.3.2</label><title hwp:id="title-13">Discrete time dynamical model</title><p hwp:id="p-71">We model the recurrent dynamics as a probabilistic dynamical process in discrete time <italic toggle="yes">t</italic> with the probabilistic transition rule from a network state at time <italic toggle="yes">t</italic> to a network state at time <italic toggle="yes">t</italic> + 1. Let <italic toggle="yes">s<sub>k</sub></italic>(<italic toggle="yes">t</italic>) ∊ [−1, 1] for <italic toggle="yes">k</italic> ∊ [1… <italic toggle="yes">M</italic>] be the dynamical variable describing the state of unit <italic toggle="yes">k</italic> at time <italic toggle="yes">t</italic> in recurrent network.</p><p hwp:id="p-72">Let <inline-formula hwp:id="inline-formula-41"><alternatives hwp:id="alternatives-82"><inline-graphic xlink:href="157289_inline28.gif" hwp:id="inline-graphic-41"/></alternatives></inline-formula> be the total current into the readout unit <italic toggle="yes">k</italic>
<disp-formula id="eqn336" hwp:id="disp-formula-42">
<alternatives hwp:id="alternatives-83"><graphic xlink:href="157289_eqn3-36.gif" position="float" orientation="portrait" hwp:id="graphic-43"/></alternatives></disp-formula>
where the first term corresponds to the recurrent contribution and the second term represents the feedforward current from the input layer (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-6" hwp:rel-id="disp-formula-14">3.10</xref>) that is constant in time.</p><p hwp:id="p-73">The probabilistic transition rule from the state at time <italic toggle="yes">t</italic> to the state at time <italic toggle="yes">t</italic> + 1 is
<disp-formula id="eqn337" hwp:id="disp-formula-43">
<alternatives hwp:id="alternatives-84"><graphic xlink:href="157289_eqn3-37.gif" position="float" orientation="portrait" hwp:id="graphic-44"/></alternatives></disp-formula>
</p><p hwp:id="p-74">Here <italic toggle="yes">β</italic> is the <italic toggle="yes">inverse temperature parameter</italic> for the statistical model of the recurrent dynamics. We approximate this probabilistic recurrent dynamics with the <italic toggle="yes">mean field</italic> method.</p></sec><sec id="s3c3" hwp:id="sec-13" hwp:rev-id="xref-sec-13-1"><label>3.3.3</label><title hwp:id="title-14">Mean field analysis of the recurrent dynamics</title><p hwp:id="p-75">To compute the capacity of such a recurrent classifier, we analyze the recurrent dynamics in the mean field approximation. The activities of the recurrently connected units are represented by the variables <italic toggle="yes">s<sub>k</sub></italic> = {+1, −1} with <italic toggle="yes">k</italic> = 1… <italic toggle="yes">M</italic>. The mean field equation for the <italic toggle="yes">average activation</italic> of the recurrently connected intermediate layer in response to the pattern <italic toggle="yes">ν</italic>,
<disp-formula id="ueqn6" hwp:id="disp-formula-44">
<alternatives hwp:id="alternatives-85"><graphic xlink:href="157289_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-45"/></alternatives></disp-formula>
reads
<disp-formula id="eqn338" hwp:id="disp-formula-45" hwp:rev-id="xref-disp-formula-45-1 xref-disp-formula-45-2 xref-disp-formula-45-3 xref-disp-formula-45-4 xref-disp-formula-45-5 xref-disp-formula-45-6 xref-disp-formula-45-7 xref-disp-formula-45-8 xref-disp-formula-45-9 xref-disp-formula-45-10 xref-disp-formula-45-11">
<alternatives hwp:id="alternatives-86"><graphic xlink:href="157289_eqn3-38.gif" position="float" orientation="portrait" hwp:id="graphic-46"/></alternatives></disp-formula></p><p hwp:id="p-76">The average activation <italic toggle="yes">m<sup>ν</sup></italic> is close to zero if the amount of active and inactive units is approximately the same. If the majority of the units is in the active state, <italic toggle="yes">m<sup>ν</sup></italic> will be close to 1, and if the majority is inactive, <italic toggle="yes">m<sup>ν</sup></italic> will be close to -1.</p><p hwp:id="p-77">Here <italic toggle="yes">C<sub>R</sub></italic> is the average number of connections per unit, <italic toggle="yes">α</italic> is the strength of recurrent synapses (we assume they are all excitatory and of equal strength), <italic toggle="yes">β</italic> is the inverse temperature parameter and <inline-formula hwp:id="inline-formula-42"><alternatives hwp:id="alternatives-87"><inline-graphic xlink:href="157289_inline13a.gif" hwp:id="inline-graphic-42"/></alternatives></inline-formula> is the feedforward input current given by (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-7" hwp:rel-id="disp-formula-14">3.10</xref>).</p><p hwp:id="p-78">We proceed by analyzing the above equation graphically. The plot of the right-hand side is a sigmoid curve and the left-hand side is a line at 45 degrees. The intersections of these two lines determine the solutions to the equation. There are two possible situations that correspond to two different scenarios of the network dynamics.</p><p hwp:id="p-79">The first scenario, shown on the <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2a</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">2b</xref>, is characterized by having only one point of intersection of the line and the sigmoid. In this case there is only one solution to the mean field equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-1" hwp:rel-id="disp-formula-45">3.38</xref>) and only one stable state of the recurrent network. The right hand side of the equation is almost but not quite an odd function of its argument <italic toggle="yes">m<sup>ν</sup></italic>, so the sigmoidal curve representing it is slightly shifted to the left if <inline-formula hwp:id="inline-formula-43"><alternatives hwp:id="alternatives-88"><inline-graphic xlink:href="157289_inline29.gif" hwp:id="inline-graphic-43"/></alternatives></inline-formula> &gt; 0 and to the right if <inline-formula hwp:id="inline-formula-44"><alternatives hwp:id="alternatives-89"><inline-graphic xlink:href="157289_inline29a.gif" hwp:id="inline-graphic-44"/></alternatives></inline-formula> &lt; 0. If the curve is shifted to the left, the single point of its intersection with the strait line passing thorough the origin will be in the right half-plane. So, for the positive input pattern (<italic toggle="yes">η<sup>ν</sup></italic> = + 1 and <inline-formula hwp:id="inline-formula-45"><alternatives hwp:id="alternatives-90"><inline-graphic xlink:href="157289_inline13b.gif" hwp:id="inline-graphic-45"/></alternatives></inline-formula> is more likely to be positive) the mean activity of the intermediate layer in the stable state <italic toggle="yes">m<sup>ν</sup></italic> will usually be positive, while for the negative input patterns it will be negative. Even though there is a relation between the sign of the mean activity of the intermediate layer in the stable state and the class of the input pattern, this is not helpful for our purposes. The reason is that we encounter exactly the same problem as for the case of no recurrent connections: the absolute value of the average activity <italic toggle="yes">m<sup>ν</sup></italic> will decrease with the number of learned patterns <italic toggle="yes">P</italic>, which means that the number of active and inactive units in the intermediate layer will become more and more similar. Consequently, to sample this small imbalance we would require larger and larger connectivity of the final readout. In short, the regime with one stable solution (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2a</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">2b</xref>) is not much different from the case of no recurrent connections. Not surprisingly, this regime corresponds to relatively weak recurrent connections.</p><fig id="fig2" position="float" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12 xref-fig-2-13 xref-fig-2-14 xref-fig-2-15 xref-fig-2-16 xref-fig-2-17 xref-fig-2-18 xref-fig-2-19"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-80">Graphical representation of the mean field equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-2" hwp:rel-id="disp-formula-45">3.38</xref>). The left-hand side of the equation is represented by the line, and the right-hand side - by the sigmoidal curve. The slope of the sigmoidal curve is determined by the amount of dynamical noise in the update rule relative to the strength of the feedforward connections (<italic toggle="yes">βC<sub>R</sub>α</italic>), and the shift relative to <italic toggle="yes">m</italic> = 0 is based on the expected value of the feedforward input <inline-formula hwp:id="inline-formula-46"><alternatives hwp:id="alternatives-91"><inline-graphic xlink:href="157289_inline13f.gif" hwp:id="inline-graphic-46"/></alternatives></inline-formula> <bold>a</bold>. When the input pattern belongs to the “positive” class and the noise is high, there is only one solutions to the equation, which corresponds to small but positive value <italic toggle="yes">m</italic> = <italic toggle="yes">m<sub>s</sub></italic>. This solutions is stable. <bold>b</bold>. For “negative” input pattern, the solution is negative, <italic toggle="yes">m<sub>s</sub></italic> &lt; 0. <bold>c,d</bold>. In the case of low noise, there are three solutions to the mean field equation, with tow extreme solutions <italic toggle="yes">m<sub>s</sub></italic> and −<italic toggle="yes">m<sub>S</sub></italic> being stable, and the middle one <italic toggle="yes">m<sub>u</sub></italic>, which is close to zero, being unstable. For the case of positive input pattern <italic toggle="yes">m<sub>u</sub></italic> &lt; 0, and for the case of negative pattern <italic toggle="yes">m<sub>u</sub></italic> &gt; 0.</p></caption><graphic xlink:href="157289_fig2" position="float" orientation="portrait" hwp:id="graphic-47"/></fig><p hwp:id="p-81">It is the other situation, shown on the <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Figure 2c</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">2d</xref>, that is actually of interest. There are three points of intersection of the sigmoid curve of the right hand side of the equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-3" hwp:rel-id="disp-formula-45">3.38</xref>) and the straight line of the left hand side. The stable states of the network correspond to the rightmost and the leftmost solutions, that are both characterized by a large imbalance between active and inactive units (|<italic toggle="yes">m<sup>ν</sup></italic>| ∼ 1). Most importantly, these solutions are virtually insensitive to the distribution of <inline-formula hwp:id="inline-formula-47"><alternatives hwp:id="alternatives-92"><inline-graphic xlink:href="157289_inline13c.gif" hwp:id="inline-graphic-47"/></alternatives></inline-formula>, and hence to the number of learned patterns <italic toggle="yes">P</italic>. So, if we postulate that the right solution corresponds to the positive input patterns and the left solution to the negative ones, it will be easy for a downstream readout with connectivity that does not increase with <italic toggle="yes">P</italic> to distinguish between them.</p><p hwp:id="p-82">The middle intersection point <italic toggle="yes">m<sub>u</sub></italic> corresponds to the unstable solution. When the network is initialized at the state <inline-formula hwp:id="inline-formula-48"><alternatives hwp:id="alternatives-93"><inline-graphic xlink:href="157289_inline30.gif" hwp:id="inline-graphic-48"/></alternatives></inline-formula> on the left of the unstable solution <italic toggle="yes">m</italic><sub>0</sub> &lt; <italic toggle="yes">m<sub>u</sub></italic>, the recurrent dynamics will most likely evolve to the left stable state, and if initialized at <italic toggle="yes">m</italic><sub>0</sub> &gt; <italic toggle="yes">m<sub>u</sub></italic> it will evolve to the right stable state. As shown on the <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Figure 2c,d</xref> the point of unstable equilibrium will be to the left of the origin for a positive input pattern and to the right of the origin otherwise (due to the difference in the mean of the distributions of <inline-formula hwp:id="inline-formula-49"><alternatives hwp:id="alternatives-94"><inline-graphic xlink:href="157289_inline13d.gif" hwp:id="inline-graphic-49"/></alternatives></inline-formula>). Hence, initiating the network at <italic toggle="yes">m</italic><sub>0</sub> = 0 will serve the purpose of biasing the evolution of the network towards the stable state that corresponds to the class of the input pattern. If the number of learned patterns <italic toggle="yes">P</italic> is large, the point of unstable equilibrium is very close to zero <inline-formula hwp:id="inline-formula-50"><alternatives hwp:id="alternatives-95"><inline-graphic xlink:href="157289_inline31.gif" hwp:id="inline-graphic-50"/></alternatives></inline-formula>, this is the manifestation of the same problem as before, namely the decrease of the signal to noise ratio with the increasing number of learned patterns. Thus, the noise in the initial state of the network <italic toggle="yes">m</italic><sub>0</sub> should also decrease as <inline-formula hwp:id="inline-formula-51"><alternatives hwp:id="alternatives-96"><inline-graphic xlink:href="157289_inline32.gif" hwp:id="inline-graphic-51"/></alternatives></inline-formula>. This is achieved if all the units in the intermediate layer are initialized at <inline-formula hwp:id="inline-formula-52"><alternatives hwp:id="alternatives-97"><inline-graphic xlink:href="157289_inline33.gif" hwp:id="inline-graphic-52"/></alternatives></inline-formula> with equal probabilities independently from each other, and the number of units <italic toggle="yes">M</italic> is linear in <italic toggle="yes">P</italic> (the same scaling as for the committee machine discussed earlier). We use this initialization process to derive the classification capacity and to run the simulations. In the <xref ref-type="sec" rid="s4f" hwp:id="xref-sec-35-1" hwp:rel-id="sec-35">section 4.6</xref> we suggest a biologically plausible way to initialize the network at the desired point.</p><p hwp:id="p-83">To summarize, the information about the class of the input pattern is contained in the feedforward input to the intermediate recurrently connected layer. In the case of a single stable state (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Figure 2a</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-9" hwp:rel-id="F2">2b</xref>), although average activity of the network reflects this information, the signal is very small and a fully connected downstream readout is required. In the case of two stable states (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Figure 2c</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-11" hwp:rel-id="F2">2d</xref>), this small signal biases the network to choose the one corresponding to the class of the input pattern, and by doing so, the network amplifies the feedforward signal making it easy to read out by a sparsely connected downstream unit.</p></sec><sec id="s3c4" hwp:id="sec-14"><label>3.3.4</label><title hwp:id="title-15">Number of classifiable inputs</title><p hwp:id="p-84">As discussed in the previous section, the requirement for the correct classification of an input pattern by means of recurrently connected committee machine is that the average activity of the network at the initial moment <inline-formula hwp:id="inline-formula-53"><alternatives hwp:id="alternatives-98"><inline-graphic xlink:href="157289_inline34.gif" hwp:id="inline-graphic-53"/></alternatives></inline-formula> is on the correct side of the point of unstable equilibrium <inline-formula hwp:id="inline-formula-54"><alternatives hwp:id="alternatives-99"><inline-graphic xlink:href="157289_inline35.gif" hwp:id="inline-graphic-54"/></alternatives></inline-formula>, namely
<disp-formula id="eqn339" hwp:id="disp-formula-46">
<alternatives hwp:id="alternatives-100"><graphic xlink:href="157289_eqn3-39.gif" position="float" orientation="portrait" hwp:id="graphic-48"/></alternatives></disp-formula>
where <italic toggle="yes">η<sup>ν</sup></italic> is the desired output (<italic toggle="yes">η<sup>ν</sup></italic> = {±1}).</p><p hwp:id="p-85">In what follows we drop the pattern index <italic toggle="yes">ν</italic>.</p><p hwp:id="p-86">The statistics of <italic toggle="yes">m</italic><sub>0</sub> over random initializations of the network follows from its definition
<disp-formula id="ueqn7" hwp:id="disp-formula-47">
<alternatives hwp:id="alternatives-101"><graphic xlink:href="157289_ueqn7.gif" position="float" orientation="portrait" hwp:id="graphic-49"/></alternatives></disp-formula>
where each unit is initialized at <italic toggle="yes">s<sub>k</sub></italic> = +1 or <italic toggle="yes">s<sub>k</sub></italic> = −1 with equal probability:
<disp-formula id="ueqn8" hwp:id="disp-formula-48">
<alternatives hwp:id="alternatives-102"><graphic xlink:href="157289_ueqn8.gif" position="float" orientation="portrait" hwp:id="graphic-50"/></alternatives></disp-formula>
</p><p hwp:id="p-87">Since <italic toggle="yes">M</italic> is a large number, we approximate the distribution of <italic toggle="yes">m</italic><sub>0</sub> by a Gaussian distribution with these mean and variance.</p><p hwp:id="p-88">The position of the unstable equilibrium point <italic toggle="yes">m<sub>u</sub></italic>, corresponding to one of the three solutions (the one that is close to zero) of the mean field equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-4" hwp:rel-id="disp-formula-45">3.38</xref>), can not be computed analytically in the general case. However, there are parameter regimes in which we can compute the approximate first and second order statistics of <italic toggle="yes">m<sub>u</sub></italic> over random realizations of the input patterns. These parameter regimes and corresponding approximations are discussed in the following section. Once the mean <italic toggle="yes">μ<sub>u</sub></italic> which depends on the number of learned patterns <italic toggle="yes">P</italic>, and the variance <inline-formula hwp:id="inline-formula-55"><alternatives hwp:id="alternatives-103"><inline-graphic xlink:href="157289_inline36.gif" hwp:id="inline-graphic-55"/></alternatives></inline-formula> of <italic toggle="yes">m<sub>u</sub></italic> are known, the requirement to classify <italic toggle="yes">P</italic> input patterns with accuracy 1 ‒ ε can be written as (assuming the distribution of <italic toggle="yes">m<sub>u</sub></italic> to be also Gaussian)
<disp-formula id="eqn340" hwp:id="disp-formula-49" hwp:rev-id="xref-disp-formula-49-1 xref-disp-formula-49-2 xref-disp-formula-49-3 xref-disp-formula-49-4 xref-disp-formula-49-5">
<alternatives hwp:id="alternatives-104"><graphic xlink:href="157289_eqn3-40.gif" position="float" orientation="portrait" hwp:id="graphic-51"/></alternatives></disp-formula>
</p><p hwp:id="p-89">The expected number <italic toggle="yes">P</italic> of correctly classified patterns can be found by inverting the above equation.
In the following sections we consider different parameter regimes that lead to different approxima-tions for <italic toggle="yes">μ<sub>u</sub></italic> and <italic toggle="yes">σ<sub>u</sub></italic>.</p></sec><sec id="s3c5" hwp:id="sec-15" hwp:rev-id="xref-sec-15-1"><label>3.3.5</label><title hwp:id="title-16">The uniform regime</title><p hwp:id="p-90">In the current study, among other issues we are interested in the consequences of the sparsity of input representations. Since we consider the feedforward connectivity <italic toggle="yes">C<sub>F</sub></italic> to be a constant number and not to scale with the size of the network, for sparse representations there will be a substantial number of perceptrons that receive zero feedforward input. Unless the dynamical noise is very high, these units should be considered separately, and in the mean field approximation an additional order parameter should be introduced to describe their average activity. We call these units <italic toggle="yes">free units</italic>.</p><p hwp:id="p-91">Uniform regime is the parameter regime under which it is not necessary to analyze the free units separately, and the equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-5" hwp:rel-id="disp-formula-45">3.38</xref>) is valid without modifications. Obviously, when the input repre-sentations are dense, <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1, the network of the intermediate layer is in the uniform regime, since there are not enough of free units to make a difference. However, assuming the uniform regime is also valid independently of the number of free units, when the dynamical noise is very large in comparison with the feedforward input (see the next section).</p><p hwp:id="p-92">The conditions defining the uniform regime are:
<disp-formula id="eqn9a" hwp:id="disp-formula-50">
<alternatives hwp:id="alternatives-105"><graphic xlink:href="157289_ueqn9a.gif" position="float" orientation="portrait" hwp:id="graphic-52"/></alternatives></disp-formula>
</p></sec><sec id="s3c6" hwp:id="sec-16"><label>3.3.6</label><title hwp:id="title-17">Uniform regime, high noise</title><p hwp:id="p-93">One approximation we can make to find the unstable solution <italic toggle="yes">m<sub>u</sub></italic> of the mean field equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-6" hwp:rel-id="disp-formula-45">3.38</xref>) is the high noise approximation, which is defined by the requirement
<disp-formula id="eqn341" hwp:id="disp-formula-51" hwp:rev-id="xref-disp-formula-51-1">
<alternatives hwp:id="alternatives-106"><graphic xlink:href="157289_eqn3-41.gif" position="float" orientation="portrait" hwp:id="graphic-53"/></alternatives></disp-formula>
</p><p hwp:id="p-94">It follows from the expression (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-8" hwp:rel-id="disp-formula-14">3.10</xref>) for the feedforward current that this requirement is met if
<disp-formula id="eqn342" hwp:id="disp-formula-52" hwp:rev-id="xref-disp-formula-52-1">
<alternatives hwp:id="alternatives-107"><graphic xlink:href="157289_eqn3-42.gif" position="float" orientation="portrait" hwp:id="graphic-54"/></alternatives></disp-formula>
</p><p hwp:id="p-95">The condition for having three solutions of equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-7" hwp:rel-id="disp-formula-45">3.38</xref>) rather than one (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-12" hwp:rel-id="F2">Figure 2</xref>) is
<disp-formula id="ueqn9" hwp:id="disp-formula-53">
<alternatives hwp:id="alternatives-108"><graphic xlink:href="157289_ueqn9.gif" position="float" orientation="portrait" hwp:id="graphic-55"/></alternatives></disp-formula>
</p><p hwp:id="p-96">Since we are looking for the solution, which is close to zero and (<xref ref-type="disp-formula" rid="eqn341" hwp:id="xref-disp-formula-51-1" hwp:rel-id="disp-formula-51">3.41</xref>) is satisfied for most of the terms, the equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-8" hwp:rel-id="disp-formula-45">3.38</xref>) can be approximated by replacing the hyperbolic tangent by its argument:
<disp-formula id="ueqn10" hwp:id="disp-formula-54">
<alternatives hwp:id="alternatives-109"><graphic xlink:href="157289_ueqn10.gif" position="float" orientation="portrait" hwp:id="graphic-56"/></alternatives></disp-formula>
(note that this approximation is also valid for the terms with <inline-formula hwp:id="inline-formula-56"><alternatives hwp:id="alternatives-110"><inline-graphic xlink:href="157289_inline13g.gif" hwp:id="inline-graphic-56"/></alternatives></inline-formula> = 0).</p><p hwp:id="p-97">Solving this equation leads for the mean <italic toggle="yes">μ<sub>ν</sub></italic> and the standard deviation <italic toggle="yes">σ<sub>u</sub></italic> of <italic toggle="yes">m<sub>u</sub></italic>:
<disp-formula id="ueqn11" hwp:id="disp-formula-55">
<alternatives hwp:id="alternatives-111"><graphic xlink:href="157289_ueqn11.gif" position="float" orientation="portrait" hwp:id="graphic-57"/></alternatives></disp-formula>
and
<disp-formula id="eqn343" hwp:id="disp-formula-56" hwp:rev-id="xref-disp-formula-56-1 xref-disp-formula-56-2">
<alternatives hwp:id="alternatives-112"><graphic xlink:href="157289_eqn3-43.gif" position="float" orientation="portrait" hwp:id="graphic-58"/></alternatives></disp-formula></p><p hwp:id="p-98">The mean <italic toggle="yes">μ<sub>h</sub></italic> and the standard deviation <italic toggle="yes">σ<sub>h</sub></italic> of the feedforward current <inline-formula hwp:id="inline-formula-57"><alternatives hwp:id="alternatives-113"><inline-graphic xlink:href="157289_inline13e.gif" hwp:id="inline-graphic-57"/></alternatives></inline-formula> are computed from (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-9" hwp:rel-id="disp-formula-14">3.10</xref>):
<disp-formula id="eqn344" hwp:id="disp-formula-57" hwp:rev-id="xref-disp-formula-57-1">
<alternatives hwp:id="alternatives-114"><graphic xlink:href="157289_eqn3-44.gif" position="float" orientation="portrait" hwp:id="graphic-59"/></alternatives></disp-formula></p><p hwp:id="p-99">The <italic toggle="yes">C<sub>F</sub>/N</italic> term in (<xref ref-type="disp-formula" rid="eqn343" hwp:id="xref-disp-formula-56-1" hwp:rel-id="disp-formula-56">3.43</xref>) comes from the correlations between the feedforward currents <inline-formula hwp:id="inline-formula-58"><alternatives hwp:id="alternatives-115"><inline-graphic xlink:href="157289_inline13h.gif" hwp:id="inline-graphic-58"/></alternatives></inline-formula> into different readouts <italic toggle="yes">k</italic> due to overlapping connections (see <xref ref-type="app" rid="app6a" hwp:id="xref-sec-37-1" hwp:rel-id="sec-37">Appendix 6.1</xref>).</p><p hwp:id="p-100">Now the maximum number of learned patterns for the classifier in the uniform regime for high noise approximation can be computed from (<xref ref-type="disp-formula" rid="eqn340" hwp:id="xref-disp-formula-49-1" hwp:rel-id="disp-formula-49">3.40</xref>) and is given by
<disp-formula id="eqn345" hwp:id="disp-formula-58">
<alternatives hwp:id="alternatives-116"><graphic xlink:href="157289_eqn3-45.gif" position="float" orientation="portrait" hwp:id="graphic-60"/></alternatives></disp-formula>
</p><p hwp:id="p-101">We note, that because of the applicability conditions (<xref ref-type="disp-formula" rid="eqn342" hwp:id="xref-disp-formula-52-1" hwp:rel-id="disp-formula-52">3.42</xref>) making the last term in the denominator small requires fine tuning of the parameter <italic toggle="yes">β</italic>.</p></sec><sec id="s3c7" hwp:id="sec-17" hwp:rev-id="xref-sec-17-1"><label>3.3.7</label><title hwp:id="title-18">Uniform regime, low noise</title><p hwp:id="p-102">The other approximation in which the equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-9" hwp:rel-id="disp-formula-45">3.38</xref>) can be solved is
<disp-formula id="eqn346" hwp:id="disp-formula-59" hwp:rev-id="xref-disp-formula-59-1">
<alternatives hwp:id="alternatives-117"><graphic xlink:href="157289_eqn3-46.gif" position="float" orientation="portrait" hwp:id="graphic-61"/></alternatives></disp-formula>
which is true if
<disp-formula id="ueqn12" hwp:id="disp-formula-60">
<alternatives hwp:id="alternatives-118"><graphic xlink:href="157289_ueqn12.gif" position="float" orientation="portrait" hwp:id="graphic-62"/></alternatives></disp-formula>
</p><p hwp:id="p-103">Under this condition, assuming the uniform regime is only valid if the input representations are dense
<disp-formula id="ueqn14" hwp:id="disp-formula-61">
<alternatives hwp:id="alternatives-119"><graphic xlink:href="157289_ueqn14.gif" position="float" orientation="portrait" hwp:id="graphic-63"/></alternatives></disp-formula>
</p><p hwp:id="p-104">The condition for having three solutions to the mean field equation in the low noise approximation becomes (see (<xref ref-type="disp-formula" rid="eqn350" hwp:id="xref-disp-formula-66-1" hwp:rel-id="disp-formula-66">3.50</xref>))
<disp-formula id="eqn347" hwp:id="disp-formula-62" hwp:rev-id="xref-disp-formula-62-1 xref-disp-formula-62-2">
<alternatives hwp:id="alternatives-120"><graphic xlink:href="157289_eqn3-47.gif" position="float" orientation="portrait" hwp:id="graphic-64"/></alternatives></disp-formula>
</p><p hwp:id="p-105">In this case the hyperbolic tangent in the equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-10" hwp:rel-id="disp-formula-45">3.38</xref>) can be approximated by the sign function
<disp-formula id="ueqn15" hwp:id="disp-formula-63">
<alternatives hwp:id="alternatives-121"><graphic xlink:href="157289_ueqn15.gif" position="float" orientation="portrait" hwp:id="graphic-65"/></alternatives></disp-formula></p><p hwp:id="p-106">Let us denote the right side of this equation by g(<italic toggle="yes">m<sub>u</sub></italic>), where
<disp-formula id="eqn348" hwp:id="disp-formula-64" hwp:rev-id="xref-disp-formula-64-1">
<alternatives hwp:id="alternatives-122"><graphic xlink:href="157289_eqn3-48.gif" position="float" orientation="portrait" hwp:id="graphic-66"/></alternatives></disp-formula>
is a stochastic function over different realizations of {<inline-formula hwp:id="inline-formula-59"><alternatives hwp:id="alternatives-123"><inline-graphic xlink:href="157289_inline13i.gif" hwp:id="inline-graphic-59"/></alternatives></inline-formula>}.</p><p hwp:id="p-107">Note that in this case, having a substantial fraction of terms with <inline-formula hwp:id="inline-formula-60"><alternatives hwp:id="alternatives-124"><inline-graphic xlink:href="157289_inline13j.gif" hwp:id="inline-graphic-60"/></alternatives></inline-formula> = 0 would lead to a discontinuity of the right hand side at <inline-formula hwp:id="inline-formula-61"><alternatives hwp:id="alternatives-125"><inline-graphic xlink:href="157289_inline35a.gif" hwp:id="inline-graphic-61"/></alternatives></inline-formula> = 0.</p><p hwp:id="p-108">The mean 〈<italic toggle="yes">g</italic>(<italic toggle="yes">m</italic>)〉 can be found by integrating over the distribution of <inline-formula hwp:id="inline-formula-62"><alternatives hwp:id="alternatives-126"><inline-graphic xlink:href="157289_inline13k.gif" hwp:id="inline-graphic-62"/></alternatives></inline-formula> (see (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-10" hwp:rel-id="disp-formula-14">3.10</xref>))
<disp-formula id="eqn349" hwp:id="disp-formula-65">
<alternatives hwp:id="alternatives-127"><graphic xlink:href="157289_eqn3-49.gif" position="float" orientation="portrait" hwp:id="graphic-67"/></alternatives></disp-formula>
</p><p hwp:id="p-109">Where <italic toggle="yes">μ<sub>h</sub></italic> and <italic toggle="yes">σ<sub>h</sub></italic> are the mean and standard deviation of <inline-formula hwp:id="inline-formula-63"><alternatives hwp:id="alternatives-128"><inline-graphic xlink:href="157289_inline13l.gif" hwp:id="inline-graphic-63"/></alternatives></inline-formula> respectively, which are given by (<xref ref-type="disp-formula" rid="eqn344" hwp:id="xref-disp-formula-57-1" hwp:rel-id="disp-formula-57">3.44</xref>).</p><p hwp:id="p-110">Thus, when averaged over training patterns, the mean field equation becomes
<disp-formula id="eqn350" hwp:id="disp-formula-66" hwp:rev-id="xref-disp-formula-66-1">
<alternatives hwp:id="alternatives-129"><graphic xlink:href="157289_eqn3-50.gif" position="float" orientation="portrait" hwp:id="graphic-68"/></alternatives></disp-formula>
and it has three solutions when the derivative of the right-hand side with respect to <italic toggle="yes">m</italic> at <italic toggle="yes">m</italic> = 0 is larger than 1, which for <italic toggle="yes">μ</italic><sub><italic toggle="yes">h</italic></sub> ⋘ <italic toggle="yes">σ<sub>h</sub></italic> immediately leads (<xref ref-type="disp-formula" rid="eqn347" hwp:id="xref-disp-formula-62-1" hwp:rel-id="disp-formula-62">3.47</xref>).</p><p hwp:id="p-111">We now return to estimating the mean and the standard deviation of <italic toggle="yes">m<sub>u</sub></italic>, which is the unstable solution to the approximated mean field equation
<disp-formula id="eqn351" hwp:id="disp-formula-67" hwp:rev-id="xref-disp-formula-67-1">
<alternatives hwp:id="alternatives-130"><graphic xlink:href="157289_eqn3-51.gif" position="float" orientation="portrait" hwp:id="graphic-69"/></alternatives></disp-formula>
where <italic toggle="yes">g</italic>(<italic toggle="yes">m</italic>) is defined by (<xref ref-type="disp-formula" rid="eqn348" hwp:id="xref-disp-formula-64-1" hwp:rel-id="disp-formula-64">3.48</xref>).</p><p hwp:id="p-112">For <italic toggle="yes">μ<sub>h</sub></italic> ⋘ <italic toggle="yes">σ<sub>h</sub></italic>, which is always the case if the number of stored patterns <italic toggle="yes">P</italic> is large enough, we assume that <italic toggle="yes">C<sub>R</sub>αm<sub>u</sub></italic> is also small compared to <italic toggle="yes">σ<sub>h</sub></italic> and check the self-consistency later. Then, we can use the approximation for the error function at small arguments to get
<disp-formula id="eqn352" hwp:id="disp-formula-68">
<alternatives hwp:id="alternatives-131"><graphic xlink:href="157289_eqn3-52.gif" position="float" orientation="portrait" hwp:id="graphic-70"/></alternatives></disp-formula>
 the variance of <italic toggle="yes">g</italic>(<italic toggle="yes">m</italic>) can be written as as sum of the diagonal and the non-diagonal terms
<disp-formula id="eqn353" hwp:id="disp-formula-69">
<alternatives hwp:id="alternatives-132"><graphic xlink:href="157289_eqn3-53.gif" position="float" orientation="portrait" hwp:id="graphic-71"/></alternatives></disp-formula>
which is similar to the expression (<xref ref-type="disp-formula" rid="eqn320" hwp:id="xref-disp-formula-24-2" hwp:rel-id="disp-formula-24">3.20</xref>) for the variance of <inline-formula hwp:id="inline-formula-64"><alternatives hwp:id="alternatives-133"><inline-graphic xlink:href="157289_inline37.gif" hwp:id="inline-graphic-64"/></alternatives></inline-formula> computed previously in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-3" hwp:rel-id="disp-formula-30">3.25</xref>). The only difference is that here the distribution of <italic toggle="yes">h<sub>k</sub></italic> is shifted by <italic toggle="yes">C<sub>R</sub>αm</italic>. However, because the mean 〈<inline-formula hwp:id="inline-formula-65"><alternatives hwp:id="alternatives-134"><inline-graphic xlink:href="157289_inline13m.gif" hwp:id="inline-graphic-65"/></alternatives></inline-formula>〉 did not affect the result (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-4" hwp:rel-id="disp-formula-30">3.25</xref>) and <italic toggle="yes">C<sub>R</sub>αm<sub>u</sub></italic> + <italic toggle="yes">μ<sub>h</sub></italic> is still negligible compared to <italic toggle="yes">σ</italic><sub>h</sub>, we can write
<disp-formula id="eqn354" hwp:id="disp-formula-70">
<alternatives hwp:id="alternatives-135"><graphic xlink:href="157289_eqn3-54.gif" position="float" orientation="portrait" hwp:id="graphic-72"/></alternatives></disp-formula>
where <italic toggle="yes">φC<sub>F</sub>,f</italic> is given in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-5" hwp:rel-id="disp-formula-30">3.25</xref>).</p><p hwp:id="p-113">As a sum of large number <italic toggle="yes">M</italic> of weakly correlated terms, <italic toggle="yes">g</italic>(<italic toggle="yes">m</italic>) can be assumed to be normally distributed and can be written as
<disp-formula id="eqn355" hwp:id="disp-formula-71">
<alternatives hwp:id="alternatives-136"><graphic xlink:href="157289_eqn3-55.gif" position="float" orientation="portrait" hwp:id="graphic-73"/></alternatives></disp-formula>
Where <italic toggle="yes">z<sup>ν</sup></italic>is a Gaussian variable with zero mean and unit variance.</p><p hwp:id="p-114">Plugging the expression for <italic toggle="yes">g</italic>(<italic toggle="yes">m</italic>) into (<xref ref-type="disp-formula" rid="eqn351" hwp:id="xref-disp-formula-67-1" hwp:rel-id="disp-formula-67">3.51</xref>), and solving for <italic toggle="yes">m<sub>u</sub></italic> we get
<disp-formula id="eqn356" hwp:id="disp-formula-72">
<alternatives hwp:id="alternatives-137"><graphic xlink:href="157289_eqn3-56.gif" position="float" orientation="portrait" hwp:id="graphic-74"/></alternatives></disp-formula>
where <italic toggle="yes">φC<sub>F</sub>,f</italic> is given in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-6" hwp:rel-id="disp-formula-30">3.25</xref>).</p><p hwp:id="p-115">So, the expectation value of <italic toggle="yes">m<sub>u</sub></italic> is
<disp-formula id="ueqn16" hwp:id="disp-formula-73">
<alternatives hwp:id="alternatives-138"><graphic xlink:href="157289_ueqn16.gif" position="float" orientation="portrait" hwp:id="graphic-75"/></alternatives></disp-formula>
and the standard deviation is given by:
<disp-formula id="ueqn17" hwp:id="disp-formula-74">
<alternatives hwp:id="alternatives-139"><graphic xlink:href="157289_ueqn17.gif" position="float" orientation="portrait" hwp:id="graphic-76"/></alternatives></disp-formula></p><p hwp:id="p-116">Because uniform regime and low noise implies dense input representation, we can use the dense ap-proximation (<xref ref-type="disp-formula" rid="eqn326" hwp:id="xref-disp-formula-31-2" hwp:rel-id="disp-formula-31">3.26</xref>) for <italic toggle="yes">φC<sub>F</sub>,f</italic>. Plugging these results into (<xref ref-type="disp-formula" rid="eqn340" hwp:id="xref-disp-formula-49-2" hwp:rel-id="disp-formula-49">3.40</xref>) leads the capacity for the uniform regime, low noise
<disp-formula id="ueqn18" hwp:id="disp-formula-75">
<alternatives hwp:id="alternatives-140"><graphic xlink:href="157289_ueqn18.gif" position="float" orientation="portrait" hwp:id="graphic-77"/></alternatives></disp-formula>
</p></sec><sec id="s3c8" hwp:id="sec-18"><label>3.3.8</label><title hwp:id="title-19">Non-uniform regimes</title><p hwp:id="p-117">When the input representation is sparse
<disp-formula id="eqn357" hwp:id="disp-formula-76" hwp:rev-id="xref-disp-formula-76-1 xref-disp-formula-76-2">
<alternatives hwp:id="alternatives-141"><graphic xlink:href="157289_eqn3-57.gif" position="float" orientation="portrait" hwp:id="graphic-78"/></alternatives></disp-formula>
there is a substantial fraction of perceptrons for which all inputs are silent, we call them the free units. If the noise is not very high <inline-formula hwp:id="inline-formula-66"><alternatives hwp:id="alternatives-142"><inline-graphic xlink:href="157289_inline38.gif" hwp:id="inline-graphic-66"/></alternatives></inline-formula> 1, these units are statistically different from those that do receive a non-zero input. To analyze such a system in the mean-field approximation, two order parameters and two coupled mean-field equations should be introduced. To avoid this complication we consider a simpler case, to which we refer to as the <italic toggle="yes">two-subnetworks</italic> regime. This regime is characterized by the recurrent connections that are relatively weak when compared to the feedforward ones, so that the state of those units that do receive non-zero feedforward input is determined by this input only. Neither recurrent input nor noise can flip them. Only the free units participate in the recurrent dynamics and their mean activity in the final state reflects the class of the input pattern. Which of the two stable states the subnetwork of free units will go to is biased by the input from the input receiving units, which do have the information about the class of the input pattern from the feedforward input.</p><p hwp:id="p-118">This approximation is valid if
<disp-formula id="eqn358" hwp:id="disp-formula-77" hwp:rev-id="xref-disp-formula-77-1 xref-disp-formula-77-2">
<alternatives hwp:id="alternatives-143"><graphic xlink:href="157289_eqn3-58.gif" position="float" orientation="portrait" hwp:id="graphic-79"/></alternatives></disp-formula>
<disp-formula id="eqn359" hwp:id="disp-formula-78" hwp:rev-id="xref-disp-formula-78-1 xref-disp-formula-78-2 xref-disp-formula-78-3">
<alternatives hwp:id="alternatives-144"><graphic xlink:href="157289_eqn3-59.gif" position="float" orientation="portrait" hwp:id="graphic-80"/></alternatives></disp-formula>
</p><p hwp:id="p-119">To be more precise, the former condition does not guarantee that the recurrent input will not be able to flip the input receiving units close to the final state, when most of the free units are synchronized. However, if this is the case, their activity already reflects the correct classification of the input pattern, and the input receiving units will flip in the right direction.</p><p hwp:id="p-120">The mean field equation (<xref ref-type="disp-formula" rid="eqn338" hwp:id="xref-disp-formula-45-11" hwp:rel-id="disp-formula-45">3.38</xref>) should now be seen as describing the subnetwork of free units, and should be modified in several ways.</p><p hwp:id="p-121">First, the number of units in the network is
<disp-formula id="eqn360" hwp:id="disp-formula-79" hwp:rev-id="xref-disp-formula-79-1">
<alternatives hwp:id="alternatives-145"><graphic xlink:href="157289_eqn3-60.gif" position="float" orientation="portrait" hwp:id="graphic-81"/></alternatives></disp-formula>
(since for small <italic toggle="yes">f</italic> the probability of all <italic toggle="yes">C<sub>F</sub></italic> independent inputs to be silent is (1 – <italic toggle="yes">f</italic>)<sup><italic toggle="yes">C<sub>F</sub></italic></sup> ≈ e<sup>−<italic toggle="yes">C<sub>F</sub> f</italic></sup>). Second, only <italic toggle="yes">C<sub>R</sub></italic>e<sup>−<italic toggle="yes">C<sub>F</sub>f</italic></sup> out of <italic toggle="yes">C<sub>R</sub></italic> recurrent connections per unit come from other free units. Also, the external input to the network now comes from other (input receiving) units in the intermediate layer, rather than from the input layer.</p><p hwp:id="p-122">The modified mean-field equation reads:
<disp-formula id="eqn361" hwp:id="disp-formula-80" hwp:rev-id="xref-disp-formula-80-1 xref-disp-formula-80-2 xref-disp-formula-80-3 xref-disp-formula-80-4">
<alternatives hwp:id="alternatives-146"><graphic xlink:href="157289_eqn3-61.gif" position="float" orientation="portrait" hwp:id="graphic-82"/></alternatives></disp-formula>
where <inline-formula hwp:id="inline-formula-67"><alternatives hwp:id="alternatives-147"><inline-graphic xlink:href="157289_inline39.gif" hwp:id="inline-graphic-67"/></alternatives></inline-formula> is the average activity of the subnetwork of free units and the index <italic toggle="yes">k</italic> runs over all the free units.</p><p hwp:id="p-123">The external input
<disp-formula id="eqn362" hwp:id="disp-formula-81" hwp:rev-id="xref-disp-formula-81-1">
<alternatives hwp:id="alternatives-148"><graphic xlink:href="157289_eqn3-62.gif" position="float" orientation="portrait" hwp:id="graphic-83"/></alternatives></disp-formula>
where the summation is over the input receiving units and <inline-formula hwp:id="inline-formula-68"><alternatives hwp:id="alternatives-149"><inline-graphic xlink:href="157289_inline13n.gif" hwp:id="inline-graphic-68"/></alternatives></inline-formula> is the feedforward current of (<xref ref-type="disp-formula" rid="eqn310" hwp:id="xref-disp-formula-14-11" hwp:rel-id="disp-formula-14">3.10</xref>) with <inline-formula hwp:id="inline-formula-69"><alternatives hwp:id="alternatives-150"><inline-graphic xlink:href="157289_inline40.gif" hwp:id="inline-graphic-69"/></alternatives></inline-formula>. <italic toggle="yes">M<sub>IR</sub></italic> is the number of input receiving units
<disp-formula id="ueqn19" hwp:id="disp-formula-82">
<alternatives hwp:id="alternatives-151"><graphic xlink:href="157289_ueqn19.gif" position="float" orientation="portrait" hwp:id="graphic-84"/></alternatives></disp-formula>
</p><p hwp:id="p-124">On average, the free unit <italic toggle="yes">k</italic> receives <italic toggle="yes">C<sub>R</sub></italic> inputs, and (1 – <italic toggle="yes">e<sup>−C<sub>F</sub>f</sup></italic>)<italic toggle="yes">C<sub>R</sub></italic> of them come from input receivers. So (<xref ref-type="disp-formula" rid="eqn362" hwp:id="xref-disp-formula-81-1" hwp:rel-id="disp-formula-81">3.62</xref>) will have on average <italic toggle="yes">C<sub>R</sub></italic>(1 – <italic toggle="yes">e<sup>−C<sub>F</sub>f</sup></italic>) non-zero terms. Assuming that this is a large number, <inline-formula hwp:id="inline-formula-70"><alternatives hwp:id="alternatives-152"><inline-graphic xlink:href="157289_inline41.gif" hwp:id="inline-graphic-70"/></alternatives></inline-formula> is a Gaussian variable with the mean given (in the leading order) by
<disp-formula id="eqn363" hwp:id="disp-formula-83">
<alternatives hwp:id="alternatives-153"><graphic xlink:href="157289_eqn3-63.gif" position="float" orientation="portrait" hwp:id="graphic-85"/></alternatives></disp-formula>
which using (<xref ref-type="disp-formula" rid="eqn313" hwp:id="xref-disp-formula-17-3" hwp:rel-id="disp-formula-17">3.13</xref>) becomes
<disp-formula id="eqn364" hwp:id="disp-formula-84" hwp:rev-id="xref-disp-formula-84-1 xref-disp-formula-84-2">
<alternatives hwp:id="alternatives-154"><graphic xlink:href="157289_eqn3-64.gif" position="float" orientation="portrait" hwp:id="graphic-86"/></alternatives></disp-formula>
(assuming 1 – <italic toggle="yes">f</italic> ≈ 1). The number of active inputs <italic toggle="yes">N</italic> connected to the intermediate unit comes from the binomial distribution, <italic toggle="yes">n</italic> ∼ <bold>B</bold>(<italic toggle="yes">N</italic>, <italic toggle="yes">f</italic>).</p><p hwp:id="p-125">The standard deviation of <inline-formula hwp:id="inline-formula-71"><alternatives hwp:id="alternatives-155"><inline-graphic xlink:href="157289_inline41a.gif" hwp:id="inline-graphic-71"/></alternatives></inline-formula> is
<disp-formula id="eqn365" hwp:id="disp-formula-85" hwp:rev-id="xref-disp-formula-85-1 xref-disp-formula-85-2">
<alternatives hwp:id="alternatives-156"><graphic xlink:href="157289_eqn3-65.gif" position="float" orientation="portrait" hwp:id="graphic-87"/></alternatives></disp-formula>
(the corrections due to correlations between different input receiving units are suppressed as 1/&gt;<italic toggle="yes">N</italic> and will become negligible for large networks when <italic toggle="yes">C<sub>R</sub></italic> does not scale with <italic toggle="yes">N</italic>).</p><p hwp:id="p-126">To find the statistics of <italic toggle="yes">m̃<sub>u</sub></italic>, the point of unstable equilibrium, we again consider high and low noise approximations, but now we should compare the inverse temperature parameter <italic toggle="yes">β</italic> to the standard deviation of <inline-formula hwp:id="inline-formula-72"><alternatives hwp:id="alternatives-157"><inline-graphic xlink:href="157289_inline41b.gif" hwp:id="inline-graphic-72"/></alternatives></inline-formula>.</p><p hwp:id="p-127">What we further call <italic toggle="yes">intermediate noise</italic> is the noise which is small on the scale of the feedforward input (<xref ref-type="disp-formula" rid="eqn359" hwp:id="xref-disp-formula-78-1" hwp:rel-id="disp-formula-78">3.59</xref>) but large when compared to the typical values of <inline-formula hwp:id="inline-formula-73"><alternatives hwp:id="alternatives-158"><inline-graphic xlink:href="157289_inline41c.gif" hwp:id="inline-graphic-73"/></alternatives></inline-formula>.</p></sec><sec id="s3c9" hwp:id="sec-19" hwp:rev-id="xref-sec-19-1 xref-sec-19-2"><label>3.3.9</label><title hwp:id="title-20">Two-subnetworks regime, intermediate noise</title><p hwp:id="p-128">The following analysis is valid if in addition to the conditions (<xref ref-type="disp-formula" rid="eqn357" hwp:id="xref-disp-formula-76-1" hwp:rel-id="disp-formula-76">3.57</xref>), (<xref ref-type="disp-formula" rid="eqn358" hwp:id="xref-disp-formula-77-1" hwp:rel-id="disp-formula-77">3.58</xref>) and (<xref ref-type="disp-formula" rid="eqn359" hwp:id="xref-disp-formula-78-2" hwp:rel-id="disp-formula-78">3.59</xref>) the dynamical noise is high in comparison to the typical external input to the subnetwork of free units:
<disp-formula id="ueqn20" hwp:id="disp-formula-86">
<alternatives hwp:id="alternatives-159"><graphic xlink:href="157289_ueqn20.gif" position="float" orientation="portrait" hwp:id="graphic-88"/></alternatives></disp-formula>
</p><p hwp:id="p-129">The condition for three solutions to the mean field equation (<xref ref-type="disp-formula" rid="eqn361" hwp:id="xref-disp-formula-80-1" hwp:rel-id="disp-formula-80">3.61</xref>) in this case is
<disp-formula id="ueqn21" hwp:id="disp-formula-87">
<alternatives hwp:id="alternatives-160"><graphic xlink:href="157289_ueqn21.gif" position="float" orientation="portrait" hwp:id="graphic-89"/></alternatives></disp-formula></p><p hwp:id="p-130">The former inequality allows us to approximate the hyperbolic tangent in (<xref ref-type="disp-formula" rid="eqn361" hwp:id="xref-disp-formula-80-2" hwp:rel-id="disp-formula-80">3.61</xref>) by its argument when looking for the unstable solution <italic toggle="yes">m̃<sub>u</sub></italic>, which is close to zero:
<disp-formula id="eqn366" hwp:id="disp-formula-88">
<alternatives hwp:id="alternatives-161"><graphic xlink:href="157289_eqn3-66.gif" position="float" orientation="portrait" hwp:id="graphic-90"/></alternatives></disp-formula>
</p><p hwp:id="p-131">Each input receiving unit <italic toggle="yes">l</italic> has <italic toggle="yes">C<sub>R</sub></italic> outgoing connections and approximately <italic toggle="yes">e<sup>-C<sub>F</sub>f</sup> C<sub>R</sub></italic> of them terminat on a free unit. Hence, the double sum can be rewritten as
<disp-formula id="eqn367" hwp:id="disp-formula-89">
<alternatives hwp:id="alternatives-162"><graphic xlink:href="157289_eqn3-67.gif" position="float" orientation="portrait" hwp:id="graphic-91"/></alternatives></disp-formula>
</p><p hwp:id="p-132">Solving this equation for <italic toggle="yes">m̃<sub>u</sub></italic> leads (see (<xref ref-type="disp-formula" rid="eqn360" hwp:id="xref-disp-formula-79-1" hwp:rel-id="disp-formula-79">3.60</xref>))
<disp-formula id="eqn368" hwp:id="disp-formula-90" hwp:rev-id="xref-disp-formula-90-1">
<alternatives hwp:id="alternatives-163"><graphic xlink:href="157289_eqn3-68.gif" position="float" orientation="portrait" hwp:id="graphic-92"/></alternatives></disp-formula>
where we have introduced <inline-formula hwp:id="inline-formula-74"><alternatives hwp:id="alternatives-164"><inline-graphic xlink:href="157289_inline42.gif" hwp:id="inline-graphic-74"/></alternatives></inline-formula>: the sign of the feedforward current averaged over the units for which this current is non-zero
<disp-formula id="ueqn22" hwp:id="disp-formula-91">
<alternatives hwp:id="alternatives-165"><graphic xlink:href="157289_ueqn22.gif" position="float" orientation="portrait" hwp:id="graphic-93"/></alternatives></disp-formula></p><p hwp:id="p-133">The statistics of <inline-formula hwp:id="inline-formula-75"><alternatives hwp:id="alternatives-166"><inline-graphic xlink:href="157289_inline42a.gif" hwp:id="inline-graphic-75"/></alternatives></inline-formula> is closely related to previously computed statistics of <italic toggle="yes">r<sup>ν</sup></italic> (see (<xref ref-type="disp-formula" rid="eqn312" hwp:id="xref-disp-formula-16-2" hwp:rel-id="disp-formula-16">3.12</xref>)), which is the sign of the feedforward current averaged over all the intermediate units. Namely,
<disp-formula id="eqn369" hwp:id="disp-formula-92">
<alternatives hwp:id="alternatives-167"><graphic xlink:href="157289_eqn3-69.gif" position="float" orientation="portrait" hwp:id="graphic-94"/></alternatives></disp-formula>
</p><p hwp:id="p-134">The expression for 〈<italic toggle="yes">r<sup>ν</sup></italic>〉 is given in (<xref ref-type="disp-formula" rid="eqn313" hwp:id="xref-disp-formula-17-4" hwp:rel-id="disp-formula-17">3.13</xref>), which leads (we approximate 1 – <italic toggle="yes">f</italic> ≈ 1)
<disp-formula id="eqn370" hwp:id="disp-formula-93" hwp:rev-id="xref-disp-formula-93-1">
<alternatives hwp:id="alternatives-168"><graphic xlink:href="157289_eqn3-70.gif" position="float" orientation="portrait" hwp:id="graphic-95"/></alternatives></disp-formula>
</p><p hwp:id="p-135">To compute the second order statistics of <inline-formula hwp:id="inline-formula-76"><alternatives hwp:id="alternatives-169"><inline-graphic xlink:href="157289_inline42c.gif" hwp:id="inline-graphic-76"/></alternatives></inline-formula>, we use the relation
<disp-formula id="ueqn23" hwp:id="disp-formula-94">
<alternatives hwp:id="alternatives-170"><graphic xlink:href="157289_ueqn23.gif" position="float" orientation="portrait" hwp:id="graphic-96"/></alternatives></disp-formula></p><p hwp:id="p-136">The covariance on the right-hand side was also computed in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-7" hwp:rel-id="disp-formula-30">3.25</xref>), which allows us to write
<disp-formula id="eqn371" hwp:id="disp-formula-95" hwp:rev-id="xref-disp-formula-95-1">
<alternatives hwp:id="alternatives-171"><graphic xlink:href="157289_eqn3-71.gif" position="float" orientation="portrait" hwp:id="graphic-97"/></alternatives></disp-formula>
</p><p hwp:id="p-137">Plugging in (<xref ref-type="disp-formula" rid="eqn370" hwp:id="xref-disp-formula-93-1" hwp:rel-id="disp-formula-93">3.70</xref>) and (<xref ref-type="disp-formula" rid="eqn371" hwp:id="xref-disp-formula-95-1" hwp:rel-id="disp-formula-95">3.71</xref>) to (<xref ref-type="disp-formula" rid="eqn368" hwp:id="xref-disp-formula-90-1" hwp:rel-id="disp-formula-90">3.68</xref>) leads the expressions for the mean and the standard deviation of <inline-formula hwp:id="inline-formula-77"><alternatives hwp:id="alternatives-172"><inline-graphic xlink:href="157289_inline43.gif" hwp:id="inline-graphic-77"/></alternatives></inline-formula>:
<disp-formula id="ueqn24" hwp:id="disp-formula-96">
<alternatives hwp:id="alternatives-173"><graphic xlink:href="157289_ueqn24.gif" position="float" orientation="portrait" hwp:id="graphic-98"/></alternatives></disp-formula>
(the mean <inline-formula hwp:id="inline-formula-78"><alternatives hwp:id="alternatives-174"><inline-graphic xlink:href="157289_inline17a.gif" hwp:id="inline-graphic-78"/></alternatives></inline-formula> is computed assuming a binomial distribution for the number of active inputs <italic toggle="yes">N</italic> connected to a readout <italic toggle="yes">n</italic> ∼ <bold>B</bold>(<italic toggle="yes">N</italic>, <italic toggle="yes">f</italic>)), and
<disp-formula id="eqn372" hwp:id="disp-formula-97">
<alternatives hwp:id="alternatives-175"><graphic xlink:href="157289_eqn3-72.gif" position="float" orientation="portrait" hwp:id="graphic-99"/></alternatives></disp-formula>
</p><p hwp:id="p-138">Now we can use (<xref ref-type="disp-formula" rid="eqn340" hwp:id="xref-disp-formula-49-3" hwp:rel-id="disp-formula-49">3.40</xref>) to compute the maximum number of learned patterns in the two-subnetworks regime under intermediate noise. The number of units in the network <italic toggle="yes">M</italic> in (<xref ref-type="disp-formula" rid="eqn340" hwp:id="xref-disp-formula-49-4" hwp:rel-id="disp-formula-49">3.40</xref>) should be replaced by the number of free units <italic toggle="yes">M</italic>e<sup><italic toggle="yes">−C<sub>F</sub>f</italic></sup>. The result is
<disp-formula id="ueqn25" hwp:id="disp-formula-98">
<alternatives hwp:id="alternatives-176"><graphic xlink:href="157289_ueqn25.gif" position="float" orientation="portrait" hwp:id="graphic-100"/></alternatives></disp-formula>
where
<disp-formula id="ueqn26" hwp:id="disp-formula-99">
<alternatives hwp:id="alternatives-177"><graphic xlink:href="157289_ueqn26.gif" position="float" orientation="portrait" hwp:id="graphic-101"/></alternatives></disp-formula></p><p hwp:id="p-139">It is helpful for analyzing this result to rewrite the expression for <italic toggle="yes">γ</italic> in terms of Δ = e<italic toggle="yes"><sup>−C<sub>F</sub>f</sup> βC<sub>R</sub>α</italic> – 1, which is the measure of how far the current parameters are from the transition to the one solution scenario (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-13" hwp:rel-id="F2">Figure 2a</xref> and <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-14" hwp:rel-id="F2">2b</xref>), at which the current framework breaks down.
<disp-formula id="ueqn27" hwp:id="disp-formula-100">
<alternatives hwp:id="alternatives-178"><graphic xlink:href="157289_ueqn27.gif" position="float" orientation="portrait" hwp:id="graphic-102"/></alternatives></disp-formula></p><p hwp:id="p-140">In the ultra-sparse approximation
<disp-formula id="ueqn28" hwp:id="disp-formula-101">
<alternatives hwp:id="alternatives-179"><graphic xlink:href="157289_ueqn28.gif" position="float" orientation="portrait" hwp:id="graphic-103"/></alternatives></disp-formula>
we can use (<xref ref-type="disp-formula" rid="eqn316" hwp:id="xref-disp-formula-20-1" hwp:rel-id="disp-formula-20">3.16</xref>) and (<xref ref-type="disp-formula" rid="eqn328" hwp:id="xref-disp-formula-33-2" hwp:rel-id="disp-formula-33">3.28</xref>) to get
<disp-formula id="ueqn29" hwp:id="disp-formula-102">
<alternatives hwp:id="alternatives-180"><graphic xlink:href="157289_ueqn29.gif" position="float" orientation="portrait" hwp:id="graphic-104"/></alternatives></disp-formula></p></sec><sec id="s3c10" hwp:id="sec-20"><label>3.3.10</label><title hwp:id="title-21">Two-subnetworks regime, low noise</title><p hwp:id="p-141">We now consider the low noise approximation to the mean field equation for the subnetwork of free units (<xref ref-type="disp-formula" rid="eqn361" hwp:id="xref-disp-formula-80-3" hwp:rel-id="disp-formula-80">3.61</xref>). This approximation is valid when in addition to (<xref ref-type="disp-formula" rid="eqn357" hwp:id="xref-disp-formula-76-2" hwp:rel-id="disp-formula-76">3.57</xref>), (<xref ref-type="disp-formula" rid="eqn358" hwp:id="xref-disp-formula-77-2" hwp:rel-id="disp-formula-77">3.58</xref>) and (<xref ref-type="disp-formula" rid="eqn359" hwp:id="xref-disp-formula-78-3" hwp:rel-id="disp-formula-78">3.59</xref>)
<disp-formula id="eqn373" hwp:id="disp-formula-103" hwp:rev-id="xref-disp-formula-103-1">
<alternatives hwp:id="alternatives-181"><graphic xlink:href="157289_eqn3-73.gif" position="float" orientation="portrait" hwp:id="graphic-105"/></alternatives></disp-formula>
</p><p hwp:id="p-142">In this approximation, the mean field equation has three solutions if
<disp-formula id="ueqn30" hwp:id="disp-formula-104">
<alternatives hwp:id="alternatives-182"><graphic xlink:href="157289_ueqn30.gif" position="float" orientation="portrait" hwp:id="graphic-106"/></alternatives></disp-formula></p><p hwp:id="p-143">This condition is derived analogously to (<xref ref-type="disp-formula" rid="eqn346" hwp:id="xref-disp-formula-59-1" hwp:rel-id="disp-formula-59">3.46</xref>).</p><p hwp:id="p-144">Under the assumption (<xref ref-type="disp-formula" rid="eqn373" hwp:id="xref-disp-formula-103-1" hwp:rel-id="disp-formula-103">3.73</xref>), the mean field equation (<xref ref-type="disp-formula" rid="eqn361" hwp:id="xref-disp-formula-80-4" hwp:rel-id="disp-formula-80">3.61</xref>) can then be approximated as
<disp-formula id="eqn374" hwp:id="disp-formula-105" hwp:rev-id="xref-disp-formula-105-1 xref-disp-formula-105-2">
<alternatives hwp:id="alternatives-183"><graphic xlink:href="157289_eqn3-74.gif" position="float" orientation="portrait" hwp:id="graphic-107"/></alternatives></disp-formula>
</p><p hwp:id="p-145">As in the <xref ref-type="sec" rid="s3c7" hwp:id="xref-sec-17-1" hwp:rel-id="sec-17">section 3.3.7</xref>, let us introduce a stochastic function <italic toggle="yes">g</italic>(<italic toggle="yes">m̃</italic>)
<disp-formula id="ueqn31" hwp:id="disp-formula-106">
<alternatives hwp:id="alternatives-184"><graphic xlink:href="157289_ueqn31.gif" position="float" orientation="portrait" hwp:id="graphic-108"/></alternatives></disp-formula></p><p hwp:id="p-146">For small values of the argument <italic toggle="yes">m̃</italic>, the mean of <italic toggle="yes">g</italic>(<italic toggle="yes">m̃</italic>) over different realizations of <inline-formula hwp:id="inline-formula-79"><alternatives hwp:id="alternatives-185"><inline-graphic xlink:href="157289_inline41d.gif" hwp:id="inline-graphic-79"/></alternatives></inline-formula> is approximated as
<disp-formula id="ueqn32" hwp:id="disp-formula-107">
<alternatives hwp:id="alternatives-186"><graphic xlink:href="157289_ueqn32.gif" position="float" orientation="portrait" hwp:id="graphic-109"/></alternatives></disp-formula>
where <italic toggle="yes">μ<sub>H</sub></italic> and <italic toggle="yes">σ<sub>H</sub></italic> are given by (<xref ref-type="disp-formula" rid="eqn364" hwp:id="xref-disp-formula-84-1" hwp:rel-id="disp-formula-84">3.64</xref>) and (<xref ref-type="disp-formula" rid="eqn365" hwp:id="xref-disp-formula-85-1" hwp:rel-id="disp-formula-85">3.65</xref>).</p><p hwp:id="p-147">To compute the variance of <italic toggle="yes">g</italic>(<italic toggle="yes">m̃</italic>) we need to know
<disp-formula id="ueqn33" hwp:id="disp-formula-108">
<alternatives hwp:id="alternatives-187"><graphic xlink:href="157289_ueqn33.gif" position="float" orientation="portrait" hwp:id="graphic-110"/></alternatives></disp-formula>
which is calculated in the Appendix 6.2, and for large absolute values of the recurrent connectivity (<italic toggle="yes">C<sub>R</sub>e<sup>−C<sub>F</sub>f</sup></italic> ≫ 1) is approximated by
<disp-formula id="eqn375" hwp:id="disp-formula-109" hwp:rev-id="xref-disp-formula-109-1">
<alternatives hwp:id="alternatives-188"><graphic xlink:href="157289_eqn3-75.gif" position="float" orientation="portrait" hwp:id="graphic-111"/></alternatives></disp-formula>
</p><p hwp:id="p-148">Assuming <inline-formula hwp:id="inline-formula-80"><alternatives hwp:id="alternatives-189"><inline-graphic xlink:href="157289_inline41e.gif" hwp:id="inline-graphic-80"/></alternatives></inline-formula> to be Gaussian, we can write
<disp-formula id="eqn376" hwp:id="disp-formula-110" hwp:rev-id="xref-disp-formula-110-1">
<alternatives hwp:id="alternatives-190"><graphic xlink:href="157289_eqn3-76.gif" position="float" orientation="portrait" hwp:id="graphic-112"/></alternatives></disp-formula>
where <italic toggle="yes">z<sup>ν</sup></italic> is a Gaussian variable with zero mean and unit variance.</p><p hwp:id="p-149">The statistics of the unstable, close to zero, solution of (<xref ref-type="disp-formula" rid="eqn374" hwp:id="xref-disp-formula-105-1" hwp:rel-id="disp-formula-105">3.74</xref>) can now be found by plugging in (<xref ref-type="disp-formula" rid="eqn376" hwp:id="xref-disp-formula-110-1" hwp:rel-id="disp-formula-110">3.76</xref>) as the right hand side of (<xref ref-type="disp-formula" rid="eqn374" hwp:id="xref-disp-formula-105-2" hwp:rel-id="disp-formula-105">3.74</xref>), and solving for <italic toggle="yes">m̃<sup>ν</sup></italic>.</p><p hwp:id="p-150">After substituting (<xref ref-type="disp-formula" rid="eqn364" hwp:id="xref-disp-formula-84-2" hwp:rel-id="disp-formula-84">3.64</xref>) and (<xref ref-type="disp-formula" rid="eqn365" hwp:id="xref-disp-formula-85-2" hwp:rel-id="disp-formula-85">3.65</xref>) for <italic toggle="yes">μ<sub>H</sub></italic> and <italic toggle="yes">σ<sub>H</sub></italic>, we get for the mean and the variance of the unstable solution <italic toggle="yes">m̃<sub>u</sub></italic> (assuming <inline-formula hwp:id="inline-formula-81"><alternatives hwp:id="alternatives-191"><inline-graphic xlink:href="157289_inline44.gif" hwp:id="inline-graphic-81"/></alternatives></inline-formula>):
<disp-formula id="ueqn34" hwp:id="disp-formula-111">
<alternatives hwp:id="alternatives-192"><graphic xlink:href="157289_ueqn34.gif" position="float" orientation="portrait" hwp:id="graphic-113"/></alternatives></disp-formula>
</p><p hwp:id="p-151">Using these expressions and (<xref ref-type="disp-formula" rid="eqn340" hwp:id="xref-disp-formula-49-5" hwp:rel-id="disp-formula-49">3.40</xref>) with <italic toggle="yes">M</italic> replaced by the number of free units <italic toggle="yes">M<sub>f</sub></italic> = <italic toggle="yes">Me<sup>−C<sub>F</sub>f</sup></italic>, we get for the maximal number of classifiable inputs in the low noise approximation of the two-subnetworks regime:
<disp-formula id="ueqn34a" hwp:id="disp-formula-112">
<alternatives hwp:id="alternatives-193"><graphic xlink:href="157289_ueqn34a.gif" position="float" orientation="portrait" hwp:id="graphic-114"/></alternatives></disp-formula></p><p hwp:id="p-152">Note, that this is the same expression as (<xref ref-type="disp-formula" rid="eqn330" hwp:id="xref-disp-formula-36-2" hwp:rel-id="disp-formula-36">3.30</xref>) for the majority vote scenario (see Results section for an intuitive explanation).</p><p hwp:id="p-153">For very sparse representations
<disp-formula id="ueqn35" hwp:id="disp-formula-113">
<alternatives hwp:id="alternatives-194"><graphic xlink:href="157289_ueqn35.gif" position="float" orientation="portrait" hwp:id="graphic-115"/></alternatives></disp-formula>
the expression simplifies to
<disp-formula id="ueqn36" hwp:id="disp-formula-114">
<alternatives hwp:id="alternatives-195"><graphic xlink:href="157289_ueqn36.gif" position="float" orientation="portrait" hwp:id="graphic-116"/></alternatives></disp-formula></p></sec></sec></sec><sec id="s4" hwp:id="sec-21"><label>4</label><title hwp:id="title-22">Results</title><sec id="s4a" hwp:id="sec-22"><label>4.1</label><title hwp:id="title-23">The task and the network architecture</title><p hwp:id="p-154">To evaluate the performance of different network architectures we consider a task in which the neural network is trained to associate a specific response to each input. The response is expressed by the activity of one output neuron, which could represent a decision, the expected value of an input stimulus or an action. Each input, for example a sensory stimulus, is a pattern of activity across <italic toggle="yes">N</italic> input neurons. Both, input and output neurons, are either active or inactive and hence the variables representing their activity are binary. Moreover, we assume that the inputs and the outputs are random and uncorrelated. Input neurons are active with probability <italic toggle="yes">f</italic>, whereas the output neuron is active on average for half of the inputs. Performing this task is equivalent to solving a binary classification problem in which each input is assigned to belong to one of two possible classes. As a measure of the performance of the network we introduce the classification capacity, the maximum number of input patterns that can be correctly classified, and determine how it scales with the total number of neurons in the network. We now consider architectures with increasing complexity and we eventually show that it is possible to design a network in which the number of classifiable inputs is large and it scales linearly with the number of neurons while each neuron has limited connectivity (i.e. the number of connections is fixed in the sense that it does not have to scale with the number of neurons).</p><sec id="s4a1" hwp:id="sec-23"><label>4.1.1</label><title hwp:id="title-24">Single readout</title><p hwp:id="p-155">The most basic network that we consider is the one in which the input neurons are directly connected to the output, which is basically the classical perceptron [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">1</xref>] (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1a</xref>). The network is trained by modifying the weights <italic toggle="yes">w<sub>i</sub></italic>. that connect each input neuron <italic toggle="yes">i</italic> to the output. The output activity o<sup><italic toggle="yes">μ</italic></sup> in response to stimulus <italic toggle="yes">μ</italic> is determined by thresholding the weighted sum of the inputs:
<disp-formula id="ueqn37" hwp:id="disp-formula-115">
<alternatives hwp:id="alternatives-196"><graphic xlink:href="157289_ueqn37.gif" position="float" orientation="portrait" hwp:id="graphic-117"/></alternatives></disp-formula>
where <italic toggle="yes">θ</italic> is a threshold and <inline-formula hwp:id="inline-formula-82"><alternatives hwp:id="alternatives-197"><inline-graphic xlink:href="157289_inline2c.gif" hwp:id="inline-graphic-82"/></alternatives></inline-formula> is the activity of neuron <italic toggle="yes">i</italic> when input pattern <italic toggle="yes">μ</italic> is selected. The weights <italic toggle="yes">w</italic>. and the threshold <italic toggle="yes">θ</italic> are learned to impose that <italic toggle="yes">o<sup>μ</sup></italic> = <italic toggle="yes">η<sup>μ</sup></italic>, where <italic toggle="yes">η<sup>μ</sup></italic> is the desired output in response to stimulus <italic toggle="yes">μ</italic>. We know from many studies (see e.g. [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-3" hwp:rel-id="ref-28">28</xref>, <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>]) that the maximum number of random inputs that can be correctly classified scales linearly with the number of input units when <italic toggle="yes">f</italic> = 1/2. This is a very favorable scaling, and actually the optimal one in the benchmark that we consider. Unfortunately, the number of connections of the output neuron is equal to the number of input neurons, and hence when the number of classifiable inputs grows, also the connectivity has to increase accordingly. This is true also in the case of sparse input representations. Indeed, for an arbitrary <italic toggle="yes">f</italic>, when we used a simple learning rule inspired by [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>]
<disp-formula id="eqn41" hwp:id="disp-formula-116" hwp:rev-id="xref-disp-formula-116-1 xref-disp-formula-116-2">
<alternatives hwp:id="alternatives-198"><graphic xlink:href="157289_eqn4-1.gif" position="float" orientation="portrait" hwp:id="graphic-118"/></alternatives></disp-formula>
we obtained in the limit of large number of input neurons <italic toggle="yes">N</italic>, that the maximum number of input patterns that can be classified <italic toggle="yes">P</italic><sub>max</sub> is given by
<disp-formula id="eqn42" hwp:id="disp-formula-117">
<alternatives hwp:id="alternatives-199"><graphic xlink:href="157289_eqn4-2.gif" position="float" orientation="portrait" hwp:id="graphic-119"/></alternatives></disp-formula>
where ε is the maximum tolerated error.</p><p hwp:id="p-156">Notice that the factor containing the coding level of the patterns <italic toggle="yes">f</italic> cannot change the scaling properties of <italic toggle="yes">P</italic>, even in the case in which the inputs become very sparse (i.e. when <italic toggle="yes">f</italic> → 0 as 1/<italic toggle="yes">N</italic>). This seems to be in contradiction with the results of [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">31</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>] in which <italic toggle="yes">P</italic> can scale as <italic toggle="yes">N</italic><sup>2</sup> when the inputs are sparse. However, it is important to remind that the <italic toggle="yes">N</italic><sup>2</sup> scaling can be achieved only when both the input and the output are sparse and in the cases that we analyzed here the output is dense (i.e. active in half of the cases).</p><p hwp:id="p-157">We now consider a different architecture that partially overcome the limitation imposed by the limited connectivty assumption.</p></sec><sec id="s4a2" hwp:id="sec-24"><label>4.1.2</label><title hwp:id="title-25">Committee machines</title><p hwp:id="p-158">Consider now the architecture of <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1b</xref> in which multiple perceptrons are combined together. We assume that each perceptron has limited connectivity, or more precisely, that when the number of input neurons becomes large (mathematically we consider the limit for <italic toggle="yes">N</italic> → ∞), the number of input connections per perceptron, <italic toggle="yes">C<sub>R</sub></italic>, does not increase (i.e. remains finite when <italic toggle="yes">N</italic> → ∞). As a consequence, each perceptron will sample only a small fraction of the input neurons, and for this reason, it will misclassify most of the inputs when <italic toggle="yes">P</italic> becomes large (<italic toggle="yes">P</italic> → ∞). More quantitatively, the fraction of correctly classified inputs will be slightly above chance level (1/2), approximately <inline-formula hwp:id="inline-formula-83"><alternatives hwp:id="alternatives-200"><inline-graphic xlink:href="157289_inline45.gif" hwp:id="inline-graphic-83"/></alternatives></inline-formula> when <italic toggle="yes">P</italic> is large, <italic toggle="yes">a</italic> is a constant.</p><p hwp:id="p-159">In this situation, each perceptron is said to be a weak classifier. However, if the responses of dif-ferent perceptrons are sufficiently independent they can be combined together to perform significantly better than any individual perceptron. Multiple perceptrons combined together make what is called a committee machine. Typically the class of an input is decided by the committee using a majority vote rule: if the majority of perceptrons are active then the output neuron should also be active, otherwise it should be inactive. The majority rule can be easily implemented by summing with equal weights the outputs of all perceptrons.</p><p hwp:id="p-160">As mentioned in <xref ref-type="sec" rid="s2" hwp:id="xref-sec-2-1" hwp:rel-id="sec-2">section 2</xref>, adding new readouts without increasing the number of input units <italic toggle="yes">N</italic> can not increase the classification capacity indefinitely, unless an additional mechanism is introduced to decorrelate the responses of different readouts. Such mechanisms may very well exist in the real brain. For example, one could imagine some local changes of synaptic plasticity during the learning phase, that make different readouts update their connections during presentation of different subsets of patterns. However, in this paper we stick to the simple learning rule ((<xref ref-type="disp-formula" rid="eqn41" hwp:id="xref-disp-formula-116-1" hwp:rel-id="disp-formula-116">4.1</xref>)), and do not consider any decorrelation mechanisms. So, in the present contexts, the only way of increasing the classification capacity of the network without reaching the saturation is to increase the number of input units <italic toggle="yes">N</italic>. Also, in order to satisfy the requirement of limited connectivity, the number of connections converging onto the same readout, <italic toggle="yes">C<sub>F</sub></italic> can not increase with <italic toggle="yes">N</italic>, and we need to add new readouts to connect to the newly added input units. We denote the number of readouts (number of committee members) by <italic toggle="yes">M</italic> and we derive the classification capacity <italic toggle="yes">P</italic><sub>max</sub> under the assumption that <italic toggle="yes">N</italic>, <italic toggle="yes">M</italic> and <italic toggle="yes">P</italic><sub>max</sub><italic toggle="yes">f</italic> are large numbers and the <italic toggle="yes">C<sub>F</sub></italic> connections of every readout are chosen randomly and independently of any other (there will be a random overlap).</p><p hwp:id="p-161">If we use the simple local learning rule (<xref ref-type="disp-formula" rid="eqn41" hwp:id="xref-disp-formula-116-2" hwp:rel-id="disp-formula-116">4.1</xref>), the maximum number of classifiable inputs is:
<disp-formula id="eqn43" hwp:id="disp-formula-118" hwp:rev-id="xref-disp-formula-118-1 xref-disp-formula-118-2">
<alternatives hwp:id="alternatives-201"><graphic xlink:href="157289_eqn4-3.gif" position="float" orientation="portrait" hwp:id="graphic-120"/></alternatives></disp-formula>
where <italic toggle="yes">φC<sub>F</sub>,f</italic> is of the order of <italic toggle="yes">C<sub>F</sub></italic>, and depends on <italic toggle="yes">C<sub>F</sub></italic> and the coding level <italic toggle="yes">f</italic>, but not on <italic toggle="yes">N</italic> or <italic toggle="yes">M</italic>. <inline-formula hwp:id="inline-formula-84"><alternatives hwp:id="alternatives-202"><inline-graphic xlink:href="157289_inline17b.gif" hwp:id="inline-graphic-84"/></alternatives></inline-formula> is the mean of <inline-formula hwp:id="inline-formula-85"><alternatives hwp:id="alternatives-203"><inline-graphic xlink:href="157289_inline46.gif" hwp:id="inline-graphic-85"/></alternatives></inline-formula> over the binomial distribution <bold>B</bold>(<italic toggle="yes">C<sub>F</sub></italic> – 1, <italic toggle="yes">f</italic>), which is approximately <inline-formula hwp:id="inline-formula-86"><alternatives hwp:id="alternatives-204"><inline-graphic xlink:href="157289_inline47.gif" hwp:id="inline-graphic-86"/></alternatives></inline-formula> in the case <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1 (dense regime), and <italic toggle="yes">C<sub>F</sub> f</italic> in the case <italic toggle="yes">C<sub>F</sub> f</italic> ⋘ 1 (ultra-sparse regime). Using also the approximations for <italic toggle="yes">φC<sub>F,f</sub></italic> in these two cases we get (we drop the subscript <sub>max</sub> from now on)
<disp-formula id="eqn44" hwp:id="disp-formula-119" hwp:rev-id="xref-disp-formula-119-1 xref-disp-formula-119-2 xref-disp-formula-119-3 xref-disp-formula-119-4 xref-disp-formula-119-5">
<alternatives hwp:id="alternatives-205"><graphic xlink:href="157289_eqn4-4.gif" position="float" orientation="portrait" hwp:id="graphic-121"/></alternatives></disp-formula>
and
<disp-formula id="eqn45" hwp:id="disp-formula-120" hwp:rev-id="xref-disp-formula-120-1">
<alternatives hwp:id="alternatives-206"><graphic xlink:href="157289_eqn4-5.gif" position="float" orientation="portrait" hwp:id="graphic-122"/></alternatives></disp-formula>
</p><p hwp:id="p-162">So, the dependence of <italic toggle="yes">P</italic> on the coding level <italic toggle="yes">f</italic> is weak, unless <italic toggle="yes">C<sub>F</sub> f</italic> becomes smaller than 1. For sparser representations, the capacity becomes proportional to <italic toggle="yes">C<sub>F</sub> f</italic>. This is not too surprising because when <italic toggle="yes">C<sub>F</sub> f</italic> &lt; 1 a significant proportion of perceptrons will read out only inactive neurons, which are not informative about the input. However, even for very sparse representations the capacity can be restored by increasing the expansion ratio <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic> (see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 4c</xref> and <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-2" hwp:rel-id="F3">4f</xref>).</p><fig id="fig4" position="float" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9 xref-fig-3-10 xref-fig-3-11 xref-fig-3-12 xref-fig-3-13 xref-fig-3-14"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-3"><p hwp:id="p-163"><bold>a</bold> – <bold>c</bold> The dependence of the classification capacity of the recurrent readout on the number of input units <italic toggle="yes">N</italic>, when the number of readouts <italic toggle="yes">M</italic> is increased proportionally to <italic toggle="yes">N</italic>. a High noise regime, see equation (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-1" hwp:rel-id="disp-formula-121">4.6</xref>). Solid curves correspond to sparse input representation, coding level <italic toggle="yes">f</italic> = 0.03 (<italic toggle="yes">C<sub>F</sub> f</italic> = 1.5) and dashed curves - to dense representation, <italic toggle="yes">f</italic> = 0.5. Different colors represent different expansion ratios <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic>. The other parameters the same as in (d). <bold>b</bold>). Same as (a) but for the case of intermediate noise and sparse input (two-subnetworks intermediate noise regime), see formula (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-1" hwp:rel-id="disp-formula-123">4.8</xref>). Solid curves: <italic toggle="yes">f</italic> = 0.004 (<italic toggle="yes">C<sub>F</sub> f</italic> = 0.2), dashed curves: <italic toggle="yes">f</italic> = 0.03. The parameters are the same as for the low <italic toggle="yes">f</italic> segment of (e). <bold>c</bold>) Same for the low dynamical noise. Solid curves correspond to sparse input representation, <italic toggle="yes">f</italic> = 0.03, two-subnetwork low noise regime, see (<xref ref-type="disp-formula" rid="eqn412" hwp:id="xref-disp-formula-128-1" hwp:rel-id="disp-formula-128">4.12</xref>). Dashed curves are for dense representations <italic toggle="yes">f</italic> = 0.4 (<italic toggle="yes">C<sub>F</sub> f</italic> = 20 ≫ 1) in the uniform low noise regime, see (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-1" hwp:rel-id="disp-formula-122">4.7</xref>). The parameters are the same as in (f), the values of a differ for the solid and dashed curves. <bold>d</bold>) The dependence of the rate of the capacity growth on the coding level of input representation <italic toggle="yes">f</italic>. High noise regime, see equation (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-2" hwp:rel-id="disp-formula-121">4.6</xref>). Different curves correspond to different expansion ratios <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic>. The other parameters are <italic toggle="yes">C<sub>F</sub></italic> = 50, <italic toggle="yes">C<sub>R</sub></italic> = 700, a = 0.035, <italic toggle="yes">β</italic> = 0.05 and ε = 0.05 ε) Same for the intermediate noise regime. Different colors correspond to different expansion ratios <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic>, the curves are discontinuous because different formulas are valid for sparse and dense representations. See (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-2" hwp:rel-id="disp-formula-123">4.8</xref>) and (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-2" hwp:rel-id="disp-formula-122">4.7</xref>). The parameters are <italic toggle="yes">C<sub>F</sub></italic> = 50, <italic toggle="yes">C<sub>R</sub></italic> = 700, <italic toggle="yes">α</italic> = 0.000075 for <italic toggle="yes">f</italic> &lt; 0.03 and a = 0.006 for <italic toggle="yes">f</italic> &gt; 0.1, <italic toggle="yes">β</italic> = 100 and ε = 0.05. The dotted curves show the committee machine result. <bold>f</bold>) Same as d) and e) for low noise regime. See equations (<xref ref-type="disp-formula" rid="eqn412" hwp:id="xref-disp-formula-128-2" hwp:rel-id="disp-formula-128">4.12</xref>) and (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-3" hwp:rel-id="disp-formula-122">4.7</xref>). Parameters: <italic toggle="yes">C<sub>F</sub></italic> = 50, <italic toggle="yes">C<sub>R</sub></italic> = 700, α = 0.00075 for <italic toggle="yes">f</italic> &lt; 0.03 and α = 0.006 for <italic toggle="yes">f</italic> &gt; 0.1, <italic toggle="yes">β</italic> = 9000 and ε = 0.05. The dotted curves show the committee machine result.</p></caption><graphic xlink:href="157289_fig4" position="float" orientation="portrait" hwp:id="graphic-123"/></fig><p hwp:id="p-164">When <italic toggle="yes">N</italic> and <italic toggle="yes">M</italic> grow at the same rate, the number of classifiable patterns increases linearly with N, as in the case of the fully connected single perceptron that we previously considered. However, now the connectivity of each perceptron is just <italic toggle="yes">C<sub>F</sub></italic>, which does not scale with <italic toggle="yes">N</italic> or <italic toggle="yes">M</italic>. This means that it is possible to overcome the limitations of sparse connectivity. Unfortunately, this is not a satisfactory solution as it just moves the problem of limited connectivity to the readout output neuron, which now has to count the votes of all <italic toggle="yes">M</italic> perceptrons, and hence needs to be connected to <italic toggle="yes">M</italic> neurons. So again, we will need a number of connections per neuron that grows linearly with <italic toggle="yes">N</italic>. We will now propose an alternative way of implementing a commitee machine, which is based on the use of recurrent connections and it will not require a fully connected output neuron (see <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 3</xref>).
</p><fig id="fig3" position="float" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-4"><p hwp:id="p-165">Summary of the scaling properties of the three architectures considered in the study. A single fully connected readout (classical perceptron) achieves a classification capacity <italic toggle="yes">P</italic> that grows linearly with the number of input neurons <italic toggle="yes">N</italic>, however, the number of feedforward connections that converge onto a single neuron <italic toggle="yes">C<sub>F</sub></italic> also increases linearly with <italic toggle="yes">N</italic>. The Committee machine solves this problem by restricting the feedforward connectivity to a constant <italic toggle="yes">C<sub>F</sub></italic>, and recovering the linear scaling of the capacity <italic toggle="yes">P</italic> by combining the decisions of <italic toggle="yes">M</italic> partially connected perceptrons via a majority vote scheme. The majority vote, however, implies a final readout, whose number of connections <italic toggle="yes">C</italic> is equal to <italic toggle="yes">M</italic>, and thus scales linearly with <italic toggle="yes">N</italic>. The aim of constructing a classifier with limited connectivity is still not met. The suggested recurrent readout architecture achieves the linear growth of the capacity while keeping <italic toggle="yes">C<sub>F</sub></italic>, <italic toggle="yes">C</italic> and the number of recurrent connections per neuron <italic toggle="yes">C<sub>R</sub></italic> constant as <italic toggle="yes">N</italic> increases.</p></caption><graphic xlink:href="157289_fig3" position="float" orientation="portrait" hwp:id="graphic-124"/></fig></sec></sec><sec id="s4b" hwp:id="sec-25" hwp:rev-id="xref-sec-25-1"><label>4.2</label><title hwp:id="title-26">Committee machines with recurrent connections</title><p hwp:id="p-166">One way to count the votes of all perceptrons while respecting the limited connectivity constraint, would be to introduce additional layers of neurons: each neuron in the first layer would count the votes of different <italic toggle="yes">C<sub>F</sub></italic> perceptrons. The neurons in the second layer would then count the votes of <italic toggle="yes">C<sub>F</sub></italic> first layer neurons, and so on. For this architecture, the number of neurons would decrease by a factor <italic toggle="yes">C<sub>F</sub></italic> in every new layer, leading to total number of neurons which would scale as log(<italic toggle="yes">M</italic>) or, equivalently, as log(<italic toggle="yes">N</italic>). It is also possible to set up a multi-layer network with the same number of layers in which every layer contains the same number of neurons <italic toggle="yes">M</italic>. This network would require more neurons, though it would be functionally equivalent to the first one that we considered. An interesting aspect of this architecture is that it can be interpreted as a recurrent network unfolded in time: if one assumes that the network dynamics is discrete in time, then every layer could be seen as the same recurrent network at a different time step. Importantly, the weights of the synaptic connections should be the same for every layer, as it is always the same network but at different time steps. As this network would also be functionally equivalent to the first multi-layer network that we discussed, a recurrent network can in principle replace a complex multi-layer readout which would require significantly more neurons.</p><p hwp:id="p-167">These considerations induced us to study the architecture represented in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Figure 1c</xref>: each perceptron of the committee machine is now connected to a randomly chosen set of the others through recurrent connections, whose weights are all the same and equal to <italic toggle="yes">α</italic>. The number of recurrent connections per perceptron is <italic toggle="yes">C<sub>R</sub></italic>.</p><p hwp:id="p-168">The recurrent dynamics has basically the role of stabilizing only two attractor states of the network: one in which all perceptrons are in the active state, and one in which they are all in the inactive state. These two states represent the two possible responses of the output and correspond to the two classes the input could belong to. The system is equivalent to a spin glass in the ferromagnetic state, or for a more biologically relevant analogy, to the recent decision making network of spiking neurons [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>] in which only the two states corresponding to the possible decisions become stable when a sensory stimulus is presented.</p><p hwp:id="p-169">Once the network has relaxed into one of the two stable states, it becomes easy to determine the class to which the input belongs, as in principle it is sufficient to read out a single perceptron. However, a single neuron readout would not be robust to noise, and hence we will consider the situation in which a number of different perceptrons are read out. We will show that this number remains finite when <italic toggle="yes">N</italic> and <italic toggle="yes">M</italic> become large, which is equivalent to saying that it is possible to construct a network, in which all the neurons, including the output neuron, have limited connectivity and the number of classifiable inputs grows linearly with <italic toggle="yes">N</italic>.</p><p hwp:id="p-170">The number of classifiable inputs is derived analytically in the Methods (<xref ref-type="sec" rid="s3" hwp:id="xref-sec-3-1" hwp:rel-id="sec-3">section 3</xref>) using a mean field approach. This number depends on the parameters that characterize the network architecture (i.e. the number and the connectivity of the different type of neurons), and on the statistics of the inputs that have to be classified. Depending on the assumptions about the parameters, there are different regimes that lead to different analytical expressions.</p><p hwp:id="p-171">There are two distinct regimes that depend on whether all the recurrently connected neurons can be considered statistically equivalent or not. We call <italic toggle="yes">uniform</italic> the regime in which all the neurons can be assumed to be equivalent. This is a reasonable assumption in many situations that we discuss below, but it might not be when the number of neurons that receive no feed-forward input, which can behave differently from the others, is sufficiently large. This number is negligible when Cf <italic toggle="yes">f</italic> ≫ 1. The uniform regime is the first one that we will study systematically. Then we will discuss the non-uniform regime.</p><sec id="s4b1" hwp:id="sec-26"><label>4.2.1</label><title hwp:id="title-27">The uniform regime</title><p hwp:id="p-172">Another factor that determines the parameter regime is the amount of noise that is injected in the neurons. It is important to test the neural system in realistic conditions and to show that it is robust to noise. We introduced noise as in the Hopfield model: the state of each neuron is stochastic and its total synaptic current determines the probability distribution of the states. The noise is characterized by a parameter <italic toggle="yes">β</italic>, which in the language of statistical mechanics would be the inverse temperature parameter. When <italic toggle="yes">β</italic> is large, the noise is small and the neurons are basically deterministic. As <italic toggle="yes">β</italic> goes to zero, the neurons become more noisy and less dependent on the total synaptic input.</p><p hwp:id="p-173">As we know from previous studies on attractor neural networks (see e.g. [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>]), the noise cannot be too large, otherwise the attractor states remain stable only for a short time. More specifically, the noise should be smaller than the recurrent input when the network already settled in one of the two attractors and most of the presynaptic neurons are in the right state. In the uniform regime, this requirement is expressed as <italic toggle="yes">β</italic><sup>−1</sup> &lt; <italic toggle="yes">C<sub>R</sub>α</italic>. Moreover, in order to guarantee attractor stability, the recurrent input should also dominate over the feed-forward one. More formally this condition can be expressed as <italic toggle="yes">A</italic> &lt; <italic toggle="yes">C<sub>R</sub>α</italic>, where <italic toggle="yes">A</italic> is approximately the range in which the feed-forward synaptic input varies when different inputs are presented. It basically determines the selectivity to the inputs in the absence of the recurrent connections (see Methods for more details).</p><p hwp:id="p-174">The relation between <italic toggle="yes">A</italic> and <italic toggle="yes">β</italic> is less constrained: the network architecture that we are discussing can work in different regimes that depend on how large the noise is compared to the typical amplitude of the feed-forward input.</p><p hwp:id="p-175">In the <italic toggle="yes">high noise</italic> regime the noise is so large compared to the feed-forward input (<italic toggle="yes">β</italic><sup>−1</sup> ≫ <italic toggle="yes">A</italic>) that all the different recurrent neurons can behave similarly (uniform regime) even when the feed-forward input is so sparse (<italic toggle="yes">C<sub>F</sub> f</italic> ≲ 1) that many neurons receive zero input. It is important to remind that in this regime the noise is large compared to <italic toggle="yes">A</italic>, but still small compared to the recurrent input. The number of classifiable patterns <italic toggle="yes">P</italic> for the high noise, always uniform regime, is given by
<disp-formula id="eqn46" hwp:id="disp-formula-121" hwp:rev-id="xref-disp-formula-121-1 xref-disp-formula-121-2 xref-disp-formula-121-3 xref-disp-formula-121-4 xref-disp-formula-121-5 xref-disp-formula-121-6 xref-disp-formula-121-7 xref-disp-formula-121-8">
<alternatives hwp:id="alternatives-207"><graphic xlink:href="157289_eqn4-6.gif" position="float" orientation="portrait" hwp:id="graphic-125"/></alternatives></disp-formula>
</p><p hwp:id="p-176">As in the committee machine case, if the number of input units <italic toggle="yes">N</italic> and the number of intermediate readouts <italic toggle="yes">M</italic> are increased in the same proportion, the number of classifiable inputs scales linearly with <italic toggle="yes">M</italic> or <italic toggle="yes">N</italic> (see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 4a</xref>). However, now there is not a single neuron that is required to have a connectivity that scales with <italic toggle="yes">N</italic>, the connectivity of each neuron can remain a finite number even when <italic toggle="yes">N</italic> and <italic toggle="yes">M</italic> become arbitrarily large. <italic toggle="yes">α</italic> is the strength of the recurrent connections and ε is the maximum tolerated error rate.</p><p hwp:id="p-177">The rate at which <italic toggle="yes">P</italic> grows with the number of input neurons <italic toggle="yes">N</italic> (we define <italic toggle="yes">P</italic>/<italic toggle="yes">N</italic> as the capacity of the system) depends on the expansion ratio <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic> (the number of intermediate readouts per input neuron), the coding level <italic toggle="yes">f</italic> and the parameters of the recurrent dynamics,<italic toggle="yes">β</italic> and <italic toggle="yes">α</italic>. The slope of the curves in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 4a</xref>, which represents the capacity, increases with the expansion ratio, but only up to a certain point. For <italic toggle="yes">f</italic> = 0.5 (dense representations) the capacity already saturates at <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic> ≈ 1 (see the dashed lines on <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figure 4a</xref>). It saturates at larger <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic> for sparser representations (f = 0.03). Changing the coding level while keeping the expansion ratio fixed can either increase or decrease the capacity. The dependence of the capacity <italic toggle="yes">P</italic>/<italic toggle="yes">N</italic> on the coding level for different values of the expansion ratio is illustrated in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 4d</xref>.</p><p hwp:id="p-178">When the noise is low compared to both the recurrent and the feedforward input, the density of the input representations starts playing a crucial role in determining whether the network is in a uniform or non-uniform regime. If the input representation is dense <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1, the network is in a uniform regime. As before, all the neurons have the same average activity, but the main source of inhomogeneity is the feedforward input rather than the noise. The number of classifiable inputs in this <italic toggle="yes">uniform low noise</italic> regime is:
<disp-formula id="eqn47" hwp:id="disp-formula-122" hwp:rev-id="xref-disp-formula-122-1 xref-disp-formula-122-2 xref-disp-formula-122-3 xref-disp-formula-122-4 xref-disp-formula-122-5 xref-disp-formula-122-6 xref-disp-formula-122-7 xref-disp-formula-122-8">
<alternatives hwp:id="alternatives-208"><graphic xlink:href="157289_eqn4-7.gif" position="float" orientation="portrait" hwp:id="graphic-126"/></alternatives></disp-formula>
</p><p hwp:id="p-179">This formula is similar to one for the high noise regime. One obvious difference is that the inverse temperature parameter <italic toggle="yes">β</italic> does not appear because we assumed to be in the low noise limit <italic toggle="yes">β</italic> → to. The dependence of the capacity on the expansion ratio <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic> is similar to one for the high noise regime. The dependence of capacity <italic toggle="yes">P</italic>/<italic toggle="yes">N</italic> on the coding level is summarized by the parts of the plots in which <italic toggle="yes">f</italic> is large in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 4e,f</xref> (right-hand side segments of the solid curves).</p><p hwp:id="p-180">When compared to the dense limit (<italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1) of the majority vote result (<xref ref-type="disp-formula" rid="eqn44" hwp:id="xref-disp-formula-119-1" hwp:rel-id="disp-formula-119">4.4</xref>), the low noise regime formula (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-4" hwp:rel-id="disp-formula-122">4.7</xref>) entails a smaller capacity. The difference comes from the last term at the denominator, which reduces the capacity. This term can be made small by tuning the parameters of the recurrent dynamics <italic toggle="yes">C<sub>R</sub></italic> or <italic toggle="yes">α</italic>, but it cannot become zero, because this would correspond to the case in which the recurrent dynamics has only one stable state, which is not suitable for performing a classification task.</p></sec><sec id="s4b2" hwp:id="sec-27" hwp:rev-id="xref-sec-27-1 xref-sec-27-2"><label>4.2.2</label><title hwp:id="title-28">Non-uniform regimes</title><p hwp:id="p-181">When the noise is small compared to the feed-forward input and the representations are sparse, the uniform approximation is not valid and the recurrent network behaves in a qualitatively different way: for each input pattern, there would be two distinct populations of neurons: the <italic toggle="yes">free neurons</italic>, which receive zero feed-forward input, and hence are not constrained (free) by the input, and all the others, the <italic toggle="yes">input-receivers.</italic> The two populations would be different for different inputs, they would have different activity distributions and would evolve in time differently, although they constantly interact with each other.</p><p hwp:id="p-182">Generally such a regime is intractable with the mean field method, so we need to make the additional assumption that the feed-forward synapses are sufficiently strong relative to the recurrent ones, so that, the non-zero feed-forward inputs are typically larger than the total recurrent inputs in the initial state (before the network reaches the final state when most of the neurons have the same activity). Furthermore, we need to assume that these feed-forward inputs are also much larger than the noise. Under all these assumptions, the state of the input-receivers is determined by the feed-forward input, at least in the initial stages of the dynamics, while the network is deciding which stable state to choose. We then need only to consider the dynamics of the sub-network of free units, treating the recurrent input from the input receivers as a fixed external input. It is this input, that contains the information about the correct classification.</p><p hwp:id="p-183">We refer to the described scenario as to the <italic toggle="yes">two-subnetworks</italic> regime. The classification capacity in two-subnetworks scenario depends also on the noise. The noise has to be small in comparison to the feed-forward input, but it can be either small or large when compared to the amplitude of the recurrent input coming from the input-receivers. This comparison distinguishes between the <italic toggle="yes">two-subnetwork low noise</italic> and the <italic toggle="yes">two-subnetwork intermediate noise</italic> regimes.</p><p hwp:id="p-184">Two-subnetwork intermediate noise regime is realized when the representations are sparse (<italic toggle="yes">C<sub>F</sub> f</italic> ≲ 1) and the noise is small relative to the feed-forward input but large in the subnetwork of free neurons, namely relative to the input into free neurons from the input-receivers. This regime leads to the classification capacity of
<disp-formula id="eqn48" hwp:id="disp-formula-123" hwp:rev-id="xref-disp-formula-123-1 xref-disp-formula-123-2 xref-disp-formula-123-3 xref-disp-formula-123-4 xref-disp-formula-123-5 xref-disp-formula-123-6 xref-disp-formula-123-7">
<alternatives hwp:id="alternatives-209"><graphic xlink:href="157289_eqn4-8.gif" position="float" orientation="portrait" hwp:id="graphic-127"/></alternatives></disp-formula>
where <italic toggle="yes">φC<sub>F,f</sub> M</italic>/<italic toggle="yes">N</italic> comes from the correlations between the input-receivers, <italic toggle="yes">φC<sub>F,f</sub></italic> is of the order of <italic toggle="yes">C<sub>F</sub></italic>, and depends on <italic toggle="yes">C<sub>F</sub></italic> and on the coding level <italic toggle="yes">f</italic>. <inline-formula hwp:id="inline-formula-87"><alternatives hwp:id="alternatives-210"><inline-graphic xlink:href="157289_inline17c.gif" hwp:id="inline-graphic-87"/></alternatives></inline-formula> is the mean of <inline-formula hwp:id="inline-formula-88"><alternatives hwp:id="alternatives-211"><inline-graphic xlink:href="157289_inline46a.gif" hwp:id="inline-graphic-88"/></alternatives></inline-formula> over the binomial distribution <bold>B</bold>(<italic toggle="yes">C<sub>F</sub></italic> – 1, <italic toggle="yes">f</italic>) and <italic toggle="yes">γ</italic> is a quantity given by:
<disp-formula id="eqn49" hwp:id="disp-formula-124" hwp:rev-id="xref-disp-formula-124-1 xref-disp-formula-124-2 xref-disp-formula-124-3 xref-disp-formula-124-4">
<alternatives hwp:id="alternatives-212"><graphic xlink:href="157289_eqn4-9.gif" position="float" orientation="portrait" hwp:id="graphic-128"/></alternatives></disp-formula>
which is the smallest (highest capacity) when the network is close to transitioning from three fixed points (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-15" hwp:rel-id="F2">Figure 2c,d</xref>) to one fixed point (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-16" hwp:rel-id="F2">Figure 2a,b</xref>).</p><p hwp:id="p-185">In the ultra-sparse limit, <italic toggle="yes">C<sub>F</sub> f</italic> ⋘ 1 the expression for <italic toggle="yes">P</italic> becomes
<disp-formula id="eqn410" hwp:id="disp-formula-125" hwp:rev-id="xref-disp-formula-125-1 xref-disp-formula-125-2 xref-disp-formula-125-3">
<alternatives hwp:id="alternatives-213"><graphic xlink:href="157289_eqn4-10.gif" position="float" orientation="portrait" hwp:id="graphic-129"/></alternatives></disp-formula>
</p><p hwp:id="p-186"><xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Figure 4b</xref> shows the linear dependence of <italic toggle="yes">P</italic> on the number of input neurons <italic toggle="yes">N</italic> for different expan-sion ratios. We can see that unless the expansion ratio is very high, even for very sparse representations (<italic toggle="yes">f</italic> = 0.004, <italic toggle="yes">C<sub>F</sub> f</italic> = 0.2) the capacity grows at a similar rate or even faster compared to the case of dense representations (<italic toggle="yes">f</italic> = 0.5, <italic toggle="yes">C<sub>F</sub> f</italic> = 25) in the high noise regime. The dependence of the capacity on the coding level is summarized in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Figure 4e</xref>, where the curves in the low <italic toggle="yes">f</italic> region correspond to the two-subnetworks intermediate noise regime (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-3" hwp:rel-id="disp-formula-123">4.8</xref>), and the segments at high values of <italic toggle="yes">f</italic> - to the uniform low noise regime (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-5" hwp:rel-id="disp-formula-122">4.7</xref>). Apart from the coding level, the only parameter that differs between the two discontinuous parts of the plot for a given expansion ratio is the strength of the recurrent connections <italic toggle="yes">α</italic>. We decided to choose different as for two parts because keeping all the parameters the same while satisfying all the conditions for the two-subnetworks intermediate noise regime at low <italic toggle="yes">f</italic> and uniform low noise regime at high <italic toggle="yes">f</italic> would have required unrealistically high values of the recurrent connectivity <italic toggle="yes">C<sub>R</sub></italic>. The dotted lines on the plot represent the results for committee machine with same expansion ratios.</p><p hwp:id="p-187">The minimal possible value of <italic toggle="yes">γ</italic> in (<xref ref-type="disp-formula" rid="eqn49" hwp:id="xref-disp-formula-124-1" hwp:rel-id="disp-formula-124">4.9</xref>) is 1 – e<sup>−<italic toggle="yes">C<sub>F</sub> f</italic></sup> To see this we rewrite the expression (<xref ref-type="disp-formula" rid="eqn49" hwp:id="xref-disp-formula-124-2" hwp:rel-id="disp-formula-124">4.9</xref>) as
<disp-formula id="eqn411" hwp:id="disp-formula-126" hwp:rev-id="xref-disp-formula-126-1">
<alternatives hwp:id="alternatives-214"><graphic xlink:href="157289_eqn4-11.gif" position="float" orientation="portrait" hwp:id="graphic-130"/></alternatives></disp-formula>
where
<disp-formula id="ueqn38" hwp:id="disp-formula-127">
<alternatives hwp:id="alternatives-215"><graphic xlink:href="157289_ueqn38.gif" position="float" orientation="portrait" hwp:id="graphic-131"/></alternatives></disp-formula>
Δ must be positive in order to have three fixed points (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-17" hwp:rel-id="F2">Figure 2c,d</xref>). This implies that the expression in the paranthesis of (<xref ref-type="disp-formula" rid="eqn411" hwp:id="xref-disp-formula-126-1" hwp:rel-id="disp-formula-126">4.11</xref>) is positive and less than 1, from where it follows that 1 – <italic toggle="yes">e<sup>−C<sub>F</sub>f</sup></italic> &lt; <italic toggle="yes">γ</italic> &lt; 1.</p><p hwp:id="p-188">The lower bound for <italic toggle="yes">γ</italic> is approximately equal to <italic toggle="yes">C<sub>F</sub></italic>f in the ultra-sparse limit. Plugging this value into (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-4" hwp:rel-id="disp-formula-123">4.8</xref>) leads to a capacity that is basically independent from <italic toggle="yes">f</italic>. This would mean that one can decrease the coding level way below 1/<italic toggle="yes">C<sub>F</sub></italic> without sacrificing the classification performance. However, keeping <italic toggle="yes">γ</italic> of the order of Cff requires having Δ of the order of <inline-formula hwp:id="inline-formula-89"><alternatives hwp:id="alternatives-216"><inline-graphic xlink:href="157289_inline47a.gif" hwp:id="inline-graphic-89"/></alternatives></inline-formula> or smaller, which entails a progressively finer adjustment of the inverse temperature parameter <italic toggle="yes">β</italic> as <italic toggle="yes">f</italic> decreases. In order to have a capacity that does not become infinitesimal when <italic toggle="yes">f</italic> goes down to <italic toggle="yes">f<sub>min</sub></italic>, <italic toggle="yes">β</italic> should be adjusted with the a maximum error <inline-formula hwp:id="inline-formula-90"><alternatives hwp:id="alternatives-217"><inline-graphic xlink:href="157289_inline48.gif" hwp:id="inline-graphic-90"/></alternatives></inline-formula>.</p><p hwp:id="p-189"><xref ref-type="fig" rid="fig6" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 6</xref> (dashed lines) shows the capacity <italic toggle="yes">P</italic>/<italic toggle="yes">N</italic> as a function of <italic toggle="yes">C<sub>F</sub> f</italic> for a different number of feed-forward connections per input unit <italic toggle="yes">C</italic> = <italic toggle="yes">MC<sub>F</sub></italic>/<italic toggle="yes">N</italic>. Along these curves, Δ is kept at a fixed value Δ = 0.2 by choosing a new <italic toggle="yes">β</italic> for every value of <italic toggle="yes">f</italic>. Note that the curves are almost flat as long as <italic toggle="yes">C<sub>F</sub> f</italic> &gt; <italic toggle="yes">C<sub>F</sub>f<sub>min</sub></italic> where <italic toggle="yes">C<sub>F</sub>f<sub>min</sub></italic> ≈ Δ<sup>2</sup> = 0.04</p><fig id="fig6" position="float" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-5"><p hwp:id="p-190">The coefficient <italic toggle="yes">P</italic>/<italic toggle="yes">N</italic> as a function of sparsity of the input representation, expressed as <italic toggle="yes">C<sub>F</sub> f</italic>, for different values of the number of outgoing connections per input neuron, <italic toggle="yes">c</italic> = <italic toggle="yes">C<sub>F</sub> M</italic>/<italic toggle="yes">N</italic>, in the sparse limit of two-subnetwork low and intermediate noise regimes, (<xref ref-type="disp-formula" rid="eqn413" hwp:id="xref-disp-formula-129-1" hwp:rel-id="disp-formula-129">4.13</xref>) and (<xref ref-type="disp-formula" rid="eqn410" hwp:id="xref-disp-formula-125-1" hwp:rel-id="disp-formula-125">4.10</xref>). For panel <bold>a.</bold> <italic toggle="yes">c</italic> =1, for panel <bold>b.</bold> <italic toggle="yes">c</italic> =2 and for panel <bold>c.</bold> <italic toggle="yes">c</italic> = 20. Solid curves correspond to the low noise regime, which is equivalent to the sparse limit of the majority vote result (see (<xref ref-type="disp-formula" rid="eqn413" hwp:id="xref-disp-formula-129-2" hwp:rel-id="disp-formula-129">4.13</xref>) and (<xref ref-type="disp-formula" rid="eqn45" hwp:id="xref-disp-formula-120-1" hwp:rel-id="disp-formula-120">4.5</xref>)), dashed lines represent the result for the intermediate amount of noise (<xref ref-type="disp-formula" rid="eqn410" hwp:id="xref-disp-formula-125-2" hwp:rel-id="disp-formula-125">4.10</xref>). The inverse temperature parameter <italic toggle="yes">β</italic> is different for different values of <italic toggle="yes">C<sub>F</sub> f</italic> so as to keep <italic toggle="yes">βαK</italic>e<italic toggle="yes"><sup>−C<sub>F</sub>f</sup></italic> = 1.2. The dotted lines show the maximum capacity possible for the given <italic toggle="yes">c</italic>. This is computed from the dense limit of the majority vote result (<xref ref-type="disp-formula" rid="eqn44" hwp:id="xref-disp-formula-119-2" hwp:rel-id="disp-formula-119">4.4</xref>), assuming that <italic toggle="yes">C<sub>F</sub></italic> is large enough so that when <italic toggle="yes">f</italic> is small enough to be neglected in comparison to 1 in the numerator of (<xref ref-type="disp-formula" rid="eqn44" hwp:id="xref-disp-formula-119-3" hwp:rel-id="disp-formula-119">4.4</xref>), we are still in the dense regime <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1.The tollerated error rate ε = 0.05.</p></caption><graphic xlink:href="157289_fig6" position="float" orientation="portrait" hwp:id="graphic-132"/></fig><p hwp:id="p-191">Clearly, the capacity in the two-subnetworks intermediate noise regime is larger than in the case of a majority vote committee machine when one assume that the sparseness of the representations is the same (see (<xref ref-type="disp-formula" rid="eqn43" hwp:id="xref-disp-formula-118-1" hwp:rel-id="disp-formula-118">4.3</xref>) and (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-5" hwp:rel-id="disp-formula-123">4.8</xref>)). This result is counterintuitive, but it can be explained: in the majority vote scenario, both the input receiving units and the free units contribute to a collective decision, even though the free units carry no information about the class of the input pattern and they actually generate noise as we assume that initially they are in a random state. In contrast, in the recurrent case, the collective state of the network is initially determined mostly by the input receiving units, which then drive the free units to the right state. The noise contained in the initial state of the free units does not affect much the initial relaxation dynamics provided that the noise in dynamics is sufficiently large (relatively low <italic toggle="yes">β</italic>).</p><p hwp:id="p-192">In the case of the majority vote committee machine, the class is decided in only one time step and the initially random free units generate a certain amount of noise that depends on their number. In the case of the recurrent dynamics, the connectivity is sparse and each neuron that participates in it samples the noisy neurons a number of times that depends on the relaxation time. If these neurons can flip randomly at every time step, then their noise is averaged out and the final effect of the free units can be smaller than in the majority vote committee machine.</p><p hwp:id="p-193">We now consider the two-subnetworks low noise regime. Not surprisingly, the capacity is identical to the sparse limit of the majority vote scenario (see (<xref ref-type="disp-formula" rid="eqn43" hwp:id="xref-disp-formula-118-2" hwp:rel-id="disp-formula-118">4.3</xref>))
<disp-formula id="eqn412" hwp:id="disp-formula-128" hwp:rev-id="xref-disp-formula-128-1 xref-disp-formula-128-2">
<alternatives hwp:id="alternatives-218"><graphic xlink:href="157289_eqn4-12.gif" position="float" orientation="portrait" hwp:id="graphic-133"/></alternatives></disp-formula>
</p><p hwp:id="p-194">And in the sparse limit, <italic toggle="yes">C<sub>F</sub> f</italic> ⋘ 1:
<disp-formula id="eqn413" hwp:id="disp-formula-129" hwp:rev-id="xref-disp-formula-129-1 xref-disp-formula-129-2 xref-disp-formula-129-3 xref-disp-formula-129-4 xref-disp-formula-129-5">
<alternatives hwp:id="alternatives-219"><graphic xlink:href="157289_eqn4-13.gif" position="float" orientation="portrait" hwp:id="graphic-134"/></alternatives></disp-formula></p><p hwp:id="p-195">This result is summarized graphically in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-10" hwp:rel-id="F3">Figure 4c</xref> and <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-11" hwp:rel-id="F3">4f</xref>. The low <italic toggle="yes">f</italic> curve segments in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-12" hwp:rel-id="F3">Figure 4f</xref> correspond to the two-subnetwork low noise regime (identical to the result for the committee machine), and the high <italic toggle="yes">f</italic> segments - to the uniform low noise regime (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-6" hwp:rel-id="disp-formula-122">4.7</xref>). The only parameter that differs between the segments of the same color is the strength of the recurrent connections <italic toggle="yes">α</italic>.</p><p hwp:id="p-196">Another way to visualize the results for the two-subnetwork regime is presented in <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 6</xref> where we plot the capacity as a function of the product <italic toggle="yes">C<sub>F</sub> f</italic>, for the two-subnetwork low noise regime in the sparse limit (<xref ref-type="disp-formula" rid="eqn413" hwp:id="xref-disp-formula-129-3" hwp:rel-id="disp-formula-129">4.13</xref>) (solid curves) and the same quantity for the two-subnetwork intermediate noise (<xref ref-type="disp-formula" rid="eqn410" hwp:id="xref-disp-formula-125-3" hwp:rel-id="disp-formula-125">4.10</xref>) (dashed curves). For the intermediate noise regime we tune the amount of noise so that <italic toggle="yes">βC<sub>R</sub>α</italic>e<italic toggle="yes"><sup>−C<sub>F</sub>f</sup></italic> is always equal to 1.2. The closer this value is to 1, the larger the advantage of the intermediate noise regime over the low noise regime (majority vote). The different colors represent different values of the number of feed-forward connections per input neuron <italic toggle="yes">c</italic> = <italic toggle="yes">C<sub>F</sub>M/N</italic>. The dotted lines show the maximum possible capacity for a given value of c, which is achieved for the dense limit of majority vote (<xref ref-type="disp-formula" rid="eqn44" hwp:id="xref-disp-formula-119-4" hwp:rel-id="disp-formula-119">4.4</xref>), assuming that <italic toggle="yes">C<sub>F</sub></italic> is large and <italic toggle="yes">f</italic> is small, so that <italic toggle="yes">f</italic> ⋘ 1, while <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1.</p><p hwp:id="p-197">It can be seen from <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 6</xref> that, as discussed before, the intermediate noise regime allows for very sparse input representations without sacrificing much the classification capacity, which is only slightly smaller than in the dense case (this ratio increases with <italic toggle="yes">c</italic> but saturates at <italic toggle="yes">P<sub>dense</sub></italic>/ <italic toggle="yes">P<sub>sparse</sub></italic> = π/2).</p><p hwp:id="p-198">This means that the representations can be very sparse, despite the limited connectivity. If there is any other computational reason for preferring sparse representations, then the readout system that we propose can still be used because it can tolerate a high degree of sparseness. This might be the case of the network architecture in the hippocampus in which the representations in the dentate gyrus (DG) are extremely sparse, and the downstream readout neurons in CA3, which would be analogous to the perceptrons in our intermediate layer, have very sparse connectivity. Although we know that moderate sparseness (f ∼ 0.1) can be highly beneficial for generalization [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>], we do not know why the representations in the DG are so sparse (f ∼ 0.01). However, our study shows that this elevated degree of sparseness does not necessarily impair the ability of the readout to perform efficiently a classification task.</p></sec></sec><sec id="s4c" hwp:id="sec-28"><label>4.3</label><title hwp:id="title-29">Simulation Results</title><p hwp:id="p-199"><xref ref-type="fig" rid="fig5" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 5</xref> shows the results of numerical simulations run to verify the predictions of the above calcula-tions. The two plots correspond to two different regimes characterized by different coding level of the input patterns representation. <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 5a</xref> shows the case of dense input representations <italic toggle="yes">C<sub>F</sub> f</italic> = 10 ≫ 1. The theoretical predictions for the majority vote scheme (committee machine) and the recurrent readout are represented by the two dashed lines, and the solid lines with the error bars depict the results of the simulations in the two scenarios. The case of sparse input representations <italic toggle="yes">C<sub>F</sub> f</italic> = 1 is shown in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 5b</xref>. As predicted by the analytical calculations, the recurrent readout scenario leads to higher classification capacity than the majority vote.</p><fig id="fig5" position="float" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-6"><p hwp:id="p-200"><bold>a</bold>. Simulation results (solid lines) and theoretical predictions (dashed lines) for the case of dense input representations, <italic toggle="yes">C<sub>F</sub> f</italic> = 10. The green curves correspond to majority vote scenario (committee machine) and the orange - to the recurrent readout in the uniform regime with relatively high noise. <bold>b</bold>. Same for the case of sparse input representation, <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1. The recurrent dynamics of the intermediate layer is in two-subnetwork regime with relatively high noise.</p></caption><graphic xlink:href="157289_fig5" position="float" orientation="portrait" hwp:id="graphic-135"/></fig><p hwp:id="p-201">The simulation plots were obtained as follows: we fixed the required accuracy of the classification at 1 – ε = 0.9 and the number of feedforward connections per readout at <italic toggle="yes">C<sub>F</sub></italic> = 50. For each number of the input units <italic toggle="yes">N</italic>, we chose the corresponding number of the intermediate readouts <italic toggle="yes">M</italic> = <italic toggle="yes">N</italic>/30, and we computed the predicted classification capacity <italic toggle="yes">P</italic>. We then trained the network with the set of <italic toggle="yes">P</italic><sub>1</sub> = <italic toggle="yes">P</italic> random and uncorrelated patterns, and tested the classification performance on a subset of 500 learned patterns. The obtained accuracy can be expressed as 1 – <inline-formula hwp:id="inline-formula-91"><alternatives hwp:id="alternatives-220"><inline-graphic xlink:href="157289_inline49.gif" hwp:id="inline-graphic-91"/></alternatives></inline-formula>. Then we trained the same network on the new set of <inline-formula hwp:id="inline-formula-92"><alternatives hwp:id="alternatives-221"><inline-graphic xlink:href="157289_inline50.gif" hwp:id="inline-graphic-92"/></alternatives></inline-formula> random patterns. We repeated the procedure 10 times, and we kept only those runs where the accuracy differed from the required one by no more then 2 percent (|<inline-formula hwp:id="inline-formula-93"><alternatives hwp:id="alternatives-222"><inline-graphic xlink:href="157289_inline49a.gif" hwp:id="inline-graphic-93"/></alternatives></inline-formula> — ε| &lt; 0.02). We then computed the mean and the standard error of the corresponding values of <italic toggle="yes">P</italic>. For the recurrent readout scenario, the recurrent connectivity was random with all the connections having the same strength, and the number of connections per unit being fixed at Cr = 200. The connectivity matrix was chosen to be symmetrical and the recurrent dynamics ran for 30 steps of synchronous update. The parameters of the recurrent dynamics for the plot of <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Figure 5a</xref> (dense input) were <italic toggle="yes">β</italic> = 0.5 and α = 0.015, and for the <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Figure 5b</xref> (sparse input): <italic toggle="yes">β</italic> = 33 and α = 0.0005</p></sec><sec id="s4d" hwp:id="sec-29"><label>4.4</label><title hwp:id="title-30">Optimizing the network architecture</title><sec id="s4d1" hwp:id="sec-30"><label>4.4.1</label><title hwp:id="title-31">Optimizing the architecture under the constraint that the total number of long-range connections is constant</title><p hwp:id="p-202">Now that we have derived the expression for the classification capacity as a function of the parameters of the network (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-3" hwp:rel-id="disp-formula-121">4.6</xref>) - (<xref ref-type="disp-formula" rid="eqn413" hwp:id="xref-disp-formula-129-4" hwp:rel-id="disp-formula-129">4.13</xref>) we can determine what would be the optimal choice of parameters that maximize the networks’ performance. We first discuss the optimization under the constraint on the total number of long range connections (i.e. the feedforward connections). More specifically, we assume that the number of inputs <italic toggle="yes">N</italic> and the total number of long-range connections <italic toggle="yes">C<sub>F</sub>M</italic> is fixed and we ask for what value of <italic toggle="yes">C<sub>F</sub></italic> (or <italic toggle="yes">M</italic>) the number of classifiable patterns <italic toggle="yes">P</italic> is maximal.</p><p hwp:id="p-203">For the majority vote scenario, when <italic toggle="yes">C<sub>F</sub> f</italic> is much larger than 1 (see (<xref ref-type="disp-formula" rid="eqn44" hwp:id="xref-disp-formula-119-5" hwp:rel-id="disp-formula-119">4.4</xref>)), <italic toggle="yes">C<sub>F</sub></italic> appears in the formulae for the capacity always as a factor of <italic toggle="yes">M</italic>, so the capacity depends on the product <italic toggle="yes">C<sub>F</sub>M</italic>. Hence, regrouping feedforward connections, i.e. changing <italic toggle="yes">C<sub>F</sub></italic> while keeping <italic toggle="yes">C</italic> = <italic toggle="yes">C<sub>F</sub>M</italic>/<italic toggle="yes">N</italic> constant, does not affect the classification capacity (unless we break the condition <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1).</p><p hwp:id="p-204">The same is true for the uniform low noise regime (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-7" hwp:rel-id="disp-formula-122">4.7</xref>), which is valid only if <italic toggle="yes">C<sub>F</sub> f</italic> ≫ 1. Even though <italic toggle="yes">C<sub>F</sub></italic> appears in the formula in the last term of the denominator without being multiplied by <italic toggle="yes">M</italic>, it is combined with the parameters of the recurrent dynamics <italic toggle="yes">C<sub>R</sub>α</italic>, which can be readjusted as <italic toggle="yes">C<sub>F</sub></italic> changes.</p><p hwp:id="p-205">In the case of high noise and uniform regime (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-4" hwp:rel-id="disp-formula-121">4.6</xref>), which is applicable for both dense and sparse representations, increasing <italic toggle="yes">C<sub>F</sub></italic>, while keeping <italic toggle="yes">C<sub>F</sub>M</italic> constant increases the capacity. For sparser representations (lower <italic toggle="yes">f</italic>), the effect is stronger.</p><p hwp:id="p-206">When <italic toggle="yes">C<sub>F</sub> f</italic> ⋘ 1, and the noise is intermediate or low, we can look again at <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 6</xref> (see <xref ref-type="sec" rid="s4b" hwp:id="xref-sec-25-1" hwp:rel-id="sec-25">Section 4.2</xref> and figure captions), but think of <italic toggle="yes">f</italic> as being fixed and <italic toggle="yes">C<sub>F</sub></italic> as changing along the horizontal axis. In the low noise regime, the capacity decreases gradually with decreasing the connectivity <italic toggle="yes">C<sub>F</sub></italic> unless the number of connections per input unit <italic toggle="yes">c</italic> is very high. Being in the intermediate noise regime, allows to go to very low values of <italic toggle="yes">C<sub>F</sub> f</italic> while preserving the classification capacity.</p><p hwp:id="p-207">To summarize, having less readout neurons with higher number of connections is generally better than more readout neurons with a lower number of connections. However, if the noise is not too high, this effect is not noticible unless the expected number of active inputs per readout neurons is of the order of 1 or smaller. In the intermediate noise regime, the feedforward connectivity can be decreased even further, down to 0.1 active inputs per readout, of even further if the number of feedforward connections per input neuron is high.</p></sec><sec id="s4d2" hwp:id="sec-31"><label>4.4.2</label><title hwp:id="title-32">Optimizing the architecture under the constraint that the total number of neurons is constant</title><p hwp:id="p-208">We now determine the optimal architecture in the case when the total number of neurons is fixed. Basically we ask how to partition the total set of neurons between the input and readout layer in order to maximize the classification capacity.</p><p hwp:id="p-209">It is straightforward to derive the optimal expansion ratio <italic toggle="yes">M</italic>/<italic toggle="yes">N</italic> from the formulas (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-5" hwp:rel-id="disp-formula-121">4.6</xref>) - (<xref ref-type="disp-formula" rid="eqn413" hwp:id="xref-disp-formula-129-5" hwp:rel-id="disp-formula-129">4.13</xref>) under the constraint <italic toggle="yes">M</italic> + <italic toggle="yes">N</italic> = <italic toggle="yes">const</italic>. In the uniform regime (see equations (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-6" hwp:rel-id="disp-formula-121">4.6</xref>) and (<xref ref-type="disp-formula" rid="eqn47" hwp:id="xref-disp-formula-122-8" hwp:rel-id="disp-formula-122">4.7</xref>)), if the parameters of the recurrent dynamics are not too far from the optimal ones, the expansion ratio that maximizes the capacity can be approximated by
<disp-formula id="ueqn39" hwp:id="disp-formula-130">
<alternatives hwp:id="alternatives-223"><graphic xlink:href="157289_ueqn39.gif" position="float" orientation="portrait" hwp:id="graphic-136"/></alternatives></disp-formula></p><p hwp:id="p-210">This corresponds to more than one perceptron for every <italic toggle="yes">C<sub>F</sub></italic> input in the non-overlapping design (i.e. when each input neuron is connected to only one perceptron and hence the populations of input neurons that are readout by each unit are not overlapping). To be more precise, a typical input is connected to approximately <italic toggle="yes">C<sub>F</sub>M</italic>/<italic toggle="yes">N</italic> œ <inline-formula hwp:id="inline-formula-94"><alternatives hwp:id="alternatives-224"><inline-graphic xlink:href="157289_inline51.gif" hwp:id="inline-graphic-94"/></alternatives></inline-formula> perceptrons. However, the number of perceptrons <italic toggle="yes">M</italic> is still much smaller than the number of inputs <italic toggle="yes">N</italic>.</p><p hwp:id="p-211">For the two-subnetworks low noise regime in the sparse limit <italic toggle="yes">C<sub>F</sub> f</italic> → 1, the optimal expansion ratio is approximately
<disp-formula id="ueqn40" hwp:id="disp-formula-131">
<alternatives hwp:id="alternatives-225"><graphic xlink:href="157289_ueqn40.gif" position="float" orientation="portrait" hwp:id="graphic-137"/></alternatives></disp-formula></p><p hwp:id="p-212">This corresponds to <inline-formula hwp:id="inline-formula-95"><alternatives hwp:id="alternatives-226"><inline-graphic xlink:href="157289_inline52.gif" hwp:id="inline-graphic-95"/></alternatives></inline-formula> connections per input unit. For sparser representataions the optimal proportion of units in the readout layer increases.</p><p hwp:id="p-213">For intermediate noise
<disp-formula id="ueqn41" hwp:id="disp-formula-132">
<alternatives hwp:id="alternatives-227"><graphic xlink:href="157289_ueqn41.gif" position="float" orientation="portrait" hwp:id="graphic-138"/></alternatives></disp-formula>
where <italic toggle="yes">γ</italic> is given by (<xref ref-type="disp-formula" rid="eqn49" hwp:id="xref-disp-formula-124-3" hwp:rel-id="disp-formula-124">4.9</xref>)</p><p hwp:id="p-214">So, when the metabolic cost of incorporating new units into the network is high, and the number of feedforward connections converging onto the same readout unit (<italic toggle="yes">C<sub>F</sub></italic>) is fixed (for example, because of spatial constraints), there is an optimal ratio between the number of input and readout units. In the case of dense input representations or high dynamical noise, this ratio is determined by the number of converging connections, <italic toggle="yes">C<sub>F</sub></italic>, whereas for sparse representations - by the coding level <italic toggle="yes">f</italic>. In both cases the number of outgoing connections per input neuron is large. For dense representations the optimal architecture is convergent (<italic toggle="yes">M</italic> &lt; <italic toggle="yes">N</italic>), while for sparse representations it can be either convergent or divergent.</p></sec></sec><sec id="s4e" hwp:id="sec-32"><label>4.5</label><title hwp:id="title-33">Multinomial Classification</title><p hwp:id="p-215">We now turn to a more difficult problem of classifying the inputs into more than two categories. The scheme presented above can be generalized in a straightforward way to serve as a multinomial classifier. We first show that in the case of multiple classes the classification capacity of the network does not change substantially. We later discuss a more realistic scenario of multiclass classification for which we can not compute the capacity analytically, but we demonstrate with simulations that the capacity decrease is moderate and, most importantly, that the linear scaling with the network size is preserved.</p><sec id="s4e1" hwp:id="sec-33" hwp:rev-id="xref-sec-33-1"><label>4.5.1</label><title hwp:id="title-34">Structured output</title><p hwp:id="p-216">The immediate generalization of the recurrent readout scheme to multinomial classification task is to introduce several populations of intermediate readout neurons, each of which would correspond to one class. The recurrent connectivity within a population would be as described before, while no recurrent connections would exist between the neurons belonging to distinct populations. The desired output pattern in response to an input from each class is then structured so that the population corresponding to the given class is active while the others are inactive. The final readout has to contain multiple readout units, one for each class. Their connectivity can still be sparse and random, but the sign of the connections would have to be adjusted based on whether it comes from the neuron in the population selective for the same class as the given final readout or not (see <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7a</xref>).</p><fig id="fig7" position="float" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-7"><p hwp:id="p-217"><bold>a.</bold> Network architecture for the case of structured output (see <xref ref-type="sec" rid="s4e1" hwp:id="xref-sec-33-1" hwp:rel-id="sec-33">Section 4.5.1</xref>). For the case of 3-way classification, the intermediate layer of readout neurons is divided into 3 subpopulations, each selective for its own class of input patterns. The recurrent connectivity is random and excitatory within subpopulations, but there is no recurrent connections between the subpopulations. The final readouts, one for each class, are connected sparsely and randomly, as before, but the sign of the connections is only positive if the presynaptic neuron belongs to the correct subpopulation, the rest are zero or negative. <bold>b.</bold> Network architecture for the case of random output (<xref ref-type="sec" rid="s4e2" hwp:id="xref-sec-34-1" hwp:rel-id="sec-34">Section 4.5.2</xref>). There are no distinct subpopulations in the intermediate layer, and the desired output pattern corresponding to each class of input patterns is chosen randomly. The recurrent connections exist between any pair of the readout neurons with equal probability. The strength of these connections, however, is now adjusted according to Hebbian learning rule (<xref ref-type="disp-formula" rid="eqn414" hwp:id="xref-disp-formula-135-1" hwp:rel-id="disp-formula-135">4.14</xref>). <bold>c.</bold> The results of the simulation for multinomial classification. The output patterns corresponding to <italic toggle="yes">L</italic> = 5 classes are chosen randomly with the coding level <italic toggle="yes">y</italic> = 1/2. The recurrent connectivity is sparse and the strength of the synapses are trained with the learning rule (<xref ref-type="disp-formula" rid="eqn414" hwp:id="xref-disp-formula-135-2" hwp:rel-id="disp-formula-135">4.14</xref>). The network of recurrently connected perceptrons is in the high noise regime with dense input representations (<italic toggle="yes">C<sub>F</sub></italic> = 50, <italic toggle="yes">f</italic> = 0.2, <italic toggle="yes">C<sub>R</sub></italic> = 200, <italic toggle="yes">α</italic> = 0.015, <italic toggle="yes">β</italic> = 0.5). The dashed line is the estimation of the capacity from the formula (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-7" hwp:rel-id="disp-formula-121">4.6</xref>) assuming two equal size subpopulations of the readouts.</p></caption><graphic xlink:href="157289_fig7" position="float" orientation="portrait" hwp:id="graphic-139"/></fig><p hwp:id="p-218">The classification capacity can now be computed in the same way as above, by noticing that each population is now doing a binary classification, selecting for one out of <italic toggle="yes">l</italic> classes. The only difference is that the proportion of ‘positive’ patterns (the output sparseness) is now <italic toggle="yes">y</italic> = 1/<italic toggle="yes">L</italic> instead of 1/2. The capacity formula for the case of sparse output is derived in the Methods (<xref ref-type="sec" rid="s3" hwp:id="xref-sec-3-2" hwp:rel-id="sec-3">section 3</xref>) and it differs from the capacity for a dense case by a factor, that depends on <italic toggle="yes">y</italic>.
<disp-formula id="ueqn42" hwp:id="disp-formula-133">
<alternatives hwp:id="alternatives-228"><graphic xlink:href="157289_ueqn42.gif" position="float" orientation="portrait" hwp:id="graphic-140"/></alternatives></disp-formula></p><p hwp:id="p-219">It should be noted, that the number of intermediate readouts <italic toggle="yes">M</italic>, entering this formula is the number of units in the population selective for a particular class. So, if total number of intermediate readout units is <italic toggle="yes">Mtotai,</italic> and all populations have equal size, it is <italic toggle="yes">M</italic> = <italic toggle="yes">M<sub>total</sub>/L</italic> = <italic toggle="yes">yM<sub>total</sub></italic>, that should enter the formulas for the capacity. So, in terms of the total number of intermediate units, in the two-subnetworks intermediate noise regime, for example (formula (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-6" hwp:rel-id="disp-formula-123">4.8</xref>)), we have
<disp-formula id="ueqn43" hwp:id="disp-formula-134">
<alternatives hwp:id="alternatives-229"><graphic xlink:href="157289_ueqn43.gif" position="float" orientation="portrait" hwp:id="graphic-141"/></alternatives></disp-formula>
</p><p hwp:id="p-220">Where <italic toggle="yes">γ</italic> is given by (<xref ref-type="disp-formula" rid="eqn49" hwp:id="xref-disp-formula-124-4" hwp:rel-id="disp-formula-124">4.9</xref>). There are two differences with respect to the binary classification case (<xref ref-type="disp-formula" rid="eqn48" hwp:id="xref-disp-formula-123-7" hwp:rel-id="disp-formula-123">4.8</xref>). The first is the prefactor, which is equal to 1/2 for the case of two classes (<italic toggle="yes">L</italic> = 2). This is the reflection of the fact, that when only two classes are possible, the current scheme is redundant - when the first population is active, the other is not, and vice versa. In the limit of large number of classes, the prefactor is equal to 1/4. The other difference is in the second term in the denominator, which rescales <italic toggle="yes">N</italic>, the number of the input units. Namely, for the number of intermediate readout units, the role of correlations between them is decreased compared to the binary classification case. This is because there is no interference between the readout neurons belonging to different populations.</p></sec><sec id="s4e2" hwp:id="sec-34" hwp:rev-id="xref-sec-34-1"><label>4.5.2</label><title hwp:id="title-35">Random output</title><p hwp:id="p-221">Another, more realistic scenario is to assign the output patterns that correspond to each of <italic toggle="yes">L</italic> classes randomly and train the existing recurrent connections with a plausible learning rule.
<disp-formula id="eqn414" hwp:id="disp-formula-135" hwp:rev-id="xref-disp-formula-135-1 xref-disp-formula-135-2 xref-disp-formula-135-3">
<alternatives hwp:id="alternatives-230"><graphic xlink:href="157289_eqn4-14.gif" position="float" orientation="portrait" hwp:id="graphic-142"/></alternatives></disp-formula></p><p hwp:id="p-222">Where the <inline-formula hwp:id="inline-formula-96"><alternatives hwp:id="alternatives-231"><inline-graphic xlink:href="157289_inline53.gif" hwp:id="inline-graphic-96"/></alternatives></inline-formula> is the output pattern corresponding to class <italic toggle="yes">a</italic>, (<italic toggle="yes">a</italic> = 1… <italic toggle="yes">L</italic>). In this case there are no structurally distinct subpopulations of the intermediate readout neurons which are defined a priori (see <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7b</xref>). In contrast, the subpopulations of neurons which represent different classes emerge as a consequence of the learning rule (<xref ref-type="disp-formula" rid="eqn414" hwp:id="xref-disp-formula-135-3" hwp:rel-id="disp-formula-135">4.14</xref>).</p><p hwp:id="p-223"><xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7</xref> shows the simulation results for a 5-way classification (<italic toggle="yes">L</italic> = 5) of dense input patterns with high dynamical noise (the parameter values are given in the figure caption). The dashed line on the figure indicates the capacity given by the formula (<xref ref-type="disp-formula" rid="eqn46" hwp:id="xref-disp-formula-121-8" hwp:rel-id="disp-formula-121">4.6</xref>) assuming that the population of <italic toggle="yes">M</italic> intermediate readouts is split into two segregated subpopulations, whose activity is opposite in all the output patterns.</p></sec></sec><sec id="s4f" hwp:id="sec-35" hwp:rev-id="xref-sec-35-1"><label>4.6</label><title hwp:id="title-36">The initial condition of the recurrent network</title><p hwp:id="p-224">An important assumption that we made in order to implement the majority vote with a recurrent readout is that the recurrent network initial condition is unbiased, or, in other words, that <italic toggle="yes">m</italic><sub>0</sub> = 0 (see <xref ref-type="sec" rid="s3c3" hwp:id="xref-sec-13-1" hwp:rel-id="sec-13">Section 3.3.3</xref>). This condition might sound difficult to realize in a network that is basically designed to amplify any small deviation from <italic toggle="yes">m</italic><sub>0</sub> = 0. However, this condition could be realized as follows: assume that before a pattern to be classified is presented, the input layer is spontaneously active. This spontaneous activity generates a feed-forward input <inline-formula hwp:id="inline-formula-97"><alternatives hwp:id="alternatives-232"><inline-graphic xlink:href="157289_inline54.gif" hwp:id="inline-graphic-97"/></alternatives></inline-formula> which causes the disordered state (<italic toggle="yes">m</italic> = 0) to be the only stable state of the recurrent network (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-18" hwp:rel-id="F2">Figure 2a</xref>). There are two conditions on the statistics of <inline-formula hwp:id="inline-formula-98"><alternatives hwp:id="alternatives-233"><inline-graphic xlink:href="157289_inline54a.gif" hwp:id="inline-graphic-98"/></alternatives></inline-formula> that are required to have <italic toggle="yes">M</italic> = 0 as the only stable state of the system in the mean field approximation. The first requirement is that <inline-formula hwp:id="inline-formula-99"><alternatives hwp:id="alternatives-234"><inline-graphic xlink:href="157289_inline54b.gif" hwp:id="inline-graphic-99"/></alternatives></inline-formula> has zero expectation value, which is satisfied if the patterns of spontaneous activity are not correlated with the training patterns. The second requirement is that the standard deviation of the distribution is large enough, to make the slope of the sigmoidal curve of <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-19" hwp:rel-id="F2">Figure 2</xref> smaller than 1. For instance, in the uniform regime (see <xref ref-type="sec" rid="s3c5" hwp:id="xref-sec-15-1" hwp:rel-id="sec-15">section 3.3.5</xref>), the latter requirement is <inline-formula hwp:id="inline-formula-100"><alternatives hwp:id="alternatives-235"><inline-graphic xlink:href="157289_inline55.gif" hwp:id="inline-graphic-100"/></alternatives></inline-formula> is the standard deviation of the feedforward current due to spontaneous activity. When the input pattern is presented then the noise is assumed to decrease to restore the conditions (<xref ref-type="disp-formula" rid="eqn347" hwp:id="xref-disp-formula-62-2" hwp:rel-id="disp-formula-62">3.47</xref>) that allow the recurrent network to have three solutions, two stable, corresponding to the possible classification outcomes, and one unstable, which was the initial state. A reduction in noise during stimulus presentation has been observed in [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>].</p></sec></sec><sec id="s5" hwp:id="sec-36"><label>5</label><title hwp:id="title-37">Discussion</title><p hwp:id="p-225">We presented a model network based on perceptrons in which all the neurons have limited connectivity and nevertheless the classification capacity grows unboundedly and linearly with the size of the network.</p><p hwp:id="p-226">The limitations on classification capacity of the individual perceptrons that are imposed by the limited connectivity are overcome by reading out multiple perceptrons, as in a committee machine. However, the readout mechanism is different from the one normally used in committee machines as it uses a recurrent attractor dynamics of committee members to generate a final vote. Thanks to the recurrent dynamics, it is then possible to readout a small sample of all the committee members to determine the committee decision. This allows for readouts that have a limited connectivity, even when the size of the network becomes very large.</p><p hwp:id="p-227">Interestingly, there are situations in which the proposed recurrent readout scheme can outperform classical readouts that are based on a majority vote despite the fact that the majority vote would require a significantly larger readout connectivity (see <xref ref-type="sec" rid="s4b2" hwp:id="xref-sec-27-1" hwp:rel-id="sec-27">sections 4.2.2</xref> and <xref ref-type="sec" rid="s3c9" hwp:id="xref-sec-19-1" hwp:rel-id="sec-19">3.3.9</xref>). For the majority vote scheme, the classification capacity drops drastically when the input representations are very sparse because the fraction of classifiers whose inputs are all silent becomes substantial and these classifiers just contribute to the noise. Instead, for the recurrent readout the classification capacity can be kept high even for very sparse representations in certain parameter regimes because the recurrent dynamics can align the ‘free’ classifiers to the majority decided by the other, informative classifiers. The lower limit on the coding level <italic toggle="yes">f</italic>, below which the capacity drops is determined by the amount of noise in the recurrent dynamics, the expansion ratio and the number of feed-forward connections per perceptron (see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-13" hwp:rel-id="F3">Figure 4</xref>).</p><p hwp:id="p-228">In general, the proposed system is robust to both sparse connectivity and sparse representations, which makes it suitable to describe neural circuits like the dentate gyrus (DG) and CA3 area, where the number of connections of downstream neurons (CA3) is much smaller than the number of neurons in the input (DG) and the neural activity in the input can be very sparse. CA3 is known to have the recurrent connections that would implement our proposed readout mechanism. We showed that for intermediate noise (see <xref ref-type="sec" rid="s4b2" hwp:id="xref-sec-27-2" hwp:rel-id="sec-27">sections 4.2.2</xref> and <xref ref-type="sec" rid="s3c9" hwp:id="xref-sec-19-2" hwp:rel-id="sec-19">3.3.9</xref>), the classification capacity stays within a reasonable range even when the expected number of active units read out by each perceptron is smaller than 1 (see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-3-14" hwp:rel-id="F3">Figures 4e, 4f</xref> and <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-5-5" hwp:rel-id="F5">6</xref>). This result nicely complements the study presented in [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">34</xref>], where the authors show that low dimensional correlated inputs require an intermediate layer of neurons (randomly connected in [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-3" hwp:rel-id="ref-34">34</xref>]). For these neurons in the intermediate layer there is an optimal sparseness level which minimizes the generalization error of a single perceptron-like readout. Here we showed that there is a readout scheme that would work also for the sparse representations required in the intermediate layer of [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-4" hwp:rel-id="ref-34">34</xref>].</p></sec></body><back><app-group hwp:id="app-group-1"><app id="app6" hwp:id="app-1"><label>6</label><title hwp:id="title-38">Appendix</title><sec id="app6a" hwp:id="sec-37" hwp:rev-id="xref-sec-37-1"><label>6.1</label><title hwp:id="title-39">A1</title><p hwp:id="p-229">In this section we derive (<xref ref-type="disp-formula" rid="eqn343" hwp:id="xref-disp-formula-56-2" hwp:rel-id="disp-formula-56">3.43</xref>), the variance of the feedforward current, averaged over the readouts
<disp-formula id="eqn61" hwp:id="disp-formula-136">
<alternatives hwp:id="alternatives-236"><graphic xlink:href="157289_eqn6-1.gif" position="float" orientation="portrait" hwp:id="graphic-143"/></alternatives></disp-formula></p><p hwp:id="p-230">The variance of <italic toggle="yes">h̅<sup>ν</sup></italic> is contributed by the diagonal terms and the non-diagonal terms, in the limit <italic toggle="yes">M</italic> → ∞ approximated by
<disp-formula id="eqn62" hwp:id="disp-formula-137" hwp:rev-id="xref-disp-formula-137-1 xref-disp-formula-137-2">
<alternatives hwp:id="alternatives-237"><graphic xlink:href="157289_eqn6-2.gif" position="float" orientation="portrait" hwp:id="graphic-144"/></alternatives></disp-formula></p><p hwp:id="p-231">For the non-diagonal terms, neglecting <inline-formula hwp:id="inline-formula-101"><alternatives hwp:id="alternatives-238"><inline-graphic xlink:href="157289_inline56.gif" hwp:id="inline-graphic-101"/></alternatives></inline-formula> corrections coming from the signal, using representation (<xref ref-type="disp-formula" rid="eqn321" hwp:id="xref-disp-formula-26-1" hwp:rel-id="disp-formula-26">3.21</xref>), we find in the same way as in the computation of (<xref ref-type="disp-formula" rid="eqn322" hwp:id="xref-disp-formula-27-2" hwp:rel-id="disp-formula-27">3.22</xref>)
<disp-formula id="eqn63" hwp:id="disp-formula-138">
<alternatives hwp:id="alternatives-239"><graphic xlink:href="157289_eqn6-3.gif" position="float" orientation="portrait" hwp:id="graphic-145"/></alternatives></disp-formula>
from the covariance of the <italic toggle="yes">z<sub>kl</sub></italic> terms. Here 〈<italic toggle="yes">n<sub>kl</sub></italic>〉 is the expectation value of the number of common active input neurons for readouts <italic toggle="yes">k</italic> and <italic toggle="yes">l</italic> (see details in <xref ref-type="sec" rid="s3b2" hwp:id="xref-sec-9-1" hwp:rel-id="sec-9">section 3.2.2</xref>). Therefore, comparing with (<xref ref-type="disp-formula" rid="eqn39" hwp:id="xref-disp-formula-12-1" hwp:rel-id="disp-formula-12">3.9</xref>) we find
<disp-formula id="eqn64" hwp:id="disp-formula-139">
<alternatives hwp:id="alternatives-240"><graphic xlink:href="157289_eqn6-4.gif" position="float" orientation="portrait" hwp:id="graphic-146"/></alternatives></disp-formula></p><p hwp:id="p-232">The 〈<italic toggle="yes">n<sub>kl</sub></italic>〉 in the limit <italic toggle="yes">N</italic>, <italic toggle="yes">M</italic> → ∞ to and finite <italic toggle="yes">C<sub>F</sub></italic>, <italic toggle="yes">f</italic> can be estimated as
<disp-formula id="eqn65" hwp:id="disp-formula-140">
<alternatives hwp:id="alternatives-241"><graphic xlink:href="157289_eqn6-5.gif" position="float" orientation="portrait" hwp:id="graphic-147"/></alternatives></disp-formula>
which gives
<disp-formula id="eqn66" hwp:id="disp-formula-141">
<alternatives hwp:id="alternatives-242"><graphic xlink:href="157289_eqn6-6.gif" position="float" orientation="portrait" hwp:id="graphic-148"/></alternatives></disp-formula>
and therefore
<disp-formula id="eqn67" hwp:id="disp-formula-142">
<alternatives hwp:id="alternatives-243"><graphic xlink:href="157289_eqn6-7.gif" position="float" orientation="portrait" hwp:id="graphic-149"/></alternatives></disp-formula></p><p hwp:id="p-233">Hence (<xref ref-type="disp-formula" rid="eqn62" hwp:id="xref-disp-formula-137-1" hwp:rel-id="disp-formula-137">6.2</xref>) reduces to
<disp-formula id="eqn68" hwp:id="disp-formula-143">
<alternatives hwp:id="alternatives-244"><graphic xlink:href="157289_eqn6-8.gif" position="float" orientation="portrait" hwp:id="graphic-150"/></alternatives></disp-formula>
and implies (??).</p></sec><sec id="app6b" hwp:id="sec-38"><label>6.2</label><title hwp:id="title-40">A2</title><p hwp:id="p-234">In this section we derive the formula (<xref ref-type="disp-formula" rid="eqn375" hwp:id="xref-disp-formula-109-1" hwp:rel-id="disp-formula-109">3.75</xref>) for the covariance of the signs of the external currents into two different free units in the two-subnetwork regime.
<disp-formula id="eqn69" hwp:id="disp-formula-144">
<alternatives hwp:id="alternatives-245"><graphic xlink:href="157289_eqn6-9.gif" position="float" orientation="portrait" hwp:id="graphic-151"/></alternatives></disp-formula>
we introduce a notation
<disp-formula id="ueqn44" hwp:id="disp-formula-145">
<alternatives hwp:id="alternatives-246"><graphic xlink:href="157289_ueqn44.gif" position="float" orientation="portrait" hwp:id="graphic-152"/></alternatives></disp-formula>
and without loss of generality assume <italic toggle="yes">α</italic> =1.</p><p hwp:id="p-235">There are two cases of contributions to the correlation, that we will call case I and case II, see <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-1" hwp:rel-id="F8">figure 8</xref>.</p><fig id="fig8" position="float" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;157289v3/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-8"><p hwp:id="p-236">Two sources of input correlations for the subnetwork of free units (orange circles), referred in the text as case I and case II. On the left diagram two free units are connected to the same input receiving unit in the readout layer (red circle). On the right diagram there is no input receiving unit that is connected to both free units, but the correlation arises from an active unit in the input layer (green circle), which is connected to the two free units indirectly.</p></caption><graphic xlink:href="157289_fig8" position="float" orientation="portrait" hwp:id="graphic-153"/></fig><p hwp:id="p-237">The case I contribution to this correlation comes from the free units <italic toggle="yes">k</italic> and <italic toggle="yes">p</italic> being connected to the same input receiving unit <italic toggle="yes">r</italic>. We neglect the probability that the overlap will be over more than one input receiving unit since we keep connectivity <italic toggle="yes">C<sub>R</sub></italic> fixed when we scale the number of units <italic toggle="yes">M</italic>. To the leading order in <italic toggle="yes">C<sub>R</sub></italic>/<italic toggle="yes">M</italic>, the probability of case I contribution is
<disp-formula id="eqn610" hwp:id="disp-formula-146">
<alternatives hwp:id="alternatives-247"><graphic xlink:href="157289_eqn6-10.gif" position="float" orientation="portrait" hwp:id="graphic-154"/></alternatives></disp-formula></p><p hwp:id="p-238">This is because a typical free unit is connected to <italic toggle="yes">C</italic><sub>R</sub>(1 – <italic toggle="yes">e<sup>−C<sub>F</sub>f</sup></italic>) out of <italic toggle="yes">M</italic>(1 – <italic toggle="yes">e<sup>−C<sub>F</sub>f</sup></italic>) input receiving units.</p><p hwp:id="p-239">The case II contribution comes from the possibility that there is an input layer unit that is active and connects to both via different input receiving neurons. The approximate robability of case II contribution, assuming <italic toggle="yes">C<sub>F</sub>/N</italic> is small, is given by
<disp-formula id="eqn611" hwp:id="disp-formula-147">
<alternatives hwp:id="alternatives-248"><graphic xlink:href="157289_eqn6-11.gif" position="float" orientation="portrait" hwp:id="graphic-155"/></alternatives></disp-formula></p><p hwp:id="p-240">To derive this probability, recall that the probability of any two readouts to be connected to the same active input is <inline-formula hwp:id="inline-formula-102"><alternatives hwp:id="alternatives-249"><inline-graphic xlink:href="157289_inline57.gif" hwp:id="inline-graphic-102"/></alternatives></inline-formula>, and there are <inline-formula hwp:id="inline-formula-103"><alternatives hwp:id="alternatives-250"><inline-graphic xlink:href="157289_inline58.gif" hwp:id="inline-graphic-103"/></alternatives></inline-formula> pairs of readouts (red units on <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-2" hwp:rel-id="F8">figure 8</xref>) connected to the given pair of free units (orange units). The probability that both units in this pair are input receiving units is already taken into account by the factor <inline-formula hwp:id="inline-formula-104"><alternatives hwp:id="alternatives-251"><inline-graphic xlink:href="157289_inline57a.gif" hwp:id="inline-graphic-104"/></alternatives></inline-formula>.</p><p hwp:id="p-241">In the case I the relevant correlation is
<disp-formula id="eqn612" hwp:id="disp-formula-148">
<alternatives hwp:id="alternatives-252"><graphic xlink:href="157289_eqn6-12.gif" position="float" orientation="portrait" hwp:id="graphic-156"/></alternatives></disp-formula></p><p hwp:id="p-242">We approximated <italic toggle="yes">C<sub>R</sub></italic> – 1 by <italic toggle="yes">C<sub>R</sub></italic> and used error function integral (<xref ref-type="disp-formula" rid="eqn313" hwp:id="xref-disp-formula-17-5" hwp:rel-id="disp-formula-17">3.13</xref>) at small argument on standard Gaussian variables <italic toggle="yes">z<sub>k</sub></italic> and <italic toggle="yes">z<sub>p</sub></italic> to transform the first line to the second line.</p><p hwp:id="p-243">In the case II the relevant correlation
<disp-formula id="eqn613" hwp:id="disp-formula-149">
<alternatives hwp:id="alternatives-253"><graphic xlink:href="157289_eqn6-13.gif" position="float" orientation="portrait" hwp:id="graphic-157"/></alternatives></disp-formula>
where <italic toggle="yes">n<sub>r</sub></italic> and <italic toggle="yes">n<sub>s</sub></italic> are from binomial distribution on <italic toggle="yes">C<sub>F</sub></italic> – 1 trials with probability <italic toggle="yes">f</italic> computed as in (<xref ref-type="disp-formula" rid="eqn325" hwp:id="xref-disp-formula-30-8" hwp:rel-id="disp-formula-30">3.25</xref>) from the correlation of the sign(<italic toggle="yes">h<sub>r</sub></italic>) and sign(<italic toggle="yes">h<sub>s</sub></italic>).</p><p hwp:id="p-244">Now we can compute 6.9 in the leading order as <italic toggle="yes">p<sub>I</sub>, p<sub>II</sub></italic> probability weighted sum of the contributions from case I and case II:
<disp-formula id="eqn614" hwp:id="disp-formula-150">
<alternatives hwp:id="alternatives-254"><graphic xlink:href="157289_eqn6-14.gif" position="float" orientation="portrait" hwp:id="graphic-158"/></alternatives></disp-formula></p><p hwp:id="p-245">At the diagonal terms we have simply
<disp-formula id="eqn615" hwp:id="disp-formula-151">
<alternatives hwp:id="alternatives-255"><graphic xlink:href="157289_eqn6-15.gif" position="float" orientation="portrait" hwp:id="graphic-159"/></alternatives></disp-formula></p><p hwp:id="p-246">Alltogether, combining the contribution from diagonal and non-diagonal terms as in (<xref ref-type="disp-formula" rid="eqn62" hwp:id="xref-disp-formula-137-2" hwp:rel-id="disp-formula-137">6.2</xref>) we find
<disp-formula id="eqn616" hwp:id="disp-formula-152" hwp:rev-id="xref-disp-formula-152-1">
<alternatives hwp:id="alternatives-256"><graphic xlink:href="157289_eqn6-16.gif" position="float" orientation="portrait" hwp:id="graphic-160"/></alternatives></disp-formula></p><p hwp:id="p-247">We will assume that <italic toggle="yes">C<sub>F</sub> f</italic> ≲ 1 so that Mf ≃ <italic toggle="yes">M</italic> and that even though <italic toggle="yes">C<sub>R</sub></italic> does not scale linearly with <italic toggle="yes">M</italic>, <italic toggle="yes">N</italic>, <italic toggle="yes">P</italic> still
<disp-formula id="eqn617" hwp:id="disp-formula-153">
<alternatives hwp:id="alternatives-257"><graphic xlink:href="157289_eqn6-17.gif" position="float" orientation="portrait" hwp:id="graphic-161"/></alternatives></disp-formula>
then we can, in fact, drop the diagonal term in (<xref ref-type="disp-formula" rid="eqn616" hwp:id="xref-disp-formula-152-1" hwp:rel-id="disp-formula-152">6.16</xref>) and take the approximation
<disp-formula id="eqn618" hwp:id="disp-formula-154">
<alternatives hwp:id="alternatives-258"><graphic xlink:href="157289_eqn6-18.gif" position="float" orientation="portrait" hwp:id="graphic-162"/></alternatives></disp-formula>
</p></sec></app></app-group><ack hwp:id="ack-1"><label>7</label><title hwp:id="title-41">Acknowledgements</title><p hwp:id="p-248">SF is supported by the Gatsby Charitable Foundation, the Simons Foundation, the Schwartz founda-tion, the Kavli foundation and the NSF’s NeuroNex program award DBI-1707398. LK acknowledges support from the grants ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-42">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Rosenblatt Frank"><given-names>Frank</given-names> <surname>Rosenblatt</surname></string-name>. <article-title hwp:id="article-title-2">The perceptron, a perceiving and recognizing automaton Project Para</article-title>. <source hwp:id="source-1">Cornell Aeronautical Laboratory</source>, <year>1957</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>[2]</label><citation publication-type="book" citation-type="book" ref:id="157289v3.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Amit Daniel J"><given-names>Daniel J</given-names> <surname>Amit</surname></string-name>. <source hwp:id="source-2">Modeling brain function: The world of attractor neural networks</source>. <publisher-name>Cambridge University Press</publisher-name>, <year>1992</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Roudi Yasser"><given-names>Yasser</given-names> <surname>Roudi</surname></string-name> and <string-name name-style="western" hwp:sortable="Latham Peter E"><given-names>Peter E</given-names> <surname>Latham</surname></string-name>. <article-title hwp:id="article-title-3">A balanced memory network</article-title>. <source hwp:id="source-3">PLoS computational biology</source>, <volume>3</volume>(<issue>9</issue>):<fpage>e141</fpage>, <year>2007</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bullmore Ed"><given-names>Ed</given-names> <surname>Bullmore</surname></string-name> and <string-name name-style="western" hwp:sortable="Sporns Olaf"><given-names>Olaf</given-names> <surname>Sporns</surname></string-name>. <article-title hwp:id="article-title-4">The economy of brain network organization</article-title>. <source hwp:id="source-4">Nature Reviews Neuroscience</source>, <volume>13</volume>(<issue>5</issue>):<fpage>336</fpage>-<lpage>349</lpage>, <year>2012</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Drew Liam J"><given-names>Liam J</given-names> <surname>Drew</surname></string-name>, <string-name name-style="western" hwp:sortable="Fusi Stefano"><given-names>Stefano</given-names> <surname>Fusi</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hen René"><given-names>René</given-names> <surname>Hen</surname></string-name>. <article-title hwp:id="article-title-5">Adult neurogenesis in the mammalian hippocampus: why the dentate gyrus?</article-title> <source hwp:id="source-5">Learning &amp; Memory</source>, <volume>20</volume>(<issue>12</issue>):<fpage>710</fpage>-<lpage>729</lpage>, <year>2013</year>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Amaral David G"><given-names>David G</given-names> <surname>Amaral</surname></string-name>, <string-name name-style="western" hwp:sortable="Ishizuka Norio"><given-names>Norio</given-names> <surname>Ishizuka</surname></string-name>, and <string-name name-style="western" hwp:sortable="Claiborne Brenda"><given-names>Brenda</given-names> <surname>Claiborne</surname></string-name>. <article-title hwp:id="article-title-6">Neurons, numbers and the hippocampal network</article-title>. <source hwp:id="source-6">Progress in brain research</source>, <volume>83</volume>:<fpage>1</fpage>-<lpage>11</lpage>, <year>1990</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Jung MW"><given-names>MW</given-names> <surname>Jung</surname></string-name> and <string-name name-style="western" hwp:sortable="McNaughton BL"><given-names>BL</given-names> <surname>McNaughton</surname></string-name>. <article-title hwp:id="article-title-7">Spatial selectivity of unit activity in the hippocampal granular layer</article-title>. <source hwp:id="source-7">Hippocampus</source>, <volume>3</volume>(<issue>2</issue>):<fpage>165</fpage>-<lpage>182</lpage>, <year>1993</year>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Chawla MK"><given-names>MK</given-names> <surname>Chawla</surname></string-name>, <string-name name-style="western" hwp:sortable="Guzowski JF"><given-names>JF</given-names> <surname>Guzowski</surname></string-name>, <string-name name-style="western" hwp:sortable="Ramirez-Amaya V"><given-names>V</given-names> <surname>Ramirez-Amaya</surname></string-name>, <string-name name-style="western" hwp:sortable="Lipa P"><given-names>P</given-names> <surname>Lipa</surname></string-name>, <string-name name-style="western" hwp:sortable="Hoffman KL"><given-names>KL</given-names> <surname>Hoffman</surname></string-name>, <string-name name-style="western" hwp:sortable="Marriott LK"><given-names>LK</given-names> <surname>Marriott</surname></string-name>, <string-name name-style="western" hwp:sortable="Worley PF"><given-names>PF</given-names> <surname>Worley</surname></string-name>, <string-name name-style="western" hwp:sortable="McNaughton BL"><given-names>BL</given-names> <surname>McNaughton</surname></string-name>, and <string-name name-style="western" hwp:sortable="Bavs CA"><given-names>CA</given-names> <surname>Bavs</surname></string-name>. <article-title hwp:id="article-title-8">Sparse, environmentally selective expression of arc rna in the upper blade of the rodent fascia dentata by brief spatial experience</article-title>. <source hwp:id="source-8">Hippocampus</source>, <volume>15</volume>(<issue>5</issue>):<fpage>579</fpage>-<lpage>586</lpage>, <year>2005</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.9" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Nilsson Nils J"><given-names>Nils J</given-names> <surname>Nilsson</surname></string-name>. <article-title hwp:id="article-title-9">Learning machines: foundations of trainable pattern-classifying systems</article-title>. <source hwp:id="source-9">McGraw-Hill</source>, <year>1965</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Parmanto Bambang"><given-names>Bambang</given-names> <surname>Parmanto</surname></string-name>, <string-name name-style="western" hwp:sortable="Munro Paul W"><given-names>Paul W</given-names> <surname>Munro</surname></string-name>, and <string-name name-style="western" hwp:sortable="Doyle Howard R"><given-names>Howard R</given-names> <surname>Doyle</surname></string-name>. <article-title hwp:id="article-title-10">Reducing variance of committee prediction with resampling techniques</article-title>. <source hwp:id="source-10">Connection Science</source>, <volume>8</volume>(<issue>3-4</issue>):<fpage>405</fpage>-<lpage>426</lpage>, <year>1996</year>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="book" citation-type="book" ref:id="157289v3.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Bishop C"><given-names>C</given-names> <surname>Bishop</surname></string-name>. <source hwp:id="source-11">Pattern recognition and machine learning (information science and statistics)</source>, <edition>1st</edition> edn. 2006. corr. 2nd printing edn. <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>, <year>2007</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="book" citation-type="book" ref:id="157289v3.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Zhou Zhi-Hua"><given-names>Zhi-Hua</given-names> <surname>Zhou</surname></string-name>. <source hwp:id="source-12">Ensemble methods: foundations and algorithms</source>. <publisher-name>CRC Press</publisher-name>, <year>2012</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Kearns Michael"><given-names>Michael</given-names> <surname>Kearns</surname></string-name>. <article-title hwp:id="article-title-11">Thoughts on hypothesis boosting</article-title>. <source hwp:id="source-13">Unpublished manuscript</source>, <volume>45</volume>:<fpage>105</fpage>, <year>1988</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Rao JNK"><given-names>JNK</given-names> <surname>Rao</surname></string-name> and <string-name name-style="western" hwp:sortable="Subrahmaniam Kathleen"><given-names>Kathleen</given-names> <surname>Subrahmaniam</surname></string-name>. <article-title hwp:id="article-title-12">Combining independent estimators and estimation in linear regression with unequal variances</article-title>. <source hwp:id="source-14">Biometrics</source>, pages <fpage>971</fpage>-<lpage>990</lpage>, <year>1971</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Efron Bradley"><given-names>Bradley</given-names> <surname>Efron</surname></string-name> and <string-name name-style="western" hwp:sortable="Morris Carl"><given-names>Carl</given-names> <surname>Morris</surname></string-name>. <article-title hwp:id="article-title-13">Combining possibly related estimation problems</article-title>. <source hwp:id="source-15">Journal of the Royal Statistical Society. Series B (Methodological)</source>, pages <fpage>379</fpage>-<lpage>421</lpage>, <year>1973</year>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Rubin Donald B"><given-names>Donald B</given-names> <surname>Rubin</surname></string-name> and <string-name name-style="western" hwp:sortable="Weisberg Sanford"><given-names>Sanford</given-names> <surname>Weisberg</surname></string-name>. <article-title hwp:id="article-title-14">The variance of a linear combination of independent estimators using estimated weights</article-title>. <source hwp:id="source-16">Biometrika</source>, <volume>62</volume>(<issue>3</issue>):<fpage>708</fpage>-<lpage>709</lpage>, <year>1975</year>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Green Edwin J"><given-names>Edwin J</given-names> <surname>Green</surname></string-name> and <string-name name-style="western" hwp:sortable="Strawderman William E"><given-names>William E</given-names> <surname>Strawderman</surname></string-name>. <article-title hwp:id="article-title-15">A james-stein type estimator for combining unbiased and possibly biased estimators</article-title>. <source hwp:id="source-17">Journal of the American Statistical Association</source>, <volume>86</volume>(<issue>416</issue>):<fpage>1001</fpage>-<lpage>1006</lpage>, <year>1991</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Breiman Leo"><given-names>Leo</given-names> <surname>Breiman</surname></string-name>. <article-title hwp:id="article-title-16">Stacked regressions</article-title>. <source hwp:id="source-18">Machine learning</source>, <volume>24</volume>(<issue>1</issue>):<fpage>49</fpage>-<lpage>64</lpage>, <year>1996</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Wolpert David H"><given-names>David H</given-names> <surname>Wolpert</surname></string-name>. <article-title hwp:id="article-title-17">Stacked generalization</article-title>. <source hwp:id="source-19">Neural networks</source>, <volume>5</volume>(<issue>2</issue>):<fpage>241</fpage>-<lpage>259</lpage>, <year>1992</year>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Breiman Leo"><given-names>Leo</given-names> <surname>Breiman</surname></string-name>. <article-title hwp:id="article-title-18">Bagging predictors</article-title>. <source hwp:id="source-20">Machine learning</source>, <volume>24</volume>(<issue>2</issue>):<fpage>123</fpage>-<lpage>140</lpage>, <year>1996</year>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Breiman Leo"><given-names>Leo</given-names> <surname>Breiman</surname></string-name>. <source hwp:id="source-21">Bias, variance, and arcing classifiers</source>. <year>1996</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>[22]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Freund Yoav"><given-names>Yoav</given-names> <surname>Freund</surname></string-name>, <string-name name-style="western" hwp:sortable="Schapire Robert E"><given-names>Robert E</given-names> <surname>Schapire</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-19">Experiments with a new boosting algorithm</article-title>. <source hwp:id="source-22">In ICML</source>, volume <volume>96</volume>, pages <fpage>148</fpage>-<lpage>156</lpage>, <year>1996</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Freund Yoav"><given-names>Yoav</given-names> <surname>Freund</surname></string-name> and <string-name name-style="western" hwp:sortable="Schapire Robert E"><given-names>Robert E</given-names> <surname>Schapire</surname></string-name>. <article-title hwp:id="article-title-20">A decision-theoretic generalization of on-line learning and an application to boosting</article-title>. <source hwp:id="source-23">Journal of computer and system sciences</source>, <volume>55</volume>(<issue>1</issue>):<fpage>119</fpage>-<lpage>139</lpage>, <year>1997</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Mitchison GJ"><given-names>GJ</given-names> <surname>Mitchison</surname></string-name> and <string-name name-style="western" hwp:sortable="Durbin RM"><given-names>RM</given-names> <surname>Durbin</surname></string-name>. <article-title hwp:id="article-title-21">Bounds on the learning capacity of some multi-layer networks</article-title>. <source hwp:id="source-24">Biological Cybernetics</source>, <volume>60</volume>(<issue>5</issue>):<fpage>345</fpage>-<lpage>365</lpage>, <year>1989</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Kwon C"><given-names>C</given-names> <surname>Kwon</surname></string-name> and <string-name name-style="western" hwp:sortable="Oh JH"><given-names>JH</given-names> <surname>Oh</surname></string-name>. <article-title hwp:id="article-title-22">Storage capacities of committee machines with overlapping and non-overlapping receptive fields</article-title>. <source hwp:id="source-25">Journal of Physics A: Mathematical and General</source>, <volume>30</volume>(<issue>18</issue>):<fpage>6273</fpage>, <year>1997</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Monasson Remi"><given-names>Remi</given-names> <surname>Monasson</surname></string-name> and <string-name name-style="western" hwp:sortable="Zecchina Riccardo"><given-names>Riccardo</given-names> <surname>Zecchina</surname></string-name>. <article-title hwp:id="article-title-23">Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks</article-title>. <source hwp:id="source-26">Physical review letters</source>, <volume>75</volume>(<issue>12</issue>):<fpage>2432</fpage>, <year>1995</year>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Urbanczik R"><given-names>R</given-names> <surname>Urbanczik</surname></string-name>. <article-title hwp:id="article-title-24">Storage capacity of the fully-connected committee machine</article-title>. <source hwp:id="source-27">Journal of Physics A: Mathematical and General</source>, <volume>30</volume>(<issue>11</issue>):<fpage>L387</fpage>-<lpage>L392</lpage>, <year>1997</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2 xref-ref-28-3"><label>[28]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Cover Thomas M"><given-names>Thomas M</given-names> <surname>Cover</surname></string-name>. <article-title hwp:id="article-title-25">Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</article-title>. <source hwp:id="source-28">IEEE Transactions on Electronic Computers</source>, <volume>EC-14</volume>(<issue>3</issue>):<fpage>326</fpage>-<lpage>334</lpage>, <year>1965</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Geller Murray"><given-names>Murray</given-names> <surname>Geller</surname></string-name> and <string-name name-style="western" hwp:sortable="Ng EW"><given-names>EW</given-names> <surname>Ng</surname></string-name>. <article-title hwp:id="article-title-26">A table of integrals of the error function. II. Additions and corrections</article-title>. <source hwp:id="source-29">J. Res. Natl. Bur. Stand</source>, <volume>75</volume>:<fpage>149</fpage>-<lpage>163</lpage>, <year>1971</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Gardner E."><given-names>E.</given-names> <surname>Gardner</surname></string-name>. <article-title hwp:id="article-title-27">Maximum storage capacity in neural networks</article-title>. <source hwp:id="source-30">EPL (Europhysics Letters)</source>, <volume>4</volume>(<issue>4</issue>):<fpage>481</fpage>, <year>1987</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><label>[31]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Tsodyks MV"><given-names>MV</given-names> <surname>Tsodyks</surname></string-name> and <string-name name-style="western" hwp:sortable="Feigel’Man MV"><given-names>MV</given-names> <surname>Feigel’Man</surname></string-name>. <article-title hwp:id="article-title-28">The enhanced storage capacity in neural networks with low activity level</article-title>. <source hwp:id="source-31">Europhysics Letters</source>, <volume>6</volume>(<issue>2</issue>):<fpage>101</fpage>-<lpage>105</lpage>, <year>1988</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Amit Daniel J"><given-names>Daniel J</given-names> <surname>Amit</surname></string-name> and <string-name name-style="western" hwp:sortable="Fusi Stefano"><given-names>Stefano</given-names> <surname>Fusi</surname></string-name>. <article-title hwp:id="article-title-29">Learning in neural networks with material synapses</article-title>. <source hwp:id="source-32">Neural Computation</source>, <volume>6</volume>(<issue>5</issue>):<fpage>957</fpage>-<lpage>982</lpage>, <year>1994</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Wang Xiao-Jing"><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name>. <article-title hwp:id="article-title-30">Probabilistic decision making by slow reverberation in cortical circuits</article-title>. <source hwp:id="source-33">Neuron</source>, <volume>36</volume>(<issue>5</issue>):<fpage>955</fpage>-<lpage>968</lpage>, <year>2002</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2 xref-ref-34-3 xref-ref-34-4"><label>[34]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Barak Omri"><given-names>Omri</given-names> <surname>Barak</surname></string-name>, <string-name name-style="western" hwp:sortable="Rigotti Mattia"><given-names>Mattia</given-names> <surname>Rigotti</surname></string-name>, and <string-name name-style="western" hwp:sortable="Fusi Stefano"><given-names>Stefano</given-names> <surname>Fusi</surname></string-name>. <article-title hwp:id="article-title-31">The sparseness of mixed selectivity neurons controls the generalization–discrimination trade-off</article-title>. <source hwp:id="source-34">The Journal of Neuroscience</source>, <volume>33</volume>(<issue>9</issue>):<fpage>3844</fpage>-<lpage>3856</lpage>, <year>2013</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="journal" citation-type="journal" ref:id="157289v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Churchland Mark M"><given-names>Mark M</given-names> <surname>Churchland</surname></string-name>, <string-name name-style="western" hwp:sortable="Byron M Yu"><given-names>M Yu</given-names> <surname>Byron</surname></string-name>, <string-name name-style="western" hwp:sortable="Cunningham John P"><given-names>John P</given-names> <surname>Cunningham</surname></string-name>, <string-name name-style="western" hwp:sortable="Sugrue Leo P"><given-names>Leo P</given-names> <surname>Sugrue</surname></string-name>, <string-name name-style="western" hwp:sortable="Cohen Marlene R"><given-names>Marlene R</given-names> <surname>Cohen</surname></string-name>, <string-name name-style="western" hwp:sortable="Corrado Greg S"><given-names>Greg S</given-names> <surname>Corrado</surname></string-name>, <string-name name-style="western" hwp:sortable="Newsome William t"><given-names>William t</given-names> <surname>Newsome</surname></string-name>, <string-name name-style="western" hwp:sortable="Clark Andrew M"><given-names>Andrew M</given-names> <surname>Clark</surname></string-name>, <string-name name-style="western" hwp:sortable="Hosseini Paymon"><given-names>Paymon</given-names> <surname>Hosseini</surname></string-name>, <string-name name-style="western" hwp:sortable="Scott Benjamin B"><given-names>Benjamin B</given-names> <surname>Scott</surname></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-32">Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title>. <source hwp:id="source-35">Nature neuroscience</source>, <volume>13</volume>(<issue>3</issue>):<fpage>369</fpage>-<lpage>378</lpage>, <year>2010</year>.</citation></ref></ref-list><fn-group hwp:id="fn-group-1"><fn id="fn1" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1"><label>1</label><p hwp:id="p-249">See equation 18 on page 158 in [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>]</p></fn></fn-group></back></article>
