<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/327601</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;327601</article-id><article-id pub-id-type="other" hwp:sub-type="slug">327601</article-id><article-id pub-id-type="other" hwp:sub-type="tag">327601</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Incorporating Context into Language Encoding Models for fMRI</article-title></title-group><author-notes hwp:id="author-notes-1"><fn fn-type="other" hwp:id="fn-1"><p hwp:id="p-1"><monospace><email hwp:id="email-1">shailee@cs.utexas.edu</email>, <email hwp:id="email-2">huth@cs.utexas.edu</email></monospace></p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Jain Shailee"><surname>Jain</surname><given-names>Shailee</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Huth Alexander G"><surname>Huth</surname><given-names>Alexander G</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">Department of Computer Science, The University of Texas at Austin</institution>, Austin, TX 78751</aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Department of Neuroscience, The University of Texas at Austin</institution>, Austin, TX 78751</aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-05-21T14:27:07-07:00">
    <day>21</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-11-21T16:33:11-08:00">
    <day>21</day><month>11</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-05-21T15:58:56-07:00">
    <day>21</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-11-21T16:40:13-08:00">
    <day>21</day><month>11</month><year>2018</year>
  </pub-date><elocation-id>327601</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-05-21"><day>21</day><month>5</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-11-20"><day>20</day><month>11</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-11-21"><day>21</day><month>11</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license hwp:id="license-1"><p hwp:id="p-2">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author’s permission.</p></license></permissions><self-uri xlink:href="327601.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/327601v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="327601.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/327601v2/327601v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/327601v2/327601v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them. Current word embedding-based approaches treat each stimulus word independently and thus ignore the influence of context on language understanding. In this work, we instead build encoding models using rich contextual representations derived from an LSTM language model. Our models show a significant improvement in encoding performance relative to state-of-the-art embeddings in nearly every brain area. By varying the amount of context used in the models and providing the models with distorted context, we show that this improvement is due to a combination of better word embeddings learned by the LSTM language model and contextual information. We are also able to use our models to map context sensitivity across the cortex. These results suggest that LSTM language models learn high-level representations that are related to representations in the human brain.</p></abstract><counts><page-count count="14"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-2">Introduction</title><p hwp:id="p-4">To extract meaning from natural speech, the human brain combines information about each word with previous words, or context. Without context, humans would be unable to understand homonyms, parse phrases, or resolve coreferences. Contextual information must thus be represented in the human cortex. One powerful tool for mapping cortical representations is encoding models, which use features extracted from stimuli to predict brain responses recorded with fMRI. Previous language encoding studies have successfully mapped word-level semantic representations using embedding vectors (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>), but this approach neglects the effect of context by assuming that the response to each word is independent. Overcoming this limitation would require extracting language features that incorporate context. However, no existing hand-designed feature space is well-suited for this task.</p><p hwp:id="p-5">Several recent studies have overcome the limitations of hand-designed feature spaces by instead using representations discovered by supervised deep neural networks. These networks are trained to perform tasks that the human brain performs: recognize objects in visual scenes (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>) or words and musical genres in sounds (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>). Stimuli from fMRI experiments are fed into the pretrained networks and activations from each layer are treated as separate feature spaces for encoding models. These representations prove highly effective at modeling brain responses in the visual and auditory cortex, suggesting that the networks discover representations similar to those in the brain. However, in both cases, representations at both the lowest-level (images, sound spectrograms) and highest-level (image categories, words, music genres) are known <italic toggle="yes">a priori</italic>, and the networks are used to discover intermediate ones. In contrast, for language the highest-level representation is unknown, making this supervised approach inapplicable.</p><p hwp:id="p-6">An alternative is to use self-supervised networks like neural language models (LMs). These models are trained to predict the next word in a sequence based on the previous words (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>), typically using long short-term memory (LSTM) networks (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>). LSTM LMs have been shown to discover high-level semantic and contextual representations that are useful for many natural language processing tasks (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>). Further, a previous study suggests that representations learned by an LM can be useful for modeling MEG brain responses to reading (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>). Thus, it seems plausible that LSTM LMs could discover contextual representations that would be effective for understanding language processing in the brain.</p><p hwp:id="p-7">Another useful property of an LSTM LM is that it can generate many different types of representations that can be used to probe diverse representations in the brain. Firstly, different brain areas are known to have different ‘temporal receptive fields’ that indicate the amount of context they are sensitive to (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>). This effect can be emulated in an LSTM by varying the number of words, or ‘context length’, used to generate representations. Secondly, LSTM LMs have multiple layers that can each capture distinct representations. Some previous natural language processing works use only the last layer (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref>) while others use a linear combination across layers (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref>). However, previous fMRI studies have found that different layers from supervised networks predicted different brain areas (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref>), suggesting that each layer should be tested separately.</p><p hwp:id="p-8">In this work, we make two broad contributions. Firstly, we use representations discovered by an LSTM LM to incorporate context into encoding models that predict fMRI responses to natural, narrative speech (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>). These contextual models perform significantly better at predicting brain responses than previously published word embedding models. Secondly, we compare the effectiveness of models that use different LSTM layers and context lengths. This reveals a hierarchy of brain areas that are sensitive to both different types of contextual information and different temporal receptive field sizes. Our LSTM-based contextual encoding models thus not only outperform the best hand-designed feature spaces for language encoding, but also provide insights into the representation of linguistic context in the cortex.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-9">Contextual language encoding model with narrative stimuli. Each word in the story is first projected into a 985-dimensional embedding space. Sequences of word representations are then fed into an LSTM network that was pretrained as a language model. In this example, we extract representations for each word in the stimulus from layer 2 of the LSTM with context length 1 (i.e. considering only one word before the current word). A low-pass Lanczos filter is used to resample the contextual representations to the low temporal resolution of fMRI, and ridge regression is used to map downsampled contextual representations to fMRI responses. The dotted line indicates the path followed by the traditional, non-contextual approach that relies solely on individual word embeddings.</p></caption><graphic xlink:href="327601_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-3">Natural Language fMRI Experiment</title><p hwp:id="p-10">We use data from an fMRI experiment where the stimulus consisted of 11 naturally spoken narrative stories from The <italic toggle="yes">Moth Radio Hour</italic>, totalling over 2 hours or roughly 23,800 words. These data were used in previous publications and are described in detail there (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>). Each story is transcribed and the transcripts are aligned to the audio, revealing the exact time when each word is spoken. These rich and complex stimuli are highly representative of the language that humans perceive on a daily basis, and understanding these stimuli relies heavily on contextual information.</p><p hwp:id="p-11">Measured brain responses consist of whole-brain blood-oxygen level dependent (BOLD) signals recorded from 6 subjects (2 female) using functional magnetic resonance imaging (fMRI), while they listened to stimuli. Images were obtained using gradient-echo EPI on a 3T Siemens TIM Trio scanner at the UC Berkeley Brain Imaging Center using a 32-channel volume coil, TR = 2.0045 seconds (yielding about 4,000 timepoints for each subject), TE = 31 ms, flip angle = 70 degrees, voxel size = 2.24 × 2.24 × 4.1 mm (slice thickness = 3.5 mm with 18% slice gap), matrix size = 100 × 100, and 30 axial slices. All experiments were approved by the UC Berkeley Committee for the Protection of Human Subjects.</p></sec><sec id="s3" hwp:id="sec-3"><label>3</label><title hwp:id="title-4">Voxelwise encoding models</title><p hwp:id="p-12">Encoding models aim to approximate the function <italic toggle="yes">f</italic>(<italic toggle="yes">S</italic>) → <italic toggle="yes">R</italic> that maps a stimulus <italic toggle="yes">S</italic> to observed brain responses <italic toggle="yes">R</italic> (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>). For natural language, <italic toggle="yes">S</italic> is a continuous string of words <italic toggle="yes">w</italic><sub>1</sub>, <italic toggle="yes">w</italic><sub>2</sub>, …, <italic toggle="yes">w<sub>T</sub></italic>. In fMRI experiments, a separate encoding model <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="327601_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> is typically estimated for each voxel based on a training dataset, {<italic toggle="yes">S<sub>trn</sub>, R<sub>trn</sub></italic>}. To evaluate model performance, the estimated model is used to predict responses in a separate testing dataset, <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="327601_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>. Model performance for a single voxel is computed as the Pearson correlation coefficient between real and predicted responses, <italic toggle="yes">r<sub>j</sub></italic> = corr(<italic toggle="yes">R<sub>test,j</sub>, Ȓ<sub>test,j</sub></italic>).</p><p hwp:id="p-13">In practice, the limited amount and quality of fMRI data cause generic nonlinear function approximators to perform poorly as encoding models. Instead, it is common to use <italic toggle="yes">linearized</italic> models, which assume that <italic toggle="yes">f</italic> is a nonlinear transformation of the stimuli into some feature space followed by a linear projection, <italic toggle="yes">f</italic>(<italic toggle="yes">S</italic>):= <italic toggle="yes">g</italic>(<italic toggle="yes">S</italic>)<sub>(<italic toggle="yes">T,P</italic>)</sub><italic toggle="yes">β</italic><sub>(<italic toggle="yes">P,V</italic>)</sub>, where <italic toggle="yes">g</italic> nonlinearly transforms <italic toggle="yes">S</italic> into a <italic toggle="yes">P</italic>-dimensional feature space, and <italic toggle="yes">β</italic> contains a separate set of <italic toggle="yes">P</italic> linear weights for each of the <italic toggle="yes">V</italic> voxels. The function <italic toggle="yes">g</italic> is typically chosen to extract stimulus features that are thought to be represented in the brain, and the weights <italic toggle="yes">β</italic> are learned using regularized linear regression. Here, we use ridge regression (L2-norm) to estimate weights for all encoding models, as described in a previous publication (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>).</p><p hwp:id="p-14">BOLD responses are low-pass relative to the stimuli that elicited them. To account for this effect we resample and low-pass filter the stimulus representation <italic toggle="yes">g</italic>(<italic toggle="yes">S</italic>)<sub>(<italic toggle="yes">T,P</italic>)</sub> to the same rate as the fMRI acquisition with the use of a Lanczos filter, yielding <italic toggle="yes">g</italic>′(<italic toggle="yes">S</italic>)<sub>(<italic toggle="yes">T′,P</italic>)</sub>. Then, to account for hemodynamic delay we use a finite impulse response (FIR) model with 4 delays (2, 4, 6, and 8 seconds), similar to previous work (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">9</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">8</xref>).</p><sec id="s3a" hwp:id="sec-4"><label>3.1</label><title hwp:id="title-5">Word embeddings for language encoding</title><p hwp:id="p-15">The key question when constructing an encoding model is how to choose the nonlinear transformation <italic toggle="yes">g</italic> that is used to represent the stimuli. The state-of-the-art language encoding model (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-5" hwp:rel-id="ref-9">9</xref>) uses a word embedding space that represents each word <italic toggle="yes">w<sub>i</sub></italic> as a 985-dimensional vector <italic toggle="yes">e<sub>i</sub></italic>. These representations are based on word co-occurrence statistics from a large corpus of English text. (Although not identical, this embedding performs similarly to popular methods such as word2vec (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>).) Word embedding spaces capture the semantic similarity between words, such that words with similar lexical meanings are represented by similar embedding vectors. Encoding models that use word embeddings have proven quite successful at predicting brain responses to both single words (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>) and continuous language (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-6" hwp:rel-id="ref-9">9</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-4" hwp:rel-id="ref-8">8</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>). However, one critical flaw is that these models assume that the brain response to each word is independent of the other words in the stimulus. Additionally, in most experiments the words are presented in isolation as opposed to a naturalistic narrative. These models treat the stimulus as a bag-of-words, ignoring temporal order and dependencies that are known to be significant for language processing in the brain (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref>). Our goal in this work is to find representations that bridge this gap by explicitly considering the order and dependency between words in a narrative.</p></sec></sec><sec id="s4" hwp:id="sec-5"><label>4</label><title hwp:id="title-6">Learning representations of context</title><p hwp:id="p-16">Language models are trained to predict the next word in a sequence given information about previous words, i.e. for some sequence [<italic toggle="yes">w<sub>i−m</sub>, w</italic><sub><italic toggle="yes">i</italic>−<italic toggle="yes">m</italic>+1</sub> … <italic toggle="yes">w<sub>i</sub>, w</italic><sub><italic toggle="yes">i</italic>+1</sub>], they aim to maximize the probability <italic toggle="yes">P</italic>(<italic toggle="yes">w</italic><sub><italic toggle="yes">i</italic>+1</sub>|<italic toggle="yes">w<sub>i−m</sub></italic> … <italic toggle="yes">w<sub>i</sub></italic>). To achieve this in an LSTM LM, representations of the words [<italic toggle="yes">w<sub>i−m</sub></italic> … <italic toggle="yes">w<sub>i</sub></italic>] are effectively combined in a hidden state <italic toggle="yes">h<sub>i</sub></italic> that can be used to predict <italic toggle="yes">w</italic><sub><italic toggle="yes">i</italic>+1</sub>. As a consequence, <italic toggle="yes">h<sub>i</sub></italic> must capture relevant contextual information (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-3" hwp:rel-id="ref-19">19</xref>) and might serve as a plausible candidate for modeling contextual representations in the brain.</p><p hwp:id="p-17">LSTM LMs comprise LSTM cells arranged as a hierarchy of <italic toggle="yes">L</italic> layers. For the word <italic toggle="yes">w<sub>i</sub></italic>, an LSTM cell at layer <italic toggle="yes">l</italic> takes two inputs: a representation of the current word from the previous layer (<italic toggle="yes">h</italic><sub><italic toggle="yes">i,l</italic>−1</sub>), and the state of the current LSTM cell after the previous word (<italic toggle="yes">h</italic><sub><italic toggle="yes">i</italic>−1,<italic toggle="yes">l</italic></sub>). The cell then performs a nonlinear transformation on these inputs to create a new word representation (<italic toggle="yes">h<sub>i,l</sub></italic>) that is both fed to the next layer and used to modify the state of the current cell for the next word. Thus, the hidden state for word <italic toggle="yes">w<sub>i</sub></italic> at layer <italic toggle="yes">l</italic> can be defined as
<disp-formula id="eqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="327601_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
where <italic toggle="yes">e<sub>i</sub></italic> is a pretrained embedding vector that is used as the input representation for <italic toggle="yes">w<sub>i</sub></italic>, and <italic toggle="yes">LSTM</italic> denotes the nonlinear function applied by the LSTM cell on the current and previous words. Therefore, layer 1 makes direct use of the embedding representation while subsequent layers increasingly depend on recurrent states.</p><p hwp:id="p-18">Before the LSTM can be used to extract contextual representations, it must be trained on the language modeling task. Our LSTM LM is pretrained on a corpus of comments scraped from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://reddit.com" ext-link-type="uri" xlink:href="http://reddit.com" hwp:id="ext-link-1">reddit.com</ext-link> (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-7" hwp:rel-id="ref-9">9</xref>), comprising over 20 million words. We chose this particular corpus because the informal and conversational nature of the text is more similar to our stimulus stories than most conventional corpora. The vocabulary is restricted to include all the words in <italic toggle="yes">S</italic> as well as the top 10,000 most frequent words from the corpus. This ensures that the network is aware of every word in <italic toggle="yes">S</italic> and can assign some prediction probability to each. For the input representations <italic toggle="yes">e<sub>i</sub></italic>, we use the 985-dimensional word embedding from the state-of-the-art encoding model (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-8" hwp:rel-id="ref-9">9</xref>). Our selected LSTM LM architecture has <italic toggle="yes">L</italic> = 3 layers, each with 985-dimensional hidden states. This dimensionality was chosen to match that of the input embedding so that every encoding model would have the same number of parameters. The LM is trained on word sequences of length <italic toggle="yes">M</italic> = 20, as we observe little gain in perplexity with longer sequences. The model is implemented in TensorFlow (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>) with cross-entropy loss. We use the RMSProp optimizer (<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>) with an initial learning rate of 0.0005 and inverse time decay at 0.7 decay rate. Additionally, we implement early-stopping when the loss starts to plateau and no dropout. After training the LSTM LM, we evaluate its performance using <italic toggle="yes">S</italic> as a test set. Due to the limited amount of stories and their high diversity, we observe no benefit of fine-tuning the LM on a part of <italic toggle="yes">S</italic> and evaluating on the rest.</p><sec id="s4a" hwp:id="sec-6"><label>4.1</label><title hwp:id="title-7">Extracting contextual information</title><p hwp:id="p-19">Our LSTM LM uses information from up to <italic toggle="yes">M</italic> = 20 previous words to construct <italic toggle="yes">L</italic> = 3 separate context representations. By varying the number of words the LSTM has seen before the representation is extracted, we can produce representations with different ‘temporal receptive field’ sizes (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-3" hwp:rel-id="ref-12">12</xref>). And by extracting representations from different layers we may be able to capture different types of high-level contextual representations (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-4" hwp:rel-id="ref-19">19</xref>).</p><p hwp:id="p-20">We extract <italic toggle="yes">M</italic> × <italic toggle="yes">L</italic> = 60 separate representations of the stimuli, one for each context length and at each layer, by forward-propagating the stimuli through the pretrained LSTM LM. To obtain context representations for a stimulus story <italic toggle="yes">S</italic> = [<italic toggle="yes">w</italic><sub>1</sub>,…, <italic toggle="yes">w<sub>n</sub></italic>] with a desired context length <italic toggle="yes">m</italic> ∈ [0..<italic toggle="yes">M</italic> − 1] and layer <italic toggle="yes">l</italic> ∈ [1..<italic toggle="yes">L</italic>], we use the following procedure. First, for each word <italic toggle="yes">w<sub>i</sub></italic> we extract the previous <italic toggle="yes">m</italic> words to form a length <italic toggle="yes">m</italic> + 1 sequence [<italic toggle="yes">w<sub>i−m</sub></italic>, …, <italic toggle="yes">w<sub>i</sub></italic>]. The <italic toggle="yes">m</italic> context words and word <italic toggle="yes">w<sub>i</sub></italic> are sequentially fed into the pretrained LSTM LM, and then the activations from layer <italic toggle="yes">l</italic> of the LSTM LM are extracted. The resulting 985-dimensional vector combines the information in <italic toggle="yes">w<sub>i</sub></italic> with the previous <italic toggle="yes">m</italic> words through recurrent connections, building an effective contextual representation for <italic toggle="yes">w<sub>i</sub></italic>. This procedure thus allows us to incorporate and experiment with varying temporal fields and contextual information in language encoding.</p></sec></sec><sec id="s5" hwp:id="sec-7"><label>5</label><title hwp:id="title-8">Results</title><p hwp:id="p-21">Here, we evaluate the performance of our contextual encoding models against the state-of-the-art embedding-based model and examine the effect of using different context layers and lengths. We test the importance of context by distorting it and observing the effect on encoding performance. Finally we examine which brain areas are best fit by different context lengths and layers, revealing a temporal hierarchy in the human language processing system. All brain maps are made using pycortex (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>).</p><sec id="s5a" hwp:id="sec-8"><label>5.1</label><title hwp:id="title-9">Language encoding performance for context representations</title><p hwp:id="p-22">After fitting the 60 contextual encoding models (with <italic toggle="yes">M</italic> = 20 different context lengths and <italic toggle="yes">L</italic> = 3 different layers) and a baseline embedding model from (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-9" hwp:rel-id="ref-9">9</xref>), we evaluate model performance by predicting fMRI responses on a test dataset comprising one 10-minute story. Model performance in subject <italic toggle="yes">s</italic> is summarized by first computing <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="327601_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula>, the fraction of variance explained by the model in the <italic toggle="yes">v<sup>th</sup></italic> voxel. Then, we sum across all voxels in subject <italic toggle="yes">s</italic>, giving <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="327601_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula>. Finally, to summarize model performance across the 6 subjects we average all the <italic toggle="yes">r</italic><sup>2</sup> values, <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="327601_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula>. Standard error of the mean (SEM) is used to compute error bars.</p><p hwp:id="p-23"><xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2(A)</xref> shows encoding model performance for each model, averaged across subjects. Contextual models significantly beat the embedding in nearly every model variation. This supports our hypothesis that contextual representations are a better match to the human brain than pure word embeddings. This effect is also seen in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>, which shows the difference in performance between the contextual and embedding models for every voxel in one subject. While some voxels show little difference, most regions are considerably improved with the contextual model.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-24">(<bold>a</bold>) Contextual encoding model performance with different context lengths and layers vs. state-of-the-art embedding. Results are averaged across 6 subjects ± adjusted standard error of the mean. Contextual models from all layers outperform the embedding. Increasing context length uniformly improves performance in every layer, and different performance across layers suggests that each represents different information. Best performance is obtained using layer 2 with long context. (<bold>b</bold>) To show that contextual models do indeed rely on context, we re-fit the encoding models with representations that used random words as context. Random-context models perform worse than the original models, and most perform worse than the embedding, except in layer 1. (<bold>c</bold>) Further, we swap context between different occurrences of each word and re-fit the encoding models. Swapped-context models also perform worse than the original, showing that linguistically plausible but wrong context also hurts the model. (<bold>d</bold>) Performance of the LSTM network on the language modeling task, as a function of context length. LM performance is measured by inverse perplexity (higher is better). Longer context leads to better LM performance.</p></caption><graphic xlink:href="327601_fig2" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-25">Difference between contextual and embedding model performance across cortex. The difference in <italic toggle="yes">r</italic> values is computed for each voxel in one subject (S1) and projected onto the subject’s flattened cortical surface. White outlines show ROIs identified using other experiments. Dashed lines indicate cuts made to the cortical surface during the flattening procedure and the green stars indicate the apex of each cut. Red voxels are best modeled by the contextual model, blue are best modeled by the embedding, and white show no difference. Voxels poorly predicted by both models appear black. Overall, the contextual model performs better, although some voxels have approximately equal performance with both models.</p></caption><graphic xlink:href="327601_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><sec id="s5a1" hwp:id="sec-9"><title hwp:id="title-10">Impact of context length</title><p hwp:id="p-26"><xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2(A)</xref> shows that model performance increases monotonically with context length, for all layers. This suggests the encoding models are successfully exploiting contextual information that the LSTM LM has learned to extract. However, model performance plateaus after 10-15 words, suggesting that the LSTM LM was unable to successfully incorporate contextual information beyond this timescale.</p><p hwp:id="p-27">This result is mirrored by the performance of the LSTM LM on the language modeling task, shown in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2(C)</xref>. Here, we test how well the LSTM LM predicts the next word from context on the stimulus <italic toggle="yes">S</italic>. For each context length <italic toggle="yes">m</italic> we compute inverse perplexity, <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="327601_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula>, where higher values mean better LM performance. The similarity between LM and encoding performance suggests that the ability of an LM to generate useful encoding representations is tied to its language modeling ability. This suggests that better LMs yield better encoding models.</p><p hwp:id="p-28">The encoding model results in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2(A)</xref> also suggest that the improvement of our model can be partially explained by the LSTM LM simply learning a better word embedding than the baseline. Models with zero context length use no contextual information and thus are simply nonlinear transformations of the input embedding. The model for layer 1 with zero context is substantially better than the embedding, suggesting that layer 1 learns to generate a new embedding that is a better match to the brain. However, the best model for each layer uses the longest context, suggesting that contextual information is important above and beyond the improved embedding. We explore this issue further in <xref ref-type="sec" rid="s5b" hwp:id="xref-sec-11-1" hwp:rel-id="sec-11">Section 5.2</xref>.</p></sec><sec id="s5a2" hwp:id="sec-10"><title hwp:id="title-11">Impact of LSTM layers</title><p hwp:id="p-29"><xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Figure 2(A)</xref> also shows that there are differences between models that use different LSTM layers. Further, the effect of context length is different for each layer. Layer 1 predicts substantially better than baseline even with short context, but only improves modestly with context length. Layer 2 predicts roughly the same as baseline with zero context, but jumps sharply with even one word of context and is the best model thereafter. Layer 3 predicts worse than the embedding with short context, but sees the largest improvement with context length. Investigating the differences between representations in different layers is an important direction for future work.</p></sec></sec><sec id="s5b" hwp:id="sec-11" hwp:rev-id="xref-sec-11-1"><label>5.2</label><title hwp:id="title-12">Distorting context representations</title><p hwp:id="p-30"><xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Figure 2(A)</xref> shows that the LSTM LM produces useful representations for encoding. However, it also shows that part of the improvement is due to the LM learning better embeddings, as can be seen by the good performance of layer 1 with zero context. To directly test the importance of context in these models, we conduct two experiments where we degrade the quality of the context.</p><p hwp:id="p-31">In the ‘random’ experiment, we modify the representation extraction procedure so that the context for word <italic toggle="yes">w<sub>i</sub></italic> consists of a randomly chosen, nonsensical sequence of words rather than the words that actually precede <italic toggle="yes">w<sub>i</sub></italic> in the stimulus. This yields a distorted representation for <italic toggle="yes">w<sub>i</sub></italic>, where the amount of distortion scales with the context length. Encoding model performance using these distorted representations is shown in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Figure 2(B)</xref>. It is obvious that the performance greatly suffers from the context distortion. Further, the impact on performance progressively increases across LSTM layers with layer 1 being affected the least, and still beating the baseline. Since the representations beyond layer 1 do not directly make use of the embedding and increasingly rely on contextual information passed through recurrence, the progressive impact across layers is justified. This demonstrates that the LSTM successfully incorporates context, although there are differences across layers.</p><p hwp:id="p-32">In the ‘swapping’ experiment, for each stimulus word and context length we swap the actual context with the context of another occurrence of the same word (e.g. “this is my <italic toggle="yes">dog</italic>” → “I saw the <italic toggle="yes">dog</italic>”). Swapping is done after inferring context vectors using the LSTM but before regression on the fMRI data. For words that only appear once in the stimulus (&lt; 6%), we leave the original context in place. For each context length, we pick the new context that has the fewest words in common with the original. Thus, the swapped contexts picked for each successive length are sometimes different, leading to the increased variance across context lengths. The results in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Figure 2(C)</xref> show that linguistically plausible but wrong contexts also greatly impact encoding performance. Additionally, we observe the same patterns of progressively greater impact across layers as in the random context models. This demonstrates even more effectively than before that the context model is picking up information relevant for cortical representations.</p></sec><sec id="s5c" hwp:id="sec-12"><label>5.3</label><title hwp:id="title-13">Model preference across cortex</title><p hwp:id="p-33">Finally, we use our models to investigate the importance of contextual information for representations in each area of the cortex.</p><sec id="s5c1" hwp:id="sec-13"><title hwp:id="title-14">Context length preference across cortex</title><p hwp:id="p-34">Although longer context produces better encoding models overall, the same is not true for every brain area. To investigate this issue we compute a context length preference index separately for each voxel. This index is defined as the projection of each voxel’s ‘context profile’, a length-<italic toggle="yes">M</italic> vector containing prediction performance for each context length with LSTM layer 2, onto the first principal component of context profiles across all well-predicted voxels (<italic toggle="yes">r</italic> &gt; 0.15). Layer 2 is used because it has the best performance overall; this analysis was repeated for the other layers and yielded similar results. This index was selected because it accounts for the fact that performance plateaus for context lengths between 10 and 20, and it avoids computing a noisy argmax across context lengths.</p><p hwp:id="p-35"><xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref> shows the context length preference for each voxel in one subject. Although most voxels prefer long context, clusters of short-context-preferring voxels appear in auditory cortex (AC), Broca’s area, and left temporo-parietal junction (TPJ). These areas thus seem to selectively represent information about the current word rather than information that is built up with context. This supports the findings of (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-4" hwp:rel-id="ref-12">12</xref>), which showed that similar brain areas respond strongly even when a story is scrambled to remove context. In a previous study, Baldassano et al. (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>) report findings that suggest different temporal scale selectivity across the cortex. Our contextual encoding model captures this timescale variance, but goes further by explicitly modeling the transformation from stimulus to response.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-36">Context length preference across cortex. An index of context length preference is computed for each voxel in one subject (S1) and projected onto that subject’s cortical surface. Voxels shown in blue are best modeled using short context, while red voxels are best modeled with long context. Non-significantly predicted voxels (mean <italic toggle="yes">r</italic> &lt; 0.11) are gray. Insets show model performance with each context length for two representative voxels, one that prefers short context (right) and one long (left). Generally, voxels in low-level language areas (AC) prefer short context, while voxels in higher-level language areas prefer long.</p></caption><graphic xlink:href="327601_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s5c2" hwp:id="sec-14"><title hwp:id="title-15">LSTM layer preference across cortex</title><p hwp:id="p-37">Earlier experiments found strong correspondence between layers of deep supervised networks and brain areas in visual (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">7</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">5</xref>) and auditory (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">11</xref>) cortex. It is unclear whether a similar pattern should be expected for self-supervised networks such as the LSTM LM used here, where there is no clear hierarchy of layers. To examine this issue we visualized layer preference across cortex in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>. Here a color is assigned to each significantly predicted voxel according to the relative encoding performance of each layer. Performance of the layer 1 model is shown using the red component of the color, layer 2 using green, and layer 3 using blue. Non-significantly predicted voxels appear gray, and voxels that are predicted equally well by each layer appear white.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-38">LSTM Layer preference across cortex. An index of layer preference is computed for each voxel in one subject (S1) and then projected onto that subject’s cortical surface. Voxels that are much better-predicted by layer 1 than the others appear red; layer 2, green; and layer 3, blue. Voxels equally well-predicted by all layers appear white. Non-significantly predicted voxels (mean <italic toggle="yes">r</italic> &lt; 0.11) are gray. Insets show model performance with each layer (averaged across context lengths) for two representative voxels, one that slightly prefers layers 1 &amp; 3 (left), and one that slightly prefers layer 2 (right). For most voxels there is little difference in performance from different layers. However, there is a slight preference in AC for layers 1 &amp; 3, and in higher semantic regions for layer 2.</p></caption><graphic xlink:href="327601_fig5" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-39">Overall the performance of all layers is highly similar, rendering most voxels nearly white. Still, layer 2 provides the best predictions by a small margin, giving many voxels a green tint. However, auditory cortex (AC) shows a clear preference for layers 1 and 3, giving it a purple tint. Thus it seems that low-level speech processing (AC) is better modeled by layers 1 and 3, while high-level processing is better modeled by layer 2. This suggests that the middle layer of the LSTM LM is learning the highest-level representations, while layers 1 and 3, which are “closer” to the word embeddings, are learning lower-level representations. This finding is in stark contrast to experiments that used supervised networks (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-4" hwp:rel-id="ref-7">7</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">5</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-4" hwp:rel-id="ref-11">11</xref>). Those experiments found that high-level cortical areas were best modeled by higher layers of the network. Our results suggest that the highest-level representations in self-supervised models might emerge at the layer which is farthest from the input.</p></sec></sec></sec><sec id="s6" hwp:id="sec-15"><label>6</label><title hwp:id="title-16">Conclusions</title><p hwp:id="p-40">In this work, we effectively incorporate context representations for language using an LSTM LM. We observe that representations outperform state-of-the-art embedding based models and also show distinct behavior across different context lengths and LSTM layers. Our findings suggest that these models do indeed incorporate context and temporal order, albeit differently across layers. Finally, we show how our models explain the differences in language processing across different cortical regions, from low-level to high-level language areas.</p><p hwp:id="p-41">As future work, we would like to explore and understand the information captured in the contextual representations that helps to model language processing in the cortex. Additionally, it would be worth exploring the roles played by different LSTM gates in developing these representations while incorporating context.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-17">Acknowledgments</title><p hwp:id="p-42">We thank Jack Gallant, Wendy de Heer, Frederic Theunissen, and Thomas Griffiths for helping design the fMRI experiment and collect the data used here; Brittany Griffin and Anwar Nuñez for segmenting and flattening cortical surfaces; and Niko Kriegeskorte for useful discussions. Data collection was supported by NSF grant IIS-1208203 and NIH NEI grant EY019684-01A1. This work was supported by grants from the Burroughs-Wellcome Fund and NVIDIA. We also acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-18">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="book" citation-type="book" ref:id="327601v2.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Abadi M."><surname>Abadi</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barham P."><surname>Barham</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen J."><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen Z."><surname>Chen</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davis A."><surname>Davis</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dean J."><surname>Dean</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Devin M."><surname>Devin</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ghemawat S."><surname>Ghemawat</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Irving G."><surname>Irving</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Isard M."><surname>Isard</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kudlur M."><surname>Kudlur</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Levenberg J."><surname>Levenberg</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Monga R."><surname>Monga</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moore S."><surname>Moore</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Murray D.G."><surname>Murray</surname>, <given-names>D.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Steiner B."><surname>Steiner</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tucker P."><surname>Tucker</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vasudevan V."><surname>Vasudevan</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Warden P."><surname>Warden</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wicke M."><surname>Wicke</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yu Y."><surname>Yu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zheng X."><surname>Zheng</surname>, <given-names>X.</given-names></string-name>: <chapter-title>Tensorflow: A system for large-scale machine learning</chapter-title>. In: <source hwp:id="source-1">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation.</source> pp. <fpage>265</fpage>–<lpage>283</lpage>. OSDI’16, <publisher-name>USENIX Association</publisher-name>, <publisher-loc>Berkeley, CA, USA</publisher-loc> (<year>2016</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dl.acm.org/citation.cfm?id=3026877.3026899" ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=3026877.3026899" hwp:id="ext-link-2">http://dl.acm.org/citation.cfm?id=3026877.3026899</ext-link></citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>[2]</label><citation publication-type="other" citation-type="journal" ref:id="327601v2.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Agrawal P."><surname>Agrawal</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stansbury D."><surname>Stansbury</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Malik J."><surname>Malik</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name>: <article-title hwp:id="article-title-2">Pixels to voxels: Modeling visual representation in the human brain</article-title>. CoRR abs/1407.5104 (<year>2014</year>)</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Baldassano C."><surname>Baldassano</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen J."><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zadbood A."><surname>Zadbood</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pillow J.W."><surname>Pillow</surname>, <given-names>J.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hasson U."><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norman K.A."><surname>Norman</surname>, <given-names>K.A.</given-names></string-name>: <article-title hwp:id="article-title-3">Discovering event structure in continuous narrative perception and memory</article-title>. <source hwp:id="source-2">Neuron</source> <volume>95</volume>(<issue>3</issue>), <fpage>709</fpage>–<lpage>721.e5</lpage> (<year>2017</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.sciencedirect.com/science/article/pii/s0896627317305937" ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/s0896627317305937" hwp:id="ext-link-3">http://www.sciencedirect.com/science/article/pii/S0896627317305937</ext-link></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ducharme R."><surname>Ducharme</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vincent P."><surname>Vincent</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Janvin C."><surname>Janvin</surname>, <given-names>C.</given-names></string-name>: <article-title hwp:id="article-title-4">A neural probabilistic language model</article-title>. <source hwp:id="source-3">J. Mach. Learn. Res.</source> <volume>3</volume>, <fpage>1137</fpage>–<lpage>1155</lpage> (<month>Mar</month> <year>2003</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dl.acm.org/citation.cfm?id=944919.944966" ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=944919.944966" hwp:id="ext-link-4">http://dl.acm.org/citation.cfm?id=944919.944966</ext-link></citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Eickenberg M."><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gramfort A."><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varoquaux G."><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thirion B."><surname>Thirion</surname>, <given-names>B.</given-names></string-name>: <article-title hwp:id="article-title-5">Seeing it all: Convolutional network layers map the function of the human visual system</article-title>. <source hwp:id="source-4">NeuroImage</source> <volume>152</volume>(<issue>Supplement C</issue>), <fpage>184</fpage>–<lpage>194</lpage> (<year>2017</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.sciencedirect.com/science/article/pii/s1053811916305481" ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/s1053811916305481" hwp:id="ext-link-5">http://www.sciencedirect.com/science/article/pii/S1053811916305481</ext-link></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Gao J.S."><surname>Gao</surname>, <given-names>J.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huth A.G."><surname>Huth</surname>, <given-names>A.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lescroart M.D."><surname>Lescroart</surname>, <given-names>M.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name>: <article-title hwp:id="article-title-6">Pycortex: an interactive surface visualizer for fmri</article-title>. <source hwp:id="source-5">Frontiers in Neuroinformatics</source> <volume>9</volume>, <fpage>23</fpage> (<year>2015</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.frontiersin.org/article/10.3389/fninf.2015.00023" ext-link-type="uri" xlink:href="https://www.frontiersin.org/article/10.3389/fninf.2015.00023" hwp:id="ext-link-6">https://www.frontiersin.org/article/10.3389/fninf.2015.00023</ext-link></citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3 xref-ref-7-4"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Güçlü U."><surname>Güçlü</surname>, <given-names>U.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Gerven M.A.J."><surname>van Gerven</surname>, <given-names>M.A.J.</given-names></string-name>: <article-title hwp:id="article-title-7">Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source hwp:id="source-6">Journal of Neuroscience</source> <volume>35</volume>(<issue>27</issue>), <fpage>10005</fpage>–<lpage>10014</lpage> (<year>2015</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.jneurosci.org/content/35/27/10005" ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/35/27/10005" hwp:id="ext-link-7">http://www.jneurosci.org/content/35/27/10005</ext-link></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3 xref-ref-8-4"><label>[8]</label><citation publication-type="website" citation-type="web" ref:id="327601v2.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="de Heer W.A."><surname>de Heer</surname>, <given-names>W.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huth A.G."><surname>Huth</surname>, <given-names>A.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Griffiths T.L."><surname>Griffiths</surname>, <given-names>T.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Theunissen F.E."><surname>Theunissen</surname>, <given-names>F.E.</given-names></string-name>: <article-title hwp:id="article-title-8">The hierarchical cortical organization of human speech processing</article-title>. <source hwp:id="source-7">Journal of Neuroscience</source> (<year>2017</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.jneurosci.org/content/early/2017/06/06/jneurosci.3267-16.2017" ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/early/2017/06/06/jneurosci.3267-16.2017" hwp:id="ext-link-8">http://www.jneurosci.org/content/early/2017/06/06/JNEUROSCI.3267-16.2017</ext-link></citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4 xref-ref-9-5 xref-ref-9-6 xref-ref-9-7 xref-ref-9-8 xref-ref-9-9"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Huth A.G."><surname>Huth</surname>, <given-names>A.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="de Heer W.A."><surname>de Heer</surname>, <given-names>W.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Griffiths T.L."><surname>Griffiths</surname>, <given-names>T.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Theunissen F.E."><surname>Theunissen</surname>, <given-names>F.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name>: <article-title hwp:id="article-title-9">Natural speech reveals the semantic maps that tile human cerebral cortex</article-title>. <source hwp:id="source-8">Nature</source> <volume>532</volume>(<issue>7600</issue>), <fpage>453</fpage>–<lpage>458</lpage> (<year>2016</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/nature17637" ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature17637" hwp:id="ext-link-9">https://doi.org/10.1038/nature17637</ext-link></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Kay K."><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prenger R."><surname>Prenger</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J."><surname>Gallant</surname>, <given-names>J.</given-names></string-name>: <article-title hwp:id="article-title-10">Identifying natural images from human brain activity</article-title>. <source hwp:id="source-9">Nature</source> <volume>452</volume>(<issue>7185</issue>), <fpage>352</fpage>–<lpage>355</lpage> (<day>3</day> <year>2008</year>)</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3 xref-ref-11-4"><label>[11]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Kell A.J."><surname>Kell</surname>, <given-names>A.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamins D.L."><surname>Yamins</surname>, <given-names>D.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shook E.N."><surname>Shook</surname>, <given-names>E.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norman-Haignere S.V."><surname>Norman-Haignere</surname>, <given-names>S.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McDermott J.H."><surname>McDermott</surname>, <given-names>J.H.</given-names></string-name>: <article-title hwp:id="article-title-11">A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title>. <source hwp:id="source-10">Neuron</source> <volume>98</volume>(<issue>3</issue>), <fpage>630</fpage>–<lpage>644.e16</lpage> (<year>2018</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.sciencedirect.com/science/article/pii/s0896627318302502" ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/s0896627318302502" hwp:id="ext-link-10">http://www.sciencedirect.com/science/article/pii/S0896627318302502</ext-link></citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2 xref-ref-12-3 xref-ref-12-4"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Lerner Y."><surname>Lerner</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Honey C.J."><surname>Honey</surname>, <given-names>C.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Silbert L.J."><surname>Silbert</surname>, <given-names>L.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hasson U."><surname>Hasson</surname>, <given-names>U.</given-names></string-name>: <article-title hwp:id="article-title-12">Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title>. <source hwp:id="source-11">Journal of Neuroscience</source> <volume>31</volume>(<issue>8</issue>), <fpage>2906</fpage>–<lpage>2915</lpage> (<year>2011</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.jneurosci.org/content/31/8/2906" ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/31/8/2906" hwp:id="ext-link-11">http://www.jneurosci.org/content/31/8/2906</ext-link></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>[13]</label><citation publication-type="other" citation-type="journal" ref:id="327601v2.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="McCann B."><surname>McCann</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bradbury J."><surname>Bradbury</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xiong C."><surname>Xiong</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Socher R."><surname>Socher</surname>, <given-names>R.</given-names></string-name>: <article-title hwp:id="article-title-13">Learned in translation: Contextualized word vectors</article-title>. CoRR abs/1708.00107 (<year>2017</year>)</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="book" citation-type="book" ref:id="327601v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Mikolov T."><surname>Mikolov</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen K."><surname>Chen</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Corrado G.S."><surname>Corrado</surname>, <given-names>G.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dean J."><surname>Dean</surname>, <given-names>J.</given-names></string-name>: <chapter-title>Distributed representations of words and phrases and their compositionality</chapter-title>. In: <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Burges C.J.C."><surname>Burges</surname>, <given-names>C.J.C.</given-names></string-name></person-group>, <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Bottou L."><surname>Bottou</surname>, <given-names>L.</given-names></string-name></person-group>, <person-group person-group-type="editor" hwp:id="person-group-3"><string-name name-style="western" hwp:sortable="Welling M."><surname>Welling</surname>, <given-names>M.</given-names></string-name></person-group>, <person-group person-group-type="editor" hwp:id="person-group-4"><string-name name-style="western" hwp:sortable="Ghahramani Z."><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name></person-group>, <person-group person-group-type="editor" hwp:id="person-group-5"><string-name name-style="western" hwp:sortable="Weinberger K.Q."><surname>Weinberger</surname>, <given-names>K.Q.</given-names></string-name></person-group> (eds.) <source hwp:id="source-12">Advances in Neural Information Processing Systems</source> <volume>26</volume>, pp. <fpage>3111</fpage>–<lpage>3119</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>. (<year>2013</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" hwp:id="ext-link-12">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</ext-link></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Mitchell T.M."><surname>Mitchell</surname>, <given-names>T.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shinkareva S.V."><surname>Shinkareva</surname>, <given-names>S.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carlson A."><surname>Carlson</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chang K.M."><surname>Chang</surname>, <given-names>K.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Malave V.L."><surname>Malave</surname>, <given-names>V.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mason R.A."><surname>Mason</surname>, <given-names>R.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Just M.A."><surname>Just</surname>, <given-names>M.A.</given-names></string-name>: <article-title hwp:id="article-title-14">Predicting human brain activity associated with the meanings of nouns</article-title>. <source hwp:id="source-13">Science</source> <volume>320</volume>(<issue>5880</issue>), <fpage>1191</fpage>–<lpage>1195</lpage> (<year>2008</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://science.sciencemag.org/content/320/5880/1191" ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/320/5880/1191" hwp:id="ext-link-13">http://science.sciencemag.org/content/320/5880/1191</ext-link></citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="other" citation-type="journal" ref:id="327601v2.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kay K."><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nishimoto S."><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J."><surname>Gallant</surname>, <given-names>J.</given-names></string-name>: <article-title hwp:id="article-title-15">Encoding and decoding in fmri</article-title> <volume>56</volume>, <fpage>400</fpage>–<lpage>10</lpage> (<day>05</day> <year>2011</year>)</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Pereira F."><surname>Pereira</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lou B."><surname>Lou</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pritchett B."><surname>Pritchett</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ritter S."><surname>Ritter</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S.J."><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kanwisher N."><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick M."><surname>Botvinick</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fedorenko E."><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name>: <article-title hwp:id="article-title-16">Toward a universal decoder of linguistic meaning from brain activation</article-title>. <source hwp:id="source-14">Nature communications</source> <volume>9</volume>(<issue>1</issue>), <fpage>963</fpage> (<year>2018</year>)</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><label>[18]</label><citation publication-type="book" citation-type="book" ref:id="327601v2.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Peters M.E."><surname>Peters</surname>, <given-names>M.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ammar W."><surname>Ammar</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhagavatula C."><surname>Bhagavatula</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Power R."><surname>Power</surname>, <given-names>R.</given-names></string-name>: <chapter-title>Semi-supervised sequence tagging with bidirectional language models</chapter-title>. In: <source hwp:id="source-15">ACL</source> (<issue>1</issue>). pp. <fpage>1756</fpage>–<lpage>1765</lpage>. <publisher-name>Association for Computational Linguistics</publisher-name> (<year>2017</year>)</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2 xref-ref-19-3 xref-ref-19-4"><label>[19]</label><citation publication-type="website" citation-type="web" ref:id="327601v2.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Peters M.E."><surname>Peters</surname>, <given-names>M.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Neumann M."><surname>Neumann</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iyyer M."><surname>Iyyer</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gardner M."><surname>Gardner</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Clark C."><surname>Clark</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee K."><surname>Lee</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zettlemoyer L."><surname>Zettlemoyer</surname>, <given-names>L.</given-names></string-name>: <article-title hwp:id="article-title-17">Deep contextualized word representations</article-title>. CoRR abs/1802.05365 (<year>2018</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="http://arxiv.org/abs/1802.05365" ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.05365" hwp:id="ext-link-14">http://arxiv.org/abs/1802.05365</ext-link></citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="other" citation-type="journal" ref:id="327601v2.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Ruder S."><surname>Ruder</surname>, <given-names>S.</given-names></string-name>: <article-title hwp:id="article-title-18">An overview of gradient descent optimization algorithms</article-title>. CoRR abs/1609.04747 (<year>2016</year>)</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="confproc" citation-type="confproc" ref:id="327601v2.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Sundermeyer M."><surname>Sundermeyer</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schlüter R."><surname>Schlüter</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ney H."><surname>Ney</surname>, <given-names>H.</given-names></string-name>: <article-title hwp:id="article-title-19">Lstm neural networks for language modeling</article-title>. In: <conf-name>Thirteenth Annual Conference of the International Speech Communication Association</conf-name> (<year>2012</year>)</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>[22]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Wehbe L."><surname>Wehbe</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Murphy B."><surname>Murphy</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Talukdar P."><surname>Talukdar</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fyshe A."><surname>Fyshe</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ramdas A."><surname>Ramdas</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mitchell T."><surname>Mitchell</surname>, <given-names>T.</given-names></string-name>: <article-title hwp:id="article-title-20">Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses</article-title>. <source hwp:id="source-16">PLOS ONE</source> <volume>9</volume>(<issue>11</issue>), <fpage>1</fpage>–<lpage>19</lpage> (<day>11</day> <year>2014</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1371/journal.pone.0112575" ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0112575" hwp:id="ext-link-15">https://doi.org/10.1371/journal.pone.0112575</ext-link></citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="other" citation-type="journal" ref:id="327601v2.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Wehbe L."><surname>Wehbe</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vaswani A."><surname>Vaswani</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knight K."><surname>Knight</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mitchell T.M."><surname>Mitchell</surname>, <given-names>T.M.</given-names></string-name>: <article-title hwp:id="article-title-21">Aligning context-based statistical models of language with brain activity during reading</article-title>. In: <source hwp:id="source-17">EMNLP</source> (<year>2014</year>)</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="327601v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Wu M.C.K."><surname>Wu</surname>, <given-names>M.C.K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="David S.V."><surname>David</surname>, <given-names>S.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name>: <article-title hwp:id="article-title-22">Complete functional characterization of sensory neurons by system identification</article-title>. <source hwp:id="source-18">Annual Review of Neuroscience</source> <volume>29</volume>(<issue>1</issue>), <fpage>477</fpage>–<lpage>505</lpage> (<year>2006</year>), <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1146/annurev.neuro.29.051605.113024" ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.29.051605.113024" hwp:id="ext-link-16">https://doi.org/10.1146/annurev.neuro.29.051605.113024</ext-link>, pMID: <pub-id pub-id-type="pMID">16776594</pub-id></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>[25]</label><citation publication-type="confproc" citation-type="confproc" ref:id="327601v2.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Xu H."><surname>Xu</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Murphy B."><surname>Murphy</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fyshe A."><surname>Fyshe</surname>, <given-names>A.</given-names></string-name>: <article-title hwp:id="article-title-23">Brainbench: A brain-image test suite for distributional semantic models</article-title>. In: <conf-name>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</conf-name>. pp. <fpage>2017</fpage>–<lpage>2021</lpage> (<year>2016</year>)</citation></ref></ref-list><sec sec-type="supplementary-material" hwp:id="sec-16"><title hwp:id="title-19">Supplement for Incorporating Context into Language Encoding Models for fMRI</title><p hwp:id="p-43">The supplementary shows additional subject flatmaps for the experiments in the main paper. For each subject, the top panel corresponds to <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3</xref> in the main text and depicts the relative performance between the contextual and embedding-based encoding models. The middle panel corresponds to <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref> in the main text and shows the timescale selectivity across the cortex. Finally, the last panel corresponds to <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5</xref> and depicts the LSTM layer preference across the cortex. The observed patterns are consistent across subjects, proving that the contextual representations extracted from an LSTM LM are effective for language encoding. Additionally, the contextualized encoding model can capture known timescale variance across the cortex.</p><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Subject S2</label><graphic xlink:href="327601_figS2" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><fig id="figS3" position="float" orientation="portrait" fig-type="figure" hwp:id="F7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Subject S3</label><graphic xlink:href="327601_figS3" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><fig id="figS4" position="float" orientation="portrait" fig-type="figure" hwp:id="F8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;327601v2/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figS4</object-id><label>Subject S4</label><graphic xlink:href="327601_figS4" position="float" orientation="portrait" hwp:id="graphic-9"/></fig></sec></back></article>
