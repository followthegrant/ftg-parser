<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.07.08.451706</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.08.451706v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.07.08.451706</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.07.08.451706</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.07.08.451706</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.07.08.451706</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Luminance and contrast of images in the THINGS database</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author address: Queensland Brain Institute, Building 79, University of Queensland, St Lucia 4072, Australia, Corresponding author email: <email hwp:id="email-1">willjharri@gmail.com</email></corresp><fn fn-type="others" hwp:id="fn-1"><p hwp:id="p-1">Data and code repository: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/v8a3q/" ext-link-type="uri" xlink:href="https://osf.io/v8a3q/" hwp:id="ext-link-1">https://osf.io/v8a3q/</ext-link></p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6408-0359</contrib-id><name name-style="western" hwp:sortable="Harrison William J"><surname>Harrison</surname><given-names>William J</given-names></name><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-6408-0359"/></contrib><aff hwp:id="aff-1"><institution hwp:id="institution-1">Queensland Brain Institute and School of Psychology, The University of Queensland</institution></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-07-09T18:30:14-07:00">
    <day>9</day><month>7</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-10-24T23:03:11-07:00">
    <day>24</day><month>10</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-07-09T18:35:26-07:00">
    <day>9</day><month>7</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-10-24T23:09:17-07:00">
    <day>24</day><month>10</month><year>2021</year>
  </pub-date><elocation-id>2021.07.08.451706</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-07-08"><day>08</day><month>7</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-10-24"><day>24</day><month>10</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-10-24"><day>24</day><month>10</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="ext-link-2">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></p></license></permissions><self-uri xlink:href="451706.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.07.08.451706v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="451706.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.07.08.451706v2/2021.07.08.451706v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.07.08.451706v2/2021.07.08.451706v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">The THINGS database is a freely available stimulus set that has the potential to facilitate the generation of theory that bridges multiple areas within cognitive neuroscience. The database consists of 26,107 high quality digital photos that are sorted into 1,854 concepts. While a valuable resource, relatively few technical details relevant to the design of studies in cognitive neuroscience have been described. We present an analysis of two key low-level properties of THINGS images, luminance and luminance contrast. These image statistics are known to influence common physiological and neural correlates of perceptual and cognitive processes. In general, we found that the distributions of luminance and contrast are in close agreement with the statistics of natural images reported previously. However, we found that image concepts are separable in their luminance and contrast: we show that luminance and contrast alone are sufficient to classify images into their concepts with above chance accuracy. We describe how these factors may confound studies using the THINGS images, and suggest simple controls that can be implemented a priori or post-hoc. We discuss the importance of using such natural images as stimuli in psychological research.</p></abstract><counts><page-count count="20"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-4">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-3">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-2"><p hwp:id="p-5">In addition to changes to improve clarity of writing, the major revision involved changing how images were classified based on luminance and contrast. I now use a leave-one-out approach that prevents circularity in this analysis and provides an unbiased estimate of classification accuracy that generalises to new, unseen images. All analysis files in the repository reflect this update.</p></fn></fn-group><fn-group content-type="external-links" hwp:id="fn-group-2"><fn fn-type="dataset" hwp:id="fn-3"><p hwp:id="p-6">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/v8a3q/?view_only=656f7f3953c2465ba2435793b1a6a478" ext-link-type="uri" xlink:href="https://osf.io/v8a3q/?view_only=656f7f3953c2465ba2435793b1a6a478" hwp:id="ext-link-3">https://osf.io/v8a3q/?view_only=656f7f3953c2465ba2435793b1a6a478</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-4">Introduction</title><p hwp:id="p-7">In recent decades, advances in computational power and analytical methods have resulted in a growing emphasis on collecting, curating, and sharing large datasets in all areas of science. So called “big data” provides an opportunity to uncover relatively high-order structure through exploratory analyses, experimentation, or a combination of both (e.g. <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Hebart et al., 2020</xref>). The THINGS database is an image database of 26,107 high quality digital photos that have been sorted into 1,854 labelled “concepts” (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Hebart et al., 2019</xref>). This database is a massive stimulus set that has the potential to advance understanding of perceptual and cognitive processes associated with more complex stimuli than typically used in lab-based settings (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Carandini et al., 2005</xref>). The THINGS images are freely available (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.17605/osf.io/jum2f" ext-link-type="uri" xlink:href="http://doi.org/10.17605/osf.io/jum2f" hwp:id="ext-link-4">http://doi.org/10.17605/osf.io/jum2f</ext-link>), but relatively few technical details relevant to the design of studies in cognitive neuroscience have been described. The aim of the present report is thus to provide a summary of variation in luminance and luminance contrast within the THINGS database. We anticipate that these data will inform and constrain the design of studies in which luminance and contrast are expected to correlate with dependent variables of primary interest.</p><p hwp:id="p-8">A great deal of visual neuroscientific theory has been developed from studies that vary the luminance and luminance contrast properties of stimuli. Indeed, there is a rich history of describing the responses of individual visual neurons as a function of contrast (e.g. <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Hubel &amp; Wiesel, 1959</xref>). Much is also known about the ways in which luminance and contrast influence perception of abstract stimuli (e.g. <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Campbell &amp; Robson, 1968</xref>) and natural images (e.g. <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Geisler, 2008</xref>). However, the purpose of this brief summary is not to provide an in-depth review of the role of basic image properties in perception and cognition – this information is available in most psychology textbooks as well as many review papers (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Graham, 1989</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Pelli &amp; Bex, 2013</xref>). Instead, we emphasise that it is well understood that the most fundamental of visual processes are highly contingent on changes in luminance and contrast. In many experimental designs, therefore, it is important to control for these image attributes if they are not of interest to the researcher.</p><p hwp:id="p-9">The THINGS database has already been leveraged to advance understanding of perception and cognition. By combining human judgements of similarity with computational modelling, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">Hebart et al (2020)</xref> quantified the psychological dimensions on which natural images vary. Their model describes 49 attributes that capture variations in conceptual and perceptual properties of the THINGS images. We recently used the THINGS images to investigate how a set of image statistics contribute to observers’ ability to detect targets in natural images (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Rideaux et al., 2021</xref>). In both studies, variability in low-level image properties are directly or indirectly related to the researchers’ hypotheses. However, unquantified contrast differences across THINGS images may confound experimental manipulations in other cases. <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Grootswagers et al (2021)</xref> recently made publicly available the THINGS-EEG database, which includes the electroencephalography (EEG) responses of 50 participants to 22,248 THINGS images, with 12 images from each of all 1,854 concepts in the database. EEG responses are known to be influenced by image contrast (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Campbell &amp; Maffei, 1970</xref>), and so analyses of this dataset may require careful treatment of the metrics we report. A further aim of the present report, therefore, is to investigate whether luminance and contrast differences across THINGS images are sufficient for above-chance decoding of image concepts.</p><p hwp:id="p-10">Differences in luminance and luminance contrast across ten randomly sampled THINGS images can be seen in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>. Although the THINGS images are full colour (24-bit; top row), we quantify achromatic variations only, as shown by the greyscale images in the middle row. The bottom row shows the log amplitude of the Fourier transformed images. The differences in distributions of contrast energy are somewhat unsurprising, given that the amplitude simply re-expresses the image (albeit in a phase-invariance format). The analyses described below systematically quantify 1) general patterns of luminance and contrast differences across the images and concepts, and 2) the degree to which concepts can be predicted from item-level differences in the luminance and contrast of THINGS images.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.08.451706v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-11">Example images from the THINGS database. Top row: images from 10 randomly chosen concepts. Middle row: greyscale versions of the example images. Bottom row: log amplitude of the Fourier-transformed images, rescaled between 0 (black) and 1 (white).</p></caption><graphic xlink:href="451706v2_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-5">Methods</title><p hwp:id="p-12">All data and code are shared via the Open Science Framework: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/v8a3q/" ext-link-type="uri" xlink:href="https://osf.io/v8a3q/" hwp:id="ext-link-5">https://osf.io/v8a3q/</ext-link>. The THINGS database can be accessed via: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.17605/osf.io/jum2f" ext-link-type="uri" xlink:href="http://doi.org/10.17605/osf.io/jum2f" hwp:id="ext-link-6">http://doi.org/10.17605/osf.io/jum2f</ext-link>.</p><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-6">Image pre-processing</title><p hwp:id="p-13">We converted each image in the THINGS database to greyscale using MATLAB’s (MathWorks) rgb2gray() function, converted the image from a uint8 to double, and divided all values by 255 to initially set the range of the image to fall between 0 and 1. Images were rescaled to 512 × 512 pixels using Matlab’s imresize() function using bicubic interpolation. The minimum image size for inclusion in the original THINGS dataset was 480 × 480 pixels, but most images were 600 × 600 or larger. Therefore, resizing to 512 × 512 resulted in a size decrease for the vast majority of images. We selected this size because it is smaller than the majority of THINGS images while being an integer power of two, the combination of which make some of our image processing analyses more efficient in the frequency domain. We discuss this issue further in the Discussion. Due to pixel interpolation, some values of the resized images fell below or above 0 and 1, respectively. Such values were clipped.</p><p hwp:id="p-14">Digital images are typically encoded with a compressive nonlinearity to over-represent relatively darker tones, and we assumed the same is true of the THINGS images (in a non-exhaustive search, we did not find any images that included the colour profile in the metadata). We therefore linearised the images prior to measuring their image spectra via gamma-correction:
<disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="451706v2_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
</p><p hwp:id="p-15">Where <italic toggle="yes">I</italic> is a source image in the range 0 – 1, <italic toggle="yes">γ</italic> is the assumed encoding gamma, and <italic toggle="yes">i</italic> is the linearised image. We set <italic toggle="yes">γ</italic> to 2 based on the findings of <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Bex et al (2005)</xref> who showed that this value provides comparable amplitude spectra as correctly calibrated images. The other terms in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">Equation 1</xref> scale the image from -1 to 1, which is convenient for interpreting the following measures. Error in the estimate of <italic toggle="yes">γ</italic> should be inconsequential to the perceptual appearance of images (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">Bex et al., 2005</xref>), but will result in small errors in the luminance calculations performed here; we are not aware of a better alternative without ground truth knowledge of each image’s encoding function. Varying <italic toggle="yes">γ</italic> is unlikely to affect the general patterns of data or conclusions we report. Perhaps more important is that an experimenter selects a consistent method when measuring images and for the correct linearised display of images in the lab.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-7">Luminance</title><p hwp:id="p-16">Luminance is a measure of the intensity of light from a given area of space after the light has passed through a model of the sensitivity of a standard eye. Although environmental luminance can be approximated from calibrated digital images using metadata regarding the camera’s encoding functions (e.g. <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Frazor &amp; Geisler, 2006</xref>), we do not have access to such information for the THINGS database. When considering the use of these images for lab-based experiments, however, the critical luminance values are those of the digital image, and the luminance of pixels on a computer display is perfectly correlated with the pixel values. With the image in the range -1 to 1 (see <italic toggle="yes">Image pre-processing</italic>, above), we express relative luminance as varying linearly from the darkest black possible tone on a given monitor, to mid-grey, to the most luminous tone possible (i.e. values -1, 0, and 1, respectively). Mean luminance, therefore, is simply the mean of an image’s pixel values:
<disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="451706v2_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
</p><p hwp:id="p-17">Where <italic toggle="yes">k</italic> is a pixel index, <italic toggle="yes">N</italic> is the total number of pixels, and <italic toggle="yes">L</italic> is the mean luminance.</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-8">RMS contrast</title><p hwp:id="p-18">Luminance contrast can be measured in different ways. <xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Bex and Makous (2002)</xref> found that root-mean-square (RMS) contrast was the best predictor of observers’ sensitivity to natural images. RMS contrast (<italic toggle="yes">C</italic><sub><italic toggle="yes">rms</italic></sub>) is simply the standard deviation of the luminance (i.e. pixel) values:
<disp-formula id="eqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="451706v2_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
</p><p hwp:id="p-19">There are other methods with which to quantify contrast, such as finding the difference between the maximum and minimum luminance values, or finding the difference between the maximum absolute value and the average value. For simplicity, however, we quantify only RMS contrast across the full image. We also use other measures of contrast to quantify energy in separable spatial frequency and orientation bands, as we describe below.</p></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-9">Correlation between luminance and RMS contrast</title><p hwp:id="p-20">We quantified the relationship between luminance and RMS contrast using a generalised linear multilevel model (GLMM) to predict contrast from luminance. The model can be written as:
<disp-formula id="eqn4" hwp:id="disp-formula-4" hwp:rev-id="xref-disp-formula-4-1">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="451706v2_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
</p><p hwp:id="p-21">Where <italic toggle="yes">β</italic><sub><italic toggle="yes">n</italic></sub> are the beta weights, <italic toggle="yes">g</italic> is the grouping factor (image concept), and <italic toggle="yes">Ĉ</italic><sub><italic toggle="yes">rms</italic></sub> is the estimated RMS contrast. We used a multilevel model because it uses the grouping of images into concepts to improve the estimate of the relationship between luminance and contrast: it quantifies the relationship between luminance and contrast for each concept simultaneously, while also diminishing the influence of concepts that are relatively highly variable in these measures (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Gelman &amp; Hill, 2007</xref>). We fit the model using Matlab’s fitglme() function.</p></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-10">Spatial frequency filtered contrast</title><p hwp:id="p-22">An image’s contrast energy at any given spatial frequency (and/or orientation) can be quantified according to amplitude in the frequency domain (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref>, bottom row). To compute contrast energy as a function of spatial frequency, therefore, we simply grouped the Fourier-transformed amplitude values according to their distances from the origin and found the mean of each group. Fourier amplitude is calculated as the absolute of the Fourier-transformed complex values. Prior to analysis of the Fourier amplitude, images were windowed inside a circular mask (radius = 256 pixels) with a cosine edge, smoothly transitioning the edges of the image to mid-grey over 12 pixels. The Fourier amplitude was windowed in the same aperture to remove off-cardinal information that had a greater frequency than the maximum frequency of cardinal orientations (i.e. maximum frequency = 256 cycles/image).</p><p hwp:id="p-23">Our 512 × 512 images cover 8 octaves (1 – 256 cycles/image). To calculate contrast energy in evenly spaced spatial frequency bands, we first computed the distance of each point in the amplitude spectrum from the origin based on the x/y distances using Pythagoras’s theorem. We then log<sub>2</sub>-scaled these values and applied the floor function to round-down each log-scaled distance to an integer. The resulting values express the distance of each point from the origin according to 1-octave wide spatial frequency bands. We then averaged the amplitude values within each band.</p></sec><sec id="s2f" hwp:id="sec-8"><title hwp:id="title-11">Orientation filtered contrast</title><p hwp:id="p-24">To quantify variations in contrast energy according to the orientation of the contrast, we summed the Fourier amplitude within each of 16 oriented bands that ranged the full circle (0 ± 90°). Oriented filters were raised cosine filters that ramped from 0 to 1 (or 1 to 0) over 11.25°. The filters therefore had a bandwidth of 22.5° at the base, and 11.25° at half-height, and extended across all spatial frequencies. Filters were evenly spaced in 11.25° steps, such that summing across all filters resulted in a value of 1 at all locations (i.e. the filters were a “complete” basis set).</p></sec><sec id="s2g" hwp:id="sec-9"><title hwp:id="title-12">Modelling spatial frequency and orientation contrast energy</title><p hwp:id="p-25">We fit models that summarise the distributions of contrast energy across spatial frequencies and orientations. Contrast energy across spatial frequencies was estimated with the function:
<disp-formula id="eqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="451706v2_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives>
</disp-formula>
</p><p hwp:id="p-26">Where <italic toggle="yes">c</italic><sub><italic toggle="yes">l</italic></sub> is estimated contrast energy at the lowest spatial frequency, <italic toggle="yes">f</italic> is the frequency, <italic toggle="yes">α</italic> is the slope, and <italic toggle="yes">E</italic><sub><italic toggle="yes">f</italic></sub> is the predicted energy. <italic toggle="yes">c</italic><sub><italic toggle="yes">l</italic></sub> and <italic toggle="yes">α</italic> are free parameters, estimated by ordinary least squares. The function was fit to the raw data (8 spatial frequencies × 26,107 images) rather than, for example, the mean contrast of each image concept at each spatial frequency. We also fit a GLMM grouping the raw data by concept. This model leverages the organisation of images into concepts to improve the estimate of the overall model fit. The model assumes that the relationship between contrast energy and spatial frequency is distributed normally across concepts, making possible a description of the data at the individual concept level. Without a specific research question, however, this model would require extensive description of parameter estimates that is beyond the scope of this report. Importantly, the GLMM produced a group-level fit that was highly similar to the simpler linear model (1.3 for linear model versus 1.39 for multi-level model), and so we focus interpretation on only the more common, simpler “1/f” model. The 1/f model provides a similar group-level description as the GLMM but cannot be interrogated to the same extent. For example, future investigations could examine whether the random effects produced by the GLMM are sufficient to predict image concept based on spatial frequency contrast distributions alone, and so this model is included in the shared analysis code.</p><p hwp:id="p-27">We fit a custom function to model contrast as a function of orientation. This model is based on a function that has been used previously to model anisotropies in perception and memory (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Taylor &amp; Bays, 2018</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Wei &amp; Stocker, 2015</xref>):
<disp-formula id="eqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="451706v2_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
</p><p hwp:id="p-28">Here, <italic toggle="yes">θ</italic> is orientation in radians, <italic toggle="yes">c</italic><sub>0</sub> is a normalising constant, and <italic toggle="yes">p</italic>(<italic toggle="yes">θ</italic>) is the probability of <italic toggle="yes">θ</italic>. To capture the expected tendency that horizontal contrast is greater than vertical contrast (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Hansen &amp; Essock, 2004</xref>), we modified the function so that the distribution of energy is itself modified by a sinusoidal function centred on horizontal energy:
<disp-formula id="eqn7" hwp:id="disp-formula-7" hwp:rev-id="xref-disp-formula-7-1">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="451706v2_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
</p><p hwp:id="p-29">Where <italic toggle="yes">c</italic><sub><italic toggle="yes">v</italic></sub> is the estimated contrast for vertical orientations (i.e. when θ = ±<italic toggle="yes">π</italic>), <italic toggle="yes">ρ</italic> sets the magnitude of the difference from peak to trough, <italic toggle="yes">b</italic> sets the rate of drop-off of energy from cardinal to oblique orientations, <italic toggle="yes">β</italic> is horizontal energy as a proportion of vertical energy, and <italic toggle="yes">E</italic><sub><italic toggle="yes">θ</italic></sub> is the estimated energy. The parameters <italic toggle="yes">c</italic><sub><italic toggle="yes">v</italic></sub>, <italic toggle="yes">b, β</italic>, and <italic toggle="yes">ρ</italic> are free parameters. As with the model estimating contrast energy from spatial frequency, the function was fit to the raw data (16 orientations × 26,107 images). We did not attempt this model within a GLMM framework.</p></sec><sec id="s2h" hwp:id="sec-10"><title hwp:id="title-13">Predicting image concepts from luminance and contrast</title><p hwp:id="p-30">We classified images into the concept labels supplied in the THINGS database using a simple custom linear classification analysis of the joint distributions of luminance and contrast. The general design of this analysis was motivated by <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Grootswagers et al (2021)</xref> who used EEG to decode which of two image concepts had been displayed to observers. In brief, we sampled every possible pair of concepts, and, for each image in each concept, we classified to which of the two concepts the image belongs using only luminance and RMS contrast. Each image, therefore, was classified according to a comparison of its own concept against every other concept one pair at a time. For example, given that there are 14 images in the first concept, “aardvark”, and 1,853 other concepts with which aardvark images were compared, there were 14 × 1853 classifications for images in the first concept alone. In total, there were 26,107 (images) × 1,853 (combinations), which is over 48 million classifications. For each pair of concepts, we found the mean proportion of correct classifications.</p><p hwp:id="p-31">We first computed the mean luminance and contrast for each concept. Mean luminance is:
<disp-formula id="eqn8" hwp:id="disp-formula-8" hwp:rev-id="xref-disp-formula-8-1">
<alternatives hwp:id="alternatives-8"><graphic xlink:href="451706v2_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
</p><p hwp:id="p-32">Here, <italic toggle="yes">L</italic><sub><italic toggle="yes">δ</italic>[<italic toggle="yes">k</italic>]</sub> is the luminance of the k-th image in concept δ, <italic toggle="yes">N</italic><sub>δ</sub> is the number of images in that concept, and <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="451706v2_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> is the resulting mean luminance for that concept. Mean contrast, <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="451706v2_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>, is computed similarly:
<disp-formula id="eqn9" hwp:id="disp-formula-9">
<alternatives hwp:id="alternatives-11"><graphic xlink:href="451706v2_eqn9.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula>
</p><p hwp:id="p-33">The luminance and contrast standard deviations were computed as follows:
<disp-formula id="eqn10" hwp:id="disp-formula-10">
<alternatives hwp:id="alternatives-12"><graphic xlink:href="451706v2_eqn10.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula>
<disp-formula id="eqn11" hwp:id="disp-formula-11" hwp:rev-id="xref-disp-formula-11-1">
<alternatives hwp:id="alternatives-13"><graphic xlink:href="451706v2_eqn11.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula>
</p><p hwp:id="p-34">Having computed the means and standard deviations for each concept, we found the Euclidean distance of each image from every concept:
<disp-formula id="eqn12" hwp:id="disp-formula-12" hwp:rev-id="xref-disp-formula-12-1 xref-disp-formula-12-2">
<alternatives hwp:id="alternatives-14"><graphic xlink:href="451706v2_eqn12.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives>
</disp-formula>
</p><p hwp:id="p-35">Where <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="451706v2_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> is a vector of the distances of image k from every concept, including its own. The subscript <italic toggle="yes">δ</italic>[<italic toggle="yes">k</italic>] refers to a given image (<italic toggle="yes">k</italic>) from a given concept (<italic toggle="yes">δ</italic>). Note that the distances are scaled by the standard deviation of luminance (<italic toggle="yes">σ</italic><sub><italic toggle="yes">L</italic></sub>) and contrast <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="451706v2_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> of each concept. While these distances can be used to classify images into their concepts, the results would be biased: each image’s luminance and contrast data contribute to the means of its own concept calculation, and so classification accuracy would overestimate how well <italic toggle="yes">new</italic> images could be classified. To remove this circularity, we used a leave-one-out cross validation approach in which each to-be-classified image did not contribute to the calculation of its own concept means. This analysis is equivalent to asking how accurately we could classify a held-out or unseen image into its correct concept, based only on its luminance and contrast and the luminance and contrast of the other images within each THINGS concept.</p><p hwp:id="p-36">For each concept, we re-computed n-new concept means and standard deviations that excluded each image within that concept. For example, there were 14 unique images in the first concept, and so we computed 14 sets of means and standard deviations, where each set represents the means and standard deviations after leaving out a different image. Rather than compute these statistics from scratch using <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-8-1" hwp:rel-id="disp-formula-8">Equation 8</xref> - <xref ref-type="disp-formula" rid="eqn11" hwp:id="xref-disp-formula-11-1" hwp:rel-id="disp-formula-11">Equation 11</xref>, it was pragmatically more efficient to compute the leave-one-out means as follows:
<disp-formula id="eqn13" hwp:id="disp-formula-13">
<alternatives hwp:id="alternatives-17"><graphic xlink:href="451706v2_eqn13.gif" position="float" orientation="portrait" hwp:id="graphic-14"/></alternatives>
</disp-formula>
</p><p hwp:id="p-37">Where μ<sub>δ[<italic toggle="yes">k</italic>]</sub> is the mean (either luminance or contrast) of concept <italic toggle="yes">δ</italic> after holding out image <italic toggle="yes">k</italic> from that concept. <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-18"><inline-graphic xlink:href="451706v2_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> is the mean luminance or contrast of the concept with all images, while x<sub>δ[<italic toggle="yes">k</italic>]</sub> is the luminance or contrast, respectively, of the held-out image. The leave-one-out standard deviations were computed as follows:
<disp-formula id="eqn14" hwp:id="disp-formula-14" hwp:rev-id="xref-disp-formula-14-1">
<alternatives hwp:id="alternatives-19"><graphic xlink:href="451706v2_eqn14.gif" position="float" orientation="portrait" hwp:id="graphic-15"/></alternatives>
</disp-formula>
</p><p hwp:id="p-38">Where <italic toggle="yes">σ</italic><sub><italic toggle="yes">δ</italic>[<italic toggle="yes">k</italic>]</sub> is the standard deviation (of either the luminance or contrast) of concept <italic toggle="yes">δ</italic> after holding out image <italic toggle="yes">k</italic> from that concept. <italic toggle="yes">σ</italic><sub><italic toggle="yes">δ</italic></sub> is the standard deviation of the luminance or contrast for the concept with all images. Finally, we computed the distance of an image from its own concept mean after having left out that image as per Pythagorean theorem shown in <xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-12-1" hwp:rel-id="disp-formula-12">Equation 12</xref>. There were then 26,107 new distances of each image from its own concept luminance-contrast centroid. These distances were substituted into <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-20"><inline-graphic xlink:href="451706v2_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula>, the vector calculated in <xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-12-2" hwp:rel-id="disp-formula-12">Equation 12</xref>, to replace the biased estimates with left-out estimates.</p><p hwp:id="p-39">The critical aspects of the equations above are that, 1) for each image, we computed the distance between each image and every concept and 2) the distance between an image and its own concept was unbiased by the image. For every possible pair of concepts, we then classified a given image from the pair as belonging to the concept that was closest to the image in luminance and contrast. If the closest concept was the image’s concept, then we scored the classification as accurate (i.e. 1); otherwise we scored the classification as incorrect (i.e. 0). Finally, we found the mean proportion correct for every pairwise concept comparison, yielding a 1,854×1,854 matrix of mean proportion correct classifications, where each matrix cell represents mean accuracy for classifying images from two competing concepts.</p><p hwp:id="p-40">This simple classification model labels images into their concepts with the accuracy expected from a template-matching ideal observer who has access to only luminance and contrast data. Accuracy could be improved by using more complex models, such as a multivariate logistic regression model or a support vector machine, but these are unnecessary for our purposes of illustrating the decodability of image concepts based solely on luminance and contrast of individual images. If our simple classification model can correctly classify images into their concepts, more sophisticated models would perform better, but the interpretation would be the same: low-level images factors confound high-level category labels.</p></sec></sec><sec id="s3" hwp:id="sec-11"><title hwp:id="title-14">Results</title><p hwp:id="p-41">We quantified the luminance and luminance contrast in each of the 26,107 THINGS images using common metrics. All analyses were performed on grey-scaled versions of the images, but we describe how luminance modifications can be made to coloured images at the end of this section.</p><sec id="s3a" hwp:id="sec-12"><title hwp:id="title-15">Contrast energy within separable spatial frequency and orientation bands</title><p hwp:id="p-42"><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref> shows the distributions of contrast energy as a function of spatial frequency (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2A</xref>) and orientation (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2B</xref>). Both sets of results are highly typical of natural images: energy scales inversely with spatial frequency (on log-log axes) and there is an over representation of cardinal orientations relative to oblique orientations. The slope of the decline of energy with increasing spatial frequency was 1.3, which is in close agreement with measurements of other natural images (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">Bex &amp; Makous, 2002</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Torralba &amp; Oliva, 2003</xref>). The slope calculated by the GLMM was 1.39, with a standard deviation of 0.1 across concepts. The relatively non-overlapping distributions in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2A</xref> show that the relationship between contrast energy and spatial frequency holds for all images in the THINGS database. The relatively broad overlapping distributions in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Figure 2B</xref>, however, show that there is greater variation in the distribution of contrast energy over orientations across THINGS images. Regardless, we found that horizontal contrast was 1.1 times greater than vertical contrast on average (parameter <italic toggle="yes">β</italic> in <xref ref-type="disp-formula" rid="eqn7" hwp:id="xref-disp-formula-7-1" hwp:rel-id="disp-formula-7">Equation 7</xref>).</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.08.451706v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-43">The distribution of contrast energy in separable spatial frequency and orientation bands. A) Contrast energy is inversely related to spatial frequency, following the typical 1/f<sup>α</sup> relationship. Distributions are histograms of energy for all 26,107 images at each spatial frequency. The solid line is the inset function. B) Contrast energy is greater for cardinal orientations than oblique orientations. The lower and upper limits of boxes show the first and third quartiles, respectively, box centres show the medians, and the whiskers show the range. The solid line is a model that captures the cardinal/oblique biases, as well as differences in horizontal versus vertical contrast, as described in the Methods.</p></caption><graphic xlink:href="451706v2_fig2" position="float" orientation="portrait" hwp:id="graphic-16"/></fig></sec><sec id="s3b" hwp:id="sec-13"><title hwp:id="title-16">Relationship between luminance and RMS contrast</title><p hwp:id="p-44"><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3A</xref> plots the luminance and RMS contrast of every image in the THINGS database. The inset marginal distributions show that both measures are approximately normally distributed. Luminance tends to be lower than mid-grey, and RMS contrast tends to be distributed around .5. The line through the data is the best fitting quadratic model fit in a GLMM framework (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-4-1" hwp:rel-id="disp-formula-4">Equation 4</xref>). The same data are plotted in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3B</xref>, with three randomly drawn concepts highlighted in different colours. Note that the means of the concepts are separable in both luminance and contrast, and there is partial overlap of the distribution of images from different concepts. These data suggest that these simple image statistics may be sufficient to classify images in their concepts with above-chance accuracy.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.08.451706v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-45">Relative luminance and RMS contrast of 26,107 images in the THINGS database. A) The relationship between luminance and RMS contrast is parabolic, such that there is a positive correlation for negative luminance images, and a negative correlation with positive luminance images. Each datum has been colour coded according to image concept but note that there are more image concepts than colour categories. The inset at top left shows the marginal histograms. Relative to mid-grey, images have a lower luminance on average (blue distribution), and are approximately normally distributed around 0.5 RMS contrast (pink distribution). B) Measures for three randomly selected image concepts, and their averages. Average measurements are shown in black outlines, with error bars showing one standard error. See Methods for details on scaling and calculations.</p></caption><graphic xlink:href="451706v2_fig3" position="float" orientation="portrait" hwp:id="graphic-17"/></fig></sec><sec id="s3c" hwp:id="sec-14"><title hwp:id="title-17">Concept classification based on luminance and contrast</title><p hwp:id="p-46">We next tested how accurately each image could be classified into its concept using luminance and contrast alone. This analysis is motivated by the fact that some variables of theoretical interest will be correlated with changes in luminance and contrast, confounding some experimental designs that leverage the THINGS images as stimuli. Such dependent variables include changes in BOLD or the amplitude of an event-related potential in neuroimaging experiments. We discuss these issues more in the Discussion.</p><p hwp:id="p-47">Classification accuracy for all 1854<sup>2</sup> pairwise comparisons are shown in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure <bold><italic toggle="yes">4</italic></bold>4A</xref>. We consider classification accuracy as being analogous to representational dissimilarity measures (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-3" hwp:rel-id="ref-12">Grootswagers et al., 2021</xref>): the greater the decoding accuracy of a pair of concepts, the more dissimilar they are in luminance-contrast space (e.g. <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3B</xref>). 63.4% of all images were correctly classified, compared with chance classification of 50%. The absolute accuracy of decoding is less important than the fact that accuracy is well above chance. This result shows that low-level image factors are a source of decodable information. Critically, these low-level factors are confounded with the high-level concept labels of the THINGS images.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.08.451706v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-48">THINGS images can be classified based on their mean luminance and RMS contrast alone. A) Proportion of correct concept discriminations for all possible pairwise comparisons of concepts. Within each image concept, there were at least 12 image exemplars that were classified as belonging to one of the 1,854 concepts. An image was classified by minimising its luminance and RMS difference from each concept average. Chance classification is 0.5. B) A detailed section of (A), highlighting the classification accuracy for a subset of 20×20 concept pairs. C) Five images that were most and least accurately identified.</p></caption><graphic xlink:href="451706v2_fig4" position="float" orientation="portrait" hwp:id="graphic-18"/></fig><p hwp:id="p-49"><xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure <bold><italic toggle="yes">4</italic></bold>4B</xref> shows a zoomed in section of the dissimilarity matrix with concept labels. As one example of differences in classification accuracy of images, when paired with beetle images, beanie images can be classified more accurately than belt buckle images. The five most and least accurately classified images are shown in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3C</xref>. While the top five most classifiable images were from the same concept (“baton”), the bottom five classifiable images come from different concepts. These example images provide an intuition behind the (un)successful classification of images. The top five images clearly differ from the bottom five in both their mean luminance and pixel variability (i.e. RMS contrast). Image concepts that cluster near the centroid of the distribution in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figure 3A</xref> will have more similar image metrics than image concepts near the edges of the distribution, and therefore will be less discriminable based on these metrics alone.</p><p hwp:id="p-50">The above-chance level of classification for paired concepts, described above, motivated us to test how well we could classify each image into its concept when compared with every other concept within the database. This analysis was the same as used for pairwise classifications, but we classified a given image according to the smallest distance between the image’s statistics and the nearest of all concept means. In this case, chance accuracy is 1/1854, or 0.05%. Using luminance and contrast alone, mean classification accuracy was 0.3% (79 images out of 26,107 were correctly classified). Although only a small minority of images were correctly classified, this level of accuracy is significantly greater than chance (binomial test: Successes = 79, N = 26107, P = 1/1854, p &lt; 10<sup>−10</sup>). We repeated the classification analysis, now including the spatial frequency and orientation contrast energy data presented in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Figure 2</xref> in addition to luminance and RMS contrast. By including these predictors, classification accuracy increased to 1.5% (400 images out of 26,107 were correctly classified; p &lt; 10<sup>−10</sup>).</p></sec></sec><sec id="s4" hwp:id="sec-15"><title hwp:id="title-18">Discussion</title><p hwp:id="p-51">In the present report we summarise the luminance and luminance contrast properties of images in the THINGS database. There were three primary results. First, the distributions of contrast energy over spatial frequency and orientation were similar to the statistics reported for other natural images (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Figure 2</xref>). Second, there was a nonlinear but predictable relationship between mean luminance and RMS contrast across images (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 3</xref>). Third, we showed that there are systematic differences in luminance and contrast between image concepts that are sufficient to classify individual images into their concepts with above-chance accuracy (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure <bold><italic toggle="yes">4</italic></bold>4</xref>). In the following, we discuss the implications of these findings in more detail, and provide an example of how to normalise luminance and contrast of coloured images to remove these factors as potential experimental confounds. We also discuss the importance of natural image databases, like THINGS, for advancing cognitive neuroscience theory.</p><sec id="s4a" hwp:id="sec-16"><title hwp:id="title-19">Contrast energy</title><p hwp:id="p-52">The contrast energy of THINGS images was well described as a linear transform of spatial frequency on log-log axes, with a slope of 1.3 (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Figure 2A</xref>). There was relatively little variation across images, with the simple 1/f model accounting for 97% of variance across all 26,107 images. Contrast energy as a function of orientation, however, was relatively more heterogeneously distributed across images (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Figure 2B</xref>). Nonetheless, our model of oriented contrast accounts for 17% of the variance in the THINGS images. There are many plausible reasons for the greater consistency of energy as a function of spatial frequency relative to orientation. As one example, <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Torralba and Oliva (2003)</xref> showed that such statistics depend on whether image content is natural or not, with relatively less cardinal-oblique contrast bias for natural images. A strength of the THINGS database is its diversity of image content. Furthermore, by design, the images in the THINGS database are digital photos with objects in various compositions and configurations. For example, some photos tightly frame a specific object, and others are framed with the camera lens rotated by some amount. These variations will have little impact on the 1/f spectrum of the images, which simply quantifies the scale invariance of the contrast distributions, but can greatly impact the distribution of oriented contrast (e.g. a picket fence rotated by 45° will have the same 1/f spectrum as the original, but its oriented contrast distribution will be translated). It is somewhat unsurprising, therefore, that oriented contrast is more variable across images. Perhaps more important to the intended use of THINGS images, however, is that there is no reason to expect that these distributions of contrast energy deviate meaningfully from those encountered in natural settings.</p></sec><sec id="s4b" hwp:id="sec-17"><title hwp:id="title-20">Luminance and RMS contrast</title><p hwp:id="p-53">Mean luminance and RMS contrast were correlated according to a quadratic function: for images with negative mean luminance, there is a positive relationship between luminance and RMS contrast; for images with positive mean luminance, there is a negative relationship between luminance and RMS contrast. The likely explanation for this nonlinear relationship is somewhat trivial: when the mean luminance of an image approaches extreme values, the distributions of pixels become skewed, resulting in lower standard deviations. <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">Frazor and Geisler (2006)</xref> used calibrated photos to estimate the luminance of images as they would have appeared in the world (as opposed to their luminance on a computer monitor), and found no systematic relationship between luminance and RMS contrast.</p><p hwp:id="p-54">A clear potential benefit of the THINGS database is to facilitate understanding of how high-level conceptual information influences perceptual, cognitive, and neural representations of visual input (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-4" hwp:rel-id="ref-12">Grootswagers et al., 2021</xref>; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Harrison, 2019</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">Hebart et al., 2019</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-3" hwp:rel-id="ref-19">2020</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Neri, 2011</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">Rideaux et al., 2021</xref>). However, the variation in luminance and contrast in the THINGS images may be an undesirable property for studies in which these factors directly influence measures of interest. Consider a hypothetical experiment that uses pupil diameter as a dependent variable. A researcher may ask whether pupil diameter predicts the frequency with which THINGS concepts appear in a given text corpus. Pupil diameter will be responsive to the luminance and contrast of each image, and we have now shown that image concepts can be predicted from these same factors alone. Therefore, the hypothetical experiment predicting concept frequency from pupil diameter would be confounded by low-level image factors unrelated to concepts per se. This basic confound would also need to be ruled out of neuroimaging experiments in which event related potentials or changes in BOLD signal depend on luminance and contrast.</p><p hwp:id="p-55">Whereas many recent models of image classification are deep neural networks (e.g. <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-3" hwp:rel-id="ref-18">Hebart et al., 2019</xref>), we showed that a highly simple model is sufficient to achieve well above chance classification accuracy. Deep neural nets provide a means by which one can find low-dimensional structure in natural images that is predictive of image content. By contrast, our model uses only luminance and RMS contrast information to classify images into their concepts. We classified an image by finding the concept whose mean luminance and RMS contrast were closest to the image (see <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3B</xref> and <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure <bold><italic toggle="yes">4</italic></bold>4</xref> and Methods). We achieved 63.4% accuracy when only two concepts were used as per <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-5" hwp:rel-id="ref-12">Grootswagers et al (2021)</xref>. High-level image structure, therefore, is not necessary for above-chance classification of high-level information. The ability to classify images using luminance and contrast alone demonstrates the potential impact these quantities could have on physiological responses to, and the neural correlates of, the THINGS images.</p></sec><sec id="s4c" hwp:id="sec-18"><title hwp:id="title-21">Normalising luminance and RMS contrast across images</title><p hwp:id="p-56">There are simple ways to guard against differences in luminance and RMS contrast confounding experiments that leverage THINGS images as stimuli. In <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref> we show an example in which we normalise the luminance and RMS contrast of the full colour images. The basic process involves converting the RGB images into a format in which luminance is dissociated from colour. We use YUV, in which the matrix Y contains the image’s luminance (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5A</xref>). Normalisation involves simply subtracting the mean luminance and dividing the result by the standard deviation. The resulting luminance plane can be multiplied by some constant that represents the desired RMS contrast (0.23 in this case). By recombining the luminance and colour planes, the image can then be converted back into RGB space. After this normalisation process, images are matched in their luminance and RMS contrast. This process effectively reduces the distribution of luminance and RMS contrast across all images in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Figure 3</xref> to a single point. Note that, although the images may tend to appear qualitatively faded, the objects (and concepts) remain clearly visible in the normalised images (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 5B</xref>). Applying the same decoder as described in the Methods and Results to these normalised images reduces decoding accuracy to 49.9%. The simple normalisation method presented here, therefore, removes luminance and contrast as decodable sources of information.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.08.451706v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><p hwp:id="p-57">Example procedure to normalise luminance and contrast. A) We convert an RGB image into YUV, where Y is the luminance of the image. Y is then normalised by removing its mean, μ<sub>Y</sub>, and dividing by its standard deviation, σ<sub>Y</sub>. RMS contrast can be set by a multiplicative constant, C<sub>N</sub>. By combining this new luminance plane with the original UV images, the image can be converted back to an RGB image, now with a mean luminance of zero and RMS contrast of C<sub>N</sub>. B) Examples of normalised images (top) compared with their originals (bottom). All images in the top row have the same mean luminance (0) and RMS contrast (0.23).</p></caption><graphic xlink:href="451706v2_fig5" position="float" orientation="portrait" hwp:id="graphic-19"/></fig></sec><sec id="s4d" hwp:id="sec-19"><title hwp:id="title-22">Other stimulus considerations</title><p hwp:id="p-58">When we included in our decoding analysis the contrast energy within spatial frequency and orientation bands in addition to RMS contrast and mean luminance, decoding accuracy improved five-fold (i.e. from 0.3% to 1.5% accuracy at classifying each image into one of 1854 concepts). This improvement comes from increasing the dimensionality of the summary data; the decoder had access to more information about each image. It may therefore be pertinent for researchers to also control for these factors. Indeed, <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Willenbockel et al (2010)</xref> reviewed several neuroimaging studies in which such basic low-level image factors may have influenced results. They developed and made available the SHINE toolbox to control for a broader range of image factors than those reported here. We refer readers to that useful resource for both a more detailed review of potential confounds in previous neuroimaging studies as well as their software. We note, however, that when using natural image stimuli, image factors <italic toggle="yes">must</italic> differ between images. It is up to the researcher to determine which image statistics they are interested in, and which ought to be matched across images. As shown in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5B</xref>, images that have had their image statistics manipulated remain easily recognisable; human image recognition is invariant over a large range of mean luminances and contrasts. We suggest that researchers interested in relatively high-level image features adopt a conservative approach to these issues and assume that, if low-level image factors <italic toggle="yes">could</italic> contaminate their models, then they probably do. This holds for behavioural, neural and machine-learning models.</p><p hwp:id="p-59">Various image metrics will be affected by decisions such as the display size of images and how an image is converted to greyscale from colour space (and back again). We converted to greyscale because we were interested only in luminance and luminance contrast. We resized images because it was more convenient to calculate image metrics from a constant-sized input. When increasing or decreasing the size of an image, one has to make a choice about how to add or remove pixel information, respectively. In our tests, using an interpolation method to either increase or reduce image size holds the mean (luminance) constant while reducing the variability (RMS contrast). Without interpolation, the mean is more variable when resizing the image, while the variability remains relatively constant. Perhaps most critical for the application of this knowledge, however, is that any calculations or controls that are performed on stimuli must be performed on the image as it will be (or was) presented to the observer.</p><p hwp:id="p-60">There are, of course, good alternatives to adjusting the low-level features of stimuli to mitigate confounds in experiments. Depending on the research question and experimental design, simply inverting the image may be sufficient to control low-level factors while disrupting higher-level and semantic processing (e.g. <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Neri, 2014</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Yin, 1969</xref>). In other cases, the same stimuli may be repeated in all conditions, while the experimenter manipulates only the instructions given to participants across conditions (e.g. <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Harrison et al., 2019</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Harrison &amp; Rideaux, 2019</xref>).</p></sec><sec id="s4e" hwp:id="sec-20"><title hwp:id="title-23">The THINGS database is a valuable resource</title><p hwp:id="p-61">We do not believe that any of the results presented here should discourage researchers from using THINGS images as experimental stimuli. Instead, we encourage the use of the THINGS database. The use of natural images as experimental stimuli will help to bridge the gap between the minimal stimuli typically used in studies of cognitive neuroscience and the real world. We hope our data will facilitate a better understanding of basic statistical properties that may be important to future research. The close match of the image spectra to known natural image statistics demonstrates that these images do not deviate from other natural images in a meaningful way. While we found that systematic luminance and contrast differences across image concepts have the potential to confound experimental designs, these properties can be easily adjusted. Adjustments can be made a priori by, for example, normalisation, or post-hoc, by using our shared data to partial out variance attributable to these low-level statistics independently of other experimental manipulations. One improvement for future iterations of either the THINGS database or other similar resources would be to include any available metadata regarding the calibration, brand and model of digital camera from which an image was obtained in order to correctly linearise images prior to analysis and use in experiments.</p></sec><sec id="s4f" hwp:id="sec-21"><title hwp:id="title-24">The importance of natural images in cognitive neuroscience</title><p hwp:id="p-62">Much of our knowledge of visual perception and cognition comes from experiments with relatively abstract stimuli. The functions of neurons in primary visual cortex are some of the best understood in cognitive neuroscience. Much of this understanding has been derived from oriented gratings of various spatial frequency, contrast, and position in the visual field (for a review, see <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Carandini et al., 2005</xref>). There is evidence that such knowledge, derived from experiments with minimal stimuli, is sufficient to predict neural encoding of complex natural images (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Nishimoto et al., 2011</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Sebastian et al., 2017</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Yoshida &amp; Ohki, 2020</xref>). In many other areas, however, there seems to be little emphasis on generalising theory beyond the abstract stimuli arbitrarily chosen by the experimenter. This seems particularly true of the cognitive neuroscience of visual attention and working memory – we refer readers to a review chapter by <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Brady and colleagues (2019)</xref> who discuss this issue at length. Whether a theory is useful for more than predicting how volunteers respond to spots of light on a computer monitor requires bringing experiments out of the lab and into the real world. Such a leap will likely come at the cost of experimental control. However, large stimulus datasets like the THINGS images provide a steppingstone between standard experimental stimuli and the complexity of the real world. THINGS images are highly diverse in their content and capture much of the diversity of natural scenes, but, as shown here, can nonetheless be described and controlled in terms of their low-level properties.</p><p hwp:id="p-63">Are digital photos sufficient to understand perceptual and cognitive processes when viewing “true” (non-digital) natural images? Anecdotally, a common criticism levelled at investigations that treat digital photos as representative of natural images is that the images are photographed for a specific purpose by a specific photographer, and therefore may not truly represent the visual images we encounter in nature (see <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Hart et al., 2009</xref> for one example in which lab-based experiments are directly compared with the real world). Although there may remain a distinction between processes involved in viewing digital photos versus non-digital images, we do not think the importance of digital photos in psychology experiments is undermined for at least three reasons. First, digital photos provide a justifiable balance between uncontrolled complexity and experimental control, and this balance requires more powerful and generalisable theory (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">Carandini et al., 2005</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Muthukrishna &amp; Henrich, 2019</xref>). Second, the human visual system is putatively shaped by the <italic toggle="yes">statistics</italic> of natural environments over evolutionary and developmental timescales, and, in general, there is no reason to expect that the statistics of digital photos differ meaningfully from the statistics of natural environments (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">Geisler, 2008</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Olshausen &amp; Field, 1996</xref>). Third, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-4" hwp:rel-id="ref-19">Hebart et al (2020)</xref> have shown how digital photos and perceptual decisions can be combined to create data-driven models that generate meaningful psychological constructs that generalise across images and contexts. Any limitations of digital photo stimuli also do not justify the sole use of arbitrary abstract stimuli in cognitive neuroscience.</p></sec></sec><sec id="s5" hwp:id="sec-22"><title hwp:id="title-25">Summary and conclusions</title><p hwp:id="p-64">Our analyses show that the THINGS images have typical distributions of luminance and contrast, and that these low-level image factors can be used to classify individual images into their concept. The separability of luminance and contrast may confound certain experimental designs, but can be controlled for easily. We see great potential in the THINGS database as a large diverse stimulus set that can be used to address a great variety of research questions in cognitive neuroscience. We are already exploiting this resource to understand perceptual processes in natural images (e.g. <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-3" hwp:rel-id="ref-28">Rideaux et al., 2021</xref>). Wide adoption of the database will likely result in yet even larger open datasets when researchers make available their results. We anticipate that the cumulative massive datasets will create new and unique opportunities to link previously disconnected cognitive neuroscience theory via a common stimulus set. This trans-disciplinary information is already being curated: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://things-initiative.org/" ext-link-type="uri" xlink:href="https://things-initiative.org/" hwp:id="ext-link-7">https://things-initiative.org/</ext-link></p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-26">Acknowledgements</title><p hwp:id="p-65">I am grateful to Tijl Grootswagers who provided feedback on an earlier draft of this manuscript and explained to me the importance of using a cross-validation analysis for image classification. I would not have done this step of the analysis without his feedback and guidance. I also thank Cameron Turner who worked out how to calculate the leave-one-out standard deviations shown in <xref ref-type="disp-formula" rid="eqn14" hwp:id="xref-disp-formula-14-1" hwp:rel-id="disp-formula-14">Equation 14</xref>. This research was supported by an Australian Research Council Discovery Early Career Research Award (DE190100136).</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-27">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Bex P. J."><surname>Bex</surname>, <given-names>P. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dakin S. C."><surname>Dakin</surname>, <given-names>S. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Mareschal I."><surname>Mareschal</surname>, <given-names>I.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-2">Critical band masking in optic flow</article-title>. <source hwp:id="source-1">Network: Computation in Neural Systems</source>, <volume>16</volume>(<issue>2–3</issue>), <fpage>261</fpage>–<lpage>284</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1080/09548980500289973" ext-link-type="uri" xlink:href="https://doi.org/10.1080/09548980500289973" hwp:id="ext-link-8">https://doi.org/10.1080/09548980500289973</ext-link></citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Bex P. J."><surname>Bex</surname>, <given-names>P. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Makous W."><surname>Makous</surname>, <given-names>W.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-3">Spatial frequency, phase, and the contrast of natural images</article-title>. <source hwp:id="source-2">Journal of the Optical Society of America A</source>, <volume>19</volume>(<issue>6</issue>), <fpage>1096</fpage>–<lpage>1106</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1364/JOSAA.19.001096" ext-link-type="uri" xlink:href="https://doi.org/10.1364/JOSAA.19.001096" hwp:id="ext-link-9">https://doi.org/10.1364/JOSAA.19.001096</ext-link></citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="book" citation-type="book" ref:id="2021.07.08.451706v2.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Brady T. F."><surname>Brady</surname>, <given-names>T. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Störmer V. S."><surname>Störmer</surname>, <given-names>V. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shafer-Skelton A."><surname>Shafer-Skelton</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Williams J. R."><surname>Williams</surname>, <given-names>J. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chapman A. F."><surname>Chapman</surname>, <given-names>A. F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schill H. M."><surname>Schill</surname>, <given-names>H. M.</given-names></string-name> (<year>2019</year>). <chapter-title>Scaling up visual attention and visual working memory to the real world</chapter-title>. <source hwp:id="source-3">In Psychology of Learning and Motivation</source> (Vol. <volume>70</volume>, pp. <fpage>29</fpage>–<lpage>69</lpage>). <publisher-name>Elsevier</publisher-name>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/bs.plm.2019.03.001" ext-link-type="uri" xlink:href="https://doi.org/10.1016/bs.plm.2019.03.001" hwp:id="ext-link-10">https://doi.org/10.1016/bs.plm.2019.03.001</ext-link></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Cadena S. A."><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Denfield G. H."><surname>Denfield</surname>, <given-names>G. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walker E. Y."><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gatys L. A."><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tolias A. S."><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bethge M."><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ecker A. S."><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-4">Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title>. <source hwp:id="source-4">PLOS Computational Biology</source>, <volume>15</volume>(<issue>4</issue>), <fpage>e1006897</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1371/journal.pcbi.1006897" ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1006897" hwp:id="ext-link-11">https://doi.org/10.1371/journal.pcbi.1006897</ext-link></citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Campbell F. W."><surname>Campbell</surname>, <given-names>F. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Maffei L."><surname>Maffei</surname>, <given-names>L.</given-names></string-name> (<year>1970</year>). <article-title hwp:id="article-title-5">Electrophysiological evidence for the existence of orientation and size detectors in the human visual system</article-title>. <source hwp:id="source-5">The Journal of Physiology</source>, <volume>207</volume>(<issue>3</issue>), <fpage>635</fpage>–<lpage>652</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1113/jphysiol.1970.sp009085" ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1970.sp009085" hwp:id="ext-link-12">https://doi.org/10.1113/jphysiol.1970.sp009085</ext-link></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Campbell F. W."><surname>Campbell</surname>, <given-names>F. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Robson J. G."><surname>Robson</surname>, <given-names>J. G.</given-names></string-name> (<year>1968</year>). <article-title hwp:id="article-title-6">Application of Fourier analysis to the visibility of gratings</article-title>. <source hwp:id="source-6">The Journal of Physiology</source>, <volume>197</volume>(<issue>3</issue>), <fpage>551</fpage>–<lpage>566</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Carandini M."><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Demb J. B."><surname>Demb</surname>, <given-names>J. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mante V."><surname>Mante</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tolhurst D. J."><surname>Tolhurst</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dan Y."><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Olshausen B. A."><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J. L."><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rust N. C."><surname>Rust</surname>, <given-names>N. C.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-7">Do we know what the early visual system does?</article-title> <source hwp:id="source-7">Journal of Neuroscience</source>, <volume>25</volume>(<issue>46</issue>), <fpage>10577</fpage>–<lpage>10597</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.3726-05.2005" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3726-05.2005" hwp:id="ext-link-13">https://doi.org/10.1523/JNEUROSCI.3726-05.2005</ext-link></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Frazor R. A."><surname>Frazor</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Geisler W. S."><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-8">Local luminance and contrast in natural images</article-title>. <source hwp:id="source-8">Vision Research</source>, <volume>46</volume>(<issue>10</issue>), <fpage>1585</fpage>–<lpage>1598</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.visres.2005.06.038" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2005.06.038" hwp:id="ext-link-14">https://doi.org/10.1016/j.visres.2005.06.038</ext-link></citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Geisler W. S."><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-9">Visual Perception and the Statistical Properties of Natural Scenes</article-title>. <source hwp:id="source-9">Annual Review of Psychology</source>, <volume>59</volume>(<issue>1</issue>), <fpage>167</fpage>–<lpage>192</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1146/annurev.psych.58.110405.085632" ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.psych.58.110405.085632" hwp:id="ext-link-15">https://doi.org/10.1146/annurev.psych.58.110405.085632</ext-link></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="book" citation-type="book" ref:id="2021.07.08.451706v2.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Gelman A."><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Hill J."><surname>Hill</surname>, <given-names>J.</given-names></string-name> (<year>2007</year>). <source hwp:id="source-10">Data Analysis Using Regression and Multilevel/Hierarchical Models</source>. <publisher-name>Cambridge University Press</publisher-name>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Graham N."><surname>Graham</surname>, <given-names>N.</given-names></string-name> (<year>1989</year>). <source hwp:id="source-11">Visual Pattern Analyzers Oxford Psychology Series No</source>. <volume>16</volume>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.cell.com/trends/neurosciences/pdf/0166-2236(90)90087-Q.pdf" ext-link-type="uri" xlink:href="http://www.cell.com/trends/neurosciences/pdf/0166-2236(90)90087-Q.pdf" hwp:id="ext-link-16">http://www.cell.com/trends/neurosciences/pdf/0166-2236(90)90087-Q.pdf</ext-link></citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2 xref-ref-12-3 xref-ref-12-4 xref-ref-12-5"><citation publication-type="website" citation-type="web" ref:id="2021.07.08.451706v2.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Grootswagers T."><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhou I."><surname>Zhou</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Robinson A. K."><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hebart M. N."><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Carlson T. A."><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-10">THINGS-EEG: Human electroencephalography recordings for 1,854 concepts presented in rapid serial visual presentation streams</article-title>. <source hwp:id="source-12">BioRxiv</source>, 2021.06.03.447008. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/2021.06.03.447008" ext-link-type="uri" xlink:href="https://doi.org/10.1101/2021.06.03.447008" hwp:id="ext-link-17">https://doi.org/10.1101/2021.06.03.447008</ext-link></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Hansen B. C."><surname>Hansen</surname>, <given-names>B. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Essock E. A."><surname>Essock</surname>, <given-names>E. A.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-11">A horizontal bias in human visual processing of orientation and its correspondence to the structural components of natural scenes</article-title>. <source hwp:id="source-13">Journal of Vision</source>, <volume>4</volume>(<issue>12</issue>), <fpage>5</fpage>–<lpage>5</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1167/4.12.5" ext-link-type="uri" xlink:href="https://doi.org/10.1167/4.12.5" hwp:id="ext-link-18">https://doi.org/10.1167/4.12.5</ext-link></citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Harrison W. J."><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-12">Segmenting processes in the human lateral geniculate nucleus</article-title>. <source hwp:id="source-14">Cortex</source>, <volume>121</volume>, <fpage>485</fpage>–<lpage>487</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.cortex.2019.07.011" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2019.07.011" hwp:id="ext-link-19">https://doi.org/10.1016/j.cortex.2019.07.011</ext-link></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Harrison W. J."><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ayeni A. J."><surname>Ayeni</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bex P. J."><surname>Bex</surname>, <given-names>P. J.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-13">Attentional selection and illusory surface appearance</article-title>. <source hwp:id="source-15">Scientific Reports</source>, <volume>9</volume>(<issue>1</issue>), <fpage>2227</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/s41598-018-37084-7" ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-018-37084-7" hwp:id="ext-link-20">https://doi.org/10.1038/s41598-018-37084-7</ext-link></citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Harrison W. J."><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rideaux R."><surname>Rideaux</surname>, <given-names>R.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-14">Voluntary control of illusory contour formation</article-title>. <source hwp:id="source-16">Attention, Perception, and Psychophysics</source>, <volume>81</volume>(<issue>5</issue>), <fpage>1522</fpage>–<lpage>1531</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3758/s13414-019-01678-8" ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13414-019-01678-8" hwp:id="ext-link-21">https://doi.org/10.3758/s13414-019-01678-8</ext-link></citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Hart B. M."><surname>Hart</surname>, <given-names>B. M.</given-names></string-name> <string-name name-style="western" hwp:sortable="‘t Vockeroth J."><surname>‘t</surname>, <given-names>Vockeroth J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schumann F."><surname>Schumann</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bartl K."><surname>Bartl</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schneider E."><surname>Schneider</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="König P."><surname>König</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Einhäuser W."><surname>Einhäuser</surname>, <given-names>W.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-15">Gaze allocation in natural stimuli: Comparing free exploration to head-fixed viewing conditions</article-title>. <source hwp:id="source-17">Visual Cognition</source>, <volume>17</volume>(<issue>6–7</issue>), <fpage>1132</fpage>–<lpage>1158</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1080/13506280902812304" ext-link-type="uri" xlink:href="https://doi.org/10.1080/13506280902812304" hwp:id="ext-link-22">https://doi.org/10.1080/13506280902812304</ext-link></citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2 xref-ref-18-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Hebart M. N."><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dickter A. H."><surname>Dickter</surname>, <given-names>A. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kidder A."><surname>Kidder</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kwok W. Y."><surname>Kwok</surname>, <given-names>W. Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Corriveau A."><surname>Corriveau</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wicklin C. V."><surname>Wicklin</surname>, <given-names>C. V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Baker C. I."><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-16">THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title>. <source hwp:id="source-18">PLOS ONE</source>, <volume>14</volume>(<issue>10</issue>), <fpage>e0223792</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1371/journal.pone.0223792" ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0223792" hwp:id="ext-link-23">https://doi.org/10.1371/journal.pone.0223792</ext-link></citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2 xref-ref-19-3 xref-ref-19-4"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Hebart M. N."><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zheng C. Y."><surname>Zheng</surname>, <given-names>C. Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pereira F."><surname>Pereira</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Baker C. I."><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-17">Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</article-title>. <source hwp:id="source-19">Nature Human Behaviour</source>, <volume>4</volume>(<issue>11</issue>), <fpage>1173</fpage>–<lpage>1185</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/s41562-020-00951-3" ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-020-00951-3" hwp:id="ext-link-24">https://doi.org/10.1038/s41562-020-00951-3</ext-link></citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Hubel D. H."><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wiesel T. N."><surname>Wiesel</surname>, <given-names>T. N.</given-names></string-name> (<year>1959</year>). <article-title hwp:id="article-title-18">Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source hwp:id="source-20">The Journal of Physiology</source>, <volume>148</volume>, <fpage>574</fpage>–<lpage>591</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Kay K. N."><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prenger R. J."><surname>Prenger</surname>, <given-names>R. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gallant J. L."><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-19">Identifying natural images from human brain activity</article-title>. <source hwp:id="source-21">Nature</source>, <volume>452</volume>(<issue>7185</issue>), <fpage>352</fpage>–<lpage>355</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/nature06713" ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06713" hwp:id="ext-link-25">https://doi.org/10.1038/nature06713</ext-link></citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="website" citation-type="web" ref:id="2021.07.08.451706v2.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Muthukrishna M."><surname>Muthukrishna</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Henrich J."><surname>Henrich</surname>, <given-names>J.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-20">A problem in theory</article-title>. <source hwp:id="source-22">Nature Human Behaviour</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/s41562-018-0522-1" ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-018-0522-1" hwp:id="ext-link-26">https://doi.org/10.1038/s41562-018-0522-1</ext-link></citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Neri P."><surname>Neri</surname>, <given-names>P.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-21">Global Properties of Natural Scenes Shape Local Properties of Human Edge Detectors</article-title>. <source hwp:id="source-23">Frontiers in Psychology</source>, <volume>2</volume>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3389/fpsyg.2011.00172" ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2011.00172" hwp:id="ext-link-27">https://doi.org/10.3389/fpsyg.2011.00172</ext-link></citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Neri P."><surname>Neri</surname>, <given-names>P.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-22">Semantic control of feature extraction from natural scenes</article-title>. <source hwp:id="source-24">Journal of Neuroscience</source>, <volume>34</volume>(<issue>6</issue>), <fpage>2374</fpage>–<lpage>2388</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.1755-13.2014" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1755-13.2014" hwp:id="ext-link-28">https://doi.org/10.1523/JNEUROSCI.1755-13.2014</ext-link></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Nishimoto S."><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vu A. T."><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benjamini Y."><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yu B."><surname>Yu</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gallant J. L."><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-23">Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title>. <source hwp:id="source-25">Current Biology</source>, <volume>21</volume>(<issue>19</issue>), <fpage>1641</fpage>–<lpage>1646</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.cub.2011.08.031" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2011.08.031" hwp:id="ext-link-29">https://doi.org/10.1016/j.cub.2011.08.031</ext-link></citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Olshausen B. A."><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Field D. J."><surname>Field</surname>, <given-names>D. J.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-24">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source hwp:id="source-26">Nature</source>, <volume>381</volume>(<issue>6583</issue>), <fpage>607</fpage>–<lpage>609</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/381607a0" ext-link-type="uri" xlink:href="https://doi.org/10.1038/381607a0" hwp:id="ext-link-30">https://doi.org/10.1038/381607a0</ext-link></citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Pelli D. G."><surname>Pelli</surname>, <given-names>D. G.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bex P."><surname>Bex</surname>, <given-names>P.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-25">Measuring contrast sensitivity</article-title>. <source hwp:id="source-27">Vision Research</source>, <volume>90</volume>, <fpage>10</fpage>–<lpage>14</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.visres.2013.04.015" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2013.04.015" hwp:id="ext-link-31">https://doi.org/10.1016/j.visres.2013.04.015</ext-link></citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2 xref-ref-28-3"><citation publication-type="website" citation-type="web" ref:id="2021.07.08.451706v2.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Rideaux R."><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="West R. K."><surname>West</surname>, <given-names>R. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bex P. J."><surname>Bex</surname>, <given-names>P. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mattingley J. B."><surname>Mattingley</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Harrison W. J."><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-26">Spatial structure, phase, and the contrast of natural images</article-title>. <source hwp:id="source-28">BioRxiv</source>, 2021.06.16.448761. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/2021.06.16.448761" ext-link-type="uri" xlink:href="https://doi.org/10.1101/2021.06.16.448761" hwp:id="ext-link-32">https://doi.org/10.1101/2021.06.16.448761</ext-link></citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Sebastian S."><surname>Sebastian</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abrams J."><surname>Abrams</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Geisler W. S."><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-27">Constrained sampling experiments reveal principles of detection in natural scenes</article-title>. <source hwp:id="source-29">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>114</volume>(<issue>28</issue>), <fpage>E5731</fpage>–<lpage>E5740</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.1619487114" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1619487114" hwp:id="ext-link-33">https://doi.org/10.1073/pnas.1619487114</ext-link></citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Taylor R."><surname>Taylor</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bays P. M."><surname>Bays</surname>, <given-names>P. M.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-28">Efficient Coding in Visual Working Memory Accounts for Stimulus-Specific Variations in Recall</article-title>. <source hwp:id="source-30">Journal of Neuroscience</source>, <volume>38</volume>(<issue>32</issue>), <fpage>7132</fpage>–<lpage>7142</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.1018-18.2018" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1018-18.2018" hwp:id="ext-link-34">https://doi.org/10.1523/JNEUROSCI.1018-18.2018</ext-link></citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Torralba A."><surname>Torralba</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Oliva A."><surname>Oliva</surname>, <given-names>A.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-29">Statistics of natural image categories</article-title>. <source hwp:id="source-31">Network: Computation in Neural Systems</source>, <volume>14</volume>(<issue>3</issue>), <fpage>391</fpage>–<lpage>412</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1088/0954-898X_14_3_302" ext-link-type="uri" xlink:href="https://doi.org/10.1088/0954-898X_14_3_302" hwp:id="ext-link-35">https://doi.org/10.1088/0954-898X_14_3_302</ext-link></citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Wei X.-X."><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Stocker A. A."><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-30">A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts</article-title>. <source hwp:id="source-32">Nature Neuroscience</source>, <volume>18</volume>(<issue>10</issue>), <fpage>1509</fpage>–<lpage>1517</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/nn.4105" ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4105" hwp:id="ext-link-36">https://doi.org/10.1038/nn.4105</ext-link></citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Willenbockel V."><surname>Willenbockel</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sadr J."><surname>Sadr</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fiset D."><surname>Fiset</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Horne G. O."><surname>Horne</surname>, <given-names>G. O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gosselin F."><surname>Gosselin</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tanaka J. W."><surname>Tanaka</surname>, <given-names>J. W.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-31">Controlling low-level image properties: The SHINE toolbox</article-title>. <source hwp:id="source-33">Behavior Research Methods</source>, <volume>42</volume>(<issue>3</issue>), <fpage>671</fpage>–<lpage>684</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3758/BRM.42.3.671" ext-link-type="uri" xlink:href="https://doi.org/10.3758/BRM.42.3.671" hwp:id="ext-link-37">https://doi.org/10.3758/BRM.42.3.671</ext-link></citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Yin R. K."><surname>Yin</surname>, <given-names>R. K.</given-names></string-name> (<year>1969</year>). <article-title hwp:id="article-title-32">Looking at upside-down faces</article-title>. <source hwp:id="source-34">Journal of Experimental Psychology</source>, <volume>81</volume>(<issue>1</issue>), <fpage>141</fpage>–<lpage>145</lpage>. <collab hwp:id="collab-1">Scopus</collab>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/h0027474" ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0027474" hwp:id="ext-link-38">https://doi.org/10.1037/h0027474</ext-link></citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.07.08.451706v2.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Yoshida T."><surname>Yoshida</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ohki K."><surname>Ohki</surname>, <given-names>K.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-33">Natural images are reliably represented by sparse and variable populations of neurons in visual cortex</article-title>. <source hwp:id="source-35">Nature Communications</source>, <volume>11</volume>(<issue>1</issue>), <fpage>872</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/s41467-020-14645-x" ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-020-14645-x" hwp:id="ext-link-39">https://doi.org/10.1038/s41467-020-14645-x</ext-link></citation></ref></ref-list></back></article>
