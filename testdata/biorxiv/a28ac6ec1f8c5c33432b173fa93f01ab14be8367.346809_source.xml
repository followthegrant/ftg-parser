<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/346809</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;346809v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;346809</article-id><article-id pub-id-type="other" hwp:sub-type="slug">346809</article-id><article-id pub-id-type="other" hwp:sub-type="tag">346809</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Ecology" hwp:journal="biorxiv"><subject>Ecology</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Machine learning to classify animal species in camera trap images: applications in ecology</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp hwp:id="corresp-1">Corresponding Authors: Michael Tabak &amp; Ryan Miller Center for Epidemiology and Animal Health United States Department of Agriculture 2150 Centre Ave., Bldg B Fort Collins, CO 80526 +1-970-494-7272 <email hwp:id="email-1">Michael.a.tabak@aphis.usda.gov</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2986-7885</contrib-id><name name-style="western" hwp:sortable="Tabak Michael A."><surname>Tabak</surname><given-names>Michael A.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-2986-7885"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Norouzzadeh Mohammad S."><surname>Norouzzadeh</surname><given-names>Mohammad S.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Wolfson David W."><surname>Wolfson</surname><given-names>David W.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Sweeney Steven J."><surname>Sweeney</surname><given-names>Steven J.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="VerCauteren Kurt C."><surname>VerCauteren</surname><given-names>Kurt C.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Snow Nathan P."><surname>Snow</surname><given-names>Nathan P.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-2" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><name name-style="western" hwp:sortable="Halseth Joseph M."><surname>Halseth</surname><given-names>Joseph M.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-3" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-8"><name name-style="western" hwp:sortable="Di Salvo Paul A."><surname>Di Salvo</surname><given-names>Paul A.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-9"><name name-style="western" hwp:sortable="Lewis Jesse S."><surname>Lewis</surname><given-names>Jesse S.</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref></contrib><contrib contrib-type="author" hwp:id="contrib-10"><name name-style="western" hwp:sortable="White Michael D."><surname>White</surname><given-names>Michael D.</given-names></name><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-1" hwp:rel-id="aff-6">6</xref></contrib><contrib contrib-type="author" hwp:id="contrib-11"><name name-style="western" hwp:sortable="Teton Ben"><surname>Teton</surname><given-names>Ben</given-names></name><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-2" hwp:rel-id="aff-6">6</xref></contrib><contrib contrib-type="author" hwp:id="contrib-12"><name name-style="western" hwp:sortable="Beasley James C."><surname>Beasley</surname><given-names>James C.</given-names></name><xref ref-type="aff" rid="a7" hwp:id="xref-aff-7-1" hwp:rel-id="aff-7">7</xref></contrib><contrib contrib-type="author" hwp:id="contrib-13"><name name-style="western" hwp:sortable="Schlichting Peter E."><surname>Schlichting</surname><given-names>Peter E.</given-names></name><xref ref-type="aff" rid="a7" hwp:id="xref-aff-7-2" hwp:rel-id="aff-7">7</xref></contrib><contrib contrib-type="author" hwp:id="contrib-14"><name name-style="western" hwp:sortable="Boughton Raoul K."><surname>Boughton</surname><given-names>Raoul K.</given-names></name><xref ref-type="aff" rid="a8" hwp:id="xref-aff-8-1" hwp:rel-id="aff-8">8</xref></contrib><contrib contrib-type="author" hwp:id="contrib-15"><name name-style="western" hwp:sortable="Wight Bethany"><surname>Wight</surname><given-names>Bethany</given-names></name><xref ref-type="aff" rid="a8" hwp:id="xref-aff-8-2" hwp:rel-id="aff-8">8</xref></contrib><contrib contrib-type="author" hwp:id="contrib-16"><name name-style="western" hwp:sortable="Newkirk Eric S."><surname>Newkirk</surname><given-names>Eric S.</given-names></name><xref ref-type="aff" rid="a9" hwp:id="xref-aff-9-1" hwp:rel-id="aff-9">9</xref></contrib><contrib contrib-type="author" hwp:id="contrib-17"><name name-style="western" hwp:sortable="Ivan Jacob S."><surname>Ivan</surname><given-names>Jacob S.</given-names></name><xref ref-type="aff" rid="a9" hwp:id="xref-aff-9-2" hwp:rel-id="aff-9">9</xref></contrib><contrib contrib-type="author" hwp:id="contrib-18"><name name-style="western" hwp:sortable="Odell Eric A."><surname>Odell</surname><given-names>Eric A.</given-names></name><xref ref-type="aff" rid="a9" hwp:id="xref-aff-9-3" hwp:rel-id="aff-9">9</xref></contrib><contrib contrib-type="author" hwp:id="contrib-19"><name name-style="western" hwp:sortable="Brook Ryan K."><surname>Brook</surname><given-names>Ryan K.</given-names></name><xref ref-type="aff" rid="a10" hwp:id="xref-aff-10-1" hwp:rel-id="aff-10">10</xref></contrib><contrib contrib-type="author" hwp:id="contrib-20"><name name-style="western" hwp:sortable="Lukacs Paul M."><surname>Lukacs</surname><given-names>Paul M.</given-names></name><xref ref-type="aff" rid="a11" hwp:id="xref-aff-11-1" hwp:rel-id="aff-11">11</xref></contrib><contrib contrib-type="author" hwp:id="contrib-21"><name name-style="western" hwp:sortable="Moeller Anna K."><surname>Moeller</surname><given-names>Anna K.</given-names></name><xref ref-type="aff" rid="a11" hwp:id="xref-aff-11-2" hwp:rel-id="aff-11">11</xref></contrib><contrib contrib-type="author" hwp:id="contrib-22"><name name-style="western" hwp:sortable="Mandeville Elizabeth G."><surname>Mandeville</surname><given-names>Elizabeth G.</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a12" hwp:id="xref-aff-12-1" hwp:rel-id="aff-12">12</xref></contrib><contrib contrib-type="author" hwp:id="contrib-23"><name name-style="western" hwp:sortable="Clune Jeff"><surname>Clune</surname><given-names>Jeff</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-24"><name name-style="western" hwp:sortable="Miller Ryan S."><surname>Miller</surname><given-names>Ryan S.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5"><label>1</label><institution hwp:id="institution-1">Center for Epidemiology and Animal Health; United States Department of Agriculture</institution>; 2150 Centre Ave., Bldg B, Fort Collins, CO 80526</aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label><institution hwp:id="institution-2">Department of Zoology and Physiology; University of Wyoming</institution>; 1000 E. University Ave., Laramie, WY 52071</aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2"><label>3</label><institution hwp:id="institution-3">Computer Science Department; University of Wyoming</institution>; 1000 E. University Ave., Laramie, WY 52071</aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1 xref-aff-4-2 xref-aff-4-3"><label>4</label><institution hwp:id="institution-4">National Wildlife Research Center; United States Department of Agriculture</institution>; 4101 Laporte Ave., Fort Collins, CO 80521</aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">College of Integrative Sciences and Arts; Arizona State University</institution>; 66307 South Backus Mall, Mesa, AZ 85212</aff><aff id="a6" hwp:id="aff-6" hwp:rev-id="xref-aff-6-1 xref-aff-6-2"><label>6</label><institution hwp:id="institution-6">Tejon Ranch Conservancy</institution>, 1037 Bear Trap Rd, Lebec, CA, 93243</aff><aff id="a7" hwp:id="aff-7" hwp:rev-id="xref-aff-7-1 xref-aff-7-2"><label>7</label><institution hwp:id="institution-7">Savannah River Ecology Laboratory; Warnell School of Forestry and Natural Resources, University of Georgia</institution>; PO Drawer E, Aiken, SC 29802, <country>USA</country></aff><aff id="a8" hwp:id="aff-8" hwp:rev-id="xref-aff-8-1 xref-aff-8-2"><label>8</label><institution hwp:id="institution-8">Range Cattle Research and Education Center; Wildlife Ecology and Conservation; University of Florida</institution>; 3401 Experiment Station, Ona, Florida 33865</aff><aff id="a9" hwp:id="aff-9" hwp:rev-id="xref-aff-9-1 xref-aff-9-2 xref-aff-9-3"><label>9</label><institution hwp:id="institution-9">Colorado Parks and Wildlife</institution>; 317 W. Prospect Rd., Fort Collins, CO 80526</aff><aff id="a10" hwp:id="aff-10" hwp:rev-id="xref-aff-10-1"><label>10</label><institution hwp:id="institution-10">Department of Animal and Poultry Science; University of Saskatchewan</institution>; 5 Campus Drive, Saskatoon, SK, <country>Canada</country> S7N 5A8</aff><aff id="a11" hwp:id="aff-11" hwp:rev-id="xref-aff-11-1 xref-aff-11-2"><label>11</label><institution hwp:id="institution-11">Wildlife Biology Program, Department of Ecosystem and Conservation Sciences; W.A. Franke College of Forestry and Conservation; University of Montana</institution>; 32 Campus Drive, Missoula, MT 59812</aff><aff id="a12" hwp:id="aff-12" hwp:rev-id="xref-aff-12-1"><label>12</label><institution hwp:id="institution-12">Department of Botany; University of Wyoming</institution>; 1000 E. University Ave., Laramie, WY 52071</aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-06-13T23:12:04-07:00">
    <day>13</day><month>6</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-07-09T13:26:51-07:00">
    <day>9</day><month>7</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-06-13T23:18:55-07:00">
    <day>13</day><month>6</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-07-09T13:33:56-07:00">
    <day>9</day><month>7</month><year>2018</year>
  </pub-date><elocation-id>346809</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-06-13"><day>13</day><month>6</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-07-09"><day>09</day><month>7</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-07-09"><day>09</day><month>7</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license hwp:id="license-1"><p hwp:id="p-1">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="346809.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/346809v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="346809.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/346809v3/346809v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/346809v3/346809v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">1. Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and have been regarded as among the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analyzed, typically by visually observing each image, in order to extract data that can be used in ecological analyses.</p><p hwp:id="p-3">2. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model.</p><p hwp:id="p-4">3. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82% accuracy, and correctly identified 94% of images containing an animal in the dataset from Tanzania. We provide an R package (Machine Learning for Wildlife Image Classification; MLWIC) that allows the users to A) implement the trained model presented here and B) train their own model using classified images of wildlife from their studies.</p><p hwp:id="p-5">4. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analyzing images. We present an R package making these methods accessible to ecologists. We discuss the implications of this technology for ecology and considerations that should be addressed in future implementations of these methods.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">artificial intelligence</kwd><kwd hwp:id="kwd-2">camera trap</kwd><kwd hwp:id="kwd-3">convolutional neural network</kwd><kwd hwp:id="kwd-4">deep neural networks</kwd><kwd hwp:id="kwd-5">image classification</kwd><kwd hwp:id="kwd-6">machine learning</kwd><kwd hwp:id="kwd-7">R package</kwd><kwd hwp:id="kwd-8">remote sensing</kwd></kwd-group><counts><page-count count="35"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">Introduction</title><p hwp:id="p-6">An understanding of species’ distributions is fundamental to many questions in ecology (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">MacArthur, 1984</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Brown, 1995)</xref>. Observations of wildlife can be used to model species distributions and population abundance and evaluate how these metrics relate to environmental conditions (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Elith, Kearney, &amp; Phillips, 2010</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Tikhonov et al., 2017)</xref>. However, developing statistically sound data for species observations is often difficult and expensive (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Underwood, Chapman, &amp; Connell, 2000)</xref> and significant effort has been devoted to correcting bias in more easily collected or opportunistic observation data (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Royle &amp; Dorazio, 2008</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">MacKenzie et al., 2017)</xref>. Recently, technological advances have improved our ability to observe animals remotely. Sampling methods such as acoustic recordings, images from crewless aircraft (or “drones”), and motion-activated cameras that automatically photograph wildlife (i.e., “camera traps”) are commonly used (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Blumstein et al., 2011</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">O’Connell et al., 2011</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Getzin et al., 2012)</xref>. These tools offer great promise for increasing efficiency of observing wildlife remotely over large geographical areas with minimal human involvement and have made considerable contributions to ecology (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Rovero et al., 2013</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Howe et al., 2017)</xref>. However, a common limitation is these methods lead to a large accumulation of data - audio and video recordings and images - which must be first classified in order to be used in ecological studies predicting occupancy or abundance (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Swanson et al., 2015</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Niedballa et al., 2016)</xref>. The large burden of classification, such as manually viewing and classifying images from camera traps, often constrains studies by reducing the sampling intensity (e.g., number of cameras deployed), limiting the geographical extent and duration of studies. Recently, machine learning has emerged as a potential solution for automatically classifying recordings and images.</p><p hwp:id="p-7">Machine learning methods have been used to classify wildlife in camera trap images with varying levels of success and human involvement in the process. One application of a machine learning approach has been to distinguish empty and non-target animal images from those containing the target species to reduce the number of images requiring manual classification. This approach has been generally successful, allowing researchers to remove up to 76% of images containing non-target species (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Swinnen et al., 2014)</xref>. Development of methods to identify several wildlife species in images has been more problematic. <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Yu et al. (2013)</xref> used sparse coding spatial pyramid matching (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Lazebnik, Schmid, &amp; Ponce, 2006)</xref> to identify 18 species in images, achieving high accuracy (82%), but their approach necessitates each training image to be manually cropped, requiring a large time investment. Attempts to use machine learning to classify species in images without manual cropping have achieved far lower accuracies: 38% (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Chen et al., 2014)</xref> and 57% (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Gomez Villa, Salazar, &amp; Vargas, 2017)</xref>. However, more recently <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Norouzzadeh et al. (2018)</xref> used convolutional neural networks with 3.2 million classified images from camera traps to automatically classify 48 species of Serengeti wildlife in images with 95% accuracy.</p><p hwp:id="p-8">Despite these advances in automatically identifying wildlife in camera trap images, the approaches remain study specific and the technology is generally inaccessible to most ecologists. Training such models typically requires extensive computer programming skills and tools for novice programmers (e.g., an R package) are limited. Making this technology available to ecologists has the potential to greatly expand ecological inquiry and non-invasive sampling designs, allowing for larger and longer-term ecological studies. In addition, automated approaches to identifying wildlife in camera trap images have important applications in detecting invasive species or sensitive species and improving their management.</p><p hwp:id="p-9">We sought to develop a machine learning approach that can be applied across study sites and provide software that ecologists can use for identification of wildlife in their own camera trap images. Using over three million identified images of wildlife from camera traps from five locations across the United States, we trained and tested deep learning models that automatically classify wildlife. We provide an R package (Machine Learning for Wildlife Image Classification; MLWIC) that allows researchers to classify camera trap images from North America or train their own machine learning models to classify images. We also address some basic issues in the potential use of machine learning for classifying wildlife in camera trap images in ecology. Because our approach nearly eliminates the need for manual curation of camera trap images we also discuss how this new technology can be applied to improve ecological studies in the future.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Materials and Methods</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Camera trap images</title><p hwp:id="p-10">Species in camera trap images from five locations across the United States (California, Colorado, Florida, South Carolina, and Texas) were identified manually by researchers (see Appendix S1 for a description of each field location). Images were either classified by a single wildlife expert or evaluated independently by two researchers; any conflicts were decided by a third observer (Appendix S1). If any part of an animal (e.g., leg or ear) was identified as being present in an image, this was included as an image of the species. This resulted in a total of 3,741,656 classified images that included 28 species or groups (see <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>) across the study locations. We present these images and their classifications for other scientists to use for model development as the North American Camera Trap Images (NACTI) dataset. Images were re-sized to a resolution of 256 × 256 pixels using a custom Python script before running models to increase processing speed. A subset of images (approximately 10%) was withheld using conditional sampling to be used for testing of the model (described below). This resulted in 3,367,383 images used to train the model and 374,273 images used for testing.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3 xref-table-wrap-1-4 xref-table-wrap-1-5 xref-table-wrap-1-6 xref-table-wrap-1-7 xref-table-wrap-1-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;346809v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-1"><p hwp:id="p-11">Accuracy of the Species Level model</p></caption><graphic xlink:href="346809_tbl1" position="float" orientation="portrait" hwp:id="graphic-1"/><graphic xlink:href="346809_tbl1a" position="float" orientation="portrait" hwp:id="graphic-2"/><graphic xlink:href="346809_tbl1b" position="float" orientation="portrait" hwp:id="graphic-3"/></table-wrap></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Machine learning process</title><p hwp:id="p-12">Supervised machine learning algorithms use training examples to “learn” how to complete a task (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Mohri, Rostamizadeh, &amp; Talwalkar, 2012</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Goodfellow, Bengio, &amp; Courville, 2016)</xref>. One popular class of machine learning algorithms is artificial neural network, which loosely mimics the learning behavior of the mammalian brain (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Gurney, 2014</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">Goodfellow et al., 2016)</xref>. An artificial neuron in a neural network has several inputs, each with an associated weight. For each artificial neuron, the inputs are multiplied by the weights, summed, and then evaluated by a non-linear function, which is called the activation function (e.g., Sigmoid, Tanh, or Sine). Usually each neuron also has an extra connection with a constant input value of 1 and its associated weight, called a “bias,” for neurons. The result of the activation function can be passed as input into other artificial neurons or serve as network outputs. For example, consider an artificial neuron with three inputs (<italic toggle="yes">I</italic><sub>1</sub>, <italic toggle="yes">I</italic><sub>2</sub>, and <italic toggle="yes">I</italic><sub>3</sub>); the output (θ) is calculated based on:
<disp-formula id="eqn1" hwp:id="disp-formula-1"><alternatives hwp:id="alternatives-1"><graphic xlink:href="346809_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives></disp-formula>
where <italic toggle="yes">w</italic><sub>1</sub>, <italic toggle="yes">w</italic><sub>2</sub>, <italic toggle="yes">w</italic><sub>3</sub> and <italic toggle="yes">w</italic><sub>4</sub> are the weights associated with each input, <italic toggle="yes">I</italic><sub><italic toggle="yes">b</italic></sub> is the bias, and <italic toggle="yes">Tanh(x)</italic> is the activation function (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1</xref>). To solve complex problems multiple neurons are needed, so we put them into a network. We arrange neurons in a hierarchical structure of layers; neurons in each layer take input from the previous layer, process them, and pass the output to the next layer. Then, an algorithm, called backpropagation (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Rumelhart, Hinton, &amp; Williams, 1986)</xref>, tunes the parameters of the neural network (weights and bias values) enabling it to produce the desired output when we feed an input to the network. This process is called training. To adjust the weights, we define a loss function as a measure of the difference between the predicted (current) output of the neural network and the correct output <italic toggle="yes">(Y).</italic> The loss function <italic toggle="yes">(L)</italic> is the mean squared error:
<disp-formula id="eqn2" hwp:id="disp-formula-2"><alternatives hwp:id="alternatives-2"><graphic xlink:href="346809_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives></disp-formula></p><p hwp:id="p-13">We compute the contribution of each weight to the loss value <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="346809_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> using the chain rule in calculus. Weights are then adjusted so the loss value is minimized. In this “weight update” step, all the weights are updated to minimize <italic toggle="yes">L:</italic>
<disp-formula id="eqn3" hwp:id="disp-formula-3"><alternatives hwp:id="alternatives-4"><graphic xlink:href="346809_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives></disp-formula>
where <italic toggle="yes">η</italic> is the learning rate and is chosen by the scientist. A higher <italic toggle="yes">η</italic> indicates larger steps are taken per training sample, which may be faster, but a value that is too large will be imprecise and can destabilize learning. After adjusting the weights, the same input should result in an output that is closer to the desired output. For more details of backpropagation and training, see <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-3" hwp:rel-id="ref-10">Goodfellow et al., 2016</xref>.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;346809v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-2"><p hwp:id="p-14">Within an artificial neural network, inputs (<italic toggle="yes">I</italic>) are multiplied by their weights (<italic toggle="yes">w</italic>), summed, and then evaluated by a non-linear function, which also accounts for bias <italic toggle="yes">(l</italic><sub><italic toggle="yes">b</italic></sub>). The output <italic toggle="yes">(θ)</italic> can be passed as input into other neurons or serve as network outputs. Backpropagation involves adjusting the weights so that a model can provide the desired output.</p></caption><graphic xlink:href="346809_fig1" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-15">In fully connected neural networks, each neuron in every layer is connected to (provides input to) every neuron in the next layer. Conversely, in convolutional neural networks, which are inspired by the retina of the human eye, several convolutional layers exist in which each neuron only receives input from a small sliding subset of neurons (“receptive field”) in the previous layer. We call the output of a group of neurons the “feature map,” which depicts the response of a neuron to its input. When we use convolutional neural networks to classify animal images, the receptive field of neurons in the first layer of the network is a sliding subset of the image. In subsequent layers, the receptive field of neurons is a sliding subset of the feature map from previous layers. We interpret the output of the final layer as the probability of the presence of species in the image. A softmax function is used at the final layer to ensure that the outputs sum to one. For more details on this process, see <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Simonyan &amp; Zisserman, 2014</xref>.</p><p hwp:id="p-16">Deep neural networks (or “deep learning”) are artificial networks with several (&gt; 3) layers of structure. In our example, we provided a set of animal images from camera traps of different species and their labels (species identifiers) to a deep neural network, and the model learned how to identify species in other images that were not used for training. Once a model is trained, we can use it to classify new images. The trained model uses the output of the final layer in the network to assign a confidence to each species or group it evaluates, where the total confidence assigned to all groups for each image sums to one. Generally, the majority of the confidence is attributed to one group, the “top guess.” For example, for 90% of the images in our test dataset, the model attributed &gt; 95% confidence to the top guess. Therefore, for the purpose of this paper, we mainly discuss accuracy with regard to the top guess, but our R package presents the five groups with the highest confidence, the “top five guesses,” and the confidence associated with each guess.</p><p hwp:id="p-17">Neural network architecture refers to several details about the network including the type and number of neurons and the number of layers. We trained a deep convolutional neural network (ResNet-18) architecture because it has few parameters, but performs well; see <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">He et al. (2016)</xref> for full details of this architecture. Networks were trained in the TensorFlow framework (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Adabi et al., 2016)</xref> using Mount Moran, a high performance computing cluster (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Advanced Research Computing Center, 2012</xref>). First, since invasive wild pigs <italic toggle="yes">(Sus scrofa)</italic> are a subject of several of our field studies, we developed a “Pig/no pig” model, in which we determined if a pig was either present or absent in the image. In the “Species Level” model, we identified images to the species level when possible. Specifically, if our classified image dataset included &lt; 2,000 images for a species, it was either grouped with taxonomically similar species (by genera, families, or order), or it was not included in the trained model (<xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>). In the “Group Level” model, species were grouped with taxonomically similar species into classifications that had ecological relevance (Appendix S2). The Group Level model contained fewer groups than the Species Level model, so that more training images were available for each group. We used both models because if the Species Level model had poor accuracy, we predicted the Group Level model would have better accuracy since more training images would be available for several groups. As it is the most broadly applicable model and is the one implemented in the MLWIC package, we will mainly discuss the Species Level model here, but show results from the Group Level to demonstrate alternative approaches.</p><p hwp:id="p-18">For each of the three models, 90% of the classified images for each species or group were used to train the model and 10% of the images were used to test it in most cases. However, we wanted to evaluate the model’s performance for each species present at each study site, so we altered training-testing allocation for the rare situations where there were few classified images of a species at a site. Specifically, with 1-9 classified images for a species at a site, we used all of these images for testing and none for training; for site-species pairs with 10-30 images, 50% were used for training and testing; and for &gt; 30 images per site for each species, 90% were allocated to training and 10% to testing (Appendices S3 - S7 show the number of training and test images for each species at each site).</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-7">Evaluating model accuracy</title><p hwp:id="p-19">Model testing was conducted by running the trained model on the withheld images that were not used to train the model. Accuracy (<italic toggle="yes">A</italic>) was assessed as the proportion of images in the test dataset (<italic toggle="yes">N</italic>) that were correctly classified (<italic toggle="yes">C</italic>) by the top guess (<italic toggle="yes">A = C/N</italic>). Top 5 accuracy (<italic toggle="yes">A</italic>5) was defined as the proportion of images in the test dataset that were correctly classified by any of the top 5 assignments (<italic toggle="yes">C</italic>5; <italic toggle="yes">A</italic>5 = <italic toggle="yes">C</italic>5/<italic toggle="yes">N</italic>). For each species or group we calculated the rate of false positives (<italic toggle="yes">FP</italic>) as the proportion of images classified as this species or group (<italic toggle="yes">N</italic><sub><italic toggle="yes">modei group</italic></sub>) by the model’s top guess that contained a different species according to human observers (<italic toggle="yes">N</italic><sub><italic toggle="yes">true other</italic></sub>; <italic toggle="yes">FP</italic> = <italic toggle="yes">N</italic><sub><italic toggle="yes">true other</italic></sub>/<italic toggle="yes">N</italic><sub><italic toggle="yes">model group</italic></sub>). We calculated the rate of false negatives for each species (<italic toggle="yes">FN</italic>) as the proportion of images observers classified as a specific species or group (<italic toggle="yes">N</italic><sub><italic toggle="yes">true group</italic></sub>) that the model’s top guess classified differently (<italic toggle="yes">N</italic><sub><italic toggle="yes">modei other</italic></sub>; <italic toggle="yes">FN</italic> = <italic toggle="yes">N</italic><sub><italic toggle="yes">model other</italic></sub>/<italic toggle="yes">N</italic><sub><italic toggle="yes">true group</italic></sub>). This assumes the observers were correct in their classification of images. We fit generalized additive models (GAMs) to the relationship between accuracy and the logarithm (base 10) of the number of images used to train the model. We also calculated the accuracy and rates of error specific to each of the five data sets from which images were acquired.</p><p hwp:id="p-20">To evaluate how the model would perform for a completely new study site in North America, we used a dataset of 5,900 classified images of ungulates (moose, cattle, elk, and wild pigs) from Saskatchewan, Canada by running the Species Level model on these images. We also evaluated the ability of the model to operate on images with a completely different species community (from Tanzania) to determine the model’s ability to correctly classify images as having an animal or being empty when encountering new species that it has not been trained to recognize. This was done using 3.2 million classified images from the Snapshot Serengeti dataset (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">Swanson et al., 2015)</xref>.</p></sec></sec><sec id="s3" hwp:id="sec-6"><title hwp:id="title-8">Results</title><p hwp:id="p-21">Our models performed well, achieving ≥ 97.5% accuracy of identifying the correct species with the top guess (<xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>). The model determining presence or absence of wild pigs had the highest accuracy of all of our models (98.6%; Pig/no pig; <xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2</xref>). For the Species Level and Group Level models, the top 5 accuracy was &gt; 99.9%. The model confidence in the correct answer varied, but was mostly &gt; 95%; see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2</xref> for confidences for each image for three example species. Supporting a similar finding for camera trap images in <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">Norouzzadeh et al. (2018)</xref>, and a general trend in deep learning (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-4" hwp:rel-id="ref-10">Goodfellow et al., 2016)</xref>, species and groups that had more images available for training were classified more accurately (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3</xref>, <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Table 1</xref>). GAMs relating the number of training images with accuracy predicted 95% accuracy could be achieved when approximately 71,000 training images were available for a species or group. However, these models were not perfect fits to the data, and for several species and groups, 95% accuracy was achieved with fewer than 70,000 images (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3</xref>). We found there was not a large effect of daytime vs. nighttime on accuracy in the Species Level model as daytime accuracy was 98.2% and nighttime accuracy was 96.6%. The top 5 accuracies for both times of day were ≥ 99.9%. When we subsetted the testing dataset by study site, we found that site-specific accuracies ranged from 90-99% (Appendices S3 - S7). The model performed poorly (0 - 22% accuracy) for species in the four instances when the model did not include training images from that site (when &lt; 10 classified images were available for the species/study site combination; Appendices S3 - S7).</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;346809v3/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-3"><p hwp:id="p-22">Accuracy (across all images for all species) of the three deep learning tasks analyzed</p></caption><graphic xlink:href="346809_tbl2" position="float" orientation="portrait" hwp:id="graphic-8"/></table-wrap><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;346809v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Fig. 2:</label><caption hwp:id="caption-4"><p hwp:id="p-23">Histograms represent the confidence assigned by all of the top five guesses by the Species Level model for each of these three example species when it was present in an image. The dashed line represents 95% confidence; the majority of model-assigned confidences were greater than this value.</p></caption><graphic xlink:href="346809_fig2" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;346809v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Fig. 3:</label><caption hwp:id="caption-5"><p hwp:id="p-24">Machine learning model accuracy increased with the size of the training dataset. Points represent each species or group of species. The line represents the result of generalized additive models relating the two variables.</p></caption><graphic xlink:href="346809_fig3" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-25">Upon further investigation, we found these images were difficult to classify manually. For example, striped skunks in Florida were misclassified in both of the images from this study site (Appendix S5). These images both contained the same individual at the same camera, and most wildlife experts would not classify it as a skunk (Appendix S8).</p><p hwp:id="p-26">When we conducted out-of-sample validation by using our model to evaluate images of ungulates from Canada, we achieved an overall accuracy of 81.8% with a top 5 accuracy of 90.9%. When we tested the ability of our model to accurately predict presence or absence of an animal in the image using the Serengeti Snapshot dataset, we found that 85.1% were classified correctly as empty, while 94.3% of images containing an animal were classified as containing an animal. Our trained model was capable of classifying approximately 2,000 images per minute on a Macintosh laptop with 16 gigabytes (GB) of RAM.</p></sec><sec id="s4" hwp:id="sec-7"><title hwp:id="title-9">Discussion</title><p hwp:id="p-27">To our knowledge, our Species Level model achieved the highest accuracy (97.5%) to date in using machine learning for wildlife image classification (a recent paper achieved 95% accuracy; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-3" hwp:rel-id="ref-24">Norouzzadeh et al., 2018)</xref>. This model performed almost as well during the night as during the day (accuracy = 97% and 98%, respectively). We provide this model as an R package (MLWIC), which is especially useful for researchers studying the species and groups available in this package (<xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-4" hwp:rel-id="T1">Table 1</xref>) in North America, as it performed well in classifying ungulates in an out-of-sample test of images from Canada. The model can also be valuable for researchers studying other species by removing images without any animals from the dataset before beginning manual classification, as we achieved high accuracy in separating empty images from those containing animals in a dataset from Tanzania. This R package can also be a valuable tool for any researchers that have classified images, as they can use the package to train their own model that can then classify any subsequent images collected.</p><sec id="s4a" hwp:id="sec-8"><title hwp:id="title-10">Optimizing camera trap use and application in ecology</title><p hwp:id="p-28">The ability to rapidly identify millions of images from camera traps can fundamentally change the way ecologists design and implement wildlife studies. Camera trap projects amass large numbers of images which require a sizable time investment to manually classify. For example, the Snapshot Serengeti project (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-3" hwp:rel-id="ref-36">Swanson et al., 2015)</xref> amassed millions of images and employed 28,000 volunteers to manually classify 1.5 million images (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Swanson et al., 2016</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Palmer et al., 2017)</xref>. We found researchers can classify approximately 200 images per hour. Therefore, a project that amasses 1 million images would require 10,000 hours for each image to be doubly observed. To reduce the number of images that need to be classified manually, ecologists using camera traps often limit the number of photos taken by reducing the size of camera arrays, reducing the duration of camera trap studies, and imposing limits on the number of photos a camera takes (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Kelly et al., 2008</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Scott et al., 2018)</xref>. This constraint can be problematic in many studies, particularly those addressing rare or elusive species that are often the subject of ecological studies (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">O’Connell et al., 2011</xref>), as these species often require more effort to detect (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Tobler et al., 2008)</xref>. Using deep learning methods to automatically classify images essentially eliminates one of the primary reasons camera trap arrays are limited in size or duration. The Species Level model in our R package can accurately classify 1 million images in less than nine hours with minimal human involvement.</p><p hwp:id="p-29">Another reason to limit the number of photos taken by camera traps is storage limitations on cameras (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Rasambainarivo et al., 2017</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Hanya et al., 2018)</xref>. When classifying images manually, we might try to use high resolution photos to improve technicians’ abilities to accurately classify images, but higher resolution photos require more storage on cameras. Our results show a model can be accurately trained and applied using low-resolution (256 × 256 pixel) images, but many of these images were re-sized from a higher resolution, which might contain more information than those which originated at a low resolution. Nevertheless, we hypothesize a model can be accurately trained using images from low resolution cameras, and our R package allows users who have such images to test this hypothesis. If supported, this can make camera trap data storage much more efficient. Typical cameras set for 2048 × 1536 pixel resolution will run out of storage space when they reach approximately 1,250 photos per GB of storage. Taking low resolution images instead can increase the number of photos stored per GB to about 10,000 and thus decrease the frequency at which researchers must visit cameras to change storage cards by a factor of eight. Minimizing human visitation also will reduce human scents and disturbances that could deter some species from visiting cameras. In the future, it may be possible to implement a machine learning model on a game camera (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Elias et al., 2017)</xref> that automatically classifies images as empty or containing animals so that empty images are discarded immediately and not stored on the camera. This type of approach could dramatically reduce the frequency with which technicians need to visit cameras. Furthermore, if models effectively use low-resolution images, it is not necessary for researchers to purchase high resolution cameras. Instead, researchers can purchase lower cost, lower resolution cameras and allocate funding toward purchasing more cameras and creating larger camera arrays.</p></sec><sec id="s4b" hwp:id="sec-9"><title hwp:id="title-11">Applications to management of invasive and sensitive species</title><p hwp:id="p-30">By removing some of the major burdens associated with the use of camera traps, our approach can be utilized by ecologists and wildlife managers to conduct more extensive camera trapping surveys than were previously possible. One potential use is in monitoring the distribution of sensitive or invasive species. For example, the distribution of invasive wild pigs in North America is commonly monitored using camera traps. Humans introduce this species into new locations that are often geographically distant from their existing range (<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Tabak et al., 2017)</xref>, which can quickly lead to newly-established populations. Camera traps could be placed in areas at risk for introduction and provide constant surveillance. An automated image classification model that simply ‘looks’ for pigs in images could monitor camera trap images and alert managers when images with pigs are found, facilitating removal of animals before populations establish. Additionally, after wild pigs have been eradicated from a region, camera traps could be used to monitor the area to verify eradication success and automatically detect re-colonization or reintroduction events. Similar approaches can be used in other study systems to more rapidly detect novel invasive species arrivals, track the effects of management interventions, monitor species of conservation concern, or monitor sensitive species following reintroduction efforts.</p></sec><sec id="s4c" hwp:id="sec-10"><title hwp:id="title-12">Limitations</title><p hwp:id="p-31">Using out-of-sample model validation on a dataset from Canada revealed a lower accuracy (82%) than at study sites from which our model was trained. Additionally, when we did not include images of species/site combinations in training the model, due to low sample sizes, the model performed poorly (Appendices S3 - S7; but these images were often difficult to classify even by wildlife experts, Appendix S8). One potential explanation is the model evaluated both the animal and the environment in the image and these are confounded in the species identification (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-4" hwp:rel-id="ref-24">Norouzzadeh et al., 2018)</xref>. Therefore, the model may have lower accuracies in environments that were not in the training dataset. Ideally, the training dataset would include training images representing the range of environments in which a species exists. Our model includes training images from diverse ecosystems, making it relevant for classifying images from many locations in North America. A further limitation is in our reported overall accuracy, which is reported across all of the images that were available for testing, and we had considerable imbalance in the number of images per species (<xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-5" hwp:rel-id="T1">Table 1</xref>). We provide accuracies for each species, so the reader can more directly inspect model accuracy. Finally, our model was trained using images that were classified by human observers, which are capable of making errors (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-3" hwp:rel-id="ref-25">O’Connell et al., 2011</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Meek, Vernes, &amp; Falzon, 2013)</xref>, meaning some of the images in our training dataset were likely misclassified. Supervised machine learning algorithms require such training examples, and therefore we are unaware of a method for training such models without the potential for human classification error. Instead, we must acknowledge that these models will make mistakes due to imperfections in both human observation and model accuracy.</p></sec><sec id="s4d" hwp:id="sec-11"><title hwp:id="title-13">Future directions</title><p hwp:id="p-32">As this new technology becomes more widely available, ecologists will need to decide how it will be applied in ecological analyses. For example, when using machine learning model output to design occupancy and abundance models, we can incorporate accuracy estimates that were generated when conducting model testing. The error of a machine learning model in identifying a species is similar to the problem of imperfect detection of wildlife when conducting field surveys. Wildlife are often not detected when they are present (false negatives) and occasionally detected when they are absent (false positives); ecologists have developed models to effectively estimate occupancy when data have these types of errors (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Royle &amp; Link, 2006</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Guillera-Arroita et al., 2017)</xref>. We can use Bayesian occupancy and abundance models where the central tendencies of the prior distributions for the false negative and false positive error rates are derived from testing the machine learning model (e.g., values in <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-6" hwp:rel-id="T1">Table 1</xref>). While we would expect false positive rates in occupancy models to resemble the false positive error rates for the machine learning model, false negative error rates would be a function of the both the machine learning model and the propensity for some species to avoid detection by cameras when they are present (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Tobler et al., 2015)</xref>.</p><p hwp:id="p-33">Another area in need of development is how to group taxa when few images are available for the species. We grouped species when few images were available for model training using an arbitrary cut off of approximately 2,000 images per group (<xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-7" hwp:rel-id="T1">Table 1</xref>). We had few images of horses <italic toggle="yes">(Equus</italic> spp.), but the model identified these images relatively well (93% accuracy), presumably because they are phenotypically different from other species in our dataset. We also had few images of opossums <italic toggle="yes">(Didelphis virginiana)</italic>, but we did not group this species because it is phenotypically different from other species in our dataset and was of ecological interest in our studies; we achieved lower accuracy for this species (78%). We also included a group for rodents from species for which we only had few images <italic toggle="yes">(Erethizon dorsatum, Marmota flaviventris, Genomys</italic> spp., <italic toggle="yes">Mus</italic> spp., <italic toggle="yes">Neotoma</italic> spp., <italic toggle="yes">Peromyscus</italic> spp., <italic toggle="yes">Tamais</italic> spp., and <italic toggle="yes">Rattus</italic> spp.). The model achieved relatively low accuracy for this group (79%), presumably because there were few images for training (3,279) and members of this group are phenotypically different, making it difficult for the model to train on this group. When researchers develop new machine learning models, they will need to consider the available data, the species or groups in their study, and the ecological question that the model will help address.</p><p hwp:id="p-34">Here, we mainly focused on the species or class that the model predicted with the highest confidence (the top guess), but in many cases researchers may want to incorporate information from the model’s confidence in the guess and additional model guesses. For example, if we are interested in the highest overall accuracy, we could only consider images where the confidence in the top guess is &gt; 95%. If we subset the results from our model test in this manner, we remove 10% of the images, but total accuracy increases to 99.6%. However, if the objective of a project is to identify rare species, researchers may want to focus on all images in which the model predicts that species to be in the top 5 guesses (the 5 species or groups that the model predicts to have the highest confidence). In our model test, the correct species was in the top 5 guesses in 99.9% of the images, indicating that this strategy may be viable.</p><p hwp:id="p-35">We expect the performance of machine learning models to improve in the future (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Jordan &amp; Mitchell, 2015)</xref>, allowing ecologists to further exploit this technology. Our model required manual identification of many images to obtain high levels of accuracy (<xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-8" hwp:rel-id="T1">Table 1</xref>). Our model was also limited in that we were only able to classify the presence or absence of species; we were not able to determine the number of individuals, their behavior, or demographics. Similar machine learning models are capable of including the number of animals and their behavior in classifications (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-5" hwp:rel-id="ref-24">Norouzzadeh et al., 2018)</xref>, but we could not include these factors because they were rarely recorded manually in our dataset. As machine learning techniques improve, we expect models will require fewer manually classified images to achieve high accuracy in identifying species, counting individuals, and specifying demographic information. Furthermore, as scientists begin projects intending to use machine learning to classify images, they may be more willing to spend time extracting detailed information from fewer images instead of obtaining less information from all images. This development would create a larger dataset of information from images that can be used to train models. As machine learning algorithms improve and ecologists begin considering this technology when they design studies, we think that many novel applications will arise.</p><p hwp:id="p-36">As camera trap use is a common approach to studying wildlife worldwide, there are likely now large datasets of classified images. If scientists work together and share these datasets, we can create large image libraries that span continents (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Steenweg et al., 2017)</xref>; we may eventually be able to train a machine learning model that can identify many global species and be used by researchers globally. Further, effectively sharing images and classifications can potentially be integrated with a web-based platform, similar to that employed by Camera Base (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.atrium-biodiversity.org/tools/camerabase" ext-link-type="uri" xlink:href="http://www.atrium-biodiversity.org/tools/camerabase" hwp:id="ext-link-1">http://www.atrium-biodiversity.org/tools/camerabase</ext-link>) or eMammal (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://emammal.si.edu/" ext-link-type="uri" xlink:href="https://emammal.si.edu/" hwp:id="ext-link-2">https://emammal.si.edu/</ext-link>).</p></sec></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-14">Acknowledgements</title><p hwp:id="p-37">We thank the hundreds of volunteers and employees who manually classified images and deployed camera traps. We thank Dan Walsh for facilitating cooperation amongst groups. Camera trap projects were funded by the U.S. Department of Energy under award # DE-EM0004391 to the University of Georgia Research Foundation; USDA Animal and Plant Health Inspection Service, National Wildlife Research Center and Center for Epidemiology and Animal Health; Colorado Parks and Wildlife; Canadian Natural Science and Engineering Research Council; University of Saskatchewan; and Idaho Department of Game and Fish.</p></ack><sec id="s5" hwp:id="sec-12"><title hwp:id="title-15">Data Accessibility</title><p hwp:id="p-38">The trained Species Level model is available in the R package MLWIC from github (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/mikeyEcology/MLWIC" ext-link-type="uri" xlink:href="https://github.com/mikeyEcology/MLWIC" hwp:id="ext-link-3">https://github.com/mikeyEcology/MLWIC</ext-link>). We provide the 3.7 million classified images as the North American Camera Trap Images (NACTI) dataset in a digital repository.</p></sec><sec id="s6" hwp:id="sec-13"><title hwp:id="title-16">Author Contributions</title><p hwp:id="p-39">MAT, RSM, KCV, NPS, SJS, and DWW conceived of the project; DWW, JSL, MAT, RKB, BW, PAD, JCB, MDW, BT, PES, NPS, KCV, JMH, ESN, JSI, EAO, RKB, PML, and AKM oversaw collection and manual classification of wildlife in camera trap images from the study sites; MSN and JC developed and programmed the machine learning models; MAT led the analyses and writing of the R package; EGM assisted with R package development and computing; MAT and RSM led the writing. All authors contributed critically to drafts and gave final approval for submission.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-17">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Adabi M."><surname>Adabi</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barhab P."><surname>Barhab</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen J."><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen Z."><surname>Chen</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davis A."><surname>Davis</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dean J."><surname>Dean</surname>, <given-names>J.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Zheng X."><surname>Zheng</surname>, <given-names>X.</given-names></string-name> (<year>2016</year>). <source hwp:id="source-1">TensorFlow: a system for large-scale machine learning</source> (Vol. <volume>16</volume>, pp. <fpage>265</fpage>–<lpage>283</lpage>). <conf-name>Presented at the 12th USENIX Symposium on Operating Systems Design and Implementation</conf-name>, <publisher-name>USENIX Association</publisher-name>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="website" citation-type="web" ref:id="346809v3.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><collab hwp:id="collab-1">Advanced Research Computing Center</collab>. (<year>2012</year>). <source hwp:id="source-2">Mount Moran: IBM System X cluster</source>. <publisher-loc>Laramie, WY</publisher-loc>: <publisher-name>University of Wyoming</publisher-name>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://arcc.uwyo.edu/guides/mount-moran" ext-link-type="uri" xlink:href="https://arcc.uwyo.edu/guides/mount-moran" hwp:id="ext-link-4">https://arcc.uwyo.edu/guides/mount-moran</ext-link></citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Blumstein D. T."><surname>Blumstein</surname>, <given-names>D. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mennill D. J."><surname>Mennill</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Clemins P."><surname>Clemins</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Girod L."><surname>Girod</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yao K."><surname>Yao</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Patricelli G."><surname>Patricelli</surname>, <given-names>G.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Kirschel A. N. G."><surname>Kirschel</surname>, <given-names>A. N. G.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-2">Acoustic monitoring in terrestrial environments using microphone arrays: applications, technological considerations and prospectus: Acoustic monitoring</article-title>. <source hwp:id="source-3">Journal of Applied Ecology</source>, <volume>48</volume>(<issue>3</issue>), <fpage>758</fpage>–<lpage>767</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1365–2664.2011.01993.x</pub-id></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Brown J. H."><surname>Brown</surname>, <given-names>J. H.</given-names></string-name> (<year>1995</year>). <source hwp:id="source-4">Macroecology</source>. <publisher-name>University of Chicago Press</publisher-name>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="other" citation-type="journal" ref:id="346809v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Chen G."><surname>Chen</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Han T. X."><surname>Han</surname>, <given-names>T. X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="He Z."><surname>He</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kays R."><surname>Kays</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Forrester T."><surname>Forrester</surname>, <given-names>T.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-3">Deep convolutional neural network based species recognition for wild animal monitoring</article-title> (pp. <fpage>858</fpage>–<lpage>862</lpage>). <source hwp:id="source-5">IEEE International Conference on Image Processing (ICIP)</source>. doi:<pub-id pub-id-type="doi">10.1109/ICIP.2014.7025172</pub-id></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Elias A. R."><surname>Elias</surname>, <given-names>A. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Golubovic N."><surname>Golubovic</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krintz C."><surname>Krintz</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wolski R."><surname>Wolski</surname>, <given-names>R.</given-names></string-name> (<year>2017</year>). <source hwp:id="source-6">Where’s the bear?: automating wildlife image processing using IoT and Edge Cloud Systems</source> (pp. <fpage>247</fpage>–<lpage>258</lpage>). <publisher-name>ACM Press</publisher-name>. doi:<pub-id pub-id-type="doi">10.1145/3054977.3054986</pub-id></citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Elith J."><surname>Elith</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kearney M."><surname>Kearney</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Phillips S."><surname>Phillips</surname>, <given-names>S.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-4">The art of modelling range-shifting species</article-title>. <source hwp:id="source-7">Methods in Ecology and Evolution</source>, <volume>1</volume>(<issue>4</issue>), <fpage>330</fpage>–<lpage>342</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.2041–210X.2010.00036.x</pub-id></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Getzin S."><surname>Getzin</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wiegand K."><surname>Wiegand</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schöning I."><surname>Schöning</surname>, <given-names>I.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-5">Assessing biodiversity in forests using very high-resolution images and unmanned aerial vehicles</article-title>. <source hwp:id="source-8">Methods in Ecology and Evolution</source>, <volume>3</volume>(<issue>2</issue>), <fpage>397</fpage>–<lpage>404</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.2041–210X.2011.00158.x</pub-id></citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Gomez Villa A."><surname>Gomez Villa</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Salazar A."><surname>Salazar</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Vargas F."><surname>Vargas</surname>, <given-names>F.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-6">Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks</article-title>. <source hwp:id="source-9">Ecological Informatics</source>, <volume>41</volume>, <fpage>24</fpage>–<lpage>32</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.ecoinf.2017.07.004</pub-id></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2 xref-ref-10-3 xref-ref-10-4"><citation publication-type="book" citation-type="book" ref:id="346809v3.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Goodfellow I."><surname>Goodfellow</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Courville A."><surname>Courville</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <source hwp:id="source-10">Deep Learning</source> (<edition>1st</edition> ed.). <publisher-loc>Cambridge, Massachusetts</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Guillera-Arroita G."><surname>Guillera-Arroita</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lahoz-Monfort J. J."><surname>Lahoz-Monfort</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Rooyen A. R."><surname>van Rooyen</surname>, <given-names>A. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weeks A. R."><surname>Weeks</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tingley R."><surname>Tingley</surname>, <given-names>R.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-7">Dealing with false-positive and false-negative errors about species occurrence at multiple levels</article-title>. <source hwp:id="source-11">Methods in Ecology and Evolution</source>, <volume>8</volume>(<issue>9</issue>), <fpage>1081</fpage>–<lpage>1091</lpage>. doi:<pub-id pub-id-type="doi">10.1111/2041–210X.12743</pub-id></citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="website" citation-type="web" ref:id="346809v3.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Gurney K."><surname>Gurney</surname>, <given-names>K.</given-names></string-name> (<year>2014</year>). <source hwp:id="source-12">An Introduction to Neural Networks</source> (<edition>1st</edition> ed.). <publisher-loc>London</publisher-loc>: <publisher-name>CRC Press</publisher-name>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.taylorfrancis.com/books/9781482286991" ext-link-type="uri" xlink:href="https://www.taylorfrancis.com/books/9781482286991" hwp:id="ext-link-5">https://www.taylorfrancis.com/books/9781482286991</ext-link></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Hanya G."><surname>Hanya</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Otani Y."><surname>Otani</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hongo S."><surname>Hongo</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Honda T."><surname>Honda</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Okamura H."><surname>Okamura</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Higo Y."><surname>Higo</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-8">Activity of wild Japanese macaques in Yakushima revealed by camera trapping: Patterns with respect to season, daily period and rainfall</article-title>. <source hwp:id="source-13">PLOS ONE</source>, <volume>13</volume>(<issue>1</issue>), <fpage>e0190631</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0190631</pub-id></citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang X."><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ren S."><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sun J."><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). <chapter-title>Deep Residual Learning for Image Recognition</chapter-title>. In <source hwp:id="source-14">Proceedings of the IEEE conference on computer vision and pattern recognition</source> (pp. <fpage>770</fpage>–<lpage>778</lpage>). <publisher-name>IEEE</publisher-name>. doi:<pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Howe E. J."><surname>Howe</surname>, <given-names>E. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Buckland S. T."><surname>Buckland</surname>, <given-names>S. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Després-Einspenner M.-L."><surname>Després-Einspenner</surname>, <given-names>M.-L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kühl H. S."><surname>Kühl</surname>, <given-names>H. S.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-9">Distance sampling with camera traps</article-title>. <source hwp:id="source-15">Methods in Ecology and Evolution</source>, <volume>8</volume>(<issue>11</issue>), <fpage>1558</fpage>–<lpage>1565</lpage>. doi:<pub-id pub-id-type="doi">10.1111/2041–210X.12790</pub-id></citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Jordan M. I."><surname>Jordan</surname>, <given-names>M. I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Mitchell T. M."><surname>Mitchell</surname>, <given-names>T. M.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-10">Machine learning: trends, perspectives, and prospects</article-title>. <source hwp:id="source-16">Science</source>, <volume>349</volume>(<issue>6245</issue>), <fpage>255</fpage>–<lpage>260</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.aaa8415</pub-id></citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Kelly M. J."><surname>Kelly</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Noss A. J."><surname>Noss</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Di Bitetti M. S."><surname>Di Bitetti</surname>, <given-names>M. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maffei L."><surname>Maffei</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arispe R. L."><surname>Arispe</surname>, <given-names>R. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paviolo A."><surname>Paviolo</surname>, <given-names>A.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Di Blanco Y. E."><surname>Di Blanco</surname>, <given-names>Y. E.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-11">Estimating Puma Densities from Camera Trapping across Three Study Sites: Bolivia, Argentina, and Belize</article-title>. <source hwp:id="source-17">Journal of Mammalogy</source>, <volume>89</volume>(<issue>2</issue>), <fpage>408</fpage>–<lpage>418</lpage>. doi:<pub-id pub-id-type="doi">10.1644/06-MAMM-A-424R.1</pub-id></citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Lazebnik S."><surname>Lazebnik</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schmid C."><surname>Schmid</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ponce J."><surname>Ponce</surname>, <given-names>J.</given-names></string-name> (<year>2006</year>). <chapter-title>Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</chapter-title>. In <source hwp:id="source-18">Computer vision and pattern recognition</source> (Vol. <volume>2</volume>, pp. <fpage>2169</fpage>–<lpage>2178</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>IEEE</publisher-name>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="MacArthur R. H."><surname>MacArthur</surname>, <given-names>R. H.</given-names></string-name> (<year>1984</year>). <source hwp:id="source-19">Geographical ecology: patterns in the distribution of species</source>. <publisher-loc>Princeton, New Jersey</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="MacKenzie D. I."><surname>MacKenzie</surname>, <given-names>D. I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichols J. D."><surname>Nichols</surname>, <given-names>J. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Royle J. A."><surname>Royle</surname>, <given-names>J. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pollock K. H."><surname>Pollock</surname>, <given-names>K. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bailey L. L."><surname>Bailey</surname>, <given-names>L. L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Hines J. E."><surname>Hines</surname>, <given-names>J. E.</given-names></string-name> (<year>2017</year>). <source hwp:id="source-20">Occupancy Estimation and Modeling: Inferring Patterns and Dynamics of Species Occurrence</source> (<edition>2nd</edition> ed.). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Meek P. D."><surname>Meek</surname>, <given-names>P. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vernes K."><surname>Vernes</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Falzon G."><surname>Falzon</surname>, <given-names>G.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-12">On the reliability of expert identification of small-medium sized mammals from camera trap photos</article-title>. <source hwp:id="source-21">Wildlife Biology in Practice</source>, <volume>9</volume>(<issue>2</issue>). doi:<pub-id pub-id-type="doi">10.2461/wbp.2013.9.4</pub-id></citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Mohri M."><surname>Mohri</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rostamizadeh A."><surname>Rostamizadeh</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Talwalkar A."><surname>Talwalkar</surname>, <given-names>A.</given-names></string-name> (<year>2012</year>). <source hwp:id="source-22">Foundations of Machine Learning</source>. <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Niedballa J."><surname>Niedballa</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sollmann R."><surname>Sollmann</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Courtiol A."><surname>Courtiol</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wilting A."><surname>Wilting</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-13">camtrapR: an R package for efficient camera trap data management</article-title>. <source hwp:id="source-23">Methods in Ecology and Evolution</source>, <volume>7</volume>(<issue>12</issue>), <fpage>1457</fpage>–<lpage>1462</lpage>. doi:<pub-id pub-id-type="doi">10.1111/2041-210X.12600</pub-id></citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2 xref-ref-24-3 xref-ref-24-4 xref-ref-24-5"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Norouzzadeh M. S."><surname>Norouzzadeh</surname>, <given-names>M. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nguyen A."><surname>Nguyen</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kosmala M."><surname>Kosmala</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Swanson A."><surname>Swanson</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palmer M. S."><surname>Palmer</surname>, <given-names>M. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Packer C."><surname>Packer</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Clune J."><surname>Clune</surname>, <given-names>J.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-14">Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning</article-title>. <source hwp:id="source-24">Proceedings of the National Academy of Sciences</source>, <fpage>201719367</fpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1719367115</pub-id></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2 xref-ref-25-3"><citation publication-type="book" citation-type="book" ref:id="346809v3.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="O’Connell A. F."><surname>O’Connell</surname>, <given-names>A. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichols J. D."><surname>Nichols</surname>, <given-names>J. D.</given-names></string-name>, &amp; <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Karanth K. U."><surname>Karanth</surname>, <given-names>K. U.</given-names></string-name></person-group> (Eds.). (<year>2011</year>). <source hwp:id="source-25">Camera traps in animal ecology: methods and analyses</source>. <publisher-loc>Tokyo; New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Palmer M. S."><surname>Palmer</surname>, <given-names>M. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fieberg J."><surname>Fieberg</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Swanson A."><surname>Swanson</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kosmala M."><surname>Kosmala</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Packer C."><surname>Packer</surname>, <given-names>C.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-15">A ‘dynamic’ landscape of fear: prey responses to spatiotemporal variations in predation risk across the lunar cycle</article-title>. <source hwp:id="source-26">Ecology Letters</source>, <volume>20</volume>(<issue>11</issue>), <fpage>1364</fpage>–<lpage>1373</lpage>. doi:<pub-id pub-id-type="doi">10.1111/ele.12832</pub-id></citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Rasambainarivo F."><surname>Rasambainarivo</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Farris Z. J."><surname>Farris</surname>, <given-names>Z. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Andrianalizah H."><surname>Andrianalizah</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Parker P. G."><surname>Parker</surname>, <given-names>P. G.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-16">Interactions between carnivores in Madagascar and the risk of disease transmission</article-title>. <source hwp:id="source-27">EcoHealth</source>, <volume>14</volume>(<issue>4</issue>), <fpage>691</fpage>–<lpage>703</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10393–017-1280–7</pub-id></citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Rovero F."><surname>Rovero</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zimmermann F."><surname>Zimmermann</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bersi D."><surname>Bersi</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Meek P."><surname>Meek</surname>, <given-names>P.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-17">‘Which camera trap type and how many do I need?’ A review of camera features and study designs for a range of wildlife research applications</article-title>. <source hwp:id="source-28">Hystrix, the Italian Journal of Mammalogy</source>, <volume>24</volume>(<issue>2</issue>), <fpage>1</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="book" citation-type="book" ref:id="346809v3.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Royle J. A."><surname>Royle</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dorazio R. M."><surname>Dorazio</surname>, <given-names>R. M.</given-names></string-name> (<year>2008</year>). <source hwp:id="source-29">Hierarchical modeling and inference in ecology</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Royle J. A."><surname>Royle</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Link W. A."><surname>Link</surname>, <given-names>W. A.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-18">Generalized site occupancy models allowing for false positive and false negative errors</article-title>. <source hwp:id="source-30">Ecology</source>, <volume>57</volume>(<issue>4</issue>), <fpage>835</fpage>–<lpage>841</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Rumelhart D. E."><surname>Rumelhart</surname>, <given-names>D. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hinton G. E."><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Williams R. J."><surname>Williams</surname>, <given-names>R. J.</given-names></string-name> (<year>1986</year>). <article-title hwp:id="article-title-19">Learning representations by back-propagating errors</article-title>. <source hwp:id="source-31">Nature</source>, <volume>323</volume>(<issue>6088</issue>), <fpage>533</fpage>–<lpage>536</lpage>. doi:<pub-id pub-id-type="doi">10.1038/323533a0</pub-id></citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Scott A. B."><surname>Scott</surname>, <given-names>A. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Phalen D."><surname>Phalen</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hernandez-Jover M."><surname>Hernandez-Jover</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Singh M."><surname>Singh</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Groves P."><surname>Groves</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Toribio J.-A. L. M. L."><surname>Toribio</surname>, <given-names>J.-A. L. M. L.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-20">Wildlife presence and interactions with chickens on Australian commercial chicken farms assessed by camera traps</article-title>. <source hwp:id="source-32">Avian Diseases</source>, <volume>62</volume>(<issue>1</issue>), <fpage>65</fpage>–<lpage>72</lpage>. doi:<pub-id pub-id-type="doi">10.1637/11761–101917-Reg.1</pub-id></citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="website" citation-type="web" ref:id="346809v3.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Simonyan K."><surname>Simonyan</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Zisserman A."><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-21">Very deep convolutional networks for large-scale image recognition</article-title>. <source hwp:id="source-33">ArXiv:1409.1556 [Cs]</source>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://arxiv.org/abs/1409.1556" ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556" hwp:id="ext-link-6">http://arxiv.org/abs/1409.1556</ext-link></citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Steenweg R."><surname>Steenweg</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hebblewhite M."><surname>Hebblewhite</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kays R."><surname>Kays</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ahumada J."><surname>Ahumada</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fisher J. T."><surname>Fisher</surname>, <given-names>J. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burton C."><surname>Burton</surname>, <given-names>C.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Rich L. N."><surname>Rich</surname>, <given-names>L. N.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-22">Scaling-up camera traps: monitoring the planet’s biodiversity with networks of remote sensors</article-title>. <source hwp:id="source-34">Frontiers in Ecology and the Environment</source>, <volume>15</volume>(<issue>1</issue>), <fpage>26</fpage>–<lpage>34</lpage>. doi:<pub-id pub-id-type="doi">10.1002/fee.1448</pub-id></citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Swanson A."><surname>Swanson</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kosmala M."><surname>Kosmala</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lintott C."><surname>Lintott</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Packer C."><surname>Packer</surname>, <given-names>C.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-23">A generalized approach for producing, quantifying, and validating citizen science data from wildlife images</article-title>. <source hwp:id="source-35">Conservation Biology</source>, <volume>30</volume>(<issue>3</issue>), <fpage>520</fpage>–<lpage>531</lpage>. doi:<pub-id pub-id-type="doi">10.1111/cobi.12695</pub-id></citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2 xref-ref-36-3"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Swanson A."><surname>Swanson</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kosmala M."><surname>Kosmala</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lintott C."><surname>Lintott</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simpson R."><surname>Simpson</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith A."><surname>Smith</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Packer C."><surname>Packer</surname>, <given-names>C.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-24">Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna</article-title>. <source hwp:id="source-36">Scientific Data</source>, <volume>2</volume>, <fpage>150026</fpage>. doi:<pub-id pub-id-type="doi">10.1038/sdata.2015.26</pub-id></citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Swinnen K. R. R."><surname>Swinnen</surname>, <given-names>K. R. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reijniers J."><surname>Reijniers</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Breno M."><surname>Breno</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Leirs H."><surname>Leirs</surname>, <given-names>H.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-25">A novel method to reduce time investment when processing videos from camera trap studies</article-title>. <source hwp:id="source-37">PLoS ONE</source>, <volume>9</volume>(<issue>6</issue>), <fpage>e98881</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0098881</pub-id></citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Tabak M. A."><surname>Tabak</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Piaggio A. J."><surname>Piaggio</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Miller R. S."><surname>Miller</surname>, <given-names>R. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sweitzer R. A."><surname>Sweitzer</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ernest H. B."><surname>Ernest</surname>, <given-names>H. B.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-26">Anthropogenic factors predict movement of an invasive species</article-title>. <source hwp:id="source-38">Ecosphere</source>, <volume>8</volume>(<issue>6</issue>), <fpage>e01844</fpage>. doi:<pub-id pub-id-type="doi">10.1002/ecs2.1844</pub-id></citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Tikhonov G."><surname>Tikhonov</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abrego N."><surname>Abrego</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dunson D."><surname>Dunson</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ovaskainen O."><surname>Ovaskainen</surname>, <given-names>O.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-27">Using joint species distribution models for evaluating how species-to-species associations depend on the environmental context</article-title>. <source hwp:id="source-39">Methods in Ecology and Evolution</source>, <volume>8</volume>(<issue>4</issue>), <fpage>443</fpage>–<lpage>452</lpage>. doi:<pub-id pub-id-type="doi">10.1111/2041–210X.12723</pub-id></citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Tobler M. W."><surname>Tobler</surname>, <given-names>M. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carrillo-Percastegui S. E."><surname>Carrillo-Percastegui</surname>, <given-names>S. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leite Pitman R."><surname>Leite Pitman</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mares R."><surname>Mares</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Powell G."><surname>Powell</surname>, <given-names>G.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-28">An evaluation of camera traps for inventorying large- and medium-sized terrestrial rainforest mammals</article-title>. <source hwp:id="source-40">Animal Conservation</source>, <volume>11</volume>(<issue>3</issue>), <fpage>169</fpage>–<lpage>178</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1469–1795.2008.00169.x</pub-id></citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Tobler M. W."><surname>Tobler</surname>, <given-names>M. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zúñiga Hartley A."><surname>Zúñiga Hartley</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carrillo-Percastegui S. E."><surname>Carrillo-Percastegui</surname>, <given-names>S. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Powell G. V. N."><surname>Powell</surname>, <given-names>G. V. N.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-29">Spatiotemporal hierarchical modelling of species richness and occupancy using camera trap data</article-title>. <source hwp:id="source-41">Journal of Applied Ecology</source>, <volume>52</volume>(<issue>2</issue>), <fpage>413</fpage>–<lpage>421</lpage>. doi:<pub-id pub-id-type="doi">10.1111/1365–2664.12399</pub-id></citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Underwood A."><surname>Underwood</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chapman M."><surname>Chapman</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Connell S."><surname>Connell</surname>, <given-names>S.</given-names></string-name> (<year>2000</year>). <article-title hwp:id="article-title-30">Observations in ecology: you can’t make progress on processes without understanding the patterns</article-title>. <source hwp:id="source-42">Journal of Experimental Marine Biology and Ecology</source>, <volume>250</volume>(<issue>1–2</issue>), <fpage>97</fpage>–<lpage>115</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0022–0981(00)00181–7</pub-id></citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="346809v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Yu X."><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang J."><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kays R."><surname>Kays</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jansen P. A."><surname>Jansen</surname>, <given-names>P. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang T."><surname>Wang</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Huang T."><surname>Huang</surname>, <given-names>T.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-31">Automated identification of animal species in camera trap images</article-title>. <source hwp:id="source-43">EURASIP Journal on Image and Video Processing</source>, <volume>2013</volume>(<issue>1</issue>). doi:<pub-id pub-id-type="doi">10.1186/1687–5281-2013–52</pub-id></citation></ref></ref-list><sec id="s7" hwp:id="sec-14"><title hwp:id="title-18">Supporting Information</title><p hwp:id="p-40"><bold>Appendix S1.</bold> Site descriptions for each of the study locations</p><p hwp:id="p-41"><bold>Appendix S2.</bold> Accuracy of the Group Level for each species</p><p hwp:id="p-42"><bold>Appendix S3.</bold> Accuracy of the Species Level model at the Tejon research site in California.</p><p hwp:id="p-43"><bold>Appendix S4.</bold> Accuracy of the Species Level model in Colorado</p><p hwp:id="p-44"><bold>Appendix S5.</bold> Accuracy of the Species Level model at Buck Island Ranch in Florida</p><p hwp:id="p-45"><bold>Appendix S6.</bold> Accuracy of the Species Level model at the Camp Bullis Military Training Center in Texas</p><p hwp:id="p-46"><bold>Appendix S7.</bold> Accuracy of the Species Level model at the Savannah River Ecology Laboratory in South Carolina</p><p hwp:id="p-47"><bold>Appendix S8.</bold> Image classified as a striped skunk by humans, but cattle by the Species Level model</p></sec></back></article>
