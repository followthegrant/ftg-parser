<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/299933</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;299933</article-id><article-id pub-id-type="other" hwp:sub-type="slug">299933</article-id><article-id pub-id-type="other" hwp:sub-type="tag">299933</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Multimodal evidence on shape and surface information in individual face processing</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>To whom correspondence should be addressed. E-Mail: <email hwp:id="email-1">dan.nemrodov@utoronto.ca</email> Tel : +1 (647) 772-0047</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0186-3847</contrib-id><name name-style="western" hwp:sortable="Nemrodov Dan"><surname>Nemrodov</surname><given-names>Dan</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-0186-3847"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3814-1015</contrib-id><name name-style="western" hwp:sortable="Behrmann Marlene"><surname>Behrmann</surname><given-names>Marlene</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3814-1015"/></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Niemeier Matthias"><surname>Niemeier</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Drobotenko Natalia"><surname>Drobotenko</surname><given-names>Natalia</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Nestor Adrian"><surname>Nestor</surname><given-names>Adrian</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4"><label>1</label><institution hwp:id="institution-1">Department of Psychology at Scarborough, University of Toronto</institution>, 1265 Military Trail, Toronto, Ontario, M1C1A4, <country>Canada</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Department of Psychology at Carnegie Mellon University</institution>, 5000 Forbes Ave, Pittsburgh, PA 15213, <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Center for the Neural Basis of Cognition, University of Pittsburgh</institution>, PA 4400 Fifth Ave, Pittsburgh, PA 15213, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-04-17T13:39:08-07:00">
    <day>17</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-04-17T13:39:08-07:00">
    <day>17</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-04-17T13:44:51-07:00">
    <day>17</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-04-17T13:44:51-07:00">
    <day>17</day><month>4</month><year>2018</year>
  </pub-date><elocation-id>299933</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-04-17"><day>17</day><month>4</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-04-17"><day>17</day><month>4</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-04-17"><day>17</day><month>4</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="299933.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/299933v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="299933.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/299933v1/299933v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/299933v1/299933v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">The significance of shape and surface information for face perception is well established, yet their relative contribution to recognition and their neural underpinnings await clarification. Here, we employ image reconstruction to retrieve, assess and visualize such information using behavioral, electroencephalography and functional magnetic resonance imaging data.</p><p hwp:id="p-3">Our results indicate that both shape and surface information can be successfully recovered from each modality but that the latter is better recovered than the former, consistent with its key role for face representations. Further, shape and surface information exhibit similar spatiotemporal profiles, rely on the extraction of specific visual features, such as eye shape or skin tone, and reveal a systematic representational structure, albeit with more cross-modal consistency for shape than surface.</p><p hwp:id="p-4">Thus, the present results help elucidate the representational basis of individual face recognition while, methodologically, they showcase the utility of image reconstruction and clarify its reliance on diagnostic visual information.</p></abstract><counts><page-count count="50"/></counts></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-2">Introduction</title><p hwp:id="p-5">The segregation of shape and surface information defines a fundamental aspect of visual processing and cortical organization (<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Livingstone &amp; Hubel, 1988</xref>; <xref ref-type="bibr" rid="c88" hwp:id="xref-ref-88-1" hwp:rel-id="ref-88">Van Essen &amp; Deyoe, 1995</xref>) both in the human (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Cant, Large, McCall, &amp; Goodale, 2008</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Lafer-Sousa, Conway, &amp; Kanwisher, 2016</xref>; <xref ref-type="bibr" rid="c90" hwp:id="xref-ref-90-1" hwp:rel-id="ref-90">Vinberg &amp; Grill-Spector, 2008</xref>) and the monkey brain (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Conway, Moeller, &amp; Tsao, 2007</xref>). Accordingly, this distinction has played a prominent role in accounts of face recognition (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Bruce &amp; Young, 1998</xref>). Extensive research has documented the importance of both types of information in face perception (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Biederman &amp; Kalocsai, 1997</xref>, <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Jiang, Blanz, &amp; O’Toole, 2006</xref>; <xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">O’Toole, Vetter, &amp; Blanz, 1999</xref>; <xref ref-type="bibr" rid="c78" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">Russell et al., 2007</xref>; <xref ref-type="bibr" rid="c80" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">Russell &amp; Sinha, 2007</xref>; <xref ref-type="bibr" rid="c91" hwp:id="xref-ref-91-1" hwp:rel-id="ref-91">Vuong, Peissig, Harrison, &amp; Tarr, 2005</xref>), but the relative weight of shape and surface properties has been heavily debated, with either the former (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Jiang, Blanz, &amp; Rossion, 2011</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Lai, Oruc &amp; Barton, 2013</xref>) or the latter (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Bruce et al., 1991</xref>; <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Bruce &amp; Langton, 1994</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Hole, George, Eaves, &amp; Rasek, 2002</xref>; <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Kaufmann &amp; Schweinberger, 2008</xref>; <xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">Russell, Sinha, Biederman &amp; Nederhouser, 2006</xref>) considered dominant. Arguably, this debate arises from a lack of specificity in identifying the shape and surface features critical for individual face processing (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Burton et al., 2015</xref>). Thus, the current research aims to uncover the nature of the information involved in individual face processing along with its accompanying neural profile.</p><p hwp:id="p-6">To address the challenge above, here, we appeal to neural-based image reconstruction (<xref ref-type="bibr" rid="c81" hwp:id="xref-ref-81-1" hwp:rel-id="ref-81">Shen, Dwivedi, Majima, Horikawa &amp; Kamitani, 2018</xref>; <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Miyawaki et al., 2008</xref>; <xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">Naselaris et al., 2009</xref>, <xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">Nishimoto et al., 2011</xref>; <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">Thirion et al., 2006</xref>), namely, the endeavor of reconstructing the appearance of visual objects from neural activity prompted by their processing. While this endeavor has relied primarily on functional magnetic resonance imaging (fMRI), more recently, additional modalities have been used successfully as well. For instance, facial image reconstruction has been carried out using single-cell recordings (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Chang &amp; Tsao, 2017</xref>), electroencephalography (EEG) data (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Nemrodov et al., 2018</xref>) and behavioral data (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Chang et al., 2017</xref>; <xref ref-type="bibr" rid="c92" hwp:id="xref-ref-92-1" hwp:rel-id="ref-92">Zhan et al., 2017</xref>), in addition to fMRI (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Cowen, Chun, &amp; Kuhl, 2014</xref>; <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Lee &amp; Kuhl, 2016</xref>; <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Nestor, Plaut, &amp; Behrmann, 2016</xref>). Thus, in theory, image reconstruction can provide a powerful platform for investigating shape/surface processing in face individuation via multiple behavioral and neuroimaging modalities. Concretely, image reconstruction can be used to uncover, assess and compare facial shape and surface information recovered from distinct modalities.</p><p hwp:id="p-7">To this end, we rely on data assessing individual face processing gleaned from behavioral (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Nestor, Plaut &amp; Behrmann, 2013</xref>), EEG (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-2" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>) and fMRI data (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-2" hwp:rel-id="ref-67">Nestor, Plaut &amp; Behrmann, 2016</xref>). Specifically, for each modality, we aim to recover the shape and surface content of a common set of face stimuli as perceived by human observers (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>). In addition, the same procedure is conducted with an image-based theoretical observer (TO) allowing us to compare the informational content of multiple empirical and TO reconstructions.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-8">Schematic illustration of the reconstruction procedure. Behavioral, fMRI, EEG and TO data associated with viewing face stimuli support, separately, the estimation of a multidimensional face space (for convenience, a single example based on EEG data, indicated by the dark blue arrow, is shown, but similar results can be achieved from other modalities). Shape information and surface information are derived from the structure of this space and combined into facial image reconstructions (only a representative subset of fiducial points are displayed; L*, a* and b* correspond to the lightness, red-green and yellow-blue channels of color vision as encoded in CIEL*a*b*). Due to copyright restrictions all identifiable stimulus images were replaced with computer-generated images not used during experimental testing.</p></caption><graphic xlink:href="299933_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-9">Accordingly, we appeal here to an influential approach for analyzing face images into shape and surface properties (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Craw &amp; Cameron, 1991</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Kramer, Jenkins &amp; Burton, 2016</xref>; <xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">Tiddemann, Burt &amp; Perrett, 2001</xref>; <xref ref-type="bibr" rid="c89" hwp:id="xref-ref-89-1" hwp:rel-id="ref-89">Vetter &amp; Troje, 1995</xref>). Specifically, this approach involves marking the positions of a set of fiducial points (e.g., the corners of the eyes or the tip of the nose) that deliver shape information. Then, faces are warped to a standard shape (i.e., a preset configuration of fiducial points) yielding ‘shape-free’ images that deliver surface information. To be clear, shape derived in this manner encompasses two sources of information: configural information, conceived as metric distances between different face parts (<xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Maurer et al., 2002</xref>; <xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">Tanaka &amp; Gordon, 2011</xref>), and local information associated with the geometric structure of specific face parts such as eye shape or mouth shape (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Cabeza &amp; Kato, 2000</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Gold, Mundy &amp; Tjan, 2012</xref>; <xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">Rakover, 2002</xref>). In contrast, surface contains information about the reflectance properties of a face (e.g., hue, specularity, albedo) that also play a role in individual face recognition (<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Hancock, Burton &amp; Bruce, 1996</xref>; <xref ref-type="bibr" rid="c80" hwp:id="xref-ref-80-2" hwp:rel-id="ref-80">Russell et al., 2007</xref>; <xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">Taschereau-Dumouchel et al., 2010</xref>) – such information is alternatively referred to as ‘texture’, ‘pigmentation’ or ‘surface reflectance’.</p><p hwp:id="p-10">The appeal to shape-surface decomposition allows us to address a number of related questions. First, can image reconstruction separately recover facial shape and surface information from different modalities and, if so, how well? Second, what is the spatiotemporal profile of shape and surface processing? Third, what specific shape/surface features are recovered through reconstruction? And forth, do different modalities reveal similar or complementary information about face representations? More generally, the present work evaluates and confirms the ability of a novel methodological paradigm to exploit multimodal evidence in the effort to elucidate the representational content of individual face processing.</p><p hwp:id="p-11">In summary, the current work embarks on a comprehensive investigation of facial shape and surface processing by appeal to powerful and novel image-reconstruction methodology as applied to multimodal data. Accordingly, this work serves a twofold purpose by shedding light on the psychological and neural profile of facial shape/surface processing and by clarifying the informational content responsible for the success of image reconstruction.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-3">Results</title><p hwp:id="p-12">Our investigation relies on the representational structure underlying facial processing as revealed by multiple modalities for a common set of stimuli. This structure allows us to relate the outcome of different modalities to each other as well as to derive shape and surface estimates of face representations. Such estimates can be assessed in terms of their reconstruction success and of their spatiotemporal neural profile. Further, such estimates can be recombined into image reconstructions approximating the visual appearance of the percepts associated with viewing specific face stimuli.</p><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-4">Representational similarity</title><p hwp:id="p-13">Estimates of pairwise face similarity were computed across 108 images (54 identities x 2 expressions, neutral and happy) for each of four data types: (i) behavioral, based on similarity ratings; (ii) EEG, based on neural discriminability across occipitotemporal (OT) electrodes; (iii) fMRI, based on neural discriminability across multiple fusiform gyrus (FG) areas, and (iv) TO, based on pixelwise image similarity. In particular, we note that fMRI estimates relied jointly on patterns of activation from four distinct FG areas (see Materials and Methods, Representational similarity analysis) - these areas were selected due to their ability to support face decoding in previous work (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-3" hwp:rel-id="ref-67">Nestor et al, 2016</xref>). For clarity, stimulus-specific multivoxel patterns were concatenated here across the four areas and subjected to pattern classification.</p><p hwp:id="p-14">Next, Spearman rank correlations were computed for each pair of modalities across corresponding similarity estimates (i.e., 1431 pairwise estimates for 54 identities, averaged across expressions). Overall, we found that all data types correlated with each other (p&lt;0.01, Bonferroni-corrected) providing initial evidence for common representational structure across modalities (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2a</xref>).</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-15">Correlations between different data types were based on: (a) pairwise face similarity/discriminability estimates (Spearman correlation across 1431 facial identity pairs); (b) shape and (c) surface reconstruction accuracy (Pearson correlation across 54 facial identities). All modalities are correlated with each other in terms of face similarity but only some in terms of shape and surface information (** p&lt;0.01, *** p&lt;0.001; Bonferroni correction across comparisons).</p></caption><graphic xlink:href="299933_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-5">Shape and surface reconstruction</title><p hwp:id="p-16">Accuracy estimates of shape and surface reconstruction were separately computed for each data type (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>). Critically, we found that all estimates were above chance for both facial expressions (permutation test, p&lt;0.05, Bonferroni correction for 24 comparisons). Overall, for empirical modalities, surface reconstructions were more accurate than shape reconstructions (paired-comparison permutation test, p&lt;0.01) with the exception of fMRI results for happy faces where the difference was only marginally significant (p=0.085). In contrast, TO results showed no difference in accuracy between shape and surface (p&gt;0.151), suggesting that the reconstruction method can, in theory, retrieve the two types of information with equal success from the current stimulus sets. Further, no difference was found between neutral and happy faces for either shape or surface for any data type (p&gt;0.104 for all other than fMRI-based shapes, in which case happy faces yielded marginally more accurate reconstructions than neutral ones, p=0.079).</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3 with 1 supplement.</label><caption hwp:id="caption-3"><p hwp:id="p-17">Shape and surface reconstruction accuracies for four modalities. All accuracy estimates are above chance (all <italic toggle="yes">p</italic>’s&lt;0.01, Bonferroni-corrected; two-tailed permutation tests; confidence intervals based on 10<sup>4</sup> shuffles of identity labels in the corresponding face space). Surface information was retrieved more accurately than shape information for empirical modalities (two-tailed paired-comparison permutation tests; * <italic toggle="yes">p</italic>&lt;0.05; ** <italic toggle="yes">p</italic>&lt;0.01) but not for TO.</p><p hwp:id="p-18">Figure 3 – Figure supplement 1. Accuracy estimates of surface reconstruction for each color channel. Results are shown for each modality collapsed across emotional expression. All estimates are above chance (all <italic toggle="yes">p</italic>’s&lt;0.01, Bonferroni-corrected; two-tailed permutation tests; confidence intervals based on 10<sup>4</sup> shuffles of identity labels in the corresponding face space). No difference was noticed across color channels for any modality (all <italic toggle="yes">p</italic>’s&gt;0.05; two-tailed paired-comparison permutation tests).</p></caption><graphic xlink:href="299933_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-19">Given the systematic advantage of surface over shape reconstructions, we proceeded to assess reconstruction accuracy separately for each color channel. This assessment is particularly relevant since informative shape-from-shading cues may be present as lightness patterns in what we refer to as ‘facial surfaces’ (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Attick, Griffin &amp; Redlich, 1996</xref>). Hence, the surface advantage noticed above could be due to another source of shape information rather than to genuinely ‘shape-free’ surface information. To evaluate this possibility, we estimated reconstruction accuracy for each color channel: we averaged such estimates across expression, given the absence of an expression effect above, and we compared them with each other separately for each modality. This analysis revealed that color components and lightness support equivalent levels of reconstruction accuracy for every modality (Figure 1 – figure supplement 1). More precisely, no difference was noticed between any two components (two-tailed paired-comparison permutation tests; p&gt;0.05, uncorrected) ruling out the shape-from-shading hypothesis above.</p><p hwp:id="p-20">Next, regarding the relative performance of different modalities, we note that fMRI seemed to perform more poorly than other data types. However, a direct comparison of different modalities in terms of accuracy may be misleading in that the corresponding experiments followed different protocols suitable for the corresponding modalities (e.g., different experimental tasks, different numbers of trials, different numbers of participants). At the same time, we note though that representational similarity analysis confirmed the presence of corresponding structure across data types. To further explore this correspondence in terms of shape and surface information, reconstruction accuracy, averaged across expressions, was correlated for each pair of data types (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2b, c</xref>). This analysis found, in the case of shape, that empirical modalities all correlated with each other with the exception of fMRI and TO. However, in the case of surface properties, behavioral reconstructions were correlated with EEG and TO, but not with fMRI, pointing to potentially different surface information available in fMRI data.</p><p hwp:id="p-21">To clarify the results above in the context of the relationship between brain and behavior, multiple linear regression was employed to account for behavioral accuracy based on all other data types. Separate analyses for shape and surface information both yielded significant models (shape: R<sup>2</sup><sub>adj</sub>=0.55, <italic toggle="yes">p</italic>&lt;0.001; surface: R<sup>2</sup><sub>adj</sub>=0.47, <italic toggle="yes">p</italic>&lt;0.001). In more detail, for shape information, EEG (<italic toggle="yes">β</italic>=0.41, p&lt;0.001), fMRI (<italic toggle="yes">β</italic>=0.14, <italic toggle="yes">p</italic>=0.01) and TO (<italic toggle="yes">β</italic>=0.39, <italic toggle="yes">p</italic>&lt;0.001) all provided significant independent contributions in accounting for behavior. In contrast, for surface information, only TO (<italic toggle="yes">β</italic>=0.64, <italic toggle="yes">p</italic>&lt;0.001) and, marginally, EEG (<italic toggle="yes">β</italic>=0.14, <italic toggle="yes">p</italic>=0.075), were significant predictors of behavioral accuracy. Thus, different modalities appear to contain only partly overlapping information and to make distinct contributions in accounting for behavioral performance.</p><p hwp:id="p-22">Next, to pinpoint the source of recovered information, accuracy was locally computed for each fiducial point, in the case of shape, and for each pixel and color channel, in the case of surface properties. Specifically, the coordinates of each fiducial point within a reconstructed image were compared relative to the corresponding point in the stimulus images and point-specific accuracy was estimated as the percentage of instances for which the Euclidean distance to the corresponding point in the target stimulus was smaller than to that in any other stimulus. Accuracy heatmaps averaged across all reconstructed shapes are displayed in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>. This analysis revealed that the shape of the eyes was better recovered than other information for all empirical modalities. The same appeared to be the case for TO reconstructions; however, additional information regarding the shape of the mouth and the eyes was also recovered relatively well here.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-23">Accuracy heatmaps for shape reconstruction across fiducial points overlaid on average neutral and happy faces. The size of the circles is proportional with position variance across the face (i.e., larger circles indicate more variability in fiducial point position across different individual faces) while color indicates average reconstruction accuracy across 54 facial identities. Shape information is best approximated across the eyes though differences in both global and local accuracy can be noticed across modalities.</p></caption><graphic xlink:href="299933_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-24">A similar procedure was followed for deriving surface heatmaps except that pixel intensity values (e.g., lightness as coded in the L* channel), rather than geometrical coordinates, were considered in this case (see <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref> and <xref ref-type="fig" rid="figS5" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Figure 5 – supplement figure 1</xref> for neutral and happy faces, respectively). Overall, information from multiple areas of the face and multiple color channels appeared to contribute to reconstruction success. For instance, the lightness of the cheeks along with the color of the forehead, especially as encoded in the red-green channel, appeared to be correctly recovered. As expected, and in agreement with the correlation results above, fMRI heatmaps evinced lower levels of accuracy while TO heatmaps evinced the highest overall accuracy.</p><fig id="figS3" position="float" orientation="portrait" fig-type="figure" hwp:id="F11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Figure 3 – Figure supplement 1.</label><caption hwp:id="caption-11"><p hwp:id="p-86">Accuracy estimates of surface reconstruction for each color channel. Results are shown for each modality collapsed across emotional expression. All estimates are above chance (all p’s&lt;0.01, Bonferroni-corrected; two-tailed permutation tests; confidence intervals based on 10<sup>4</sup> shuffles of identity labels in the corresponding face space). No difference was noticed across color channels for any modality (all p’s&gt;0.05; two-tailed paired-comparison permutation tests).</p></caption><graphic xlink:href="299933_figS3" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><fig id="figS5" position="float" orientation="portrait" fig-type="figure" hwp:id="F12" hwp:rev-id="xref-fig-12-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figS5</object-id><label>Figure 5 – Figure supplement 1.</label><caption hwp:id="caption-12"><p hwp:id="p-87">Accuracy heatmaps of surface reconstruction for happy faces across pixels and color channels. Color indicates average reconstruction accuracy across 54 facial identities. Multiple areas of the face and multiple color channels provide accurate information for reconstruction purposes; differences in both global and local accuracy can be noticed across modalities.</p></caption><graphic xlink:href="299933_figS5" position="float" orientation="portrait" hwp:id="graphic-12"/></fig><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5 with 1 supplement.</label><caption hwp:id="caption-5"><p hwp:id="p-25">Accuracy heatmaps of surface reconstruction for neutral faces across pixels and color channels. Color indicates average reconstruction accuracy across 54 facial identities. Multiple areas of the face and multiple color channels provide accurate information for reconstruction purposes; differences in both global and local accuracy can be noticed across modalities.</p><p hwp:id="p-26">Figure 5 – Figure supplement 1. Accuracy heatmaps of surface reconstruction for happy faces across pixels and color channels.</p></caption><graphic xlink:href="299933_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-6">Facial image reconstruction</title><p hwp:id="p-27">Shape and surface information, as retrieved separately from each data type, was combined into recomposed image reconstructions – for examples see <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref>. Reconstructions appeared to capture, for any given modality, visual properties indicative of facial identity.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-6"><p hwp:id="p-28">Examples of face image reconstructions (numbers in the upper left indicate experimental-based estimates of reconstruction accuracy; other image-based accuracy estimates are displayed for recomposed faces in the top right, for shape in the bottom left and for surface in the bottom right corners).*</p><p hwp:id="p-29">*Images of face stimuli could not be reproduced due to copyright restrictions</p></caption><graphic xlink:href="299933_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-30">To evaluate this claim, empirical data were collected from a novel group of naïve observers who matched reconstructions against corresponding stimuli. Performance (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7</xref>) was well above chance for all modalities and expressions (<italic toggle="yes">p</italic>’s&lt;0.001; one-sample two-tailed t-test against 50% chance performance in a 2AFC task; Bonferroni correction across comparisons). A two-way repeated measures analysis (4 modalities x 2 expressions) found, as expected, a main effect of modality (F(3,75)=18.30, <italic toggle="yes">p</italic>&lt;0.001, <italic toggle="yes">η<sup>2</sup></italic>=0.196) but no effect of expression (<italic toggle="yes">p</italic>=0.140) and no interaction (<italic toggle="yes">p</italic>=0.333). While a direct comparison of empirical modalities in terms of accuracy may be misleading, as discussed above, it is of interest to assess how closely empirical modalities can approach the level of TO performance. Planned comparisons between TO and each empirical modality, collapsed across expressions, showed that TO surpassed fMRI (<italic toggle="yes">p</italic>&lt;0.001), but not EEG (<italic toggle="yes">p</italic>&gt;0.556) or behavioral data, which provided marginally better results (<italic toggle="yes">p</italic>=0.067).</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-7"><p hwp:id="p-31">Reconstruction accuracy based on (a) experimental estimates and (b) image-based estimates. The outcome of planned comparisons is shown (a) between TO and other modalities as well as (b) between recomposed faces and other types of reconstruction (*** <italic toggle="yes">p</italic>&lt;0.001). Error bars indicate ±1SE (a) across participants and (b) across items.</p></caption><graphic xlink:href="299933_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-32">Next, we assessed whether the combination of shape and surface provides any advantage over reconstructed shapes and surface in isolation, as well as over a previous version of reconstruction that does not appeal to shape-surface decomposition (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-4" hwp:rel-id="ref-67">Nestor et al, 2016</xref>), for short here ‘intact reconstructions’. To this end, we considered objective estimates of reconstruction accuracy (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7b</xref>): for recomposed faces this was computed across pixel intensities in the same manner in which surface accuracy was derived. Since permutation tests were not feasible for reconstructed faces, as they rely on the manual combination of shape and surface information (see Materials and Methods), parametric tests were conducted here across items (e.g., 54 facial identities). We note that while parametric tests are less conservative than the permutation tests above, their goal here is not to assess performance against chance but, rather, to explore differences across different types of reconstruction. A three-way analysis of variance (4 data types X 4 reconstruction types X 2 expressions) found, as expected, a main effect of data type (F(3,159)=44.00, <italic toggle="yes">p</italic>&lt;0.001, <italic toggle="yes">η<sup>2</sup></italic>=0.160), a main effect of reconstruction type (F(3,159)=25.82, <italic toggle="yes">p</italic>&lt;0.001, <italic toggle="yes">η<sup>2</sup></italic>=0.054) along with an interaction between data type and reconstruction type (F(9,477)=3.36, <italic toggle="yes">p</italic>=0.016, <italic toggle="yes">η<sup>2</sup></italic>=0.008). No significant effect or interactions were found for expression (<italic toggle="yes">p</italic>&gt;0.05). Further pairwise comparisons found that recomposed faces were reconstructed more accurately than shape in all instances (<italic toggle="yes">p</italic>’s&lt;0.001) (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7b</xref>) but only surpassed surface in the case of TO (<italic toggle="yes">p</italic>&lt;0.001). Last, recomposed reconstructions were systematically more accurate than intact reconstructions (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-5" hwp:rel-id="ref-67">Nestor et al, 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-3" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>); however, the difference did not reach significance for any modality (<italic toggle="yes">p</italic>’s&gt;0.139).</p><p hwp:id="p-33">Thus, the benefit of combining shape and surface reconstruction for reconstruction purposes is clearly apparent only for TO. This result is consistent with the less efficient retrieval of shape information from behavioral and neural data noted above and suggests comparatively higher reliance on surface information in visual face processing.</p></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-7">The spatiotemporal profile of shape and surface processing</title><p hwp:id="p-34">To investigate in further detail the temporal dynamics of shape and surface processing, reconstruction accuracy was computed for both types of information across occipitotemporal (OT) electrodes using a ∼10ms sliding window - the time course of reconstruction averaged across facial identities is displayed in <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref> and an example of recomposed reconstruction over time is shown in Movie 1. Overall, we found that surface information was more accurately retrieved than shape information, yet both evinced multiple intervals of above-chance reconstruction (two-tailed permutation test; FDR correction over time). Specifically, they both reached significance around 150ms after stimulus onset and gradually declined after 300ms.</p><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-8"><p hwp:id="p-35">The time course of EEG-based reconstruction accuracy for shape and surface information estimated with a sliding ∼10ms temporal window. Accuracy for both types of information was above chance across multiple intervals as indicated by corresponding segments at the top of the plot (permutation test; FDR-correction across time, <italic toggle="yes">q</italic>&lt;0.01).</p></caption><graphic xlink:href="299933_fig8" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-36">To further clarify the neural locus of relevant information, reconstruction results were computed across fMRI patterns in bilateral pairs of FG areas. Specifically, to evaluate the posterior-to-anterior progression of information, reconstruction results were recomputed separately for bilateral posterior FG areas, for anterior FG areas as well as for inferior frontal gyrus areas capable of supporting face decoding (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-6" hwp:rel-id="ref-67">Nestor et al, 2016</xref>). Reconstruction results (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure 9</xref>) pointed to above-chance accuracy for both shape and surface information in posterior as well as in anterior FG areas, with equivalent levels of accuracy across regions, but not in IFG areas (two-tailed permutation test; Bonferroni correction).</p><fig id="fig9" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9.</label><caption hwp:id="caption-9"><p hwp:id="p-37">Reconstruction accuracy for three bilateral ROIs. Fusiform gyrus areas but not inferior frontal gyrus areas supported above-chance reconstruction of shape and surface (*** <italic toggle="yes">p</italic>&lt;0.001, two-tailed permutation tests; Bonferroni correction).</p></caption><graphic xlink:href="299933_fig9" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-38">Last, to relate the temporal and the spatial profile of reconstruction-relevant information, accuracy estimates from EEG and fMRI were correlated across time intervals and regions separately for shape and surface properties (Pearson correlation; FDR-correction across time points). In the case of shape, the results found a significant correlation between posterior FG-based estimates and EEG estimates around 180 ms after stimulus onset (<xref ref-type="fig" rid="fig10" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Figure 10</xref>). Multiple time points around 170 ms, but also for subsequent intervals evinced significant correlations with anterior FG estimates but not with IFG ones. A similar investigation of surface information revealed smaller correlation values and no significant correlation with any ROI (<xref ref-type="fig" rid="figS10" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Figure 10 – Figure supplement 1</xref>) in agreement with the results reported above (e.g., <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2c</xref>).</p><fig id="figS10" position="float" orientation="portrait" fig-type="figure" hwp:id="F13" hwp:rev-id="xref-fig-13-1 xref-fig-13-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIGS10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figS10</object-id><label>Figure 10 – supplement figure 1.</label><caption hwp:id="caption-13"><p hwp:id="p-88">Correlation of reconstruction accuracy for surface based on fMRI data from three ROIs and from EEG data across ∼10ms temporal intervals. No interval reaches significance (Pearson correlation; FDR correction across time, <italic toggle="yes">q</italic>&lt;0.01).</p></caption><graphic xlink:href="299933_figS10" position="float" orientation="portrait" hwp:id="graphic-13"/></fig><fig id="fig10" position="float" orientation="portrait" fig-type="figure" hwp:id="F10" hwp:rev-id="xref-fig-10-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;299933v1/FIG10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">fig10</object-id><label>Figure 10 with 1 supplement.</label><caption hwp:id="caption-10"><p hwp:id="p-39">Correlation of reconstruction accuracy for shape based on fMRI data from three ROIs and from EEG data across ∼10ms temporal intervals. Results are shown for shape. Intervals of significance are marked by corresponding segments at the top of the plot (Pearson correlation; FDR correction across time, <italic toggle="yes">q</italic>&lt;0.01).</p><p hwp:id="p-40"><xref ref-type="fig" rid="figS10" hwp:id="xref-fig-13-2" hwp:rel-id="F13">Figure 10 – supplement figure 1</xref>. Correlation of surface reconstruction accuracy for surface based on fMRI data from three ROIs and from EEG data across ∼10ms temporal intervals. No interval reaches significance (Pearson correlation; FDR correction across time, <italic toggle="yes">q</italic>&lt;0.01).</p></caption><graphic xlink:href="299933_fig10" position="float" orientation="portrait" hwp:id="graphic-10"/></fig></sec></sec><sec id="s3" hwp:id="sec-7"><title hwp:id="title-8">Discussion</title><p hwp:id="p-41">The present study examined the representational basis of shape and surface information underlying individual face processing. This investigation capitalized on a robust approach to image reconstruction to uncover and relate relevant representational structures captured by distinct modalities. The ability to retrieve such information successfully and consistently enabled us to address a number of key questions as follows.</p><p hwp:id="p-42">First, we examined the possibility of reconstructing both shape and surface information from each of four data types: behavioral, EEG, fMRI and TO. Our results confirmed that this is indeed possible while also revealing the advantage of surface over shape for face representations. Specifically, surface information was reconstructed more accurately than shape information for each of the empirical modalities but not for an image-based TO that yielded equivalent estimates of reconstruction accuracy for the two. Also, surface information was recovered with equivalent levels of success across different color channels, speaking to the role of chromatic information in face representations. The relative contribution of shape and surface properties continues to be contested. For example, the dominant role of surface properties in face recognition has been extensively documented for familiar faces (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Burton et al., 2005</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Calder et al., 2001</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">Hancock, Burton, &amp; Bruce, 1996</xref>; <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">Kaufmann &amp; Schweinberger, 2008</xref>; <xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-2" hwp:rel-id="ref-79">Russell et al., 2006</xref>; <xref ref-type="bibr" rid="c91" hwp:id="xref-ref-91-2" hwp:rel-id="ref-91">Vuong et al., 2005</xref>) while such a role has been assumed by shape properties in the case of unfamiliar faces (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">Jiang, Blanz, &amp; Rossion, 2011</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-2" hwp:rel-id="ref-50">Lai, Oruc, &amp; Barton, 2013</xref>). The importance of shape is also consistent with the value of configural information for holistic face perception (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Leder, &amp; Carbon, 2006</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-2" hwp:rel-id="ref-58">Maurer, Le Grand, &amp; Mondloch, 2002</xref>; <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">McKone &amp; Yovel, 2009</xref>; <xref ref-type="bibr" rid="c74" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">Piepers &amp; Robbins, 2012</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">Richler et al., 2009</xref>; <xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-2" hwp:rel-id="ref-82">Tanaka &amp; Gordon, 2011</xref>). Yet, other evidence suggests that even for unfamiliar faces, surface could provide dominant cues (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Itz, Golle et al., 2017</xref>; <xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-3" hwp:rel-id="ref-79">Russell, Sinha, Biederman, &amp; Nederhouser, 2006</xref>) and that configural shape information is of limited use (<xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-2" hwp:rel-id="ref-83">Taschereau-Dumouchel, Rossion, Schyns, &amp; Gosselin, 2010</xref>). In agreement with such evidence, we find that shape information is relatively underrepresented compared to its surface counterpart for unfamiliar faces. As a caveat to this conclusion, we note that the present investigation did not consider three-dimensional face shape (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Jiang, Blanz &amp; O’Toole, 2009</xref>) but only two-dimensional information – additional 3D cues may facilitate the recovery of overall shape information and lead to more precise representations (at least as accurately as that of surface properties). Such an outcome would be especially relevant for face perception in more naturalistic settings.</p><p hwp:id="p-43">Second, we aimed to characterize the spatiotemporal profile of shape and surface processing. With regard to temporal dynamics, previous work targeting specific ERP components has yielded rather inconsistent results. For instance, sensitivity to the shape of unfamiliar faces has been found at the latency of the N170 ERP component (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Caharel, Jiang, Blanz, &amp; Rossion, 2009</xref>) but also earlier, for P1 (<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Itz, Schweinberger, &amp; Kaufmann, 2016</xref>), and, in the case of familiar faces, later for P200 (<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Itz, Schweinberger, Schulz, &amp; Kaufmann, 2014</xref>). Similarly, surface processing is apparent first at the latency of the N250 component for both familiar and unfamiliar faces (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Caharel et al., 2009</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-2" hwp:rel-id="ref-39">Itz, Schweinberger, Schulz, &amp; Kaufmann, 2014</xref>), yet other studies have also found N170 sensitivity to facial surface (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Balas &amp; Nelson, 2010</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Brebner et al, 2011</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Minami, Nakajima, Changvisommid, &amp; Nakauch, 2015</xref>). Unlike previous work, the present investigation used pattern analysis applied to entire epochs rather than univariate analyses targeting specific ERP components. Following this approach, we found that reconstruction accuracy reaches significance for both shape and surface around 150ms and exhibits an extended interval of above-chance performance. From a theoretical standpoint, we note that a similar time course for shape and surface processing could prove advantageous for the efficient integration of this information into unified face percepts. Also, the present results are in broad agreement with the presence of facial identity information at the latency of N170 (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Caharel et al., 2009</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Itier &amp; Taylor, 2002</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">Nemrodov et al., 2016</xref>).</p><p hwp:id="p-44">Regarding the cortical locus of shape and surface processing, previous work has found sensitivity to shape (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Gao &amp; Wilson, 2013</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Gilaie-Dotan, Gelbard-Sagiv, &amp; Malach, 2010</xref>) and surface (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Harris, Young, &amp; Andrews, 2014</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Jiang et al., 2009</xref>) information within the fusiform face area (FFA), consistent with the notion that this region contains unified representations of faces (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Liu, Harris, &amp; Kanwisher, 2010</xref>). Also, recent research has found equivalent adaptation effects to shape and surface information in the occipital face area (OFA) and in the FFA (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Andrews et al., 2016</xref>). While our investigation targeted regions localized through pattern analysis rather than face-selective regions per se, our results also support the idea of shape and surface integration within common regions subserving facial identity representations. Specifically, shape and surface information was recovered from posterior and anterior fusiform areas able to discriminate different facial identities. In contrast, another IFG region capable of such discrimination was unable to support either shape or surface reconstruction. Interestingly, a frontal area involved in face processing has been found in the human and monkey brain (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Axelrod &amp; Yovel, 2015</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">Rajimehr, Young &amp; Tootell, 2009</xref>; <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-1" hwp:rel-id="ref-87">Tsao et al., 2008</xref>). Recent work has argued that this area hosts higher-level, view-invariant facial representations (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Guntupalli, Wheeler &amp; Gobbini, 2017</xref>) facilitating access to person knowledge through the extended system for face perception (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Collins &amp; Olson, 2014</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Haxby et al, 2000</xref>). This possibility accounts for the inability of the IFG to support image reconstruction, as reported above, while also pointing to the need to characterize more precisely the transformation of visual information across a hierarchy of face processing regions.</p><p hwp:id="p-45">Third, we assessed the correspondence of facial information retrieved by different modalities. A widely influential approach, representational similarity analysis (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Kriegeskorte, Mur &amp; Bandettini, 2008</xref>), has been instrumental in relating representational structures captured by different neuroimaging modalities and computational models (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Carlin &amp; Kriegeskorte, 2017</xref>; Carlson et al., 2013; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Cichy, Pantazis, &amp; Oliva, 2016</xref>; <xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Olander et al., 2017</xref>). Overall, this approach has uncovered both commonalities and complementarity in the visual information retrieved by different modalities (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Cichy, Pantazis &amp; Oliva, 2016</xref>). Our results agree with this conclusion while further analyzing the source of common/distinct visual information in terms of shape and surface. Specifically, RSA revealed overall similarity of representational structure for faces across different modalities. However, a more detailed investigation of reconstructed information showed that shape is more consistently recovered across modalities relative to surface dimensions. For instance, fMRI yielded surface reconstructions that did not match well their behavioral and EEG counterparts. Yet, this finding could be due to the fact that the fMRI signal considered reflects different processing stages of facial information relative to other modalities.</p><p hwp:id="p-46">To address this possibility, reconstruction accuracy was systematically related across different intervals and areas. Interestingly, in the case of shape, this analysis revealed significant correlations between EEG-based results and their fMRI counterpart. Specifically, such correlations were noted as early as 170ms after stimulus onset for both posterior and anterior FG areas. However, these correlations only persisted at later latencies for aFG areas consistent with a spatiotemporal hierarchy of processing steps in face perception. In contrast, in the case of surface, EEG and fMRI yielded lower, non-significant correlations suggesting partly different representations recovered by these two neuroimaging modalities.</p><p hwp:id="p-47">At a finer-grained level, the issues discussed above emphasize the need to elucidate the specific features underlying face processing and their neural representations. An evaluation of reconstruction heatmaps show that eye shape and color information is especially well retrieved in agreement with the role of such information for face recognition (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Ince et al., 2016</xref>; <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Issa &amp; DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">Nestor, Vettel &amp; Tarr, 2008</xref>). Additional information regarding nose and mouth shape could also be retrieved (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Abudarham &amp; Yovel, 2016</xref>) while forehead and cheek colors were recovered with various degrees of accuracy across different color channels. Importantly, such cues appeared to reflect objective information content as revealed by TO reconstructions. Further, heatmaps of lighting and color channels exhibited different spatial patterns suggesting that chromatic information supplements lighting-based representations of facial identity (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-2" hwp:rel-id="ref-68">Nestor et al, 2013</xref>). Chromatic information stored in high-level visual areas may facilitate such representations and account for the proximity of these areas to face-processing cortex (<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">Lafer-Souza et al, 2016</xref>).</p><p hwp:id="p-48">While the present investigation targets the representational basis of shape and surface processing via image reconstruction, we also note here the converse aspect of this investigation. Specifically, we inquire into the benefit of shape and surface decomposition for image reconstruction purposes. Previous work has relied on the coarse alignment of facial features to minimize the need for such decomposition (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Cowen, Chun, &amp; Kuhl, 2014</xref>; <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-7" hwp:rel-id="ref-67">Nestor, Plaut, &amp; Behrmann, 2016</xref>; but see <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">Chang &amp; Tsao, 2017</xref>; <xref ref-type="bibr" rid="c92" hwp:id="xref-ref-92-2" hwp:rel-id="ref-92">Zhan et al., 2017</xref>). Hence, it is useful to assess, in the context of the present investigation, whether recomposed faces are more accurately reconstructed than intact ones. Interestingly, decomposition appeared to provide a systematic advantage to reconstruction across modalities, yet this advantage did not reach significance in any given case. Clearly, shape-surface decomposition would be required when dealing with pronounced image variability such as that due to viewpoint. However, if such variability is controlled across stimuli and if feature alignment is successfully carried out in advance it appears that shape-surface decomposition confers, at most, a minimum advantage to reconstruction.</p><p hwp:id="p-49">Of relevance here, we note that shape-surface manipulations introduce a systematic loss of information due to image (re)warping and that this loss is likely to limit reconstruction success (see Materials and Methods, Stimulus shape-surface re/re-composition). To address this and, also, to allow the extension of current work to a wider, more diverse class of face images, state-of-the-art algorithms for fiducial point detection and shape analysis could be employed (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Ahdid, Taifi, Safi &amp; Manaut, 2016</xref>). Such methods would confer robustness to shape-surface decomposition and facilitate the integration of reconstruction results with algorithms for face recognition that rely on elaborate schemas for shape/surface analysis (<xref ref-type="bibr" rid="c93" hwp:id="xref-ref-93-1" hwp:rel-id="ref-93">Zhao, Chellappa, Phillips &amp; Rosenfeld, 2003</xref>) – such integration could serve specific goals for translational research (e.g., the automatic identification of a face image reconstructed from eyewitness memory).</p><p hwp:id="p-50">At the other end, we note certain theoretical limitations associated with the use of naturalistic stimuli. Of specific relevance here is the covariation of different types of information conveyed by shape and surface properties. For instance, shading as captured by surfaces can provide cues to 3D shape (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Kramer, Jenkins, &amp; Burton, 2016</xref>). Such covariation constraints the interpretation of our results in important ways. Concretely, in the context of empirical modalities, we found no advantage to combining shape and surface information over using surface information in isolation (though such an advantage was noticed for TO). The lack of this advantage could simply reflect the covariation of shape and surface properties in natural images. Thus, to better disentangle the distinct contribution of such properties to face representations, one may be better served by artificial stimuli that manipulate orthogonally distinct types of information and, also, by image processing techniques that ascribe 3D cues exclusively to shape (<xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-2" hwp:rel-id="ref-72">O’Toole et al, 1999</xref>; <xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Paysan, Knothe, Amberg, Romdhani, Vetter, 2009</xref>).</p><p hwp:id="p-51">In summary, the present work seeks to uncover the representational underpinnings of shape and surface processing in face perception through the use of novel image-reconstruction methodology. Our results show that such information can be reliably extracted from multiple modalities, that its representational structure is partly shared across modalities and that its spatiotemporal profile speaks to the close integration of shape and surface cues in face processing. More generally, the present findings showcase the value of image reconstruction methodology in elucidating the content and the neural profile of visual representations.</p></sec><sec id="s4" hwp:id="sec-8"><title hwp:id="title-9">Materials and methods</title><sec id="s4a" hwp:id="sec-9"><title hwp:id="title-10">Stimuli</title><p hwp:id="p-52">A common subset of 108 stimulus images was identified across three different studies investigating empirical and computational aspects of unfamiliar face recognition (see Experimental procedures). Images of 54 individuals displaying neutral and happy facial expressions were selected from three databases: AR (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Martinez &amp; Benavente, 1998</xref>), FEI (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">Thomaz &amp; Giraldi, 2010</xref>) and Radboud (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Langner et al., 2010</xref>). All images featured young adult Caucasian males with frontal view, gaze and illumination. The stimuli were selected so that no facial accessories, hair or makeup obscured the internal features of the face and so that all happy expressions displayed an open-mouth smile. These images were: (a) scaled uniformly and aligned with roughly the same position of the eyes and the nose; (b) cropped to eliminate background; (c) normalized with the same mean and root mean square (RMS) contrast values separately for each color channel in CIEL*a*b* color space, and (d) reduced to the same size (95 X 64 pixels). Note that this procedure did not change the aspect ratio of the images though the position of the eyes and the nose was roughly the same across stimuli. Thus, every effort was made to homogenize the stimulus set both in terms of low-level and high-level face properties preventing the potential contribution of such factors to image reconstruction.</p></sec><sec id="s4b" hwp:id="sec-10"><title hwp:id="title-11">Participants</title><p hwp:id="p-53">All participants (age range across studies: 18-34 years; 21 males, 22 females) were Caucasian adults with normal or corrected-to-normal vision and no history of cognitive or neurological disorder. All participants provided informed consent and all experimental procedures were approved by the Research Ethics Board at University of Toronto and/or the Institutional Review Board at Carnegie Mellon University.</p></sec><sec id="s4c" hwp:id="sec-11"><title hwp:id="title-12">Experimental procedures</title><p hwp:id="p-54">Data used for reconstruction purposes were selected from three previous studies as follows.</p><p hwp:id="p-55">Behavioral data consisted of similarity ratings with pairs of faces acquired from 22 participants (reported in <xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-3" hwp:rel-id="ref-68">Nestor et al, 2013</xref>, Experiment 1). Briefly, on each trial, participants were presented with two facial identities, one neutral and one happy, side by side, for 400ms, and were asked to judge their visual similarity on a 5-point scale. Each participant rated all possible 1431 facial pairs, corresponding to 54 facial identities - for clarity, only a subset of the original data were considered here (i.e., 6 additional facial identities were not used in the EEG study summarized below and, hence, were excluded from further analyses of behavioral data).</p><p hwp:id="p-56">EEG data were previously acquired from 13 participants who performed a go/no-go gender categorization task (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-4" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>). On ‘no-go’ trials, participants viewed the stimuli described above while, on ‘go’ trials, they were asked to press a designated key in response to the appearance of a female face. Each of the 108 main stimuli was presented for 300 ms and repeated across 64 trials for each participant.</p><p hwp:id="p-57">fMRI data were acquired from 8 participants who performed a continuous one-back identity task (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-8" hwp:rel-id="ref-67">Nestor et al, 2016</xref>). Briefly, on each trial, participants viewed a stimulus for 900 ms and responded whether the current stimulus displayed the same individual as that presented on the previous trial, irrespective of emotional expression. The experiment used a wide-spaced design (8s trials) and allowed for the repetition of each stimulus for a minimum of 10 trials across five 1-hr sessions for each participant. Again, only a subset of the stimuli used in the original study is considered here to enable direct comparison with data from the other modalities.</p><p hwp:id="p-58">To be clear, we note that the neuroimaging studies above (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-5" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>; <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-9" hwp:rel-id="ref-67">Nestor, Plaut &amp; Behrmann, 2016</xref>) did not separate shape and surface cues for reconstruction purposes nor did they assess the contribution of such cues to visual face representations. Further, the behavioral study above (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-4" hwp:rel-id="ref-68">Nestor et al, 2013</xref>) did not target any form of image reconstruction and, thus, it provides a new testing ground for reconstruction endeavors.</p></sec><sec id="s4d" hwp:id="sec-12"><title hwp:id="title-13">Representational similarity analyses</title><p hwp:id="p-59">Our reconstruction procedure fundamentally relies on the structure of representational (dis)similarity matrices (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>) to derive facial image features and to use such features for reconstruction purposes. Hence, the first step of our investigation is to construct such matrices separately for each data type.</p><p hwp:id="p-60">Specifically, for each modality and for each participant, a similarity matrix was designed to store pairwise similarity estimates across 54 facial identities. In the case of behavioral data, these estimates were readily available in the form of similarity ratings. In the case of EEG and fMRI data such estimates were derived through one-against-one pattern classification of different identities, separately for each expression, using linear support vector machines (SVM). Briefly, pairwise classification was applied across EEG spatiotemporal patterns recorded across at 12 occipitotemporal (OT) electrodes (left: P5, P7, P9, PO3, PO7, O1; right: P6, P8, P10, PO4, PO8, O2) during an interval spanning 50-650ms after stimulus onset – these spatiotemporal patterns were selected based on their ability to support face decoding (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-6" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>). Analogously, for fMRI, classification was applied across multivoxel patterns within areas supporting above-chance face discrimination as identified through prior searchlight mapping. Such areas were previously identified (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-10" hwp:rel-id="ref-67">Nestor et al, 2016</xref>) bilaterally in the posterior fusiform gyrus (group-map Talairach coordinates: left FG, –39, –49, –16; right posterior FG, 39, −69, −4), the anterior fusiform gyrus (left FG, –34, –36, –14; right anterior FG/parahippocampal gyrus, 31, −19, −9) and the inferior frontal gyrus (left IFG, −36, 24, −9; right IFG, 46, 11, −1). Thus, discrimination accuracy computed across neural patterns in these regions was used to estimate the similarity of the stimuli eliciting such patterns – a more detailed account of data preprocessing and pattern analyses can be found in the studies above.</p><p hwp:id="p-61">In addition, objective measures of image similarity in CIEL*a*b* color space were computed for the purpose of constructing a theoretical observer (TO) exploiting low-level visual similarity. To this end, pixelwise Euclidean distances were computed across all pairs of facial identities separately for each expression and the results were stored in corresponding similarity matrices. Of note, while more elaborate models of face similarity are of interest (e.g., <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">Carlin &amp; Kriegeskorte, 2017</xref>), the TO above is particularly relevant given that pixelwise similarity is also one of the main criteria for assessing reconstruction quality. Thus, a TO matching typical criteria for result assessment is particularly well-suited for estimating an upper limit of reconstruction success.</p><p hwp:id="p-62">Last, similarity estimates were averaged across participants and across expressions to deliver a single similarity matrix for each modality: behavioral, EEG, fMRI and TO. These resulting estimates were related across modalities via Spearman correlation to estimate the presence of a common representational structure.</p></sec><sec id="s4e" hwp:id="sec-13"><title hwp:id="title-14">Stimulus shape-surface de/re-composition</title><p hwp:id="p-63">All stimuli were tested for their ability to undergo reliable shape-surface decomposition and recomposition. Specifically, for reconstruction purposes, all stimuli were analyzed into shapes (i.e., configurations of fiducial points, such as the corners of the eyes and the tip of the nose, labeled with their geometric coordinates) and shape-free surfaces (i.e., facial images warped to a common shape template). To this end, fiducial points were manually marked for each stimulus using the Interface toolbox (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">Kramer, Jenkins, &amp; Burton, 2016</xref>) and, then, the marked stimulus was warped to a preset shape template. Thus, the shape of each stimulus is represented as a vector of fiducial point coordinates (82 points x 2 in-plane coordinates) while its surface is represented by a template-warped image.</p><p hwp:id="p-64">The process above was then reversed by recombining shapes and surfaces into approximations of the original stimuli. This procedure was carried out to estimate information loss inherent to de/re-composition due to image (re)warping and, thus, to assess, the objective cost of shape/surface manipulations for reconstruction. To this end, rewarped versions of the stimuli obtained through recomposition were compared against actual stimuli. Concretely, for each rewarped stimulus we computed the ratio between the pixelwise Euclidean distance relative to its original version and the distance to every other stimulus, one at a time; hence, ratios larger than 1 would render rewarped stimuli more similar to other facial identities. The outcome of these computations (mean ± 1SD across 54 identities) yielded ratios of 0.333 (±0.054) and 0.329 (±0.052) for neutral and happy faces, respectively. Thus, shape-surface de/re-composition generally preserves identity information but it does introduce systematic image distortions likely to limit reconstruction success.</p></sec><sec id="s4f" hwp:id="sec-14"><title hwp:id="title-15">Reconstruction procedure</title><p hwp:id="p-65">The current procedure builds upon previous work (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-11" hwp:rel-id="ref-67">Nestor et al, 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-7" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>) aimed at deriving pictorial features directly from the structure of empirical data and to use them for facial image reconstruction. Here, we further develop this procedure to derive separate sets of shape and surface features from multiple data types and we assess their relative contribution to face representations as revealed by image reconstruction.</p><p hwp:id="p-66">For each modality, the reconstruction procedure was separately conducted following a sequence of steps (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref>). Concisely, in order to reconstruct the appearance of any given target, the procedure involved: (i) selecting a similarity submatrix containing the pairwise similarity of all faces other than the target; (ii) estimating the dimensions that structure face space by applying multidimensional scaling (MDS) to the resulting submatrix; (iii) deriving, for each dimension, shape and surface features through a strategy akin to reverse correlation; (iv) assessing feature significance and selecting a subset of informative features; (v) projecting the target face into the existing face space based on its similarity with the other faces; (vi) reconstructing the shape and the surface of the target face through a linear combination of informative features, and (vii) combining the resulting shape and surface into a single image reconstruction of the target face.</p><p hwp:id="p-67">In more detail, the leave-one-out procedure enforces non-circularity by excluding the reconstruction target from the estimation of face space and its underlying features. Specifically, a face space construct was derived from the pairwise similarity of 53 facial identities and, then, its corresponding features were used in the reconstruction of the target face. To this end, a 20-dimensional face space was estimated through metric MDS, given that this number of dimensions accounted for more than 90% of data variance for any modality and, also, that it agrees with previous estimates of face space dimensionality in human recognition (i.e., 15-22) (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Lewis, 2004</xref>). Then, a corresponding number of shape and surface features were computed for each dimension through an approach akin to reverse correlation/image classification (see (<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">Murray, 2011</xref>) for a review). Notably, this approach aims to synthesize facial features responsible for face space topography through a linear combination of face properties (i.e., shape vectors or surface images). This combination was computed as a sum of shapes and surfaces for all faces weighted by the z-scored coordinates of the corresponding faces on any given dimension. Thus, the outcome of these computations delivers, for each dimension, one shape feature, or ‘classification vector’ (CV), and one surface feature, or ‘classification image’ (CIM) – for clarity, each surface feature consists in a triplet of images, one for each color channel in CIEL*a*b*.</p><p hwp:id="p-68">Further, we considered the possibility that not all face space dimensions encode visual information (e.g., as opposed to higher-level semantic information or just noise). Also, it is possible that sources of shape and surface information are differently distributed across dimensions and their corresponding features. Hence, it is important to identify relevant subsets of features that can contribute meaningful information to reconstruction. To this end, each feature was assessed for the presence of significant information. Specifically, all face space identities were randomly shuffled with respect to their coordinates on each dimension and a corresponding feature was recomputed for a total of 10<sup>3</sup> permutations. Then, each true feature was compared to all permutation-based features, fiducial point by fiducial point, in the case of shape, or pixel by pixel, for each CIEL*a*b* color channel, in the case of surface (two-tailed permutation test; FDR correction across points and pixels, respectively; <italic toggle="yes">q</italic> &lt; 0.1). Following this procedure, only features that contained significant shape or surface information were selected for reconstruction purposes.</p><p hwp:id="p-69">Next, the target face was projected into the existing face space. To this end, a new MDS solution was constructed for all 54 identities and aligned with the original one via Procrustes analysis using the 53 common identities between the two spaces. The resulting alignment provides us with a mapping between the two spaces that allows us to project the target face and to retrieve its coordinates in the original space. Then, informative features were linearly combined proportionally to the coordinates of the target face on each corresponding dimension and their sum was added to the average shape and surface of the 53 faces used for feature derivation. We note that face space was uniformly scaled under the constraint that all reconstructed surfaces should have the same RMS contrast and mean value in each color channel as the surfaces of the experimental stimuli – these values were equated across experimental stimuli in an effort to minimize the contribution of low-level images differences to perception (see Stimuli above). A similar manipulation was also conducted for shape reconstructions to ensure that the variance of fiducial point coordinates matched that of the stimulus shapes. Last, the shape and surface thus computed were manually combined using the Interface toolbox into a single reconstruction to which we refer as a ‘recomposed face’.</p><p hwp:id="p-70">For clarity, face space is constructed here across facial identities irrespective of emotional expression (e.g., by averaging similarity matrices for the two expressions). However, reconstruction proceeds by deriving and combining features separately for neutral and happy faces (i.e., a neutral face is reconstructed from features derived from other neutral faces while a happy face is reconstructed from features derived from other happy faces). Another possibility would be to consider separate spaces for each expression; however, previous investigations evaluated and confirmed the invariance of neural-based face space across these two emotions (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-8" hwp:rel-id="ref-66">Nemrodov et al, 2018</xref>).</p><p hwp:id="p-71">Finally, to evaluate the benefit of considering separately shape and surface information for reconstruction purposes, another set of image reconstructions were computed without appeal to this decomposition. Specifically, face stimuli were treated in the same manner as shape-free surfaces above. We refer to the outcome of this procedure as ‘intact reconstructions’.</p></sec><sec id="s4g" hwp:id="sec-15"><title hwp:id="title-16">Estimation of reconstruction accuracy</title><p hwp:id="p-72">The accuracy of reconstruction results was assessed in two different ways: by objective image-based measures and experimentally, by additional behavioral testing.</p><p hwp:id="p-73">In detail, each reconstructed shape was compared to its target via the Euclidean distance computed across corresponding fiducial point coordinates. Then, the accuracy of its reconstruction was measured as the proportion of instances for which that distance was smaller than the distance between the reconstruction and any stimulus shape other than the target. This procedure was conducted for all reconstructions, separately for each expression and each modality.</p><p hwp:id="p-74">Reconstructed surface accuracy was measured similarly, except that Euclidean distances were computed across pixel values in CIEL*a*b* space. The same procedure was also followed for recomposed and intact reconstructions.</p><p hwp:id="p-75">Statistical significance was then assessed through permutation tests. Specifically, the shape and the surface of each target was recomputed based on the random shuffling of identity labels across all points in face space (for a total of 10<sup>4</sup> permutations). Then, the accuracy of the true reconstructions was related to that of permutation-based reconstructions (two-tailed permutation tests).</p><p hwp:id="p-76">However, this procedure was not feasible in the case of recomposed faces (since that would require the manual recombination of shape and surface for a prohibitively high number of permutation-based reconstructions). For this reason, and, also, to provide a complementary evaluation of reconstruction results, additional behavioral testing was conducted as follows.</p><p hwp:id="p-77">Reconstructed images consisting of 432 recomposed faces (54 identities X 2 expressions X 4 modalities: behavioral, EEG, fMRI, and TO) were judged in terms of their relative similarity to the actual stimuli. To this end, 27 new participants (seven males and twenty females, age range: 18-25) were asked to match image reconstructions to their targets in a two-alternative forced choice (2AFC) task. Specifically, each reconstruction was displayed in the presence of two stimuli, one of which was the actual target and the other another face image. Thus, on each trial, a display was shown containing a reconstructed image, at the top, and two stimuli side by side, at the bottom, all of which had the same expression and the same size (see Stimuli). Each display was presented until participants indicated which stimulus was more similar to the top image by pressing a designated left/right key. For each participant, any reconstructed image was presented 4 times along with different foils so that, across participants, each reconstruction was presented together with every possible foil. Stimulus order was pseudorandomized so that no reconstruction appeared twice on consecutive trials and target stimuli appeared equally often on the left/right side. Each participant completed 1728 trials divided equally across 9 blocks. Experimental testing was conducted within a single 1.5-hr session.</p><p hwp:id="p-78">Parametric statistical analyses were next conducted to assess reconstruction success against chance (one-sample two-tailed t-tests against 50% chance-level participant performance) as well as the relative success of reconstruction across modalities and expressions (2-way factorial analysis of variance: 4 modalities X 2 expressions).</p><p hwp:id="p-79">To further compare and account for the outcome of different types of reconstruction, additional analyses were conducted as detailed below.</p></sec><sec id="s4h" hwp:id="sec-16"><title hwp:id="title-17">Evaluation of reconstruction results</title><p hwp:id="p-80">First, parametric tests across items (i.e., across facial identities) were carried out to estimate the relative success of reconstruction results. Specifically, we conducted a three-way factorial analysis of variance (4 modalities X 2 expressions X 4 reconstruction types: shape, surface, recomposed and intact reconstructions) along with planned comparisons – accuracies were collapsed across expressions given the lack of evidence for a corresponding effect from prior analyses. Of note, parametric analyses across items provide a liberal way to estimate reconstruction success (e.g., compared to the permutation tests above); however, the goal of the current analysis was not to estimate significance against a preset chance level but rather to evaluate differences in reconstruction success across modalities and reconstruction types.</p><p hwp:id="p-81">We note that comparing modalities in terms of overall reconstruction accuracy can be informative, by answering, for instance, how closely empirical modalities can approach TO-level performance. However, such an analysis provides an incomplete and potentially misleading picture of the relationship across empirical modalities since any difference, or lack thereof, can be the outcome of differences in experimental designs separately optimized for each modality (e.g., stimulus duration, number of stimulus presentations, task, number of participants).</p><p hwp:id="p-82">Second, and aiming to address the concern above, reconstruction results were related with each other via correlation across facial identities for each pair of modalities. Specifically, image-based accuracies were related to each other across modalities, separately for shape and surface, via Pearson correlation. We note that this investigation parallels representational similarity analysis with the difference that, here, we correlate item-specific reconstruction accuracies as opposed to pairwise item similarity estimates. In addition, to clarify the relationship between behavioral and neural-based results, linear regression was used to account for behavioral reconstruction accuracies in terms of their EEG and fMRI counterparts separately for shape and surface reconstructions.</p><p hwp:id="p-83">Third, to clarify the spatiotemporal profile of the information supporting shape and surface reconstruction, additional analyses were conducted across time, for EEG, and across different ROIs, for fMRI. Specifically, for the former, reconstruction was conducted across smaller 10ms temporal intervals between −100 and 700ms (i.e., across 60-dimensional vectors; 12 OT electrodes x 5 consecutive time points) providing us with the time course of reconstruction accuracy. In the case of fMRI, reconstruction was conducted for distinct pairs of bilateral ROIs in the posterior FG, the anterior FG and the IFG. Image-based accuracy was then estimated for each temporal interval and for each ROI pair and significance was estimated by two-tailed permutation tests.</p></sec></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-18">Acknowledgments.</title><p hwp:id="p-84">This research was supported by the Natural Sciences and Engineering Research Council of Canada (A.N. and M.N.), by the National Institute of Health (to M.B.), and by a Research Competitiveness Fund Award (A.N.).</p></ack><sec sec-type="COI-statement" hwp:id="sec-17"><title hwp:id="title-19">Competing interests</title><p hwp:id="p-85">The authors declare no competing financial or non-financial interests.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-20">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Abudarham N."><surname>Abudarham</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Yovel G."><surname>Yovel</surname>, <given-names>G.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-2">Reverse engineering the face space: Discovering the critical features for face identification</article-title>. <source hwp:id="source-1">Journal of Vision</source>, <volume>16</volume>(<issue>3</issue>), <fpage>40</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1167/16.3.40" ext-link-type="uri" xlink:href="https://doi.org/10.1167/16.3.40" hwp:id="ext-link-2">https://doi.org/10.1167/16.3.40</ext-link></citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Ahdid R."><surname>Ahdid</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Taifi K."><surname>Taifi</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Safi S."><surname>Safi</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Manaut B."><surname>Manaut</surname>, <given-names>B.</given-names></string-name> (n.d.). <article-title hwp:id="article-title-3">A Survey on Facial Feature Points Detection Techniques and Approaches</article-title>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://waset.org/publications/10005826" ext-link-type="uri" xlink:href="http://waset.org/publications/10005826" hwp:id="ext-link-3">http://waset.org/publications/10005826</ext-link></citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Andrews T. J."><surname>Andrews</surname>, <given-names>T. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baseler H."><surname>Baseler</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jenkins R."><surname>Jenkins</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Young A. W."><surname>Young</surname>, <given-names>A. W.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-4">Contributions of feature shapes and surface cues to the recognition and neural representation of facial identity</article-title>. <source hwp:id="source-2">Cortex</source>, <volume>83</volume>, <fpage>280</fpage>–<lpage>291</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.cortex.2016.08.008" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.cortex.2016.08.008" hwp:id="ext-link-4">http://doi.org/10.1016/j.cortex.2016.08.008</ext-link></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Attick JJ"><surname>Attick</surname> <given-names>JJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Griffin PA"><surname>Griffin</surname> <given-names>PA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Redlich AN"><surname>Redlich</surname> <given-names>AN</given-names></string-name>. (<year>1996</year>). <article-title hwp:id="article-title-5">Statistical approach to shape from shading: reconstruction of 3D face surfaces from single 2D images</article-title>. <source hwp:id="source-3">Plast Recon Surg</source> <volume>120</volume>:<fpage>432</fpage>–<lpage>57</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Axelrod V."><surname>Axelrod</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Yovel G."><surname>Yovel</surname>, <given-names>G.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-6">Successful Decoding of Famous Faces in the Fusiform Face Area</article-title>. <source hwp:id="source-4">PLOS ONE</source>, <volume>10</volume>(<issue>2</issue>), <fpage>e0117126</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1371/journal.pone.0117126" ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0117126" hwp:id="ext-link-5">https://doi.org/10.1371/journal.pone.0117126</ext-link></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Balas B."><surname>Balas</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nelson C. A."><surname>Nelson</surname>, <given-names>C. A.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-7">The role of face shape and pigmentation in other-race face perception: An electrophysiological study</article-title>. <source hwp:id="source-5">Neuropsychologia</source>, <volume>48</volume>(<issue>2</issue>), <fpage>498</fpage>–<lpage>506</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/J.NEUROPSYCHOLOGIA.2009.10.007" ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.NEUROPSYCHOLOGIA.2009.10.007" hwp:id="ext-link-6">https://doi.org/10.1016/J.NEUROPSYCHOLOGIA.2009.10.007</ext-link></citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Biederman I."><surname>Biederman</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kalocsai P."><surname>Kalocsai</surname>, <given-names>P.</given-names></string-name> (<year>1997</year>). <article-title hwp:id="article-title-8">Neurocomputational bases of object and face recognition. Philosophical Transactions of the Royal Society of London. Series B</article-title>, <source hwp:id="source-6">Biological Sciences</source>, <volume>352</volume>(<issue>1358</issue>), <fpage>1203</fpage>–<lpage>19</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1098/rstb.1997.0103" ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.1997.0103" hwp:id="ext-link-7">https://doi.org/10.1098/rstb.1997.0103</ext-link></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Brebner J. L."><surname>Brebner</surname>, <given-names>J. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krigolson O."><surname>Krigolson</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Handy T. C."><surname>Handy</surname>, <given-names>T. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Quadflieg S."><surname>Quadflieg</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Turk D. J."><surname>Turk</surname>, <given-names>D. J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-9">The importance of skin color and facial structure in perceiving and remembering others: An electrophysiological study</article-title>. <source hwp:id="source-7">Brain Research</source>, <volume>1388</volume>, <fpage>123</fpage>–<lpage>133</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/J.BRAINRES.2011.02.090" ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.BRAINRES.2011.02.090" hwp:id="ext-link-8">https://doi.org/10.1016/J.BRAINRES.2011.02.090</ext-link></citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Bruce V."><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Healey P."><surname>Healey</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burton M."><surname>Burton</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doyle T."><surname>Doyle</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Coombes A."><surname>Coombes</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Linney A."><surname>Linney</surname>, <given-names>A.</given-names></string-name> (<year>1991</year>). <article-title hwp:id="article-title-10">Recognising facial surfaces</article-title>. <source hwp:id="source-8">Perception</source>, <volume>20</volume>(<issue>6</issue>), <fpage>755</fpage>–<lpage>769</lpage>. doi:<pub-id pub-id-type="doi">10.1068/p200755</pub-id></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Bruce V."><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Langton S."><surname>Langton</surname>, <given-names>S.</given-names></string-name> (<year>1994</year>). <article-title hwp:id="article-title-11">The use of pigmentation and shading information in recognising the sex and identities of faces</article-title>. <source hwp:id="source-9">Perception</source>, <volume>23</volume>(<issue>7</issue>), <fpage>803</fpage>–<lpage>822</lpage>. doi:<pub-id pub-id-type="doi">10.1068/p230803</pub-id></citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="book" citation-type="book" ref:id="299933v1.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Bruce V."><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Young A. W."><surname>Young</surname>, <given-names>A. W.</given-names></string-name> (<year>1998</year>). <source hwp:id="source-10">In the eye of the beholder: The science of face perception</source>. <publisher-loc>Oxford, England; New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jenkins R."><surname>Jenkins</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hancock P. J. B."><surname>Hancock</surname>, <given-names>P. J. B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="White D."><surname>White</surname>, <given-names>D.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-12">Robust representations for face recognition: The power of averages</article-title>. <source hwp:id="source-11">Cognitive Psychology</source>, <volume>51</volume>(<issue>3</issue>), <fpage>256</fpage>–<lpage>284</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/J.COGPSYCH.2005.06.003" ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.COGPSYCH.2005.06.003" hwp:id="ext-link-9">https://doi.org/10.1016/J.COGPSYCH.2005.06.003</ext-link></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schweinberger S. R."><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jenkins R."><surname>Jenkins</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kaufmann J. M."><surname>Kaufmann</surname>, <given-names>J. M.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-13">Arguments against a configural processing account of familiar face recognition</article-title>. <source hwp:id="source-12">Perspectives on Psychological Science: A Journal of the Association for Psychological Science</source>, <volume>10</volume>(<issue>4</issue>), <fpage>482</fpage>–<lpage>496</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1745691615583129</pub-id></citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Cabeza R."><surname>Cabeza</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kato T."><surname>Kato</surname>, <given-names>T.</given-names></string-name> (<year>2000</year>). <article-title hwp:id="article-title-14">Features are Also Important: Contributions of Featural and Configural Processing to Face Recognition</article-title>. <source hwp:id="source-13">Psychological Science</source>, <volume>11</volume>(<issue>5</issue>), <fpage>429</fpage>–<lpage>433</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1111/1467-9280.00283" ext-link-type="uri" xlink:href="https://doi.org/10.1111/1467-9280.00283" hwp:id="ext-link-10">https://doi.org/10.1111/1467-9280.00283</ext-link></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Caharel S."><surname>Caharel</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="d’Arripe O."><surname>d’Arripe</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ramon M."><surname>Ramon</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jacques C."><surname>Jacques</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rossion B."><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-15">Early adaptation to repeated unfamiliar faces across viewpoint changes in the right hemisphere: Evidence from the N170 ERP component</article-title>. <source hwp:id="source-14">Neuropsychologia</source>, <volume>47</volume>(<issue>3</issue>), <fpage>639</fpage>–<lpage>643</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.11.016</pub-id></citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Caharel S."><surname>Caharel</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jiang F."><surname>Jiang</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rossion B."><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-16">Recognizing an individual face: 3D shape contributes earlier than 2D surface reflectance information</article-title>. <source hwp:id="source-15">NeuroImage</source>, <volume>47</volume>(<issue>4</issue>), <fpage>1809</fpage>–<lpage>1818</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.05.065</pub-id></citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Calder A. J."><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Miller P."><surname>Miller</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Young A. W."><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Akamatsu S."><surname>Akamatsu</surname>, <given-names>S.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-17">A principal component analysis of facial expressions</article-title>. <source hwp:id="source-16">Vision Research</source>, <volume>41</volume>(<issue>9</issue>), <fpage>1179</fpage>–<lpage>1208</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/S0042-6989(01)00002-5" ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0042-6989(01)00002-5" hwp:id="ext-link-11">https://doi.org/10.1016/S0042-6989(01)00002-5</ext-link></citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Cant J. S."><surname>Cant</surname>, <given-names>J. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Large M."><surname>Large</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McCall L."><surname>McCall</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Goodale M. A."><surname>Goodale</surname>, <given-names>M. A.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-18">Independent processing of form, colour, and texture in object perception</article-title>. <source hwp:id="source-17">Perception</source>, <volume>37</volume>(<issue>1</issue>), <fpage>57</fpage>–<lpage>78</lpage>. doi:<pub-id pub-id-type="doi">10.1068/p5727</pub-id></citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Carlin J. D."><surname>Carlin</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-19">Adjudicating between face-coding models with individual-face fMRI responses</article-title>. <source hwp:id="source-18">PLOS Computational Biology</source>, <volume>13</volume>(<issue>7</issue>), <fpage>e1005604</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1371/journal.pcbi.1005604" ext-link-type="uri" xlink:href="http://doi.org/10.1371/journal.pcbi.1005604" hwp:id="ext-link-12">http://doi.org/10.1371/journal.pcbi.1005604</ext-link></citation></ref><ref id="c20" hwp:id="ref-20"><citation publication-type="website" citation-type="web" ref:id="299933v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Carlson T. A."><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ritchie J. B."><surname>Ritchie</surname>, <given-names>J. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Durvasula S."><surname>Durvasula</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ma J."><surname>Ma</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-20">Reaction Time for Object Categorization Is Predicted by Representational Distance</article-title>. <source hwp:id="source-19">Journal of Cognitive Neuroscience</source>, <volume>26</volume>(<issue>1</issue>), <fpage>132</fpage>–<lpage>142</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1162/jocn_a_00476" ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00476" hwp:id="ext-link-13">https://doi.org/10.1162/jocn_a_00476</ext-link></citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Chang C. H."><surname>Chang</surname>, <given-names>C. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nemrodov D."><surname>Nemrodov</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee A. C. H."><surname>Lee</surname>, <given-names>A. C. H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nestor A."><surname>Nestor</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-21">Memory and perception-based facial image reconstruction</article-title>. <source hwp:id="source-20">Scientific Reports</source>, <volume>7</volume>(<issue>1</issue>), <fpage>2</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-017-06585-2</pub-id> [doi]</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Chang L."><surname>Chang</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-22">The code for facial identity in the primate brain</article-title>. <source hwp:id="source-21">Cell</source>, <volume>169</volume>(<issue>6</issue>), <fpage>1028</fpage>.<lpage>e14</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id></citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Cichy R. M."><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pantazis D."><surname>Pantazis</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Oliva A."><surname>Oliva</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-23">Similarity-Based Fusion of MEG and fMRI Reveals Spatio-Temporal Dynamics in Human Cortex During Visual Object Recognition</article-title>. <source hwp:id="source-22">Cerebral Cortex</source>, <volume>26</volume>(<issue>8</issue>), <fpage>3563</fpage>–<lpage>3579</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/cercor/bhw135" ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw135" hwp:id="ext-link-14">https://doi.org/10.1093/cercor/bhw135</ext-link></citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Collins J. A."><surname>Collins</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Olson I. R."><surname>Olson</surname>, <given-names>I. R.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-24">Knowledge is power: How conceptual knowledge transforms visual cognition</article-title>. <source hwp:id="source-23">Psychonomic Bulletin &amp; Review</source>, <volume>21</volume>(<issue>4</issue>), <fpage>843</fpage>–<lpage>860</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3758/s13423-013-0564-3" ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13423-013-0564-3" hwp:id="ext-link-15">https://doi.org/10.3758/s13423-013-0564-3</ext-link></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Conway B. R."><surname>Conway</surname>, <given-names>B. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moeller S."><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-25">Specialized color modules in macaque extrastriate cortex</article-title>. <source hwp:id="source-24">Neuron</source>, <volume>56</volume>(<issue>3</issue>), <fpage>560</fpage>–<lpage>573</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.008</pub-id></citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Cowen A. S."><surname>Cowen</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chun M. M."><surname>Chun</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kuhl B. A."><surname>Kuhl</surname>, <given-names>B. A.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-26">Neural portraits of perception: Reconstructing face images from evoked brain activity</article-title>. <source hwp:id="source-25">NeuroImage</source>, <volume>94</volume>(<issue>Supplement C</issue>), <fpage>12</fpage>–<lpage>22</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.018</pub-id></citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Craw I."><surname>Craw</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cameron P."><surname>Cameron</surname>, <given-names>P.</given-names></string-name> (<year>1991</year>). <chapter-title>Parameterising Images for Recognition and Reconstruction</chapter-title>. <source hwp:id="source-26">In BMVC91</source> (pp. <fpage>367</fpage>–<lpage>370</lpage>). <publisher-loc>London</publisher-loc>: <publisher-name>Springer London</publisher-name>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1007/978-1-4471-1921-0_52" ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-1-4471-1921-0_52" hwp:id="ext-link-16">https://doi.org/10.1007/978-1-4471-1921-0_52</ext-link></citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Gao X."><surname>Gao</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wilson H. R."><surname>Wilson</surname>, <given-names>H. R.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-27">The neural representation of face space dimensions</article-title>. <source hwp:id="source-27">Neuropsychologia</source>, <volume>51</volume>(<issue>10</issue>), <fpage>1787</fpage>–<lpage>1793</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/J.NEUROPSYCHOLOGIA.2013.07.001" ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.NEUROPSYCHOLOGIA.2013.07.001" hwp:id="ext-link-17">https://doi.org/10.1016/J.NEUROPSYCHOLOGIA.2013.07.001</ext-link></citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Gilaie-Dotan S."><surname>Gilaie-Dotan</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gelbard-Sagiv H."><surname>Gelbard-Sagiv</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Malach R."><surname>Malach</surname>, <given-names>R.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-28">Perceptual shape sensitivity to upright and inverted faces is reflected in neuronal adaptation</article-title>. <source hwp:id="source-28">Neuroimage</source>, <volume>50</volume>(<issue>2-3</issue>), <fpage>383</fpage>–<lpage>395</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.077</pub-id></citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Gold J. M."><surname>Gold</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mundy P. J."><surname>Mundy</surname>, <given-names>P. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tjan B. S."><surname>Tjan</surname>, <given-names>B. S.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-29">The Perception of a Face Is No More Than the Sum of Its Parts</article-title>. <source hwp:id="source-29">Psychological Science</source>, <volume>23</volume>(<issue>4</issue>), <fpage>427</fpage>–<lpage>434</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1177/0956797611427407" ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797611427407" hwp:id="ext-link-18">https://doi.org/10.1177/0956797611427407</ext-link></citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Guntupalli J. S."><surname>Guntupalli</surname>, <given-names>J. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wheeler K. G."><surname>Wheeler</surname>, <given-names>K. G.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gobbini M. I."><surname>Gobbini</surname>, <given-names>M. I.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-30">Disentangling the Representation of Identity from Head View Along the Human Face Processing Pathway</article-title>. <source hwp:id="source-30">Cerebral Cortex</source>, <volume>27</volume>(<issue>1</issue>), <fpage>46</fpage>–<lpage>53</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/cercor/bhw344" ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw344" hwp:id="ext-link-19">https://doi.org/10.1093/cercor/bhw344</ext-link>`</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><citation publication-type="other" citation-type="journal" ref:id="299933v1.32" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Hancock P. J. B"><surname>Hancock</surname> <given-names>P. J. B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname> <given-names>A. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bruce V."><surname>Bruce</surname> <given-names>V.</given-names></string-name> (<year>1996</year>) <article-title hwp:id="article-title-31">Face processing: human perception and principal components analysis, Memory and Cognition</article-title>, <volume>24</volume> (<issue>1</issue>), pp. <fpage>26</fpage>–<lpage>40</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Harris R. J."><surname>Harris</surname>, <given-names>R. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Young A. W."><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Andrews T. J."><surname>Andrews</surname>, <given-names>T. J.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-32">Brain regions involved in processing facial identity and expression are differentially selective for surface and edge information</article-title>. <source hwp:id="source-31">NeuroImage</source>, <volume>97</volume>, <fpage>217</fpage>–<lpage>223</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.032</pub-id></citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Haxby J.V."><surname>Haxby</surname>, <given-names>J.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoffman E.A."><surname>Hoffman</surname>, <given-names>E.A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gobbini M.I."><surname>Gobbini</surname>, <given-names>M.I.</given-names></string-name> (<year>2000</year>). <article-title hwp:id="article-title-33">The distributed human neural system for face perception</article-title>. <source hwp:id="source-32">Trends in Cognitive Sciences</source>, <volume>4</volume>(<issue>6</issue>): <fpage>223</fpage>–<lpage>233</lpage></citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Hole G. J."><surname>Hole</surname>, <given-names>G. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="George P. A."><surname>George</surname>, <given-names>P. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eaves K."><surname>Eaves</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rasek A."><surname>Rasek</surname>, <given-names>A.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-34">Effects of geometric distortions on face-recognition performance</article-title>. <source hwp:id="source-33">Perception</source>, <volume>31</volume>(<issue>10</issue>), <fpage>1221</fpage>–<lpage>1240</lpage>. doi:<pub-id pub-id-type="doi">10.1068/p3252</pub-id></citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Itier R. J."><surname>Itier</surname>, <given-names>R. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Taylor M. J."><surname>Taylor</surname>, <given-names>M. J.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-35">Inversion and contrast polarity reversal affect both encoding and recognition processes of unfamiliar faces: A repetition study using ERPs</article-title>. <source hwp:id="source-34">NeuroImage</source>, <volume>15</volume>(<issue>2</issue>), <fpage>353</fpage>–<lpage>372</lpage>. doi:<pub-id pub-id-type="doi">10.1006/nimg.2001.0982</pub-id></citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Itz M. L."><surname>Itz</surname>, <given-names>M. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Golle J."><surname>Golle</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Luttmann S."><surname>Luttmann</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schweinberger S. R."><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kaufmann J. M."><surname>Kaufmann</surname>, <given-names>J. M.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-36">Dominance of texture over shape in facial identity processing is modulated by individual abilities</article-title>. <source hwp:id="source-35">British Journal of Psychology</source>, <volume>108</volume>(<issue>2</issue>), <fpage>369</fpage>–<lpage>396</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1111/bjop.12199" ext-link-type="uri" xlink:href="https://doi.org/10.1111/bjop.12199" hwp:id="ext-link-20">https://doi.org/10.1111/bjop.12199</ext-link></citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Itz M. L."><surname>Itz</surname>, <given-names>M. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schweinberger S. R."><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kaufmann J. M."><surname>Kaufmann</surname>, <given-names>J. M.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-37">Effects of caricaturing in shape or color on familiarity decisions for familiar and unfamiliar faces</article-title>. <source hwp:id="source-36">PloS One</source>, <volume>11</volume>(<issue>2</issue>), <fpage>e0149796</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0149796</pub-id></citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1 xref-ref-39-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Itz M. L."><surname>Itz</surname>, <given-names>M. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schweinberger S. R."><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schulz C."><surname>Schulz</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kaufmann J. M."><surname>Kaufmann</surname>, <given-names>J. M.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-38">Neural correlates of facilitations in face learning by selective caricaturing of facial shape or reflectance</article-title>. <source hwp:id="source-37">NeuroImage</source>, <volume>102</volume>(<issue>Part 2</issue>), <fpage>736</fpage>–<lpage>747</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.08.042</pub-id></citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Jiang F."><surname>Jiang</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="O’Toole A. J."><surname>O’Toole</surname>, <given-names>A. J.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-39">Probing the visual representation of faces with adaptation</article-title>. <source hwp:id="source-38">Psychological Science</source>, <volume>17</volume>(<issue>6</issue>), <fpage>493</fpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01734.x</pub-id></citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Jiang F."><surname>Jiang</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="O’Toole A. J."><surname>O’Toole</surname>, <given-names>A. J.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-40">Three-Dimensional Information in Face Representations Revealed by Identity Aftereffects</article-title>. <source hwp:id="source-39">Psychological Science</source>, <volume>20</volume>(<issue>3</issue>), <fpage>318</fpage>–<lpage>325</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1111/j.1467-9280.2009.02285.x" ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2009.02285.x" hwp:id="ext-link-21">https://doi.org/10.1111/j.1467-9280.2009.02285.x</ext-link></citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Jiang F."><surname>Jiang</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rossion B."><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-41">Holistic processing of shape cues in face identification: Evidence from face inversion, composite faces, and acquired prosopagnosia</article-title>. <source hwp:id="source-40">Visual Cognition</source>, <volume>19</volume>(<issue>8</issue>), <fpage>1003</fpage>–<lpage>1034</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1080/13506285.2011.604360" ext-link-type="uri" xlink:href="https://doi.org/10.1080/13506285.2011.604360" hwp:id="ext-link-22">https://doi.org/10.1080/13506285.2011.604360</ext-link></citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Jiang F."><surname>Jiang</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dricot L."><surname>Dricot</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goebel R."><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rossion B."><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-42">Neural correlates of shape and surface reflectance information in individual faces</article-title>. <source hwp:id="source-41">Neuroscience</source>, <volume>163</volume>(<issue>4</issue>), <fpage>1078</fpage>–<lpage>1091</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroscience.2009.07.062</pub-id></citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Kaufmann J. M."><surname>Kaufmann</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schweinberger S. R."><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-43">Distortions in the brain? ERP effects of caricaturing familiar and unfamiliar faces</article-title>. <source hwp:id="source-42">Brain Research</source>, <volume>1228</volume>(Supplement C), <fpage>177</fpage>–<lpage>188</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.brainres.2008.06.092</pub-id></citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3"><citation publication-type="website" citation-type="web" ref:id="299933v1.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Kramer R. S. S."><surname>Kramer</surname>, <given-names>R. S. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jenkins R."><surname>Jenkins</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname>, <given-names>A. M.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-44">InterFace: A software package for face image warping, averaging, and principal components analysis</article-title>. <source hwp:id="source-43">Behavior Research Methods</source>, <fpage>1</fpage>–<lpage>10</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3758/s13428-016-0837-7" ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13428-016-0837-7" hwp:id="ext-link-23">https://doi.org/10.3758/s13428-016-0837-7</ext-link></citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mur M."><surname>Mur</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bandettini P. A."><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-45">Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source hwp:id="source-44">Frontiers in Systems Neuroscience</source>, <volume>2</volume>, <fpage>4</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.3389/neuro.06.004.2008" ext-link-type="uri" xlink:href="http://doi.org/10.3389/neuro.06.004.2008" hwp:id="ext-link-24">http://doi.org/10.3389/neuro.06.004.2008</ext-link></citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Ince R. A. A."><surname>Ince</surname>, <given-names>R. A. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jaworska K."><surname>Jaworska</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross J."><surname>Gross</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Panzeri S."><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Rijsbergen N. J."><surname>van Rijsbergen</surname>, <given-names>N. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rousselet G. A."><surname>Rousselet</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schyns P. G."><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-46">The Deceptively Simple N170 Reflects Network Information Processing Mechanisms Involving Visual Feature Coding and Transfer Across Hemispheres</article-title>. <source hwp:id="source-45">Cerebral Cortex</source>, <volume>26</volume>(<issue>11</issue>), <fpage>4123</fpage>–<lpage>4135</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/cercor/bhw196" ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw196" hwp:id="ext-link-25">https://doi.org/10.1093/cercor/bhw196</ext-link></citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Issa E. B."><surname>Issa</surname>, <given-names>E. B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="DiCarlo J. J."><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-47">Precedence of the eye region in neural processing of faces</article-title>. <source hwp:id="source-46">The Journal of Neuroscience □: The Official Journal of the Society for Neuroscience</source>, <volume>32</volume>(<issue>47</issue>), <fpage>16666</fpage>–<lpage>82</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.2391-12.2012" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2391-12.2012" hwp:id="ext-link-26">https://doi.org/10.1523/JNEUROSCI.2391-12.2012</ext-link></citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Lafer-Sousa R."><surname>Lafer-Sousa</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Conway B. R."><surname>Conway</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kanwisher N. G."><surname>Kanwisher</surname>, <given-names>N. G.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-48">Color-biased regions of the ventral visual pathway lie between face- and place-selective regions in humans, as in macaques</article-title>. <source hwp:id="source-47">The Journal of Neuroscience</source>, <volume>36</volume>(<issue>5</issue>), <fpage>1682</fpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1 xref-ref-50-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Lai M."><surname>Lai</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oruç I."><surname>Oruç</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barton J. J. S."><surname>Barton</surname>, <given-names>J. J. S.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-49">The role of skin texture and facial shape in representations of age and identity</article-title>. <source hwp:id="source-48">Cortex</source>, <volume>49</volume>(<issue>1</issue>), <fpage>252</fpage>–<lpage>265</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/J.CORTEX.2011.09.010" ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.CORTEX.2011.09.010" hwp:id="ext-link-27">https://doi.org/10.1016/J.CORTEX.2011.09.010</ext-link></citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Langner O."><surname>Langner</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dotsch R."><surname>Dotsch</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bijlstra G."><surname>Bijlstra</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wigboldus D. H. J."><surname>Wigboldus</surname>, <given-names>D. H. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawk S. T."><surname>Hawk</surname>, <given-names>S. T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="van Knippenberg A."><surname>van Knippenberg</surname>, <given-names>A.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-50">Presentation and validation of the Radboud Faces Database</article-title>. <source hwp:id="source-49">Cognition &amp; Emotion</source>, <volume>24</volume>(<issue>8</issue>), <fpage>1377</fpage>–<lpage>1388</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1080/02699930903485076" ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930903485076" hwp:id="ext-link-28">https://doi.org/10.1080/02699930903485076</ext-link></citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Leder H."><surname>Leder</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Carbon C.-C."><surname>Carbon</surname>, <given-names>C.-C.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-51">Face-specific configural processing of relational information</article-title>. <source hwp:id="source-50">British Journal of Psychology</source>, <volume>97</volume>(<issue>1</issue>), <fpage>19</fpage>–<lpage>29</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1348/000712605X54794" ext-link-type="uri" xlink:href="https://doi.org/10.1348/000712605X54794" hwp:id="ext-link-29">https://doi.org/10.1348/000712605X54794</ext-link></citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Lee H."><surname>Lee</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kuhl B. A."><surname>Kuhl</surname>, <given-names>B. A.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-52">Reconstructing perceived and retrieved faces from activity patterns in lateral parietal cortex</article-title>. <source hwp:id="source-51">The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>36</volume>(<issue>22</issue>), <fpage>6069</fpage>–<lpage>6082</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4286-15.2016</pub-id></citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Lewis M."><surname>Lewis</surname>, <given-names>M.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-53">Face-space-R: Towards a unified account of face recognition</article-title>. <source hwp:id="source-52">Visual Cognition</source>, <volume>11</volume>(<issue>1</issue>), <fpage>29</fpage>–<lpage>69</lpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Liu J."><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris A."><surname>Harris</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kanwisher N."><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-54">Perception of face parts and face configurations: An fMRI study</article-title>. <source hwp:id="source-53">Journal of Cognitive Neuroscience</source>, <volume>22</volume>(<issue>1</issue>), <fpage>203</fpage>–<lpage>211</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn.2009.21203</pub-id></citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="book" citation-type="book" ref:id="299933v1.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Livingstone M."><surname>Livingstone</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Hubel D."><surname>Hubel</surname>, <given-names>D.</given-names></string-name> (<year>1988</year>). <chapter-title>Segregation of form, color, movement, and depth: anatomy, physiology, and perception</chapter-title>. <source hwp:id="source-54">Science</source> (<publisher-loc>New York, N.Y</publisher-loc>.), <volume>240</volume>(<issue>4853</issue>), <fpage>740</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><citation publication-type="other" citation-type="journal" ref:id="299933v1.57" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Martinez A. M."><surname>Martinez</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Benavente R."><surname>Benavente</surname>, <given-names>R.</given-names></string-name> (<year>1998</year>). <article-title hwp:id="article-title-55">The AR face database</article-title>. (<issue>No. #24</issue>).</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1 xref-ref-58-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Maurer D."><surname>Maurer</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grand R. Le"><surname>Grand</surname>, <given-names>R. Le</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Mondloch C. J."><surname>Mondloch</surname>, <given-names>C. J.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-56">The many faces of configural processing</article-title>. <source hwp:id="source-55">Trends in Cognitive Sciences</source>, <volume>6</volume>(<issue>6</issue>), <fpage>255</fpage>–<lpage>260</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="McKone E."><surname>McKone</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Yovel G."><surname>Yovel</surname>, <given-names>G.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-57">Why does picture-plane inversion sometimes dissociate perception of features and spacing in faces, and sometimes not? Toward a new theory of holistic processing</article-title>. <source hwp:id="source-56">Psychonomic Bulletin &amp; Review</source>, <volume>16</volume>(<issue>5</issue>), <fpage>778</fpage>–<lpage>797</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3758/PBR.16.5.778" ext-link-type="uri" xlink:href="https://doi.org/10.3758/PBR.16.5.778" hwp:id="ext-link-30">https://doi.org/10.3758/PBR.16.5.778</ext-link></citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Minami T."><surname>Minami</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nakajima K."><surname>Nakajima</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Changvisommid L."><surname>Changvisommid</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nakauchi S."><surname>Nakauchi</surname>, <given-names>S.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-58">The effects of facial color and inversion on the N170 event-related potential (ERP) component</article-title>. <source hwp:id="source-57">Neuroscience</source>, <volume>311</volume>, <fpage>341</fpage>–<lpage>348</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuroscience.2015.10.019" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroscience.2015.10.019" hwp:id="ext-link-31">https://doi.org/10.1016/j.neuroscience.2015.10.019</ext-link></citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Miyawaki Y."><surname>Miyawaki</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uchida H."><surname>Uchida</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamashita O."><surname>Yamashita</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sato M."><surname>Sato</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morito Y."><surname>Morito</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tanabe H. C."><surname>Tanabe</surname>, <given-names>H. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sadato N."><surname>Sadato</surname>, <given-names>N.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kamitani Y."><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-59">Visual image reconstruction from human brain activity using a combination of multiscale local image decoders</article-title>. <source hwp:id="source-58">Neuron</source>, <volume>60</volume>(<issue>5</issue>), <fpage>915</fpage>–<lpage>929</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2008.11.004</pub-id></citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Murray R. F."><surname>Murray</surname>, <given-names>R. F.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-60">Classification images: A review</article-title>. <source hwp:id="source-59">Journal of Vision</source>, <volume>11</volume>(<issue>5</issue>), <fpage>1</fpage>–<lpage>25</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1167/11.5" ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.5" hwp:id="ext-link-32">https://doi.org/10.1167/11.5</ext-link>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prenger R. J."><surname>Prenger</surname>, <given-names>R. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kay K. N."><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oliver M."><surname>Oliver</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gallant J. L."><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-61">Bayesian reconstruction of natural images from human brain activity</article-title>. <source hwp:id="source-60">Neuron</source>, <volume>63</volume>(<issue>6</issue>), <fpage>902</fpage>–<lpage>915</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.006</pub-id></citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Nishimoto S."><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vu A. T."><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benjamini Y."><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yu B."><surname>Yu</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gallant J. L."><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-62">Reconstructing visual experiences from brain activity evoked by natural movies</article-title>. <source hwp:id="source-61">Current Biology</source>, <volume>21</volume>(<issue>19</issue>), <fpage>1641</fpage>–<lpage>6</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.cub.2011.08.031" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2011.08.031" hwp:id="ext-link-33">https://doi.org/10.1016/j.cub.2011.08.031</ext-link></citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Nemrodov D."><surname>Nemrodov</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niemeier M."><surname>Niemeier</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mok J. N. Y."><surname>Mok</surname>, <given-names>J. N. Y.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nestor A."><surname>Nestor</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-63">The time course of individual face recognition: A pattern analysis of ERP signals</article-title>. <source hwp:id="source-62">NeuroImage</source>, <volume>132</volume>, <fpage>469</fpage>–<lpage>476</lpage>. doi:<pub-id pub-id-type="doi">S1053-8119(16)00202-0</pub-id></citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1 xref-ref-66-2 xref-ref-66-3 xref-ref-66-4 xref-ref-66-5 xref-ref-66-6 xref-ref-66-7 xref-ref-66-8"><citation publication-type="website" citation-type="web" ref:id="299933v1.66" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Nemrodov D."><surname>Nemrodov</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niemeier M."><surname>Niemeier</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Patel A."><surname>Patel</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nestor A."><surname>Nestor</surname>, <given-names>A.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-64">The Neural Dynamics of Facial Identity Processing: insights from EEG-Based Pattern Analysis and Image Reconstruction</article-title>. <source hwp:id="source-63">Eneuro</source>, ENEURO.0358-17.2018. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/ENEURO.0358-17.2018" ext-link-type="uri" xlink:href="https://doi.org/10.1523/ENEURO.0358-17.2018" hwp:id="ext-link-34">https://doi.org/10.1523/ENEURO.0358-17.2018</ext-link></citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1 xref-ref-67-2 xref-ref-67-3 xref-ref-67-4 xref-ref-67-5 xref-ref-67-6 xref-ref-67-7 xref-ref-67-8 xref-ref-67-9 xref-ref-67-10 xref-ref-67-11"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Nestor A."><surname>Nestor</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Plaut D. C."><surname>Plaut</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Behrmann M."><surname>Behrmann</surname>, <given-names>M.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-65">Feature-based face representations and image reconstruction from behavioral and neural data</article-title>. <source hwp:id="source-64">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>113</volume>(<issue>2</issue>), <fpage>416</fpage>–<lpage>421</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1514551112</pub-id></citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1 xref-ref-68-2 xref-ref-68-3 xref-ref-68-4"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Nestor A."><surname>Nestor</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Plaut D. C."><surname>Plaut</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Behrmann M."><surname>Behrmann</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-66">Face-space architectures</article-title>. <source hwp:id="source-65">Psychological Science</source>, <volume>24</volume>(<issue>7</issue>), <fpage>1294</fpage>–<lpage>1300</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0956797612464889</pub-id></citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.69" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Nestor A."><surname>Nestor</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vettel J. M."><surname>Vettel</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tarr M. J."><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-67">Task-specific codes for face recognition: How they shape the neural representation of features for detection and individuation</article-title>. <source hwp:id="source-66">PLoS ONE</source>, <volume>3</volume>(<issue>12</issue>).</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Ölander K."><surname>Ölander</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Muukkonen I."><surname>Muukkonen</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Numminen J."><surname>Numminen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Salmela V."><surname>Salmela</surname>, <given-names>V.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-68">Representational similarity analysis of EEG and fMRI responses to face identities and emotional expressions</article-title>. <source hwp:id="source-67">Journal of Vision</source>, <volume>17</volume>(<issue>10</issue>), <fpage>271</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1167/17.10.271" ext-link-type="uri" xlink:href="https://doi.org/10.1167/17.10.271" hwp:id="ext-link-35">https://doi.org/10.1167/17.10.271</ext-link></citation></ref><ref id="c71" hwp:id="ref-71"><citation publication-type="book" citation-type="book" ref:id="299933v1.71" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="O’Toole A. J."><surname>O’Toole</surname>, <given-names>A. J.</given-names></string-name> (<year>2011</year>). <chapter-title>Cognitive and Computational Approaches to Face Perception</chapter-title>. In (Eds. <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Calder A."><given-names>A.</given-names> <surname>Calder</surname></string-name>, <string-name name-style="western" hwp:sortable="Rhodes G."><given-names>G.</given-names> <surname>Rhodes</surname></string-name>, <string-name name-style="western" hwp:sortable="Johnson M."><given-names>M.</given-names> <surname>Johnson</surname></string-name>, &amp; <string-name name-style="western" hwp:sortable="Haxby J. V."><given-names>J. V.</given-names> <surname>Haxby</surname></string-name></person-group>). <source hwp:id="source-68">Oxford Handbook of Face Perception</source>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>Oxford: UK</publisher-loc>.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1 xref-ref-72-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="O’Toole A. J."><surname>O’Toole</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vetter T."><surname>Vetter</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name> (<year>1999</year>). <article-title hwp:id="article-title-69">Three-dimensional shape and two-dimensional surface reflectance contributions to face recognition: An application of three-dimensional morphing</article-title>. <source hwp:id="source-69">Vision Research</source>, <volume>39</volume>(<issue>18</issue>), <fpage>3145</fpage>–<lpage>3155</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0042-6989(99)00034-6</pub-id></citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.73" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Paysan P."><surname>Paysan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knothe R."><surname>Knothe</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Amberg B."><surname>Amberg</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Romdhani S."><surname>Romdhani</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Vetter T."><surname>Vetter</surname>, <given-names>T.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-70">A 3D Face Model for Pose and Illumination Invariant Face Recognition</article-title>. <source hwp:id="source-70">In 2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</source> (pp. <fpage>296</fpage>–<lpage>301</lpage>). <publisher-name>IEEE</publisher-name>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1109/AVSS.2009.58" ext-link-type="uri" xlink:href="https://doi.org/10.1109/AVSS.2009.58" hwp:id="ext-link-36">https://doi.org/10.1109/AVSS.2009.58</ext-link></citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Piepers D. W."><surname>Piepers</surname>, <given-names>D. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Robbins R. A."><surname>Robbins</surname>, <given-names>R. A.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-71">A Review and Clarification of the Terms “holistic,” “configural,” and “relational” in the Face Perception Literature</article-title>. <source hwp:id="source-71">Frontiers in Psychology</source>, <volume>3</volume>, <fpage>559</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3389/fpsyg.2012.00559" ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00559" hwp:id="ext-link-37">https://doi.org/10.3389/fpsyg.2012.00559</ext-link></citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Rajimehr R."><surname>Rajimehr</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Young J. C."><surname>Young</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tootell R. B. H."><surname>Tootell</surname>, <given-names>R. B. H.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-72">An anterior temporal face patch in human cortex, predicted by macaque maps</article-title>. <source hwp:id="source-72">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>106</volume>(<issue>6</issue>), <fpage>1995</fpage>–<lpage>2000</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.0807304106" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0807304106" hwp:id="ext-link-38">https://doi.org/10.1073/pnas.0807304106</ext-link></citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Rakover S. S."><surname>Rakover</surname>, <given-names>S. S.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-73">Featural vs. configurational information in faces: A conceptual and empirical analysis</article-title>. <source hwp:id="source-73">British Journal of Psychology</source>, <volume>93</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>30</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1348/000712602162427" ext-link-type="uri" xlink:href="https://doi.org/10.1348/000712602162427" hwp:id="ext-link-39">https://doi.org/10.1348/000712602162427</ext-link></citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Richler J. J."><surname>Richler</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mack M. L."><surname>Mack</surname>, <given-names>M. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gauthier I."><surname>Gauthier</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Palmeri T. J."><surname>Palmeri</surname>, <given-names>T. J.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-74">Holistic processing of faces happens at a glance</article-title>. <source hwp:id="source-74">Vision Research</source>, <volume>49</volume>(<issue>23</issue>), <fpage>2856</fpage>–<lpage>2861</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/J.VISRES.2009.08.025" ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.VISRES.2009.08.025" hwp:id="ext-link-40">https://doi.org/10.1016/J.VISRES.2009.08.025</ext-link></citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.78" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Russell R."><surname>Russell</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Biederman I."><surname>Biederman</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nederhouser M."><surname>Nederhouser</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sinha P."><surname>Sinha</surname>, <given-names>P.</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-75">The utility of surface reflectance for the recognition of upright and inverted faces</article-title>, <volume>47</volume>(<issue>2</issue>), <fpage>157</fpage>–<lpage>165</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.visres.2006.11.002" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2006.11.002" hwp:id="ext-link-41">https://doi.org/10.1016/j.visres.2006.11.002</ext-link></citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1 xref-ref-79-2 xref-ref-79-3"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.79" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Russell R."><surname>Russell</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sinha P."><surname>Sinha</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Biederman I."><surname>Biederman</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nederhouser M."><surname>Nederhouser</surname>, <given-names>M.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-76">Is pigmentation important for face recognition? evidence from contrast negation</article-title>. <source hwp:id="source-75">Perception</source>, <volume>35</volume>(<issue>6</issue>), <fpage>749</fpage>–<lpage>759</lpage>. doi:<pub-id pub-id-type="doi">10.1068/p5490</pub-id> [doi]</citation></ref><ref id="c80" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1 xref-ref-80-2"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.80" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="Russell R."><surname>Russell</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sinha P."><surname>Sinha</surname>, <given-names>P.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-77">Real-world face recognition: The importance of surface reflectance properties</article-title>. <source hwp:id="source-76">Perception</source>, <volume>36</volume>(<issue>9</issue>), <fpage>1368</fpage>–<lpage>1374</lpage>. doi:<pub-id pub-id-type="doi">10.1068/p5779</pub-id></citation></ref><ref id="c81" hwp:id="ref-81" hwp:rev-id="xref-ref-81-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.81" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Shen G."><surname>Shen</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dwivedi K."><surname>Dwivedi</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Majima K."><surname>Majima</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Horikawa T."><surname>Horikawa</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kamitani Y."><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-78">End-to-end deep image reconstruction from human brain activity</article-title>. <source hwp:id="source-77">bioRxiv</source>, <fpage>272518</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/272518" ext-link-type="uri" xlink:href="https://doi.org/10.1101/272518" hwp:id="ext-link-42">https://doi.org/10.1101/272518</ext-link></citation></ref><ref id="c82" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1 xref-ref-82-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.82" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Tanaka J. W."><surname>Tanaka</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gordon I."><surname>Gordon</surname>, <given-names>I.</given-names></string-name> (<year>2011</year>). <source hwp:id="source-78">Features, Configuration, and Holistic Face Processing</source>. <publisher-name>Oxford University Press</publisher-name>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/oxfordhb/9780199559053.013.0010" ext-link-type="uri" xlink:href="https://doi.org/10.1093/oxfordhb/9780199559053.013.0010" hwp:id="ext-link-43">https://doi.org/10.1093/oxfordhb/9780199559053.013.0010</ext-link></citation></ref><ref id="c83" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1 xref-ref-83-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.83" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Taschereau-Dumouchel V."><surname>Taschereau-Dumouchel</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rossion B."><surname>Rossion</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schyns P. G."><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gosselin F."><surname>Gosselin</surname>, <given-names>F.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-79">Interattribute Distances do not Represent the Identity of Real World Faces</article-title>. <source hwp:id="source-79">Frontiers in Psychology</source>, <volume>1</volume>, <fpage>159</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3389/fpsyg.2010.00159" ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2010.00159" hwp:id="ext-link-44">https://doi.org/10.3389/fpsyg.2010.00159</ext-link></citation></ref><ref id="c84" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Thomaz C. E."><surname>Thomaz</surname>, <given-names>C. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Giraldi G. A."><surname>Giraldi</surname>, <given-names>G. A.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-80">A new ranking method for principal components analysis and its application to face image analysis</article-title>. <source hwp:id="source-80">Image and Vision Computing</source>, <volume>28</volume>(<issue>6</issue>), <fpage>902</fpage>–<lpage>913</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.imavis.2009.11.005" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.imavis.2009.11.005" hwp:id="ext-link-45">https://doi.org/10.1016/j.imavis.2009.11.005</ext-link></citation></ref><ref id="c85" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.85" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Tiddeman B."><surname>Tiddeman</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burt M."><surname>Burt</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Perrett D."><surname>Perrett</surname>, <given-names>D.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-81">Prototyping and transforming facial textures for perception research</article-title>. <source hwp:id="source-81">IEEE Computer Graphics and Applications</source>, <volume>21</volume>(<issue>4</issue>), <fpage>42</fpage>–<lpage>50</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1109/38.946630" ext-link-type="uri" xlink:href="https://doi.org/10.1109/38.946630" hwp:id="ext-link-46">https://doi.org/10.1109/38.946630</ext-link></citation></ref><ref id="c86" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.86" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Thirion B."><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duchesnay E."><surname>Duchesnay</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hubbard E."><surname>Hubbard</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dubois J."><surname>Dubois</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poline J."><surname>Poline</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lebihan D."><surname>Lebihan</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dehaene S."><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-82">Inverse retinotopy: Inferring the visual content of images from brain activation patterns</article-title>. <source hwp:id="source-82">NeuroImage</source>, <volume>33</volume>(<issue>4</issue>), <fpage>1104</fpage>–<lpage>1116</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.06.062</pub-id></citation></ref><ref id="c87" hwp:id="ref-87" hwp:rev-id="xref-ref-87-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.87" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-87"><string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schweers N."><surname>Schweers</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moeller S."><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-83">Patches of face-selective cortex in the macaque frontal lobe</article-title>. <source hwp:id="source-83">Nature Neuroscience</source>, <volume>11</volume>(<issue>8</issue>), <fpage>877</fpage>–<lpage>879</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/nn.2158" ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2158" hwp:id="ext-link-47">https://doi.org/10.1038/nn.2158</ext-link></citation></ref><ref id="c88" hwp:id="ref-88" hwp:rev-id="xref-ref-88-1"><citation publication-type="book" citation-type="book" ref:id="299933v1.88" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-88"><string-name name-style="western" hwp:sortable="Van Essen D. C."><surname>Van Essen</surname> <given-names>D. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Deyoe E.A."><surname>Deyoe</surname> <given-names>E.A.</given-names></string-name> (<year>1995</year>). <source hwp:id="source-84">The Cognitive Neurosciences</source>. <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Gazzaniga M. S."><given-names>M. S.</given-names> <surname>Gazzaniga</surname></string-name></person-group> (ed.). <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c89" hwp:id="ref-89" hwp:rev-id="xref-ref-89-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.89" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-89"><string-name name-style="western" hwp:sortable="Vetter T."><surname>Vetter</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Troje N."><surname>Troje</surname>, <given-names>N.</given-names></string-name> (<year>1995</year>). <source hwp:id="source-85">Separation of texture and two-dimensional shape in images of human faces</source> (pp. <fpage>118</fpage>–<lpage>125</lpage>). <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1007/978-3-642-79980-8_14" ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-642-79980-8_14" hwp:id="ext-link-48">https://doi.org/10.1007/978-3-642-79980-8_14</ext-link></citation></ref><ref id="c90" hwp:id="ref-90" hwp:rev-id="xref-ref-90-1"><citation publication-type="journal" citation-type="journal" ref:id="299933v1.90" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-90"><string-name name-style="western" hwp:sortable="Vinberg J."><surname>Vinberg</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Grill-Spector K."><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-84">Representation of shapes, edges, and surfaces across multiple cues in the human visual cortex</article-title>. <source hwp:id="source-86">Journal of Neurophysiology</source>, <volume>99</volume>(<issue>3</issue>), <fpage>1380</fpage>–<lpage>1393</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.01223.2007</pub-id></citation></ref><ref id="c91" hwp:id="ref-91" hwp:rev-id="xref-ref-91-1 xref-ref-91-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.91" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-91"><string-name name-style="western" hwp:sortable="Vuong Q. C."><surname>Vuong</surname>, <given-names>Q. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peissig J. J."><surname>Peissig</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harrison M. C."><surname>Harrison</surname>, <given-names>M. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Tarr M. J."><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-85">The role of surface pigmentation for recognition revealed by contrast reversal in faces and Greebles</article-title>. <source hwp:id="source-87">Vision Research</source>, <volume>45</volume>(<issue>10</issue>), <fpage>1213</fpage>–<lpage>1223</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.visres.2004.11.015" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2004.11.015" hwp:id="ext-link-49">https://doi.org/10.1016/j.visres.2004.11.015</ext-link></citation></ref><ref id="c92" hwp:id="ref-92" hwp:rev-id="xref-ref-92-1 xref-ref-92-2"><citation publication-type="website" citation-type="web" ref:id="299933v1.92" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-92"><string-name name-style="western" hwp:sortable="Zhan J."><surname>Zhan</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garrod O. B."><surname>Garrod</surname>, <given-names>O. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Rijsbergen N. J."><surname>van Rijsbergen</surname>, <given-names>N. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schyns P. G."><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-86">Efficient Information Contents Flow Down from Memory to Predict the Identity of Faces</article-title>. <source hwp:id="source-88">bioRxiv</source>, <fpage>125591</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/125591" ext-link-type="uri" xlink:href="https://doi.org/10.1101/125591" hwp:id="ext-link-50">https://doi.org/10.1101/125591</ext-link></citation></ref><ref id="c93" hwp:id="ref-93" hwp:rev-id="xref-ref-93-1"><citation publication-type="website" citation-type="web" ref:id="299933v1.93" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-93"><string-name name-style="western" hwp:sortable="Zhao W."><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chellappa R."><surname>Chellappa</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Phillips P. J."><surname>Phillips</surname>, <given-names>P. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rosenfeld A."><surname>Rosenfeld</surname>, <given-names>A.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-87">Face recognition</article-title>. <source hwp:id="source-89">ACM Computing Surveys</source>, <volume>35</volume>(<issue>4</issue>), <fpage>399</fpage>–<lpage>458</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1145/954339.954342" ext-link-type="uri" xlink:href="https://doi.org/10.1145/954339.954342" hwp:id="ext-link-51">https://doi.org/10.1145/954339.954342</ext-link></citation></ref></ref-list></back></article>
