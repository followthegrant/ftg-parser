<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/234724</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;234724v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;234724</article-id><article-id pub-id-type="other" hwp:sub-type="slug">234724</article-id><article-id pub-id-type="other" hwp:sub-type="tag">234724</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">The tortoise and the hare: interactions between reinforcement learning and working memory</article-title></title-group><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Collins Anne G.E."><surname>Collins</surname><given-names>Anne G.E.</given-names></name></contrib><aff id="a1" hwp:id="aff-1">
<institution hwp:id="institution-1">Department of Psychology, Helen Wills Neuroscience Institute</institution>, UC Berkeley.
</aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-12-15T08:45:25-08:00">
    <day>15</day><month>12</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-12-15T08:45:25-08:00">
    <day>15</day><month>12</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-12-15T08:50:52-08:00">
    <day>15</day><month>12</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-12-15T08:50:52-08:00">
    <day>15</day><month>12</month><year>2017</year>
  </pub-date><elocation-id>234724</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-12-14"><day>14</day><month>12</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2017-12-14"><day>14</day><month>12</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-12-15"><day>15</day><month>12</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></p></license></permissions><self-uri xlink:href="234724.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/234724v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="234724.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/234724v1/234724v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/234724v1/234724v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Learning to make rewarding choices in response to stimuli depends on a slow but steady process, reinforcement learning, and a fast and flexible, but capacity limited process, working memory. Using both systems in parallel, with their contributions weighted based on performance, should allow us to leverage the best of each system: rapid early learning, supplemented by long term robust acquisition. However, this assumes that using one process does not interfere with the other. We use computational modeling to investigate the interactions between the two processes in a behavioral experiment, and show that working memory interferes with reinforcement learning. Previous research showed that neural representations of reward prediction errors, a key marker of reinforcement learning, were blunted when working memory was used for learning. We thus predicted that arbitrating in favor of working memory to learn faster in simple problems would weaken the reinforcement learning process. We tested this by measuring performance in a delayed testing phase where the use of working memory was impossible, and thus subject choices depended on reinforcement learning. Counter-intuitively, but confirming our predictions, we observed that associations learned most easily were retained worse than associations learned slower: using working memory to learn quickly came at the cost of long-term retention. Computational modeling confirmed that this could only be accounted for by working memory interference in reinforcement learning computations. These results further our understanding of how multiple systems contribute in parallel to human learning, and may have important applications for education and computational psychiatry.</p></abstract><counts><page-count count="27"/></counts></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-2">Introduction</title><p hwp:id="p-3">When facing a challenge (such as responding to a natural disaster), we often need to pursue two solutions in parallel: emergency measures for the immediate future, and carefully thought-out long-term plans. These measures make different trade-offs between speed and efficiency, and neither is better than the other in absolute. Allocating finite resources to multiple strategies that involve such different trade-offs can mitigate their limitations and make use of their benefits. Recent work shows that human learning, even in very simple environments, follows this principle: it involves multiple parallel neurocognitive systems that arbitrate differently between immediate, effortful efficacy, and slower, long-term robustness (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Bornstein, Khaw, Shohamy, &amp; Daw, 2017</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Daw, Gershman, Seymour, Dayan, &amp; Dolan, 2011</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">R. Poldrack et al., 2001</xref>). In this article, we present a new experiment to computationally characterize how these systems work together to accomplish short- and long-term learning.</p><p hwp:id="p-4">We focus on two well-defined systems, reinforcement learning (RL) and working memory (WM). WM enables fast and accurate single-trial learning of any kind of information, with two limitations – a capacity or resource limit, and a temporal limit, such that we can only accurately remember a small amount of information for a short time, after which we may forget (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Baddeley, 2012</xref>). RL, in contrast, enables slower but robust reward-based learning of the value of choices (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Dayan &amp; Daw, 2008</xref>). Separable (yet partially overlapping) brain networks support these two systems: WM is centrally dependent on prefrontal cortex (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Cools, 2011</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Miller &amp; Cohen, 2001</xref>), while RL relies on dopaminergic signaling in the striatum (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Frank &amp; O’Reilly, 2006</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Montague, Dayan, &amp; Sejnowski, 1996</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Schultz, 2013</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Tai, Lee, Benavidez, Bonci, &amp; Wilbrecht, 2012</xref>). We previously developed a simple experimental protocol to show that both systems are used in parallel for instrumental learning (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Collins, Ciullo, Frank, &amp; Badre, 2017</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>). In this protocol, participants used deterministic feedback to learn the correct action to pick in response to a stimulus; critically, in different blocks, they needed to learn associations for a different number of stimuli in a new set. This <italic toggle="yes">set size</italic> manipulation allowed us to disentangle the contributions of WM to learning from those of RL, because WM is capacity-limited (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Cowan, 2010</xref>), while RL is not. In particular, this manipulation was essential in identifying which of the systems was causing learning dysfunction in clinical populations (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Collins, Albrecht, Waltz, Gold, &amp; Frank, 2017</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Collins, Brown, Gold, Waltz, &amp; Frank, 2014</xref>).</p><p hwp:id="p-5">However, how the two systems interact is still poorly understood. Previous models assumed that the RL and WM systems compete for choice, but otherwise learn independently of each other. In particular, if RL was fully independent from WM, values learned through RL should only depend on reward history, and be independent of set size. Recent evidence shows that this may not be the case, and that WM may instead interfere in RL computations. Indeed, in an fMRI study, we showed that RL signals were blunted in low set sizes, when working memory was sufficient to learn stimulus-response associations (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">Collins, Ciullo, et al., 2017</xref>). These results were confirmed in an EEG study (Collins &amp; Frank, submitted), where we additionally found that trial-by-trial markers of WM use predicted weaker RL learning signals. Last, we found behavioral evidence that the value of different items was better retained when learned under high load than under low load (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-3" hwp:rel-id="ref-6">Collins, Albrecht, Waltz, Gold, &amp; Frank, 2017</xref>), again hinting at an interference of WM with RL computations. Together, these results hint at a mechanism by which RL learning is weakened in low set sizes, when working memory is most successful. We hypothesize that successful WM use (in particular in low set sizes) may interfere with RL, either by inhibiting learning, or, more consistently with our most recent EEG findings, by communicating to the RL system its quickly acquired expectations and thus making positive reward prediction error signals less strong.</p><p hwp:id="p-6">In this project, we further investigate the nature of interactions between WM and RL during learning. Specifically, we propose an improvement to our previous protocol that allows us to 1) characterize short-term learning vs. long-term learning and retention, 2) investigate how WM use impacts both forms of learning, and 3) computationally characterize the complete, integrated, interactive learning process. In the new protocol (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1A</xref>), the learning phase includes multiple blocks of one of two set sizes – low (3) and high (6); after a short, irrelevant task to provide a delay, the experimental protocol ends with a surprise testing phase in extinction to assess the retention of the associations acquired during the learning phase. This testing phase probes how well participants remember the correct choice for all the stimuli they learned previously. We hypothesize that WM plays no direct role in testing phase choices (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1B</xref>) because, in the absence of feedback, no new information is available in the testing phase: participants decide based on experience acquired more than 10 minutes prior, for 54 different stimulus-action associations, both beyond the extent of working memory maintenance. Thus, the testing phase serves as a purer marker of RL function than could be obtained with the original design (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>), allowing us to observe how RL-learned associations depend on the learning context (high/low set size); and thus to investigate the interaction of WM and RL.</p><fig id="fig1" position="float" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;234724v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><title hwp:id="title-3"><bold>A) Experimental protocol</bold>. Left: the experiment includes a learning phase with 14 learning blocks of set size <italic toggle="yes">ns</italic>=<italic toggle="yes">3</italic> or <italic toggle="yes">6</italic>; followed by a short irrelevant task and a surprise testing phase. Right: examples of learning phase and testing phase trials. <bold>B) Model schematic.</bold> We assume that WM and RL both contribute competitively to choice during learning; the weight <italic toggle="yes">η</italic> of WM vs. RL for choice depends on the capacity and set size. WM stores exact information but may forget; RL learns from reward prediction errors (RPE) the value of selecting an action for a given stimulus (e.g. the triangle). We additionally hypothesize that WM influences RL computations (dashed arrow) by contributing expectations to the computation of the RPE. Testing phase does not involve WM contributions, but only RL. <bold>C) Model predictions (schematic):</bold> if <italic toggle="yes">η</italic>(<italic toggle="yes">ns</italic>=<italic toggle="yes">3</italic>)&lt;<italic toggle="yes">η</italic>(<italic toggle="yes">ns</italic>=<italic toggle="yes">6</italic>), the model predicts worse performance under high load during learning; however items learned under high load are predicted to be better retained during testing, because WM interferes less with RL.</title></caption><graphic xlink:href="234724_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-7">Based on our previous imaging results showing blunted RL signals in low set sizes, we made the counter-intuitive prediction that retention of associations learned under low set sizes would be worse than in high set sizes (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1C</xref>). Furthermore, we predicted that a model incorporating interference of WM in RL computations (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1B</xref>, methods) would capture behavior better than models assuming either single systems or independent WM and RL modules competing for choice.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Methods</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Subjects</title><p hwp:id="p-8">We first tested 49 University of California, Berkeley undergraduate students (31 female; ages 18-40, mean 20.5). To replicate the results obtained with this first sample, we then tested a second, independent sample, which included 42 University of California, Berkeley undergraduate students (26 female; ages 18-29, mean 20.7). Because the effects in the second sample fully replicated the results obtained with the first sample, we report here the results with participants pooled together. We excluded 6 participants based on poor overall performance in the task indicating lack of involvement (less than 75% average accuracy at asymptotic performance). The final sample included N = 85 participants. All participants provided written informed consent, and the Committee for the Protection of Human Subjects at the University of California, Berkeley, approved the research.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Experimental protocol</title><sec id="s2b1" hwp:id="sec-5"><title hwp:id="title-7">General</title><p hwp:id="p-9">The methods are modified from previous published versions of this experimental protocol (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-4" hwp:rel-id="ref-6">Collins, Albrecht, et al., 2017</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Collins et al., 2014</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-5" hwp:rel-id="ref-6">Collins, Ciullo, et al., 2017</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>, submitted). Subjects performed a learning experiment in which they used reinforcement feedback (“+1” or “0”) to figure out the correct key to press in response to several visual stimuli (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1A</xref>). The experiment was separated into three phases: a learning phase comprising multiple independent learning blocks (mean duration 26 min, range [22-34]); an unrelated memory task (mean duration 11 min, range [10-16]; and a surprise testing phase assessing what was retained from the learning phase (mean duration 7 min, range [6-8]).</p></sec><sec id="s2b2" hwp:id="sec-6"><title hwp:id="title-8">Learning phase</title><p hwp:id="p-10">The learning phase was separated into 14 blocks, with a new set of visual stimuli of set size <italic toggle="yes">ns</italic> in each block, with <italic toggle="yes">ns</italic>=<italic toggle="yes">3</italic> or <italic toggle="yes">ns</italic>=<italic toggle="yes">6</italic>. Each block included 12-14 presentations of each visual stimuli in a pseudo-randomly interleaved manner (controlling for a uniform distribution of delay between two successive presentations of the same stimulus within [<italic toggle="yes">1:2*ns</italic>] trials, and the number of presentations of each stimulus), for a total of <italic toggle="yes">ns</italic>*<italic toggle="yes">13</italic> trials. At each trial, a stimulus was presented centrally on a black background. Subjects had up to 1.5 seconds to answer by pressing one of three keys with their right hand. Key press was followed by visual feedback presentation for 0.5 seconds, followed by a fixation of 0.5 seconds before the onset of the next trial. For each image, the correct key press was the same for the entire block. Pressing this key led to a truthful “+1” feedback, while pressing any other key led to a truthful “0” feedback. Failure to answer within 1.5 seconds was indicated by a “no valid answer” message. Stimuli in a given block were all from a single category (e.g. colors, fruits, animals), and did not repeat across blocks.</p><p hwp:id="p-11">We varied the set size <italic toggle="yes">ns</italic> across blocks: out of 14 blocks, 8 had set size <italic toggle="yes">ns</italic>=<italic toggle="yes">3</italic> and 6 set size <italic toggle="yes">ns</italic>=<italic toggle="yes">6</italic>. The first and last block of the learning phase were of set size <italic toggle="yes">ns</italic>=<italic toggle="yes">3</italic>. We have shown in previous research that varying set size provides a way to investigate the contributions of capacity- and resource-limited working memory to reinforcement learning.</p></sec><sec id="s2b3" hwp:id="sec-7"><title hwp:id="title-9">N-back</title><p hwp:id="p-12">Following the learning phase, participants performed a classic visual N-back task (N = 2-5). The purpose of this phase was to provide a time buffer between the learning phase and the surprise testing phase. We do not analyze the N-back performance here.</p></sec><sec id="s2b4" hwp:id="sec-8"><title hwp:id="title-10">Testing phase</title><p hwp:id="p-13">At the end of the N-back task, participants were informed that they would be tested on what they had learned during the first phase of the experiment. The testing phase included a single block where all stimuli from the learning phase (excepting block 1 and block N to limit recency and primacy effects – thus a total of 54 stimuli: 36 learned in set size 6, and 18 learned in set size 3 blocks) were presented 4 times each, for a total of 216 trials. The order was pseudo-randomized to ensure that each stimulus was presented once in each quarter of the test phase. Each trial was identical to the learning phase, except that no feedback was presented so that participants could not learn during this phase.</p></sec></sec><sec id="s2c" hwp:id="sec-9"><title hwp:id="title-11">Model free analysis</title><sec id="s2c1" hwp:id="sec-10"><title hwp:id="title-12">Learning phase</title><p hwp:id="p-14">Learning curves were constructed by computing the proportion of correct answers to all stimuli of each set size as a function of their iteration number. Trials with missing answers were excluded. Reaction time learning curves were limited to correct choices. Asymptotic performance/reaction times were assessed over the last 6 presentations of each stimulus.</p><p hwp:id="p-15">We also used a multiple logistic/linear regression analysis to predict correct choices/reaction times in the learning phase. The main predictor was the “reward” predictor (<italic toggle="yes">#R</italic>), which indicated the number of previous correct choices for a given stimulus and was expected to capture the effect of reward history on learning (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-5" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>). Predictors expected to capture working memory contributions were 1) the set size <italic toggle="yes">ns</italic> of the block in which a stimulus was learned and 2) the delay, or number of trials since the subject last selected the correct action for the current stimulus (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1A</xref>). Last, we also included the block number to investigate whether exposure to the learning phase modified performance. All regressors were z-scored prior to entering into the regression model. For visualization purposes in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">figures 2</xref> and <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-3-1" hwp:rel-id="F3">5</xref>, weights are sigmoid transformed and scaled to [-1,1] (keeping 0 as the no effect value). Specifically, the transformation is <italic toggle="yes">β</italic><bold>←</bold><italic toggle="yes">(2/(1+exp(-β)))-1</italic>.</p><fig id="fig2" position="float" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;234724v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><title hwp:id="title-13">Behavioral results show opposite effect of load in learning and retention.</title><p hwp:id="p-16"><bold>A-top)</bold> Proportion of correct trials as a function of stimulus iteration number, for set sizes <italic toggle="yes">ns = 3/6.</italic> <bold>A-bottom)</bold> Same for reaction times (RT). <bold>B-top)</bold> Proportion of correct trials at asymptotic learning performance (last 5 iterations of a stimulus - black line) and during test phase – grey line). <bold>B-bottom)</bold> RT’s for testing and asymptotic learning. <bold>C)</bold> Difference between low and high set sizes shows significant, opposite results for learning and test phase, in both performance (top) and RT’s (bottom). <bold>D-top)</bold> Transformed logistic regression weights from learning phase show expected effects of learning block and reward history (due to practice and reinforcement learning), as well as expected negative effects of set size and delay, characterizing working memory contributions. <bold>D-bottom)</bold> Transformed logistic regression weights from the test phase also show expected effects of learning block and reward history, but show better test phase performance with higher set size, contrary to learning phase. Open circles represent individual subjects from original experiment; stars represent individual subjects from the replication sample. The results are identical. Error bars indicate standard error of the mean.</p></caption><graphic xlink:href="234724_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><fig id="fig5" position="float" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;234724v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-3"><title hwp:id="title-14">RLWMi validation.</title><p hwp:id="p-17">Model simulations with fit parameters. <bold>A):</bold> Model simulations reproduce learning curves. <bold>B-C):</bold> Model simulations reproduce the opposite effects of set size <italic toggle="yes">ns</italic> between learning and testing phases, for both choices and reaction times. <bold>D-E):</bold> Logistic regression analysis of the learning and testing phase choices capture behavioral effects (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2</xref>), including the opposite effects of set size on performance.</p></caption><graphic xlink:href="234724_fig5" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec><sec id="s2c2" hwp:id="sec-11"><title hwp:id="title-15">Testing phase</title><p hwp:id="p-18">We computed the performance as the proportion of correct choices separately for stimuli that had been learned in blocks of set size <italic toggle="yes">ns</italic> = <italic toggle="yes">3</italic> or <italic toggle="yes">ns</italic> = <italic toggle="yes">6</italic>. We also used multiple logistic/linear regression analysis for the testing phase, with reward, set size, and block regressors. The reward regressor was the asymptotic performance for a given stimulus; and the set size and block regressors were defined as the set size and block number of the block during which a given stimulus had been learned in the learning phase.</p></sec><sec id="s2c3" hwp:id="sec-12"><title hwp:id="title-16">Errors</title><p hwp:id="p-19">To investigate the types of errors made during the testing phase we identified a set of stimuli that met the following criteria: 1) the participant had made at least one mistake for that stimulus during the learning phase, and 2) one of the two incorrect actions had been selected more often than the other during the learning phase. This allowed us to define for the testing phase a perseverative error as choosing the action that had been selected most, and an optimal error as choosing the erroneous action that had been selected least (this would correspond to best avoidance of previously unrewarding actions). We computed the proportion of perseverative vs. optimal errors separately for each set size.</p></sec><sec id="s2c4" hwp:id="sec-13"><title hwp:id="title-17">Computational modeling</title><p hwp:id="p-20">We simultaneously modeled the learning and testing phases based on our theory, as shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1C</xref>. The models assume that dynamically changing learned associations control learning phase policy, and that policies acquired at the end of the learning phase are used during the testing phase.</p></sec><sec id="s2c5" hwp:id="sec-14"><title hwp:id="title-18">Models</title><p hwp:id="p-21">We tested three families of models:
<list list-type="simple" hwp:id="list-1"><list-item hwp:id="list-item-1"><label>-</label><p hwp:id="p-22"><bold><italic toggle="yes">RLs:</italic></bold> Pure reinforcement learning models</p></list-item><list-item hwp:id="list-item-2"><label>-</label><p hwp:id="p-23"><bold><italic toggle="yes">RLWM:</italic></bold> mixture models with <italic toggle="yes">independent</italic> WM and RL; where both WM and RL contributed to learning, but only RL contributed to testing</p></list-item><list-item hwp:id="list-item-3"><label>-</label><p hwp:id="p-24"><bold><italic toggle="yes">RLWMi:</italic></bold> RLWM mixture models with <italic toggle="yes">interacting</italic> WM and RL; where both WM and RL contributed to learning, but only RL contributed to testing</p></list-item></list>
</p></sec></sec><sec id="s2d" hwp:id="sec-15"><title hwp:id="title-19"><italic toggle="yes">RL models</italic></title><p hwp:id="p-25">Computational models in the class of RL models are built off a classic formulation of RL models (see below), with two key parameters – learning rate <italic toggle="yes">α</italic> and softmax temperature <italic toggle="yes">β</italic>. We also investigate RL models that include various combinations of additional mechanisms that may help provide a better account of behavior, while remaining in the family of single process RL models. These additional mechanisms include undirected noise, forgetting, perseveration, initial bias, and dependence of learning rate on task conditions. We describe all mechanisms below.</p><sec id="s2d1" hwp:id="sec-16"><title hwp:id="title-20">Classic RL.</title><p hwp:id="p-26">For each stimulus <italic toggle="yes">s</italic>, and action <italic toggle="yes">a</italic>, the expected reward <italic toggle="yes">Q</italic>(<italic toggle="yes">s,a</italic>) is learned as a function of reinforcement history. Specifically, the <italic toggle="yes">Q</italic> value for the selected action given the stimulus is updated upon observing each trial’s reward outcome r<sub>t</sub> (1 for correct, 0 for incorrect) as a function of the prediction error between expected and observed reward at trial <italic toggle="yes">t</italic>:
<disp-formula id="ueqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="234724_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
where <italic toggle="yes">δ<sub>t</sub></italic>= <italic toggle="yes">r<sub>t</sub></italic> − <italic toggle="yes">Q<sub>t</sub></italic>(<italic toggle="yes">s,a</italic>) is the prediction error, and <italic toggle="yes">α</italic> is the learning rate. Choices are generated probabilistically with greater likelihood of selecting actions that have higher <italic toggle="yes">Q</italic>-values, using the softmax choice policy, which defines the probabilistic rule for choosing actions in response to a stimulus:
<disp-formula id="ueqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="234724_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula></p><p hwp:id="p-27">Here, <italic toggle="yes">β</italic> is an inverse temperature determining the degree with which differences in <italic toggle="yes">Q</italic>-values are translated into more deterministic choice, and the sum is over the three possible actions a<sub>i</sub>. Choice policy during the testing phase is identical to the policy at the end of the learning phase.</p></sec><sec id="s2d2" hwp:id="sec-17"><title hwp:id="title-21">Undirected noise.</title><p hwp:id="p-28">The softmax allows for stochasticity in choice, but stochasticity is more impactful when the value of each action is close to the values of the alternative actions. We also allow for “slips” of action (i.e., even when Q-value differences are large; also called “irreducible noise” or lapse rate). Given a model’s policy π = p(a|s), adding undirected noise consists in defining the new mixture choice policy:
<disp-formula id="ueqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="234724_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives>
</disp-formula>
where U is the uniform random policy (U(a) = 1/n<sub>A</sub>, with number of actions n<sub>A</sub>=3), and the parameter 0 &lt; ε &lt; 1 controls the amount of noise (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Collins &amp; Frank, 2013</xref>; <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Collins &amp; Koechlin, 2012</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Guitart-Masip et al., 2012</xref>). Intuitively, this undirected noise captures a choice policy where with probability <italic toggle="yes">1 − ε</italic> the agent picks choices normally, but with probability <italic toggle="yes">ε</italic>, the agent lapses and acts randomly. (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Nassar &amp; Frank, 2016</xref>) showed that failing to take this irreducible noise into account can make model fits be unduly influenced by rare odd data points (e.g. that might arise from attentional lapses), and that this problem is remedied by using the hybrid softmax-ε-greedy choice function as used here.</p></sec><sec id="s2d3" hwp:id="sec-18"><title hwp:id="title-22">Forgetting.</title><p hwp:id="p-29">We allow for potential decay or forgetting in Q-values on each trial, additionally updating all Q-values at each trial, according to:
<disp-formula id="ueqn4" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="234724_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
where 0 &lt; ϕ &lt; 1 is a decay parameter which at each trial pulls the estimates of values towards the initial value Q<sub>0</sub> = 1/n<sub>A</sub>. This parameter allows us to capture forgetting during learning.</p></sec><sec id="s2d4" hwp:id="sec-19"><title hwp:id="title-23">Perseveration.</title><p hwp:id="p-30">To allow for potential neglect of negative, as opposed to positive feedback, we estimate a perseveration parameter <italic toggle="yes">pers</italic> such that for negative prediction errors (<italic toggle="yes">δ</italic> &lt; 0), the learning rate <italic toggle="yes">α</italic> is reduced by <italic toggle="yes">α</italic> = <italic toggle="yes">(1-pers)α</italic>. Thus, values of <italic toggle="yes">pers</italic> near 1 indicate perseveration with complete neglect of negative feedback, whereas values near 0 indicate equal learning from negative and positive feedback.</p></sec><sec id="s2d5" hwp:id="sec-20"><title hwp:id="title-24">Initial bias.</title><p hwp:id="p-31">Although we initialize the Q-values to a fixed value for all stimuli and actions, participants may come in with a preference for pressing action 1 with their index finger, for example (or other biases). To account for subjective biases in action choices and allow for better estimation of other parameters, we use the first choice made by a participant for each stimulus as a potential marker of this bias, and introduce an initial bias parameter <italic toggle="yes">init</italic> that boosts the initial value of this first, prior to the first learning update. Specifically, the initial bias update is Q<sub>0</sub>(s,a<sub>first</sub>(s)) = 1/n<sub>A</sub>+<italic toggle="yes">init*</italic>(1-1/n<sub>A</sub>).</p></sec><sec id="s2d6" hwp:id="sec-21"><title hwp:id="title-25">Additional mechanisms.</title><p hwp:id="p-32">We test two additional mechanisms to attempt to provide better fit within the “RL only” family of models. The first mechanism assumes that learning rates may be different for each set size. The second mechanism assumes that two sets of RL values are learned in parallel and independently, with independent learning rates: one process controls learning phase policy; the other controls testing phase policy only. The best-fitting model in the “RL only” family of models includes all mechanisms described above, for a total of 9 parameters (softmax <italic toggle="yes">β</italic>, 4 learning rates [<italic toggle="yes">α</italic><sub>learn</sub>(3), <italic toggle="yes">α</italic><sub>learn</sub>(6), <italic toggle="yes">α</italic><sub>test</sub>(3), <italic toggle="yes">α</italic><sub>test</sub>(6)], decay, <italic toggle="yes">pers</italic>, undirected noise <italic toggle="yes">ε</italic>, initial bias). This best model (<bold><italic toggle="yes">RLs</italic></bold>), in addition to the simplest RL model (for baseline), are simulated in <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 3</xref> for predictions. The RL model predicts no effect of phase or set size on performance (top row of <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 3</xref>: the two model learning curves are overlapping); the RLs model’s predictions are dependent on the values of the 4 learning rate parameters, but the model is flexible enough to capture opposite effects of set size in learning and test phases (second row of <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 3</xref>).</p><fig id="fig3" position="float" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7 xref-fig-4-8 xref-fig-4-9 xref-fig-4-10 xref-fig-4-11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;234724v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-4"><title hwp:id="title-26">Model Predictions.</title><p hwp:id="p-33">Model simulations with fit parameters. <bold>Left)</bold> Proportion of correct trials as a function of stimulus iteration number, for set sizes <italic toggle="yes">ns = 3/6</italic>. <bold>Right)</bold> Proportion of correct trials at asymptotic learning performance (last 5 iterations of a stimulus – black line) and during test phase – grey line). Simulations were run with best fit parameters, 100 times per subject. Models are RL (classic two-parameter RL), RLs (best model in the RL family, including two independent Q-tables for learning and testing, and 2 learning rates per set size per learning process), RLWM (with independent working memory and reinforcement learning modules), and RWMi (with interacting modules). Dotted lines show participants’ behavior; full lines model simulations. Note that the simulated learning curves for the RL model overlap, showing no effect of set-size.</p></caption><graphic xlink:href="234724_fig3" position="float" orientation="portrait" hwp:id="graphic-8"/></fig></sec></sec><sec id="s2e" hwp:id="sec-22"><title hwp:id="title-27"><italic toggle="yes">RL+WM independent models</italic></title><p hwp:id="p-34">Models in the RLWM family include separate RL and WM modules, and assume that learning phase choice is a mixture of the RL and WM policy, while testing phase choice follows RL policy.</p><p hwp:id="p-35">The RL module of the RLWM models is a classic RL model as described above, characterized by parameters α and β, with the additional mechanisms of initial bias and perseveration.</p><p hwp:id="p-36">The WM module stores information about weights between stimuli and actions, W(s,a), which are initialized similarly to RL Q-values. To model fast storage of information, we assume perfect retention of the previous trial’s information, such that W<sub>t+1</sub>(s<sub>t</sub>,a<sub>t</sub>) = r<sub>t</sub>. To model delay-sensitive aspects of working memory (where active maintenance is increasingly likely to fail with intervening time and other stimuli), we assume that WM weights decay at each trial according to W<sub>t+1</sub><bold>←</bold> W<sub>t</sub> + ϕ (W<sub>0</sub>−W<sub>t</sub>). The WM policy uses the W(s,a) weights in a softmax with added undirected noise, using the same noise parameters as the RL module.</p><p hwp:id="p-37">Contrary to previous published versions of this model (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">Collins et al., 2014</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-6" hwp:rel-id="ref-6">Collins, Ciullo, et al., 2017</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-6" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>), we cannot estimate WM capacity directly in this protocol, because we only sample set sizes 3 and 6. Thus, we model the limitations of WM involvement in choice with a fixed set-size-dependent mixture parameter <italic toggle="yes">η</italic><sub>ns</sub> for the overall choice policy:
<disp-formula id="ueqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="234724_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula></p><p hwp:id="p-38">For the testing phase, we assume that P<sub>test</sub>(a|s) = P<sub>RL</sub>(a|s).</p><p hwp:id="p-39">The best model in this family thus includes 8 parameters (softmax <italic toggle="yes">β</italic>, RL learning rate <italic toggle="yes">α</italic>, WM decay <italic toggle="yes">ϕ, pers</italic>, undirected noise <italic toggle="yes">ε</italic>, initial bias, two mixture parameters <italic toggle="yes">η</italic><sub>3</sub> and <italic toggle="yes">η</italic><sub>6</sub>). This model predicts identical effects of set size in the learning and testing phases, with worse performance in lower set sizes (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 3</xref>, third row).</p></sec><sec id="s2f" hwp:id="sec-23"><title hwp:id="title-28"><italic toggle="yes">RL+WM interacting models</italic></title><p hwp:id="p-40">This family of models is identical to the previous one, with the exception that the WM module influences the RL computations. The RL module update is still assumed to follow <italic toggle="yes">Q<sub>t+1</sub>(s,a)</italic> = <italic toggle="yes">Q<sub>t</sub>(s,a) + α x δ<sub>t</sub></italic>,. However, WM contributes cooperatively to the computation of the reward prediction error <italic toggle="yes">δ<sub>t</sub></italic> by contributing to the expectation in proportion to WM’s involvement in choice:
<disp-formula id="ueqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="234724_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula></p><p hwp:id="p-41">The best-fitting model in this family, <italic toggle="yes">RLWMi</italic>, has the same parameters as the no-interaction RL+WM models. This model predicts opposite effects of set size in the learning and test phases, with worse performance in lower set sizes (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig. 3</xref>, third row). We tested a variant (<italic toggle="yes">RLWMii</italic>) that assumes independent <italic toggle="yes">η</italic><sub>ns</sub> parameters for policy mixture and interaction mechanisms; it provided a significantly worse fit (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 4</xref>). We also explored a competitive interaction model, whereby WM inhibited RL computations by decreasing learning rate. This model fit equally well as RLWMi, but cannot account for previous EEG findings (Collins &amp; Frank, submitted; see discussion), thus we only report here the cooperative interaction.</p><fig id="fig4" position="float" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;234724v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-5"><title hwp:id="title-29">Model fitting.</title><p hwp:id="p-42"><bold>Left:</bold> Difference in AIC between different models and best model RLWMi. The RLs model is a flexible model including only RL mechanisms; the RLWM model includes an RL module and a working memory module that are independent, but compete for choice. RLWMi additionally includes an interaction, whereby WM contributes to RL computations. RLWMii allows separate weighting for WM’s contribution to choice and to RL computations. Error bars indicate standard error of the mean. <bold>Right:</bold> Mixture weights η indicate contribution of WM to choice and RL computations in both set sizes for all subjects (initial experiment: open circles; replication: stars). As expected under the hypothesis that the WM module represents working memory, we observe consistently lower contribution of WM in high than in low set sizes (<italic toggle="yes">η</italic><sub>6</sub> &lt; <italic toggle="yes">η</italic><sub>3</sub>).</p></caption><graphic xlink:href="234724_fig4" position="float" orientation="portrait" hwp:id="graphic-11"/></fig></sec><sec id="s2g" hwp:id="sec-24"><title hwp:id="title-30">Model fitting and validation</title><p hwp:id="p-43">We used the Matlab constrained optimization function fmincon to fit parameters (the Mathworks Inc., Natick, Massachusetts, USA). This was iterated with 20 randomly chosen starting points, to increase the likelihood of finding a global rather than local optimum. All parameters were fit with constraints [0 1], except the softmax parameter β, which was constrained to [0 100].</p><p hwp:id="p-44">We used the Akaike Information Criterion to penalize model complexity (AIC; <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Akaike, 1974</xref>). Results were identical when using Bayesian Information Criterion (BIC; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Schwarz 1978</xref>), which was used to compute exceedance probability. Comparing the RL-only models, independent RLWM, interacting RLWMi, and double interacting RLWMii, the simple interacting RLWMi model was strongly favored with exceedance probability of <italic toggle="yes">1-5e<sup>−6</sup></italic> over the whole group (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Stephan, Penny, Daunizeau, Moran, &amp; Friston, 2009</xref>).</p><p hwp:id="p-45">Model selection alone is insufficient to assess whether the best fitting model sufficiently captures the data. To test whether models capture the key features of the behavior (e.g., learning curves), we simulated each model with fit parameters for each subject, with 100 repetitions per subject then averaged to represent this subject’s contribution (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig. 3</xref>).</p></sec></sec><sec id="s3" hwp:id="sec-25"><title hwp:id="title-31">Results</title><p hwp:id="p-46">Results from the learning phase replicated previous experiments (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-7" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>)(<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2A</xref>, <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">2D</xref> top). Specifically, we found that participants learned to select correct actions in both set sizes (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig. 2A</xref> top), but were slower to learn in blocks with a set size of <italic toggle="yes">ns</italic> = <italic toggle="yes">6</italic>. This was characterized in a logistic regression analysis which identified two main contributions to learning. First, reinforcement learning was characterized by a positive sensitivity to reward history (number of previous correct trials; sign test p &lt; 10e-4): the more previous correct choices they had, the more likely participants were to pick the correct choice on the current trial. Second, working memory was characterized by a negative effect of set size and delay on performance since the last correct iteration of the current stimulus (both p’s &lt; 10e-4): participants were less likely to pick the correct actions under higher load, or if there had been more intervening trials since they last chose correctly. Furthermore, we identified a practice effect, such that performance was higher in later blocks (sign test, p = 0.01). These results were also paralleled in reaction times (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig. 2A</xref> bottom), such that factors facilitating correct choice (reward history and block) also lead to faster choices (p &lt; 10e-4 and p = 0.002, respectively), and factors making choice harder lead to slower choices (set size and delay; both p’s &lt; 10e-4). At its asymptote, performance was lower in high than low set sizes (t(84) = 6.8, p &lt; 0.0001; 66/85 participants: p &lt; 0.0001). This effect was coupled with higher reaction times (t(84) = 26.9, p &lt;0.0001; 85/85 participants: p &lt; 0.0001).</p><p hwp:id="p-47">Model simulations showed that a classic reinforcement learning model could not account for the learning phase effects (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Fig. 3</xref> RL), and that while more complex single process reinforcement learning models can capture a qualitative effect of set size with set size dependent learning rates, they do not quantitatively capture the learning phase dynamics (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-8" hwp:rel-id="F4">Fig. 3</xref> RLs). In contrast, we replicate our previous finding that RLWM models can capture learning phase dynamics well (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-9" hwp:rel-id="F4">Fig. 3</xref> RLWM, RLWMi).</p><p hwp:id="p-48">We next analyzed testing phase performance as a function of the set size in which an item had been learned. As expected from our hypothesis that learning relies on RL and WM, but testing on RL only, we found overall worse performance (t(84)=10.5, p&lt;10e-4) in the testing phase compared to asymptotic performance in the learning phase, as well as faster reaction times (t(84)=21.1, p&lt;10e-4). Furthermore, our theory assumes that learning in high set size blocks relies more on RL than in low set size blocks, and that testing phase relies only on RL. Thus, we predicted that individual differences in testing phase behavior should be better predicted by individual differences in asymptotic learning phase performance in high than low set size blocks, and that this should be true for stimuli learned in either set size. Unsurprisingly, a multiple regression analysis with both asymptotic set size 3 and 6 learning phase performance showed that set size 6, but not set size 3, was significantly predictive of set size 6 test performance (t(84) = 5.06, p &lt; 10e-4 for ns = 6; t(84) = 1.8, p &gt; .05 for ns = 3). Surprisingly, but confirming our prediction, we also found that set size 3 testing phase performance was predicted by set size 6, but not set size 3 asymptotic learning phase performance (t(84) = 4.5, p = 0.0000 for ns = 6; t(84) = −0.02; p = 0.98 for ns = 3). These results support our hypothesis that testing phase performance relies on RL, which is more expressed during learning of set size 6 than 3.</p><p hwp:id="p-49">Next, we investigated testing phase performance as a function of set size of the block during which the stimulus was learned. It is important to remember that if testing phase performance reflected the history of choices and rewards during learning, participants should be better able to select correct actions for stimuli in low set sizes rather than in high set sizes (following their asymptotic learning performance). Instead, we found that participants were significantly better at selecting the appropriate action for items they learned in high set sizes (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2B</xref> t(84) = 3.8, p = 0.0003; 58/85 participants: sign test p = 0.001), indicating a counterintuitive robustness of learning under high load. This was also visible in reaction times, which were faster for items learned under high load (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Fig. 2B</xref> t(84) = 3.5, p = 0.0008; 57/85 participants: p = 0.002).</p><p hwp:id="p-50">We confirmed these results with multiple regression analyses, using learning block, learning set size, and asymptotic average rewards as predictors. Results showed that learning block and reward history accounted for significant variance in testing phase performance and RTs (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2D</xref>; sign tests p’s &lt; 10e-4), with effects in the same direction as learning phase performance and RTs, reflecting variance in the learning process. However, set size predicted variance in the testing phase in the opposite direction from in the learning phase (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2D</xref>; better choices, p &lt; 10e-4, faster RTs, p = 2.10e-4). Together, these results confirm our prediction that despite apparent successful learning in low set sizes, stimulus-action associations were formed in a way that was stronger in high set sizes, as observed in testing phase performance.</p><p hwp:id="p-51">Next, we sought to confirm that these effects were not due to differences in reward history. First, we limited testing phase analysis to items where the participant chose correctly for all last 6 iterations (thus reaching 100% asymptotic performance in both set sizes). Within this subset, we again observed better test performance in high set size stimuli, with a stronger effect (p &lt; 10-4, t(84) = 7.6). Second, we asked whether higher performance in set size 6 might be accounted for by participants better avoiding choices that were unrewarded during learning, which would have been experienced more in set size 6 than set size 3. However, we found that the types of errors made by participants in both set sizes were not different (t(84) = 0.01,p = 0.99), and significantly suboptimal (one-sample t-test vs. chance both t’s &gt; 12, proportion of optimal error trials 27%; see methods). Thus superior performance in high set sizes was not due to a better avoidance of incorrect actions due to higher sampling of errors during learning. Indeed, RLWM models with no interaction between RL and WM predicted the <italic toggle="yes">opposite</italic> effect of set size in the testing phase (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-10" hwp:rel-id="F4">Fig. 3</xref> RLWM).</p><p hwp:id="p-52">Instead, we hypothesized that the reversal in the effect of set size on performance between learning and testing was indicative of WM interfering with RL computations during learning. Specifically, we hypothesized that in low set size blocks, which were putatively within working memory capacity, the ability to solve the learning problem with WM would lead to interference with RL computations. We propose a mechanism whereby WM is able not only to store stimulus action associations, but also to predict a correct outcome when this association is used. This prediction is used cooperatively in a mixture with the RL expectation to compute reward prediction error (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig.1C</xref>); and thus when WM is ahead of RL, it decreases positive reward prediction errors, and effectively impedes learning in the RL process (Collins &amp; Frank, submitted). This would then lead to less well-learned associations in low compared to high set sizes. We characterized the contributions of WM and RL to choice and to the reward prediction error with a single mixture parameter for each set size, η<sub>ns</sub> (see methods for full model description). If this mixture parameter represents contributions of capacity-limited WM, we should observe η<sub>6</sub>&lt;η<sub>3</sub>. This is indeed what we observed with parameters fit on individual subjects’ choices (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig. 4</xref> right).</p><p hwp:id="p-53">We tested this model (RLWMi) against three other main models (as described in the methods). The first alternative (RLs) assumed only RL processes, and was made flexible enough to be able to reproduce all qualitative effects observed empirically. This required assuming that two independent sets of association weights were learned in parallel, one used during learning and the other during testing; and that learning rates were different for both, as well as for different set sizes. The second alternative model (RLWM) was similar to the main model (RLWMi), but without the interaction for learning. The last alternative model (RLWMii), was identical to RLWMi, but offered flexibility in separately parameterizing the contribution of WM to choice and to learning. We fitted participants’ behavior with the three models, and found that RLWMi fit significantly better than the other three models (exceedance probability = 1). Model simulations (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-4-11" hwp:rel-id="F4">Fig. 3</xref>) confirmed that RLWMi provided the best qualitative and quantitative account of the data. These results confirm that working memory is needed to account for learning behavior, and that an interference with RL is the best way to account for counterintuitive testing phase effects. It is important to note that letting the contributions of WM to choice and to RL be independent does not capture additional variance in behavior, hinting that these two functions might share a single, coupled mechanism.</p><p hwp:id="p-54">To validate the RLWMi model, we simulated it with individual participants’ fit parameters (100 times per participant). Simulations provided a good qualitative and quantitative fit to the choice results in both learning and testing phase. Specifically, the simulations produced learning curves that were very close to participants’ (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 5A</xref>), and reproduced the opposite set size effects in asymptotic learning phase and testing phase (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 5B, D, E</xref>). We did not capture the practice effect (block; <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 5D, E</xref>). We also simulated reaction times by assuming a negative linear dependence on the model-determined probability of the choice, such that faster reaction times occurred for choices with higher confidence. Using these simulated reaction times, we reproduced the observed pattern whereby reaction times are faster for low set sizes than high set sizes during learning, but the opposite is true for the testing phase (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 5C</xref>). This is because during learning, the WM contribution to choice leads to more confidence in the choices and faster RTs; but the higher WM interference during learning leads to less well-learned Q-values for low set sizes, which translates to slower RTs during testing. It is important to note that the model was fit only to the choices, not RTs, so this is an independent test of the model’s ability to capture empirical data.</p></sec><sec id="s4" hwp:id="sec-26"><title hwp:id="title-32">Discussion</title><p hwp:id="p-55">Our new experimental protocol allows us to demonstrate a surprising finding: associations learned in a more complex problem end up being learned more robustly, and are better retained in the long-term, than easy to learn associations. This finding provides behavioral evidence for an interaction between working memory and reinforcement learning. Specifically, we showed previously that RL and WM compete for choice during learning (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-4" hwp:rel-id="ref-7">Collins et al., 2014</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-8" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>). Here, we show an additional interaction between them whereby working memory impedes RL computations. These new results behaviorally and computationally support our previous observations that WM and RL are not independent modules, but that RL neural signals are weakened by WM use (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-7" hwp:rel-id="ref-6">Collins, Albrecht, et al., 2017</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-8" hwp:rel-id="ref-6">Collins, Ciullo, et al., 2017</xref>)(Collins &amp; Frank, submitted). It also strengthens our previous result showing that stimulus value was better learned in high set sizes (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-9" hwp:rel-id="ref-6">Collins, Albrecht, et al., 2017</xref>), but offers a more robust computational account for this finding.</p><p hwp:id="p-56">Working memory and reinforcement learning implement different trade-offs for learning: WM allows for very fast learning of information that is not durably retained, while RL allows for slow, integrative learning of associations that are robustly stored. Previous models assumed that humans can benefit from the best of both worlds by shifting the weight given to each system as a function of their reliability (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-5" hwp:rel-id="ref-7">Collins et al., 2014</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-10" hwp:rel-id="ref-6">Collins, Ciullo, et al., 2017</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-9" hwp:rel-id="ref-9">Collins &amp; Frank, 2012</xref>). Specifically, assuming that the effortless RL process simply occurs independently in the background, we can use WM to the maximum of its reliability for learning, and use RL first as a back-up, but as it becomes more reliable than WM, shift to “automatized” RL behavior only. The results presented here show instead that the trade-off cannot be completely eliminated by carefully arbitrating between RL and WM during learning: the use of WM in easy problems weakens RL learning, and thus leads to faster learning at the cost of long-term retention.</p><p hwp:id="p-57">We proposed a computational mechanism by which working memory may impede RL learning. Despite its negative effect on long-term learning, this mechanism can be thought of as cooperative. Indeed, we suggest that when an association is held in working memory, working memory can also contribute to corresponding reward expectations. Our computational mechanism assumes that WM’s expectation is weighted with RL’s expected value, and that this mixture expectation is then used to compute RL’s reward prediction error. Thus, when WM is reliable and learns faster than RL, this mechanism generates lower reward prediction errors in correct trials than an independent RL mechanism would, which in turn leads to a weaker update of associations in the RL system. This mechanism is compatible with our observations in an EEG experiment, where trial-by-trial neural markers of working memory use predicted lower markers of reward prediction errors (Collins &amp; Frank, under review).</p><p hwp:id="p-58">We showed that this cooperative interaction accounted for trial-by-trial choices during learning and testing better than models assuming no interactions. However, other interaction mechanisms could also be considered - in particular, a competitive mechanism provides a similar fit to participants’ choices (Collins &amp; Frank, submitted). This mechanism simply assumes that the RL learning rate is decreased, in proportion to WM contributions to choice. This leads to slower RL learning, not due to weakened reward prediction errors as in the cooperative mechanism, but due to a weaker effect of reward prediction learning on updates. Competition and cooperation mechanisms make separable predictions for neural signals: the competition mechanism predicts that RPEs will decrease slower in low set sizes due to the slow learning rate, while the cooperation mechanism predicts that they will decrease faster in low set sizes, due to accurate WM predictions. EEG results confirmed the latter prediction (Collins &amp; Frank, submitted), thus supporting the cooperation mechanism prediction. However, it will be important in future work to show behavioral evidence disambiguating the two mechanisms.</p><p hwp:id="p-59">This leaves opens an important question – are there situations in which a cooperative mechanism might be beneficial? In our experimental protocol, it seems suboptimal, as it leads to weakened learning in the long-term. RL algorithms are only guaranteed to converge to true estimates of expected value if the reward prediction error is not tampered with; interference may bias, or slow computations, as is seen here. It is thus puzzling that we might have evolved an interaction mechanism that actively weakens one of our most robust learning systems. Future research will need to determine whether this bias might be normative in more natural environments – for example, it is possible that situations in which learning is required to be very fast are also volatile environments, where we might need to change our behavior quickly. In that case, not having built as strong an association might allow for more flexible behavior. Another hypothesis is that this interaction reflects constraints of neural network implementations, and is thus a side-effect of another normative mechanism: specifically, that the contribution of working memory to choice, which helps learn faster in low set sizes, is not separable from its contributions to the reward expectation (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1C</xref>). While we do not have direct evidence for this hypothesis, our model fitting results provide a clue in its favor: a more flexible model that allowed contributions of WM to choice and RL to be uncoupled did not provide a better fit, and the additional degrees of freedom led to overfitting. Thus, no additional variance was captured by letting choice and learning interactions be separable, pointing to the possibility that they are indeed coupled mechanisms. Future research will need to clarify this.</p><p hwp:id="p-60">We focused on how two neuro-cognitive systems, working memory and reinforcement learning, worked together for learning, highlighting the effects of their interactions in a testing phase where only reinforcement learning was used. However, it is likely that one other system – long term memory – contributes to testing performance, and potentially also to learning (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Bornstein, Khaw, Shohamy, &amp; Daw, 2017</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Bornstein &amp; Norman, 2017</xref>). Indeed, others have shown that long-term memory encoding may compete for resources with reinforcement learning processes (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">R. A. Poldrack &amp; Packard, 2003</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Wimmer, Braun, Daw, &amp; Shohamy, 2014</xref>), and other interactions may be possible (for a review, see (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Gershman &amp; Daw, 2017</xref>)). Here, we observed in the testing phase that participants performed better for associations learned closer to the testing phase, an effect reminiscent of the well-documented recency effect in long term memory recall (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Sederberg, Howard, &amp; Kahana, 2008</xref>). Our model currently does not capture this effect, or other potential contributions of long term memory. While including it is beyond the scope of this work, it will be important in future research to investigate how this third mechanism interacts with RL and WM for learning.</p><p hwp:id="p-61">In summary, our results show a counter-intuitive but robust (and replicable) finding – that while learning under high load is slower and more effortful, it actually allows for better long-term learning and retention. This appears to be due to the fact that faster learning under low load cuts the corner with working memory, and by doing so undermines more robust encoding of associations via reinforcement learning. Our findings highlight complex interactions between multiple learning systems not only at the level of decisions, but also at the level of learning computations. When learners arbitrate in favor of fast and efficient working memory use over RL in simple situations, they simultaneously undermine the computations from this slower but more robust system, leading to worse long-term performance. This result may have important implication in numerous domains, as learning is an important part of our daily lives – for example in educational settings. Understanding how multiple systems work together for learning is also a crucial step to identifying the causes of learning dysfunction in many clinical populations, and thus better targeting treatments.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-33">Acknowledgements</title><p hwp:id="p-62">I thank Nora Harhen and Sarah Master for data collection.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-34">Bibliography</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Akaike H."><surname>Akaike</surname>, <given-names>H.</given-names></string-name> (<year>1974</year>). <article-title hwp:id="article-title-2">A new look at the statistical model identification</article-title>. <source hwp:id="source-1">IEEE Transactions on Automatic Control</source>, <volume>19</volume>(<issue>6</issue>), <fpage>716</fpage>-<lpage>723</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1109/TAC.1974.1100705" ext-link-type="uri" xlink:href="http://doi.org/10.1109/TAC.1974.1100705" hwp:id="ext-link-2">http://doi.org/10.1109/TAC.1974.1100705</ext-link></citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Baddeley A."><surname>Baddeley</surname>, <given-names>A.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-3">Working memory: theories, models, and controversies</article-title>. <source hwp:id="source-2">Annual Review of Psychology</source>, <volume>63</volume>, <fpage>1</fpage>-<lpage>29</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1146/annurev-psych-120710-100422" ext-link-type="uri" xlink:href="http://doi.org/10.1146/annurev-psych-120710-100422" hwp:id="ext-link-3">http://doi.org/10.1146/annurev-psych-120710-100422</ext-link></citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bornstein A. M."><surname>Bornstein</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khaw M. W."><surname>Khaw</surname>, <given-names>M. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-4">Reminders of past choices bias decisions for reward in humans</article-title>. <source hwp:id="source-3">Nature Communications</source>, <volume>8</volume>(<month>May</month>), <fpage>1</fpage>-<lpage>9</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1038/ncomms15958" ext-link-type="uri" xlink:href="http://doi.org/10.1038/ncomms15958" hwp:id="ext-link-4">http://doi.org/10.1038/ncomms15958</ext-link></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bornstein A. M."><surname>Bornstein</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Norman K. A."><surname>Norman</surname>, <given-names>K. A.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-5">Reinstated episodic context guides sampling-based decisions for reward</article-title>. <source hwp:id="source-4">Nature Neuroscience</source>, <volume>20</volume>(<issue>7</issue>), <fpage>997</fpage>-<lpage>1003</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1038/nn.4573" ext-link-type="uri" xlink:href="http://doi.org/10.1038/nn.4573" hwp:id="ext-link-5">http://doi.org/10.1038/nn.4573</ext-link></citation></ref><ref id="c5" hwp:id="ref-5"><citation publication-type="book" citation-type="book" ref:id="234724v1.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Burnham K. P."><surname>Burnham</surname>, <given-names>K. P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Anderson D. R."><surname>Anderson</surname>, <given-names>D. R.</given-names></string-name> (<year>2002</year>). <source hwp:id="source-5">Model Selection and Multi-Model Inference: A Practical Information-Theoretic Approach (Google eBook)</source>. <publisher-name>Springer</publisher-name>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.citeulike.org/group/7954/article/4425594" ext-link-type="uri" xlink:href="http://www.citeulike.org/group/7954/article/4425594" hwp:id="ext-link-6">http://www.citeulike.org/group/7954/article/4425594</ext-link></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2 xref-ref-6-3 xref-ref-6-4 xref-ref-6-5 xref-ref-6-6 xref-ref-6-7 xref-ref-6-8 xref-ref-6-9 xref-ref-6-10"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Albrecht M. A."><surname>Albrecht</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Waltz J. A."><surname>Waltz</surname>, <given-names>J. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gold J. M."><surname>Gold</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-6">Interactions Among Working Memory, Reinforcement Learning, and Effort in Value-Based Choice: A New Paradigm and Selective De fi cits in Schizophrenia</article-title>. <source hwp:id="source-6">Biological Psychiatry</source>, <volume>82</volume>(<issue>6</issue>), <fpage>431</fpage>-<lpage>439</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.biopsych.2017.05.017" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.biopsych.2017.05.017" hwp:id="ext-link-7">http://doi.org/10.1016/j.biopsych.2017.05.017</ext-link></citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3 xref-ref-7-4 xref-ref-7-5"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brown J. K."><surname>Brown</surname>, <given-names>J. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gold J. M."><surname>Gold</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Waltz J. A."><surname>Waltz</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-7">Working memory contributions to reinforcement learning impairments in schizophrenia</article-title>. <source hwp:id="source-7">Journal of Neuroscience</source>, <volume>34</volume>(<issue>41</issue>), <fpage>13747</fpage>-<lpage>13756</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1523/JNEUR0SCI.0989-14.2014" ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUR0SCI.0989-14.2014" hwp:id="ext-link-8">http://doi.org/10.1523/JNEUR0SCI.0989-14.2014</ext-link></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ciullo B."><surname>Ciullo</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Badre D."><surname>Badre</surname>, <given-names>D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-8">Working memory load strengthens reward prediction errors</article-title>. <source hwp:id="source-8">The Journal of Neuroscience</source>, <volume>37</volume>(<issue>16</issue>), <fpage>2700</fpage>-<lpage>16</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1523/JNEUR0SCI.2700-16.2017" ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUR0SCI.2700-16.2017" hwp:id="ext-link-9">http://doi.org/10.1523/JNEUR0SCI.2700-16.2017</ext-link></citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4 xref-ref-9-5 xref-ref-9-6 xref-ref-9-7 xref-ref-9-8 xref-ref-9-9"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-9">How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title>. <source hwp:id="source-9">The European Journal of Neuroscience</source>, <volume>35</volume>(<issue>7</issue>), <fpage>1024</fpage>-<lpage>35</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1111/j.1460-9568.2011.07980.x" ext-link-type="uri" xlink:href="http://doi.org/10.1111/j.1460-9568.2011.07980.x" hwp:id="ext-link-10">http://doi.org/10.1111/j.1460-9568.2011.07980.x</ext-link></citation></ref><ref id="c10" hwp:id="ref-10"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-10">Within and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory</article-title>. <source hwp:id="source-10">Doi.org</source>, <volume>184812</volume>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1101/184812" ext-link-type="uri" xlink:href="http://doi.org/10.1101/184812" hwp:id="ext-link-11">http://doi.org/10.1101/184812</ext-link></citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J. M."><surname>Frank</surname>, <given-names>M. J. M.</given-names></string-name> J. (<year>2013</year>). <article-title hwp:id="article-title-11">Cognitive control over learning: Creating, clustering, and generalizing task-set structure</article-title>. <source hwp:id="source-11">Psychological Review</source>, <volume>120</volume>(<issue>1</issue>), <fpage>190</fpage>-<lpage>229</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1037/a0030852" ext-link-type="uri" xlink:href="http://doi.org/10.1037/a0030852" hwp:id="ext-link-12">http://doi.org/10.1037/a0030852</ext-link></citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Koechlin E."><surname>Koechlin</surname>, <given-names>E.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-12">Reasoning, learning, and creativity: Frontal lobe function and human decision-making</article-title>. <source hwp:id="source-12">PLoS Biology</source>, <volume>10</volume>(<issue>3</issue>), <elocation-id>e1001293</elocation-id>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1371/journal.pbio.1001293" ext-link-type="uri" xlink:href="http://doi.org/10.1371/journal.pbio.1001293" hwp:id="ext-link-13">http://doi.org/10.1371/journal.pbio.1001293</ext-link></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Cools R."><surname>Cools</surname>, <given-names>R.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-13">Dopaminergic control of the striatum for high-level cognition</article-title>. <source hwp:id="source-13">Current Opinion in Neurobiology</source>, <volume>21</volume>(<issue>3</issue>), <fpage>402</fpage>-<lpage>7</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.conb.2011.04.002" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.conb.2011.04.002" hwp:id="ext-link-14">http://doi.org/10.1016/j.conb.2011.04.002</ext-link></citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Cowan N."><surname>Cowan</surname>, <given-names>N.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-14">The Magical Mystery Four: How is Working Memory Capacity Limited, and Why?</article-title> <source hwp:id="source-14">Current Directions in Psychological Science: A Journal of the American Psychological Society</source>, <volume>19</volume>(<issue>1</issue>), <fpage>51</fpage>-<lpage>57</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1177/0963721409359277" ext-link-type="uri" xlink:href="http://doi.org/10.1177/0963721409359277" hwp:id="ext-link-15">http://doi.org/10.1177/0963721409359277</ext-link></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S. J."><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour B."><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dolan R. J."><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-15">Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source hwp:id="source-15">Neuron</source>, <volume>69</volume>(<issue>6</issue>), <fpage>1204</fpage>-<lpage>15</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.neuron.2011.02.027" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuron.2011.02.027" hwp:id="ext-link-16">http://doi.org/10.1016/j.neuron.2011.02.027</ext-link></citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-16">Decision theory, reinforcement learning, and the brain</article-title>. <source hwp:id="source-16">Cognitive, Affective &amp; Behavioral Neuroscience</source>, <volume>8</volume>(<issue>4</issue>), <fpage>429</fpage>-<lpage>53</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.3758/CABN.8.4.429" ext-link-type="uri" xlink:href="http://doi.org/10.3758/CABN.8.4.429" hwp:id="ext-link-17">http://doi.org/10.3758/CABN.8.4.429</ext-link></citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="O’Reilly R. C."><surname>O’Reilly</surname>, <given-names>R. C.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-17">A mechanistic account of striatal dopamine function in human cognition: psychopharmacological studies with cabergoline and haloperidol</article-title>. <source hwp:id="source-17">Behavioral Neuroscience</source>, <volume>120</volume>(<issue>3</issue>), <fpage>497</fpage>-<lpage>517</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1037/0735-7044.1203.497" ext-link-type="uri" xlink:href="http://doi.org/10.1037/0735-7044.1203.497" hwp:id="ext-link-18">http://doi.org/10.1037/0735-7044.1203.497</ext-link></citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Gershman S. J."><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-18">Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework</article-title>. <source hwp:id="source-18">Annual Review of Psychology</source>, <volume>68</volume>(<issue>1</issue>), <fpage>101</fpage>-<lpage>128</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1146/annurev-psych-122414-033625" ext-link-type="uri" xlink:href="http://doi.org/10.1146/annurev-psych-122414-033625" hwp:id="ext-link-19">http://doi.org/10.1146/annurev-psych-122414-033625</ext-link></citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Guitart-Masip M."><surname>Guitart-Masip</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huys Q. J. M."><surname>Huys</surname>, <given-names>Q. J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fuentemilla L."><surname>Fuentemilla</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duzel E."><surname>Duzel</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dolan R. J."><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-19">Go and no-go learning in reward and punishment: interactions between affect and effect</article-title>. <source hwp:id="source-19">NeuroImage</source>, <volume>62</volume>(<issue>1</issue>), <fpage>154</fpage>-<lpage>66</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.neuroimage.2012.04.024" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuroimage.2012.04.024" hwp:id="ext-link-20">http://doi.org/10.1016/j.neuroimage.2012.04.024</ext-link></citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Miller E."><surname>Miller</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cohen J."><surname>Cohen</surname>, <given-names>J.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-20">An Integrative Theory Of Prefrontal Cortex</article-title>. <source hwp:id="source-20">Annual Review of Neuroscience</source>, <fpage>167</fpage>-<lpage>202</lpage>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.annualreviews.org/doi/abs/10.1146/annurev.neuro.24.L167" ext-link-type="uri" xlink:href="http://www.annualreviews.org/doi/abs/10.1146/annurev.neuro.24.L167" hwp:id="ext-link-21">http://www.annualreviews.org/doi/abs/10.1146/annurev.neuro.24.L167</ext-link></citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Montague P. R."><surname>Montague</surname>, <given-names>P. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sejnowski T. J."><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-21">A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source hwp:id="source-21">The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>16</volume>(<issue>5</issue>), <fpage>1936</fpage>-<lpage>47</lpage>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.ncbi.nlm.nih.gov/pubmed/8774460" ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/8774460" hwp:id="ext-link-22">http://www.ncbi.nlm.nih.gov/pubmed/8774460</ext-link></citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Nassar M. R."><surname>Nassar</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-22">Taming the beast: Extracting generalizable knowledge from computational models of cognition</article-title>. <source hwp:id="source-22">Current Opinion in Behavioral Sciences</source>, <volume>11</volume>, <fpage>49</fpage>-<lpage>54</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.cobeha.2016.04.003" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.cobeha.2016.04.003" hwp:id="ext-link-23">http://doi.org/10.1016/j.cobeha.2016.04.003</ext-link></citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Poldrack R. A."><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Packard M. G."><surname>Packard</surname>, <given-names>M. G.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-23">Competition among multiple memory systems: converging evidence from animal and human brain studies</article-title>. <source hwp:id="source-23">Neuropsychologie</source>, <volume>41</volume>(<issue>3</issue>), <fpage>245</fpage>-<lpage>251</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/S0028-3932(02)00157-4" ext-link-type="uri" xlink:href="http://doi.org/10.1016/S0028-3932(02)00157-4" hwp:id="ext-link-24">http://doi.org/10.1016/S0028-3932(02)00157-4</ext-link></citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Poldrack R. a"><surname>Poldrack</surname>, <given-names>R. a</given-names></string-name>, <string-name name-style="western" hwp:sortable="Clark J."><surname>Clark</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paré-Blagoev E. J."><surname>Paré-Blagoev</surname>, <given-names>E. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Creso Moyano J."><surname>Creso Moyano</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Myers C."><surname>Myers</surname>, <given-names>C.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Gluck M. a."><surname>Gluck</surname>, <given-names>M. a.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-24">Interactive memory systems in the human brain</article-title>. <source hwp:id="source-24">Nature</source>, <volume>414</volume>(<month>November</month>), <fpage>546</fpage>-<lpage>550</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1038/35107080" ext-link-type="uri" xlink:href="http://doi.org/10.1038/35107080" hwp:id="ext-link-25">http://doi.org/10.1038/35107080</ext-link></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Schultz W."><surname>Schultz</surname>, <given-names>W.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-25">Updating dopamine reward signals</article-title>. <source hwp:id="source-25">Current Opinion in Neurobiology</source>, <volume>23</volume>(<issue>2</issue>), <fpage>229</fpage>-<lpage>38</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.conb.2012.11.012" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.conb.2012.11.012" hwp:id="ext-link-26">http://doi.org/10.1016/j.conb.2012.11.012</ext-link></citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Schwarz G."><surname>Schwarz</surname>, <given-names>G.</given-names></string-name> (<year>1978</year>). <article-title hwp:id="article-title-26">Estimating the Dimension of a Model</article-title>. <source hwp:id="source-26">The Annals of Statistics</source>, <volume>6</volume>(<issue>2</issue>), <fpage>461</fpage>-<lpage>464</lpage>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://projecteuclid.org/euclid.aos/1176344136" ext-link-type="uri" xlink:href="http://projecteuclid.org/euclid.aos/1176344136" hwp:id="ext-link-27">http://projecteuclid.org/euclid.aos/1176344136</ext-link></citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Sederberg P. B."><surname>Sederberg</surname>, <given-names>P. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Howard M. W."><surname>Howard</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kahana M. J."><surname>Kahana</surname>, <given-names>M. J.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-27">A context-based theory of recency and contiguity in free recall</article-title>. <source hwp:id="source-27">Psychological Review</source>, <volume>115</volume>(<issue>4</issue>), <fpage>893</fpage>-<lpage>912</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1037/a0013396" ext-link-type="uri" xlink:href="http://doi.org/10.1037/a0013396" hwp:id="ext-link-28">http://doi.org/10.1037/a0013396</ext-link></citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Stephan K. E."><surname>Stephan</surname>, <given-names>K. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Penny W. D."><surname>Penny</surname>, <given-names>W. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daunizeau J."><surname>Daunizeau</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moran R. J."><surname>Moran</surname>, <given-names>R. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Friston K. J."><surname>Friston</surname>, <given-names>K. J.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-28">Bayesian model selection for group studies</article-title>. <source hwp:id="source-28">NeuroImage</source>, <volume>46</volume>(<issue>4</issue>), <fpage>1004</fpage>-<lpage>17</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1016/j.neuroimage.2009.03.025" ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuroimage.2009.03.025" hwp:id="ext-link-29">http://doi.org/10.1016/j.neuroimage.2009.03.025</ext-link></citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Tai L.-H."><surname>Tai</surname>, <given-names>L.-H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee a M."><surname>Lee</surname>, <given-names>a M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benavidez N."><surname>Benavidez</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bonci A."><surname>Bonci</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wilbrecht L."><surname>Wilbrecht</surname>, <given-names>L.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-29">Transient stimulation of distinct subpopulations of striatal neurons mimics changes in action value</article-title>. <source hwp:id="source-29">Nature Neuroscience</source>, <volume>15</volume>(<issue>9</issue>), <fpage>1281</fpage>-<lpage>9</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1038/nn.3188" ext-link-type="uri" xlink:href="http://doi.org/10.1038/nn.3188" hwp:id="ext-link-30">http://doi.org/10.1038/nn.3188</ext-link></citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="234724v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Wimmer G. E."><surname>Wimmer</surname>, <given-names>G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Braun E. K."><surname>Braun</surname>, <given-names>E. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-30">Episodic Memory Encoding Interferes with Reward Learning and Decreases Striatal Prediction Errors</article-title>. <source hwp:id="source-30">Journal of Neuroscience</source>, <volume>34</volume>(<issue>45</issue>), <fpage>14901</fpage>-<lpage>14912</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.1523/JNEUROSCI.0204-14.2014" ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUROSCI.0204-14.2014" hwp:id="ext-link-31">http://doi.org/10.1523/JNEUROSCI.0204-14.2014</ext-link></citation></ref></ref-list></back></article>
