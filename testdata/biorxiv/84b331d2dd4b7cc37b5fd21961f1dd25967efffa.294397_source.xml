<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/294397</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;294397</article-id><article-id pub-id-type="other" hwp:sub-type="slug">294397</article-id><article-id pub-id-type="other" hwp:sub-type="tag">294397</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">An ultralight head-mounted camera system integrates detailed behavioral monitoring with multichannel electrophysiology in freely moving mice</article-title></title-group><author-notes hwp:id="author-notes-1"><fn fn-type="equal" id="n1" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>*</label><p hwp:id="p-1">These authors contributed equally</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Meyer Arne F."><surname>Meyer</surname><given-names>Arne F.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Poort Jasper"><surname>Poort</surname><given-names>Jasper</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="O’Keefe John"><surname>O’Keefe</surname><given-names>John</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Sahani Maneesh"><surname>Sahani</surname><given-names>Maneesh</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Linden Jennifer F."><surname>Linden</surname><given-names>Jennifer F.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">Gatsby Computational Neuroscience Unit, University College London (UCL)</institution>, London W1T 4JG, <country>UK</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label><institution hwp:id="institution-2">Sainsbury Wellcome Centre, UCL</institution>, London W1T 4JG, <country>UK</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Department of Cell and Developmental Biology, UCL</institution>, London WC1E 6BT, <country>UK</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Ear Institute, UCL</institution>, London WC1X 8EE, <country>UK</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Department of Neuroscience, Physiology and Pharmacology, UCL</institution>, London WC1E 6BT, <country>UK</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-04-03T21:15:27-07:00">
    <day>3</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-04-03T21:15:27-07:00">
    <day>3</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-04-03T21:20:44-07:00">
    <day>3</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-04-03T21:20:44-07:00">
    <day>3</day><month>4</month><year>2018</year>
  </pub-date><elocation-id>294397</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-04-03"><day>03</day><month>4</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-04-03"><day>03</day><month>4</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-04-03"><day>03</day><month>4</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="294397.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/294397v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="294397.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/294397v1/294397v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/294397v1/294397v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Summary</title><p hwp:id="p-3">Breakthroughs in understanding the neural basis of natural behavior require neural recording and intervention to be paired with high-fidelity multimodal behavioral monitoring. An extensive genetic toolkit for neural circuit dissection, and well-developed neural recording technology, make the mouse a powerful model organism for systems neuroscience. However, methods for high-bandwidth acquisition of behavioral signals in mice remain limited to fixed-position cameras and other off-animal devices, complicating the monitoring of animals freely engaged in natural behaviors. Here, we report the development of an ultralight head-mounted camera system combined with head-movement sensors to simultaneously monitor eye position, pupil dilation, whisking, and pinna movements along with head motion in unrestrained, freely behaving mice. The power of the combined technology is demonstrated by observations linking eye position to head orientation; whisking to non-tactile stimulation; and, in electrophysiological experiments, visual cortical activity to volitional head movements.</p></abstract><counts><page-count count="37"/></counts></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-2">Introduction</title><p hwp:id="p-4">A fundamental goal of neuroscience is to understand how neural circuits integrate a wide range of inputs to produce flexible and adaptive behaviors in natural settings. To approach this goal in its most general form, it will be essential to monitor and manipulate both neural activity and behavioural variables while animals interact naturally with their environments. The availability of genetic tools to dissect neural circuitry (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Luo et al., 2008</xref>) and to construct models of human disease (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Gatz and Ittner, 2008</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Nestler and Hyman, 2010</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Chesselet and Richter, 2011</xref>) has driven the emergence of the mouse as a key model organism in systems neuroscience (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Carandini and Churchland, 2013</xref>). An increasingly wide array of technologies are available to measure and manipulate neural activity in mice (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Voigts et al., 2008</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Luo et al., 2008</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Jun et al., 2017</xref>). However, detailed monitoring of behavior, especially in freely moving animals, remains a major challenge (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Krakauer et al., 2017</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Juavinett et al., 2018</xref>). To address this challenge, we developed an ultralight head-mounted camera system to measure eye position, pupil dilation, whisking, pinna movements and other behavioral signals in freely moving mice, which we combined with head-movement monitoring and multichannel electrophysiology.</p><p hwp:id="p-5">Despite the longstanding ability to record neural activity in unrestrained rodents (e.g., <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">O’Keefe and Dostrovsky, 1971</xref>), many current studies of the neural basis of behaviour have relied on awake but head-restrained animals (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Carandini and Churchland, 2013</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">Juavinett et al., 2018</xref>). Head fixation enables tight control of sensory inputs, facilitates intracranial recording or imaging, and simplifies experimental manipulations that would be diffi cult in freely moving animals. However, results obtained in head-restrained animals may not generalize to more natural sensory and behavioral conditions (<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">Tatler and Land, 2011</xref>; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Felsen and Dan, 2005</xref>). For example, the change in vestibular inputs following head fixation may have widespread effects throughout the brain (<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Rancz et al., 2015</xref>), and it is debated whether spatial navigation by head-fixed animals in virtual reality environments is comparable to spatial navigation in freely moving animals (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Dombeck et al., 2007</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Domnisoru et al., 2013</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Schmidt-Hieber and Hausser, 2013</xref>; <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Aghajan et al., 2015</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Minderer et al., 2016</xref>). While the level of experimental control and the availability of techniques for monitoring neural activity are more limited in studies of freely moving animals, such investigations have provided important insights into brain function during behavior that might not have been obtained in more constrained experimental settings, for instance revealing cells that represent an animal’s spatial location and head direction (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-2" hwp:rel-id="ref-41">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">Taube et al., 1990</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Fyhn et al., 2004</xref>).</p><p hwp:id="p-6">Detailed behavioral measurement in freely moving mice remains a major challenge because of the animal’s small size (average adult weight only <italic toggle="yes">∼</italic>20-25 grams; <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.jax.org" ext-link-type="uri" xlink:href="http://www.jax.org" hwp:id="ext-link-2">www.jax.org</ext-link>). Externally mounted video cameras have been used to track aspects of gross locomotor behavior including gait (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Machado et al., 2015</xref>) and posture (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Hong et al., 2015</xref>; <xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Wiltschko et al., 2015</xref>), and (in semi-stationary mice and when permitted by the camera angle) whisking (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-2" hwp:rel-id="ref-67">Voigts et al., 2008</xref>; <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Roy et al., 2011</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Nashaat et al., 2017</xref>) and head and eye movements (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Kretschmer et al., 2015</xref>, <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">2017</xref>). However, the perspective of the external camera limits the potential for continuous measurement of whisking, pupil diameter or eye position in actively exploring mice (although <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Payne and Raymond, 2017</xref>, have successfully monitored horizontal eye movements using a magnetic field approach).</p><p hwp:id="p-7">The new miniaturized head-mounted tracking system reported here makes it possible to continuously monitor multiple behavioral variables, such as eye and pinna movements, whisking, eating and licking, together with head movements, in combination with chronic neural recording from unrestrained mice. A recent study developed a head-mounted eye tracking system for the rat (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">Wallace et al., 2013</xref>). However, given the comparatively small size of the mouse, we required a system with greatly reduced weight and footprint. Moreover, the method used in rats relied on detection of reference points recorded by multiple video cameras and additional head-mounted LEDs to track orientation and movement of the head. Instead, we used small lightweight inertial sensors to track the orientation and movements of the head (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Mizell, 2003</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Pasquet et al., 2016</xref>), simplifying the process of relating these variables to the camera outputs even under demanding natural conditions.</p><p hwp:id="p-8">The system generates stable video output, leaves mouse behavior largely unchanged, and does not affect the quality of concomitant neural recordings. We demonstrate the potential of the system in a series of experiments in freely moving mice. First, we show that variables such as whisking frequency and pupil size vary systematically with behavioral state, and that these changes are correlated with neural activity, thereby generalizing results obtained in head-restrained mice to natural behaviors (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">McGinley et al., 2015</xref>). Second, we demonstrate that a large fraction of variability in eye position in freely moving mice is explained by head movements, as has also been observed in rats (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-2" hwp:rel-id="ref-69">Wallace et al., 2013</xref>). We find systematic relationships between eye position and head orientation in freely moving mice, suggesting that mice stabilize their gaze with respect to the horizontal plane, even in the dark. Third, we demonstrate that neural activity in primary visual cortex (V1) is strongly modulated by head movements even in the absence of visual input. This effect does not depend on variability in eye movements and cannot be explained by whisking or locomotion. These results demonstrate how the new camera system can lead to novel insight into the interactions between different behaviors and their relation with neural activity.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-3">Results</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-4">A miniature head-mounted camera system for freely moving mice</title><p hwp:id="p-9">The ultra-lightweight head-mounted camera system (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1A</xref>) consisted of a miniature CMOS image sensor with integrated video data cable; a custom 3D-printed holder for the image sensor; an infrared (IR) LED illumination source; and an IR mirror on a custom lightweight extension arm. The mirror reflected only IR light and allowed visible light to pass through, so it was visually transparent to the mouse (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Peirson et al., 2017</xref>). The weight of the camera system including the image sensor was approximately 1.3 grams (see <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure S1</xref>, <xref rid="tblS1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table S1</xref>, and Methods for the list of parts). We wrote custom software (see Methods) to synchronize video and neural data and to integrate video recordings with open-source systems for neural data acquisition (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.open-ephys.org" ext-link-type="uri" xlink:href="http://www.open-ephys.org" hwp:id="ext-link-3">http://www.open-ephys.org</ext-link>). The camera system recorded video frames with image resolution 640×480 pixels at frame rates of up to 90 Hz (<xref rid="figS2" ref-type="fig" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Figure S2</xref>); thus, video images could be aligned to neural data with a temporal precision of 11.1 ms.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><title hwp:id="title-5">Simultaneous measurement of multiple behavioral variables and neural activity in a freely-moving mouse.</title><p hwp:id="p-10">(A) Neural activity recorded with a chronic tetrode implant; video data simultaneously recorded using a miniature CMOS image sensor and an infrared (IR) mirror mounted on the implant with a custom lightweight holder. An IR light source on the camera holder illuminates the region of interest, which is imaged via the IR mirror. The mirror reflects only IR light, allowing visible light to pass through so the animal’s vision is not obstructed. Head motion and orientation are measured using an accelerometer integrated into the neural recording headstage. (B) Extraction of pitch and roll from low-pass filtered accelerometer signals. White arrow indicates direction opposite to gravity component. Turquoise arrow indicates orientation of vertical (ventral-dorsal) head axis. (C) A mouse freely explores its environment while wearing the head-mounted camera system. Absolute position is measured using external cameras. (D) Example traces of simultaneously recorded behavioral and neural data. Pictures of eye position in third row were acquired at times of dots on pupil position traces in the fourth row.</p></caption><graphic xlink:href="294397_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-11">The camera system was attached during each recording session to a miniature connector built into a chronically implanted custom tetrode drive with 8-16 individually movable tetrodes (based on an existing implant design, <xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Voigts et al., 2013</xref>). Power to the IR LED was provided through the digital neural recording head-stage, which was also attached to the implant for each recording session. The headstage board included an integrated 3-axis accelerometer to measure the movement and orientation of the animal’s head (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-2" hwp:rel-id="ref-43">Pasquet et al., 2016</xref>) (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1B</xref> and Methods; see <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7</xref> and Methods for measurement of rotational movements). The mouse freely explored a small circular environment, while body position was monitored using an external camera (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1C</xref>). The combined system allowed the simultaneous measurement of pupil position, pupil dilation, whisker pad movement, head movement, head orientation, body position and body speed together with neural activity in freely moving mice (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1D</xref>).</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Operation of camera does not impair neural recording quality</title><p hwp:id="p-12">We first checked that the camera system did not interfere with electrophysiological recordings, by comparing neural recordings with camera powered and operating or switched off. <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2A,B</xref> shows signals from an example tetrode channel. The isolation of action-potential spikes appeared unchanged with the camera on or off, and the projection of spike waveforms into a noise-whitened robust PCA (NWrPC) space (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Sahani, 1999</xref>) was similar in both conditions (insets in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2A,B</xref>) as was the power spectrum of the broadband signal (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2C</xref>). We quantified spectral differences between conditions across recordings by computing the power ratio between counter-balanced camera-on and camera-off recording segments obtained during each recording session. Across the frequency range of neural signals (2 to 6000 Hz), differences in log-power ratios were close to 0 dB, and within one standard deviation of the within-condition (camera-off) variability (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2D-F</xref>). We also compared the signal-to-noise ratio (SNR) for spike waveforms recorded from the same cells with the camera on or off (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Figure 2G</xref> and Methods), and found no significant difference (Wilcoxon signed-rank test, P =0.19). Thus the operation of the head-mounted camera had no discernable impact on the signal quality of electrophysiological recordings.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><title hwp:id="title-7">Neural recording quality with head-mounted camera.</title><p hwp:id="p-13">(A) Broadband electrode signal (top), high-pass filtered signal (middle), and extracted spikes (bottom) with head-mounted camera activated ("camera-on" condition). Inset shows projections of spike waveforms for identified cells into the space defined by the first two noise-whitened robust principal components (NWrPC 1 and 2). (B) The same as in A but with head-mounted camera de-activated ("camera-off" condition). Scales apply to bottom and top traces in A and B. (C) Power density spectrum of broadband electrode signals for camera-on and -off conditions (10 minutes each) recorded in the same session. Note that the two lines are closely overlapping. (D-F) Mean log power ratio between broadband electrode signals in camera-on and -off conditions (10 minutes each condition per session) across recording sessions for three mice (mouse 1 &amp; 2, <italic toggle="yes">n</italic> = 9; mouse 3, <italic toggle="yes">n</italic> = 19). Pale lines indicate standard deviation of log power ratios for recordings in camera-off condition alone. (G) Signal-to-noise ratio (SNR) between identified spike waveforms and noise in the high-pass filtered signal, for camera-on versus camera-off conditions (158 single-units, 11 multi-units).</p></caption><graphic xlink:href="294397_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-8">Camera images remain stable as the mouse moves</title><p hwp:id="p-14">Next, we measured the stability of video recordings from the head-mounted camera. We identified a rigid part of the implant visible in the image frame as a reference (grey outline in inset image in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3A</xref>) and used motion registration (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Dubbs et al., 2016</xref>) to determine the x- and y-displacement of the image in each frame, relative to the average image position across frames. When displacements occurred, they were typically on the order of a single pixel (40 <italic toggle="yes">µ</italic>m; <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3A</xref>). The diameter of the mouse eye and pupil are approximately 3.4 mm (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Sakatani and Isa, 2004</xref>) and 0.4 - 1.6 mm (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">McGinley et al., 2015</xref>), respectively. Thus, on average, camera image displacements in freely exploring mice were 1-2 orders of magnitude smaller than eye or pupil diameter. Moreover, average inter-frame image movement (i.e., change in 2D displacement between successive frames) was less than 4 <italic toggle="yes">µ</italic>m in mice freely exploring a circular environment, compared to less than 1 <italic toggle="yes">µ</italic>m in a control condition when the same animals were head-fixed on a cylindrical treadmill (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3B</xref> and Methods).</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><title hwp:id="title-9">Image stability during movement.</title><p hwp:id="p-15">(A) Camera view of the left eye (top) with inset showing reference for image registration (gray rectangle). Traces below show example frame-by-frame displacements of camera image in x- (middle) and y- (bottom) directions. (B) Average 2D inter-frame image movement for three mice, recorded while animals were either freely exploring a circular environment or head-fixed on a cylindrical treadmill. Number of freely moving and head-fixed recordings (10 minutes each): mouse 1, <italic toggle="yes">n</italic> = 55 and 22; mouse 2, <italic toggle="yes">n</italic> = 35 and 29; mouse 3, <italic toggle="yes">n</italic> = 14 and 14, respectively. (C) Cumulative distribution of inter-frame image movements. Note that image movement is zero for nearly 95% of frames. (D-E) Average inter-frame image movement as a function of body speed (D) or head acceleration (E), for three mice. Thin grey lines indicate relative frequency of body speed (D) or head acceleration (E), respectively.</p></caption><graphic xlink:href="294397_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-16">We also investigated the frequency with which image movements occurred in freely moving mice. <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3C</xref> shows the cumulative distribution of inter-frame image movements, after excluding frames in which the reference was occluded, e.g., during grooming (less than 0.6% of all frames, see Methods). In nearly 95% of analyzed frames, no image movement was observed. In 98-99% of frames, the maximal shift was one pixel (40 <italic toggle="yes">µ</italic>m; see marked points in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figure 3C</xref>).</p><p hwp:id="p-17">Finally, we investigated whether image movement was related to mouse behavior. There was no evident relationship between average image movement per frame and body speed (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 3D</xref>). We also tested for a relationship with head acceleration (after removing the gravity component, see <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1D</xref> and Methods) and found an increase in image movement with stronger head accelerations, but these strong head movements were rare in all three mice (head acceleration magnitude less than 0.2 g for 95% of the recorded frames in all mice; <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3E</xref>). Moreover, even when mice made head movements with a magnitude of 1 g, the average image movement per frame did not exceed about 10 <italic toggle="yes">µ</italic>m.</p><p hwp:id="p-18">We conclude that the head-mounted camera system produced stable video recordings, even when mice were grooming or actively exploring objects in complex and enriched environments (see Movie S2).</p></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-10">Patterns of behavior are minimally disturbed by camera system</title><p hwp:id="p-19">Previous work has shown that mice tolerate the tetrode implant with only minimal changes in natural behavior (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-2" hwp:rel-id="ref-68">Voigts et al., 2013</xref>). We wondered whether the additional weight of the head-mounted camera system might alter gross locomotor and exploratory behaviors in our animals. We analyzed the head-mounted accelerometer signals obtained from two implanted mice with and without the camera attached, during repeated sessions of free exploration across more than two months. We developed a semi-automatic state-segmentation algorithm to segment the recordings into four behaviors (active exploration, quiescence, grooming, eating) based on the short-term spectra of the accelerometer signals (see Methods, <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4B,C</xref> and <xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Figure S4</xref>). We found that this approach more accurately matched human observer segmentation (with cross-validation) than approaches based on segmenting the time-domain accelerometer signals directly (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Venkatraman et al., 2010</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Dhawale et al., 2017</xref>) (<xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-2" hwp:rel-id="F12">Figure S4D,E</xref>). Cross-validated classifications of behavioral state using the spectra-based algorithm matched classifications by a human observer over 96% of the time both with and without the camera attached, with no significant difference in classification performance between the two conditions (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4D</xref>, Fisher’s exact test, <italic toggle="yes">P</italic>=0.40; <xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Figure S4A</xref>, <italic toggle="yes">P</italic>=0.13).</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7 xref-fig-4-8 xref-fig-4-9 xref-fig-4-10 xref-fig-4-11 xref-fig-4-12 xref-fig-4-13 xref-fig-4-14 xref-fig-4-15 xref-fig-4-16 xref-fig-4-17"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><title hwp:id="title-11">Impact of head-mounted camera on basic mouse behavior.</title><p hwp:id="p-20">(A) Recordings were performed with ("Implant+cam") and without ("Implant") head-mounted camera system. (B) Example accelerometer traces for one motion axis recorded in different behavioral states. (C) Power spectra of accelerometer signals shown in B, extracted from a 20-minute recording. The different behavioral states can be reliably discriminated based on the power spectra. Shaded areas indicate standard error. (D) Confusion matrix illustrating cross-validated classification performance of a semi-automatic state-segmentation algorithm based on head-mounted accelerometer signal spectra ("Predicted state"), compared to behavioral state classifications based on manual annotation of external video and other data ("Human observer"; see Methods). Left: mouse with implant and camera. Right: with implant only. (E) Distribution of proportions of time per session spent in different behavioral states for mouse 1. Dark and light colors of each hue indicate condition with and without camera, respectively. Number of sessions: implant+cam <italic toggle="yes">n</italic> = 21, implant alone <italic toggle="yes">n</italic> = 11. (F) Log-probability distribution of head orientation for the mouse in E, with implant and camera (left) and with implant alone (right). Gray arrow indicates direction opposite gravity; turquoise arrow indicates mean head orientation. (G) Log-probability distribution of measured body speed for mouse 1. (H-J) The same as in E-G for mouse 2. Number of sessions: implant+cam <italic toggle="yes">n</italic> = 18, implant alone <italic toggle="yes">n</italic> = 11.</p></caption><graphic xlink:href="294397_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-21">The successful semi-automated segmentation of behavioral states allowed us to objectively compare mouse behavior with and without the camera. Behavioral patterns varied from day to day (<xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-4" hwp:rel-id="F12">Figure S4B,C</xref>), but both animals spent the majority of time in the active exploration state in most sessions (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4E,H</xref>). The proportion of time spent in each behavioral state depended in part on session number relative to the first recording (<xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-5" hwp:rel-id="F12">Figure S4B,C</xref>). However, we found no statistically significant differences between implant+cam and implant alone conditions in the proportion of time spent in each state for either mouse (permutation test, <italic toggle="yes">P</italic>=0.07 for mouse 1, <italic toggle="yes">P</italic>=0.12 for mouse 2; see Methods for details). Each mouse divided its time similarly between the four behavioral states with and without the camera (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4E,H</xref>).</p><p hwp:id="p-22">Since the majority of time was spent in the active exploration state, we examined behavior in this state more closely, paying specific attention to head movements and body speed (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Figure 4F,I</xref>). The addition of the camera produced a slight change in average head position (mouse 1: −7°pitch, +5°roll; mouse 2: −3°pitch, +5°roll), which was not statistically significant for either mouse (permutation tests; mouse 1: <italic toggle="yes">P</italic>=0.41 pitch, <italic toggle="yes">P</italic>=0.06 roll; mouse 2: <italic toggle="yes">P</italic>=0.92 pitch, <italic toggle="yes">P</italic>=0.37 roll). The camera also produced a small reduction in the standard deviation of head pitch, and a small increase in the standard deviation of head roll (mouse 1: +3°pitch SD, −4°roll SD; mouse 2: +4°pitch SD, −6°roll SD), each statistically significant in one of the two mice (permutation tests; mouse 1: <italic toggle="yes">P</italic>=0.04 pitch SD, <italic toggle="yes">P</italic>=0.12 roll SD; mouse 2: <italic toggle="yes">P</italic>=0.17 pitch SD, <italic toggle="yes">P</italic>=0.04 roll SD), and even here the differences were relatively small (−11% for pitch in mouse 1 and +30% for roll in mouse 2). Distributions of body speed during active exploration were unaffected by the camera (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Figure 4G,J</xref>; permutation test, <italic toggle="yes">P</italic>=0.35 mouse 1, <italic toggle="yes">P</italic>=0.39 mouse 2; see Methods). We conclude that active exploratory head and body movements were minimally affected by the presence of the head-mounted camera.</p></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-12">Pupil diameter and whisking correlate with behavioral and neural state in freely moving mice</title><p hwp:id="p-23">We next explored the capacity of the combined implant and camera system to identify correlations between behavioral and neural variables. <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5A</xref> shows a 6-minute extract from a 40-minute recording session of several behavioral and neural variables which included active and quiescent states, as well as grooming and eating (see Movie S3 for a longer 10-minute segment).</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6 xref-fig-5-7 xref-fig-5-8 xref-fig-5-9 xref-fig-5-10 xref-fig-5-11 xref-fig-5-12 xref-fig-5-13 xref-fig-5-14 xref-fig-5-15 xref-fig-5-16 xref-fig-5-17 xref-fig-5-18 xref-fig-5-19 xref-fig-5-20 xref-fig-5-21"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><title hwp:id="title-13">Continuous monitoring of behavioral and neural variables in freely moving mice.</title><p hwp:id="p-24">(A) Example traces of simultaneously measured behavioral and neural variables (5 minutes from a 40-minute recording). Colored rectangles above traces indicate behavioral states assigned by the behavioral segmentation algorithm. (B) Low (2-10 Hz) and high frequency (10-20 Hz) LFP power in V1 in active and quiescent states (mean ± SEM). LFP power normalized by low frequency power in quiescent state. (C) Distribution of pupil diameters in active and quiescent states. (D) Correlation coeffi cient between low-frequency (2-10 Hz) LFP power and pupil diameter during quiescent state. Only segments in which the head was still for at least 15 seconds were used for the analysis. (E) Distribution of whisker pad movement frequencies in active and quiescent states (30 Hz frame rates). (F) Log-probability distributions of head orientation in different behavioral states. (G) Log-probability distributions of simultaneously measured horizontal and vertical eye position in the same states. Same colorbar as in F.</p></caption><graphic xlink:href="294397_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-25">Previous studies in head-restrained mice have demonstrated that local field potential (LFP) power in the low-frequency (2-10 Hz) band is significantly reduced in the active compared to the quiescent state in sensory cortex (<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Poulet and Petersen, 2008</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-3" hwp:rel-id="ref-34">McGinley et al., 2015</xref>). Moreover, in head-fixed animals, pupil diameter is inversely related to low-frequency LFP power, and increased during active behavior and reduced during quiescence (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-4" hwp:rel-id="ref-34">McGinley et al., 2015</xref>). We found that these relations also hold in primary visual cortex (V1) in freely moving mice (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5B-D</xref>). Normalized low-frequency LFP power was significantly lower in the active than quiescent state (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 5B</xref>; two-sample <italic toggle="yes">t</italic>-test, <italic toggle="yes">P</italic> &lt;0.001), and the distribution of pupil diameters was shifted to larger values in the active state (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5C</xref>; Wilcoxon rank-sum tests for difference in medians, <italic toggle="yes">P</italic> &lt;0.001). Low-frequency LFP power and pupil diameter were not only inversely affected by changes between active and quiescent behavioral states, but also negatively correlated in simultaneous recordings within the same behavioral state. We analyzed correlations between LFP power and pupil diameter for quiescent recording segments during which the mouse kept its head in a constant position for at least 15 seconds, to minimize fluctuations in pupil diameter from changes in eye illumination (see also Methods). There was a strong negative correlation between pupil diameter and low-frequency LFP power in these recordings (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Figure 5D</xref>; median correlation coeffi cient −0.44 versus 0 for shuffl ed data, Wilcoxon rank-sum test <italic toggle="yes">P</italic>=0.02).</p><p hwp:id="p-26">Previous studies in head-restrained mice have also reported that the frequency of whisking is increased in the active compared to the quiescent behavioral state (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Moore, 2004</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">Poulet and Petersen, 2008</xref>; <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-3" hwp:rel-id="ref-51">Reimer et al., 2014</xref>). To examine whisking frequency in freely moving mice, we extracted whisker pad movements from the head-mounted camera images (see Methods and <xref rid="figS3" ref-type="fig" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Figure S3</xref> for details) and observed an increased frequency of whisker pad movements in the active state (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Figure 5E</xref>; Wilcoxon rank-sum test for Wilcoxon rank-sum tests for difference in medians, <italic toggle="yes">P</italic> &lt;0.001), confirming previous findings in head-restrained mice. We also discovered an aspect of whisking behavior that has not, to our knowledge, been reported previously in head-restrained mice: sounds that were presented when the mouse was immobile reliably evoked whisker pad movements that were comparable in magnitude to whisker pad movements observed during active exploration (<xref rid="figS5" ref-type="fig" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Figure S5</xref>).</p><p hwp:id="p-27">The head-mounted camera system also enabled measurement and analysis of head movements and head-movement-related behavior, which cannot be studied in head-restrained animals. We measured the distributions of head orientation (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-7" hwp:rel-id="F5">Figure 5F</xref>) and eye position (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-8" hwp:rel-id="F5">Figure 5G</xref>) in four behavioral states (quiescent, active, grooming and eating), by segmentation of behavioral data from continuous 40-minute recording sessions (see <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Figure 4A-D</xref>). The distributions of both head orientation and eye position had wider spreads during active exploration than during quiescence (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-9" hwp:rel-id="F5">Figure 5F,G</xref>; permutation test, <italic toggle="yes">P</italic> &lt;0.001 for head pitch/roll and horizontal/vertical eye positions; see Methods). More specifically, the distributions in the quiescent state appeared to be dominated by particular combinations of head orientation and eye position that the mouse preferred at rest. In contrast, there was a different pattern during grooming: distinct modes of head orientation (which appeared to correspond to different grooming movements, e.g. forepaws over the nose and muzzle, strokes with the hindleg), combined with the same modal eye position (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-10" hwp:rel-id="F5">Figure 5F,G</xref>). Similarly, eye position remained relatively constant during eating, despite changes in head orientation. These observations indicate that head-eye coordination differs between behavioral states; eye-movement patterns are more restricted relative to head orientation during grooming and eating than during active exploration.</p><p hwp:id="p-28">These results demonstrate that the head-mounted camera system enables detailed characterization of the relationship between multiple behavioural variables and neural activity in freely behaving mice. In addition, it can help to reveal subtle aspects of natural behavior, such as sound-evoked whisking movements and differences in head-eye coordination between behavioral states.</p></sec><sec id="s2f" hwp:id="sec-8"><title hwp:id="title-14">Eye position depends on head orientation in freely moving mice</title><p hwp:id="p-29">We wondered if the broader distribution of eye positions in actively exploring mice (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-11" hwp:rel-id="F5">Figure 5G</xref> and <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6A</xref>, top) compared to quiescent mice (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-12" hwp:rel-id="F5">Figure 5G</xref>) or head-restrained mice moving on a cylindrical treadmill (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6A</xref>, bottom, Movie S5) was related to the larger range of head orientations during active exploration (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-13" hwp:rel-id="F5">Figure 5F</xref>). Previous results in head-restrained mice (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Andreescu et al., 2005</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Oommen and Stahl, 2008</xref>) and freely moving rats (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-3" hwp:rel-id="ref-69">Wallace et al., 2013</xref>) have suggested that average eye position varies systematically with orientation of the head. Indeed, in head-restrained, passively rotated mice, eye position varies systematically with head pitch and roll (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">Oommen and Stahl, 2008</xref>). Therefore, we used head-mounted accelerometers to measure head orientation (pitch and roll) (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6B,C</xref> and Methods).</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6 xref-fig-6-7 xref-fig-6-8 xref-fig-6-9 xref-fig-6-10 xref-fig-6-11 xref-fig-6-12 xref-fig-6-13 xref-fig-6-14 xref-fig-6-15 xref-fig-6-16 xref-fig-6-17"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-6"><title hwp:id="title-15">Systematic relationships between eye position and head orientation in freely moving mice.</title><p hwp:id="p-30">(A) Measured eye positions (red dots) in a freely moving mouse (top) and in the same mouse during head fixation on a cylindrical treadmill (bottom). (B) Method for simultaneous recording of eye position and head acceleration. (C) Head orientation (pitch/roll) was computed from low-pass-filtered head acceleration signals and was used to train models to predict eye position (arrows). (D) Measured eye positions compared to head-orientation-based predictions of a linear model. Model parameters were determined using training data different from the test data shown here. (E) Fraction of variance in eye position explained by head orientation, based on cross-validated predictions of linear (light gray) or nonlinear (dark gray) model. Top, horizontal eye position. Bottom, vertical eye position. 20 recordings in 3 mice (<italic toggle="yes">n</italic> = 8, 6, 6 in mouse 1,2,3 respectively, 10 minutes each). (F) Fraction of variance in eye position explained by head orientation using the nonlinear model in light (<italic toggle="yes">n</italic> = 10 recordings) and dark (<italic toggle="yes">n</italic> = 4 recordings) conditions (all sessions from one mouse, 10 minutes each). (G) Horizontal (blue lines) and vertical eye position (red lines) as a function of head pitch. Dark and pale lines show interaction with head roll: " 0°", 15° &lt; head roll <italic toggle="yes">&lt;</italic> 15°; "&lt;-15°", head roll <italic toggle="yes">&lt;</italic> 15°; "&gt;15°", head roll <italic toggle="yes">&gt;</italic> 15°. (H) Illustration of systematic dependence of horizontal and vertical eye position on head pitch, for pitch = 0°(top) and pitch = 25°(bottom). Eye and eye coordinate system (h/v) rotates with head. (I,J) The same as in G,H but as a function of head roll, and with dark and pale lines showing interaction with head pitch.</p></caption><graphic xlink:href="294397_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-31">First, we examined the accuracy with which head pitch and roll predicted eye position (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Figure 6C</xref>). Regression models based on these two variables were able to capture a large fraction of the variation in horizontal and vertical eye positions (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Figure 6D,E</xref>; see also Methods) For a simple linear model, cross-validated explained variance between measured and predicted eye position was 52% for horizontal and 79% for vertical eye position; for a nonlinear model (see Methods), explained variance was 64% and 84% respectively (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Figure 6E</xref>). Results were consistent over multiple months within and across mice, as indicated by the stability of regression model weights (<xref rid="figS6" ref-type="fig" hwp:id="xref-fig-14-1" hwp:rel-id="F14">Figure S6A</xref>). Explained variances were comparable in light or dark conditions (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-7" hwp:rel-id="F6">Figure 6F</xref>; see Methods for details), indicating that this effect of head orientation on eye position was driven by vestibular input or efferent signals rather than visual input (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">Andreescu et al., 2005</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-3" hwp:rel-id="ref-42">Oommen and Stahl, 2008</xref>).</p><p hwp:id="p-32">Model predictions of eye position based on head pitch and roll were significantly more accurate for vertical than horizontal eye position (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-8" hwp:rel-id="F6">Figure 6E</xref>; Wilcoxon signed-rank test, <italic toggle="yes">P</italic>=2 <italic toggle="yes">•</italic> 10<sup><italic toggle="yes">−</italic>6</sup> for linear model, <italic toggle="yes">P</italic>=1 <italic toggle="yes">•</italic> 10<sup><italic toggle="yes">−</italic>5</sup> for nonlinear model). We wondered if the horizontal eye position might be more affected than the vertical by correlated movements across the two eyes independent of head orientation and used a dual-camera system to monitor both eyes simultaneously (<xref rid="figS6" ref-type="fig" hwp:id="xref-fig-14-2" hwp:rel-id="F14">Figure S6</xref> and Movie S1). We then trained predictive models on data from each eye individually and found that the interocular error correlation (the correlation between variability in eye position not explained by pitch and roll of both eyes) was significantly stronger for horizontal than vertical eye position (<italic toggle="yes">cc</italic> = 0.72 horizontal, <italic toggle="yes">cc</italic> = 0.11 vertical; Wilcoxon signed rank text, <italic toggle="yes">P</italic> = 0.002; <italic toggle="yes">n</italic> = 10 recordings in one mouse, 10 minutes each).</p><p hwp:id="p-33">We further asked if rotational head movements around the gravity axis (yaw), which are not well captured by the head-mounted linear accelerometer, might also account for the apparently weaker dependence of horizontal than vertical eye position on head orientation. To test this, we added a gyroscope to the implant (see Methods). Including rotations about the yaw axis increased the variance explained by the linear and nonlinear models by approximately 0.10 in horizontal and 0.02 in vertical eye position (<xref rid="figS6" ref-type="fig" hwp:id="xref-fig-14-3" hwp:rel-id="F14">Figure S6D</xref>), confirming some contribution of head yaw movements to prediction of horizontal eye position. The linear weights associated with the yaw signal were also remarkably similar across recordings (<xref rid="figS6" ref-type="fig" hwp:id="xref-fig-14-4" hwp:rel-id="F14">Figure S6B</xref>). In three recordings in the mouse with dual-camera implants and gyroscope, we found that interocular error correlation in the horizontal direction increased from 0.72 (head pitch/roll only) to 0.78 (including yaw as covariate) with no change in interocular error correlation in the vertical direction (0.12). Thus, coupled variation of eye position unexplained by orientation or rotation occurs primarily in the horizontal direction and may be caused by correlated eye movements not dependent on head movement, for example during resetting eye movements (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">van Alphen et al., 2001</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Stahl, 2004</xref>) or continuous drift towards a resting eye position (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-2" hwp:rel-id="ref-64">van Alphen et al., 2001</xref>; <xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-4" hwp:rel-id="ref-69">Wallace et al., 2013</xref>).</p><p hwp:id="p-34"><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-9" hwp:rel-id="F6">Figure 6G,I</xref> summarizes the effects of head orientation on eye position. Both horizontal and vertical eye position varied systematically (and approximately linearly) with head pitch (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-10" hwp:rel-id="F6">Figure 6G</xref>) while vertical eye position was primarily affected by head roll (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-11" hwp:rel-id="F6">Figure 6I</xref>), consistent with reports in head-fixed mice (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-4" hwp:rel-id="ref-42">Oommen and Stahl, 2008</xref>) and freely moving rats (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-5" hwp:rel-id="ref-69">Wallace et al., 2013</xref>). Predictions of horizontal eye position were further improved by incorporating head yaw signals from a head-mounted gyroscope (<xref rid="figS6" ref-type="fig" hwp:id="xref-fig-14-5" hwp:rel-id="F14">Figure S6E</xref>). These results indicate that eye position is closely linked to head orientation in freely moving mice, even in the dark and even when the animals are exploring objects in enriched environments (Movie S6).</p></sec><sec id="s2g" hwp:id="sec-9"><title hwp:id="title-16">Rapid eye movements are strongly linked to head movements in freely moving mice</title><p hwp:id="p-35">We next investigated the relationship between eye and head dynamics. Angular head velocity was measured with the head-mounted gyroscope described above. Eye speed measurements taken around the time of increases in head rotation speed revealed a close correspondence between the temporal profiles of eye movements and head movements (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7A</xref>), with eye movements on average in opposite directions to head movements (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7B</xref>). These results are consistent with the observed dependence of eye position on head orientation (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-12" hwp:rel-id="F6">Figure 6G-J</xref>) and with the expected effects of the vestibulo-ocular reflex (VOR; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-2" hwp:rel-id="ref-60">Stahl, 2004</xref>).</p><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6 xref-fig-7-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-7"><title hwp:id="title-17">Coupling between eye movements and head movements.</title><p hwp:id="p-36">(A) Dynamics of head movement (top) and eye movement (bottom) during head movement initiation. Head rotation speed was measured using a gyroscope attached to the implant; eye speed computed from pupil positions. Traces were aligned to the onset of head movement (rotational speed ≥ 15°/s with at least 0.5 s of no movement before onset). Plots show mean SEM for <italic toggle="yes">n</italic> = 160 head movement events in one mouse, recorded in 14 different 10-minute sessions across more than 4 months. Inset shows average cross-correlation between head and eye speed; note peak at zero time lag. (B) Top: average horizontal eye velocity as a function of head velocity about the yaw axis. Directions as shown in inset. Bottom: average vertical eye velocity as a function of head velocity about the roll axis. In both directions, eye movements counteract head rotations. Plots show mean SEM (smaller than line width). Same dataset as in A. (C) Rapid eye movements occurring in the absence of head movements. Example traces showing magnitude of head acceleration computed from accelerometer signals (top), horizontal/vertical eye positions (middle), and eye speed computed from eye positions (bottom). Saccadic-like eye movements occurring in the absence of head movements (thin vertical lines) were identified by detecting eye movements with peak eye speed <italic toggle="yes">&gt;</italic>250°/s which occurred when head movements were below a fixed threshold (0.0625 g). (D) Cumulative probability of the time between detected saccadic-like eye movements and the preceding head movement (solid dark line). For comparison, cumulative probability is also shown for simulated data (solid gray line) with the same saccadic-like eye movement rate but with saccades occurring at random times within the recorded head-still times (dashed line). Saccadic-like eye movements were significantly more likely to occur soon after a head movement than would be expected by chance (Kolmogorov-Smirnov test, <italic toggle="yes">P</italic> = 3.5 10<sup><italic toggle="yes">−</italic>8</sup>). Same dataset as in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-13" hwp:rel-id="F6">Figure 6</xref> (20 recordings in 3 mice, 10 minutes each). (E) Same as in D but for the time between saccadic-like eye movements and subsequent head movements. Saccadic-like eye movements were significantly more likely to occur just before a head movement than would be expected by chance (Kolmogorov-Smirnov test, <italic toggle="yes">P</italic> = 2.2 10<sup><italic toggle="yes">−</italic>7</sup>). (F) Changes in horizontal and vertical eye position from 20 ms before to 20 ms after the time of peak speed in saccadic-like eye movements. Saccadic-like eye movements tend to be larger horizontally than vertically. Same dataset as in D and E. (G) Same as in F but for mice head-fixed on a cylindrical treadmill (4 recordings in 2 mice, 10 minutes each).</p></caption><graphic xlink:href="294397_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-37">Despite this close overall coupling between head and eye movements, saccadic-like (<italic toggle="yes">&gt;</italic>250°/s, see for example <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Sakatani and Isa, 2007</xref>) eye movements were occasionally observed in the absence of head movements (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Figure 7C</xref>), occurring at an average rate of 0.044 Hz during head-still times. Moreover, these saccadic-like eye movements were not uniformly distributed during head-still times, but were significantly more likely to occur right before or after a head movement (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Figure 7D,E</xref>). Saccadic-like eye movements were qualitatively similar in freely moving animals and head-fixed mice. <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Figure 7F,G</xref> shows the distribution of eye displacements in the horizontal and vertical direction for saccadic-like eye movements in freely moving and head-fixed mice, respectively. Interestingly, the largest eye displacements in freely moving mice were observed in the horizontal direction, consistent with the pattern in head-fixed animals. In freely moving mice, however, the range of horizontal eye displacements was slightly reduced (median movement magnitude 9.9°and 17.7°, respectively; Wilcoxon test, <italic toggle="yes">P</italic> &lt; 3 <italic toggle="yes">•</italic> 10<sup><italic toggle="yes">−</italic>8</sup>), perhaps reflecting greater reliance on head movements for gaze shifts in freely moving animals.</p><p hwp:id="p-38">We conclude that eye movements are generally closely coupled to head movements in freely moving mice. Occasionally, the eye moves in the absence of head movement - but this typically happens just before or after a head movement. Together with the previous observation that average eye position is closely linked to head orientation even during active exploration, these results indicate strong interactions between eye and head movements at both fast and slow timescales in freely moving mice.</p></sec><sec id="s2h" hwp:id="sec-10"><title hwp:id="title-18">Visual cortex activity is modulated by head movements in the dark</title><p hwp:id="p-39">When combined with an implanted neural recording device, the head-mounted camera and motion sensor make it possible to investigate how brain activity is modulated during natural movements in freely moving mice. Previous work has indicated that locomotion modulates visual cortical activity in head-restrained mice (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Saleem et al., 2013</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-4" hwp:rel-id="ref-51">Reimer et al., 2014</xref>). We wondered whether head movements would evoke distinct patterns of activity in visual cortex (V1), given that V1 receives substantial vestibular input accompanying eye movements (<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-2" hwp:rel-id="ref-50">Rancz et al., 2015</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">Vclez-Fort et al., 2018</xref>) along with inputs from many other non-primary sensory areas (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Leinweber et al., 2017</xref>). We measured pupil, whisker pad, and head movements along with neural activity in single cells in V1 while animals freely explored a circular environment (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8A</xref>) in the dark (to exclude the possibility of uncontrolled visual inputs during head movement). We tracked the body of the mouse and excluded periods of gross body movement (<italic toggle="yes">≥</italic> 1 cm/s) to analyze head movements that were not accompanied by locomotion (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Figure 8B</xref> and Methods).</p><fig id="fig8" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3 xref-fig-8-4 xref-fig-8-5 xref-fig-8-6 xref-fig-8-7 xref-fig-8-8 xref-fig-8-9 xref-fig-8-10 xref-fig-8-11 xref-fig-8-12 xref-fig-8-13 xref-fig-8-14 xref-fig-8-15"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8:</label><caption hwp:id="caption-8"><title hwp:id="title-19">Head movement-related modulation of firing in visual cortex.</title><p hwp:id="p-40">(A) Chronic tetrode implant, head-mounted camera system, and head-mounted accelerometer were used to record neural activity in primary visual cortex (V1), eye positions, whisker pad movements, and head movements while mice explored a circular environment in the dark. (B) Top, body position and speed were tracked using an external camera. Middle, periods when body speed exceeded 1 cm/s (gray rectangle) were excluded from consideration in order to focus on head movements occuring without locomotion. Bottom, a head movement episode (red area) was defined as a period when body speed was less than 1 cm/s and head movement was above threshold (dashed line) following at least 0.5 seconds below threshold (before head movement onset). (C) Raster plots for three simultaneously recorded V1 cells, showing spike times relative to head movement onset. Rasters are displayed vertically according to onset index (i.e., time order) within recording (left axis). Red histograms show the average spike rate across trials (right axis). For all three cells, firing rate was significantly modulated by head movement (Wilcoxon signed-rank test, pre versus post movement onset; <italic toggle="yes"><italic toggle="yes">P</italic> &lt;</italic> 0.001). (D) Raster plots for the same cells as in C but aligned to locomotion onset (threshold 1 cm/s) for mouse head-fixed on a cylindrical treadmill. (E) Division of eye movement onsets into those well-predicted by a model based on head orientation (correlation ≥ 0.5) and other eye movements (correlation ≤ 0.5). (F) Raster plots and firing rate histograms for the same three cells as in C, for the two types of eye movement onsets shown in E. Spike train data same as in C, but including only head-movement onset events for which the eye movement could be reliably extracted. Rasters are grouped vertically by eye movement onset type as indicated by colored y-axis bars ("predictable", black; "other", yellow). Spike rate histograms shown overlaid using same color convention. (G) Summary of modulation indices (MI; see text) for V1 activity when aligned to head movement onsets, eye movement onsets that were predictable from head acceleration, or eye movement onsets that were not predictable from head orientation. Plot shows mean <italic toggle="yes">±</italic> SEM across 16 recordings (20-40 minutes each) in 3 mice (74 cells with at least 2 spikes per second).</p></caption><graphic xlink:href="294397_fig8" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-41">Activity was tightly locked to head movement onsets in many visual cortical cells. In total, 55% (41/74) of V1 cells were significantly modulated by head movement (see Methods for details on spike sorting and data extraction). We observed both increases and decreases in firing rate even for simultaneously recorded cells (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Figure 8C</xref>). To quantify the movement-related response modulation of individual cells, we computed a modulation index MI=(Post-Pre)/(Post+Pre), where Post and Pre are the mean firing rates for 1 s after and before movement onset, respectively. As shown for the three simultaneously recorded cells in <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-4" hwp:rel-id="F8">Figure 8C</xref> and <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-5" hwp:rel-id="F8">D</xref>, V1 response modulation at the onset of head movements without locomotion in unrestrained mice could be similar to or different from V1 response modulation at the onset of locomotion in the same animals head-fixed on a cylindrical treadmill. There was no significant correlation between the firing patterns of 74 V1 cells recorded in both conditions in 3 different mice (Wald test, <italic toggle="yes">P</italic> = 0.18; <xref rid="figS7" ref-type="fig" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Figure S7A</xref>). This observation suggests that head movements can affect firing rates of visual cortex neurons independently of locomotion.</p><p hwp:id="p-42">Head movements were tightly coupled to eye movements in freely moving mice (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-7" hwp:rel-id="F7">Figure 7A</xref>). To disentangle the effects of these variables on V1 responses, we first asked if eye movements differentially affected V1 activity in the dark. We extracted the first eye movement in the period around head movement onset by measuring optical flow of the pupil edges in the dark (Methods and <xref rid="figS8" ref-type="fig" hwp:id="xref-fig-16-1" hwp:rel-id="F16">Figure S8</xref>). We then used the eye position models described above to predict eye movements from head accelerometer data. Head/eye movement onsets were classified by whether the initial eye movement was in the same direction as the predicted eye movement based on head movement (correlation <italic toggle="yes">≥</italic> 0.5) or in another direction (correlation <italic toggle="yes">&lt;</italic> 0.5). Approximately half of eye movements were in the direction predicted by the model based on head movement (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-6" hwp:rel-id="F8">Figure 8E</xref>). Sorting the trials of the cells in <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-7" hwp:rel-id="F8">Figure 8C</xref> according to whether or not the eye movement was predictable from the head movement did <italic toggle="yes">not</italic> reveal any systematic differences between the two conditions (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-8" hwp:rel-id="F8">Figure 8F</xref>). There was no significant difference in absolute modulation indices between unsorted and sorted conditions (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-9" hwp:rel-id="F8">Figure 8G</xref>) across all recorded cells with at least 20 trials in all conditions (N=37; Wilcoxon signed-rank test, <italic toggle="yes">P</italic> = 0.8). Indeed, MI values for spike trains aligned to head movement onsets accompanied by either "predictable" or "other" eye movements were both statistically indistinguishable from MI values obtained when the same spike trains were aligned to onsets of the head movements regardless of eye movements (<xref rid="figS7" ref-type="fig" hwp:id="xref-fig-15-2" hwp:rel-id="F15">Figure S7B,C</xref>; <italic toggle="yes">P</italic> = 0. 06 for "head mvmt" vs "predictable", <italic toggle="yes">P</italic> = 0.15 for "head mvmt" vs "other"; Wilcoxon signed-rank test). These results suggest that eye movements did not have a differential impact on the observed modulation of V1 responses, beyond that predicted by head movements alone.</p><p hwp:id="p-43">We next asked whether whisker movements differentially affected modulation of neural responses in V1. Whisking was not as strongly coupled to head movements as eye movements. We aligned V1 recordings to the onset of whisking and observed that the magnitude of modulation was generally reduced (Wilcoxon signed rank test, <italic toggle="yes">P</italic> = 0.001) compared to alignment to head movement. Furthermore, alignment to the onset of whisking in the absence of head motion resulted in an even greater reduction (Wilcoxon signed rank test, <italic toggle="yes"><italic toggle="yes">P</italic> &lt;</italic> 3 <italic toggle="yes">•</italic> 10<sup><italic toggle="yes">−</italic>6</sup>; <xref rid="figS7" ref-type="fig" hwp:id="xref-fig-15-3" hwp:rel-id="F15">Figure S7D-F</xref>). This indicates that head movements modulate V1 activity more strongly than whisking movements in most recorded cells.</p><p hwp:id="p-44">We conclude that head movements modulate V1 activity in freely moving mice, even in the dark and in the absence of locomotion. Moreover, while head, eye, and whisker movements are coupled in freely moving mice, modulation of V1 activity by head movements cannot be fully explained in terms of coupled eye movements or whisking alone.</p></sec></sec><sec id="s3" hwp:id="sec-11"><title hwp:id="title-20">Discussion</title><p hwp:id="p-45">The mouse is a prominent animal model in neuroscience, but behavioral monitoring in freely moving mice has been limited by the absence of video tracking methods in head-centered coordinates. To overcome this limitation we developed a miniature ultra-lightweight head-mounted video camera system, and combined it with movement sensors to monitor multiple behavioral variables including pupil size and eye position, head, whisker pad and body movements, and integrated it with a chronic multielectrode implant to record neural activity in freely moving animals (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Figure 1</xref>). The camera system is stable, enabling precise and continuous monitoring of behavioral variables and minimizing the amount of postprocessing required to extract the variables of interest. Inter-frame image movement was less than 1 pixel (corresponding to about 40 <italic toggle="yes">µ</italic>m) in about 99% of all video images, even when the mice were grooming, exploring complex environments, or interacting with objects in the environment (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Figure 3</xref> and Movie S2). Crucially, mouse behavior was similar with and without the camera system, allowing accurate monitoring of pupil size, eye position, whisking, and other variables during natural behaviors. The operation of the camera system did not affect the quality of simultaneous electrophysiological recordings.</p><p hwp:id="p-46">This new head-mounted camera system significantly expands the range of scientific questions that can be addressed in freely moving mice. Ethological studies could reveal the precise characteristics of behavior such as eye movements, whisking, and other motor outputs. Sensory neuroscientists could use the system to validate experimental results obtained under conditions of head or body restraint - while directly studying sensory processing under more natural conditions. Studies of non-sensory brain areas, including associative and motor areas, could identify sources of behavioral variability that drive neural activity but have been previously hard to measure. Mouse models of disease could be examined to establish or to exclude deficits in eye movements, whisking or other motor outputs.</p><p hwp:id="p-47">Here we have shown that the head-mounted camera system can provide new insights into the relationships between eye, head, and whisking movements and neural activity in freely moving mice. In many animals, eye and head movements are intimately related and both are used for orienting gaze towards salient objects (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Land, 2015</xref>). However, very little is known about their coordination in mice, even though this information could provide important general insights into how non-foveate animals use vision during natural behavior. We observed prominent changes in the distributions of both head orientation and eye position in different behavioral states in freely moving mice. When we quantified this relationship using predictive models, we discovered that a large fraction of the variation in eye position could be predicted from head orientation, consistent with findings from a previous study in the freely moving rat (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-6" hwp:rel-id="ref-69">Wallace et al., 2013</xref>). Our results suggest that freely moving mice stabilize their gaze relative to the horizontal plane. Crucially, our data show that this gaze stabilization does not only happen on average but also at a fine temporal resolution (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-14" hwp:rel-id="F6">Figure 6</xref> and Movie S6), and therefore may play an important role in mouse vision. We also found that the systematic relationships between eye position and head orientation were preserved across months, across mice, and in the dark as well as the light, suggesting that head-orientation-related changes in eye position are driven by vestibular rather than visual input (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-5" hwp:rel-id="ref-42">Oommen and Stahl, 2008</xref>).</p><p hwp:id="p-48">While models based on head orientation and rotational head movements were able to explain most variation in eye position particularly in the vertical direction, there was still considerable unexplained variance in the horizontal direction (about 10-50%). By using two head-mounted cameras we found that horizontal eye positions not explained by head orientation were strongly correlated across both eyes, even after taking into account rotational movements of the head. Whether these correlations resulted from resetting eye movements not locked to head movements (e.g. <xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-3" hwp:rel-id="ref-64">van Alphen et al., 2001</xref>) or active shifts in gaze will need to be determined in future work. Most of the present experiments were done in a circular environment without salient visual objects. However, in enriched environments it appeared that mice did not orient their eyes towards objects even when they actively explored them (Movie S1). Moreover, even saccadic-like eye movements occurring without a coincident head movement were significantly more likely to occur just before or just after a head movement than would have been expected by chance. Future experiments might use the camera system to investigate whether freely moving mice encountering highly salient or moving visual objects produce more eye movements that are not coupled to head orientation or head movements. Furthermore, monitoring not only the eyes but also the environment using a head-mounted camera facing outward without IR mirror (Movie S1) will help to clarify the link between head and eye movements and visual inputs.</p><p hwp:id="p-49">We also demonstrated how the camera system can be combined with motion sensors and chronic neural recording devices to discover new relationships between motor-related variables and neural activity in the visual cortex. About 55% of V1 cells were modulated by head movements in the absence of locomotion, even in the dark, i.e., in the absence of any visual input. Both enhancement and suppression of firing were seen, even for cells recorded at the same time. These results were not explained by variations in eye movements or whisking. Recent work has demonstrated that locomotion can modulate activity in sensory cortex (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Schneider et al., 2014</xref>). For example, in mouse primary visual cortex, neural responses are generally enhanced when head-fixed animals run on a treadmill compared to when they are stationary (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-3" hwp:rel-id="ref-40">Niell and Stryker, 2010</xref>); in contrast, in primary auditory cortex, neural responses are typically suppressed by locomotion (<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-2" hwp:rel-id="ref-59">Schneider et al., 2014</xref>). We measured changes in neural activity in primary visual cortex either during head movements in the absence of locomotion when the mouse was freely moving, or during locomotion when the mouse was head-fixed on a cylindrical treadmill. We found that the directions of modulation in the same V1 neuron could be different for locomotion-related and head-movement-related responses, and that there was no significant correlation between the two types of movements. These results demonstrate that modulation of early sensory cortical areas by motor outputs is both more general (i.e., occurring for many forms of movement) and more specific (i.e., manifested differently for different forms of movement) than previously thought. Future work will be needed to identify whether the movement-related signals are used for suppression of sensory coding during self-generated movement (e.g. saccadic suppression, <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Duffy and Burchfiel, 1975</xref>), for the computation of the mismatch between sensory input and expected input (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Keller et al., 2012</xref>), or for the integration of sensory inputs with signals related to spatial navigation (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">Saleem et al., 2013</xref>). We anticipate that important progress can be made by combining our method with new tools for virtual reality in freely moving animals (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Stowers et al., 2017</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Del Grosso et al., 2017</xref>) to provide both detailed behavioral and stimulus control.</p><p hwp:id="p-50">The new system is open-source and we provide all required software and design files. To our knowledge this is the first open-source head-mounted video tracking system for small laboratory animals. The system uses widely available components (e.g., camera sensor, single-board computer and connectors) or 3D-printable parts (camera holder), and the total cost is low (see parts list) which should further promote its adoption. Moreover, this ultra-lightweight system could be easily adapted for use in larger animals, such as rats, ferrets, and monkeys. At the moment the system is tethered, but especially in larger animals it is possible to add batteries to power the system so it can be used in conjunction with wireless recording methods (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Fan et al., 2011</xref>; <xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">Yin et al., 2014</xref>). In the mouse, a major challenge remains the weight of the combination of headposts, cameras, parts for neural recordings, batteries and wireless transmitters, but technical developments in miniaturizing all these components might make entirely wireless head-mounted neural recording and behavioral monitoring systems feasible in the near future. Furthermore, the system is modular and could be integrated with alternative methods for recording neural activity, such as high-density silicon probes (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Jun et al., 2017</xref>) or head-mounted fluorescence microscopes (<xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Zong et al., 2017</xref>), and/or combined with technologies for optogenetic manipulation of neural activity during behavioral monitoring (<xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">Wu et al., 2015</xref>).</p><p hwp:id="p-51">Because the position of the camera and mirror can easily be customized, the view can be modified to include other variables of interest. For example, a small modification to the arm holding the mirror is suffi cient to provide a detailed image of the pinna (Movie S1) to provide insights into how pinna movement contributes to the processing of incoming sounds, e.g., during sound localization, in freely moving animals. The camera could also be used to monitor the movement of single whiskers in head-centered coordinates, as opposed to the whisker pad movements tracked in the current study, without the need for external tracking cameras, computation of absolute position in space, or attachment of markers to single whiskers (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-3" hwp:rel-id="ref-67">Voigts et al., 2008</xref>; <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-2" hwp:rel-id="ref-53">Roy et al., 2011</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">Nashaat et al., 2017</xref>). Finally, the camera system can also be used to capture images of the nose, mouth, and/or paws, to monitor how mice interact with their environment when they explore novel objects (see Movie S1 for a mouse interacting with Lego and foraging) and during social behaviors such as mating and fighting. Thus, the system has the potential to greatly increase the range and scope of experimental questions that can be addressed about natural behaviors in freely moving mice and other small laboratory animals.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-21">Acknowledgements</title><p hwp:id="p-52">We thank Jakob Voigts for providing the drive parts for the initial experiments, and John Stahl and Takashi Kodama for their advice on tracking eye position in the dark. The authors are also grateful to Stephen M. Town and Nicholas A. Lesica for their comments on the manuscript. This work was supported by the Biotechnology and Biological Sciences Research Council (BB/P007201; J.F.L.); Action on Hearing Loss (G77; J.F.L.); the Gatsby Charitable Foundation (M.S.) (GAT3212, J. O’K); the Simons Foundation (SCGB323228; M.S.); the UCL Excellence fellowship (J.P.); and the Wellcome Trust (090843/C/09/Z, 090843/D/09/Z, J. O’K). J.O’K. is a Wellcome Trust Principal Research Fellow (203020/Z/16/Z).</p></ack><sec id="s4" hwp:id="sec-12"><title hwp:id="title-22">Author contributions</title><p hwp:id="p-53">A.F.M and J.P. conceived the project and designed and performed the experiments; A.F.M. designed and constructed the experimental setup and the camera system; A.F.M. performed computational and statistical analyses and prepared figures; J.F.L. supervised animal procedures; M.S. and J.F.L. supervised analysis; All authors discussed the results; A.F.M. and J.P. drafted the manuscript; All authors contributed to the writing of the manuscript.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-23">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Aghajan Z. M."><surname>Aghajan</surname>, <given-names>Z. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Acharya L."><surname>Acharya</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moore J. J."><surname>Moore</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cushman J. D."><surname>Cushman</surname>, <given-names>J. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vuong C."><surname>Vuong</surname>, <given-names>C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Mehta M. R."><surname>Mehta</surname>, <given-names>M. R.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-2">Impaired spatial selectivity and intact phase precession in two-dimensional virtual reality</article-title>. <source hwp:id="source-1">Nature Neuroscience</source>, <volume>18</volume>:<fpage>121</fpage>–<lpage>128</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Andreescu C. E."><surname>Andreescu</surname>, <given-names>C. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="De Ruiter M. M."><surname>De Ruiter</surname>, <given-names>M. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="De Zeeuw C. I."><surname>De Zeeuw</surname>, <given-names>C. I.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="De Jeu M. T. G."><surname>De Jeu</surname>, <given-names>M. T. G.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-3">Otolith deprivation induces optokinetic compensation</article-title>. <source hwp:id="source-2">Journal of Neurophysiology</source>, <volume>94</volume>:<fpage>3487</fpage>–<lpage>3496</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Carandini M."><surname>Carandini</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Churchland A. K."><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-4">Probing perceptual decisions in rodents</article-title>. <source hwp:id="source-3">Nature Neuroscience</source>, <volume>16</volume>(<issue>7</issue>):<fpage>824</fpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Chen G."><surname>Chen</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="King J. A."><surname>King</surname>, <given-names>J. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burgess N."><surname>Burgess</surname>, <given-names>N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="O’Keefe J."><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-5">How vision and movement combine in the hippocampal place code</article-title>. <source hwp:id="source-4">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>110</volume>:<fpage>378</fpage>–<lpage>383</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Chesselet M.-F."><surname>Chesselet</surname>, <given-names>M.-F.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Richter F."><surname>Richter</surname>, <given-names>F.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-6">Modelling of parkinson’s disease in mice</article-title>. <source hwp:id="source-5">The Lancet Neurology</source>, <volume>10</volume>(<issue>12</issue>):<fpage>1108</fpage>–<lpage>1118</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="other" citation-type="journal" ref:id="294397v1.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Del Grosso N. A."><surname>Del Grosso</surname>, <given-names>N. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Graboski J. J."><surname>Graboski</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen W."><surname>Chen</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanco Hernandez E."><surname>Blanco Hernandez</surname>, <given-names>E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Sirota A."><surname>Sirota</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <chapter-title>Virtual reality system for freely-moving rodents</chapter-title>. <source hwp:id="source-6">bioRxiv</source>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Dhawale A. K."><surname>Dhawale</surname>, <given-names>A. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poddar R."><surname>Poddar</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wolff S. B."><surname>Wolff</surname>, <given-names>S. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Normand V. A."><surname>Normand</surname>, <given-names>V. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kopelowitz E."><surname>Kopelowitz</surname>, <given-names>E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Olveczky B. P."><surname>Olveczky</surname>, <given-names>B. P.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-7">Automated long-term recording and analysis of neural activity in behaving animals</article-title>. <source hwp:id="source-7">eLife</source>, <volume>6</volume>:<fpage>e27702</fpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Dombeck D. A."><surname>Dombeck</surname>, <given-names>D. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khabbaz A. N."><surname>Khabbaz</surname>, <given-names>A. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Collman F."><surname>Collman</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adelman T. L."><surname>Adelman</surname>, <given-names>T. L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Tank D. W."><surname>Tank</surname>, <given-names>D. W.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-8">Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title>. <source hwp:id="source-8">Neuron</source>, <volume>56</volume>:<fpage>43</fpage>–<lpage>57</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Domnisoru C."><surname>Domnisoru</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kinkhabwala A. A."><surname>Kinkhabwala</surname>, <given-names>A. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Tank D. W."><surname>Tank</surname>, <given-names>D. W.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-9">Membrane potential dynamics of grid cells</article-title>. <source hwp:id="source-9">Nature</source>, <volume>495</volume>:<fpage>199</fpage>–<lpage>204</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Dubbs A."><surname>Dubbs</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Guevara J."><surname>Guevara</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Yuste R."><surname>Yuste</surname>, <given-names>R.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-10">moco: Fast motion correction for calcium imaging</article-title>. <source hwp:id="source-10">Frontiers in Neuroinformatics</source>, <volume>10</volume>:<fpage>6</fpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Duffy F. H."><surname>Duffy</surname>, <given-names>F. H.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Burchfiel J. L."><surname>Burchfiel</surname>, <given-names>J. L.</given-names></string-name> (<year>1975</year>). <article-title hwp:id="article-title-11">Eye movement-related inhibition of primate visual neurons</article-title>. <source hwp:id="source-11">Brain Research</source>, <volume>89</volume>:<fpage>121</fpage>–<lpage>132</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Fan D."><surname>Fan</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rich D."><surname>Rich</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holtzman T."><surname>Holtzman</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ruther P."><surname>Ruther</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dalley J. W."><surname>Dalley</surname>, <given-names>J. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lopez A."><surname>Lopez</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rossi M. A."><surname>Rossi</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barter J. W."><surname>Barter</surname>, <given-names>J. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Salas-Meza D."><surname>Salas-Meza</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Herwik S."><surname>Herwik</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holzhammer T."><surname>Holzhammer</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morizio J."><surname>Morizio</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Yin H. H."><surname>Yin</surname>, <given-names>H. H.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-12">A wireless multi-channel recording system for freely behaving mice and rats</article-title>. <source hwp:id="source-12">PLOS One</source>, <volume>6</volume>:<fpage>e22033</fpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="confproc" citation-type="confproc" ref:id="294397v1.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Farneback G."><surname>Farneback</surname>, <given-names>G.</given-names></string-name> (<year>2003</year>). <source hwp:id="source-13">Two-frame motion estimation based on polynomial expansion</source>. In <conf-name>Proceedings of the 13th Scandinavian Conference on Image Analysis, LNCS 2749</conf-name>, pages <fpage>363</fpage>–<lpage>370</lpage>, <conf-loc>Gothenburg, Sweden</conf-loc>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Felsen G."><surname>Felsen</surname>, <given-names>G.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dan Y."><surname>Dan</surname>, <given-names>Y.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-13">A natural approach to studying vision</article-title>. <source hwp:id="source-14">Nature Neuroscience</source>, <volume>8</volume>:<fpage>1643</fpage>–<lpage>1646</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Fitzgibbon A."><surname>Fitzgibbon</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pilu M."><surname>Pilu</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Fisher R. B."><surname>Fisher</surname>, <given-names>R. B.</given-names></string-name> (<year>1999</year>). <article-title hwp:id="article-title-14">Direct least square fitting of ellipses</article-title>. <source hwp:id="source-15">IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>21</volume>(<issue>5</issue>):<fpage>476</fpage>–<lpage>480</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Fu Y."><surname>Fu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tucciarone J. M."><surname>Tucciarone</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Espinosa J. S."><surname>Espinosa</surname>, <given-names>J. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sheng N."><surname>Sheng</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Darcy D. P."><surname>Darcy</surname>, <given-names>D. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nicoll R. A."><surname>Nicoll</surname>, <given-names>R. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang Z. J."><surname>Huang</surname>, <given-names>Z. J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Stryker M. P."><surname>Stryker</surname>, <given-names>M. P.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-15">A cortical circuit for gain control by behavioral state</article-title>. <source hwp:id="source-16">Cell</source>, <volume>156</volume>:<fpage>1139</fpage>–<lpage>1152</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Fyhn M."><surname>Fyhn</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Molden S."><surname>Molden</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Witter M. P."><surname>Witter</surname>, <given-names>M. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moser E. I."><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Moser M.-B."><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-16">Spatial representation in the entorhinal cortex</article-title>. <source hwp:id="source-17">Science</source>, <volume>305</volume>:<fpage>1258</fpage>–<lpage>1264</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Götz J."><surname>Götz</surname>, <given-names>J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ittner L. M."><surname>Ittner</surname>, <given-names>L. M.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-17">Animal models of alzheimer’s disease and frontotemporal dementia</article-title>. <source hwp:id="source-18">Nature Reviews Neuroscience</source>, <volume>9</volume>(<issue>7</issue>):<fpage>532</fpage>–<lpage>544</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Hong W."><surname>Hong</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kennedy A."><surname>Kennedy</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burgos-Artizzu X. P."><surname>Burgos-Artizzu</surname>, <given-names>X. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zelikowsky M."><surname>Zelikowsky</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Navonne S. G."><surname>Navonne</surname>, <given-names>S. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perona P."><surname>Perona</surname>, <given-names>P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Anderson D. J."><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-18">Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning</article-title>. <source hwp:id="source-19">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>112</volume>:<fpage>E5351</fpage>–<lpage>E5360</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Juavinett A. L."><surname>Juavinett</surname>, <given-names>A. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erlich J. C."><surname>Erlich</surname>, <given-names>J. C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Churchland A. K."><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-19">Decision-making behaviors: weighing ethology, complexity, and sensorimotor compatibility</article-title>. <source hwp:id="source-20">Current Opinion in Neurobiology</source>, <volume>49</volume>:<fpage>42</fpage>–<lpage>50</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Jun J. J."><surname>Jun</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Steinmetz N. A."><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Siegle J. H."><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Denman D. J."><surname>Denman</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bauza M."><surname>Bauza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barbarits B."><surname>Barbarits</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee A. K."><surname>Lee</surname>, <given-names>A. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anastassiou C. A."><surname>Anastassiou</surname>, <given-names>C. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Andrei A."><surname>Andrei</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Aydin c."><surname>Aydin</surname>, <given-names>c.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barbic M."><surname>Barbic</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blanche T. J."><surname>Blanche</surname>, <given-names>T. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bonin V."><surname>Bonin</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Couto J."><surname>Couto</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dutta B."><surname>Dutta</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gratiy S. L."><surname>Gratiy</surname>, <given-names>S. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gutnisky D. A."><surname>Gutnisky</surname>, <given-names>D. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hausser M."><surname>Hausser</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Karsh B."><surname>Karsh</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ledochowitsch P."><surname>Ledochowitsch</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lopez C. M."><surname>Lopez</surname>, <given-names>C. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mitelut C."><surname>Mitelut</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Musa S."><surname>Musa</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Okun M."><surname>Okun</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pachitariu M."><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Putzeys J."><surname>Putzeys</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rich P. D."><surname>Rich</surname>, <given-names>P. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rossant C."><surname>Rossant</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sun W.-L."><surname>Sun</surname>, <given-names>W.-L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Svoboda K."><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carandini M."><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris K. D."><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koch C."><surname>Koch</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Keefe J."><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Harris T. D."><surname>Harris</surname>, <given-names>T. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-20">Fully integrated silicon probes for high-density recording of neural activity</article-title>. <source hwp:id="source-21">Nature</source>, <volume>551</volume>:<fpage>232</fpage>–<lpage>236</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Keller G. B."><surname>Keller</surname>, <given-names>G. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bonhoeffer T."><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hubener M."><surname>Hubener</surname>, <given-names>M.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-21">Sensorimotor mismatch signals in primary visual cortex of the behaving mouse</article-title>. <source hwp:id="source-22">Neuron</source>, <volume>74</volume>(<issue>5</issue>):<fpage>809</fpage>–<lpage>815</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Kim C. K."><surname>Kim</surname>, <given-names>C. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adhikari A."><surname>Adhikari</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Deisseroth K."><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-22">Integration of optogenetics with complementary methodologies in systems neuroscience</article-title>. <source hwp:id="source-23">Nature Reviews Neuroscience</source>, <volume>18</volume>(<issue>4</issue>):<fpage>222</fpage>–<lpage>235</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="other" citation-type="journal" ref:id="294397v1.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Kingma D. P."><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ba J."><surname>Ba</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). <chapter-title>Adam: A method for stochastic optimization</chapter-title>. <source hwp:id="source-24">In Proceedings of the 3rd International Conference on Learning Representations {ICLR)</source>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Krakauer J. W."><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ghazanfar A. A."><surname>Ghazanfar</surname>, <given-names>A. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gomez-Marin A."><surname>Gomez-Marin</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="MacIver M. A."><surname>MacIver</surname>, <given-names>M. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Poeppel D."><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-23">Neuroscience needs behavior: Correcting a reductionist bias</article-title>. <source hwp:id="source-25">Neuron</source>, <volume>93</volume>:<fpage>480</fpage>–<lpage>490</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Kretschmer F."><surname>Kretschmer</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sajgo S."><surname>Sajgo</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kretschmer V."><surname>Kretschmer</surname>, <given-names>V.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Badea T. C."><surname>Badea</surname>, <given-names>T. C.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-24">A system to measure the optokinetic and optomotor response in mice</article-title>. <source hwp:id="source-26">Journal of Neuroscience Methods</source>, <volume>256</volume>:<fpage>91</fpage>–<lpage>105</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Kretschmer F."><surname>Kretschmer</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tariq M."><surname>Tariq</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chatila W."><surname>Chatila</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu B."><surname>Wu</surname>, <given-names>B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Badea T. C."><surname>Badea</surname>, <given-names>T. C.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-25">Comparison of optomotor and optokinetic reflexes in mice</article-title>. <source hwp:id="source-27">Journal of Neurophysiology</source>, <volume>118</volume>:<fpage>300</fpage>–<lpage>316</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Land M. F."><surname>Land</surname>, <given-names>M. F.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-26">Eye movements of vertebrates and their relation to eye form and function</article-title>. <source hwp:id="source-28">Journal of Comparative Physiology A</source>, <volume>201</volume>(<issue>2</issue>):<fpage>195</fpage>–<lpage>214</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Leinweber M."><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ward D. R."><surname>Ward</surname>, <given-names>D. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sobczak J. M."><surname>Sobczak</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Attinger A."><surname>Attinger</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Keller G. B."><surname>Keller</surname>, <given-names>G. B.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-27">A sensorimotor circuit in mouse cortex for visual flow predictions</article-title>. <source hwp:id="source-29">Neuron</source>, <volume>95</volume>:<fpage>1420</fpage>–<lpage>1432.e5</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Luo L."><surname>Luo</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Callaway E. M."><surname>Callaway</surname>, <given-names>E. M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Svoboda K."><surname>Svoboda</surname>, <given-names>K.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-28">Genetic dissection of neural circuits</article-title>. <source hwp:id="source-30">Neuron</source>, <volume>57</volume>:<fpage>634</fpage>–<lpage>660</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Machado A. S."><surname>Machado</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Darmohray D. M."><surname>Darmohray</surname>, <given-names>D. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fayad J."><surname>Fayad</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marques H. G."><surname>Marques</surname>, <given-names>H. G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Carey M. R."><surname>Carey</surname>, <given-names>M. R.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-29">A quantitative framework for whole-body coordination reveals specific deficits in freely walking ataxic mice</article-title>. <source hwp:id="source-31">eLife</source>, <fpage>4</fpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.32" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="MacKay D. J."><surname>MacKay</surname>, <given-names>D. J.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-30">Bayesian non-linear modeling for the prediction competition</article-title>. <source hwp:id="source-32">In Maximum Entropy and Bayesian Methods</source>, pages <fpage>221</fpage>–<lpage>234</lpage>. <month>Springer</month>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Marz T."><surname>Marz</surname>, <given-names>T.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-31">Image inpainting based on coherence transport with adapted distance functions</article-title>. <source hwp:id="source-33">SIAM Journal on Imaging Sciences</source>, <volume>4</volume>(<issue>4</issue>):<fpage>981</fpage>–<lpage>1000</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2 xref-ref-34-3 xref-ref-34-4"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="McGinley M. J."><surname>McGinley</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="David S. V."><surname>David</surname>, <given-names>S. V.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="McCormick D. A."><surname>McCormick</surname>, <given-names>D. A.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-32">Cortical membrane potential signature of optimal states for sensory signal detection</article-title>. <source hwp:id="source-34">Neuron</source>, <volume>87</volume>(<issue>1</issue>):<fpage>179</fpage>–<lpage>192</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Minderer M."><surname>Minderer</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harvey C. D."><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Donato F."><surname>Donato</surname>, <given-names>F.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Moser E. I."><surname>Moser</surname>, <given-names>E. I.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-33">Neuroscience: Virtual reality explored</article-title>. <source hwp:id="source-35">Nature</source>, <volume>533</volume>:<fpage>324</fpage>–<lpage>325</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="confproc" citation-type="confproc" ref:id="294397v1.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Mizell D."><surname>Mizell</surname>, <given-names>D.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-34">Using gravity to estimate accelerometer orientation</article-title>. In <conf-name>Proceedings of the 7th IEEE International Symposium on Wearable Computers, ISWC ‘03</conf-name>, pages <fpage>252</fpage>–, <conf-loc>Washington, DC, USA. IEEE Computer Society</conf-loc>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Moore C. I."><surname>Moore</surname>, <given-names>C. I.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-35">Frequency-dependent processing in the vibrissa sensory system</article-title>. <source hwp:id="source-36">Journal of Neurophysiology</source>, <volume>91</volume>:<fpage>2390</fpage>–<lpage>2399</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Nashaat M. A."><surname>Nashaat</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oraby H."><surname>Oraby</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pena L. B."><surname>Pena</surname>, <given-names>L. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dominiak S."><surname>Dominiak</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Larkum M. E."><surname>Larkum</surname>, <given-names>M. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Sachdev R. N. S."><surname>Sachdev</surname>, <given-names>R. N. S.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-36">Pixying behavior: A versatile real-time and post hoc automated optical tracking method for freely moving and head fixed animals</article-title>. <source hwp:id="source-37">eNeuro</source>, <fpage>4</fpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Nestler E. J."><surname>Nestler</surname>, <given-names>E. J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hyman S. E."><surname>Hyman</surname>, <given-names>S. E.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-37">Animal models of neuropsychiatric disorders</article-title>. <source hwp:id="source-38">Nature Neuroscience</source>, <volume>13</volume>(<issue>10</issue>):<fpage>1161</fpage>–<lpage>1169</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2 xref-ref-40-3"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Niell C. M."><surname>Niell</surname>, <given-names>C. M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Stryker M. P."><surname>Stryker</surname>, <given-names>M. P.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-38">Modulation of visual responses by behavioral state in mouse visual cortex</article-title>. <source hwp:id="source-39">Neuron</source>, <volume>65</volume>:<fpage>472</fpage>–<lpage>479</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1 xref-ref-41-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="O’Keefe J."><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dostrovsky J."><surname>Dostrovsky</surname>, <given-names>J.</given-names></string-name> (<year>1971</year>). <article-title hwp:id="article-title-39">The hippocampus as a spatial map. preliminary evidence from unit activity in the freely-moving rat</article-title>. <source hwp:id="source-40">Brain Research</source>, <volume>34</volume>:<fpage>171</fpage>–<lpage>175</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2 xref-ref-42-3 xref-ref-42-4 xref-ref-42-5 xref-ref-42-6"><citation publication-type="supplementary-material" citation-type="journal" ref:id="294397v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Oommen B. S."><surname>Oommen</surname>, <given-names>B. S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Stahl J. S."><surname>Stahl</surname>, <given-names>J. S.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-40">Eye orientation during static tilts and its relationship to spontaneous head pitch in the laboratory mouse</article-title>. <source hwp:id="source-41">Brain Research</source>, <volume>1193</volume>(<issue>Supplement C</issue>):<issue>57</issue> <fpage>66</fpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1 xref-ref-43-2 xref-ref-43-3"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Pasquet M. O."><surname>Pasquet</surname>, <given-names>M. O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tihy M."><surname>Tihy</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gourgeon A."><surname>Gourgeon</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pompili M. N."><surname>Pompili</surname>, <given-names>M. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Godsil B. P."><surname>Godsil</surname>, <given-names>B. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lena C."><surname>Lena</surname>, <given-names>C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dugue G. P."><surname>Dugue</surname>, <given-names>G. P.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-41">Wireless inertial measurement of head kinematics in freely-moving rats</article-title>. <source hwp:id="source-42">Scientific reports</source>, <volume>6</volume>:<fpage>35689</fpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Payne H. L."><surname>Payne</surname>, <given-names>H. L.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Raymond J. L."><surname>Raymond</surname>, <given-names>J. L.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-42">Magnetic eye tracking in mice</article-title>. <source hwp:id="source-43">eLife</source>, <fpage>6</fpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Pedregosa F."><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varoquaux G."><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gramfort A."><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Michel V."><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thirion B."><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grisel O."><surname>Grisel</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blondel M."><surname>Blondel</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prettenhofer P."><surname>Prettenhofer</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weiss R."><surname>Weiss</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dubourg V."><surname>Dubourg</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vanderplas J."><surname>Vanderplas</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Passos A."><surname>Passos</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cournapeau D."><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brucher M."><surname>Brucher</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perrot M."><surname>Perrot</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Duchesnay E."><surname>Duchesnay</surname>, <given-names>E.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-43">Scikit-learn: Machine learning in Python</article-title>. <source hwp:id="source-44">Journal of Machine Learning Research</source>, <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="other" citation-type="journal" ref:id="294397v1.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Peirson S. N."><surname>Peirson</surname>, <given-names>S. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brown L. A."><surname>Brown</surname>, <given-names>L. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pothecary C. A."><surname>Pothecary</surname>, <given-names>C. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benson L. A."><surname>Benson</surname>, <given-names>L. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Fisk A. S."><surname>Fisk</surname>, <given-names>A. S.</given-names></string-name> (<year>2017</year>). <chapter-title>Light and the laboratory mouse</chapter-title>. <source hwp:id="source-45">Journal of Neuroscience Methods</source>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Perkon I."><surname>Perkon</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kosir A."><surname>Kosir</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Itskov P. M."><surname>Itskov</surname>, <given-names>P. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tasic J."><surname>Tasic</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Diamond M. E."><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-44">Unsupervised quantification of whisking and head movement in freely moving rodents</article-title>. <source hwp:id="source-46">Journal of Neurophysiology</source>, <volume>105</volume>:<fpage>1950</fpage>–<lpage>1962</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Poort J."><surname>Poort</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khan A. G."><surname>Khan</surname>, <given-names>A. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pachitariu M."><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nemri A."><surname>Nemri</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Orsolic I."><surname>Orsolic</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krupic J."><surname>Krupic</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bauza M."><surname>Bauza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sahani M."><surname>Sahani</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Keller G. B."><surname>Keller</surname>, <given-names>G. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mrsic-Flogel T. D."><surname>Mrsic-Flogel</surname>, <given-names>T. D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hofer S. B."><surname>Hofer</surname>, <given-names>S. B.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-45">Learning enhances sensory and multiple non-sensory representations in primary visual cortex</article-title>. <source hwp:id="source-47">Neuron</source>, <volume>86</volume>:<fpage>1478</fpage>–<lpage>1490</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Poulet J. F."><surname>Poulet</surname>, <given-names>J. F.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Petersen C. C."><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-46">Internal brain state regulates membrane potential synchrony in barrel cortex of behaving mice</article-title>. <source hwp:id="source-48">Nature</source>, <volume>454</volume>(<issue>7206</issue>):<fpage>881</fpage>–<lpage>885</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1 xref-ref-50-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Rancz E. A."><surname>Rancz</surname>, <given-names>E. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moya J."><surname>Moya</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Drawitsch F."><surname>Drawitsch</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brichta A. M."><surname>Brichta</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Canals S."><surname>Canals</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Margrie T. W."><surname>Margrie</surname>, <given-names>T. W.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-47">Widespread vestibular activation of the rodent cortex</article-title>. <source hwp:id="source-49">Journal of Neuroscience</source>, <volume>35</volume>:<fpage>5926</fpage>–<lpage>5934</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2 xref-ref-51-3 xref-ref-51-4"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Reimer J."><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Froudarakis E."><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cadwell C. R."><surname>Cadwell</surname>, <given-names>C. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yatsenko D."><surname>Yatsenko</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Denfield G. H."><surname>Denfield</surname>, <given-names>G. H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Tolias A. S."><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-48">Pupil fluctuations track fast switching of cortical states during quiet wakefulness</article-title>. <source hwp:id="source-50">Neuron</source>, <volume>84</volume>(<issue>2</issue>):<fpage>355</fpage>–<lpage>362</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.52" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Rolston J. D."><surname>Rolston</surname>, <given-names>J. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross R. E."><surname>Gross</surname>, <given-names>R. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Potter S. M."><surname>Potter</surname>, <given-names>S. M.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-49">Common median referencing for improved action potential detection with multielectrode arrays</article-title>. <source hwp:id="source-51">In 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source>, pages <fpage>1604</fpage>–<lpage>1607</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1 xref-ref-53-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Roy S."><surname>Roy</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bryant J. L."><surname>Bryant</surname>, <given-names>J. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cao Y."><surname>Cao</surname>, <given-names>Y.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Heck D. H."><surname>Heck</surname>, <given-names>D. H.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-50">High-precision, three-dimensional tracking of mouse whisker movements with optical motion capture technology</article-title>. <source hwp:id="source-52">Frontiers in Behavioral Neuroscience</source>, <volume>5</volume>:<fpage>27</fpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1 xref-ref-54-2"><citation publication-type="thesis" citation-type="thesis" ref:id="294397v1.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Sahani M."><surname>Sahani</surname>, <given-names>M.</given-names></string-name> (<year>1999</year>). <chapter-title>Latent variable models for neural data analysis</chapter-title>. PhD thesis, <source hwp:id="source-53">California Institute of Technology</source>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Sakatani T."><surname>Sakatani</surname>, <given-names>T.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Isa T."><surname>Isa</surname>, <given-names>T.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-51">Pc-based high-speed video-oculography for measuring rapid eye movements in mice</article-title>. <source hwp:id="source-54">Neuroscience Research</source>, <volume>49</volume>:<fpage>123</fpage>–<lpage>131</lpage>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Sakatani T."><surname>Sakatani</surname>, <given-names>T.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Isa T."><surname>Isa</surname>, <given-names>T.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-52">Quantitative analysis of spontaneous saccade-like rapid eye movements in c57bl/6 mice</article-title>. <source hwp:id="source-55">Neuroscience Research</source>, <volume>58</volume>:<fpage>324</fpage>–<lpage>331</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Saleem A. B."><surname>Saleem</surname>, <given-names>A. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ayaz A."><surname>Ayaz</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jeffery K. J."><surname>Jeffery</surname>, <given-names>K. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris K. D."><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Carandini M."><surname>Carandini</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-53">Integration of visual motion and locomotion in mouse visual cortex</article-title>. <source hwp:id="source-56">Nature Neuroscience</source>, <volume>16</volume>:<fpage>1864</fpage>–<lpage>1869</lpage>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Schmidt-Hieber C."><surname>Schmidt-Hieber</surname>, <given-names>C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hausser M."><surname>Hausser</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-54">Cellular mechanisms of spatial navigation in the medial entorhinal cortex</article-title>. <source hwp:id="source-57">Nature Neuroscience</source>, <volume>16</volume>:<fpage>325</fpage>–<lpage>331</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1 xref-ref-59-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Schneider D. M."><surname>Schneider</surname>, <given-names>D. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nelson A."><surname>Nelson</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Mooney R."><surname>Mooney</surname>, <given-names>R.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-55">A synaptic and circuit basis for corollary discharge in the auditory cortex</article-title>. <source hwp:id="source-58">Nature</source>, <volume>513</volume>:<fpage>189</fpage>–<lpage>194</lpage>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1 xref-ref-60-2"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Stahl J. S."><surname>Stahl</surname>, <given-names>J. S.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-56">Using eye movements to assess brain function in mice</article-title>. <source hwp:id="source-59">Vision Research</source>, <volume>44</volume>:<fpage>3401</fpage>–<lpage>3410</lpage>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Stowers J. R."><surname>Stowers</surname>, <given-names>J. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hofbauer M."><surname>Hofbauer</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bastien R."><surname>Bastien</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Griessner J."><surname>Griessner</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Higgins P."><surname>Higgins</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Farooqui S."><surname>Farooqui</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fischer R. M."><surname>Fischer</surname>, <given-names>R. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nowikovsky K."><surname>Nowikovsky</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haubensak W."><surname>Haubensak</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Couzin I. D."><surname>Couzin</surname>, <given-names>I. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tessmar-Raible K."><surname>Tessmar-Raible</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Straw A. D."><surname>Straw</surname>, <given-names>A. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-57">Virtual reality for freely moving animals</article-title>. <source hwp:id="source-60">Nature Methods</source>, <volume>14</volume>:<fpage>995</fpage>–<lpage>1002</lpage>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Tatler B. W."><surname>Tatler</surname>, <given-names>B. W.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Land M. F."><surname>Land</surname>, <given-names>M. F.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-58">Vision and the representation of the surroundings in spatial memory</article-title>. <source hwp:id="source-61">Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>366</volume>:<fpage>596</fpage>–<lpage>610</lpage>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Taube J. S."><surname>Taube</surname>, <given-names>J. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Muller R. U."><surname>Muller</surname>, <given-names>R. U.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Ranck J. B."><surname>Ranck</surname>, <given-names>J. B.</given-names></string-name> (<year>1990</year>). <article-title hwp:id="article-title-59">Head-direction cells recorded from the postsubiculum in freely moving rats. i. description and quantitative analysis</article-title>. <source hwp:id="source-62">Journal of Neuroscience</source>, <volume>10</volume>:<fpage>420</fpage>–<lpage>435</lpage>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1 xref-ref-64-2 xref-ref-64-3"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="van Alphen A. M."><surname>van Alphen</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stahl J. S."><surname>Stahl</surname>, <given-names>J. S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="De Zeeuw C. I."><surname>De Zeeuw</surname>, <given-names>C. I.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-60">The dynamic characteristics of the mouse horizontal vestibulo-ocular and optokinetic response</article-title>. <source hwp:id="source-63">Brain Research</source>, <volume>890</volume>:<fpage>296</fpage>–<lpage>305</lpage>.</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.65" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Velez-Fort M."><surname>Velez-Fort</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bracey E. F."><surname>Bracey</surname>, <given-names>E. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Keshavarzi S."><surname>Keshavarzi</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rousseau C. V."><surname>Rousseau</surname>, <given-names>C. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cossell L."><surname>Cossell</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lenzi S. C."><surname>Lenzi</surname>, <given-names>S. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Strom M."><surname>Strom</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Margrie T. W."><surname>Margrie</surname>, <given-names>T. W.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-61">A Circuit for Integration of Headand Visual-Motion Signals in Layer 6 of Mouse Primary Visual Cortex</article-title>. <source hwp:id="source-64">Neuron</source>, <volume>0</volume>(<issue>0</issue>).</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1 xref-ref-66-2 xref-ref-66-3"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Venkatraman S."><surname>Venkatraman</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jin X."><surname>Jin</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Costa R. M."><surname>Costa</surname>, <given-names>R. M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Carmena J. M."><surname>Carmena</surname>, <given-names>J. M.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-62">Investigating neural correlates of behavior in freely behaving rodents using inertial sensors</article-title>. <source hwp:id="source-65">Journal of Neurophysiology</source>, <volume>104</volume>(<issue>1</issue>):<fpage>569</fpage>–<lpage>575</lpage>.</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1 xref-ref-67-2 xref-ref-67-3"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Voigts J."><surname>Voigts</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sakmann B."><surname>Sakmann</surname>, <given-names>B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Celikel T."><surname>Celikel</surname>, <given-names>T.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-63">Unsupervised whisker tracking in unrestrained behaving animals</article-title>. <source hwp:id="source-66">Journal of Neurophysiology</source>, <volume>100</volume>:<fpage>504</fpage>–<lpage>515</lpage>.</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1 xref-ref-68-2 xref-ref-68-3"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Voigts J."><surname>Voigts</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Siegle J. H."><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pritchett D. L."><surname>Pritchett</surname>, <given-names>D. L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Moore C. I."><surname>Moore</surname>, <given-names>C. I.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-64">The flexdrive: an ultra-light implant for optical control and highly parallel chronic recording of neuronal ensembles in freely moving mice</article-title>. <source hwp:id="source-67">Frontiers in Systems Neuroscience</source>, <volume>7</volume>:<fpage>8</fpage>.</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1 xref-ref-69-2 xref-ref-69-3 xref-ref-69-4 xref-ref-69-5 xref-ref-69-6"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Wallace D. J."><surname>Wallace</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greenberg D. S."><surname>Greenberg</surname>, <given-names>D. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sawinski J."><surname>Sawinski</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rulla S."><surname>Rulla</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Notaro G."><surname>Notaro</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kerr J. N. D."><surname>Kerr</surname>, <given-names>J. N. D.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-65">Rats maintain an overhead binocular field at the expense of constant fusion</article-title>. <source hwp:id="source-68">Nature</source>, <volume>498</volume>(<issue>7452</issue>):<fpage>65</fpage>–<lpage>69</lpage>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Wiltschko A. B."><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnson M. J."><surname>Johnson</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iurilli G."><surname>Iurilli</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peterson R. E."><surname>Peterson</surname>, <given-names>R. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Katon J. M."><surname>Katon</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pashkovski S. L."><surname>Pashkovski</surname>, <given-names>S. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abraira V. E."><surname>Abraira</surname>, <given-names>V. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adams R. P."><surname>Adams</surname>, <given-names>R. P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Datta S. R."><surname>Datta</surname>, <given-names>S. R.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-66">Mapping sub-second structure in mouse behavior</article-title>. <source hwp:id="source-69">Neuron</source>, <volume>88</volume>(<issue>6</issue>):<fpage>11211135</fpage>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Wu F."><surname>Wu</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stark E."><surname>Stark</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ku P.-C."><surname>Ku</surname>, <given-names>P.-C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wise K. D."><surname>Wise</surname>, <given-names>K. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Buzsaki G."><surname>Buzsaki</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Yoon E."><surname>Yoon</surname>, <given-names>E.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-67">Monolithically integrated μleds on silicon neural probes for high-resolution optogenetic studies in behaving animals</article-title>. <source hwp:id="source-70">Neuron</source>, <volume>88</volume>(<issue>6</issue>):<fpage>1136</fpage>–<lpage>1148</lpage>.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Yin M."><surname>Yin</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Borton D. A."><surname>Borton</surname>, <given-names>D. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Komar J."><surname>Komar</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Agha N."><surname>Agha</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lu Y."><surname>Lu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li H."><surname>Li</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Laurens J."><surname>Laurens</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lang Y."><surname>Lang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Q."><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bull C."><surname>Bull</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Larson L."><surname>Larson</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rosler D."><surname>Rosler</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bezard E."><surname>Bezard</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Courtine G."><surname>Courtine</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Nurmikko A. V."><surname>Nurmikko</surname>, <given-names>A. V.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-68">Wireless Neurosensor for Full-Spectrum Electrophysiology Recordings during Free Behavior</article-title>. <source hwp:id="source-71">Neuron</source>, <volume>84</volume>(<issue>6</issue>):<fpage>1170</fpage>–<lpage>1182</lpage>.</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><citation publication-type="journal" citation-type="journal" ref:id="294397v1.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Zong W."><surname>Zong</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu R."><surname>Wu</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li M."><surname>Li</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hu Y."><surname>Hu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Y."><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li J."><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rong H."><surname>Rong</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu H."><surname>Wu</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xu Y."><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lu Y."><surname>Lu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jia H."><surname>Jia</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fan M."><surname>Fan</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhou Z."><surname>Zhou</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang Y."><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang A."><surname>Wang</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen L."><surname>Chen</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Cheng H."><surname>Cheng</surname>, <given-names>H.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-69">Fast high-resolution miniature two-photon microscopy for brain imaging in freely behaving mice</article-title>. <source hwp:id="source-72">Nature Methods</source>, <volume>14</volume>:<fpage>713</fpage>–<lpage>719</lpage>.</citation></ref></ref-list><sec id="s5" hwp:id="sec-13"><title hwp:id="title-24">Methods</title><sec id="s5a" hwp:id="sec-14"><title hwp:id="title-25">Contact for Reagent and Resource Sharing</title><p hwp:id="p-54">Further information and requests for resources and reagents should be directed to and will be fulfilled by the Lead Contact, Jennifer F Linden (j.linden©ucl.ac.uk).</p></sec></sec><sec id="s6" hwp:id="sec-15"><title hwp:id="title-26">Experimental Model and Subject Details</title><sec id="s6a" hwp:id="sec-16"><title hwp:id="title-27">Animals</title><p hwp:id="p-55">Experiments were performed on male C57Bl/6J mice (Charles River) for visual cortex recordings and male C57Bl/6J and CBA/Ca mice (Charles River) for the sound experiment. After surgical implantation of chronic implants for neural recordings, mice were individually housed on a 12-h reversed light-dark cycle (lights off at 12.00 noon). Water and food were available ad libitum. All experimental procedures were carried out in accordance with a UK Home Offi ce Project Licence approved under the United Kingdom Animals (Scientific Procedures) Act of 19J86.</p></sec></sec><sec id="s7" hwp:id="sec-17"><title hwp:id="title-28">Methods Details</title><sec id="s7a" hwp:id="sec-18"><title hwp:id="title-29">Surgical procedures</title><p hwp:id="p-56">For chronic implants, we used custom tetrode hyperdrives with 8-16 individually movable tetrodes, constructed according to a published design (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-3" hwp:rel-id="ref-68">Voigts et al., 2013</xref>). Tetrodes were made from HM-L coated 90% platinum/10% iridium 17 <italic toggle="yes">µ</italic>m diameter wire (California Fine Wire). A miniature male connector (Omnetics NPD-18-DD-GS) was attached to the front of the drive body (see "Construction of the lightweight camera system") for connection of the camera system during behavioral experiments.</p><p hwp:id="p-57">Mice aged 58-65 days were anaesthetized with 1-2% isoflurane and injected with analgesia (Carprofen, 5 mg/kg IP). Ophthalmic ointment (Alcon) was applied to the eyes, and sterile saline (0.1 ml) injected SC as needed to maintain hydration. A circular piece of dorsal scalp was removed and the underlying skull was cleaned and dried. A custom machined aluminum head-plate was then cemented onto the skull using dental adhesive (Superbond C&amp;B). A small craniotomy was made over the left primary visual cortex (V1) (2.5 mm lateral, 1 mm anterior to the transverse sinus). The tetrode drive was positioned above the craniotomy and fixed to the skull with dental adhesive. A pinhole craniotomy above the right prefrontal cortex contralateral to the tetrode implant for the ground screw (000-120 × 1/16, Antrin Miniature Specialties). The ground screw and implant were then secured with more dental adhesive and dental cement (Kemdent Simplex Rapid). Mice were allowed to recover from surgery for at least five days before experiments began.</p></sec><sec id="s7b" hwp:id="sec-19"><title hwp:id="title-30">Neural recordings in head-fixed and freely moving mice</title><p hwp:id="p-58">All experiments were conducted in a custom double-walled sound-shielded anechoic chamber. Animals became accustomed to handling and gentle restraint over two to three days, before they were head-fixed and placed on a custom styrofoam cylinder (20 cm diameter, on a ball-bearing mounted axis). After animals were head-fixed the headstage was connected to the implant and the camera holder was connected to the miniature connector on the outside of the implant, together with two cables from the headstage which provided power to the IR light-emitting diode (IR LED).</p><p hwp:id="p-59">We confirmed that each tetrode recording site was in monocular V1 by presenting stimuli on a screen contralateral to the implant side and identifying the approximate receptive field position of recorded cells as described previously (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Poort et al., 2015</xref>). Luminance of visual stimuli was calibrated using a luminance meter (Konica Minolta, LS-100). Running speed on the cylinder was detected with a rotary encoder (Kubler, 1024 steps per rotation) and single steps were extracted using a microcontroller (Arduino Uno), sent to the recording system as transistor-transistor logic (TTL) pulses and recorded along with neural data.</p><p hwp:id="p-60">For experiments in freely moving mice, the implant was gently held while allowing the mouse to walk or run on a running wheel, and headstage and camera system were connected as for the head-restrained experiments. The animal was then released into a circular environment for experiments in the freely moving condition. Two different circular environments were used. The first environment (diameter 30 centimeters) consisted of white plastic material. Eight LED lights (ULT300, Digital Daffodil) combined with custom cut light diffuser sheets (Perspex) were used to provide homogeneous lighting which facilitated tracking of the eye (see "Extraction of pupil positions from camera images"). For the sound experiment (<xref rid="figS5" ref-type="fig" hwp:id="xref-fig-13-2" hwp:rel-id="F13">Figure S5</xref>) a loudspeaker was mounted 1 meter above the center of the environment (see "Sound presentation"). The second environment (diameter 22 centimeters) consisted of black plastic materal with a semi-transparent perspex floor to allow reliable tracking of body position using an external camera from below (see "Analysis of head movement onsets"). This second environment was used to perform recordings in the dark (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-10" hwp:rel-id="F8">Figure 8</xref>).</p><p hwp:id="p-61">Neural activity was recorded with a 32-channel Intan RHD 2132 amplifier board (Intan Technologies) connected to an open-ephys acquisition board (Open Ephys) via an ultralightweight flexible serial peripheral interface cable (Intan Technologies). Data were sampled at 30 kHz and saved to disk for off-line analysis.</p></sec><sec id="s7c" hwp:id="sec-20"><title hwp:id="title-31">Electrophysiological data analysis</title><p hwp:id="p-62">Electrophysiogical recordings were analysed off-line using Bayesian spike-sorting techniques (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-2" hwp:rel-id="ref-54">Sahani, 1999</xref>). To detect action potentials the common median reference was subtracted across channels (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Rolston et al., 2009</xref>) with subsequent high-pass filtering with a cutoff of 600 Hz, and action potentials were detected by finding time points exceeding 3.5 times the standard deviation of the noise. Action potentials were automatically clustered. Single units or small clusters of neurons were accepted only if the spike-sorter reported both false-negative and false-positive rates below 5 %. Clustered units were verified manually and units were classified as single-unit (SU) if fewer than 0.5 % of the spikes occurred within the typical refractory period of a cortical neuron (<italic toggle="yes">≤</italic> 2 ms). All other units were deemed multi-units (MUs).</p><p hwp:id="p-63">The effect of the head-mounted camera system on neural recording quality was assessed using raw broadband signals and spike units (158 SUs and 11 MUs). The power spectral density (PSD) of broadband signals was estimated using Welch’s method with a 2 s long Hann window and 1 s overlap. For each condition, the PSD of all electrode channels was computed separately and the log-scaled PSDs averaged afterwards to yield a single estimate of the PSD (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Figure 2C</xref>). To quantify the difference across all recordings, we computed the PSD ratio between segments with camera on and off (10 minutes each) recorded during the same session without disconnecting the neural recording headstage (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Figure 2D-F</xref>). The order of the two conditions was balanced across sessions to reduce potential effects of behavioral changes during each session (e.g., mice typically explored the environment more during the early part of the recording). Within-condition variability for the implantonly condition was estimated by computing the standard deviation of PSD ratios for different non-overlapping 60 s segments from the same recording. The signal-to-noise ratio (SNR) between spikes and high-pass filtered electrode signals (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Figure 2E</xref>) was computed as the power of the electrode channel of each tetrode with maximum depolarization, and the noise power extracted from electrode signals between spikes (with a 2 ms margin around spikes). All data recorded during the same session were spike sorted together to avoid the need to manually register spike clusters between conditions.</p><p hwp:id="p-64">To compute the power in the local field potential (LFP), raw traces were first bandpass filtered at 2 - 10 Hz (low-frequency LFP in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-14" hwp:rel-id="F5">Figure 5</xref>) or 10 - 20 Hz (higher-frequency LFP in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-15" hwp:rel-id="F5">Figure 5</xref>) using a zero-phase fourth-order Butterworth filter with subsequent squaring of the filter output. The resulting estimate of the LFP power was smoothed with a normalized Gaussian window with a standard deviation of 2 seconds before computing the correlation with pupil dilation (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-16" hwp:rel-id="F5">Figure 5D</xref>). For visualization, LFP power was normalized such that low-frequency LFP power had a mean value of 1 (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-17" hwp:rel-id="F5">Figure 5A,C</xref>).</p></sec><sec id="s7d" hwp:id="sec-21"><title hwp:id="title-32">Construction of the lightweight camera system</title><p hwp:id="p-65">We used a commercially available camera module (Adafruit 1937) with an Omnivision OV5647 sensor capable of 640 × 480 pixels per frame at up to 90 Hz. The CMOS camera sensor has dimensions of 8.2 mm x 11.3 mm x 4.8 mm and weighs 0.5 grams (including suspended part of the cable). The infrared (IR) filter was removed to allow monitoring of behavioral variables in dark conditions using IR light. The sensor was attached to the neural implant using a custom camera holder. The camera holder consisted of a 3D printed frame with clips for holding the camera sensor (<xref rid="figS1" ref-type="fig" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Figure S1</xref>). A lightweight 21G steel cannula (length <italic toggle="yes">∼</italic> 2 cm) for holding the IR mirror (Qioptiq, NIR-Blocking Filter, Calflex-X) was bent by about 75°in the middle, inserted with one end into a hole in the frame and fixed with epoxy resin (Araldite Steel). The mirror was cut to size 7 mm x 7 mm and attached to the cannula via a 3D printed holder. This enabled fine adjustment of the mirror relative to the camera sensor by moving the mirror along the cannula, rotating the mirror around the cannula, and also by further bending the cannula. A miniature connector (Omnetics NSD-18-DD-GS) for mounting the camera system to the implant was attached to the back of the 3D printed holder base using super glue (Loctite Power Flex Gel). After final adjustment of the mirror, either during surgery or during head-fixation of the animal on a running wheel (see "Neural recordings in head-fixed and freely moving mice"), the cannula and the mirror holder were permanently fixed using a thin layer of strong epoxy resin (Araldite Rapid). STL and OpenSCAD source files for the camera and mirror holders will be made freely available.</p><p hwp:id="p-66">Illumination of the camera’s field of view, including eye and whisker pad, was provided by a small IR LED (Vishay VSMB2943GX01) mounted to either the bottom or the side of the camera holder, depending on the angle between camera sensor, mirror, and implant. The IR LED was powered by the headstage via two 36AWG wires and a small-package current-limiting resistor (Farnell Multicomp, 100 - 180 Ohm, metric package size 3216). Custom cut gold pins (RS Pro Male and Female Solder D-sub Connector Contact, 481-493 and 481-500) soldered to the wires and the headstage allowed quick and stable connection during experiments. All parts, including weight and estimated cost, are summarized in <xref rid="tblS1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table S1</xref>. An example camera holder is shown in <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Figure S1B</xref>.</p></sec><sec id="s7e" hwp:id="sec-22"><title hwp:id="title-33">Interfacing with the camera</title><p hwp:id="p-67">The camera was connected to a single-board computer (Raspberry Pi 3 model B, Raspberry Pi Foundation) with ARM architecture and VideoCore 4 graphics processing unit (GPU). Data from the camera were read out with custom software using the Multi-media Abstraction Layer (MMAL) API (Broadcom Europe). Because miniature cameras such as the one used for the head-mounted system do not typically provide additional output signals to synchronize frame acquisition, we used the following approach to avoid dropped frames during recording and to obtain time stamps that were precisely synchronized with neural recordings. First, each frame was annotated with a time stamp from the GPU immediately after acquisition. Once the frame was received and decoded by the custom software, a TTL signal pulse was sent to the recording system using the general-purpose input/output capabilities of the single-board computer. The difference between the acquisition and TTL signal time stamps was saved to a separate file for post-hoc alignment of TTL time stamps and neural data. Communication between the computer for recording neural data and the single-board computer for controlling the camera was done via ethernet using the ZeroMQ messaging library (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://zeromq.org/" ext-link-type="uri" xlink:href="http://zeromq.org/" hwp:id="ext-link-4">http://zeromq.org/</ext-link>). Automatic starting/stopping of the camera system was controlled using a custom plugin for the open-ephys recording system (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.open-ephys.org/" ext-link-type="uri" xlink:href="http://www.open-ephys.org/" hwp:id="ext-link-5">http://www.open-ephys.org/</ext-link>). Code for frame acquisition, TTL time stamp generation and alignment, and the plugin for controlling the camera will be made freely available.</p><p hwp:id="p-68"><xref rid="figS2" ref-type="fig" hwp:id="xref-fig-10-2" hwp:rel-id="F10">Figure S2</xref> demonstrates precision of aligned time stamps for a blinking LED (Vishay TSAL4400, typical rise/fall time 800 ns) recorded under the same conditions as the behavioral data in the experiments, for different video resolutions and frame rates. The LED was driven by a microcontroller (Teensy 3.2, PJRC) and the same signal was sent to the recording system. The pixel corresponding to the maximum LED intensity was identified and LED onset times were extracted from the pixel intensity trace by thresholding at 0.5 full intensity.</p></sec><sec id="s7f" hwp:id="sec-23"><title hwp:id="title-34">Detection of camera image movements</title><p hwp:id="p-69">For each recording, movement of the camera image was detected by selecting a region of interest (ROI) that contained a part of the neural implant (inset in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Figure 3A</xref>). A correlation-based algorithm (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">Dubbs et al., 2016</xref>) was used to detect movements between the average ROI (averaged across all recorded images) and the ROI for each video image. Using the average ROI as reference image ensured that whisker or hair movements on single images did not have an impact on the overall detection performance. Images with changes in brightness exceeding three standard deviations were excluded from the analysis to remove periods when the camera view was blocked, e.g., during grooming. On average only 0.6% and 0.2% of the camera images were removed from the freely moving and head-restrained recordings based on this criterion, respectively.</p></sec><sec id="s7g" hwp:id="sec-24"><title hwp:id="title-35">Extraction of pupil positions from camera images</title><p hwp:id="p-70">In order to perform tracking of pupil positions, it was necessary to remove bright regions from the camera image resulting from reflections of the illumination IR LED on the cornea. Therefore, contiguous bright regions on the recorded camera frames were detected by thresholding, and a binary mask was generated. Thresholds were manually selected for each session to include the major IR LED reflections. The original frame and the binary mask were used to estimate the values of masked pixels using non-texture image inpainting (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Marz, 2011</xref>). An ellipse was fitted to the processed frame by thresholding, contour extraction, and least-squares ellipse fitting (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Fitzgibbon et al., 1999</xref>). Contour extraction thresholds were manually adjusted for each session and only ellipses with mean pixel intensities below a user-defined threshold and with areas above another user-defined threshold were kept to reduce false positive rates. Thresholds were selected based on a small number of eye frames (<italic toggle="yes">≤</italic> 2%) randomly selected from the whole recording. Finally, ellipses were manually verified using custom software including a graphical user interface. Ellipse fitting code will be made freely available.</p><p hwp:id="p-71">In behavioral experiments where we tracked the eye position in the dark, we administered an eye drop of physostigmine salicylate (0.1-0.2%) 30 minutes in advance to limit pupil dilation (see for example <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-6" hwp:rel-id="ref-42">Oommen and Stahl, 2008</xref>).</p></sec><sec id="s7h" hwp:id="sec-25"><title hwp:id="title-36">Extraction of whisking pad movement from camera images</title><p hwp:id="p-72">Movement of the whisker pad was extracted by selecting a rectangular region of the camera image containing the whisker pad. Dense optical flow was computed (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Farneback, 2003</xref>) and the average optical flow across all pixels was used as a measure of whisker pad movement in horizontal (related to azimuth) and vertical (related to elevation) directions. All analyses in this study were based on horizontal movements.</p><p hwp:id="p-73">We compared whisker pad movements recorded using the head-mounted camera (60 Hz) to data recorded simultaneously using an external camera (100 Hz) from above while the mouse was head-fixed. The head-mounted camera was able to capture important aspects of whisking including the whisking frequency and fluctuations in whisking envelope (<xref rid="figS3" ref-type="fig" hwp:id="xref-fig-11-2" hwp:rel-id="F11">Figure S3</xref>).</p><p hwp:id="p-74">In some experiments described here (e.g., <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-18" hwp:rel-id="F5">Figure 5</xref>, <xref rid="figS5" ref-type="fig" hwp:id="xref-fig-13-3" hwp:rel-id="F13">Figure S5</xref>), the camera system was operated with a frame rate of 30 Hz, and therefore whisker pad movements were measured only up to 15 Hz. In principle, however, the camera could be run at 90 Hz frame rates to capture more detailed aspects of whisking (e.g., whisker angles), using more sophisticated algorithms to extract these parameters at high frame rates (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Perkon et al., 2011</xref>).</p></sec><sec id="s7i" hwp:id="sec-26"><title hwp:id="title-37">Extraction of head orientations from accelerometer signals</title><p hwp:id="p-75">Gravity components in the accelerometer signals were estimated by low-pass filtering each channel with a zero-phase second-order Butterworth filter with a cut-off frequency of 2 Hz (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-3" hwp:rel-id="ref-43">Pasquet et al., 2016</xref>). Pitch, defined as the angle between the naso-occipital axis and the horizontal gravity plane, was extracted by computing the angle between the gravity vector and the y/z plane with normal vector <bold>e</bold><sub><italic toggle="yes">x</italic></sub>= (1,0, 0)<sup>T</sup>. Roll, defined as the angle between the interaural axis and the horizontal gravity plane, was extracted by computing the angle between the gravity vector and the x/z plane with normal vector <bold>e</bold><sub><italic toggle="yes">y</italic></sub>= (0, 1, 0)<sup>T</sup>.</p><p hwp:id="p-76">To compute head orientation maps (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-8" hwp:rel-id="F4">Figure 4F,I</xref> and <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-19" hwp:rel-id="F5">Figure 5F</xref>) the low-pass filtered accelerometer signals were transformed into spherical coordinates (with elevation angle Θ and azimuthal angle Φ). A 2D histogram of head orientation vectors with a bin size of 5°for both elevation and azimuth was computed on the unfolded sphere. In order to visualize the histogram on a sphere, the number of samples within a each bin was normalized by the corresponding quadrangle area. Normalized histogram data were color-coded on a logarithmic scale.</p></sec><sec id="s7j" hwp:id="sec-27"><title hwp:id="title-38">Behavioral segmentation</title><p hwp:id="p-77">Behavioral states were segmented using a semi-automatic classification algorithm. In a first step, about 1 - 2 hours of video recorded using external CMOS cameras (The Imaging Source, 20-50 Hz frame rate) were annotated manually for each mouse and for each condition ("Implant+cam" and "Implant" in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-9" hwp:rel-id="F4">Figure 4</xref>). Only behavioral segments with a duration of at least 2 seconds were assigned a behavioral state.</p><p hwp:id="p-78">The behaviors that categorized were "grooming" (G), "eating" (E), "quiescence" (Q), and "active exploration" (A). Grooming comprised different stereotypical movements, e.g., movement of the forepaws over the nose and muzzle, strokes of forepaws across vibrissae and eye, and strokes with the hindleg. These movements were typically periodic and therefore easily distinguishable from the other behaviors. Eating was identified during chewing on seeds added to the environment. As chewing was also evident as artifacts on electrode channels we used this information during manual annotation but not during automatic segmentation. Because the sessions in which seeds were added to the environment were not balanced across conditions, we accounted for this during the analysis shown in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-10" hwp:rel-id="F4">Figure 4E,H</xref> by assigning the mean value across sessions with seeds to those without seeds. Periods when the mouse was still for at least 2 seconds were classified as quiescence and periods when the mouse was exploring the environment and not grooming or eating were classified as active exploration.</p><p hwp:id="p-79">We found that segmentation based on the time-domain accelerometer signals (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-2" hwp:rel-id="ref-66">Venkatraman et al., 2010</xref>; <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Dhawale et al., 2017</xref>) resulted in relatively low accuracy of identification of the behaviors described above. We therefore developed an algorithm performing segmentation in the frequency domain that considerably increased accuracy compared to segmentation based on time-domain signals (<xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-6" hwp:rel-id="F12">Figure S4C,D</xref>). The algorithm worked as follows: accelerometer signals (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-11" hwp:rel-id="F4">Figure 4B</xref>) were transformed into a spectro-temporal representation using a short-time Fourier transform (STFT) with a Hann window of length 2 s and a window shift of 40 ms. At each time step, the log-scaled magnitude of the transformed accelerometer signals was recast as a single vector containing data from all accelerometer channels. The middle point of the window was used as reference point for the annotated behavioral category. A Multilayer Perceptron (MLP) with one hidden layer (N=100 hidden units with rectified-linear activation functions) was then fit to the data. The network was trained using the backpropagation algorithm and the weights were optimized using a stochastic gradient-based solver with adaptive momentum estimation (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Kingma and Ba, 2014</xref>) via the sklearn Python package (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Pedregosa et al., 2011</xref>).</p><p hwp:id="p-80">We evaluated the prediction performance of the model using cross-validation. That is, the data set was divided into 4 parts, model parameters were estimated leaving out one of the parts, and the predictive quality of the model fit was evaluated on the part left out. This procedure was repeated leaving out each of the 4 parts in turn and the prediction accuracy averaged to yield an estimate of the goodness-of-fit of the model. The confusion matrices in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-12" hwp:rel-id="F4">Figure 4C</xref> and supplemental <xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-7" hwp:rel-id="F12">Figure S4</xref> show the cross-validated true positive rate computed from the manually annotated data ("Human observer") and the prediction of the model.</p><p hwp:id="p-81">To assess the differences between occupancies of the different states in the two experimental conditions ("Implant+cam" and "Implant" in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-13" hwp:rel-id="F4">Figure 4E,H</xref>) we computed the least absolute deviation (L1 norm) between the distributions for both conditions. To confirm the significance of this difference, we used a permutation test. A null distribution was generated by shuffl ing "Implant+cam" and "Implant" condition labels across recording sessions. This approach ensured that any significant differences from the null distribution could be attributed to the presence of the camera rather than time of the recording session (see <xref rid="figS4" ref-type="fig" hwp:id="xref-fig-12-8" hwp:rel-id="F12">Figure S4B,C</xref>). The permutation procedure was repeated 10000 times, and a <italic toggle="yes">P</italic>-value was generated by computing the fraction of permutations with least absolute deviations larger than the value computed on the original data set. The same permutation procedure was used to determine the significance of the difference between body speed distributions in the active state (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-14" hwp:rel-id="F4">Figure 4G,J</xref>). Mean and variance of head orientations (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-15" hwp:rel-id="F4">Figure 4F,I</xref>) were computed using a permutation test for the difference in circular mean and variance as test statistic, respectively.</p></sec></sec><sec id="s8" hwp:id="sec-28"><title hwp:id="title-39">Sound presentation</title><p hwp:id="p-82">Broadband noise burst stimuli (50 ms, 50 or 55 dB SPL, noise bust rate 0.5 Hz or 1 Hz) were generated using custom software, converted to an analog signal (HDSPe AIO, RME), amplified (RB-850, Rotel), and delivered via a loudspeaker (XT25TG30-04, Tymphany) mounted about 1 meter above the circular environment. Sound pressure levels of the acoustic stimuli were measured (40BF <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="294397_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> in free-field microphone and 26AC preamplifier, GRAS) and calibrated to the center of the circular environment. In the experiments shown in <xref rid="figS5" ref-type="fig" hwp:id="xref-fig-13-4" hwp:rel-id="F13">Figure S5</xref>, recordings with and without acoustical stimulation were interleaved (up to 5 minutes each, total duration 30 minutes) during periods when the animal was quiescent and immobile.</p></sec><sec id="s9" hwp:id="sec-29"><title hwp:id="title-40">Prediction of eye position using head orientation</title><p hwp:id="p-83">Pupil positions were extracted from video data (sampled at 42-60 Hz) as described in "Extraction of pupil positions from camera images". Only time points at which the pupil could be detected were included in the analysis and no smoothing was applied for the analysis. For visualization, extracted eye position and pupil dilation traces were smoothed using a 3-point Gaussian window with coeffi cients (0.072, 0.855, 0.072). Head pitch and roll were computed from signals recorded using the 3-axis accelerometer (sampled at 7500 Hz) integrated into the neural recording as described in "Extraction of head orientations from accelerometer signals"</p><p hwp:id="p-84">For each pupil position <italic toggle="yes">p<sub>i</sub>, i</italic> = 1, 2<italic toggle="yes">, …, N</italic>, the most recent history of each signal within a time window of 500 ms was recast as vector <bold>u</bold><sub><italic toggle="yes">i</italic></sub>, <bold>v</bold><sub><italic toggle="yes">i</italic></sub>for pitch and roll, respectively. Linear interpolation was used to find the pitch/roll at time lags −500, −475, −450, …, 0 ms.</p><p hwp:id="p-85">Two different models were trained using the resulting data. The linear model assumes that pupil positions are related to the pitch and roll via</p><disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="294397_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula><p hwp:id="p-86">The linear weighting vectors <bold>k</bold><sub>pitch</sub> and <bold>k</bold><sub>roll</sub>, and the offset term <italic toggle="yes">k</italic><sub>0</sub> were found using a Bayesian method for determining the relevance of inputs, known as Automatic Relevance Determination (ARD) (<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">MacKay, 1996</xref>). Because the relation between accelerometer signals and pupil position can potentially be nonlinear we also tested a Multi-Layer Perceptron as described in Results.</p><p hwp:id="p-87">In some experiments we added a lightweight gyroscope sensor (MPU-9250, InvenSense, San Jose, US) to measure angular velocity, including rotations about the yaw axis. The sensor was calibrated using a stepper motor (Adafruit 324) and a contact tachometer (DT-2235B, Lutron Electronic). To approximate angular yaw position we convolved the velocity signal with an exponential decay function with time constant <italic toggle="yes">τ</italic> = 1 s and extended the models to also included the recent history of angular positions (<xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">Eq. 1</xref> and <xref rid="figS6" ref-type="fig" hwp:id="xref-fig-14-6" hwp:rel-id="F14">Figure S6F</xref>).</p><p hwp:id="p-88">The prediction performance of the different models was evaluated using cross-validation as described above (but with <italic toggle="yes">n</italic> = 5 fold). Similarity between predicted and measured eye positions was quantified using the coeffi cient of determination <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="294397_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> where rss is the residual sum of squares and tss is the total sum of squares.</p><p hwp:id="p-89">In behavioral experiments where we tracked the eye position in the dark ("Extraction of pupil positions from camera images"), we typically recorded 2-3 segments (10 minutes each) before administration of an eye drop of physostigmine salicylate and one recording with eye drop (10 minutes) about 30 minutes after administration. About 20 minutes after the dark recording the pupil size was too small to allow for reliable tracking. This procedure was repeated on four different days in one mouse resulting in 10 recordings with light on and 4 recordings in the dark.</p></sec><sec id="s10" hwp:id="sec-30"><title hwp:id="title-41">Analysis of head movement onsets</title><p hwp:id="p-90">Data for analysis of head movement onsets was collected while mice were exploring a circular environment (see "Neural recordings in head-fixed and freely moving mice"). The bottom of the circular environment consisted of an acrylic sheet that allowed reliable tracking of the mouse’s body using a camera placed below the environment, even in the presence of headstage and camera cables. Head movements were extracted from accelerometer signals by subtracting the gravity components (see "Extraction of head orientations from accelerometer signals"). The magnitude of head movements was computed as
<disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="294397_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula>
where <italic toggle="yes">a</italic><sub><italic toggle="yes">x</italic></sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">y</italic></sub>, and <italic toggle="yes">a</italic><sub><italic toggle="yes">z</italic></sub> are the head acceleration components along x, y, and z channels of the accelerometer, respectively, sampled at time step <italic toggle="yes">t</italic>. The magnitude was smoothed using a low-pass filter with a cut-off frequency of 2 Hz and thresholded using a fixed threshold across all mice and recordings (0.0625 g). Positive threshold crossings were classified as head movement onset if the smoothed magnitude of the accelerometer signals was (i) below the threshold for at least 0.5 s before and (ii) above the threshold for at least 0.5 s after the threshold crossing (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-11" hwp:rel-id="F8">Figure 8B</xref>). Moreover, movement onsets during locomotion periods (body speed <italic toggle="yes">≥</italic> 1 cm/s) were excluded from the analysis in <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-12" hwp:rel-id="F8">Figure 8</xref>. Onsets of whisker pad movements and locomotion were computed in the same way as head movement onsets but whisking thresholds were selected separately for each mouse and the minimum duration above threshold was 0.1 s to account for faster movements of whiskers. Because mice were only occasional running during each recording session, presumably due to the relatively small size of the circular environment, we computed locomotion onsets for mice running on a cylindrical treadmill (threshold 1 cm/s) in the dark.</p><p hwp:id="p-91">For the analysis, spike times were aligned to head movement onsets for each recorded V1 cell. To quantify the extent to which head-movement-related activity modulated the activity of each cell, we computed a modulation index (MI) defined as
<disp-formula id="eqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="294397_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula>
with <italic toggle="yes">n</italic><sub>pre</sub> and <italic toggle="yes">n</italic><sub>post</sub> denoting the average number of spikes 1 s before and 1 s after movement onset, respectively. MI values reported here were computed without subtraction of the baseline firing rate.</p><p hwp:id="p-92">Because tracking of the pupil in the dark can be challenging due to increased pupil dilation (and because the effect of pharmacological intervention to reduce pupil dilation is not known, see "Extraction of pupil positions from camera images"), we extracted initial eye movements after movement onsets by measuring optical flow of the pupil edges in the dark. The region of the camera image containing the eye was filtered using a median filter with a window length of 15 pixels before computing optical flow of the pupil edges. This step ensured that movements of hair or IR LED reflections did not impair optical flow measurements. The flow for each pixel was computed using the same dense algorithm as for the whisker pad movements. To convert optical flow (measured in pixels per frame) to horizontal and vertical eye positions, we integrated the average flow for each dimension across time (i.e. frames). The integrated flow provides an approximation to initial eye movements after a head movement onset (but might diverge after some time due to potentially leaky integration of the flow measure). Comparing flow-based pupil positions to direct pupil fitting in dim light conditions (i.e. when the enlarged pupil was still possible to identify using ellipse-based pupil fitting), we found that analysis of optical flow of pupil edges yielded reliable estimates of eye positions after head movement onsets in the dark (<xref rid="figS8" ref-type="fig" hwp:id="xref-fig-16-2" hwp:rel-id="F16">Figure S8</xref>).</p><p hwp:id="p-93">To test whether different types of eye movements have an effect on the observed head movement-related modulation of V1 firing, we divided movement onsets into two groups: head movement onsets that were consistent with predictions of eye positions based on the models described above, and onsets that were not consistent with model predictions. Because the observed modulations of V1 firing were fast (typically appearing less than 100 ms after the head movement onset) we used the x/y values of first peak of the measured and predicted eye movements as an approximation to the initial movement. This yielded one x/y pair for the measured and one x/y pair for the predicted trace following an onset. Only pairs with a maximum/minimum within 100 ms after the head movement onset were included in the analysis. The values in <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-13" hwp:rel-id="F8">Figure 8E</xref> and <xref rid="figS8" ref-type="fig" hwp:id="xref-fig-16-3" hwp:rel-id="F16">Figure S8C</xref> show the correlations between both x/y pairs.</p></sec><sec id="s11" hwp:id="sec-31"><title hwp:id="title-42">Quantification and Statistical Analysis</title><p hwp:id="p-94">Specifics on the statistical methodologies and software used for various analyses are described in the corresponding sections in Results, figure legends, Methods details, and supplemental figures. Statistical test results are described as significant in the text where <italic toggle="yes">P</italic> &lt; 0.05.</p></sec><sec id="s12" hwp:id="sec-32"><title hwp:id="title-43">Data and code availability</title><p hwp:id="p-95">Software to control the camera and to perform data extraction, along with 3D models for custom parts in the camera system, will be made available at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.gatsby.ucl.ac.uk/resources/mousecam/" ext-link-type="uri" xlink:href="http://www.gatsby.ucl.ac.uk/resources/mousecam/" hwp:id="ext-link-6">http://www.gatsby.ucl.ac.uk/resources/mousecam/</ext-link>. Further data from this study are available from the corresponding authors upon reasonable request.</p></sec><sec id="s13" sec-type="supplementary-material" hwp:id="sec-33"><sec hwp:id="sec-34"><title hwp:id="title-44">Supplemental Figures</title><fig id="figS1" position="float" fig-type="figure" orientation="portrait" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1:</label><caption hwp:id="caption-9"><title hwp:id="title-45">Design of the miniature head-mounted camera system. Related to <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Figure 1</xref>.</title><p hwp:id="p-96">(A) 3D computer-aided design (CAD) model showing the entire camera assembly, including camera sensor, camera holder, IR mirror, IR LED, miniature connector on the camera holder, and complementary connector permanently fixed to the drive body of the tetrode implant. The camera sensor is mounted on the camera holder via small clips on the sides of the 3D-printed holder. Connectors allow the camera system to be attached to the neural implant for recording sessions, and removed otherwise. (B) Example camera holder. Black and red wires are connected to ground (G) and positive (+) pins on the headstage (respectively), to provide power to the IR LED via the current-limiting resistor. Euro cent coin (left) and ruler (bottom) are shown for size comparison.</p></caption><graphic xlink:href="294397_figS1" position="float" orientation="portrait" hwp:id="graphic-12"/></fig><fig id="figS2" position="float" fig-type="figure" orientation="portrait" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Figure S2:</label><caption hwp:id="caption-10"><title hwp:id="title-46">Correction of video timestamps. Related to <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Figure 1</xref>.</title><p hwp:id="p-97">(A) Distributions of raw frame timestamps (left) generated using the single-board computer controlling the head-mounted camera. Timestamps were normalized by the frame interval of the camera (∆<sub><italic toggle="yes">t</italic></sub>) and mean and <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="294397_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> are indicated by the solid and dashed orange lines, respectively. The cumulative fraction of timestamps with time ±<italic toggle="yes">τ</italic> around the mean (right) reveals that not all timestamps were generated within the interval <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="294397_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> as indicated by the shaded red area. Therefore using the mean delay does not allow for reliable and precise synchronization of camera and neural data. Each row represents results for the video format shown on the left. (B) The same but after correcting video timestamps as described in Methods. All frame timestamps were within the frame interval [0, ∆<sub><italic toggle="yes">t</italic></sub>).</p></caption><graphic xlink:href="294397_figS2" position="float" orientation="portrait" hwp:id="graphic-13"/></fig><fig id="figS3" position="float" fig-type="figure" orientation="portrait" hwp:id="F11" hwp:rev-id="xref-fig-11-1 xref-fig-11-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Figure S3:</label><caption hwp:id="caption-11"><title hwp:id="title-47">Correspondence between whisker pad movement estimates obtained from head-mounted and external camera images. Related to <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-16" hwp:rel-id="F4">Figure 4</xref>.</title><p hwp:id="p-98">(A) Top: Optical flow extracted from external camera images of the whiskers viewed from above in a head-fixed mouse; sampling rate 100 Hz. Bottom: Optical flow extracted simultaneously from head-mounted camera images of the whisker pad; sampling rate 60 Hz. Optical flow was normalized to the interval [−1, 1]. Black lines indicate envelope obtained by low-pass filtering the magnitude of the optical flow with a causal filter with a cutoff frequency of 5 Hz. (B) Zoomed-in traces for gray shaded region in A. (C) Cross-correlation function between envelopes extracted from the external camera signal and the head-mounted camera signal in A. (D) Power spectral densities estimated from the optical flow signals in A.</p></caption><graphic xlink:href="294397_figS3" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><fig id="figS4" position="float" fig-type="figure" orientation="portrait" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3 xref-fig-12-4 xref-fig-12-5 xref-fig-12-6 xref-fig-12-7 xref-fig-12-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figS4</object-id><label>Figure S4:</label><caption hwp:id="caption-12"><title hwp:id="title-48">Details on behavioral segmentation. Related to <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-17" hwp:rel-id="F4">Figure 4</xref>.</title><p hwp:id="p-99">(A) Confusion matrix showing cross-validated classification performance for second mouse. Top: Mouse with implant and camera. Bottom: Mouse with implant only. (B,C) Time spent in the different behavioral states as a function of days after first recording for the two mice. Top: Mouse with implant and camera. Bottom: Mouse with implant only. Same legend as in F. (D,E) Cross-validated segmentation accuracy (true positive rate) for the temporal accelerometer representation ("Temporal", gray bars) used in <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-3" hwp:rel-id="ref-66">Venkatraman et al. (2010)</xref> and the spectral (short-term Fourier transform) representation proposed in this study ("STFT", black bars) for the two mice, both with and without camera. Total time of annotated data: "implant+camera", 90 minutes for mouse 1 and 60 minutes for mouse 2; "implant": 60 minutes for mouse 1 and 60 minutes for mouse 2. (F,G) Cross-validated segmentation accuracy as a function of training data size for the "implant + camera" condition. 100% corresponds to 58 minutes and 45 minutes of recorded data for mouse 1 and 2, respectively. Validation set sizes were 19 and 15 minutes for mouse 1 and 2, respectively, and validation was performed using the same 4-fold cross-validation scheme as in D and E. The results suggest that about about 75% of the annotated training data are suffi cient for behavioral segmentation with <italic toggle="yes">&gt;</italic>90% accuracy (gray dotted line).</p></caption><graphic xlink:href="294397_figS4" position="float" orientation="portrait" hwp:id="graphic-15"/></fig><fig id="figS5" position="float" fig-type="figure" orientation="portrait" hwp:id="F13" hwp:rev-id="xref-fig-13-1 xref-fig-13-2 xref-fig-13-3 xref-fig-13-4 xref-fig-13-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figS5</object-id><label>Figure S5:</label><caption hwp:id="caption-13"><title hwp:id="title-49">Sound-evoked whisker movements measured with the head-mounted camera. Related to <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-20" hwp:rel-id="F5">Figure 5</xref>.</title><p hwp:id="p-100">(A) Top: Sounds were presented via a loudspeaker mounted 1 meter above the center of the circular environment. Bottom: Example frame from head-mounted camera focused on whiskers from above (gray rectangle). (B) Example traces showing head movement (top) and whisker pad movement magnitude (bottom) for an active period when the mouse was exploring the environment, and a still period when the mouse was immobile. During the still period, noise bursts (55 dB SPL, 50 ms) were presented every 2 seconds. Sound onset-triggered head movements (top) and whisker pad movements (bottom). The dashed orange line indicates one standard deviation for movements observed in the active period, for comparison. Sound-evoked whisker movements follow a stereotypical protraction/retraction movement pattern (inset). (D,E) The same as in B,C but for second mouse. Noise bursts (50 db SPL, 50 ms) were presented every second.</p></caption><graphic xlink:href="294397_figS5" position="float" orientation="portrait" hwp:id="graphic-16"/></fig><fig id="figS6" position="float" fig-type="figure" orientation="portrait" hwp:id="F14" hwp:rev-id="xref-fig-14-1 xref-fig-14-2 xref-fig-14-3 xref-fig-14-4 xref-fig-14-5 xref-fig-14-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figS6</object-id><label>Figure S6:</label><caption hwp:id="caption-14"><title hwp:id="title-50">Details on prediction of eye position from head orientation. Related to <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-15" hwp:rel-id="F6">Figure 6</xref>.</title><p hwp:id="p-101">(A) Average weights of linear model as a function of time after first recording, illustrating stability of estimated weights across recording sessions. Top and bottom rows show weights for horizontal and vertical eye position, respectively. Repeated recording days indicate multiple recordings (10 minutes each) on the same day. Same data as for the three mice shown in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-16" hwp:rel-id="F6">Figure 6E</xref>. (B) The same as in A but for 14 recordings in one mouse with accelerometer and gyroscope sensors (yaw). (C) Simultaneous monitoring of both eyes using two head-mounted camera systems in a freely moving mouse. Example traces show prediction errors of the pitch/roll-based nonlinear model for horizontal (top) and vertical (bottom) eye position. Yellow lines, left eye. Black lines, right eye. (D) Correlation between model prediction errors for the two eyes, for 6 experiments in one mouse. Results indicate that failures to predict horizontal eye position based on head orientation (pitch/roll) were strongly correlated between the two eyes. (E) In addition to head acceleration, rotations about the yaw axis were measured using a head-mounted gyroscope (bottom left). Example traces show predictions of a nonlinear model trained with pitch/roll (black) and pitch/roll and yaw rate (orange). Cross-validated prediction performance increased mostly for horizontal eye position (bottom right). Increases in explained variance were statistically significant for both horizontal and vertical eye position, and for both linear and nonlinear models (Wilcoxon signed-rank test, <italic toggle="yes"><italic toggle="yes">P</italic> &lt;</italic> 1 <italic toggle="yes">•</italic> 10<sup><italic toggle="yes">−</italic>3</sup>; <italic toggle="yes">n</italic> = 14 recordings; 10 minutes each).</p></caption><graphic xlink:href="294397_figS6" position="float" orientation="portrait" hwp:id="graphic-17"/></fig><fig id="figS7" position="float" fig-type="figure" orientation="portrait" hwp:id="F15" hwp:rev-id="xref-fig-15-1 xref-fig-15-2 xref-fig-15-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figS7</object-id><label>Figure S7:</label><caption hwp:id="caption-15"><p hwp:id="p-102">Modulation of V1 activity by head movements versus locomotion or whisking. Related to <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-14" hwp:rel-id="F8">Figure 8</xref>.</p><p hwp:id="p-103">(A) Relationship between modulation indices (MIs) for V1 spike trains either recorded when mice were head-fixed on a cylindrical treadmill and aligned to locomotion onsets, or recorded when mice were unrestrained but immobile (body speed ≥ 1 cm/s) and aligned to head movement onsets. Plot shows data from all V1 cells recorded in 3 mice which had firing rates of at least 2 spikes/s in both conditions. Histograms show marginal MI distributions for head movement onsets (top) and locomotion (right). Blue numbers indicate absolute numbers of cells in each quadrant. (B) Comparison of the MI values for the same V1 spike trains for head and eye movements in which the eye movement was predictable from the head movement. (C) Same as B, but for head and eye movements in which the eye movement was not predictable from the head movement. (D) Spike rasters and rate histograms for recordings from two V1 cells, aligned to whisker pad movement onset after excluding periods of head movements. Other conventions as in <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-15" hwp:rel-id="F8">Figure 8C</xref>. (E) The same cells as in H, but with spike times aligned to head movement onsets. (F) Comparison of absolute MIs for V1 activity when aligned to head movement onsets (red bar), whisking onsets when head movements are included (light gray bar), and whisking onsets excluding periods of head movement (dark gray bar). Plot shows mean ± SEM across 16 recordings (10-40 minutes each) in 3 mice.</p></caption><graphic xlink:href="294397_figS7" position="float" orientation="portrait" hwp:id="graphic-18"/></fig><fig id="figS8" position="float" fig-type="figure" orientation="portrait" hwp:id="F16" hwp:rev-id="xref-fig-16-1 xref-fig-16-2 xref-fig-16-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/FIGS8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F16</object-id><object-id pub-id-type="publisher-id">figS8</object-id><label>Figure S8:</label><caption hwp:id="caption-16"><title hwp:id="title-51">Optical flow-based pupil movement extraction.</title><p hwp:id="p-104">(A) Frames showing typical pupil dilation in dark (top), dim light (middle), and normal light (bottom) conditions. Dim and normal light conditions allowed direct fitting of pupil position. (B) Example horizontal and vertical eye movement traces extracted in the dim light condition, either by fitting an ellipse to the pupil (blue line) or by integrating optical flow (black line) of the pupil edge. Trends were removed by high-pass filtering. (C) Correlations between movement onset-triggered ellipse-fitted and optical flow-based pupil position traces for ten minutes of recorded data (144 onsets).</p></caption><graphic xlink:href="294397_figS8" position="float" orientation="portrait" hwp:id="graphic-19"/></fig></sec><sec id="s14" hwp:id="sec-35"><title hwp:id="title-52">Supplemental Tables</title><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;294397v1/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Table S1:</label><caption hwp:id="caption-17"><title hwp:id="title-53">Parts required for building the miniature head-mounted camera system.</title><p hwp:id="p-105">Weights were measured using a calibrated micro scale (Satorius CPA225D, Goettingen, Germany). Prices for steel cannulae, mirror tiles, and wires were estimated without taking the cost of tools (e.g., glass cutter) into account. 3D printed parts were printed using a commercially available printer (Ultimaker 2+, Geldermalsen, the Netherlands) and PLA material (colorfabb, Belfeld, the Netherlands).</p></caption><graphic xlink:href="294397_tblS1" position="float" orientation="portrait" hwp:id="graphic-20"/></table-wrap></sec><sec id="s15" hwp:id="sec-36"><title hwp:id="title-54">Supplemental Movies</title><p hwp:id="p-106">Movie S1: <bold>Different views with the head-mounted camera</bold> Example views and behaviors that can be monitored using the head-mounted camera (interaction with objects, foraging, pinna movements, simultaneous monitoring of both eyes, simultaneous monitoring of eye/whisker movements and environment using two head-mounted cameras).</p><p hwp:id="p-107">Movie S2: <bold>Video image stability</bold>. Examples demonstrating stability of the head-mounted camera system during different behaviors (locomotion, grooming, running on wheel). Shown are raw video frames (i.e. without motion correction).</p><p hwp:id="p-108">Movie S3: <bold>Continuous monitoring of behavior</bold>. Example segment (10 minutes, playback x 25) of the data shown in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-21" hwp:rel-id="F5">Figure 5</xref>.</p><p hwp:id="p-109">Movie S4: <bold>Sound-related head and whisker movements.</bold> An example segment of the data shown in <xref rid="figS5" ref-type="fig" hwp:id="xref-fig-13-5" hwp:rel-id="F13">Figure S5B,C</xref>.</p><p hwp:id="p-110">Movie S5: <bold>Eye movements in freely moving and head-fixed mice.</bold> Eye movements measured using the head-mounted camera system (left) and for the same mouse when it was head-fixexd on a cylindrical treadmill. No stimuli or visual feedback were provided during the head-fixed recording.</p><p hwp:id="p-111">Movie S6: <bold>Prediction of eye movements.</bold> Measured (red) and predicted (blue) eye position of a freely exploring mouse. Predictions based on a nonlinear model and head pitch/roll as shown in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-17" hwp:rel-id="F6">Figure 6D,E</xref>.</p></sec></sec></back></article>
