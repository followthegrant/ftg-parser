<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/461129</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;461129</article-id><article-id pub-id-type="other" hwp:sub-type="slug">461129</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">461129</article-id><article-id pub-id-type="other" hwp:sub-type="tag">461129</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Animal Behavior and Cognition" hwp:journal="biorxiv"><subject>Animal Behavior and Cognition</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">From predictive models to cognitive models: Separable behavioral processes underlying reward learning in the rat</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author; email: <email hwp:id="email-1">kevin.miller@ucl.ac.uk</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3465-2512</contrib-id><name name-style="western" hwp:sortable="Miller Kevin J."><surname>Miller</surname><given-names>Kevin J.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3465-2512"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Botvinick Matthew M."><surname>Botvinick</surname><given-names>Matthew M.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4201-561X</contrib-id><name name-style="western" hwp:sortable="Brody Carlos D."><surname>Brody</surname><given-names>Carlos D.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-4201-561X"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">Princeton Neuroscience Institute, Princeton University</institution>, Princeton, NJ, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Department of Ophthalmology, University College London</institution>, London, <country>UK</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2"><label>3</label><institution hwp:id="institution-3">DeepMind</institution>, London, <country>UK</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Gatsby Computational Neuroscience Unit, University College London</institution>, London, <country>UK</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Howard Hughes Medical Institute and Department of Molecular Biology, Princeton University</institution>, Princeton NJ, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-11-02T20:39:17-07:00">
    <day>2</day><month>11</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-02-19T18:33:22-08:00">
    <day>19</day><month>2</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-11-02T20:46:10-07:00">
    <day>2</day><month>11</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-02-19T18:36:33-08:00">
    <day>19</day><month>2</month><year>2021</year>
  </pub-date><elocation-id>461129</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-11-02"><day>02</day><month>11</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2021-02-19"><day>19</day><month>2</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-02-19"><day>19</day><month>2</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></p></license></permissions><self-uri xlink:href="461129.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/461129v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="461129.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/461129v3/461129v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/461129v3/461129v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Cognitive models are a fundamental tool in computational neuroscience, embodying in software precise hypotheses about the algorithms by which the brain gives rise to behavior. The development of such models is often a hypothesis-first process, drawing on inspiration from the literature and the creativity of the individual researcher to construct a model, and afterwards testing the model against experimental data. Here, we adopt a complementary approach, in which richly characterizing and summarizing the patterns present in a dataset reveals an appropriate cognitive model, without recourse to an <italic toggle="yes">a priori</italic> hypothesis. We apply this approach to a large behavioral dataset from rats performing a dynamic reward learning task. The revealed model suggests that behavior in this task can be understood as a mixture of three components with different timescales: a quick-learning reward-seeking component, a slower-learning perseverative component, and a very slow “gambler’s fallacy” component.</p></abstract><counts><page-count count="19"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">Introduction</title><p hwp:id="p-4">A fundamental goal of cognitive neuroscience is to understand the algorithms by which the brain gives rise to behavior. One of the basic tools used in this pursuit is cognitive modeling. This involves constructing software agents which are capable of performing tasks, and tuning them such that their behavior matches as closely as possible the behavior of human or animal subjects. It also involves treating the algorithms implemented by these agents as precise hypotheses about the neural algorithms implemented by the brain to perform those same tasks (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Corrado &amp; Doya, 2007</xref>; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Daw, 2011</xref>; <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">O’Doherty et al., 2007</xref>; <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Wilson &amp; Collins, 2019</xref>). Cognitive models have been used to shed light on the neural mechanisms of many aspects of cognition, including learning (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Daw &amp; Doya, 2006</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Lee et al., 2012</xref>), memory (<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Norman et al., 2008</xref>), attention (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Heinke &amp; Humphreys, 2005</xref>), and decision-making (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Gold &amp; Shadlen, 2007</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Hanks &amp; Summerfield, 2017</xref>; <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Sugrue et al., 2005</xref>).</p><p hwp:id="p-5">Evaluating a cognitive model requires considering two fundamentally different criteria. The first is whether it is accurately able to predict behavioral data. This predictive accuracy can be quantified, and a number of tools exist for comparing it objectively among models (e.g. cross-validation; for overview see <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Gelman et al., 2013</xref>, Chapter 7). The second is whether it is plausible that the brain implements similar algorithms to the model. Though aspects of this can be quantified – many researchers agree that simpler models are preferable, and tools exist for quantifying model simplicity (but see <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Gelman &amp; Rubin, 1995</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Raftery, 1995</xref>; <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Wagenmakers et al., 2010</xref>) – it is ultimately a subjective evaluation that relies on outside knowledge about the psychology and biology of the system.</p><p hwp:id="p-6">The development of new cognitive models often focuses first on cognitive plausibility. Researchers instantiate plausible hypotheses as software agents, test them against experimental data, and refine them when necessary to produce a better fit. One popular and successful approach, for example, draws inspiration from optimality, looking to algorithms which are provably the best solutions that exist for a particular problem (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Gold &amp; Shadlen, 2002</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Griffiths et al., 2015</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Körding, 2007</xref>). Another draws inspiration from artificial intelligence, looking to algorithms which are successful in practice at solving a wide variety of computational problems (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Daw &amp; Doya, 2006</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Dayan &amp; Niv, 2008</xref>). These and similar approaches have been extremely productive, but are subject to two important limitations. The first is that they typically consider a relatively small number of models. They are typically able to determine that a particular model is the best of a set, but not that it is good in an absolute sense (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">Daw, 2011</xref>). The second is that the development of new models depends on inspiration from adjacent fields or on the creativity of the individual researcher, limiting the pace of hypothesis generation.</p><p hwp:id="p-7">Here, we adopt an alternative approach, which focuses first on predictive accuracy. This approach begins with a model that, given a particular characterization of the behavioral task, is maximally flexible. This model is not itself plausible as a cognitive hypothesis, but provides a first characterization of the patterns present in the dataset, as well as a standard against which to measure the predictive accuracy of other models. The approach continues with a process of successive model reduction: patterns are identified in the fit parameters of a more-flexible model, and a less-flexible model is proposed that embeds those patterns into its structural assumptions. If the predictive accuracy of the less-flexible model is similar to that of the more-flexible one, it is itself adopted as a target for further reduction.</p><p hwp:id="p-8">This approach requires fitting highly flexible models, with large numbers of free parameters, and this requires in turn the collection of large datasets. In the present paper, we took advantage of high-throughput methods for studying decision-making in rats (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Erlich et al., 2011</xref>) to collect a large behavioral dataset from a classic reward-guided learning task, the “two-armed bandit” task (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Samejima et al., 2005</xref>). Our approach of successive model reduction resulted ultimately in a relatively compact model, with just seven free parameters, that captures the major patterns present in rat behavior on this task. This model can be viewed as a cognitive model, describing a candidate for the neural algorithm implemented by the rat brain to solve the task. When viewed in this way, it suggests that behavior is the product of three distinct mechanisms – a reward-seeking mechanism that considers only the most recent few trials, and tends to repeat choices that led to rewards and switch away from those that did not; a perseverative mechanism that considers a slightly longer history, and tends to repeat choices regardless of their outcomes; and a “lose-stay” mechanism that considers a much longer history, and tends to repeat choices that did not lead to rewards. Each of these components was present consistently across rats, with rat-by-rat variability in their precise strengths and time constants. The first two components (reward-seeking and perseverative) are broadly consistent with existing theory, and represent refinements to existing models; the last component (lose-stay) is to our knowledge unexpected.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Results</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Task</title><p hwp:id="p-9">Rats performed a probabilistic reward learning task (the "two-armed bandit" task; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Sul et al., 2010</xref>) in daily sessions (20 rats; 1,946 total sessions; 1,087,140 total trials). In each trial, the rat first entered a central noseport, then selected between a noseport on the left and one on the right (“choice ports”), and finally received either a water reward or a reward omission (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1a</xref>). The probability of reward at each choice port evolved slowly over time according to a random walk (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1b</xref>). Task performance required ongoing learning about the current reward contingencies, and selecting the port with the higher reward probability. The rats performed well at this task, tending to select the nose port that was associated with a higher probability of reward (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1c</xref>). Importantly, rats’ performance did not match that of an agent incorporating a Bayesian ideal observer (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1c</xref>, see Methods), meaning that the patterns in their behavior are not purely a function of the task itself, but rather of the strategy with which the rats approach it.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure One:</label><caption hwp:id="caption-1"><title hwp:id="title-6">Two-armed bandit task for rats.</title><p hwp:id="p-10"><bold>A)</bold> On each trial, rat selects one of two possible choice ports. <bold>B)</bold> Probability of reward at each port evolves over time according to an independent random walk. <bold>C)</bold> Rats tend to select the port with the higher reward probability. Their performance does not match that of an ideal observer agent.</p></caption><graphic xlink:href="461129v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-7">Unconstrained Models</title><p hwp:id="p-11">First, we consider predictive models which characterize in a very general way the relationship between a rat’s choices and the recent history of choices and rewards. For these models, and those that follow, we choose to model the rats choice on each trial, using as predictors the recent history of choices and their outcomes. These “unconstrained” models can be thought of as look-up tables, where each entry gives the probability that the rat will choose left vs. right following a particular history of <italic toggle="yes">N</italic> past choices and rewards (n-markov models; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-3" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>). Specifically, we label the choice made (left or right) and the outcome received (reward or omission) on trial <italic toggle="yes">t</italic> as <italic toggle="yes">C<sub>t</sub></italic> and <italic toggle="yes">O<sub>p</sub></italic> respectively, and we define the history, <italic toggle="yes">H,</italic> as matching between a pair of trial if the choices and outcomes that preceded those trials are identical:
<disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="461129v3_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
where <italic toggle="yes">δ</italic> is the Kroenecker delta function, which takes on a value of one when its arguments match and zero otherwise, and <italic toggle="yes">N</italic> is a hyperparameter governing how many past trials are considered by the model. The <italic toggle="yes">δ<sub>H<sub>t</sub>,<sub>H<sub>τ</sub></sub></sub></italic> variable takes on a value of one only when the <italic toggle="yes">N</italic> choices and outcomes preceding trial <italic toggle="yes">τ</italic> match exactly with those preceding trial <italic toggle="yes">τ.</italic> The model’s predicted choice probability for each port is equal to the fraction of trials with matching histories in which the rat selected that port:
<disp-formula id="eqn2" hwp:id="disp-formula-2" hwp:rev-id="xref-disp-formula-2-1">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="461129v3_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula></p><p hwp:id="p-12">Predicted choice probabilities for <italic toggle="yes">N</italic>=1 and <italic toggle="yes">N</italic>=2 are shown in figure two for an example rat, and in <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-6-1" hwp:rel-id="F6">figure S1</xref> for all rats. The unconstrained model introduces no assumptions about the way in which recent past choices and outcomes influence future behavior. The cost of this flexibility is a very large number of free parameters: one per possible history <italic toggle="yes">H</italic>. Each trial within that history can be one of four types (left/right choice, reward/omission outcome), yielding 4<sup><italic toggle="yes">N</italic></sup> parameters per rat. Fora given dataset, it will be possible to obtain useful estimates of these parameters only up to some value of <italic toggle="yes">N</italic>. For larger values, there will histories that repeat only rarely, resulting in a small number of nonzero entries in <italic toggle="yes">δ<sub>H<sub>t</sub>,<sub>H<sub>τ</sub></sub></sub></italic> and a poor estimate of choice probability. A result of this is overfitting, in which a model fit to a particular dataset provides a better quality of fit to that dataset than to a separate dataset generated in the same way. We evaluate the performance of the models by computing a normalized likelihood score (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-3" hwp:rel-id="ref-14">Daw, 2011</xref>), using two-fold cross-validation. Specifically, we estimate one set ofparameters using only the even-numbered sessions from a particular rat and another using only the odd-numbered sessions, then evaluate each set of parameters both on the sessions used to estimate it (“training dataset”) and on the other sessions (“test dataset”). We find that test-set likelihood is similar to training-set likelihood for models with short history windows (<italic toggle="yes">N</italic> ≲ 4), indicating relatively little overfitting (example rat: <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">figure 2C</xref>, all rats: <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-6-2" hwp:rel-id="F6">figure S1</xref>). For models with longer history windows (<italic toggle="yes">N</italic> ≳ 4), testing-set and training-set likelihood diverge, indicating that substantial overfitting has occurred. The training-set likelihood provides a lower bound on the true quality of model fit, while the testing-set likelihood provides an upper bound. In the region where these are similar, we have a reliable estimate of model quality, which we can use to compare against other models. This estimate provides a ceiling on the performance that can be achieved by any model that considers only recent choices and rewards, since the unconstrained model is the most flexible such model.</p><fig id="figS1" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1:</label><caption hwp:id="caption-7"><title hwp:id="title-32">Fits of the unconstrained model for all rats</title></caption><graphic xlink:href="461129v3_figS1" position="float" orientation="portrait" hwp:id="graphic-22"/></fig><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure Two:</label><caption hwp:id="caption-2"><title hwp:id="title-8">Unconstrained models.</title><p hwp:id="p-13"><bold>A)</bold> Unconstrained model considering one previous trial, fit to a dataset from an example rat. This model comprises four parameters, each giving the log choice probability ratio immediately following each of the four possible trial types. <bold>B)</bold> Unconstrained model considering two previous trials, fit to the same example rat. This model comprises sixteen parameters, each giving the log choice probability ratio immediately following a particular pair of trial types. <bold>C)</bold> Quality of fit for unconstrained models considering different history lengths. Normalized likelihood was computed using cross-validation, and likelihoods for both the fit datasets (training dataset) and the held-out datasets (testing dataset) are plotted. Likelihoods are similar for both datasets when the order is small.</p></caption><graphic xlink:href="461129v3_fig2" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-9">Linear models</title><p hwp:id="p-14">Next, we consider predictive models which mitigate overfitting by introducing an assumption about the way in which past trials choices and outcomes affect future behavior. In particular, these models assume that each past trial exerts an influence on future choice that is independent of the influence of all other past trials. The influence of each past trial is summed, and this sum is used to compute choice probability. The most general models in this family fit the relationship of the sum to choice probability (linear-nonlinear-poisson models; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Corrado et al., 2005</xref>), but we consider here models which additionally assume that this relationship is well-approximated by a logit function (logistic regression models; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Lau &amp; Glimcher, 2005</xref>). For our task, the most general of these can be written as:
<disp-formula id="eqn2a" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="461129v3_eqn2a.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
where <italic toggle="yes">C<sub>t</sub></italic> is the choice made on trial <italic toggle="yes">t</italic> (coded as +1 for right and −1 for left), <italic toggle="yes">O<sub>t</sub></italic> is the outcome received (+1 for a reward and −1 for an omission), and the <italic toggle="yes">βs</italic> are fit weighting parameters. Like the unconstrained models, these models consider a history of <italic toggle="yes">N</italic> recent trials, but unlike them, they do not assign a parameter to each possible history. Instead, they assign a set of weights to each slot within that history, quantifying the effect on upcoming choice when a trial of each type occupies that slot. In our model, these weights are organized into three vectors: <italic toggle="yes">β<sub>X</sub></italic>, which quantifies a “reward seeking” pattern in which the animal tends to repeat choices that have led to rewards and to switch away from choices that have led to omissions; <italic toggle="yes">β<sub>C</sub></italic>, which quantifies a “choice perseveration” pattern in which the animal tends to repeat past choices without regard to their outcomes; and <italic toggle="yes">β<sub>O</sub></italic>, which would quantify any pattern in which outcomes affect choice without regard to the side on which they were delivered (the symmetry of the task suggests this term is likely to be near-zero). The model also contains a bias parameter <italic toggle="yes">β<sub>bias</sub></italic>, which quantifies any fixed left/right choice bias.</p><p hwp:id="p-15">Fits of the linear model reveal large and consistent effects of reward seeking and choice perseveration, and relatively weak and inconsistent main effects of outcome (example rat: <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3A</xref>; all rats: <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure S2</xref>). These fits were relatively resistant to overfitting: normalized likelihoods for the testing and training datasets diverged from one another at much larger <italic toggle="yes">N</italic> than those of the unconstrained model (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3B</xref>, compare to <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2B</xref>). This resistance to overfitting comes at the cost of the assumption that past outcomes contribute independently to current choice. One way to test this assumption is to compare the cross-validated likelihood scores of the unconstrained and the linear models directly: if the additional flexibility of the unconstrained model allows it to achieve a higher score, then meaningful nonlinear interactions must exist in the dataset. We find that the linear and the unconstrained model earn similar likelihood scores up to <italic toggle="yes">N</italic> ≈ 4, and that for larger <italic toggle="yes">N</italic> the linear model achieves larger scores, as the unconstrained models begin to overfit (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3C</xref>). At no value of <italic toggle="yes">N</italic> do the unconstrained models outperform the linear models. This partially validates the assumptions of the linear models. It rules out strategies incorporating nonlinear interactions within the most recent few trials (e.g. “repeat the choice until getting two omissions in a row, then switch”), but not those incorporating longer-term nonlinear interactions (e.g. “if the most recent choice matches the choice from six trials ago, repeat it”). These results also provide a standard against which to evaluate other models which incorporate both its assumptions as well as others. Such models will be less flexible than the linear models, and the performance of the linear models provides a ceiling on the predictive accuracy that can be expected from them.</p><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Figure S2:</label><caption hwp:id="caption-8"><title hwp:id="title-33">Fits of the linear model for all rats</title></caption><graphic xlink:href="461129v3_figS2" position="float" orientation="portrait" hwp:id="graphic-23"/></fig><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure Three:</label><caption hwp:id="caption-3"><title hwp:id="title-10">Linear Models.</title><p hwp:id="p-16"><bold>A)</bold> Linear model considering a history of length twenty, fit to the example rat. The model consists of three sets of weights characterizing the influence of past trials on current choice. Reward-seeking weights (left) characterize the tendency to repeat choices that led to rewards and switch away from choices that led to omissions; perseveration weights (middle) characterize the tendency to repeat past choices regardless of their outcomes; outcome weights (right) characterize the tendency for outcomes (reward/omission) to influence choice regardless of which choice preceded them. <bold>B)</bold> Quality of model fit for the example rat, as a function of the number of past trials considered. Normalized likelihood for both the fit datasets (“training dataset”) and the held-out datasets (“testing dataset”) are plotted. Likelihoods are similar for both datasets, even when considering large numbers of past trials (compare to <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2C</xref>). <bold>C)</bold> Difference in normalized cross-validated likelihood between linear and unconstrained models considering the same history length. Quality of fit of the linear model approximates or exceeds that of the unconstrained model for all history lengths.</p></caption><graphic xlink:href="461129v3_fig3" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-11">Mixture-of-Exponentials Models</title><p hwp:id="p-17">Inspecting the fit weights of the linear models (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3a</xref>, <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure S2</xref>), several patterns are apparent. For all rats, reward-seeking weights are large and positive for the most recent few trials (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figure 3A</xref>, left), while perseverative weights were smaller, but extended further back in time (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Figure 3A</xref>, middle). Outcome weights were small and showed patterns that were inconsistent across rats (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Figure 3A</xref>, right). We sought to capture these patterns using “mixture of exponentials” models, each comprising a mixture of exponential components.</p><p hwp:id="p-18">Each exponential component exhibits either reward-seeking or perseveration, and was characterized by an update rate <italic toggle="yes">α</italic>, determining the component’s timescale, as well as by a weighting parameter <italic toggle="yes">β</italic>, determining the strength of its influence on behavior. These parameters govern the evolution of a hidden variable, which we denote <italic toggle="yes">V</italic> for reward-seeking components and <italic toggle="yes">H</italic> for perseverative components. These hidden variables were set to zero on the first trial of each session and updated after each trial according to the following rules:
<disp-formula id="eqn3" hwp:id="disp-formula-4" hwp:rev-id="xref-disp-formula-4-1 xref-disp-formula-4-2">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="461129v3_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
where <italic toggle="yes">C<sub>t</sub></italic> and <italic toggle="yes">O<sub>t</sub></italic> are the choice made and outcome received on trial <italic toggle="yes">t</italic> (coded +1 or −1, as above). Choice probability on each trial was determined by summing the influence of each of the exponential processes, as well as overall bias:
<disp-formula id="eqn4" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="461129v3_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
where <italic toggle="yes">m</italic> and <italic toggle="yes">n</italic> are hyperparameters controlling the number of reward-seeking and of perseverative components, and <italic toggle="yes">β<sub>v</sub></italic> and <italic toggle="yes">β<sub>h</sub></italic> are vectors of weighting parameters (of length <italic toggle="yes">m</italic> and <italic toggle="yes">n</italic> respectively).</p><p hwp:id="p-19">We computed the quality of model fit as a function of the number of components included (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4A</xref>). We found that the dataset was best explained by a mixture of exactly two reward-seeking and two perseverative components. Removing a component of either type resulted in a lower quality of fit, whether the removed component was reward-seeking (mean decrease in normalized likelihood of 0.21 percentage points, standard error 0.04, p=2×10<sup>-4</sup>, signrank test) or perseverative (mean 0.31, sem 0.05, p=1×10<sup>-4</sup>). Allowing the model to use an additional component of either type did not meaningfully improve quality of fit, whether the additional component was reward-seeking (mean improvement 0.01 percentage points, sem 0.006, p=0.14) or perseverative (mean improvement 0.01 percentage points, sem 0.02, p=0.07). We also found that a model with two reward-seeking and two perseverative components narrowly outperformed the best linear model for each rat (mean difference in normalized likelihood 0.16 percentage points, sem 0.04, p=0.001; <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4A</xref>).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7 xref-fig-4-8 xref-fig-4-9 xref-fig-4-10 xref-fig-4-11 xref-fig-4-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><title hwp:id="title-12">Mixture-of-Exponentials Models.</title><p hwp:id="p-20"><bold>A)</bold> Difference in normalized cross-validated likelihood between mixture-of-exponentials models with different numbers of exponential components and the best linear model for each rat. Left: Model with two reward-seeking components and different numbers of perseverative components. Left inset: Magnified view showing normalized likelihood differences for models with one, two, or three reward-seeking components. Right: Models with two perseverative components and different numbers of reward-seeking components. Right inset: Magnified view showing normalized likelihood differences for models with one, two, or three reward-seeking components. Quality of fit increases until two processes of each type are included, and then does not continue to increase.<bold>B)</bold> Learning rates of a mixture-of-exponentials model with two processes of each type fit to each rat. Learning rates of the slower perseverative and the slower reward-seeking component are strongly correlated, with a slope close to one. <bold>C)</bold> Weights of a mixture-of-exponentials model with two processes of each type fit to each rat. Weights of the slower perseverative and the slower reward-seeking component are strongly correlated, with a slope close to negative one.</p></caption><graphic xlink:href="461129v3_fig4" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-21">Having established that it provides a good match to our rats’ data, we fit a model containing two reward-seeking and two perseverative components to the full dataset for each rat, and examined its fit parameters. We label each component “faster” or “slower” according to whether it had the larger or the smaller learning rate of the two components of its type. To reveal patterns among the fit parameters, we plot the learning rates (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig 4B</xref>) and weights (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig 4C</xref>) of the slower reward-seeking component against those of the slower perseverative component (red), and similarly the parameters of the faster reward-seeking component against those of the faster reward-seeking component (green). The learning rate of the slow reward-seeking component and of the slow perseverative component were highly correlated (r<sup>2</sup>=0.50, p&lt;1×10<sup>-3</sup>), and many of them were nearly equal (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig 4B</xref>, red points close to diagonal dotted line). This suggests a further model reduction: these two slow components may reflect the same underlying process. Similarly, the fit weight of the slow reward-seeking component and of the slow perseverative component were highly correlated (r<sup>2</sup>=0.68, p&lt;1×10<sup>-5</sup>), and many of them were nearly equal in sign, though opposite in magnitude (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig 4C</xref>, red points close to diagonal dotted line). This suggests that the slow underlying process may exhibit equal-and-opposite influences of reward-seeking and perseveration. The parameters associated with the two fast components showed little correlation (r<sup>2</sup>=0.04, p=0.68 for learning rates; r<sup>2</sup>=0.13, p=0.12 for weights), suggesting that each reflects a separate underlying process (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Figure 4B</xref> and <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-8" hwp:rel-id="F4">4C</xref>, green points show no clear structure).</p><p hwp:id="p-22">In sum, fits of the mixture-of-exponentials models indicate that rat behavior can be well-captured by a model containing two reward-seeking components and two perseverative components. Fit parameters suggest that the slower reward-seeking and slower perseverative components might reflect a common underlying process containing equal-and-opposite influences of each.</p></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-13">Cognitive Model</title><p hwp:id="p-23">Finally, we sought to capture the behavioral patterns revealed by the mixture-of-exponentials models using a simpler model, and to express this model using notation typical of cognitive models. We write this as a mixture-of-agents model, in which each rat is modeled as a mixture of three independent agents following different strategies: a reward-seeking agent, a perseverative agent, and an agent influenced by each pattern in an equal-but-opposite way.</p><p hwp:id="p-24">The reward-seeking agent represents a single internal variable <italic toggle="yes">V.</italic> This quantity is initialized to zero at the beginning of each session, and updated after each trial according to the following rule:
<disp-formula id="eqn5" hwp:id="disp-formula-6" hwp:rev-id="xref-disp-formula-6-1">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="461129v3_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula></p><p hwp:id="p-25">This is equivalent to an exponential reward-seeking process, as in <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-4-1" hwp:rel-id="disp-formula-4">equation 3</xref>. The variable <italic toggle="yes">V</italic> is updated towards a value of+1 following rewarded left choices (<italic toggle="yes">C<sub>t</sub></italic>=1; <italic toggle="yes">O<sub>t</sub></italic>=1) and unrewarded right choices (<italic toggle="yes">C<sub>t</sub></italic> =−1; <italic toggle="yes">O<sub>t</sub></italic> =−1), and updated towards a value of −1 following rewarded right choices (<italic toggle="yes">C</italic> =-1; <italic toggle="yes">O<sub>t</sub></italic> =1) and unrewarded left choices (<italic toggle="yes">C<sub>t</sub></italic>= 1; <italic toggle="yes">O<sub>t</sub></italic>=−1). It represents the reward-seeking agent’s current relative preference for the right vs. the left choice port. This mechanism is similar to error-correcting mechanisms found in many animal learning and reinforcement learning theories (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Bush &amp; Mosteller, 1953</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Rescorla &amp; Wagner, 1972</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Sutton &amp; Barto, 2017</xref>), though different in that these models typically track the expected values of different actions independently, whereas the reward-seeking agent tracks only a single variable, which is affected by outcomes at both ports.</p><p hwp:id="p-26">The perseverative agent represents a single internal variable <italic toggle="yes">H,</italic> which is initialized to zero at the beginning of each session, and updated after each trial according to:
<disp-formula id="eqn6" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="461129v3_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula></p><p hwp:id="p-27">This is equivalent to an exponential perseverative process, as in <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-4-2" hwp:rel-id="disp-formula-4">equation 3</xref>. The variable <italic toggle="yes">H</italic> is updated towards +1 following left choices, regardless of reward, and towards a value of −1 following right choices. It represents a recency-weighted estimate of how often each port has been chosen. This can be thought of as a process of Hebbian plasticity that strengthens the representations of recently taken actions – a mechanism which has been proposed to underlie the formation of habits (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Ashby et al., 2010</xref>; <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Miller et al., 2019</xref>).</p><p hwp:id="p-28">The final agent represents an internal variable <italic toggle="yes">G.</italic> It is initialized to zero at the beginning of each session, and updated after each trial according to:
<disp-formula id="eqn7" hwp:id="disp-formula-8">
<alternatives hwp:id="alternatives-8"><graphic xlink:href="461129v3_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula></p><p hwp:id="p-29">This is equivalent to an exponential process containing positive perseveration balanced by equal negative reward-seeking. Following rewarded trials (<italic toggle="yes">O<sub>t</sub></italic>= 1), these influences cancel one another, and <italic toggle="yes">G</italic> is updated towards 0. Following unrewarded trials (<italic toggle="yes">O<sub>t</sub></italic>= −1), these influences add to one another, and <italic toggle="yes">G</italic> is updated towards either +2 (left choices, <italic toggle="yes">C<sub>t</sub></italic>= 1)or −2 (right choices, <italic toggle="yes">C<sub>t</sub></italic>= −1). These updates result in <italic toggle="yes">G</italic> tracking a recency-weighted average of the relative number of losses incurred at each port. This pattern is reminiscent of the “gambler’s fallacy”, in which human subjects seem to believe that a long sequence of losing outcomes predicts that a winning outcome is likely to follow (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Oskarsson et al., 2009</xref>; <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Tversky &amp; Kahneman, 1971</xref>).The choice made by the model on each trial is determined by a weighted average of <italic toggle="yes">V</italic>, <italic toggle="yes">H</italic>, and <italic toggle="yes">G</italic>, as well as bias:
<disp-formula id="eqn8" hwp:id="disp-formula-9" hwp:rev-id="xref-disp-formula-9-1">
<alternatives hwp:id="alternatives-9"><graphic xlink:href="461129v3_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives>
</disp-formula></p><p hwp:id="p-30">The complete cognitive model has seven parameters: three learning rates (<italic toggle="yes">α<sub>v</sub></italic>, <italic toggle="yes">α<sub>h</sub></italic> and <italic toggle="yes">α<sub>g</sub></italic>) and four weights (<italic toggle="yes">β<sub>v</sub></italic>, <italic toggle="yes">β<sub>h</sub></italic>, <italic toggle="yes">β<sub>g</sub></italic>, and <italic toggle="yes">β<sub>bias</sub></italic>). It can be viewed as a simplification of the four-component mixture-of-exponentials model, with parameter equivalencies given by <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>. Like this model, the cognitive model also narrowly outperforms the best linear model in terms of cross-validated likelihood (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig 5</xref>, blue). Removing any one of its components results in a significant decrease in quality of fit (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig 5</xref>, red), while adding additional components of any of the three types does not significantly increase quality of fit (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig 5</xref>, green).</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-5"><p hwp:id="p-31">Parameter equivalencies between a mixture-of-exponentials model with three components and the cognitive model.</p></caption><graphic xlink:href="461129v3_tbl1" position="float" orientation="portrait" hwp:id="graphic-14"/></table-wrap><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;461129v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-6"><title hwp:id="title-14">Cognitive Model Comparison.</title><p hwp:id="p-32">Difference in normalized cross-validated likelihood between our cognitive model and various alternatives. The first two alternatives (blue) are models described earlier in the paper: the linear model and the mixture of exponentials model. The next three alternatives (green) are versions of our model that have been expanded to include an extra copy of one of the three components. The next three alternatives (red) are versions of our model that have been reduced by removing one of the three components. Finally are five models from the literature (orange): Q-Learning (e.g. <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-3" hwp:rel-id="ref-31">Kim et al., 2009</xref>), Optimistic RL (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Lefebvre et al., 2017</xref>), Differential Forgetting Q-Learning (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-4" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>), a hybrid combining both optimistic and differential forgetting mechanisms, and a Bayesian ideal observer model (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">Daw et al., 2006</xref>). The inset shows an expanded view of the first five models.</p></caption><graphic xlink:href="461129v3_fig5" position="float" orientation="portrait" hwp:id="graphic-15"/></fig></sec><sec id="s2f" hwp:id="sec-8"><title hwp:id="title-15">Comparison to Existing Models</title><p hwp:id="p-33">Reward learning tasks like ours have been extensively studied in behavioral neuroscience, and many cognitive models have been used to interpret behavioral and neural data. We evaluated several of these to see whether they could provide a better fit than our cognitive model (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5</xref>, orange). The first was a q-learning model, variants of which have been used in a wide variety of tasks and species (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Bari et al., 2019</xref>; e.g. <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Barraclough et al., 2004</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-4" hwp:rel-id="ref-31">Kim et al., 2009</xref>). This model is similar to the reward-seeking component of our model in that it maintains internal variables related to reward history, but different in that it uses separate variables to track reward history at each port, which are updated only on trials where their port is visited. The second (“optimistic RL” <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">Lefebvre et al., 2017</xref>) is an extension to the q-learning model in which different learning rates are applied to better-than-expected and to worse-than expected outcomes. The third (“differential forgetting” <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-5" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>) is an alternative extension, which updates estimates both for visited and unvisited ports, using different learning rates, and also applies different learning targets in the case of positive and negative outcomes. Fourth was a hybrid model containing both optimistic and differential forgetting extensions. Finally, we also considered an ideal observer model, which uses a Bayesian filter to update a belief distribution over the current reward probabilities (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">Daw et al., 2006</xref>), and selects actions based both on immediate expected reward and on reduction of uncertainty (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Wilson et al., 2014</xref>). We evaluated each of these models using two-fold cross validation, and found that our three-component cognitive model provided a better fit than any of them, for all rats in our dataset (n=20; p=10<sup>-4</sup>, signrank test). This validates the idea that our approach of successive model reduction can result in cognitive models that compare favorably, in terms of predictive performance, with others in the literature.</p></sec></sec><sec id="s3" hwp:id="sec-9"><title hwp:id="title-16">Discussion</title><p hwp:id="p-34">We trained rats to perform a reward-learning task in which they repeatedly selected between two ports with constantly-changing probabilities of reinforcement, and we sought to build a cognitive model capturing the patterns in their behavior. This work adds to a large body of previous work modeling behavior in tasks of this kind (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-4" hwp:rel-id="ref-17">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-6" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-5" hwp:rel-id="ref-31">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Samejima et al., 2005</xref>), building on it by collecting a large behavioral dataset, and by adopting a data-first modeling approach to reveal behavioral patterns in a hypothesis-neutral way.</p><p hwp:id="p-35">In keeping with this literature, we built models which sought to predict the choice of the subject on each trial, using as predictors the history of choices made and rewards received on recent trials. In making this decision, we choose to ignore (to treat as noise) all other factors that might influence choice, including factors such as satiety or fatigue that may play an important role in modulating behavior. We began by fitting the most flexible models possible that are consistent with this choice, the unconstrained models. Because of their flexibility, these models provide a ceiling on the quality of fit achievable by any model that considers the same inputs as they do. In the regime where the unconstrained model does not overfit, this ceiling is also a useful benchmark – any model which accurately captures the generative process should fit the data approximately as well as the unconstrained model does. An important precedent to this benchmarking step is found in <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-7" hwp:rel-id="ref-30">Ito &amp; Doya (2009)</xref>, who similarly fit unconstrained models, and compared them to several cognitive models. Our work adds to this by comparing training-dataset and testing-dataset performance to determine the range in which the benchmark established by these models is valid, and by utilizing a larger dataset, which allows us to establish a benchmark for models considering longer histories.</p><p hwp:id="p-36">With this benchmark in hand, we fit a set of models with an additional constraint of linearity – these models assume that the outcome of each past trial contributes independently to present choice. The performance of the linear models matches the benchmark established by the unconstrained models, suggesting that their linearity assumption is a valid one. These linear models have an important precedent in earlier work (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Lau &amp; Glimcher, 2005</xref>), which used similar models to characterize behavioral data from primates performing reward-guided decision tasks. Our work builds on this in that it validates these models with respect to the benchmark from the unconstrained models, and constructs models that are further reduced, including one that can be viewed as a cognitive model.</p><p hwp:id="p-37">Fits of the linear model indicate that rats have strong tendencies of “reward-seeking” (repeat choices that lead to reward, switch from those that do not) and “perseveration” (repeat past choices without regard to outcome). We sought to compress these patterns using a model that assumes they arise from a mixture of several exponential processes with distinct weights. We found that the dataset from each rat was best modeled as arising from a mixture of exactly two exponential processes of each type, and that the parameters of these showed consistent patterns across rats. Specifically, the slower reward-seeking and the slower perseverative process had learning rates that were nearly equal, and had weights that were nearly equal-and-opposite. Finally, we wrote a cognitive model that captured these patterns using a mixture of agents model. This type of model has important precedents in mixture-of-agents models for related tasks, including working-memory/reinforcement-learning (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Collins &amp; Frank, 2012</xref>), episodic-memory/reinforcement-learning (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Bornstein &amp; Daw, 2012</xref>) and model-based/model-free (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Daw et al., 2011</xref>). Our cognitive model comprised a mixture of three agents. The fastest was a reward-seeking agent, while the middle process was a perseverative “habits” agent, and the slowest was a “lose-stay” agent that tends to repeat choices that have led in the past to losses.</p><p hwp:id="p-38">The first, reward-seeking, component of the model is consistent with a long body of literature on reward-guided decision making, beginning with Thorndike’s “law of effect”, which holds that actions that have led to reward in the past are likely to be repeated in the future (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Thorndike, 1911</xref>). It is reminiscent of popular reinforcement learning algorithms, such as the delta rule (<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-2" hwp:rel-id="ref-50">Sutton &amp; Barto 2017</xref>, which are commonly used to model behavior on tasks of this kind (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Bari et al., 2019</xref>; <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">Barraclough et al., 2004</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-5" hwp:rel-id="ref-17">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-6" hwp:rel-id="ref-31">Kim et al., 2009</xref>). These algorithms are similar to our exponential process in that they maintain a cached variable, updating it on each trial by taking its weighted average with a value from that trial’s observation. They are different in that they typically maintain one cached value per possible action, while our process caches only a single value, representing the relative history of recent outcomes at each port.</p><p hwp:id="p-39">Perseveration in reward-guided tasks has been observed in a wide variety of tasks and species (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Balcarras et al., 2016</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-8" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-7" hwp:rel-id="ref-31">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-3" hwp:rel-id="ref-33">Lau &amp; Glimcher, 2005</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Lee et al., 2005</xref>; <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Rutledge et al., 2009</xref>). One common way of modeling it is to consider it as arising from some modulation to a reward-seeking process – for example using different learning rates for positive and negative outcomes (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-9" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-3" hwp:rel-id="ref-36">Lefebvre et al., 2017</xref>), or by fitting a perseverative weight that considers only the immediately previous trial (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">Daw et al., 2011</xref>). Our results suggest that perseveration results from a process that is separable from reward-seeking (having its own timecourse), and that it considers a relatively large number of past trials. The idea of a separable perseverative process is consistent with Thorndike’s second law, the “law of exercise”, which holds that actions which have often been taken in the past are likely to be repeated in the future (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">Thorndike, 1911</xref>). It is also consistent with ideas from the psychology of habit formation (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Dickinson, 1985</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Graybiel, 2008</xref>; <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Wood &amp; Rünger, 2016</xref>), which propose that habits are fixed patterns of responding that result from simple repetition of past actions (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">Ashby et al., 2010</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Miller et al., 2018</xref>, 2019). While we are not aware of other models for bandit tasks that incorporate this mechanism, a very similar one has been proposed to play a role in sensory decision-making (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Akaishi et al., 2014</xref>).</p><p hwp:id="p-40">The “lose-stay” pattern of repeating choices that have led in the past to losses is reminiscent of the “gambler’s fallacy”, in which humans seem to believe that a series of losing outcomes predicts that a winning outcome is likely imminent. This pattern occurs when humans attempt to predict the next outcome of a process that they believe to be random, such as the flip of a coin or the spin of a roulette wheel (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-2" hwp:rel-id="ref-41">Oskarsson et al., 2009</xref>). While no clear consensus exists as to the cause of the gambler’s fallacy, several explanations have been proposed. Perhaps the oldest proposes that humans believe in a “law of small numbers”, expecting that even very short sequences will include an equal number of winning and losing outcomes, and that a surplus of losses in the past must therefore be balanced out by an increased proportion of wins in the future (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">Tversky &amp; Kahneman, 1971</xref>). This is proposed to be an example of the more-general “representativeness heuristic”, in which people expect each instance from a category to embody the properties of that category as a whole. More recent accounts propose that humans have complex, if often erroneous, internal models of what to expect from “random” processes, and that these give rise to the gambler’s fallacy (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-3" hwp:rel-id="ref-41">Oskarsson et al., 2009</xref>). A final proposal is that the gambler’s fallacy results from a rational bias that arises when a resource-limited learning system attempts to learn from the true statistics of random sequences (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Hahn &amp; Warren, 2009</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Sun et al., 2015</xref>). None of these proposals seems to us to provide an immediate explanation of our lose-stay agent, but they do raise hope that some related cognitive explanation might be possible. To our knowledge, previous studies using similar tasks have not reported a pattern of this kind. It is possible that it is specific in some way to our experimental setup (e.g. specific to rats, specific to nose-ports, specific to fluid rewards, etc). It is also possible that it is present broadly in behavior on tasks of this kind, but has not previously been identified on account of being unexpected and somewhat subtle.</p><p hwp:id="p-41">This three-agents cognitive model has a number of features that make it a reasonable hypothesis about the algorithm actually used by the brain to perform our task. Each of its agents (reward-seeking, perseveration, and lose-stay) has a relatively simple implementation, with just two free parameters that govern the evolution of just one hidden variable, and is cognitively plausible given the rest of what is known about the psychology and biology of behavior on tasks of this kind. It makes several claims that are distinct from those of other cognitive models in the field. One of these is that the reward-seeking component of behavior contains a single latent variable that is influenced by rewards following both choices, rather than separate latent variables for each available action. Another is that there is a separable lose-stay (gambler’s fallacy) component of behavior. This model also provides a better quantitative fit to our dataset than do other cognitive models drawn from the literature. We produced this model by a process of successive model reduction: beginning with a highly general model that provides a good fit to data but is not itself cognitively plausible, identifying patterns in the fit parameters of this model, writing a simpler model that embodies those patterns as structural assumptions, and repeating. We believe that it validates the idea that this approach of successive reduction is capable of producing useful cognitive models.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-17">Acknowledgements</title><p hwp:id="p-42">We would like to thank Stefano Palminteri, Alex Piet, Kim Stachenfeld, Nathaniel Daw, Yael Niv, Bas van Ophuesden, Chuck Kopec, and Jeff Erlich for helpful discussions, and Kim Stachenfeld, Alex Piet, and Anna Lebedeva for helpful comments on the manuscript.</p></ack><sec id="s4" hwp:id="sec-10"><title hwp:id="title-18">Methods</title><sec id="s4a" hwp:id="sec-11"><title hwp:id="title-19">Subjects</title><p hwp:id="p-43">All subjects were adult male Long-Evans rats (Taconic Biosciences), placed on a restricted water schedule to motivate them to work for water rewards. Rats were housed on a reverse 12-hour light cycle and trained during the dark phase of the cycle. Rats were pair housed during behavioral training. Animal use procedures were approved by the Princeton University Institutional Animal Care and Use Committee and carried out in accordance with NIH standards.</p></sec><sec id="s4b" hwp:id="sec-12"><title hwp:id="title-20">Behavioral Apparatus</title><p hwp:id="p-44">Rats were trained in custom behavioral chambers (Island Motion, NY) located inside sound- and light-attenuated boxes (Coulbourn Instruments, PA). Each chamber was outfitted with three nose ports arranged horizontally. Each port contained a white light emitting diode (LED) for delivering visual stimuli, as well as an infrared LED and phototransistor for detecting rats’ entries into the port. The left and right ports also contained sipper tubes for delivering water rewards. Training was controlled by a custom protocol written using the bControl behavioral control system. Rats were placed into and removed from training chambers by technicians blind to the experiment being run.</p></sec><sec id="s4c" hwp:id="sec-13"><title hwp:id="title-21">Two-Armed Bandit Task: Training Pipeline</title><p hwp:id="p-45">Here we outline a procedure suitable for efficiently training naive rats on the two-armed bandit task. Automated code for training rats using this pipeline via the bControl behavioral control system will be made available on the Brody lab website (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://brodylab.org/" ext-link-type="uri" xlink:href="http://brodylab.org/" hwp:id="ext-link-2">http://brodylab.org/</ext-link>). This formalization of our training procedure into a software pipeline should also facilitate efforts to replicate our task in other labs, as the pipeline can readily be downloaded and identically re-run.</p><sec id="s4c1" hwp:id="sec-14"><title hwp:id="title-22">Phase I: Sipper tube familiarization</title><p hwp:id="p-46">Training began with an initial period of sipper tube familiarization. In each trial of this phase, the LED in either the left or the right port would illuminate, and a 50 uL reward would be delivered if the rat entered this port. Training in this phase continued until the rat was completing approximately 200 trials per day. Typically, training in this phase requires three to five sessions.</p></sec><sec id="s4c2" hwp:id="sec-15"><title hwp:id="title-23">Phase II: Trial structure familiarization</title><p hwp:id="p-47">Training in this phase typically lasted three to seven days. In the second phase, each trial began with illumination of the central port’s LED. When the rat entered this port, its LED would extinguish, and the LED in either the left or the right port would illuminate. Entry into this port resulted in reward. Training continued until the rat was completing approximately 200 trials per day.</p></sec><sec id="s4c3" hwp:id="sec-16"><title hwp:id="title-24">Phase IIIa: Performance-triggered flips</title><p hwp:id="p-48">In this phase, decision-making and dynamic reward contingencies are introduced. Trials began with illumination of the central port’s LED. In 90% of trials (“free choice trials”), both the left and right port’s LEDs would illuminate and the rat could enter either. In half of the remaining trials (5% of total), only the left port’s LED would illuminate, and in the other half, only the right port’s LED would illuminate. In these “instructed choice” trials, the rat was required to enter the lit port in order for the task to continue. In all trial types, reward contingencies were dynamic and changed in blocks. In each block, either the left or the right port was “good”, while the other was “bad”. Entry into the good port resulted in reward, while entry into the bad port resulted only in the initiation of a new trial. Whether the left or the right port was good was initialized randomly at the beginning of each session, and reversed if the rat selected the good port on more than 80% of the previous 50 free choice trials. Training in this phase continues until the rat achieves a large number of flips per session. Typically this requires only one or two sessions.</p></sec><sec id="s4c4" hwp:id="sec-17"><title hwp:id="title-25">Phases IIIb and IIIc</title><p hwp:id="p-49">These phases are the same as phase IIIa, except that the good and bad ports are rewarded 90% and 10% of the time in phase IIIb, and 80% and 20% of the time in phase IIIc. Training in each phase continues until the rat achieves a large number of block changes per session. Typically this requires two to five sessions.</p></sec><sec id="s4c5" hwp:id="sec-18"><title hwp:id="title-26">Final Task: Drifting Two-Armed Bandit Task</title><p hwp:id="p-50">In the final task, reward probability of the two ports no longer depends on the behavior of the rat, and no longer shifts in blocks. Reward probability of the left and right port are sampled randomly between zero and one at the start of each session, and are updated after each trial following a Gaussian random walk. Specifically, the reward probability is sampled according to <monospace>P<sub>t+1</sub> ~ Gaussian(μ=P<sub>t</sub>, σ=0.15)</monospace>, where <italic toggle="yes">P<sub>t</sub></italic> is the probability of reward on trial <italic toggle="yes">t</italic>. If this would result in a probability greater than one (less than zero), <italic toggle="yes">P<sub>t</sub></italic> is set to one (zero) instead.</p></sec></sec><sec id="s5a" hwp:id="sec-19"><title hwp:id="title-27">Model Fitting and Comparison</title><p hwp:id="p-51">All model fitting was done using custom code written in Matlab (Mathworks). In all cases, models were fit by maximum-likelihood or maximum-a-posteriori, with separate fits for each rat. Unconstrained models (n-markov models) were fit according to <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">equations one</xref> and <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-2-1" hwp:rel-id="disp-formula-2">two</xref> (see Main Text). Linear models were fit using the Matlab function <monospace>glmfit</monospace>, implementing unregularized logistic regression. Mixture-of-exponentials models and cognitive models were fit using custom likelihood functions written in the probabilistic programming language Stan (B. <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Carpenter et al., 2016</xref>), accessed using the MatlabStan (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Stan Development Team, 2016</xref>) interface. Mixture-of-exponentials models were fit using weakly informative priors: a <monospace>beta(3,3)</monospace> prior for learning rate parameters, and a <monospace>gaussian(0,1)</monospace> prior for weighting parameters.</p><p hwp:id="p-52">For analysis reporting fit parameters (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2ab</xref>, <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Fig. 3a</xref>, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-9" hwp:rel-id="F4">Fig. 4bc</xref>, Supplemental Figures), models were fit to the entire dataset for each rat. For analysis reporting model comparison (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig. 2c</xref>, <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Fig. 3bc</xref>, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-10" hwp:rel-id="F4">Fig. 4a</xref>, <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig. 5</xref>) each model was fit twice – once to behavioral data from even-numbered sessions, and once to odd-numbered sessions. The likelihood of each model was evaluated both on the dataset that it was fit to (even or odd; “training dataset”), and once to the other dataset (odd or even; “testing dataset”). To compare likelihood across rats, we compute “normalized likelihood” (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-4" hwp:rel-id="ref-14">Daw, 2011</xref>)
<disp-formula id="eqn9" hwp:id="disp-formula-10">
<alternatives hwp:id="alternatives-10"><graphic xlink:href="461129v3_eqn9.gif" position="float" orientation="portrait" hwp:id="graphic-16"/></alternatives>
</disp-formula>
where <italic toggle="yes">L<sub>t</sub></italic> indicates the likelihood the model assigned to the choice that the rat actually made on trial <italic toggle="yes">t</italic>, <italic toggle="yes">n</italic> indicates the total number of trials performed by the rat. This can be interpreted as the geometric mean over the probabilities with which the model would have taken the actions which the rat actually took.</p></sec><sec id="s5b" hwp:id="sec-20"><title hwp:id="title-28">Q-Learning Models</title><p hwp:id="p-53">We compared our cognitive model (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-6-1" hwp:rel-id="disp-formula-6">equations 5</xref>–<xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-1" hwp:rel-id="disp-formula-9">8</xref>) to several others from the literature (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-11" hwp:rel-id="F4">Fig 4</xref>). Many of these are variants of q-learning models, and can be written as special cases of a “hybrid” model, each with different parameter constraints. The hybrid model maintains separate estimated values for the left and for the right port (<italic toggle="yes">Q<sub>left</sub></italic> and <italic toggle="yes">Q<sub>right</sub></italic>), both of which are initialized to zero at the beginning of each session. One each trial, the model selects a port according to a softmax decision rule based on Q:
<disp-formula id="eqn10" hwp:id="disp-formula-11" hwp:rev-id="xref-disp-formula-11-1">
<alternatives hwp:id="alternatives-11"><graphic xlink:href="461129v3_eqn10.gif" position="float" orientation="portrait" hwp:id="graphic-17"/></alternatives>
</disp-formula>
where <italic toggle="yes">β<sub>Q</sub></italic> and <italic toggle="yes">β<sub>bias</sub></italic> are weighting parameters (also called “inverse temperature” parameters) governing the relative importance of estimated port values, which vary trial by trial, and of a fixed left/right bias. After each trial, the estimated value of the chosen and unchosen ports are updated according to:
<disp-formula id="eqn11" hwp:id="disp-formula-12" hwp:rev-id="xref-disp-formula-12-1">
<alternatives hwp:id="alternatives-12"><graphic xlink:href="461129v3_eqn11.gif" position="float" orientation="portrait" hwp:id="graphic-18"/></alternatives>
</disp-formula>
Where <italic toggle="yes">C<sub>t</sub></italic> is the action taken on trial <italic toggle="yes">t</italic> (left or right), <italic toggle="yes">O<sub>t</sub></italic> is the outcome received on trial <italic toggle="yes">t</italic> (reward or omission), the <italic toggle="yes">α</italic> parameters represent learning rates, and the <italic toggle="yes">κ</italic> parameters represent learning targets. Parameters with a subscript 1 govern update of the estimated value of the port that was chosen, while those with subscript 2 govern update of the estimated value of the unchosen port. Parameters with subscript + govern updates following rewarded trials, while those with subscript - govern updates following unrewarded trials.</p><p hwp:id="p-54">Of the models we consider using this notation, the “Q learning” model is the most constrained. This model does not update the estimated value of the unchosen port (α<sub>2+</sub> = α<sub>2-</sub> = <italic toggle="yes">κ</italic><sub>2+</sub> = <italic toggle="yes">κ</italic><sub>2-</sub> = 0). It also uses the same learning rate for rewarded and for unrewarded trials (α<sub>1+</sub> = α<sub>1-</sub>), and constrains the learning targets (<italic toggle="yes">κ</italic><sub>1+</sub>= 1; <italic toggle="yes">κ</italic><sub>1-</sub>= 0). This model has three free parameters: <italic toggle="yes">β<sub>Q</sub></italic>, <italic toggle="yes">β<sub>bias</sub></italic>, and <italic toggle="yes">α</italic><sub>1</sub>.</p><p hwp:id="p-55">“Optimistic RL” (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-4" hwp:rel-id="ref-36">Lefebvre et al., 2017</xref>) relaxes some of these constraints, allowing different learning rates on rewarded and on unrewarded trials, allowing <italic toggle="yes">α</italic><sub>1+</sub> and <italic toggle="yes">α</italic><sub>1-</sub> to separately vary. It retains the constraints on learning about the unchosen port (<italic toggle="yes">α</italic><sub>2+</sub> = <italic toggle="yes">α</italic><sub>2-</sub> = <italic toggle="yes">κ</italic><sub>2+</sub> = <italic toggle="yes">κ</italic><sub>2-</sub> = 0), and on the learning targets (<italic toggle="yes">κ</italic><sub>1+</sub>= 1; <italic toggle="yes">κ</italic><sub>1-</sub>= 0). This model has four free parameters: <italic toggle="yes">β<sub>Q</sub></italic>, <italic toggle="yes">β<sub>bias</sub></italic>, <italic toggle="yes">α</italic><sub>1+</sub>, and <italic toggle="yes">α</italic><sub>1-</sub>.</p><p hwp:id="p-56">“Differential Forgetting Q-learning” (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-10" hwp:rel-id="ref-30">Ito &amp; Doya, 2009</xref>) relaxes a different set of constraints. This model allows learning targets for the chosen option (<italic toggle="yes">κ</italic><sub>1+</sub> and <italic toggle="yes">κ</italic><sub>1-</sub>) to vary freely. It also allows “forgetting”, in which the value of the unchosen option is updated. This involves allowing <italic toggle="yes">α</italic><sub>2+</sub> and <italic toggle="yes">α</italic><sub>2-</sub> to take values other than zero. In this model, forgetting is identical between rewarded and unrewarded trials (<italic toggle="yes">α</italic><sub>2+</sub> = <italic toggle="yes">α</italic><sub>2-</sub>), and always has a target of 0 (<italic toggle="yes">κ</italic><sub>2+</sub> = <italic toggle="yes">κ</italic><sub>2-</sub> = 0). The DF-Q model also constrains <italic toggle="yes">β<sub>Q</sub></italic>=1, since this parameter is redundant with <italic toggle="yes">κ</italic><sub>1+</sub>. This model has five free parameters: <italic toggle="yes">β<sub>bias</sub></italic>, <italic toggle="yes">α</italic><sub>1</sub>, <italic toggle="yes">α</italic><sub>2</sub>, <italic toggle="yes">κ</italic><sub>1+</sub> and <italic toggle="yes">κ</italic><sub>1-</sub>.</p><p hwp:id="p-57">Finally, we also consider the complete hybrid model given by <xref ref-type="disp-formula" rid="eqn10" hwp:id="xref-disp-formula-11-1" hwp:rel-id="disp-formula-11">equations 10</xref> and <xref ref-type="disp-formula" rid="eqn11" hwp:id="xref-disp-formula-12-1" hwp:rel-id="disp-formula-12">11</xref>. This model has ten free parameters: <italic toggle="yes">β<sub>Q</sub></italic>, <italic toggle="yes">β<sub>bias</sub></italic>, <italic toggle="yes">α</italic><sub>1+</sub>, <italic toggle="yes">α</italic><sub>1-</sub>, <italic toggle="yes">α</italic><sub>2+</sub>, <italic toggle="yes">α</italic><sub>2-</sub>, <italic toggle="yes">κ</italic><sub>1+</sub>, <italic toggle="yes">κ</italic><sub>1-</sub>, <italic toggle="yes">κ</italic><sub>2+</sub>, and <italic toggle="yes">κ</italic><sub>2-</sub> Some of these parameters are redundant with one another.</p><p hwp:id="p-58">All q-learning models were instantiated and fit using the probabilistic programming language Stan (Bob <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Carpenter et al., 2017</xref>) via its Matlab interface (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">Stan Development Team, 2016</xref>). Models were fit by maximum-a-posteriori, using weakly informative priors of <monospace>beta (3, 3)</monospace> prior for learning rate parameters (<italic toggle="yes">α</italic>), and <monospace>gaussian (0, 1)</monospace> for weighting and target parameters (<italic toggle="yes">β</italic> and <italic toggle="yes">α</italic>).</p></sec><sec id="s5c" hwp:id="sec-21"><title hwp:id="title-29">Ideal Observer Models</title><p hwp:id="p-59">We also compared our cognitive model to an ideal observer model. This model implements a Bayesian filter, which maintains a belief distribution over the reward probabilities of each port. We use <italic toggle="yes">θ<sub>left</sub></italic> and <italic toggle="yes">θ<sub>right</sub></italic> to parameterize the reward probability on the left and on the right port, and <italic toggle="yes">B<sub>t, right</sub></italic> and <italic toggle="yes">B<sub>t</sub></italic> indicate the observer’s belief at time <italic toggle="yes">t</italic> over these probabilities. We approximate these distributions numerically, allowing theta to take 101 discrete values equally spaced between zero and one. Belief distributions are initialized to be uniform at the start of each session, and updated twice following each trial. The first is a Bayesian update applied to the belief distribution for the chosen port:
<disp-formula id="eqn12" hwp:id="disp-formula-13">
<alternatives hwp:id="alternatives-13"><graphic xlink:href="461129v3_eqn12.gif" position="float" orientation="portrait" hwp:id="graphic-19"/></alternatives>
</disp-formula>
where <italic toggle="yes">B<sub>i</sub></italic> is the belief distribution associated with the port chosen on trial <italic toggle="yes">t</italic>, <italic toggle="yes">O<sub>t</sub></italic> is the outcome observed on trial <italic toggle="yes">t</italic>, and <italic toggle="yes">θ</italic> parameterizes the probability of reward at that port. Because of this, <italic toggle="yes">P</italic>(<italic toggle="yes">O<sub>t</sub></italic>|<italic toggle="yes">Θ</italic>) = <italic toggle="yes">Θ</italic> on rewarded trials, and <italic toggle="yes">P</italic>(<italic toggle="yes">O</italic><sub>t</sub>|<italic toggle="yes">Θ</italic>) = (1-<italic toggle="yes">Θ</italic>) on unrewarded trials. The second update accounts the trial-by-trial drift in reward probabilities. Each belief distribution is convolved with a gaussian kernel whose standard deviation (<italic toggle="yes">σ</italic>) is a free parameter corresponding to the observer’s belief about the environmental drift rate.</p><p hwp:id="p-60">To make choices, the ideal observer first computes the mean (<italic toggle="yes">V</italic>) and variance (<italic toggle="yes">U</italic>) of the reward probabilities given its current belief distributions, which quantify the expected immediate reward given its current beliefs, as well as the uncertainty in that expectation:
<disp-formula id="eqn13" hwp:id="disp-formula-14">
<alternatives hwp:id="alternatives-14"><graphic xlink:href="461129v3_eqn13.gif" position="float" orientation="portrait" hwp:id="graphic-20"/></alternatives>
</disp-formula></p><p hwp:id="p-61">The decision on each trial was made using a weighted average of the differences in these quantities:
<disp-formula id="eqn13a" hwp:id="disp-formula-15">
<alternatives hwp:id="alternatives-15"><graphic xlink:href="461129v3_eqn13a.gif" position="float" orientation="portrait" hwp:id="graphic-21"/></alternatives>
</disp-formula>
where <italic toggle="yes">β<sub>v</sub></italic> quantifies the model’s tendency to select the port with the currently higher expected reward probability, <italic toggle="yes">β<sub>explore</sub></italic> quantifies a tendency to choose the side with currently greater uncertainty in reward probability ("directed exploration"; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-6" hwp:rel-id="ref-17">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">Wilson et al., 2014</xref>), and <italic toggle="yes">β<sub>bias</sub></italic> quantifies a fixed bias towards the left or the right port. The model therefore had four free parameters: these four <italic toggle="yes">β</italic>s, as well as <italic toggle="yes">σ</italic>. For quantitative model comparison (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-12" hwp:rel-id="F4">Figure 4</xref>), we computed normalized likelihoods using two-fold cross-validation, as described above, and maximum likelihood fits using code written in Matlab. For generating synthetic data (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1c</xref>), we set σ to its true generative value of 0.15, β<sub><italic toggle="yes">v</italic></sub> to 100, and both <italic toggle="yes">β<sub>explore</sub></italic> and <italic toggle="yes">β<sub>bias</sub></italic> to zero.</p></sec><sec id="s5d" hwp:id="sec-22"><title hwp:id="title-30">Software and Data Availability</title><p hwp:id="p-62">Automated code for training rats on the two-armed bandit task will be made available on the Brody lab website upon publication. Software used for data analysis, as well as processed data will be made publicly available upon publication. Raw data are available from the authors upon request.</p></sec></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-31">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Akaishi R."><surname>Akaishi</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Umeda K."><surname>Umeda</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nagase A."><surname>Nagase</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sakai K."><surname>Sakai</surname>, <given-names>K.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-2">Autonomous mechanism of internal choice estimate underlies decision inertia</article-title>. <source hwp:id="source-1">Neuron</source>, <volume>81</volume>(<issue>1</issue>), <fpage>195</fpage>–<lpage>206</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Ashby F. G."><surname>Ashby</surname>, <given-names>F. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Turner B. O."><surname>Turner</surname>, <given-names>B. O.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Horvitz J. C."><surname>Horvitz</surname>, <given-names>J. C.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-3">Cortical and basal ganglia contributions to habit learning and automaticity</article-title>. <source hwp:id="source-2">Trends in Cognitive Sciences</source>, <volume>14</volume>(<issue>5</issue>), <fpage>208</fpage>–<lpage>215</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Balcarras M."><surname>Balcarras</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ardid S."><surname>Ardid</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kaping D."><surname>Kaping</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Everling S."><surname>Everling</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Womelsdorf T."><surname>Womelsdorf</surname>, <given-names>T.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-4">Attentional Selection Can Be Predicted by Reinforcement Learning of Task-relevant Stimulus Features Weighted by Value-independent Stickiness</article-title>. <source hwp:id="source-3">Journal of Cognitive Neuroscience</source>, <volume>28</volume>(<issue>2</issue>), <fpage>333</fpage>–<lpage>349</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bari B. A."><surname>Bari</surname>, <given-names>B. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grossman C. D."><surname>Grossman</surname>, <given-names>C. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lubin E. E."><surname>Lubin</surname>, <given-names>E. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rajagopalan A. E."><surname>Rajagopalan</surname>, <given-names>A. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cressy J. I."><surname>Cressy</surname>, <given-names>J. I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cohen J. Y."><surname>Cohen</surname>, <given-names>J. Y.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-5">Stable Representations of Decision Variables for Flexible Behavior</article-title>. In <source hwp:id="source-4">Neuron</source> (Vol. <volume>103</volume>, Issue <issue>5</issue>, pp. <fpage>922</fpage>–<lpage>933</lpage>.<page-range>e7</page-range>). <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuron.2019.06.001" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2019.06.001" hwp:id="ext-link-3">https://doi.org/10.1016/j.neuron.2019.06.001</ext-link></citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Barraclough D. J."><surname>Barraclough</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Conroy M. L."><surname>Conroy</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-6">Prefrontal cortex and decision making in a mixed-strategy game</article-title>. <source hwp:id="source-5">Nature Neuroscience</source>, <volume>7</volume>(<issue>4</issue>), <fpage>404</fpage>–<lpage>410</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Bornstein A. M."><surname>Bornstein</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-7">Dissociating hippocampal and striatal contributions to sequential prediction learning</article-title>. <source hwp:id="source-6">The European Journal of Neuroscience</source>, <volume>35</volume>(<issue>7</issue>), <fpage>1011</fpage>–<lpage>1023</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Brunton B. W."><surname>Brunton</surname>, <given-names>B. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick M. M."><surname>Botvinick</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Brody C. D."><surname>Brody</surname>, <given-names>C. D.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-8">Rats and humans can optimally accumulate evidence for decision-making</article-title>. <source hwp:id="source-7">Science</source>, <volume>340</volume>(<issue>6128</issue>), <fpage>95</fpage>–<lpage>98</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Bush R. R."><surname>Bush</surname>, <given-names>R. R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Mosteller F."><surname>Mosteller</surname>, <given-names>F.</given-names></string-name> (<year>1953</year>). <article-title hwp:id="article-title-9">A Stochastic Model with Applications to Learning</article-title>. <source hwp:id="source-8">Annals of Mathematical Statistics</source>, <volume>24</volume>(<issue>4</issue>), <fpage>559</fpage>–<lpage>585</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.9" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Carpenter B."><surname>Carpenter</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gelman A."><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoffman M. D."><surname>Hoffman</surname>, <given-names>M. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goodrich B."><surname>Goodrich</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Betancourt M."><surname>Betancourt</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brubaker M."><surname>Brubaker</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Guo J."><surname>Guo</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li P."><surname>Li</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Riddell A."><surname>Riddell</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-10">Stan: A probabilistic programming language</article-title>. <source hwp:id="source-9">Journal of Statistical Software</source>, <volume>76</volume>(<issue>1</issue>). <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.osti.gov/biblio/1430202" ext-link-type="uri" xlink:href="https://www.osti.gov/biblio/1430202" hwp:id="ext-link-4">https://www.osti.gov/biblio/1430202</ext-link></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="website" citation-type="web" ref:id="461129v3.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Carpenter B."><surname>Carpenter</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gelman A."><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoffman M."><surname>Hoffman</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goodrich B."><surname>Goodrich</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Betancourt M."><surname>Betancourt</surname>, <given-names>M.</given-names></string-name>, M, B., <string-name name-style="western" hwp:sortable="Guo J."><surname>Guo</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li P."><surname>Li</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Riddell A."><surname>Riddell</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <source hwp:id="source-10">Stan: A Probabilistic Programming Language</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://mc-stan.org" ext-link-type="uri" xlink:href="http://mc-stan.org" hwp:id="ext-link-5">http://mc-stan.org</ext-link></citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Collins A. G. E."><surname>Collins</surname>, <given-names>A. G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Frank M. J."><surname>Frank</surname>, <given-names>M. J.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-11">How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title>. <source hwp:id="source-11">The European Journal of Neuroscience</source>, <volume>35</volume>(<issue>7</issue>), <fpage>1024</fpage>–<lpage>1035</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Corrado G. S."><surname>Corrado</surname>, <given-names>G. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname>, <given-names>K.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-12">Understanding neural coding through the model-based analysis of decision making</article-title>. <source hwp:id="source-12">The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>27</volume>(<issue>31</issue>), <fpage>8178</fpage>–<lpage>8180</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Corrado G. S."><surname>Corrado</surname>, <given-names>G. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sugrue L. P."><surname>Sugrue</surname>, <given-names>L. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seung H. S."><surname>Seung</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Newsome W. T."><surname>Newsome</surname>, <given-names>W. T.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-13">Linear-Nonlinear-Poisson models of primate choice dynamics</article-title>. <source hwp:id="source-13">Journal of the Experimental Analysis of Behavior</source>, <volume>84</volume>(<issue>3</issue>), <fpage>581</fpage>–<lpage>617</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2 xref-ref-14-3 xref-ref-14-4"><citation publication-type="other" citation-type="journal" ref:id="461129v3.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-14">Trial-by-trial data analysis using computational models</article-title>. In <source hwp:id="source-14">Decision Making, Affect, and Learning</source> (pp. <fpage>3</fpage>–<lpage>38</lpage>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname>, <given-names>K.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-15">The computational neurobiology of learning and reward</article-title>. <source hwp:id="source-15">Current Opinion in Neurobiology</source>, <volume>16</volume>(<issue>2</issue>), <fpage>199</fpage>–<lpage>204</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S. J."><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour B."><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dolan R. J."><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-16">Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source hwp:id="source-16">Neuron</source>, <volume>69</volume>(<issue>6</issue>), <fpage>1204</fpage>–<lpage>1215</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3 xref-ref-17-4 xref-ref-17-5 xref-ref-17-6"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Doherty J. P."><surname>O’Doherty</surname>, <given-names>J. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour B."><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dolan R. J."><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-17">Cortical substrates for exploratory decisions in humans</article-title>. <source hwp:id="source-17">Nature</source>, <volume>441</volume>(<issue>7095</issue>), <fpage>876</fpage>–<lpage>879</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Niv Y."><surname>Niv</surname>, <given-names>Y.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-18">Reinforcement learning: the good, the bad and the ugly</article-title>. <source hwp:id="source-18">Current Opinion in Neurobiology</source>, <volume>18</volume>(<issue>2</issue>), <fpage>185</fpage>–<lpage>196</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Dickinson A."><surname>Dickinson</surname>, <given-names>A.</given-names></string-name> (<year>1985</year>). <article-title hwp:id="article-title-19">Actions and Habits: The Development of Behavioural Autonomy</article-title>. <source hwp:id="source-19">Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source>, <volume>308</volume>(<issue>1135</issue>), <fpage>67</fpage>–<lpage>78</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Erlich J. C."><surname>Erlich</surname>, <given-names>J. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bialek M."><surname>Bialek</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Brody C. D."><surname>Brody</surname>, <given-names>C. D.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-20">A cortical substrate for memory-guided orienting in the rat</article-title>. <source hwp:id="source-20">Neuron</source>, <volume>72</volume>(<issue>2</issue>), <fpage>330</fpage>–<lpage>343</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="book" citation-type="book" ref:id="461129v3.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Gelman A."><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carlin J. B."><surname>Carlin</surname>, <given-names>J. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stern H. S."><surname>Stern</surname>, <given-names>H. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dunson D. B."><surname>Dunson</surname>, <given-names>D. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vehtari A."><surname>Vehtari</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rubin D. B."><surname>Rubin</surname>, <given-names>D. B.</given-names></string-name> (<year>2013</year>). <source hwp:id="source-21">Bayesian Data Analysis</source>, <edition>Third</edition> Edition. <publisher-name>CRC Press</publisher-name>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Gelman A."><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rubin D. B."><surname>Rubin</surname>, <given-names>D. B.</given-names></string-name> (<year>1995</year>). <article-title hwp:id="article-title-21">Avoiding Model Selection in Bayesian Social Research</article-title>. <source hwp:id="source-22">Sociological Methodology</source>, <volume>25</volume>, <fpage>165</fpage>–<lpage>173</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Gold J. I."><surname>Gold</surname>, <given-names>J. I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Shadlen M. N."><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-22">Banburismus and the brain: decoding the relationship between sensory stimuli, decisions, and reward</article-title>. <source hwp:id="source-23">Neuron</source>, <volume>36</volume>(<issue>2</issue>), <fpage>299</fpage>–<lpage>308</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Gold J. I."><surname>Gold</surname>, <given-names>J. I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Shadlen M. N."><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-23">The neural basis of decision making</article-title>. <source hwp:id="source-24">Annual Review of Neuroscience</source>, <volume>30</volume>, <fpage>535</fpage>–<lpage>574</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Graybiel A. M."><surname>Graybiel</surname>, <given-names>A. M.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-24">Habits, Rituals, and the Evaluative Brain</article-title>. <source hwp:id="source-25">Annual Review of Neuroscience</source>, <volume>31</volume>(<issue>1</issue>), <fpage>359</fpage>–<lpage>387</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Griffiths T. L."><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lieder F."><surname>Lieder</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Goodman N. D."><surname>Goodman</surname>, <given-names>N. D.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-25">Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic</article-title>. <source hwp:id="source-26">Topics in Cognitive Science</source>, <volume>7</volume>(<issue>2</issue>), <fpage>217</fpage>–<lpage>229</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Hahn U."><surname>Hahn</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Warren P. A."><surname>Warren</surname>, <given-names>P. A.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-26">Perceptions of randomness: why three heads are better than four</article-title>. <source hwp:id="source-27">Psychological Review</source>, <volume>116</volume>(<issue>2</issue>), <fpage>454</fpage>–<lpage>461</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Hanks T. D."><surname>Hanks</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Summerfield C."><surname>Summerfield</surname>, <given-names>C.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-27">Perceptual Decision Making in Rodents, Monkeys, and Humans</article-title>. <source hwp:id="source-28">Neuron</source>, <volume>93</volume>(<issue>1</issue>), <fpage>15</fpage>–<lpage>31</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Heinke D."><surname>Heinke</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Humphreys G. W."><surname>Humphreys</surname>, <given-names>G. W.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-28">Computational models of visual selective attention: A review</article-title>. <source hwp:id="source-29">Connectionist Models in Cognitive Psychology</source>, <volume>1</volume>(<issue>4</issue>), <fpage>273</fpage>–<lpage>312</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2 xref-ref-30-3 xref-ref-30-4 xref-ref-30-5 xref-ref-30-6 xref-ref-30-7 xref-ref-30-8 xref-ref-30-9 xref-ref-30-10"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Ito M."><surname>Ito</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname>, <given-names>K.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-29">Validation of Decision-Making Models and Analysis of Decision Variables in the Rat Basal Ganglia</article-title>. <source hwp:id="source-30">Journal of Neuroscience</source>, <volume>29</volume>(<issue>31</issue>), <fpage>9861</fpage>–<lpage>9874</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2 xref-ref-31-3 xref-ref-31-4 xref-ref-31-5 xref-ref-31-6 xref-ref-31-7"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Kim H."><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sul J. H."><surname>Sul</surname>, <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huh N."><surname>Huh</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Jung M. W."><surname>Jung</surname>, <given-names>M. W.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-30">Role of striatum in updating values of chosen actions</article-title>. <source hwp:id="source-31">The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>29</volume>(<issue>47</issue>), <fpage>14701</fpage>–<lpage>14712</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="website" citation-type="web" ref:id="461129v3.32" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Körding K."><surname>Körding</surname>, <given-names>K.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-31">Decision theory: what舠 should舡 the nervous system do?</article-title> <source hwp:id="source-32">Science</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://science.sciencemag.org/content/318/5850/606.short?casa_token=wkEY-9w4VpwAAAAA:t43fMyKd-E8I7E5B_o3sewwvyP3qFwUjHbjmnIeERVwNO2CvqSH5y21LkuNmPUKAZzcCiWJ82VHK8VI" ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/318/5850/606.short?casa_token=wkEY-9w4VpwAAAAA:t43fMyKd-E8I7E5B_o3sewwvyP3qFwUjHbjmnIeERVwNO2CvqSH5y21LkuNmPUKAZzcCiWJ82VHK8VI" hwp:id="ext-link-6">http://science.sciencemag.org/content/318/5850/606.short?casa_token=wkEY-9w4VpwAAAAA:t43fMyKd-E8I7E5B_o3sewwvyP3qFwUjHbjmnIeERVwNO2CvqSH5y21LkuNmPUKAZzcCiWJ82VHK8VI</ext-link></citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2 xref-ref-33-3"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Lau B."><surname>Lau</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Glimcher P. W."><surname>Glimcher</surname>, <given-names>P. W.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-32">Dynamic response-by-response models of matching behavior in rhesus monkeys</article-title>. <source hwp:id="source-33">Journal of the Experimental Analysis of Behavior</source>, <volume>84</volume>(<issue>3</issue>), <fpage>555</fpage>–<lpage>579</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGreevy B. P."><surname>McGreevy</surname>, <given-names>B. P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barraclough D. J."><surname>Barraclough</surname>, <given-names>D. J.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-33">Learning and decision making in monkeys during a rock-paper-scissors game</article-title>. <source hwp:id="source-34">Cognitive Brain Research</source>, <volume>25</volume>(<issue>2</issue>), <fpage>416</fpage>–<lpage>430</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seo H."><surname>Seo</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Jung M. W."><surname>Jung</surname>, <given-names>M. W.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-34">Neural basis of reinforcement learning and decision making</article-title>. <source hwp:id="source-35">Annual Review of Neuroscience</source>, <volume>35</volume>, <fpage>287</fpage>–<lpage>308</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2 xref-ref-36-3 xref-ref-36-4"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Lefebvre G."><surname>Lefebvre</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lebreton M."><surname>Lebreton</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meyniel F."><surname>Meyniel</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bourgeois-Gironde S."><surname>Bourgeois-Gironde</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Palminteri S."><surname>Palminteri</surname>, <given-names>S.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-35">Behavioural and neural characterization of optimistic reinforcement learning</article-title>. <source hwp:id="source-36">Nature Human Behaviour</source>, <volume>1</volume>(<issue>4</issue>), <fpage>0067</fpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Miller K. J."><surname>Miller</surname>, <given-names>K. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shenhav A."><surname>Shenhav</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ludvig E. A."><surname>Ludvig</surname>, <given-names>E. A.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-36">Habits without values</article-title>. <source hwp:id="source-37">Psychological Review</source>, <volume>126</volume>(<issue>2</issue>), <fpage>292</fpage>–<lpage>311</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="book" citation-type="book" ref:id="461129v3.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Miller K. J."><surname>Miller</surname>, <given-names>K. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shenhav A."><surname>Shenhav</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pezzulo G."><surname>Pezzulo</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ludvig E."><surname>Ludvig</surname>, <given-names>E.</given-names></string-name> (<year>2018</year>). <chapter-title>Re-aligning models of habitual and goal-directed decision-making</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Morris R."><given-names>R.</given-names> <surname>Morris</surname></string-name>, <string-name name-style="western" hwp:sortable="Bornstein A."><given-names>A.</given-names> <surname>Bornstein</surname></string-name>, &amp; <string-name name-style="western" hwp:sortable="Shenhav A."><given-names>A.</given-names> <surname>Shenhav</surname></string-name></person-group> (Eds.), <source hwp:id="source-38">Goal-Directed Decision Making: Computations and Neural Circuits</source>. <publisher-name>Elsevier</publisher-name>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="other" citation-type="journal" ref:id="461129v3.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Norman K. A."><surname>Norman</surname>, <given-names>K. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Detre G. J."><surname>Detre</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Polyn S. M."><surname>Polyn</surname>, <given-names>S. M.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-37">Computational models of episodic memory</article-title>. <source hwp:id="source-39">The Cambridge Handbook of Computational Psychology</source>, <fpage>189</fpage>–<lpage>224</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="O’Doherty J. P."><surname>O’Doherty</surname>, <given-names>J. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hampton A."><surname>Hampton</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kim H."><surname>Kim</surname>, <given-names>H.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-38">Model-based fMRI and its application to reward learning and decision making</article-title>. <source hwp:id="source-40">Annals of the New York Academy of Sciences</source>, <volume>1104</volume>(<issue>1</issue>), <fpage>35</fpage>–<lpage>53</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1 xref-ref-41-2 xref-ref-41-3"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Oskarsson A. T."><surname>Oskarsson</surname>, <given-names>A. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Van Boven L."><surname>Van Boven</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McClelland G. H."><surname>McClelland</surname>, <given-names>G. H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Hastie R."><surname>Hastie</surname>, <given-names>R.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-39">What’s next? Judging sequences of binary events</article-title>. <source hwp:id="source-41">Psychological Bulletin</source>, <volume>135</volume>(<issue>2</issue>), <fpage>262</fpage>–<lpage>285</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="other" citation-type="journal" ref:id="461129v3.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Raftery A. E."><surname>Raftery</surname>, <given-names>A. E.</given-names></string-name> (<year>1995</year>). <article-title hwp:id="article-title-40">Bayesian model selection in social research</article-title>. <source hwp:id="source-42">Sociological Methodology</source>, <fpage>111</fpage>–<lpage>163</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Rescorla R. A."><surname>Rescorla</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wagner A. R."><surname>Wagner</surname>, <given-names>A. R.</given-names></string-name> (<year>1972</year>). <article-title hwp:id="article-title-41">A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</article-title>. <source hwp:id="source-43">Classical Conditioning II: Current Research and Theory</source>, <volume>2</volume>, <fpage>64</fpage>–<lpage>99</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Rutledge R. B."><surname>Rutledge</surname>, <given-names>R. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lazzaro S. C."><surname>Lazzaro</surname>, <given-names>S. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lau B."><surname>Lau</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Myers C. E."><surname>Myers</surname>, <given-names>C. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gluck M. A."><surname>Gluck</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Glimcher P. W."><surname>Glimcher</surname>, <given-names>P. W.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-42">Dopaminergic drugs modulate learning rates and perseveration in Parkinson’s patients in a dynamic foraging task</article-title>. <source hwp:id="source-44">The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>29</volume>(<issue>48</issue>), <fpage>15104</fpage>–<lpage>15114</lpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Samejima K."><surname>Samejima</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ueda Y."><surname>Ueda</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kimura M."><surname>Kimura</surname>, <given-names>M.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-43">Representation of action-specific reward values in the striatum</article-title>. <source hwp:id="source-45">Science</source>, <volume>310</volume>(<issue>5752</issue>), <fpage>1337</fpage>–<lpage>1340</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2"><citation publication-type="website" citation-type="web" ref:id="461129v3.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><collab hwp:id="collab-1">Stan Development Team</collab>. (<year>2016</year>). <source hwp:id="source-46">MatlabStan: The MATLAB interface to Stan</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://mc-stan.org/matlab-stan.html" ext-link-type="uri" xlink:href="http://mc-stan.org/matlab-stan.html" hwp:id="ext-link-7">http://mc-stan.org/matlab-stan.html</ext-link>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Sugrue L. P."><surname>Sugrue</surname>, <given-names>L. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Corrado G. S."><surname>Corrado</surname>, <given-names>G. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Newsome W. T."><surname>Newsome</surname>, <given-names>W. T.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-44">Choosing the greater of two goods: neural currencies for valuation and decision making</article-title>. <source hwp:id="source-47">Nature Reviews. Neuroscience</source>, <volume>6</volume>(<issue>5</issue>), <fpage>363</fpage>–<lpage>375</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Sul J. H."><surname>Sul</surname>, <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kim H."><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huh N."><surname>Huh</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee D."><surname>Lee</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Jung M. W."><surname>Jung</surname>, <given-names>M. W.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-45">Distinct roles of rodent orbitofrontal and medial prefrontal cortex in decision making</article-title>. <source hwp:id="source-48">Neuron</source>, <volume>66</volume>(<issue>3</issue>), <fpage>449</fpage>–<lpage>460</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Sun Y."><surname>Sun</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Reilly R. C."><surname>O’Reilly</surname>, <given-names>R. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhattacharyya R."><surname>Bhattacharyya</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith J. W."><surname>Smith</surname>, <given-names>J. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu X."><surname>Liu</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wang H."><surname>Wang</surname>, <given-names>H.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-46">Latent structure in random sequences drives neural learning toward a rational bias</article-title>. <source hwp:id="source-49">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>112</volume>(<issue>12</issue>), <fpage>3788</fpage>–<lpage>3792</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1 xref-ref-50-2"><citation publication-type="book" citation-type="book" ref:id="461129v3.50" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Sutton R. S."><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barto A. G."><surname>Barto</surname>, <given-names>A. G.</given-names></string-name> (<year>2017</year>). <source hwp:id="source-50">Reinforcement Learning: An Introduction (second edi ed.)</source>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA, USA</publisher-loc>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><citation publication-type="book" citation-type="book" ref:id="461129v3.51" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Thorndike E. L."><surname>Thorndike</surname>, <given-names>E. L.</given-names></string-name> (<year>1911</year>). <source hwp:id="source-51">Animal Intelligence: Experimental Studies</source>. <publisher-name>Macmillan</publisher-name>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Tversky A."><surname>Tversky</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kahneman D."><surname>Kahneman</surname>, <given-names>D.</given-names></string-name> (<year>1971</year>). <article-title hwp:id="article-title-47">Belief in the law of small numbers</article-title>. <source hwp:id="source-52">Psychological Bulletin</source>, <volume>76</volume>(<issue>2</issue>), <fpage>105</fpage>–<lpage>110</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Wagenmakers E.-J."><surname>Wagenmakers</surname>, <given-names>E.-J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lodewyckx T."><surname>Lodewyckx</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kuriyal H."><surname>Kuriyal</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Grasman R."><surname>Grasman</surname>, <given-names>R.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-48">Bayesian hypothesis testing for psychologists: A tutorial on the Savage-Dickey method</article-title>. <source hwp:id="source-53">Cognitive Psychology</source>, <volume>60</volume>(<issue>3</issue>), <fpage>158</fpage>–<lpage>189</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="website" citation-type="web" ref:id="461129v3.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Wilson R. C."><surname>Wilson</surname>, <given-names>R. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Collins A. G."><surname>Collins</surname>, <given-names>A. G.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-49">Ten simple rules for the computational modeling of behavioral data</article-title>. <source hwp:id="source-54">eLife</source>, <fpage>8</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.7554/eLife.49547" ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.49547" hwp:id="ext-link-8">https://doi.org/10.7554/eLife.49547</ext-link></citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Wilson R. C."><surname>Wilson</surname>, <given-names>R. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Geana A."><surname>Geana</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="White J. M."><surname>White</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ludvig E. A."><surname>Ludvig</surname>, <given-names>E. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cohen J. D."><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-50">Humans use directed and random exploration to solve the explore-exploit dilemma</article-title>. <source hwp:id="source-55">Journal of Experimental Psychology. General</source>, <volume>143</volume>(<issue>6</issue>), <fpage>2074</fpage>–<lpage>2081</lpage>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="journal" citation-type="journal" ref:id="461129v3.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Wood W."><surname>Wood</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rünger D."><surname>Rünger</surname>, <given-names>D.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-51">Psychology of Habit</article-title>. <source hwp:id="source-56">Annual Review of Psychology</source>, <volume>67</volume>, <fpage>289</fpage>–<lpage>314</lpage>.</citation></ref></ref-list></back></article>
