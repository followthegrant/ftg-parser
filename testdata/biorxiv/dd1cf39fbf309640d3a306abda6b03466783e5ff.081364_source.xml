<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/081364</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;081364</article-id><article-id pub-id-type="other" hwp:sub-type="slug">081364</article-id><article-id pub-id-type="other" hwp:sub-type="tag">081364</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Reconstructing cell cycle and disease progression using deep learning</article-title></title-group><author-notes hwp:id="author-notes-1"><fn id="n1" fn-type="equal" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>*</label><p hwp:id="p-1">These authors contributed equally to this work.</p></fn><fn id="n2" hwp:id="fn-2" hwp:rev-id="xref-fn-2-1"><label>†</label><p hwp:id="p-2"><email hwp:id="email-1">fabian.theis@helmholtz-muenchen.de</email></p></fn><fn id="n3" hwp:id="fn-3" hwp:rev-id="xref-fn-3-1"><label>‡</label><p hwp:id="p-3"><email hwp:id="email-2">alex.wolf@helmholtz-muenchen.de</email></p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Eulenberg Philipp"><surname>Eulenberg</surname><given-names>Philipp</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="author-notes" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Köhler Niklas"><surname>Köhler</surname><given-names>Niklas</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="author-notes" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Blasi Thomas"><surname>Blasi</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Filby Andrew"><surname>Filby</surname><given-names>Andrew</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Carpenter Anne E."><surname>Carpenter</surname><given-names>Anne E.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Rees Paul"><surname>Rees</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-3" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><name name-style="western" hwp:sortable="Theis Fabian J."><surname>Theis</surname><given-names>Fabian J.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-1" hwp:rel-id="aff-6">6</xref><xref ref-type="author-notes" rid="n2" hwp:id="xref-fn-2-1" hwp:rel-id="fn-2">†</xref></contrib><contrib contrib-type="author" hwp:id="contrib-8"><name name-style="western" hwp:sortable="Wolf F. Alexander"><surname>Wolf</surname><given-names>F. Alexander</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref><xref ref-type="author-notes" rid="n3" hwp:id="xref-fn-3-1" hwp:rel-id="fn-3">‡</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5"><label>1</label><institution hwp:id="institution-1">Helmholtz Zentrum München – German Research Center for Environmental Health, Institute of Computational Biology</institution>, Neuherberg, Munich, <country>Germany</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label><institution hwp:id="institution-2">Department of Physics, Arnold Sommerfeld Center for Theoretical Physics</institution>, LMU München, Munich, <country>Germany</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2 xref-aff-3-3"><label>3</label><institution hwp:id="institution-3">Imaging Platform at the Broad Institute of Harvard and Massachusetts Institute of Technology</institution>, Cambridge, Massachusetts, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Flow Cytometry Core Facility, Faculty of Medical Sciences, Newcastle University</institution>, Newcastle upon Tyne, <country>UK</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">College of Engineering, Swansea University</institution>, Singleton Park, Swansea, <country>UK</country></aff><aff id="a6" hwp:id="aff-6" hwp:rev-id="xref-aff-6-1"><label>6</label><institution hwp:id="institution-6">Department of Mathematics</institution>, TU München, Munich, <country>Germany</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2016-10-17T12:45:33-07:00">
    <day>17</day><month>10</month><year>2016</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-06-05T10:50:48-07:00">
    <day>5</day><month>6</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2016-10-17T13:05:14-07:00">
    <day>17</day><month>10</month><year>2016</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-06-05T11:02:52-07:00">
    <day>5</day><month>6</month><year>2017</year>
  </pub-date><elocation-id>081364</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2016-10-16"><day>16</day><month>10</month><year>2016</year></date>
<date date-type="rev-recd" hwp:start="2017-06-05"><day>05</day><month>6</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-06-05"><day>05</day><month>6</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-4">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="081364.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/081364v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="081364.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/081364v2/081364v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/081364v2/081364v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-5">We show that deep convolutional neural networks combined with non-linear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by recon-structing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a 6-fold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer.</p></abstract><counts><page-count count="14"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-2">Introduction</title><p hwp:id="p-6">A major challenge and opportunity in biology is interpreting the increasing amount of information-rich and high-throughput single-cell data. Here, we focus on imaging data from fluorescence microscopy (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Pepperkok and Ellenberg, 2006</xref>), in particular from imaging flow cytometry (IFC), which combines the fluorescence sensitivity and high-throughput capabilities of flow cytometry with single-cell imaging (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Basiji <italic toggle="yes">et al.</italic>, 2007</xref>). Imaging flow cytometry is unusually well-suited to deep learning as it provides very high sample numbers and image data from several channels, that is, high-dimensional, spatially correlated data. Deep learning is therefore capable of processing the dramatic increase in information content — compared to spatially integrated fluorescence intensity measurements as in conventional flow cytometry (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Brown and Wittwer, 2000</xref>) — in IFC data. Also, IFC provides one image for each single cell, and hence does not require whole-image segmentation.</p><p hwp:id="p-7">Deep learning enables improved data analysis for high-throughput microscopy as compared to traditional machine learning methods (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Eliceiri <italic toggle="yes">et al.</italic>, 2012</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic>, 2016</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Jones <italic toggle="yes">et al.</italic>, 2009</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Dao <italic toggle="yes">et al.</italic>, 2016</xref>). This is mainly due to three general advantages of deep learning over traditional machine learning: there is no need for cumbersome preprocessing and manual feature definition, prediction accuracy is improved, and learned features can be visualized to uncover their biological meaning. In particular, we demonstrate that this enables reconstructing continuous biological processes, which has stimulated much research effort in the past years (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic>, 2015</xref>; <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Bendall <italic toggle="yes">et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Trapnell <italic toggle="yes">et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Haghverdi <italic toggle="yes">et al.</italic>, 2016</xref>). Only one of the other recent works on deep learning in high-throughput microscopy discusses the visualization of network features (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Pärnamaa and Parts, 2016</xref>), but none deal with continuous biological processes (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Chen <italic toggle="yes">et al.,</italic> 2016a</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Kraus <italic toggle="yes">et al.,</italic> 2016</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Dürr and Sick, 2016</xref>; <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Kandaswamy <italic toggle="yes">et al.,</italic> 2016</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">Pärnamaa and Parts, 2016</xref>).</p><p hwp:id="p-8">When aiming at an understanding of a specific biological process, one often only has coarse-grained labels for a few qualitative stages, for instance, cell cycle or disease stages. While a continuous label could be efficiently used in a regression based approach, qualitative labels are better used in a classification-based approach. In particular, if the ordering of the categorical labels at hand is not known, a regression based approach will fail. Also, the detailed quantitative information necessary for a continuous label is usually only available if a phenomenon is already understood on a molecular level and markers that quantitatively characterize the phenomenon are available. While this is possible for cell cycle when carrying out elaborate experiments where such markers are measured (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.,</italic> 2015</xref>; <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.,</italic> 2016</xref>), in many other cases, this is too tedious, has severe side effects with unwanted influences on the phenomenon itself or is simply not possible as markers for a specific phenomenon are not known. Therefore, we propose a general workflow that uses a deep convolutional neural network combined with classification and visualization based on non-linear dimension reduction (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1</xref>).</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1</label><caption hwp:id="caption-1"><title hwp:id="title-3">Overview of analysis workflow.</title><p hwp:id="p-9">Images from all channels of a high-throughput microscope are uniformly resized and directly fed into the neural network, which is trained using categorical labels. The learned features are used for both classification and visualization.</p></caption><graphic xlink:href="081364_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Materials and Methods</title><p hwp:id="p-10">The primary dataset in this paper consists in raw IFC images of 32,266 asynchronously growing immortalized human T lymphocyte cells (Jurkat cells), which was previously analyzed using tra­ ditional machine learning (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-3" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.,</italic> 2016</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Hennig <italic toggle="yes">et al.,</italic> 2016</xref>). Images of these cells can be classified into seven different stages of cell cycle (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>), including phases of interphase (G1, S and G2) and phases of mitosis (Prophase, Anaphase, Metaphase and Telophase). In this data set, ground truth is based on the inclusion of two fluorescent stains: propidium iodine (PI) to quantify each cell’s DNA content and the mitotic protein monoclonal #2 (MPM2) antibody to identify cells in mitotic phases. These stains allow each cell to be labeled through a combination of algorithmic segmentation, morphology analysis of the fluorescence channels, and user inspection (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-4" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.,</italic> 2016</xref>). Note that 97.78% of samples in the dataset belong to one of the interphase classes G1, Sand G2. The strong class imbalance in the dataset is related to the fact that interphase lasts — when considering the actual length of the biological process — a much longer period of time than mitosis.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2</label><caption hwp:id="caption-2"><title hwp:id="title-5">Representative images for the cell cycle stages as measured in brightfield, darkfield and fluorescence channels.</title><p hwp:id="p-11">Seven cell cycle stages define seven classes. We only show one representative image for the interphase classes G1, S, and G2, which can hardly be distinguished by eye.</p></caption><graphic xlink:href="081364_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-12">To substantiate the generality of our results, we study a second dataset that was collected with a technology other than IFC and is related to a biological process other than cell cycle: 30.000 publicly available images from the Diabetic Retinopathy Detection Challenge (2015). Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is diagnosed by trained humans based on the presence of lesions visible in color fundus photographies of the retina and is classified into four disease states: “healthy”, “mild”, “medium’’, and “severe”.</p><p hwp:id="p-13">Recent advances in deep learning have shown that deep neural networks are able to learn powerful feature representations (<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Krizhevsky <italic toggle="yes">et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Vincent <italic toggle="yes">et al.,</italic> 2010</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Szegedy <italic toggle="yes">et al.,</italic> 2015</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">LeCun <italic toggle="yes">et al.</italic>, 2015</xref>). We adapt the widely used “Inception” architecture (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Szegedy <italic toggle="yes">et al.</italic>, 2015</xref>) and, for the IFC data, optimize it for treating the relatively small input dimensions. The architecture consists in 13 three-layer “dual-path” modules (<xref rid="figS3" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Suppl. Fig. S3</xref>), which process and aggregate visual information at an increasing scale. These 39 layers are followed by a standard convolution layer, a fully connected layer and the softmax classifier. Training this 42-layer deep network does not present any computational difficulty, as the first three layers consist in reduction dual-path modules (<xref rid="figS3" ref-type="fig" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Suppl. Fig. S3b</xref>), which strongly reduce the original input dimensions prior to convolutions in the following normal dual-path modules. The number of kernels used in each layer increases towards the end, until 336 feature maps with size 8 × 8 are obtained. A final average pooling operation melts the local resolution of these maps and generates the last 336-dimensional layer, which serves as an input for both classification and visualization.</p><p hwp:id="p-14">This neural network operates directly on uniformly resized images. It is trained with labeled images using stochastic gradient descent with standard parameters (see Suppl. Notes). For the IFC data, we focus on the case in which only brightfield and darkfield channels are used as input for the network, during training, visualization and prediction. As stated before, this case is of high interest as a fluorescent markers might affect the biological process under study or adequate markers are not known. We note, however, that technical imperfections in the IFC data capture might always lead to a minor amount of fluorescence signal, activated by a fluorescence channel, in the darkfield and brightfield channels, a phenomenon known as “bleed through” (see Suppl. Notes).</p></sec><sec id="s3" hwp:id="sec-3"><title hwp:id="title-6">Results</title><p hwp:id="p-15">To show how learned features of the neural network can be used to visualize, organize and biologically interpret single-cell data, we study the activations in the last layer of the neural network (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Donahue <italic toggle="yes">et al.</italic>, 2013</xref>). We refer to this as studying the <italic toggle="yes">activation space representation</italic> of the data. The approach is motivated by the fact that the neural network strives to organize data in the last layer in a linearly separable way, given that it is directly followed by a softmax classifier. Distances from the separating hyperplanes in this space can be interpreted as similarities between cells in terms of the features extracted by the network. Cells with similar feature representations are close to each other and cells with different class assignments are far away from each other. As we will see, this gives a much more fine-grained notion of biological similarity than provided by the class labels used for labeling the training set. Evidently, it automatically generalizes to the unseen, new data in the validation data set.</p><p hwp:id="p-16">The activation space of our network’s last layer has 336 dimensions and is much too high-dimensional to be accessible for human interpretation. We use non-linear dimension reduction to visualize the data in a lower dimensional space, in particular, t-distributed stochastic neighbor embedding (tSNE) (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">van der Maaten and Hinton, 2008</xref>) (see Suppl. Video).</p><sec id="s3a" hwp:id="sec-4"><title hwp:id="title-7">Reconstructing cell cycle progression</title><p hwp:id="p-17">In this visualization, we observe that the Jurkat cell data is organized in a long stretched cylinder along which cell cycle phases are ordered in the chronologically correct order (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3a</xref>). This is remarkable as the network has been provided with neither structure within the class labels nor the relation among classes. The learned features evidently allow reconstructing the continuous temporal progression from the raw IFC data, and by that define a continuous distance between the phenotypes of different cell cycle phases.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7 xref-fig-3-8 xref-fig-3-9 xref-fig-3-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3</label><caption hwp:id="caption-3"><title hwp:id="title-8">Cell-cycle reconstruction and detection of abnormal cells.</title><p hwp:id="p-18"><bold>a,</bold> tSNE visualization of the validation dataset in activation space representation. All interphase classes (G1, S, G2) and the two mitotic phases with the highest number of representatives are shown (Prophase: red, Metaphase: blue). Telophase and Anaphase are not visible due to their low number representatives. <bold>b,</bold> tSNE visualization of data from the interphase classes (G1, S, G2) in activation space. The color map now shows the DNA content of cells. A cluster of damaged cells is indicated with an arrow. <bold>c,</bold> Randomly picked representatives from the bulk of undamaged cells. <bold>d,</bold> Randomly picked representatives from the cluster of damaged cells.</p></caption><graphic xlink:href="081364_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-19">We separately visualized just those cells annotated as being in the interphase classes (G1, S, G2) (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3b</xref>) and colored them with the DNA content obtained from one of the fluorescent channels (PI). The DNA content reflects the continuous progression of cells in G1, S and G2 on a more fine-grained level. Its correspondence with the longitudinal direction of the cylinder found by tSNE demonstrates that the temporal order learned by the neural network is accurate even beyond the categorical class labels.</p></sec><sec id="s3b" hwp:id="sec-5"><title hwp:id="title-9">Detecting abnormal cells in an unsupervised manner</title><p hwp:id="p-20">Both tSNE visualizations (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3a,b</xref>) produce a small, separate cluster highlighted with an arrow in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3b</xref>. This cluster is learned in an unsupervised way as cell cycle phase labels provide no information about it: it contains cells from all three interphase classes. While cells in the bulk have high circularity and well defined borders (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3c</xref>), cells in the small cluster are characterized by morphological abnormalities such as broken cell walls and outgrowths, signifying dead cells (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Fig. 3d</xref>).</p></sec><sec id="s3c" hwp:id="sec-6"><title hwp:id="title-10">Deep learning automatically performs segmentation</title><p hwp:id="p-21">We interpret the data representation encoded in one of the trained intermediate layers of the neural network by inspecting its activation patterns using exemplary input data from several classes (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4</xref>). These activation patterns are the essential information transmitted through the network. They show the response of various kernels on their input. By inspecting the activation patterns, we obtain an insight into what the network is “focusing on” in order to organize data. We observe a strong response to features that arise from the cell border thickness (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4</xref>, map 1), to area-based features (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4</xref>, map 2), as well as cross-channel features. For example, map 4 in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4</xref> shows high response to the difference of information from the brightfield channel, as seen in map 2, and scatter intensities, as seen in map 3. A strong response of the neural network to area-based features as in map 2 could indicate that the network learned to perform a segmentation task.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4</label><caption hwp:id="caption-4"><title hwp:id="title-11">Exemplary activation patterns of intermediate layers.</title><p hwp:id="p-22">Plotted are activations after the second convolutional module for examples of single cells from four different phases: <bold>a,</bold> G1 <bold>b,</bold> G2 <bold>c,</bold> Anaphase and <bold>d,</bold> Telophase. The response maps mark regions of high activation. Map 1 responds to the cell boundaries. Map 2 responds to the internal area of the cells. Map 3 extracts the localized scatter intensities. Map 4 constitutes a cross-channel feature, which correlates with the difference of map 2 and 3.</p></caption><graphic xlink:href="081364_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s3d" hwp:id="sec-7"><title hwp:id="title-12">Deep learning outperforms boosting for cell cycle classification</title><p hwp:id="p-23">We study the classification performance of Deep learning on the validation data set shown in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Fig. 3</xref>. We first focus on the case in which G1, S and G2 phases are considered as a single class. Using five-fold cross-validation on the 32,266 cells, we obtain an accuracy of 98.73%±0.16%. This means a 6-fold improvement in error rate over the 92.35% accuracy for the same task on the same data in prior work using boosting on features extracted via image analysis (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-5" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic>, 2016</xref>). The confusion matrix obtained using boosting show high true positive rates for the mitotic phases (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 5a</xref>). For example, no cells in Anaphase and Telophase are wrongly classified, as indicated by the zeros in the off-diagonal entries of the two lower rows of the matrix (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig. 5a</xref>). This means high <italic toggle="yes">sensitivity</italic>, most cells from mitotic phases are correctly classified as such. Still this comes at the price of low <italic toggle="yes">precision</italic>: many cells from the interphase class are classified as mitotic phases, as indicated by the high numbers in the off-diagonal entries of the first row of the matrix (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig. 5a</xref>). Deep learning, by contrast, achieves high sensitivity and precision, leading to an almost diagonal confusion matrix (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig. 5b</xref>). Further Deep learning allows to classify all seven cell cycle stages with an accuracy of 79.40%±0.77% (see Suppl. Notes and <xref rid="figS2" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Suppl. Fig. S2</xref>).</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5</label><caption hwp:id="caption-5"><title hwp:id="title-13">Confusion matrices for boosting and Deep learning for classification of five classes.</title><p hwp:id="p-24">To compare with previous work (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-6" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic>, 2016</xref>), the three interphase phases (G1, S, G2) are treated as a single class. Red numbers denote absolute numbers of cells in each entry of the confusion matrix, that is, diagonal elements correspond to precision. Coloring of the matrix is obtained by normalizing absolute numbers to column sums. <bold>a,</bold> Boosting (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-7" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic>, 2016</xref>), which leads to 92.35% accuracy. <bold>b,</bold> Deep learning, which leads to 98.73%±0.16% accuracy.</p></caption><graphic xlink:href="081364_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec><sec id="s3e" hwp:id="sec-8"><title hwp:id="title-14">Reconstructing disease progression</title><p hwp:id="p-25">Consider now the second dataset, which deals with diabetic retinopathy (DR), as described above. Having trained the neural network with four different qualitative disease states, “healthy”, “mild”, “medium”, and “severe”, we observe a reconstructed disease progression (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Fig. 6</xref>) for 8000 samples in the validation dataset, that is, the four disease states are ordered along disease severity, even though the network has not been provided with the ordering information. Similar to the cell cycle example, the ordering ensures that only neighboring classes overlap, as visible from the tSNE plot (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Fig. 6a</xref>). Hence, the confusion matrix (not shown) displays a similar close-to tridiagonal structure as for the cell cycle (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig. 5b</xref> and <xref rid="figS2" ref-type="fig" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Suppl. Fig. S2a</xref>).</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6</label><caption hwp:id="caption-6"><title hwp:id="title-15">Reconstruction of disease progression in diabetic retinopathy.</title><p hwp:id="p-26"><bold>a,</bold> tSNE visualization of activation space representation, colored according to the disease states. <bold>b,</bold> Randomly chosen images for each class.</p></caption><graphic xlink:href="081364_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec></sec><sec id="s4" hwp:id="sec-9"><title hwp:id="title-16">Discussion</title><p hwp:id="p-27">The visualization of the data as encoded in the last layer of the network using tSNE demonstrates how deep learning overcomes a well known issue of traditional machine learning. When trained on a continuous biological process using discrete class labels, traditional machine learning often fail to resolve the continuum (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Eliceiri <italic toggle="yes">et al.</italic>, 2012</xref>). Reconstructing continuous biological processes though is possible in the context of so-called pseudotime algorithms (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Bendall <italic toggle="yes">et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">Trapnell <italic toggle="yes">et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Haghverdi <italic toggle="yes">et al.</italic>, 2016</xref>). For the cell-cycle it has been demonstrated by <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-3" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic> (2015)</xref>, but in a very different setting. These authors measured five stains that uniquely define the cell cycle and then applied a pseudotime algorithm (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-3" hwp:rel-id="ref-3">Bendall <italic toggle="yes">et al.</italic>, 2014</xref>) within this five-dimensional space. This procedure is only possible if stains that correlate with a given process of interest are known, if they do not interact with the process and if the elaborate experiments for measuring the intensity of these stains can be carried out. We, by contrast, use raw images directly and the learned features of the neural network automatically constitute a feature space in which data is continuously organized. In the Suppl. Notes, we demonstrate that pseudotime algorithms fail at solving this much harder problem.</p><p hwp:id="p-28">Deep learning is able to reconstruct continuous processes based on categorical labels as adjacent classes are morphologically more similar than classes that are temporally further separated. If this assumption does not hold, also pseudotime algorithms fail to reconstruct a process. This can be better understood when inspecting <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Fig. 6a</xref>, where we show the tSNE visualization of the validation set for the diabetic retinopathy (DR) data. Samples are organized in the correct order of progression through disease states, from healthy to severe DR. However, between the healthy cluster (green) and the mild DR cluster (orange), one observes an area of slightly reduced sampling density (dashed line). This should not be attributed to “less data points having been sampled in this region” but should be seen as a consequence of the fact that the overlap between the “healthy” stage and the “mild” stage is smaller than the overlap of the diseased stages among each other. If there was no overlap between “healthy” and “mild” stages, the tSNE would show a complete separation of the healthy cluster from the rest of the data. Such a behavior is typically observed if the underlying data is not sampled from a continuous process.</p><p hwp:id="p-29">The unsupervised detection of a discrete cluster of abnormal cells for the Jurkat cell data indicates that the neural network learns the cluster of abnormal cells independently of the cell-cycle-label based training. The model is therefore not only capable of resolving a biological process, but generates features that are general enough to separate incorrectly labeled cells that do not belong to the process. None of the mentioned pseudotime algorithms is capable of this. This shows the ability of deep learning to find unknown phenotypes and processes without knowledge about features or labels. Also, there is a high practical use of the detection of damaged cells. The Jurkat cell data set has been preprocessed using the IDEAS<sup>®</sup> analysis software to remove images of abnormal cells. In particular, out of focus cells were removed by gating for images with gradient RMS and debris was removed by gating for circular objects with a large area. The discovery of a cluster of abnormal cells shows the limitations of this approach and provides a solution to it.</p><p hwp:id="p-30">An advantage of using a neural network for cell classification in IFC is its speed. Traditional techniques rely on image segmentation and measurement, time-consuming processes limited to roughly 10 cells per second. Neural network predictions, by contrast, are extremely fast, as the main computation consists in parallelizable matrix multiplications (“forward propagations”), which can be performed using optimized numeric libraries. This yields a roughly 100-fold improvement in speed to about 1000 cells per second with a single GPU. Aside from much faster analysis of large cell populations, this opens the door to “sorting on the fly”: imaging flow cytometers currently do not allow physically sorting individual cells into separate receptacles based on measured parameters, due to these speed limitations.</p></sec><sec id="s5" hwp:id="sec-10"><title hwp:id="title-17">Conclusion</title><p hwp:id="p-31">Given the compelling performance on reconstructing the cell cycle, we expect deep learning to be helpful for understanding a wide variety of biological processes involving continuous morphology changes. Examples include developmental stages of organisms and the progression of healthy states to disease states, situations that have often been non-ideally reduced to binary classification problems. Ignoring intrinsic heterogeneity has likely hindered a deeper insight into the mechanisms at work. Analysis as demonstrated here could reveal morphological signatures at much earlier stages than previously recognized.</p><p hwp:id="p-32">Our results indicate that reconstructing biological process is possible for a wide variety of image data, if enough samples are available. Although generally lower-throughput in terms of the number of cell processed, conventional microscopy is nevertheless still high-throughput and can usually provide higher resolution images than IFC. Furthermore, given that multi-spectral methods are advancing rapidly, imaging mass spectrometry is allowing dozens of labeled channels to be acquired (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Bodenmiller <italic toggle="yes">et al.</italic>, 2012</xref>; <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Angelo <italic toggle="yes">et al.</italic>, 2014</xref>). Due to its basic structure and high flexibility, our deep learning framework can accommodate a large increase in the number of available channels.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-18">Acknowledgments</title><p hwp:id="p-33">F.A.W. acknowledges support by the Helmholtz Postdoc Programme, Initiative and Networking Fund of the Helmholtz Association. P.R. and A.E.C. acknowledge the support of the Biotechnology and Biological Sciences Research Council/ National Science Foundation under grant BB/N005163/1 and NSF DBI 1458626.</p></ack><sec id="s7" sec-type="supplementary-metarial" hwp:id="sec-11"><title hwp:id="title-19">Supplemental Notes</title><sec id="s7a" hwp:id="sec-12"><title hwp:id="title-20">Pseudotime-based reconstruction of cell cycle</title><p hwp:id="p-34">To compare our deep learning results with the class of pseudotime algorithms (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-4" hwp:rel-id="ref-3">Bendall <italic toggle="yes">et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-3" hwp:rel-id="ref-28">Trapnell <italic toggle="yes">et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-3" hwp:rel-id="ref-15">Haghverdi <italic toggle="yes">et al.</italic>, 2016</xref>; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-4" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic>, 2015</xref>), we use Diffusion Pseudotime (DPT) (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-4" hwp:rel-id="ref-15">Haghverdi <italic toggle="yes">et al.</italic>, 2016</xref>) in the implementation of Scanpy (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Wolf <italic toggle="yes">et al.</italic>, 2017</xref>). DPT has recently been very favorably discussed by the authors of Monocle (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Qiu <italic toggle="yes">et al.</italic>, 2017</xref>), one of the most established pseudotime algorithms (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-4" hwp:rel-id="ref-28">Trapnell <italic toggle="yes">et al.</italic>, 2014</xref>), and is more robust than Wanderlust (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-5" hwp:rel-id="ref-3">Bendall <italic toggle="yes">et al.</italic>, 2014</xref>), the underlying algorithm of <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-5" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic> (2015)</xref>. Using DPT, we infer the progression of Jurkat cells based on different sets of features:
<list list-type="bullet" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-35">deep learning (this work),</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-36">classical image features (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-8" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic>, 2016</xref>),</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-37">marker intensity (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-6" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic>, 2015</xref>).</p></list-item></list></p><p hwp:id="p-38">For this, we focus on predicting the DNA content of cells, which measures the progression of cell cycle during G1, S and G2 phase as in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-8" hwp:rel-id="F3">Fig. 3b</xref>. Deep learning outperforms classical image extraction techniques (<xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. S1a versus b</xref>): Both in the tSNE and the pseudotime versus DNA content scatter, a high correlation is only visible in the case of deep learning (Pearson correlation of 0.56 versus 0.021). Note that the tSNE in <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig. S1a</xref> shows the same data as <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-9" hwp:rel-id="F3">Fig. 3b</xref>, but in a two-dimensional tSNE.</p><fig id="figS1" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Supplementary Figure S1</label><caption hwp:id="caption-7"><title hwp:id="title-21">Comparison of pseudotime inference based on different feature sets.</title><p hwp:id="p-39">The first row shows scatter plots of DNA content versus pseudotime. The second row shows tSNE visualizations of the data represented in the respective features spaces. <bold>a,</bold> Deep learning (this work), <bold>b,</bold> classical features (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-9" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic>, 2016</xref>) <bold>c,</bold> marker intensity, analogous to <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-7" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic> (2015)</xref>, with a feature space of dimension 5, but here, only one dimension stores the marker that measures DNA content. <bold>d,</bold> Same as <bold>c</bold>, but for a feature space of dimension 2.</p></caption><graphic xlink:href="081364_figs1" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-40">When using the marker expression directly as an input for the pseudotime reconstruction, as done by <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-8" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic> (2015)</xref>, the quality of the reconstruction depends on the number of informative features. Here, we consider the five-dimensional (<xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Fig. S1c</xref>) and the two-dimensional case (<xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Fig. S1d</xref>). In both cases, the first feature is the marker intensity measuring DNA content, and other features are non-specific classical image features. It is not astonishing that in the latter case (<xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Fig. S1d</xref>), pseudotime correlates very well with the DNA content. If, as done by <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-9" hwp:rel-id="ref-14">Gut <italic toggle="yes">et al.</italic> (2015)</xref>, only the informative feature is used as an input, one obtains perfect agreement. It is important to note that this requires knowledge of the markers and measuring the known markers, both of which poses strong constraints on the problem of interest. Also, note that only the deep learning based features are not only able to reconstruct the process, but at the same time separate the cluster of abnormal cells (tSNE of <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Fig. S1a</xref>, clearly marked in the 3-dimensional version <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-10" hwp:rel-id="F3">Fig. 3b</xref>).</p><p hwp:id="p-41">Finally, note that the marker expression cannot only be used in pseudotime algorithms but also directly as an input for a regression based machine learning evalution. This has been studied for the cell-cycling Jurkat cells by <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-10" hwp:rel-id="ref-4">Blasi <italic toggle="yes">et al.</italic> (2016)</xref>, who in addition to the classification discussed in the main text above, trained a regression boosting on the classical image features using the marker for DNA content as a continuous label. Not astonishingly, the results for this were much better (Pearson correlations of 0.786 to 0.894). The disadvantages of such an analysis and the advantages of our approach have already been discussed.</p><p hwp:id="p-42">More visualizations and details on the analysis of this section are available here.</p></sec><sec id="s7b" hwp:id="sec-13"><title hwp:id="title-22">Classification of all seven cell-cycle phases</title><p hwp:id="p-43">We also evaluated the full seven-class problem in which the three interphase classes are considered individually. Here, we obtain an accuracy of 79.40%±0.77%. This number serves as an orientation for what deep learning could be able to achieve on this particularly hard classification problem — G1, S and G2 are extremely difficult to distinguish (see <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2</xref>), even when using information from the fluorescence channels. The accuracy might therefore be affected by wrong labelling, it might be higher if all fluorescence channels were used as input for the neural network, and it might be slightly lower if “bleed through” enriched brightfield and darkfield images. If high classification accuracy is of importance, and one is not only interested in visualizing and interpreting the data, these questions have to be answered from case to case. Their answer depends in particular on how labels are generated and how many channels of the IFC are used.</p><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3 xref-fig-8-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Supplementary Figure S2</label><caption hwp:id="caption-8"><title hwp:id="title-23">Performance of Deep learning for classification of seven classes.</title><p hwp:id="p-44"><bold>a,</bold> Confusion matrix. Red numbers denote absolute numbers of cells in each entry of the confusion matrix. Coloring of the matrix is obtained by normalizing absolute numbers to column sums, that is, diagonal elements correspond to precision. <bold>b,</bold> Class-specific Receiver Operating Characteristics.</p></caption><graphic xlink:href="081364_figs2" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-45">Here, we confirm that the considerably lower accuracy as compared to the five-class problem results primarily from cells in the S phase being wrongly classified as either G1 or G2 (<xref rid="figS2" ref-type="fig" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Fig. S2a</xref>). This is also shown by the Receiver Operating Characteristic, which relates the true positive rate (sensitivity) with the false positive rate (fall-out) as the classification threshold changes (<xref rid="figS2" ref-type="fig" hwp:id="xref-fig-8-4" hwp:rel-id="F8">Fig. S2b</xref>). Integrating the curve to obtain the standard performance metric “Area under the curve” (AUC). Even though the AUC for the S phase is still high with 0.87, it is the lowest among the majority classes (G1, S, G2), and therefore has a strong effect on the accuracy. Overall we find that all seven classes yield high values, greater than 0.85, and four of the seven classes, yield very high values, greater than 0.95.</p></sec><sec id="s7c" hwp:id="sec-14"><title hwp:id="title-24">Technical Notes</title><sec id="s7c1" hwp:id="sec-15"><title hwp:id="title-25">Preprocessing</title><p hwp:id="p-46">Our algorithmic workflow of cell cycle analysis with Deep Neural Networks begins with brightfield and darkfield images from the cells. In order to allow uniform training of our network on the whole dataset, we resize the images to 66 × 66 pixels by stretching the border pixels. We choose this method over individual image rescaling to avoid the destruction of possibly important size relation information between cells.</p><p hwp:id="p-47">The data set used in this paper has been preprocessed using the IDEAS<sup>®</sup> software (Merck Millipore Inc.) to remove images of abnormal cells. In particular, we removed out of focus cells by gating for images with gradient RMS and removed debris by gating for circular cells with a large area.</p></sec><sec id="s7c2" hwp:id="sec-16"><title hwp:id="title-26">Network architecture</title><p hwp:id="p-48"><xref rid="figS3" ref-type="fig" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Figure S3</xref> shows normal and reduction dual-path modules, the basic elements of the deep learning architecture discussed in the main text. Kernel sizes, stride and number of filters are indicated in the figure.</p><p hwp:id="p-49">The network architecture consists of 42 layers, which results in a total number of parameters of about 2 mio. It is build up starting with 3 dual-path reduction modules, followed by 10 normal dual-path modules, one pooling layer, one fully connected layer and the softmax layer. Each dual-path module consists in 3 layers: a convolution layer, a batch normalization layer and an activation layer. Although there is no “big” fundamental difference between dualpath and standard convolution modules, dual-path based networks tend to converge a little better in practice, since the gradient flow from pooling and convolution in the reduction module counteracts the vanishing gradient problem: not the entire gradient gets multiplied by approx 10<italic toggle="yes">−</italic>4 convolutional weight, pooling just lets it through.</p><fig id="figS3" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;081364v2/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Supplementary Figure S3</label><caption hwp:id="caption-9"><title hwp:id="title-27">Dual-path modules.</title><p hwp:id="p-50"><bold>a,</bold> Normal dual-path module. <bold>b,</bold> Reduction dual-path module. The numbers beneath the convolution operations indicate the kernel sizes, stride and the number of filters.</p></caption><graphic xlink:href="081364_figs3" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-51">In the first (input) layer, all IFC channels are combined in a linear operation by feeding them in the channel — which equals the color — dimension of the convolution input. This means the convolution uses kernels which convolve over all channels simultaneously. The number of 3×3 kernel weights then is nine times the number of channels. Increasing the number of channels simply increases the “kernel depth” in the color dimension, and hence, is trivial.</p><p hwp:id="p-52">We note that the specific choice of the architecture is a matter of experience and cannot be further justified. Readers might use these parameters to reproduce our results or produce similar results on similar problems. We also note that there is some freedom in the specific choice of the architecture, small modifications will not qualitatively alter the results.</p></sec><sec id="s7c3" hwp:id="sec-17"><title hwp:id="title-28">Training details</title><p hwp:id="p-53">The network was trained for 100 epochs using stochastic gradient descent with standard parameters: 0.9 momentum, a fixed learning rate of 0.01 up to epoch 85 and of 0.001 afterwards as well as a slightly regularizing weight decay of 0.0005. Training took around 7 h and was stopped manually by inspecting convergence cross-entropy.</p></sec><sec id="s7c4" hwp:id="sec-18"><title hwp:id="title-29">Implementation</title><p hwp:id="p-54">For the results presented in this paper, we implemented neural networks using the MxNet frame-work (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Chen <italic toggle="yes">et al.</italic>, 2016b</xref>) on a NVIDIA Titan X GPU. MxNet is lightweight, fast and memory efficient and available from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/dmlc/mxnet" ext-link-type="uri" xlink:href="https://github.com/dmlc/mxnet" hwp:id="ext-link-2">https://github.com/dmlc/mxnet</ext-link>. Due to the fast progress in the development of deep learning software packages, in the meanwhile, we have also implemented and successfully tested our architecture using TensorFlow, which is available from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/tensorflow/tensorflow" ext-link-type="uri" xlink:href="https://github.com/tensorflow/tensorflow" hwp:id="ext-link-3">https://github.com/tensorflow/tensorflow</ext-link>. The user might choose the software package according to personal preferences.</p></sec><sec id="s7c5" hwp:id="sec-19"><title hwp:id="title-30">Nonlinear dimension reduction</title><p hwp:id="p-55">We use tSNE (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">van der Maaten and Hinton, 2008</xref>). See Scanpy (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Wolf <italic toggle="yes">et al.</italic>, 2017</xref>) for a package that provides several non-linear dimension reduction methods for visualizing single-cell data.</p></sec><sec id="s7c6" hwp:id="sec-20"><title hwp:id="title-31">Bleed through</title><p hwp:id="p-56">The data acquired using the ImageStream was fully compensated using typical control images (see Ref. (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Filby <italic toggle="yes">et al.</italic>, 2011</xref>)) so the image tiffs would have minimal bleed through between channels. We could not detect even a slight indication of bleed through in the Jurkat cell data, neither upon inspection by eye, nor upon correlating the integrated intensity of each fluorescence channel with the integrated intensity of bright and darkfield channels, respectively. We then checked the existence of bleed through in the Cytometer used for data generation by switching off the light source of the brightfield channel, while keeping the fluorescence excitation on. We would then expect zero intensity in the brightfield images, but instead measured a slight intensity stemming from the fluorescence channels. This common technical aspect of IFC measurements merits an own investigation and will appear elsewhere. Here, our aim is to compare methodologies rather than to claim absolute levels of accuracy.</p></sec></sec></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-32">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Angelo M."><surname>Angelo</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bendall S. C."><given-names>S. C.</given-names> <surname>Bendall</surname></string-name>, <string-name name-style="western" hwp:sortable="Finck R."><given-names>R.</given-names> <surname>Finck</surname></string-name>, <etal>et al.</etal> (<year>2014</year>), <article-title hwp:id="article-title-2">Multiplexed ion beam imaging of human breast tumors</article-title>, <source hwp:id="source-1">Nature Medicine</source> <volume>20</volume>(<issue>4</issue>), <fpage>436</fpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Basiji D. A."><surname>Basiji</surname>, <given-names>D. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ortyn W. E."><given-names>W. E.</given-names> <surname>Ortyn</surname></string-name>, <string-name name-style="western" hwp:sortable="Liang L."><given-names>L.</given-names> <surname>Liang</surname></string-name>, <string-name name-style="western" hwp:sortable="Venkatachalam V."><given-names>V.</given-names> <surname>Venkatachalam</surname></string-name>, and <string-name name-style="western" hwp:sortable="Morrissey P."><given-names>P.</given-names> <surname>Morrissey</surname></string-name> (<year>2007</year>), <article-title hwp:id="article-title-3">Cellular image analysis and imaging by flow cytometry</article-title>, <source hwp:id="source-2">Clinics in Laboratory Medicine</source> <volume>27</volume>(<issue>3</issue>), <fpage>653</fpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2 xref-ref-3-3 xref-ref-3-4 xref-ref-3-5"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bendall S. C."><surname>Bendall</surname>, <given-names>S. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davis K. L."><given-names>K. L.</given-names> <surname>Davis</surname></string-name>, <string-name name-style="western" hwp:sortable="Amir E.-a. D."><given-names>E.-a. D.</given-names> <surname>Amir</surname></string-name>, <string-name name-style="western" hwp:sortable="Tadmor M. D."><given-names>M. D.</given-names> <surname>Tadmor</surname></string-name>, <string-name name-style="western" hwp:sortable="Simonds E. F."><given-names>E. F.</given-names> <surname>Simonds</surname></string-name>, <string-name name-style="western" hwp:sortable="Chen T. J."><given-names>T. J.</given-names> <surname>Chen</surname></string-name>, <string-name name-style="western" hwp:sortable="Shenfeld D. K."><given-names>D. K.</given-names> <surname>Shenfeld</surname></string-name>, <string-name name-style="western" hwp:sortable="Nolan G. P."><given-names>G. P.</given-names> <surname>Nolan</surname></string-name>, and <string-name name-style="western" hwp:sortable="Pe’er D."><given-names>D.</given-names> <surname>Pe’er</surname></string-name> (<year>2014</year>), <article-title hwp:id="article-title-4">Single-cell trajectory detection uncovers progression and regulatory coordination in human B cell development</article-title>, <source hwp:id="source-3">Cell</source> <volume>157</volume>(<issue>3</issue>), <fpage>714</fpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2 xref-ref-4-3 xref-ref-4-4 xref-ref-4-5 xref-ref-4-6 xref-ref-4-7 xref-ref-4-8 xref-ref-4-9 xref-ref-4-10"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Blasi T."><surname>Blasi</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hennig H."><given-names>H.</given-names> <surname>Hennig</surname></string-name>, <string-name name-style="western" hwp:sortable="Summers H. D."><given-names>H. D.</given-names> <surname>Summers</surname></string-name>, <string-name name-style="western" hwp:sortable="Theis F. J."><given-names>F. J.</given-names> <surname>Theis</surname></string-name>, <string-name name-style="western" hwp:sortable="Cerveira J."><given-names>J.</given-names> <surname>Cerveira</surname></string-name>, <string-name name-style="western" hwp:sortable="Patterson J. O."><given-names>J. O.</given-names> <surname>Patterson</surname></string-name>, <string-name name-style="western" hwp:sortable="Davies D."><given-names>D.</given-names> <surname>Davies</surname></string-name>, <string-name name-style="western" hwp:sortable="Filby A."><given-names>A.</given-names> <surname>Filby</surname></string-name>, <string-name name-style="western" hwp:sortable="Carpenter A. E."><given-names>A. E.</given-names> <surname>Carpenter</surname></string-name>, and <string-name name-style="western" hwp:sortable="Rees P."><given-names>P.</given-names> <surname>Rees</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-5">Label-free cell cycle analysis for high-throughput imaging flow cytometry</article-title>, <source hwp:id="source-4">Nature Communications</source> <volume>7</volume>, <fpage>10256</fpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Bodenmiller B."><surname>Bodenmiller</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zunder E. R."><given-names>E. R.</given-names> <surname>Zunder</surname></string-name>, <string-name name-style="western" hwp:sortable="Finck R."><given-names>R.</given-names> <surname>Finck</surname></string-name>, <etal>et al.</etal> (<year>2012</year>), <article-title hwp:id="article-title-6">Multiplexed mass cytometry profiling of cellular states perturbed by small-molecule regulators</article-title>, <source hwp:id="source-5">Nature Biotechnology</source> <volume>30</volume>(<issue>9</issue>), <fpage>858</fpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Brown M."><surname>Brown</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wittwer C."><given-names>C.</given-names> <surname>Wittwer</surname></string-name> (<year>2000</year>), <article-title hwp:id="article-title-7">Flow cytometry: principles and clinical applications in hematology</article-title>, <source hwp:id="source-6">Clinical chemistry</source> <volume>46</volume>(<issue>8</issue>), <fpage>1221</fpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Chen C. L."><surname>Chen</surname>, <given-names>C. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mahjoubfar A."><given-names>A.</given-names> <surname>Mahjoubfar</surname></string-name>, <string-name name-style="western" hwp:sortable="Tai L.-C."><given-names>L.-C.</given-names> <surname>Tai</surname></string-name>, <string-name name-style="western" hwp:sortable="Blaby I. K."><given-names>I. K.</given-names> <surname>Blaby</surname></string-name>, <string-name name-style="western" hwp:sortable="Huang A."><given-names>A.</given-names> <surname>Huang</surname></string-name>, <string-name name-style="western" hwp:sortable="Niazi K. R."><given-names>K. R.</given-names> <surname>Niazi</surname></string-name>, and <string-name name-style="western" hwp:sortable="Jalali B."><given-names>B.</given-names> <surname>Jalali</surname></string-name> (<year>2016a</year>), <article-title hwp:id="article-title-8">Deep learning in label-free cell classification</article-title>, <source hwp:id="source-7">Scientific Reports</source> <volume>6</volume>, <fpage>21471</fpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Chen T."><surname>Chen</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li M."><given-names>M.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Li Y."><given-names>Y.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Lin M."><given-names>M.</given-names> <surname>Lin</surname></string-name>, <string-name name-style="western" hwp:sortable="Wang N."><given-names>N.</given-names> <surname>Wang</surname></string-name>, <string-name name-style="western" hwp:sortable="Wang M."><given-names>M.</given-names> <surname>Wang</surname></string-name>, <string-name name-style="western" hwp:sortable="Xiao T."><given-names>T.</given-names> <surname>Xiao</surname></string-name>, <string-name name-style="western" hwp:sortable="Xu B."><given-names>B.</given-names> <surname>Xu</surname></string-name>, <string-name name-style="western" hwp:sortable="Zhang C."><given-names>C.</given-names> <surname>Zhang</surname></string-name>, and <string-name name-style="western" hwp:sortable="Zhang Z."><given-names>Z.</given-names> <surname>Zhang</surname></string-name> (<year>2016b</year>), <article-title hwp:id="article-title-9">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</article-title>, in <source hwp:id="source-8">In Neural Information Processing Systems, Workshop on Machine Learning Systems</source>, <volume>1512</volume>.<fpage>01274</fpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Dao D."><surname>Dao</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fraser A. N."><given-names>A. N.</given-names> <surname>Fraser</surname></string-name>, <string-name name-style="western" hwp:sortable="Hung J."><given-names>J.</given-names> <surname>Hung</surname></string-name>, <string-name name-style="western" hwp:sortable="Ljosa V."><given-names>V.</given-names> <surname>Ljosa</surname></string-name>, <string-name name-style="western" hwp:sortable="Singh S."><given-names>S.</given-names> <surname>Singh</surname></string-name>, and <string-name name-style="western" hwp:sortable="Carpenter A. E."><given-names>A. E.</given-names> <surname>Carpenter</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-10">Cellprofiler analyst: interactive data exploration, analysis, and classification of large biological image sets</article-title>, <source hwp:id="source-9">Bioinformatics</source> <volume>32</volume>(<issue>20</issue>), <fpage>3210</fpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Donahue J."><surname>Donahue</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jia Y."><given-names>Y.</given-names> <surname>Jia</surname></string-name>, <string-name name-style="western" hwp:sortable="Vinyals O."><given-names>O.</given-names> <surname>Vinyals</surname></string-name>, <string-name name-style="western" hwp:sortable="Hoffman J."><given-names>J.</given-names> <surname>Hoffman</surname></string-name>, <string-name name-style="western" hwp:sortable="Zhang N."><given-names>N.</given-names> <surname>Zhang</surname></string-name>, <string-name name-style="western" hwp:sortable="Tzeng E."><given-names>E.</given-names> <surname>Tzeng</surname></string-name>, and <string-name name-style="western" hwp:sortable="Darrell T."><given-names>T.</given-names> <surname>Darrell</surname></string-name> (<year>2013</year>), <source hwp:id="source-10">Decaf: Adeep convolutional activation feature for generic visual recognition</source>, <volume>1310</volume>.<fpage>1531</fpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Dürr O."><surname>Dürr</surname>, <given-names>O.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Sick B."><given-names>B.</given-names> <surname>Sick</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-11">Single-cell phenotype classification using deep convolutional neural networks</article-title>, <source hwp:id="source-11">Journal of Biomolecular Screening</source> <volume>21</volume>(<issue>9</issue>), <fpage>998</fpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Eliceiri K. W."><surname>Eliceiri</surname>, <given-names>K. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Berthold M. R."><given-names>M. R.</given-names> <surname>Berthold</surname></string-name>, <string-name name-style="western" hwp:sortable="Goldberg I. G."><given-names>I. G.</given-names> <surname>Goldberg</surname></string-name>, <etal>et al.</etal> (<year>2012</year>), <article-title hwp:id="article-title-12">Biological imaging software tools</article-title>, <source hwp:id="source-12">Nature Methods</source> <volume>9</volume>(<issue>7</issue>), <fpage>697</fpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Filby A."><surname>Filby</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perucha E."><given-names>E.</given-names> <surname>Perucha</surname></string-name>, <string-name name-style="western" hwp:sortable="Summers H."><given-names>H.</given-names> <surname>Summers</surname></string-name>, <string-name name-style="western" hwp:sortable="Rees P."><given-names>P.</given-names> <surname>Rees</surname></string-name>, <string-name name-style="western" hwp:sortable="Chana P."><given-names>P.</given-names> <surname>Chana</surname></string-name>, <string-name name-style="western" hwp:sortable="Heck S."><given-names>S.</given-names> <surname>Heck</surname></string-name>, <string-name name-style="western" hwp:sortable="Lord G. M."><given-names>G. M.</given-names> <surname>Lord</surname></string-name>, and <string-name name-style="western" hwp:sortable="Davies D."><given-names>D.</given-names> <surname>Davies</surname></string-name> (<year>2011</year>), <article-title hwp:id="article-title-13">An imaging flow cytometric method for measuring cell division history and molecular symmetry during mitosis</article-title>, <source hwp:id="source-13">Cytometry Part A</source> <volume>79A</volume>(<issue>7</issue>), <fpage>496</fpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2 xref-ref-14-3 xref-ref-14-4 xref-ref-14-5 xref-ref-14-6 xref-ref-14-7 xref-ref-14-8 xref-ref-14-9"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Gut G."><surname>Gut</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tadmor M. D."><given-names>M. D.</given-names> <surname>Tadmor</surname></string-name>, <string-name name-style="western" hwp:sortable="Pe’er D."><given-names>D.</given-names> <surname>Pe’er</surname></string-name>, <string-name name-style="western" hwp:sortable="Pelkmans L."><given-names>L.</given-names> <surname>Pelkmans</surname></string-name>, and <string-name name-style="western" hwp:sortable="Liberali P."><given-names>P.</given-names> <surname>Liberali</surname></string-name> (<year>2015</year>), <article-title hwp:id="article-title-14">Trajectories of cell-cycle progression from fixed cell populations</article-title>, <source hwp:id="source-14">Nature Methods</source> <volume>12</volume>(<issue>10</issue>), <fpage>951</fpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2 xref-ref-15-3 xref-ref-15-4"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Haghverdi L."><surname>Haghverdi</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Büttner M."><given-names>M.</given-names> <surname>Büttner</surname></string-name>, <string-name name-style="western" hwp:sortable="Wolf F. A."><given-names>F. A.</given-names> <surname>Wolf</surname></string-name>, <string-name name-style="western" hwp:sortable="Buettner F."><given-names>F.</given-names> <surname>Buettner</surname></string-name>, and <string-name name-style="western" hwp:sortable="Theis F. J."><given-names>F. J.</given-names> <surname>Theis</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-15">Diffusion pseudotime robustly reconstructs branching cellular lineages</article-title>, <source hwp:id="source-15">Nature Methods</source> <volume>13</volume>, <fpage>845</fpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Hennig H."><surname>Hennig</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rees P."><given-names>P.</given-names> <surname>Rees</surname></string-name>, <string-name name-style="western" hwp:sortable="Blasi T."><given-names>T.</given-names> <surname>Blasi</surname></string-name>, <string-name name-style="western" hwp:sortable="Kamentsky L."><given-names>L.</given-names> <surname>Kamentsky</surname></string-name>, <string-name name-style="western" hwp:sortable="Hung J."><given-names>J.</given-names> <surname>Hung</surname></string-name>, <string-name name-style="western" hwp:sortable="Dao D."><given-names>D.</given-names> <surname>Dao</surname></string-name>, <string-name name-style="western" hwp:sortable="Carpenter A. E."><given-names>A. E.</given-names> <surname>Carpenter</surname></string-name>, and <string-name name-style="western" hwp:sortable="Filby A."><given-names>A.</given-names> <surname>Filby</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-16">An open-source solution for advanced imaging flow cytometry data analysis using machine learning</article-title>, <source hwp:id="source-16">Methods</source> doi:<pub-id pub-id-type="doi">10.1016/j.ymeth.2016.08.018</pub-id>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Jones T. R."><surname>Jones</surname>, <given-names>T. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carpenter A. E."><given-names>A. E.</given-names> <surname>Carpenter</surname></string-name>, <string-name name-style="western" hwp:sortable="Lamprecht M. R."><given-names>M. R.</given-names> <surname>Lamprecht</surname></string-name>, <etal>et al.</etal> (<year>2009</year>), <article-title hwp:id="article-title-17">Scoring diverse cellular morphologies in image-based screens with iterative feedback and machine learning</article-title>, <source hwp:id="source-17">PNAS</source> <volume>106</volume>(<issue>6</issue>), <fpage>1826</fpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Kandaswamy C."><surname>Kandaswamy</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Silva L. M."><given-names>L. M.</given-names> <surname>Silva</surname></string-name>, <string-name name-style="western" hwp:sortable="Alexandre L. A."><given-names>L. A.</given-names> <surname>Alexandre</surname></string-name>, and <string-name name-style="western" hwp:sortable="Santos J. M."><given-names>J. M.</given-names> <surname>Santos</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-18">High-content analysis of breast cancer using single-cell deep transfer learning</article-title>, <source hwp:id="source-18">Journal of Biomolecular Screening</source> <volume>21</volume>(<issue>3</issue>), <fpage>252</fpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Kraus O. Z."><surname>Kraus</surname>, <given-names>O. Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ba L. J."><given-names>L. J.</given-names> <surname>Ba</surname></string-name>, and <string-name name-style="western" hwp:sortable="Frey B."><given-names>B.</given-names> <surname>Frey</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-19">Classifying and segmenting microscopy images with deep multiple instance learning</article-title>, <source hwp:id="source-19">Bioinformatics</source> <volume>32</volume>(<issue>12</issue>), <fpage>i52</fpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="book" citation-type="book" ref:id="081364v2.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Krizhevsky A."><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hinton G. E."><given-names>G. E.</given-names> <surname>Hinton</surname></string-name> (<year>2012</year>), <chapter-title>Imagenet classification with deep convolutional neural networks</chapter-title>, in <source hwp:id="source-20">Advances in Neural Information Processing Systems</source> <volume>25</volume>, edited by <string-name name-style="western" hwp:sortable="Pereira F."><given-names>F.</given-names> <surname>Pereira</surname></string-name>, <string-name name-style="western" hwp:sortable="Burges C. J. C."><given-names>C. J. C.</given-names> <surname>Burges</surname></string-name>, <string-name name-style="western" hwp:sortable="Bottou L."><given-names>L.</given-names> <surname>Bottou</surname></string-name>, and <string-name name-style="western" hwp:sortable="Weinberger K. Q."><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name> (<publisher-name>Curran Associates, Inc</publisher-name>.) pp. <fpage>1097</fpage>–<lpage>1105</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="LeCun Y."><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hinton G."><given-names>G.</given-names> <surname>Hinton</surname></string-name> (<year>2015</year>), <article-title hwp:id="article-title-20">Deep learning</article-title>, <source hwp:id="source-21">Nature</source> <volume>521</volume>(<issue>7553</issue>), <fpage>436</fpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="van der Maaten L."><surname>van der Maaten</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Hinton G."><given-names>G.</given-names> <surname>Hinton</surname></string-name> (<year>2008</year>), <article-title hwp:id="article-title-21">Visualizing data using t-sne</article-title>, <source hwp:id="source-22">Journal of Machine Learning Research</source> <volume>9</volume>(<issue>Nov</issue>), <fpage>2579</fpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Pepperkok R."><surname>Pepperkok</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Ellenberg J."><given-names>J.</given-names> <surname>Ellenberg</surname></string-name> (<year>2006</year>), <article-title hwp:id="article-title-22">High-throughput fluorescence microscopy for systems biology</article-title>, <source hwp:id="source-23">Nat. Rev. Mol. Cell Biol</source>. <volume>7</volume>(<issue>9</issue>), <fpage>690</fpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Pärnamaa T."><surname>Pärnamaa</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Parts L."><given-names>L.</given-names> <surname>Parts</surname></string-name> (<year>2016</year>), <article-title hwp:id="article-title-23">Accurate classification of protein subcellular localization from high throughput microscopy images using deep learning</article-title>, <source hwp:id="source-24">bioRixv</source> doi:<pub-id pub-id-type="doi">10.1101/050757</pub-id>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Qiu X."><surname>Qiu</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mao Q."><given-names>Q.</given-names> <surname>Mao</surname></string-name>, <string-name name-style="western" hwp:sortable="Tang Y."><given-names>Y.</given-names> <surname>Tang</surname></string-name>, <string-name name-style="western" hwp:sortable="Wang L."><given-names>L.</given-names> <surname>Wang</surname></string-name>, <string-name name-style="western" hwp:sortable="Chawla R."><given-names>R.</given-names> <surname>Chawla</surname></string-name>, <string-name name-style="western" hwp:sortable="Pliner H."><given-names>H.</given-names> <surname>Pliner</surname></string-name>, and <string-name name-style="western" hwp:sortable="Trapnell C."><given-names>C.</given-names> <surname>Trapnell</surname></string-name> (<year>2017</year>), <article-title hwp:id="article-title-24">Reversed graph embedding resolves complex single-cell developmental trajectories</article-title>, <source hwp:id="source-25">bioRxiv</source> doi:<pub-id pub-id-type="doi">10.1101/110668</pub-id>.</citation></ref><ref id="c26" hwp:id="ref-26"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Schroff F."><surname>Schroff</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kalenichenko D."><given-names>D.</given-names> <surname>Kalenichenko</surname></string-name>, and <string-name name-style="western" hwp:sortable="Philbin J."><given-names>J.</given-names> <surname>Philbin</surname></string-name> (<year>2015</year>), <source hwp:id="source-26">Facenet: A unified embedding for face recognition and clustering</source>, <volume>1503</volume>.<fpage>03832</fpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><citation publication-type="confproc" citation-type="confproc" ref:id="081364v2.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Szegedy C."><surname>Szegedy</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu W."><given-names>W.</given-names> <surname>Liu</surname></string-name>, <string-name name-style="western" hwp:sortable="Jia Y."><given-names>Y.</given-names> <surname>Jia</surname></string-name>, <string-name name-style="western" hwp:sortable="Sermanet P."><given-names>P.</given-names> <surname>Sermanet</surname></string-name>, <string-name name-style="western" hwp:sortable="Reed S."><given-names>S.</given-names> <surname>Reed</surname></string-name>, <string-name name-style="western" hwp:sortable="Anguelov D."><given-names>D.</given-names> <surname>Anguelov</surname></string-name>, <string-name name-style="western" hwp:sortable="Erhan D."><given-names>D.</given-names> <surname>Erhan</surname></string-name>, <string-name name-style="western" hwp:sortable="Vanhoucke V."><given-names>V.</given-names> <surname>Vanhoucke</surname></string-name>, and <string-name name-style="western" hwp:sortable="Rabinovich A."><given-names>A.</given-names> <surname>Rabinovich</surname></string-name> (<year>2015</year>), <article-title hwp:id="article-title-25">Going deeper with convolutions</article-title>, in <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>1</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2 xref-ref-28-3 xref-ref-28-4"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Trapnell C."><surname>Trapnell</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cacchiarelli D."><given-names>D.</given-names> <surname>Cacchiarelli</surname></string-name>, <string-name name-style="western" hwp:sortable="Grimsby J."><given-names>J.</given-names> <surname>Grimsby</surname></string-name>, <string-name name-style="western" hwp:sortable="Pokharel P."><given-names>P.</given-names> <surname>Pokharel</surname></string-name>, <string-name name-style="western" hwp:sortable="Li S."><given-names>S.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Morse M."><given-names>M.</given-names> <surname>Morse</surname></string-name>, <string-name name-style="western" hwp:sortable="Lennon N. J."><given-names>N. J.</given-names> <surname>Lennon</surname></string-name>, <string-name name-style="western" hwp:sortable="Livak K. J."><given-names>K. J.</given-names> <surname>Livak</surname></string-name>, <string-name name-style="western" hwp:sortable="Mikkelsen T. S."><given-names>T. S.</given-names> <surname>Mikkelsen</surname></string-name>, and <string-name name-style="western" hwp:sortable="Rinn J. L."><given-names>J. L.</given-names> <surname>Rinn</surname></string-name> (<year>2014</year>), <article-title hwp:id="article-title-26">The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells</article-title>, <source hwp:id="source-27">Nature Biotechnology</source> <volume>32</volume>(<issue>4</issue>), <fpage>381</fpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="081364v2.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Vincent P."><surname>Vincent</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Larochelle H."><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name name-style="western" hwp:sortable="Lajoie I."><given-names>I.</given-names> <surname>Lajoie</surname></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, and <string-name name-style="western" hwp:sortable="Manzagol P.-A."><given-names>P.-A.</given-names> <surname>Manzagol</surname></string-name> (<year>2010</year>), <article-title hwp:id="article-title-27">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</article-title>, <source hwp:id="source-28">Journal of Machine Learning Research ll(Dec</source>), <fpage>3371</fpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><citation publication-type="other" citation-type="journal" ref:id="081364v2.30" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Wolf F. A."><surname>Wolf</surname>, <given-names>F. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Angerer P."><given-names>P.</given-names> <surname>Angerer</surname></string-name>, <string-name name-style="western" hwp:sortable="Simon L."><given-names>L.</given-names> <surname>Simon</surname></string-name>, and <string-name name-style="western" hwp:sortable="Theis F. J."><given-names>F. J.</given-names> <surname>Theis</surname></string-name> (<year>2017</year>), <article-title hwp:id="article-title-28">Scanpy for larg e-scale single-cell analysis</article-title>, <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/theislab/scanpy" ext-link-type="uri" xlink:href="https://github.com/theislab/scanpy" hwp:id="ext-link-4">https://github.com/theislab/scanpy</ext-link>.</citation></ref></ref-list></back></article>
