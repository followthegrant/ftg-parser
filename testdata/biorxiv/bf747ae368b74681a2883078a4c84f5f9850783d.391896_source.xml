<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/391896</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;391896</article-id><article-id pub-id-type="other" hwp:sub-type="slug">391896</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">391896</article-id><article-id pub-id-type="other" hwp:sub-type="tag">391896</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Eye movements during text reading align with the rate of speech production</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label><bold>Corresponding Author</bold>: Dr. Benjamin Gagl, <email hwp:id="email-1">benjamin.gagl@univie.ac.at</email>, Department of Linguistics, Sensengasse 3, A-1090 Wien</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2339-6293</contrib-id><name name-style="western" hwp:sortable="Gagl Benjamin"><surname>Gagl</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-2339-6293"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Gregorova Klara"><surname>Gregorova</surname><given-names>Klara</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Golch Julius"><surname>Golch</surname><given-names>Julius</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Hawelka Stefan"><surname>Hawelka</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9935-8621</contrib-id><name name-style="western" hwp:sortable="Sassenhagen Jona"><surname>Sassenhagen</surname><given-names>Jona</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-9935-8621"/></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Tavano Alessandro"><surname>Tavano</surname><given-names>Alessandro</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><name name-style="western" hwp:sortable="Poeppel David"><surname>Poeppel</surname><given-names>David</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-2" hwp:rel-id="aff-5">5</xref><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-1" hwp:rel-id="aff-6">6</xref><xref ref-type="aff" rid="a7" hwp:id="xref-aff-7-1" hwp:rel-id="aff-7">7</xref></contrib><contrib contrib-type="author" hwp:id="contrib-8"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0827-1721</contrib-id><name name-style="western" hwp:sortable="Fiebach Christian J."><surname>Fiebach</surname><given-names>Christian J.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a8" hwp:id="xref-aff-8-1" hwp:rel-id="aff-8">8</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-0827-1721"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5"><label>1</label><institution hwp:id="institution-1">Department of Psychology, Goethe University Frankfurt</institution>, Frankfurt am Main, <country>Germany</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label><institution hwp:id="institution-2">Center for Individual Development and Adaptive Education of Children at Risk (IDeA)</institution>, Frankfurt am Main, <country>Germany</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Department of Linguistics, University of Vienna</institution>, Vienna, <country>Austria</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Centre for Cognitive Neuroscience, University of Salzburg</institution>, Salzburg, <country>Austria</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1 xref-aff-5-2"><label>5</label><institution hwp:id="institution-5">Max Planck Institute for Empirical Aesthetics</institution>, Frankfurt am Main, <country>Germany</country></aff><aff id="a6" hwp:id="aff-6" hwp:rev-id="xref-aff-6-1"><label>6</label><institution hwp:id="institution-6">Department of Psychology, New York University</institution>, <country>United States of America</country></aff><aff id="a7" hwp:id="aff-7" hwp:rev-id="xref-aff-7-1"><label>7</label><institution hwp:id="institution-7">Max-Planck-NYU Center for Language</institution>, Music, and Emotion (CLaME)</aff><aff id="a8" hwp:id="aff-8" hwp:rev-id="xref-aff-8-1"><label>8</label><institution hwp:id="institution-8">Brain Imaging Center, Goethe University Frankfurt</institution>, Frankfurt am Main, <country>Germany</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-08-14T19:51:18-07:00">
    <day>14</day><month>8</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-05-28T05:33:15-07:00">
    <day>28</day><month>5</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-08-14T19:57:42-07:00">
    <day>14</day><month>8</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-05-28T05:37:19-07:00">
    <day>28</day><month>5</month><year>2021</year>
  </pub-date><elocation-id>391896</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-08-14"><day>14</day><month>8</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2021-05-27"><day>27</day><month>5</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-05-28"><day>28</day><month>5</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">Â© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="391896.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/391896v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="391896.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/391896v3/391896v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/391896v3/391896v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Across languages, the speech signal is characterized by a predominant modulation of the amplitude spectrum between about 4.3-5.5Hz, reflecting the production and processing of linguistic information chunks (syllables, words) every â¼200ms. Interestingly, â¼200ms is also the typical duration of eye fixations during reading. Prompted by this observation, we demonstrate that German readers sample written text at â¼5Hz. A subsequent meta-analysis with 142 studies from 14 languages replicates this result, but also shows that sampling frequencies vary across languages between 3.9Hz and 5.2Hz, and that this variation systematically depends on the complexity of the writing systems (character-based vs. alphabetic systems, orthographic transparency). Finally, we demonstrate empirically a positive correlation between speech spectrum and eye-movement sampling in low-skilled readers. Based on this convergent evidence, we propose that during reading, our brainâs linguistic processing systems imprint a preferred processing rate, i.e., the rate of spoken language production and perception, onto the oculomotor system.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">reading</kwd><kwd hwp:id="kwd-2">speech</kwd><kwd hwp:id="kwd-3">eye-movements</kwd><kwd hwp:id="kwd-4">linguistic information sampling</kwd><kwd hwp:id="kwd-5">z-string scanning</kwd></kwd-group><counts><page-count count="42"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-4">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-1"><p hwp:id="p-4">The main change to the previous version is the addition of Study 4, including a new, pre-registered, empirical investigation directly comparing the eye movement sampling rate and the speech rate in native German speakers.</p></fn></fn-group></notes></front><body><sec hwp:id="sec-1"><p hwp:id="p-5">Speech production and perception form a quasi-rhythmic information processing cycle<sup><xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref></sup>. During spoken communication, our brain entrains to the frequency structure of the speech signal<sup><xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>,<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref></sup>, suggesting that the temporal structure of the linguistic stimulus drives neural processes in auditory and language processing systems<sup><xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref></sup>. Across languages, the amplitude modulation spectrum of natural speech peaks consistently in a frequency range between 4.3 and 5.5Hz<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref></sup>, which reflects that informative signals (e.g., syllables<sup><xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref></sup>) are processed by the listenersâ brains every â¼200ms<sup><xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref></sup>. Interestingly â and we hypothesize not accidentally â a typical eye fixation during reading has a very similar duration, i.e., between â¼200ms for orthographically transparent writing systems like German or Finnish<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>,<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref></sup> and â¼250ms for character-based systems like Chinese<sup><xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref>,<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref></sup>.</p><p hwp:id="p-6">Abundant research has used eye-movement recordings to study reading at high temporal resolution, exploring, for example, how reading is influenced by word length, word frequency, or word predictability given a sentence context<sup><xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref></sup>. Among various measures that can be derived from eye-movement recordings, timing measures like fixation duration are most frequently examined and considered precise markers of reading speed<sup><xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>,<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref></sup>. These temporally highly-resolved measurements have so far only been analyzed at the level of individual items â typically words. However, other domains of cognitive research (like attention<sup><xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref></sup>) demonstrate that eye movements can also be subjected to frequency-based analyses. We here demonstrate that a frequency-based exploration of how written text is sampled by the eyes can open up new perspectives onto several fundamental questions related to the process of reading, including whether reading is related to spoken language processing as recent investigations of word-per-minute measures suggest<sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref></sup> and whether the visual systemâs sampling of linguistic input differs from eye movements during non-linguistic tasks or between different languages or writing systems.</p><p hwp:id="p-7">To address these foundational questions, we first used an empirical dataset<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref></sup> to determine eye-movement sampling frequencies for 50 native speakers of German during sentence reading compared to a non-linguistic control task, using two different methodologies. Next, to determine the generality of these results and to investigate possible cross-linguistic differences in the sampling rate of reading, we conducted a meta-analysis of 124 studies from 14 different languages. To this end, we established a frequency analysis for fixation durations extracted from published eye-tracking studies. Finally, we acquired two novel datasets, one with 48 non-native and one with 86 native speakers of German, to investigate directly the relationship between the sampling frequency of reading and speech production rates on a subject-by-subject level. Experimental and meta-analytic results show (i) that written text is sampled in the same frequency range as spoken language, (ii) that the sampling rate of reading has an upper limit at â¼5Hz, observable in languages with transparent orthographies, (iii) that this rate can be modulated depending on the complexity of the writing system (e.g., in character-based as opposed to alphabetic scripts or in alphabetic scripts with opaque grapheme-phoneme mapping), and (iv) that a direct coupling between reading and speech rates is only found in persons with lower levels of reading skill.</p></sec><sec id="s1" hwp:id="sec-2"><title hwp:id="title-5">Results: Estimating the sampling rate of reading</title><p hwp:id="p-8">50 healthy volunteers read sentences from the Potsdam Sentence Corpus (144 sentences presented as a whole; 1,138 words in total; see Ref.<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref></sup>) while movements of their right eye were tracked (resolution: 1,000Hz). As non-linguistic control task, participants scanned âz-stringsâ that were constructed by replacing all letters of the sentence by the letter âzâ (e.g., â<italic toggle="yes">Ein berÃ¼hmter Maler hat sich selbst ein Ohr abgeschnittenâ/ A famous painter cut off his own ear</italic>. was transformed to â<italic toggle="yes">Zzz zzzzzzzzz Zzzzz zzz zzzz zzzzzz zzz Zzz zzzzzzzzzzzzz</italic>.â; see Methods for details and Ref.<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref></sup> for previous results from this dataset). Given that fixation numbers do not differ significantly between sentences and z-strings<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">17</xref>â<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref></sup>, similar scan paths are assumed which qualifies z-strings as valid control stimuli for reading experiments (see Supplementary Information 1 for a detailed comparison of scan-paths).</p><sec id="s1a" hwp:id="sec-3"><title hwp:id="title-6">Fixation durations</title><p hwp:id="p-9">After preprocessing (leading to removal of 3.1% of the data), we estimated mean fixation durations separately for each participant and experimental condition. <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1a</xref> shows that fixation durations (presented here as subject-specific means) are shorter for reading than scanning (average: 197ms vs. 249ms, respectively; Effect size: 52ms; Cohenâs d =1.57; <italic toggle="yes">t</italic>(49)=11.1; p&lt;.001). This has been reported previously for this dataset<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-4" hwp:rel-id="ref-17">17</xref></sup> and replicates earlier results for German<sup><xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref></sup>, English<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref></sup>, and French<sup><xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref></sup> in which fixation durations increased from reading to scanning between 38-42ms.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11 xref-fig-1-12 xref-fig-1-13 xref-fig-1-14 xref-fig-1-15 xref-fig-1-16"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-10">Reading-related sampling rates. (a) Subject-specific mean fixation durations from 50 participants (dots), the overall mean (circle), and confidence intervals (colored bars) while reading sentences on the Potsdam sentence corpus<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-3" hwp:rel-id="ref-10">10</xref></sup> and scanning z-strings. Lines connect reading with z-string scanning data, per subject, to visualize effects at the single-subject level. Violin plot shows the distribution of individual means (Blue: Scanning; Green: Reading; similar in d and f). (b) Mean saccade probability (across all participants and stimuli, separated by task) relative to the first saccade of the sentence, with a non-linear regression line. (c) The sampling period t of one event was defined as the duration of a fixation plus its preceding saccade. Displayed is the distribution of these sampling periods for sentence reading (green) and z-string scanning (blue), with estimated means (â+â symbol and dashed lines) and modes (asterisk and solid lines). (d) Subject-specific mean sampling frequencies f (i.e., equals to 1/t) and the overall mean (crossed circle) based on the sampling periods shown in c. (e) Power-spectrum for reading and z-string scanning, estimated across all participants using Fourier transform analysis. (f) Individual peak frequencies estimated from individual power spectra and their mean (crossed circle). See Methods for details.</p></caption><graphic xlink:href="391896v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-11">As a first characterization of rhythmic eye-movement patterns during reading, we plotted for each sample point after stimulus onset the probability that a saccade occurs (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1b</xref>). This analysis demonstrates distinct peaks visible at regular intervals, providing evidence that eye-movements follow a rhythmic structure in both reading and scanning. Importantly, this rhythmic pattern is more pronounced and faster during reading. Dominant sampling rates were estimated directly from fixation durations, as well as using classical frequency analysis. While the former approach is important because fixation durations are also the basis for the subsequent meta-analysis, the latter approach allows us to evaluate the validity of fixation-duration-based frequency estimation.</p><p hwp:id="p-12">To estimate sampling rates from fixation durations, we first estimated sampling periods <italic toggle="yes">t</italic> (i.e., the time from the start of a saccade to the start of the next), by adding to each fixation duration (N=112,547) the duration of the preceding saccade. <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1c</xref> shows the distribution of all sampling periods across participants, separately for reading and z-string scanning. Note that due to the ex-Gaussian distribution of fixation durations typical for fixation duration data<sup><xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref></sup>, the mean (dashed line) overestimates the central tendency, whereas the mode (solid line) â by definition â is a better representation of the predominant sampling period (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1c</xref>). Next, we estimated an eye-movement sampling frequency <italic toggle="yes">f</italic> for each participant and condition, by dividing 1sec by the subject-specific mode of the sampling period in seconds. This revealed a higher average sampling rate for reading (5.0Hz) relative to the control task (4.2Hz; <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1d</xref>). This difference was significant (Cohenâs d=-1.16; <italic toggle="yes">t</italic>(49)=-8.2; p&lt;.001) and 45 of 50 participants showed a numeric reduction of sampling frequency from reading to scanning (grey lines in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1d</xref>). We find virtually the same pattern of effects when regressive saccades are removed (i.e., when analyzing only single fixation cases; Cohenâs d=-1.0; t(49)= -6.9; p&lt;.001; absolute values: 4.9Hz and 4.2Hz for reading and scanning, respectively) and only slightly higher values when restricting analyses to inter-word re-fixations (i.e., fixations after regressive saccades; 5.2Hz vs. 4.6Hz, respectively; Cohenâs d=-0.8; t(48)=-5.5; p&lt;.001; note that one participant was excluded due to the absence of regressive saccades in the scanning task). Sampling rates of reading and scanning, thus, are highly similar between forward-oriented and regressive eye movements (<italic toggle="yes">r</italic>=0.6; <italic toggle="yes">t</italic>(96)=6.7; <italic toggle="yes">p</italic>&lt;.001). Therefore, all further analyses will not differentiate between these cases. Note that estimating sampling rates from the mean (rather than mode) of fixation durations results in lower rates for reading (4.5Hz) and scanning (3.7Hz). This results from an overestimation of the central tendency by the mean in right skewed distributions (see <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1c</xref>) and indicates that this procedure would be inadequate.</p><p hwp:id="p-13">Finally, power spectra of reading vs. z-string scanning were estimated using canonical frequency analysis. For each task, we created a time series (resolution 1,000Hz) starting with the first saccade of the first participant and ending with the last fixation of the last participant, with a â1â at the exact time of saccade onset and â0â elsewhere. Note that saccade onsets are the appropriate event for generating this time series, as they are the re-occurring event and can be measured with high accuracy<sup><xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref></sup>. Subsequently, power spectra of these task-specific event timecourses were estimated via Fourier Transform to visualize periodic signal components across subjects (see Methods). Corroborating the results of the first analysis approach, a prominent peak was found at 5Hz for reading and a somewhat less pronounced peak at â¼4Hz for scanning (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Figure 1e</xref>). To compare these estimates between reading and scanning, we next estimated separate power spectra for each participant. Individual peaks were retrieved, averaged (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Figure 1f</xref>), and statistically compared. This analysis reproduces the sampling frequencies estimated from the mode of fixation durations, with frequencies of 5.0Hz and 4.4Hz for reading and scanning, respectively (Cohenâs d=-1.12; <italic toggle="yes">t</italic>(49)=-7.9; p&lt;.001). There was a high correlation between the two analysis approaches (reading: <italic toggle="yes">r</italic>=.80; <italic toggle="yes">t</italic>(48)=9.3; <italic toggle="yes">p</italic>&lt;.001; scanning: <italic toggle="yes">r</italic>=.62; <italic toggle="yes">t</italic>(48)=5.5; <italic toggle="yes">p</italic>&lt;.001), which underscores the validity of sampling-duration-based frequency estimations.</p><p hwp:id="p-14">To summarize, a quantitative frequency-domain characterization of eye-tracking data shows that the predominant sampling frequency during reading in German, across participants, is found at â¼5Hz. This frequency representation of the reading process falls squarely within the boundaries of the predominant modulation frequencies of 4.3-5.5Hz determined for speech signals across languages<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref></sup> which in turn have a clear reflection in the neuronal response to speech<sup><xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">3</xref></sup>. We observed the â¼5Hz peak during reading using two different analysis strategies, i.e., when estimating sampling frequencies from saccade and fixation durations and when analyzing the sequence of saccade events in the frequency domain. Attentive scanning of z-strings shows highly similar scan path characteristics compared to reading<sup><xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>,<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-3" hwp:rel-id="ref-19">19</xref></sup>, but a significantly lower sampling frequency at â¼4.2Hz, convergent with findings from non-linguistic attentional reorienting tasks<sup><xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>,<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref></sup>.</p><p hwp:id="p-15">An analysis of the pupil response in this same dataset had previously indicated higher cognitive effort during reading as compared to z-string scanning<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-5" hwp:rel-id="ref-17">17</xref></sup>. This finding most likely reflects the additional involvement of reading-specific and linguistic processes, like lexical-semantic access, beyond the oculomotor sampling itself. Thus, the specific sampling rate observed for reading is unlikely to be driven exclusively by (perceptual or cognitive) features of the stimulus. In light of the overlap with the rate of spoken language, we tentatively propose that the observed sampling rate of â¼5Hz may reflect functional constraints imposed by the interface nature that the process of reading has between visual and linguistic processing (which developed primarily based on spoken language). We speculate that the brainâs language systems impose the cortical rate at which speech is produced and perceived onto oculomotor programming systems exclusively during reading, possibly to optimize language-related information processing.</p><p hwp:id="p-16">This hypothesis predicts that the overlap of reading and speech rates should generalize across languages and writing systems. On the other hand, writing systems differ substantially between languages<sup><xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">11</xref>,<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref></sup>, and even within writing systems, the mapping from orthography to meaning differs between languages<sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref></sup>. For example, the letter <italic toggle="yes">a</italic> in <italic toggle="yes">cat</italic> vs. <italic toggle="yes">ball</italic> maps onto two different speech sounds in English, whereas it maps onto the same sound in the German translations of these words (<italic toggle="yes">Katze</italic> vs. <italic toggle="yes">Ball</italic>). This letter-to-sound correspondence strongly influences reading acquisition<sup><xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref></sup>, so that among the alphabetic writing systems, opaque orthographies (writing systems like English with inconsistent letter-to-sound correspondences) are associated with lower reading accuracy during the first years of learning to read. These differences would be suggestive of cross-linguistic differences in the frequency at which written text can be sampled, and recent experimental evidence like the observation of longer fixation durations for Chinese as compared to Finnish or English<sup><xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-4" hwp:rel-id="ref-11">11</xref></sup> seems to support this prediction.</p></sec></sec><sec id="s2" hwp:id="sec-4"><title hwp:id="title-7">Results: Cross-linguistic meta-analysis of reading rates</title><p hwp:id="p-17">To investigate the language generality of the alignment between speech and reading rates, we conducted a meta-analysis of sampling frequencies during reading in 14 different languages, based on 1,420 fixation duration estimates extracted from 124 original studies published between 2006 and 2016 (see <italic toggle="yes">Methods</italic> for selection criteria). In addition to this cross-linguistic comparison, we examined (a) possible differences between character-based vs. alphabetic writing systems and (b) the effect of letter-to-sound correspondence among alphabetic writing systems. Also, we explore (c) the cross-linguistic correlation between eye-movement sampling frequencies and language-specific peaks of the speech modulation spectra, and (d) the association between reading rates and information density (linguistic information per syllable<sup><xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref></sup>) across languages.</p><p hwp:id="p-18">All studies selected for inclusion reported mean fixation durations. However, as discussed above, mean fixation durations are not a valid representation of the predominant sampling duration in fixation data and accordingly not the preferred basis for calculating the sampling rate of reading. We used 29 full empirical datasets to develop a transformation function that allowed us to estimate the mode from the mean fixation durations reported in the original publications. In brief, this involved fitting ex-Gaussian distributions to the empirical distributions of these datasets, retrieving distributional parameters (including mean and mode), and on this basis optimizing a regression-based transformation that estimates the mode from the mean (see <italic toggle="yes">Methods</italic> and <italic toggle="yes">Supplementary Methods</italic> for details). For the meta-analysis, mean durations were extracted from published studies and transformed to the mode. The sampling period <italic toggle="yes">t</italic> (interval from saccade onset to end of following fixation; see above) was obtained by adding an estimate (the mode saccade duration from Study 1; 29ms) to the mean fixation duration. Lastly, the sampling frequency was calculated as <italic toggle="yes">f</italic>=1/<italic toggle="yes">t</italic>.</p><sec id="s2a" hwp:id="sec-5"><title hwp:id="title-8">Fixation durations and sampling frequency: Descriptive statistics</title><p hwp:id="p-19"><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref> shows that the majority of mean fixation durations derived from the reading studies were between 200-300ms (upper panel), which transforms to mean sampling frequencies between 3.9-5.2Hz (lower panel). Note that languages with only one original study (Arabic, Italian, Polish) were excluded. As expected, the majority of studies was conducted in English<sup><xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref></sup>. 10 of the 14 languages in our meta-analysis fall between the minimum (4.3Hz) and the maximum (5.5Hz) of previously reported<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-3" hwp:rel-id="ref-6">6</xref></sup> language-specific peaks of the speech amplitude modulation spectra (see <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2</xref>, lower panel, dashed lines). The remaining four languages fell within the range of one standard deviation around the language-specific speech peaks (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2</xref>, lower panel, dotted lines). Considering the language-specific confidence intervals, only for Chinese can we be confident that the sampling rate is lower than the range of the speech amplitude modulation spectra<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-4" hwp:rel-id="ref-6">6</xref></sup>. Of the 1,420 individual sampling rate values derived from the studies included in our meta-analysis, only 3.0% fell below, and only 0.3% were above the range of one standard deviation of the mean (see violin plot in the lower panel of <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figure 2</xref>). Nevertheless, the mean sampling rate of reading observed when averaging across all languages is at the lower bound of the speech modulation range, i.e., at 4.3Hz (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Figure 2</xref>, lower panel).</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-20">Meta-analysis of reading-related sampling rates. Fixation durations (upper panel) and corresponding eye movement sampling frequencies (lower panel) for 14 different languages. Violin plots (left) represent the respective distributions of all 1,420 duration/frequency values extracted from the included studies, independent of language. Bars reflect confidence intervals, and circles reflect the mean. In the right panel, each dot reflects one study (mean number of fixation durations per study: 12.4); Bars reflect confidence intervals, and circles reflect the mean across studies for each language. In the lower panel, the dashed lines represent the range of the means of the peak amplitude modulation spectrum that was empirically determined for speech in different languages in independent work<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-5" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-5" hwp:rel-id="ref-6">6</xref></sup>. The dotted lines represent the range between the lowest mean minus one standard deviation and the highest mean plus one standard deviation for the same data (that was manually read out from <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 3c</xref> in Ref.<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-6" hwp:rel-id="ref-5">5</xref></sup> and from Figure 7 in Ref.<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-6" hwp:rel-id="ref-6">6</xref></sup>). For Arabic, 1 study/ 12 fixation durations are available, Chinese 20/205, Dutch 5/45, English 65/965, Finnish 3/21, French 2/3, German 14/48, Hebrew 3/28, Italian 1/1, Jap. 2/12, Korean 2/39, Polish 1/1, Spanish 4/10 and Thai 3/30.</p></caption><graphic xlink:href="391896v3_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2b" hwp:id="sec-6"><title hwp:id="title-9">Effect of writing system on sampling frequency</title><p hwp:id="p-21">The observed cross-linguistic differences, arguably, are related to different language characteristics. One plausible hypothesis is that the higher perceptual complexity of character-based scripts (as opposed to alphabetic scripts<sup><xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">24</xref></sup>) may modulate the rate at which written text is sampled. <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3a</xref> shows that the eye-movement sampling frequency is significantly lower for Chinese (the only character-based language included; n=205 estimated sampling rates from 20 studies; mean: 3.9Hz) than for alphabetic languages (n=1,215 sampling rates; 97 studies; mean: 4.5Hz; effect size estimate (Est) of difference: -.70Hz; Standard error (SE): .13; <italic toggle="yes">t</italic>=5.2; see <italic toggle="yes">Methods</italic> for details on linear mixed-effects modeling).</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-22">Comparison of writing systems. (a) Character vs. alphabetic contrast, including 205 fixation durations from 20 Chinese reading studies (brown) and 1,215 fixation durations from 97 studies of reading in alphabetic languages (grey). (b) The effect of language transparency/opacity. Only studies from alphabetic languages for which the number of grapheme-to-phoneme rules could be quantitatively estimated from published computational models (see <italic toggle="yes">Methods</italic>) were used (four languages with a total of N = 1,025 fixation durations). Dots reflect each study, and crossed circles reflect the mean across studies for each language. The dashed line in (b) represents the approximation of the language transparency/opacity effect based on a linear regression.</p></caption><graphic xlink:href="391896v3_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec><sec id="s2c" hwp:id="sec-7"><title hwp:id="title-10">Effect of orthographic complexity on sampling frequency</title><p hwp:id="p-23">Within alphabetic languages, one plausible hypothesis is that the orthographic difficulty of writing systems influences the speed of sampling the visual input<sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref></sup>. To examine this, we quantified orthographic difficulty as a continuous predictor representing the number of grapheme-to-phoneme conversion rules<sup><xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref></sup> as defined by computationally implemented dual-route models of visual word recognition<sup><xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref></sup>. Graphemes are letters or letter combinations that map onto one or multiple speech sounds (phonemes); for illustration, remember the above example of mapping the grapheme <italic toggle="yes">a</italic> onto one (Katze/Ball) vs. two (<italic toggle="yes">cat</italic>/<italic toggle="yes">ball) phonemes</italic>, requiring one vs. two rules. To date, computational implementations are available for only five out of the nine alphabetic languages included in this meta-analysis, which restricts this test to English, French, German, and Dutch (with n = 965, 3, 48, 45 data points, respectively; Italian, the fifth language, was excluded due to lack of sufficient data points). A detailed comparison of these language-specific model implementations can be found in Ref.<sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref></sup>. <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3b</xref> demonstrates that less transparent writing systems (operationalized as more grapheme-to-phoneme rules) elicit significantly lower sampling frequencies (Est: -.10Hz; SE=.03; <italic toggle="yes">t</italic>=3.0). Highly transparent orthographies (German, Dutch) produce relatively fast sampling rates around 5Hz (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3b</xref>).</p></sec><sec id="s2d" hwp:id="sec-8"><title hwp:id="title-11">Effects of speech rate and information density on sampling rates</title><p hwp:id="p-24">Lastly, we explored the effect of cross-linguistic differences in speech rate<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-7" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-7" hwp:rel-id="ref-6">6</xref></sup> and information density (information per syllable<sup><xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">27</xref></sup>) on the observed eye-movement sampling rates (see <italic toggle="yes">Methods</italic>). To control for the strong effects of orthographic differences on sampling rates (as above), linear mixed models were calculated that also included the factor alphabetic vs. character-based script (effect size estimates/Est in both models&lt;-0.57Hz; SE&lt;0.15; <italic toggle="yes">t</italic>&gt;4). Neither the between-language differences in speech frequencies (Est: -0.03; SE=.03; <italic toggle="yes">t</italic>=0.8) nor information density (Est: - 0.03; SE=0.05; <italic toggle="yes">t</italic>=0.7) showed significant effects on the eye-movement sampling rate (all analyses including Chinese, Dutch, English, French, Japanese, and Spanish).</p><p hwp:id="p-25">As further exploration, we investigated the relationship between speech and reading rates within alphabetic languages for which estimates of orthographic complexity (grapheme-to-phoneme rules) could be taken into account (English, French, Dutch). This analysis also failed to produce significant effects (Est.:0.06; SE=0.06; t=1.0). Still, the result indicated a positive relationship between peak speech modulation rate and eye-movement sampling rate. Note that we report this analysis despite its low statistical power (with only three languages), to motivate future investigations of the relationship between speech and reading rates.</p><p hwp:id="p-26">The meta-analysis (i) replicates the results obtained for German in the first section, (ii) shows that eye-movement sampling frequencies of most languages fall into the range of the peaks of speech amplitude modulation spectra (4.3-5.5Hz)<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-8" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-8" hwp:rel-id="ref-6">6</xref></sup> determined in independent research, and (iii) shows a systematic modulation of reading rates by the perceptual difficulty of orthographic systems. We found similar average sampling rates for languages of comparable orthographic transparency levels (e.g., German and Dutch) and highest reading rates (â¼5Hz) in transparent (i.e., relatively easy-to-process) writing systems. Our tentative proposal that the linguistic processing systems underlying speech production and comprehension provide the temporal frame that âdrivesâ the oculomotor machinery during reading would predict a direct relationship between the rate of speech production and the sampling rate of reading. In the present meta-analysis, this proposal could only be tested cross-linguistically and using highly aggregated data, and we found no robust support for this proposal. However, these analyses included small sample sizes as they were limited by the number of languages included. To investigate this proposal in more detail, we next conducted two new studies that examine the existence of associations between speech and reading rates at a subject-by-subject level.</p></sec></sec><sec id="s3" hwp:id="sec-9"><title hwp:id="title-12">Results: Association of individual differences in speech and reading rates</title><p hwp:id="p-27">We tested the correlation between peaks in the speech modulation spectra of individual speakers and their eye-movement sampling rates during reading in two experiments. First, we tested 48 learners of German (Study 3), as we expected to observe higher variabilities in both measures in non-native language learners than in native speakers<sup><xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref></sup> and a more direct relationship between speaking and reading (similar to letter-by-letter reading in beginning readers<sup><xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref></sup>). We recorded eye movements from each participant while reading German sentences (implemented analogously to the reading task in Study 1) and a speech sample based on a âsmall talk interviewâ (22 questions, on average 18 minutes of speech per participant, range: 6 to 28min). Also, we controlled for individual differences in reading proficiency statistically by adding a standard measure of reading skill<sup><xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>,<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref></sup> (see <italic toggle="yes">Methods</italic>). The eye-movement sampling frequency was estimated based on the fixation durations (i.e., as in Experiment 1), and the speech modulation spectrum was examined analogous to previous reports<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-9" hwp:rel-id="ref-6">6</xref></sup>.</p><p hwp:id="p-28"><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4a</xref> shows the average speech modulation spectrum across participants (black line), with a peak at 4.2Hz, and individual spectra from all participants (gray lines). As expected for language learners, the peak of the spectrum was below that of native speakers (compare <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Figure 1</xref>) and on the lower border of the cross-linguistic range of mean speech rates<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-9" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-10" hwp:rel-id="ref-6">6</xref></sup> (compare <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Figure 2</xref>). Nevertheless, all participants produced peaks between 3Hz and 6Hz (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4b</xref>), i.e., within the previously reported<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-10" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-11" hwp:rel-id="ref-6">6</xref></sup> range of the standard deviations around the language-specific mean peaks (dotted line in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4c</xref>). The peaks of individual speech modulation spectra and eye movement sampling frequencies were in a comparable range (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Figure 4b</xref>; confirmed by a significant equivalence test<sup><xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref></sup>: <italic toggle="yes">t</italic>(47)=-2.0, <italic toggle="yes">p</italic>=.03) and positively correlated (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Figure 4c</xref>; Effect estimate/Est=0.32; SE=0.15; <italic toggle="yes">t</italic>=2.1; <italic toggle="yes">p</italic>=0.04). Note that this correlation effect was estimated while controlling for individual differences in reading proficiency (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Figure 4d</xref>; Est=0.032; SE=0.016; <italic toggle="yes">t</italic>=2.1; <italic toggle="yes">p</italic>=0.04) by calculating a linear model that estimates the individual eye-movement sampling rate with speech modulation rate as predictor.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-29">Relationship of speech and reading rates in non-native German speakers. (a) Speech modulation spectrum from 48 non-native speakers of German. Y-axis: speech modulation index<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-12" hwp:rel-id="ref-6">6</xref></sup>; X-axis: speech modulation rate. For additional comparison, we present the mean range (dashed lines) and standard deviations (dotted lines) of the speech amplitude modulation spectra across languages, which were read out from <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3c</xref> in Ref. <sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-11" hwp:rel-id="ref-5">5</xref></sup> and from Figure 7 in Ref. <sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-13" hwp:rel-id="ref-6">6</xref></sup>. (b) Eye movement sampling frequency in reading and the mean amplitude modulation spectrum in speech, for each participant. Lines connect the reading and speech frequencies of each individual, the violin plots represent the distribution of the data, bars represent the standard error of the mean and circles reflect the mean. (c) Positive correlation of the individual peaks of the speech modulation spectrum (x-axis), reflecting each participantâs speech rate, with the eye-movement frequency (y-axis) from the same participants. (d) Correlation between the eye-movement frequency (y-axis) and a paper-pencil based reading score (x-axis) reflecting a positive association of the eye-movement sampling rate and reading performance. Note that in (c) and (d) we present the individual sampling frequencies corrected for either reading skill and speech frequency, respectively, based on predictions from the fitted linear regression models used for statistical analysis.</p></caption><graphic xlink:href="391896v3_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-30">In a second, pre-registered study (Study 4), we assessed the relationship between speech modulation spectrum peaks and eye movement sampling frequencies in a group of 86 native speakers of German (see Method; preregistration: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/mjhkz" ext-link-type="uri" xlink:href="https://osf.io/mjhkz" hwp:id="ext-link-2">https://osf.io/mjhkz</ext-link>). We replicated the finding that the peaks of the speech modulation spectra and eye movement sampling frequencies were in the same range (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5a</xref>; equivalence tests for left and right eye: tâs&gt;3.9, pâs&lt;.001). The pre-registered correlation analysis showed a small positive, albeit not significant relationship between eye-movement sampling and speech modulation rates (Est.=0.07; SE=0.04; t=1.8; p=0.08). For further exploration (i.e., non-registered post-hoc analysis), we separately investigated and compared four subgroups created by a 2Ã2 combination of reading speed and reading accuracy. Specifically, we implemented a median split based on reading speed measured with a standardized German reading test (adult version of the SLS<sup><xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">35</xref></sup>; fast vs. slow: Median: 78% vs. 60%) and, orthogonal to this, divided the sample based on their sentence comprehension accuracies in the eye-tracking experiment (errors present vs. absent: Median 0% vs. 15%). Only readers with the lowest skill level, i.e., slow and low comprehension performance, showed a robust positive association (N=21; <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5b</xref>, bottom right; Est.=0.33; SE=0.10; <italic toggle="yes">t</italic>=3.4; <italic toggle="yes">p</italic>=0.002). None of the other groups showed a significant correlation, resulting in a reliable interaction effect (Est.=-0.05; SE=0.02; <italic toggle="yes">t</italic>=2.8; <italic toggle="yes">p</italic>=0.005) which was also present when the percentage of regressions, skipping, and single fixation probabilities (see <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>) were added as covariates to the model. Note that the low-reading skill group had a lower reading speed than the âslow onlyâ group that produced no errors (Est.=0.08; SE=0.04; <italic toggle="yes">t</italic>=2.0; <italic toggle="yes">p</italic>=0.049), but still had a substantially higher reading speed compared to the non-native readers from Study 3 (Est.=0.57; SE=0.04; <italic toggle="yes">t</italic>=14.42; <italic toggle="yes">p</italic>&lt;.001; for general eye-movement characteristics of both experiments see <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>). In sum, we replicated the results of Study 3 selectively by demonstrating that the correlation between reading and speaking rates is limited to (here native German) speakers with low reading skills.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><p hwp:id="p-31">Relationship of speech and reading rates in 86 native German speakers. (a) Eye movement sampling frequency measured during reading for the left and right eyes (left and center columns) and the mean amplitude modulation spectrum of samples of spoken speech (right column). Grey dots represent individual data points from all participants; lines connect the reading and speech frequencies of each participant. The violin plots represent the distribution of the data, filled bars represent the standard error of the mean, and circles reflect the mean. (b) The correlation between the speech modulation spectrum (x-axis) with the eye-movement sampling frequency (y-axis). The four panels represent performance subgroups depending on reading speed (slow vs. fast; median split) and whether they produced errors in an independent standardized reading test (see Methods for details).</p></caption><graphic xlink:href="391896v3_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-6"><p hwp:id="p-32">Reading speed, reading comprehension, and basic eye tracking measures (fixation durations, skipping probability, single fixation cases, and percentage of regressions) for Study 3 and Study 4. For Study 4, data are presented separately for the four performance-based subgroups. All values reflect means and standard deviations.</p></caption><graphic xlink:href="391896v3_tbl1" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap></sec><sec id="s4" hwp:id="sec-10"><title hwp:id="title-13">Discussion</title><p hwp:id="p-33">This (to our knowledge first) frequency-based investigation of eye-movements during reading shows that reading operates in a generally comparable frequency domain as the production and perception of natural speech. We first reproduced in a frequency-domain analysis previous insights based on fixation duration measures<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-6" hwp:rel-id="ref-17">17</xref>â<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-4" hwp:rel-id="ref-19">19</xref></sup>, i.e., that eye-movements sample text with a higher rate than during comparable, cognitively less challenging non-linguistic tasks. More importantly, we demonstrate that the sampling frequency of reading lies within the range of previously observed speech rates for one language, German. Next, by integrating across languages data from 124 empirical studies, we show that eye-movement sampling varies between â¼3.9Hz and â¼5.2Hz, indicating a higher variability than previously assumed. While it was generally believed average fixation durations are similar even for very distinct orthographies like Chinese and English (Rayner<sup><xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-3" hwp:rel-id="ref-12">12</xref></sup>, p.1461), our meta-analytic results that show significantly higher sampling rates for alphabetic compared to character-based writing systems. However, average speech rates have been shown to vary more narrowly around 5Hz across languages (i.e., Ref.<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-12" hwp:rel-id="ref-5">5</xref></sup>: 4.3-5.4Hz; Ref.<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-14" hwp:rel-id="ref-6">6</xref></sup>: 4.3-5.5Hz), a range that would exclude the lower frequencies we observed for reading. Our meta-analytic findings indicate that this might result from differences in the complexity of the underlying orthographies (e.g., character vs. alphabetic), such that more computationally âdifficultâ orthographies might slow down reading relative to highly transparent alphabetic orthographies.</p><p hwp:id="p-34">Subsequently, we demonstrate in two independent empirical studies that second language learners (of German) read in a lower frequency range than native readers (â¼4.3Hz vs. â¼4.7Hz) and that only language learners and low skilled native readers show a positive correlation between individual reading and speech rates. Combined, these results suggest that reading, i.e., an internally controlled visual-perceptual process involving sophisticated oculomotor programming, is remarkably well temporally aligned with the rate at which spoken language is produced (and perceived). We tentatively suggest that this observed association between speech and reading supports the existence of fundamental perceptual principles underlying the temporal structure of linguistic information processing, irrespective of modality<sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref></sup>.</p><p hwp:id="p-35">Text is a temporally stable visual stimulus. However, our eye-movements impose temporal structure onto the linguistic input when sequentially sampling a text. The reading process - including the oculomotor programs - thus serves as an interface between a stable external percept and linguistic processing systems optimized for analyzing sequential speech input. The observation of faster sampling rates during reading as compared to parsing non-linguistic letter strings (Study 1) indicates that sampling rates are not exclusively driven by the physical layout of the stimulus or by the cognitive effort of processing the stimulus (in which case they should have been slower). We tentatively propose that neural processors dedicated to the linguistic analysis of speech impose their preferred timing onto the process of reading. Evidence for the principled possibility of such internally driven entrainment of reading comes from the observation that manipulating the speed of âinner speechâ during reading has a causal effect on reading speed<sup><xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>â<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref></sup>.</p><p hwp:id="p-36">Our meta-analysis demonstrates that the sampling rate of reading varies between languages â but falls within the range of speech rates identified in cross-linguistic studies<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-13" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-15" hwp:rel-id="ref-6">6</xref></sup>. The meta-analysis also shows higher sampling rates for transparent vs. opaque orthographies, which converges with transparency effects within languages<sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">29</xref>,<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref></sup> and cross-linguistic studies investigating reading development<sup><xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">26</xref>,<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>,<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref></sup>. Direct associations between reading and speech rates could not be established in the meta-analysis given the small number of languages for which all necessary parameters were available. Empirical Studies 3 and 4 show this relationship on a subject-by-subject level, however only in less-skilled readers. This suggests that increasing reading expertise makes the tight control of reading by linguistic processors in the brain obsolete.</p><p hwp:id="p-37">The differential coupling of speech and reading rates in low-skilled but not high-skilled readers may also result from other phenomena well-established in reading research, like word skipping, para-foveal preprocessing, and re-fixations. Reading is not merely a sequence of word-to-word fixations. From time-to-time, we skip words as a result of parafoveal pre-processing<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref></sup>, which describes visual word recognition based on low acuity visual information from parafoveal regions of the retina. Also, words are sometimes fixated multiple times, e.g., to correct perceptual errors after suboptimal landing at the beginning or end of a word<sup><xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">14</xref>,<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-3" hwp:rel-id="ref-18">18</xref>,<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref></sup> or when semantic inconsistencies must be resolved by re-reading<sup><xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>,<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref></sup>. We<sup><xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-3" hwp:rel-id="ref-14">14</xref></sup> and others<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-4" hwp:rel-id="ref-10">10</xref></sup> have shown that overall probabilities for word skipping and multiple fixations on a word are comparable (â¼20%) when reading the sentence materials used here. However, low-skilled readers show lower skipping rates (reflecting reduced parafoveal preprocessing<sup><xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>â<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref></sup>) and more re-fixations on the same word<sup><xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref></sup>. Thus, readers with lower skills focus more on fixated words and their components (letters, syllables)<sup><xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref></sup>, indicating a greater alignment between the phonological properties of the words and the eye-movements they elicit during reading<sup><xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">33</xref></sup>, suggesting that low-skilled readers sample written text with a temporal resolution close to the speech processing rate. In contrast, the faster reading rates of fluent readers indicate that they utilize the static nature of text better by processing the fixated word and following words (based on parafoveal vision) within one âsampleâ.</p><p hwp:id="p-38">The specific characteristics of fixation behavior and text presentation during reading can also provide context for another intriguing phenomenon, i.e., the significantly lower sampling rates in character-based than alphabetic writing systems (while overall reading times for sentences with the same content are comparable between the two writing systems<sup><xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-5" hwp:rel-id="ref-11">11</xref></sup>). Here, fewer fixations per sentence are needed to sample the entire stimulus, while the increased perceptual complexity and information density<sup><xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-6" hwp:rel-id="ref-11">11</xref></sup> lead to longer fixation durations, relative to alphabetic languages. In the non-linguistic control task, participants were presented with stimuli consisting of many repetitions of the same letter. In this case, information density and perceptual complexity are low compared to all real-language stimuli. Nevertheless, we observed longer fixation durations in the z-string scanning task, which may indicate the presence of qualitatively different cognitive processes compared to reading.</p><p hwp:id="p-39">The frequency representation of reading-related eye-tracking data that we advance here can be construed as ânothing butâ a transformation of fixation and saccade duration data. This transformation also comes at the cost of zooming out to a âmeso-levelâ representation of the data<sup><xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">1</xref></sup>, at which we rely on aggregated data (i.e., only one data point per participant), which is against the trend in eye-movement research of focusing on investigating single words and using regression methods for detailed analysis of, e.g., the influence of word characteristics<sup><xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref></sup>. Still, the frequency perspective proposed here provides a novel perspective onto component processes of reading as an interface between linguistic and orthographic processing. This new approach to reading research opens up several interesting new research questions. For example, it becomes possible to compare reading behavior more directly to evidence from other measurement modalities, such as oscillatory brain activation data<sup><xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>,<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref></sup>, and to other cognitive-psychological domains, such as attention<sup><xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-3" hwp:rel-id="ref-15">15</xref>,<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref></sup>, which typically do not have the advantage of exact duration measurements for different events of interest (e.g., during covert attention). Maybe most importantly, the frequency perspective on reading offers direct links to several neurodynamic phenomena in speech perception<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-14" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-16" hwp:rel-id="ref-6">6</xref></sup>, including the observation that dyslexic children<sup><xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref>,<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref></sup> and adults<sup><xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref></sup> show altered cortical tracking of speech signals in the oscillatory domain.</p><p hwp:id="p-40">In conclusion, we show that during reading, our eyes âsampleâ written text in the same frequency range in which speech is produced and perceived, which suggests that extracting information from linguistic stimuli follows a similar temporal structure in time irrespective of modality. A plausible mechanism is to assume that linguistic processing has a preferred cortical rate of information uptake and thus acts as an internal temporal driver for eye-movements elicited during reading. Thus, eye-movements in reading are utilized as a temporal interface between a stable physical stimulus â written text â and brain systems that have evolved to process signals whose temporal structure is constrained by the characteristics of our vocal tract<sup><xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-3" hwp:rel-id="ref-1">1</xref></sup>. However, our empirical data also demonstrate that a direct coupling between speech and reading rates is only present in persons with low reading skills, which calls for future work to clarify the mediating role of reading expertise for the temporal relationship between speech processing and reading rates. We suggest that the novel frequency perspective on reading adopted here opens up new research paths, such as understanding slow or impaired reading, second language learning, or more directly investigating the commonalities and differences between reading and other cognitive processes.</p></sec><sec id="s5" hwp:id="sec-11"><title hwp:id="title-14">Methods</title><sec id="s5a" hwp:id="sec-12"><title hwp:id="title-15">Study 1, 3 and 4, Participants</title><p hwp:id="p-41">Fifty (13 male; 18â47years old; M=24years; students at University of Salzburg) native speakers of German participated in Study 1, forty-nine (13 male; 18â74years old; M=24 years) non-native German speakers participated in Study 3, and eighty-six (36male; 18â53years old; M=25years; five had to be excluded based on preregistered outlier correction boundaries for both speech and reading rates; +-3 standard deviations) German speakers participated in Study 4 after giving informed consent according to procedures approved by the respective local ethics committee. For Study 1, see our original publication of this dataset<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-7" hwp:rel-id="ref-17">17</xref></sup> for more details. Note that relative to the previously published report, one participant was added. For Study 3, participants with varying mother tongues (Arabic, Azerbaijani, Bulgarian, Chinese, English, Farsi, French, Georgian, Indonesian, Italian, Japanese, Persian, Russian, Serbo-Croatian, Spanish, Turkish, Ukrainian, Hungarian, Urdu, and Uzbek) and, for Study 4, native German participants were recruited on the campus of Goethe University Frankfurt as part of a larger study. Also note that six participants from Study 3 became literate without the acquisition of an alphabetic script. The power estimation for the fourth and final study resulted at sample size of 90 while assuming the effect size from study 3 and a power of .9 (see preregistration for more details: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/mjhkz" ext-link-type="uri" xlink:href="https://osf.io/mjhkz" hwp:id="ext-link-3">https://osf.io/mjhkz</ext-link>).</p></sec><sec id="s5b" hwp:id="sec-13"><title hwp:id="title-16">Procedure Study 1</title><p hwp:id="p-42">Movements of the right eye were tracked with a sampling rate of 1,000Hz (Eyelink 1000, tower mount system; SR-Research, Ontario, Canada). We used a forehead and chin rest to fixate the head of participants at a distance of 60cm from a 21â CRT screen. In the reading task, we used the Potsdam Sentence Corpus<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-5" hwp:rel-id="ref-10">10</xref></sup> (PSC) which consists of 144 sentences and a total of 1,138 words. Participants were instructed to read silently for comprehension, which was controlled by simple comprehension questions after 38 of the 144 sentences.</p><p hwp:id="p-43">As a non-linguistic control task, participants performed a z-string scanning task using stimuli in which all letters of the sentence corpus were replaced by the letter z (preserving letter case, punctuation, and word boundaries. Participants were instructed to visually scan the meaningless z-strings as if they were reading; for obvious reasons, no comprehension questions were administered in this condition. Z-string scanning has been used as control task in previous studies<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-8" hwp:rel-id="ref-17">17</xref>â<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref></sup>. While it is difficult to find a reasonable control task for reading (see, e.g., Discussion in Ref.<sup><xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-4" hwp:rel-id="ref-18">18</xref></sup>), z-string scanning proved to be interesting because participants produce similar scan path patterns (i.e., similar number of fixations) as when reading<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-9" hwp:rel-id="ref-17">17</xref>â<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-5" hwp:rel-id="ref-19">19</xref></sup>.</p><p hwp:id="p-44">Interestingly, while z-string scanning produced longer mean fixation durations than reading, the pupil response indicated higher cognitive effort in reading, in the dataset used here<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-10" hwp:rel-id="ref-17">17</xref></sup>. We take this dissociation between cognitive effort and reading time as evidence for the operation of reading-specific cognitive processes that go beyond mere attentional processes.</p><p hwp:id="p-45">In each task, a 9-point standard calibration was performed before the 10 practice trials, before the experimental trials, and after a break halfway through the experiment. A calibration was considered accurate when the mean error was below .5Â° of visual angle. Visual stimuli were presented in black letters (mono-spaced, bold Courier New font; 14 pt., width â¼.3Â°) on white background with a 1,024 Ã 768 pixel resolution and a refresh rate of 120 Hz, using Experiment Builder software (SR Research, Ontario, Canada). In both tasks, a trial started when an eye-fixation was found at a dot presented 100 pixels from the left margin of the monitor, at the horizontal level of the fixation cross. For this fixation check, real-time analysis of eye-tracking data was used to present the sentence only when a fixation of at least 100ms was identified on the position of the dot. If no fixation was registered on the dot for 10 seconds, a re-calibration procedure was initiated. Following the fixation check, the stimulus (i.e., sentence or z-string) appeared, with the center of the first word presented at the position of the fixation dot. As a consequence, participants always fixated the center of the first word of the sentence first. Stimulus presentation was terminated when participants fixated an X in the lower right corner of the screen after the sentence was read. As noted, in about 25% of sentences, the presentation was followed by a comprehension question to assure that participants processed sentences semantically. This procedure was practiced in ten trials prior to the main experiment.</p></sec><sec id="s5c" hwp:id="sec-14"><title hwp:id="title-17">Procedure Study 3 and Study 4</title><p hwp:id="p-46">Eye movement measurements during reading were acquired using the same stimulus materials and experimental procedures as in Study 1, with three exceptions: We used a desktop-mount eye tracker, a horizontal 3-point calibration procedure, and we did not implement the z-string scanning task. All other parameters were unchanged. For Study 4, we not only measured the right eye but also the left eye (i.e., binocular measurement). To acquire a speech sample from each participant, we conducted a brief interview in German involving 22 questions about, e.g., last weekendâs activities (see the full list of questions in the Supplementary Methods). Speech was recorded with the Audacity software (Version 2.1.3; <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.audacityteam.org/" ext-link-type="uri" xlink:href="https://www.audacityteam.org/" hwp:id="ext-link-4">https://www.audacityteam.org/</ext-link>) on a standard computer.</p></sec></sec><sec id="s6" hwp:id="sec-15"><title hwp:id="title-18">Data analysis</title><sec id="s6a" hwp:id="sec-16"><title hwp:id="title-19">Fixation durations</title><p hwp:id="p-47">The first word of each sentence was excluded from analyses, since the first word is known to be contaminated by stimulus onset effects. A total of 994 words were analyzed per subject. For each participant, all fixation durations from all analyzed words were extracted. Words with fixation durations shorter than 60ms and longer than 1,000ms and saccade durations longer than 80ms were removed from the analysis (3.1% of the data) since they likely reflect machine error. On the basis of the remaining fixation durations, each participantâs individual mean fixation duration was calculated, separately for the reading and scanning tasks. To account for the well-known fact that eye fixation data have an ex-Gaussian distributions (see <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Figure 1c</xref>) data were log-transformed resulting in a normal distribution (Kolmogorov-Smirnov test not significant; <italic toggle="yes">D</italic> &lt; .14; <italic toggle="yes">p</italic> &gt; .7).</p></sec><sec id="s6b" hwp:id="sec-17"><title hwp:id="title-20">Estimation of the sampling frequency</title><p hwp:id="p-48">To estimate the sampling frequency of eye movements in reading, a repetitive event needed to be identified. We defined the time between the onset of a saccade and the onset of the following saccade as the <italic toggle="yes">sampling period</italic>, which can be transformed into a frequency value. Note that we used the EyeLink eye-trackerâs built-in saccade detection algorithm, which in a recent comparative evaluation showed the best detection rates for saccade onsets compared to all other algorithms<sup><xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref></sup>. The distribution of the sampling period is ex-Gaussian, for both reading and z-string scanning (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-12" hwp:rel-id="F1">Figure 1c</xref>). Ex-Gaussian distributions are a convolution of a normal distribution and an exponential distribution reflecting the rightward skew. As <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-13" hwp:rel-id="F1">Figure 1c</xref> shows, the central tendency is best represented by the mode, so that all subsequent calculations were based on the participant-specific mode of the sampling period (which we denote <italic toggle="yes">t</italic>). These subject-specific representations of the predominant sampling period were transformed to an individual eye movement sampling frequency (<italic toggle="yes">f</italic> = 1 / <italic toggle="yes">t</italic>).</p></sec><sec id="s6c" hwp:id="sec-18"><title hwp:id="title-21">Power spectrum</title><p hwp:id="p-49">We, secondly, performed a canonical frequency analysis by estimating a power spectrum for reading and scanning in Study 1. For <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-14" hwp:rel-id="F1">Figure 1e</xref>, we estimated the power spectrum based on a time series starting with the first saccade of the first participant and ending with the last fixation of the final participant for each task. For <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-15" hwp:rel-id="F1">Figure 1f</xref>, the time series was cut into participant-specific time series, so that individual peaks could be recovered for each participant for each task. The time series was implemented as a sparse sequence of zeros and ones (resolution: 1,000 entries per second), set to one at time points at which a saccade was initiated, and zero otherwise. Subsequently, a Fast Fourier Transform was used to estimate a power spectrum (power spectral density; <italic toggle="yes">psd_welch</italic> function from MNE-Python<sup><xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref></sup> ; 0-100Hz, length of the FFT used = 4096 samples) for each event time course separately.</p></sec><sec id="s6d" hwp:id="sec-19"><title hwp:id="title-22">Speech amplitude modulation spectrum in Study 3 and 4</title><p hwp:id="p-50">In a first step, all non-participant audio signals were removed from the speech samples (i.e., interviewer questions and pauses before answers). To obtain the amplitude modulation spectrum we adapted the AM_FM_Spectra script<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-17" hwp:rel-id="ref-6">6</xref></sup> (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/LeoVarnet/AM_FM_Spectra" ext-link-type="uri" xlink:href="https://github.com/LeoVarnet/AM_FM_Spectra" hwp:id="ext-link-5">https://github.com/LeoVarnet/AM_FM_Spectra</ext-link>). The first adaptation divided the recording of each participant into speech segments of 10s length, resulting in a mean number of 110 segments per participants (range 35 to 167). In Study 4, we used 20s segments to increase the efficiency of the analysis (mean number of segments: 55; range 15 to 81). The second adaptation was an increase in the resolution of the amplitude modulation spectrum by decreasing the widths of the modulation filters from 3 to 10 per octave. After the speech amplitude modulation was estimated for each 10s speech segment, we retrieved the frequency at the peak of the modulation spectrum. Thereafter, we removed outliers by first eliminating unrealistic values lower than 2 and higher than 10 Hz, and then removed all values larger and smaller than two standard deviations from the mean. This procedure removed 3% of the data. Finally, we estimated the mean across all segments for each participant. Here we found that one participant, in both studies, had a mean amplitude modulation spectrum larger than three standard deviations from the mean of the sample; this participant was excluded from the analysis.</p></sec></sec><sec id="s7" hwp:id="sec-20"><title hwp:id="title-23">Meta-analysis</title><p hwp:id="p-51">We included empirical studies that report eye-tracking results from natural reading tasks, published between 2006 and 2016. These studies were identified by the search term <italic toggle="yes">eye movement in</italic> â<italic toggle="yes">natural reading</italic>â <italic toggle="yes">or</italic> â<italic toggle="yes">sentence reading</italic>â <italic toggle="yes">or</italic> â<italic toggle="yes">text reading</italic>â in the <italic toggle="yes">PubMed</italic> (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.ncbi.nlm.nih.gov/pubmed" ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed" hwp:id="ext-link-6">https://www.ncbi.nlm.nih.gov/pubmed</ext-link>) and <italic toggle="yes">PsychInfo</italic> (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://health.ebsco.com/products/psycinfo" ext-link-type="uri" xlink:href="https://health.ebsco.com/products/psycinfo" hwp:id="ext-link-7">https://health.ebsco.com/products/psycinfo</ext-link>) databases. Additionally, 10 studies were manually identified (e.g., on the basis of reference lists in the identified papers). From the resulting sample of 124 articles we extracted 1,420 fixation durations, including mean fixation durations (all fixations on a word combined; 10% of the meta-analytic dataset), first fixation durations (duration of the first fixation on a word, which is most often reported in eye-tracking studies; 67% of the meta-analytic dataset), and single fixation durations (fixation duration in case a word was fixated only once, which is the predominant case for normal readers<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-6" hwp:rel-id="ref-10">10</xref></sup>; 23% of the meta-analytic dataset). A full list of all included studies can be found in the Supplementary Note. Note that the results of the above-reported experiment (Study 1) and its previous analysis<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-11" hwp:rel-id="ref-17">17</xref></sup> were not included. This meta analytic dataset encompassed 14 different languages, with a range from one (Arabic, Italian, and Polish) to 65 (English) retrieved papers. Consistent with a general bias towards English in reading research<sup><xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref></sup>, 68% of fixation durations in our dataset were from English.</p><sec id="s7a" hwp:id="sec-21"><title hwp:id="title-24">Frequency estimation</title><p hwp:id="p-52">In order to estimate the predominant sampling frequency, per published study, we have to take into account, once more, the ex-Gaussian distribution of fixation duration data. Following the general trend in the eye movement reading literature, most studies reported only mean fixation durations (with the exception of Ref.<sup><xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref></sup>, for which in addition the fitted ex-Gaussion paramaters were reported). For the purposes of the present meta-analysis, we developed a transformation function that allowed us to estimate the mode from the mean fixation durations reported in the original publications. This transformation function was then applied to transform mean fixation durations extracted from the published original studies into the mode. In a final step, sampling period was determined as mode fixation duration plus mode saccade duration (as estimated from study 1) and then converted into a frequency value.</p><p hwp:id="p-53">In brief (for details see Supplementary Methods), the development of this function involved (i) fitting ex-Gaussian distributions to the empirical distributions of fixation durations in 29 original datasets to which we had access (and which were not included into the meta-analysis), and (ii) retrieving distributional parameters for each fitted distribution (specifically: Î¼, the mean of the normally distributed component; Ï, its standard deviation; Ï, the parameter reflecting the rightward skew, representing the contribution of the exponential distribution). This allowed us to (iii) apply a regression analysis to predict the mode (dependent variable) from the mean fixation duration (predictor variable). <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6a</xref> presents the final generalized mean-to-mode transformation function, applied to all possible fixation durations in the range covered by the meta-analysis. <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6b</xref> shows how well the modes of our 29 datasets can be recovered by this function: Despite some unsystematic noise, the numeric transformation was nearly perfect (i.e., beta = .95; SE = .23; <italic toggle="yes">t</italic>(28) = 4.1). We then used the transformation function to estimate the respective modes from the 1,420 mean fixation durations of the meta-analysis dataset. To obtain the sampling period <italic toggle="yes">t</italic> (i.e., the interval from the onset of a saccade until the end of the following fixation; see also Study 1, above), a saccade duration estimate of 29ms (i.e., the mode of saccade durations from the reading dataset used in the first experiment) was added to each of the mode fixation duration. This is feasible since saccade durations do not differ much between persons during reading (see, e.g., Ref.<sup><xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref></sup>: range 20-39ms, mean: 29ms). Finally, the sampling period values (<italic toggle="yes">t</italic>) were transformed into frequency values (<italic toggle="yes">f</italic> = 1 / <italic toggle="yes">t</italic>).</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-7"><p hwp:id="p-54">Transformation function for converting mean fixation durations into simulated mode values. The function was established using ex-Gaussian estimations on 29 empirical datasets containing fixation durations. For details see Supplementary Methods. (a) Performance of the final mean-to-mode transformation function (blue line) demonstrated here for all 248 possible fixation durations, i.e., for each millisecond within the range of fixation durations included in the meta-analysis (149 â 397 ms; x-axis: empirical means). (b) Performance of the final mean-to-mode transformation function, as demonstrated by the relationship between empirically measured (x-axis) and simulated (y-axis) modes from the 29 datasets used for establishing the transformation function.</p></caption><graphic xlink:href="391896v3_fig6" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s7b" hwp:id="sec-22"><title hwp:id="title-25">Writing system comparisons</title><p hwp:id="p-55">In order to explore whether or not the sampling frequency of reading is influenced by global characteristics of writing systems and languages, we implemented four tests. First, reading Chinese (205 fixation duration data points) was compared to all alphabetic writing systems (1,215 fixation durations). Note that the Korean (with an alphabetic syllabary orthography<sup><xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref></sup>) and Japanese (using a mixture of Kana and Kanji) studies in the meta-analysis could not be clearly assigned to the character or alphabet categories and therefore were not included into this contrast. Second, among the alphabetic scripts we examined how the differences in transparency/opaqueness of the letter-to-sound relationship influence reading rates by a continuous predictor representing the number of grapheme-to-phoneme rules as defined by computationally implemented dual-route models<sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-3" hwp:rel-id="ref-25">25</xref></sup>. A low number of grapheme-to-phoneme rules reflects a high transparency, meaning that letters more consistently represent only one speech sound. For example Italian, Dutch, and German are considered transparent orthographies, with 59, 104, and 130 rules, respectively <sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-4" hwp:rel-id="ref-25">25</xref></sup>). English and French, in contrast, are typically considered as in-transparent (opaque) with 226 and 340 rules, respectively, because letters map to multiple speech sounds on a regular basis. Third, we investigated the cross-linguistic relationship between variance in the peak modulation spectra from speech and mean sampling frequencies in reading. To this end, we retrieved the modulation spectra for Chinese, Dutch, English, French, Japanese, and Spanish from <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Figure 3c</xref> in Ref.<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-15" hwp:rel-id="ref-5">5</xref></sup> and from Figure 7 in Ref.<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-18" hwp:rel-id="ref-6">6</xref></sup>. The modulation spectra varied from 4.3 Hz in English to 5.4 Hz in Dutch. Finally, in the fourth test, we investigated the relationship between eye movement sampling in reading and the information density of a language. This parameter indicates how dense a language codes meaning in texts<sup><xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-3" hwp:rel-id="ref-27">27</xref></sup>. The density is coded from 0 to 1 and could be retrieved for a subgroup of languages in the present meta-analysis dataset (i.e., Chinese, English, French, German, Italian, Japanese, and Spanish) from Ref.<sup><xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-4" hwp:rel-id="ref-27">27</xref></sup>. Density varied from dense languages like Chinese (0.94) to less dense languages like Japanese (0.49).</p><p hwp:id="p-56">All four effects were analyzed using linear mixed models (LMM<sup><xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">62</xref></sup>). In addition to the parameters of interest, we accounted for experimental settings (experiment vs. corpus-based studies), for the different eye trackers used (which may also imply use of different saccade detection algorithms), and for different fixation measures reported (mean, single, or first fixation duration) by introducing these parameters into the LMMs as fixed effects. Also, for the modulation spectrum and information density comparisons we added a factor contrasting character-based (i.e., Chinese) vs. alphabetic writing systems, to account for perceptual difficulties, and for all four LMMs we estimated the random effect on the intercept of study, to take into account unspecific differences between studies. t-values larger than 2 were interpreted as significant<sup><xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">63</xref></sup>.</p></sec></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-26">Acknowledgments</title><p hwp:id="p-57">The research leading to these results has received funding from the European Communityâs Seventh Framework Programme (FP7/2013) under grant agreement nÂ° 617891 awarded to CJF and from the European Communityâs Horizon 2020 Programme under grant agreement nÂ° 707932 awarded to BG. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. We thank Jutta MÃ¼ller for helpful comments on a previous version of the manuscript.</p></ack><sec id="s8" hwp:id="sec-23"><title hwp:id="title-27">Author contributions</title><p hwp:id="p-58">B.G., D.P. and C.J.F. designed research; B.G. and S.H. performed Study 1; B.G. and J.G. performed Study 2 (Meta-analysis); B.G. and K.G. performed Study 3 and 4; B.G., A. T. and J.S. analyzed data; and B.G. and C.J.F. wrote the paper. All authors gave comments on the paper during the process.</p></sec><sec id="s9" hwp:id="sec-24"><title hwp:id="title-28">Competing interests</title><p hwp:id="p-59">The authors declare no competing interests.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-29">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2 xref-ref-1-3"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Poeppel D."><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Assaneo M. F."><surname>Assaneo</surname>, <given-names>M. F.</given-names></string-name> <article-title hwp:id="article-title-2">Speech rhythms and their neural foundations</article-title>. <source hwp:id="source-1">Nat. Rev. Neurosci</source>. <volume>21</volume>, <fpage>322</fpage>â<lpage>334</lpage> (<year>2020</year>).</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Ding N."><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Melloni L."><surname>Melloni</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang H."><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tian X."><surname>Tian</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poeppel D."><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-3">Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech</article-title>. <source hwp:id="source-2">Nat. Neurosci</source>. <volume>19</volume>, <fpage>158</fpage>â<lpage>164</lpage> (<year>2016</year>).</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Luo H."><surname>Luo</surname>, <given-names>H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poeppel D."><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-4">Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex</article-title>. <source hwp:id="source-3">Neuron</source> <volume>54</volume>, <fpage>1001</fpage>â<lpage>1010</lpage> (<year>2007</year>).</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Martin A. E."><surname>Martin</surname>, <given-names>A. E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Doumas L. A. A."><surname>Doumas</surname>, <given-names>L. A. A.</given-names></string-name> <article-title hwp:id="article-title-5">A mechanism for the cortical computation of hierarchical linguistic structure</article-title>. <source hwp:id="source-4">PLOS Biol</source>. <volume>15</volume>, <fpage>e2000663</fpage> (<year>2017</year>).</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4 xref-ref-5-5 xref-ref-5-6 xref-ref-5-7 xref-ref-5-8 xref-ref-5-9 xref-ref-5-10 xref-ref-5-11 xref-ref-5-12 xref-ref-5-13 xref-ref-5-14 xref-ref-5-15"><label>5.</label><citation publication-type="other" citation-type="journal" ref:id="391896v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Ding N."><surname>Ding</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-6">Temporal modulations in speech and music</article-title>. <source hwp:id="source-5">Neurosci. Biobehav. Rev</source>. (<year>2017</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.02.011</pub-id>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2 xref-ref-6-3 xref-ref-6-4 xref-ref-6-5 xref-ref-6-6 xref-ref-6-7 xref-ref-6-8 xref-ref-6-9 xref-ref-6-10 xref-ref-6-11 xref-ref-6-12 xref-ref-6-13 xref-ref-6-14 xref-ref-6-15 xref-ref-6-16 xref-ref-6-17 xref-ref-6-18"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Varnet L."><surname>Varnet</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ortiz-Barajas M. C."><surname>Ortiz-Barajas</surname>, <given-names>M. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erra R. G."><surname>Erra</surname>, <given-names>R. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gervain J."><surname>Gervain</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Lorenzi C."><surname>Lorenzi</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-7">A cross-linguistic study of speech modulation spectra</article-title>. <source hwp:id="source-6">J. Acoust. Soc. Am</source>. <volume>142</volume>, <fpage>1976</fpage>â<lpage>1989</lpage> (<year>2017</year>).</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Goswami U."><surname>Goswami</surname>, <given-names>U.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Leong V."><surname>Leong</surname>, <given-names>V.</given-names></string-name> <article-title hwp:id="article-title-8">Speech rhythm and temporal structure: Converging perspectives?</article-title> <source hwp:id="source-7">Lab. Phonol</source>. <volume>4</volume>, <fpage>67</fpage>â<lpage>92</lpage> (<year>2013</year>).</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Greenberg S."><surname>Greenberg</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carvey H."><surname>Carvey</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hitchcock L."><surname>Hitchcock</surname>, <given-names>L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Chang S."><surname>Chang</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-9">Temporal properties of spontaneous speechâa syllable-centric perspective</article-title>. <source hwp:id="source-8">J. Phon</source>. <volume>31</volume>, <fpage>465</fpage>â<lpage>485</lpage> (<year>2003</year>).</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>9.</label><citation publication-type="other" citation-type="journal" ref:id="391896v3.9" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Teng X."><surname>Teng</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tian X."><surname>Tian</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doelling K."><surname>Doelling</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poeppel D."><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-10">Theta band oscillations reflect more than entrainment: behavioral and neural evidence demonstrates an active chunking process</article-title>. <source hwp:id="source-9">Eur. J. Neurosci</source>. 0,.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2 xref-ref-10-3 xref-ref-10-4 xref-ref-10-5 xref-ref-10-6"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Kliegl R."><surname>Kliegl</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grabner E."><surname>Grabner</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rolfs M."><surname>Rolfs</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Engbert R."><surname>Engbert</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-11">Length, frequency, and predictability effects of words on eye movements in reading</article-title>. <source hwp:id="source-10">Eur. J. Cogn. Psychol</source>. <volume>16</volume>, <fpage>262</fpage>â<lpage>284</lpage> (<year>2004</year>).</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3 xref-ref-11-4 xref-ref-11-5 xref-ref-11-6"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Liversedge S. P."><surname>Liversedge</surname>, <given-names>S. P.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-12">Universality in eye movements and reading: A trilingual investigation</article-title>. <source hwp:id="source-11">Cognition</source> <volume>147</volume>, <fpage>1</fpage>â<lpage>20</lpage> (<year>2016</year>).</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2 xref-ref-12-3"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-13">Eye movements and attention in reading, scene perception, and visual search</article-title>. <source hwp:id="source-12">Q. J. Exp. Psychol</source>. <volume>62</volume>, <fpage>1457</fpage>â<lpage>1506</lpage> (<year>2009</year>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Hawelka S."><surname>Hawelka</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gagl B."><surname>Gagl</surname>, <given-names>B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wimmer H."><surname>Wimmer</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-14">A dual-route perspective on eye movements of dyslexic readers</article-title>. <source hwp:id="source-13">Cognition</source> <volume>115</volume>, <fpage>367</fpage>â<lpage>379</lpage> (<year>2010</year>).</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2 xref-ref-14-3"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Gagl B."><surname>Gagl</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawelka S."><surname>Hawelka</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hutzler F."><surname>Hutzler</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-15">A similar correction mechanism in slow and fluent readers after suboptimal landing positions</article-title>. <source hwp:id="source-14">Front. Hum. Neurosci</source>. <volume>8</volume>, <fpage>355</fpage> (<year>2014</year>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2 xref-ref-15-3"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Hogendoorn H."><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-16">Voluntary Saccadic Eye Movements Ride the Attentional Rhythm</article-title>. <source hwp:id="source-15">J. Cogn. Neurosci</source>. <volume>28</volume>, <fpage>1625</fpage>â<lpage>1635</lpage> (<year>2016</year>).</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Brysbaert M."><surname>Brysbaert</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-17">How many words do we read per minute? A review and meta-analysis of reading rate</article-title>. <source hwp:id="source-16">J. Mem. Lang</source>. <volume>109</volume>, <fpage>104047</fpage> (<year>2019</year>).</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3 xref-ref-17-4 xref-ref-17-5 xref-ref-17-6 xref-ref-17-7 xref-ref-17-8 xref-ref-17-9 xref-ref-17-10 xref-ref-17-11"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Gagl B."><surname>Gagl</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawelka S."><surname>Hawelka</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hutzler F."><surname>Hutzler</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-18">Systematic influence of gaze position on pupil size measurement: analysis and correction</article-title>. <source hwp:id="source-17">Behav. Res. Methods</source> <volume>43</volume>, <fpage>1171</fpage>â<lpage>1181</lpage> (<year>2011</year>).</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2 xref-ref-18-3 xref-ref-18-4"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Nuthmann A."><surname>Nuthmann</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Engbert R."><surname>Engbert</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kliegl R."><surname>Kliegl</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-19">The IOVP effect in mindless reading: Experiment and modeling</article-title>. <source hwp:id="source-18">Vision Res</source>. <volume>47</volume>, <fpage>990</fpage>â<lpage>1002</lpage> (<year>2007</year>).</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2 xref-ref-19-3 xref-ref-19-4 xref-ref-19-5"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Vitu F."><surname>Vitu</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="OâRegan J. K."><surname>OâRegan</surname>, <given-names>J. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Inhoff A. W."><surname>Inhoff</surname>, <given-names>A. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Topolski R."><surname>Topolski</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-20">Mindless reading: Eye-movement characteristics are similar in scanning letter strings and reading texts</article-title>. <source hwp:id="source-19">Percept. Psychophys</source>. <volume>57</volume>, <fpage>352</fpage>â<lpage>364</lpage> (<year>1995</year>).</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fischer M."><surname>Fischer</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-21">H. Mindless reading revisited: Eye movements during reading and scanning are different</article-title>. <source hwp:id="source-20">Percept. Psychophys</source>. <volume>58</volume>, <fpage>734</fpage>â<lpage>747</lpage> (<year>1996</year>).</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Staub A."><surname>Staub</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="White S. J."><surname>White</surname>, <given-names>S. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Drieghe D."><surname>Drieghe</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hollway E. C."><surname>Hollway</surname>, <given-names>E. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-22">Distributional Effects of Word Frequency on Eye Fixation Durations</article-title>. <source hwp:id="source-21">J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>36</volume>, <fpage>1280</fpage>â<lpage>1293</lpage> (<year>2010</year>).</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Friedman L."><surname>Friedman</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rigas I."><surname>Rigas</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abdulin E."><surname>Abdulin</surname>, <given-names>E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Komogortsev O. V."><surname>Komogortsev</surname>, <given-names>O. V.</given-names></string-name> <article-title hwp:id="article-title-23">A novel evaluation of two related and two independent algorithms for eye movement classification during reading</article-title>. <source hwp:id="source-22">Behav. Res. Methods</source> <volume>50</volume>, <fpage>1374</fpage>â<lpage>1397</lpage> (<year>2018</year>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Landau A. N."><surname>Landau</surname>, <given-names>A. N.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fries P."><surname>Fries</surname>, <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-24">Attention Samples Stimuli Rhythmically</article-title>. <source hwp:id="source-23">Curr. Biol</source>. <volume>22</volume>, <fpage>1000</fpage>â<lpage>1004</lpage> (<year>2012</year>).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Chang L.-Y."><surname>Chang</surname>, <given-names>L.-Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen Y.-C."><surname>Chen</surname>, <given-names>Y.-C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Perfetti C. A."><surname>Perfetti</surname>, <given-names>C. A.</given-names></string-name> <article-title hwp:id="article-title-25">GraphCom: A multidimensional measure of graphic complexity applied to 131 written languages</article-title>. <source hwp:id="source-24">Behav. Res. Methods</source> <fpage>1</fpage>â<lpage>23</lpage> (<year>2017</year>) doi:<pub-id pub-id-type="doi">10.3758/s13428-017-0881-y</pub-id>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2 xref-ref-25-3 xref-ref-25-4"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Schmalz X."><surname>Schmalz</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marinus E."><surname>Marinus</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Coltheart M."><surname>Coltheart</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Castles A."><surname>Castles</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-26">Getting to the bottom of orthographic depth</article-title>. <source hwp:id="source-25">Psychon. Bull. Rev</source>. <volume>22</volume>, <fpage>1614</fpage>â<lpage>1629</lpage> (<year>2015</year>).</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Seymour P. H. K."><surname>Seymour</surname>, <given-names>P. H. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Aro M."><surname>Aro</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erskine J. M."><surname>Erskine</surname>, <given-names>J. M.</given-names></string-name>, &amp; <article-title hwp:id="article-title-27">collaboration with COST Action A8 network. Foundation literacy acquisition in European orthographies</article-title>. <source hwp:id="source-26">Br. J. Psychol</source>. <volume>94</volume>, <fpage>143</fpage>â<lpage>174</lpage> (<year>2003</year>).</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2 xref-ref-27-3 xref-ref-27-4"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Pellegrino F."><surname>Pellegrino</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="CoupÃ© C."><surname>CoupÃ©</surname>, <given-names>C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Marsico E."><surname>Marsico</surname>, <given-names>E.</given-names></string-name> <article-title hwp:id="article-title-28">A Cross-Language Perspective on Speech Information Rate</article-title>. <source hwp:id="source-27">Language</source> <volume>87</volume>, <fpage>539</fpage>â<lpage>558</lpage> (<year>2011</year>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Share D. L."><surname>Share</surname>, <given-names>D. L.</given-names></string-name> <article-title hwp:id="article-title-29">On the Anglocentricities of current reading research and practice: the perils of overreliance on anâ outlierâ orthography</article-title>. <source hwp:id="source-28">Psychol. Bull</source>. <volume>134</volume>, <fpage>584</fpage> (<year>2008</year>).</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Sereno S. C."><surname>Sereno</surname>, <given-names>S. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-30">Spelling-sound regularity effects on eye fixations in reading</article-title>. <source hwp:id="source-29">Percept. Psychophys</source>. <volume>62</volume>, <fpage>402</fpage>â<lpage>409</lpage> (<year>2000</year>).</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Protopapas A."><surname>Protopapas</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Vlahou E. L."><surname>Vlahou</surname>, <given-names>E. L.</given-names></string-name> <article-title hwp:id="article-title-31">A comparative quantitative analysis of Greek orthographic transparency</article-title>. <source hwp:id="source-30">Behav. Res. Methods</source> <volume>41</volume>, <fpage>991</fpage>â<lpage>1008</lpage> (<year>2009</year>).</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Coltheart M."><surname>Coltheart</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rastle K."><surname>Rastle</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perry C."><surname>Perry</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Langdon R."><surname>Langdon</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Ziegler J."><surname>Ziegler</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-32">DRC: a dual route cascaded model of visual word recognition and reading aloud</article-title>. <source hwp:id="source-31">Psychol. Rev</source>. <volume>108</volume>, <fpage>204</fpage> (<year>2001</year>).</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Chen H.-C."><surname>Chen</surname>, <given-names>H.-C.</given-names></string-name> <article-title hwp:id="article-title-33">Lexical processing in a non-native language: Effects of language proficiency and learning strategy</article-title>. <source hwp:id="source-32">Mem. Cognit</source>. <volume>18</volume>, <fpage>279</fpage>â<lpage>288</lpage> (<year>1990</year>).</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Gagl B."><surname>Gagl</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawelka S."><surname>Hawelka</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wimmer H."><surname>Wimmer</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-34">On Sources of the Word Length Effect in Young Readers</article-title>. <source hwp:id="source-33">Sci. Stud. Read</source>. <volume>19</volume>, <fpage>289</fpage>â<lpage>306</lpage> (<year>2015</year>).</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Wimmer H."><surname>Wimmer</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-35">Characteristics of developmental dyslexia in a regular writing system</article-title>. <source hwp:id="source-34">Appl. Psycholinguist</source>. <volume>14</volume>, <fpage>1</fpage>â<lpage>33</lpage> (<year>1993</year>).</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2"><label>35.</label><citation publication-type="book" citation-type="book" ref:id="391896v3.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Auer M."><surname>Auer</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Guber G."><surname>Guber</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wimmer H."><surname>Wimmer</surname>, <given-names>H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mayringer H."><surname>Mayringer</surname>, <given-names>H.</given-names></string-name> <source hwp:id="source-35">Salzburger Lese-Screening fÃ¼r die Klassenstufen</source> <fpage>1</fpage>â<lpage>4</lpage>. (<publisher-loc>Hogrefe</publisher-loc>, <year>2005</year>).</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Lakens D."><surname>Lakens</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-36">Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses</article-title>. <source hwp:id="source-36">Soc. Psychol. Personal. Sci</source>. <volume>8</volume>, <fpage>355</fpage>â<lpage>362</lpage> (<year>2017</year>).</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Alexander J. D."><surname>Alexander</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Nygaard L. C."><surname>Nygaard</surname>, <given-names>L. C.</given-names></string-name> <article-title hwp:id="article-title-37">Reading voices and hearing text: talker-specific auditory imagery in reading</article-title>. <source hwp:id="source-37">J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>34</volume>, <fpage>446</fpage>â<lpage>459</lpage> (<year>2008</year>).</citation></ref><ref id="c38" hwp:id="ref-38"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Yao B."><surname>Yao</surname>, <given-names>B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Scheepers C."><surname>Scheepers</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-38">Contextual modulation of reading rate for direct versus indirect speech quotations</article-title>. <source hwp:id="source-38">Cognition</source> <volume>121</volume>, <fpage>447</fpage>â<lpage>453</lpage> (<year>2011</year>).</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Stites M. C."><surname>Stites</surname>, <given-names>M. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Luke S. G."><surname>Luke</surname>, <given-names>S. G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Christianson K."><surname>Christianson</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-39">The psychologist said quickly, âDialogue descriptions modulate reading speed!â</article-title> <source hwp:id="source-39">Mem. Cognit</source>. <volume>41</volume>, <fpage>137</fpage>â<lpage>151</lpage> (<year>2013</year>).</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Inhoff A. W."><surname>Inhoff</surname>, <given-names>A. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Topolski R."><surname>Topolski</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-40">Use of phonological codes during eye fixations in reading and in on-line and delayed naming tasks</article-title>. <source hwp:id="source-40">J. Mem. Lang</source>. <volume>33</volume>, <fpage>689</fpage>â<lpage>713</lpage> (<year>1994</year>).</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Landerl K."><surname>Landerl</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-41">Phonological Awareness and Rapid Automatized Naming as Longitudinal Predictors of Reading in Five Alphabetic Orthographies with Varying Degrees of Consistency</article-title>. <source hwp:id="source-41">Sci. Stud. Read</source>. <volume>23</volume>, <fpage>220</fpage>â<lpage>234</lpage> (<year>2019</year>).</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Moll K."><surname>Moll</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-42">Cognitive mechanisms underlying reading and spelling development in five European orthographies</article-title>. <source hwp:id="source-42">Learn. Instr</source>. <volume>29</volume>, <fpage>65</fpage>â<lpage>77</lpage> (<year>2014</year>).</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Gagl B."><surname>Gagl</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawelka S."><surname>Hawelka</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Richlan F."><surname>Richlan</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schuster S."><surname>Schuster</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hutzler F."><surname>Hutzler</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-43">Parafoveal preprocessing in reading revisited: Evidence from a novel preview manipulation</article-title>. <source hwp:id="source-43">J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>40</volume>, <fpage>588</fpage>â<lpage>595</lpage> (<year>2014</year>).</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Schotter E. R."><surname>Schotter</surname>, <given-names>E. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jia A."><surname>Jia</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ferreira V. S."><surname>Ferreira</surname>, <given-names>V. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-44">Preview benefit in speaking occurs regardless of preview timing</article-title>. <source hwp:id="source-44">Psychon. Bull. Rev</source>. <volume>21</volume>, <fpage>755</fpage>â<lpage>762</lpage> (<year>2014</year>).</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Vitu F."><surname>Vitu</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McConkie G. W."><surname>McConkie</surname>, <given-names>G. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kerr P."><surname>Kerr</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="OâRegan J. K."><surname>OâRegan</surname>, <given-names>J. K.</given-names></string-name> <article-title hwp:id="article-title-45">Fixation location effects on fixation durations during reading: an inverted optimal viewing position effect</article-title>. <source hwp:id="source-45">Vision Res</source>. <volume>41</volume>, <fpage>3513</fpage>â<lpage>3533</lpage> (<year>2001</year>).</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Metzner P."><surname>Metzner</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="von der Malsburg T."><surname>von der Malsburg</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vasishth S."><surname>Vasishth</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="RÃ¶sler F."><surname>RÃ¶sler</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-46">Brain Responses to World Knowledge Violations: A Comparison of Stimulus-and Fixation-triggered Event-related Potentials and Neural Oscillations</article-title>. <source hwp:id="source-46">J. Cogn. Neurosci</source>. <volume>27</volume>, <fpage>1017</fpage>â<lpage>1028</lpage> (<year>2015</year>).</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Schotter E. R."><surname>Schotter</surname>, <given-names>E. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tran R."><surname>Tran</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-47">Donât Believe What You Read (Only Once) Comprehension Is Supported by Regressions During Reading</article-title>. <source hwp:id="source-47">Psychol. Sci</source>. 0956797614531148 (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1177/0956797614531148</pub-id>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Johnson R. L."><surname>Johnson</surname>, <given-names>R. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oehrlein E. C."><surname>Oehrlein</surname>, <given-names>E. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Roche W. L."><surname>Roche</surname>, <given-names>W. L.</given-names></string-name> <article-title hwp:id="article-title-48">Predictability and parafoveal preview effects in the developing reader: Evidence from eye movements</article-title>. <source hwp:id="source-48">J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>44</volume>, <fpage>973</fpage>â<lpage>991</lpage> (<year>2018</year>).</citation></ref><ref id="c49" hwp:id="ref-49"><label>49.</label><citation publication-type="other" citation-type="journal" ref:id="391896v3.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Milledge S. V."><surname>Milledge</surname>, <given-names>S. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blythe H. I."><surname>Blythe</surname>, <given-names>H. I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Liversedge S. P."><surname>Liversedge</surname>, <given-names>S. P.</given-names></string-name> <article-title hwp:id="article-title-49">Parafoveal pre-processing in children reading English: The importance of external letters</article-title>. <source hwp:id="source-49">Psychon. Bull. Rev</source>. (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.3758/s13423-020-01806-8</pub-id>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="HÃ¤ikiÃ¶ T."><surname>HÃ¤ikiÃ¶</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bertram R."><surname>Bertram</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="HyÃ¶nÃ¤ J."><surname>HyÃ¶nÃ¤</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-50">Development of Parafoveal Processing within and across Words in Reading: Evidence from the Boundary Paradigm</article-title>. <source hwp:id="source-50">Q. J. Exp. Psychol</source>. <volume>63</volume>, <fpage>1982</fpage>â<lpage>1998</lpage> (<year>2010</year>).</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>51.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Blythe H. I."><surname>Blythe</surname>, <given-names>H. I.</given-names></string-name> <article-title hwp:id="article-title-51">Developmental Changes in Eye Movements and Visual Information Encoding Associated With Learning to Read</article-title>. <source hwp:id="source-51">Curr. Dir. Psychol. Sci</source>. <volume>23</volume>, <fpage>201</fpage>â<lpage>207</lpage> (<year>2014</year>).</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Kliegl R."><surname>Kliegl</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nuthmann A."><surname>Nuthmann</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Engbert R."><surname>Engbert</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-52">Tracking the mind during reading: The influence of past, present, and future words on fixation durations</article-title>. <source hwp:id="source-52">J. Exp. Psychol. Gen</source>. <volume>135</volume>, <fpage>12</fpage>â<lpage>35</lpage> (<year>2006</year>).</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Giraud A.-L."><surname>Giraud</surname>, <given-names>A.-L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poeppel D."><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-53">Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source hwp:id="source-53">Nat. Neurosci</source>. <volume>15</volume>, <fpage>511</fpage>â<lpage>517</lpage> (<year>2012</year>).</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Gross J."><surname>Gross</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-54">Speech Rhythms and Multiplexed Oscillatory Sensory Coding in the Human Brain</article-title>. <source hwp:id="source-54">PLOS Biol</source>. <volume>11</volume>, <fpage>e1001752</fpage> (<year>2013</year>).</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>55.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Fiebelkorn I. C."><surname>Fiebelkorn</surname>, <given-names>I. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Saalmann Y. B."><surname>Saalmann</surname>, <given-names>Y. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kastner S."><surname>Kastner</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-55">Rhythmic Sampling within and between Objects despite Sustained Attention at a Cued Location</article-title>. <source hwp:id="source-55">Curr. Biol</source>. <volume>23</volume>, <fpage>2553</fpage>â<lpage>2558</lpage> (<year>2013</year>).</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>56.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Di Liberto G. M."><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-56">Atypical cortical entrainment to speech in the right hemisphere underpins phonemic deficits in dyslexia</article-title>. <source hwp:id="source-56">NeuroImage</source> <volume>175</volume>, <fpage>70</fpage>â<lpage>79</lpage> (<year>2018</year>).</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><label>57.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.57" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Archer K."><surname>Archer</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pammer K."><surname>Pammer</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Vidyasagar T. R."><surname>Vidyasagar</surname>, <given-names>T. R.</given-names></string-name> <article-title hwp:id="article-title-57">A Temporal Sampling Basis for Visual Processing in Developmental Dyslexia</article-title>. <source hwp:id="source-57">Front. Hum. Neurosci</source>. <volume>14</volume>, (<year>2020</year>).</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>58.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.58" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Leong V."><surname>Leong</surname>, <given-names>V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Goswami U."><surname>Goswami</surname>, <given-names>U.</given-names></string-name> <article-title hwp:id="article-title-58">Impaired extraction of speech rhythm from temporal modulation patterns in speech in developmental dyslexia</article-title>. <source hwp:id="source-58">Front. Hum. Neurosci</source>. <volume>8</volume>, (<year>2014</year>).</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>59.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Gramfort A."><surname>Gramfort</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-59">MNE software for processing MEG and EEG data</article-title>. <source hwp:id="source-59">NeuroImage</source> <volume>86</volume>, <fpage>446</fpage>â<lpage>460</lpage> (<year>2014</year>).</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>60.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Rayner K."><surname>Rayner</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-60">Eye movement latencies for parafoveally presented words</article-title>. <source hwp:id="source-60">Bull. Psychon. Soc</source>. <volume>11</volume>, <fpage>13</fpage>â<lpage>16</lpage> (<year>1978</year>).</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><label>61.</label><citation publication-type="book" citation-type="book" ref:id="391896v3.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Taylor I."><surname>Taylor</surname>, <given-names>I.</given-names></string-name> <chapter-title>The Korean writing system: An alphabet? A syllabary? a logography?</chapter-title> in <source hwp:id="source-61">Processing of Visible Language</source> <fpage>67</fpage>â<lpage>82</lpage> (<publisher-name>Springer</publisher-name>, <publisher-loc>Boston, MA</publisher-loc>, <year>1980</year>). doi:<pub-id pub-id-type="doi">10.1007/978-1-4684-1068-6_5</pub-id>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><label>62.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Bates D."><surname>Bates</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="MÃ¤chler M."><surname>MÃ¤chler</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bolker B."><surname>Bolker</surname>, <given-names>B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Walker S."><surname>Walker</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-61">Fitting Linear Mixed-Effects Models Using lme4</article-title>. <source hwp:id="source-62">J. Stat. Softw</source>. <volume>67</volume>, <fpage>1</fpage>â<lpage>48</lpage> (<year>2015</year>).</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><label>63.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Kliegl R."><surname>Kliegl</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wei P."><surname>Wei</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dambacher M."><surname>Dambacher</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yan M."><surname>Yan</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zhou X."><surname>Zhou</surname>, <given-names>X.</given-names></string-name> <article-title hwp:id="article-title-62">Experimental effects and individual differences in linear mixed models: estimating the relationship between spatial, object, and attraction effects in visual attention</article-title>. <source hwp:id="source-63">Front. Psychol</source>. <volume>1</volume>, <fpage>238</fpage> (<year>2011</year>).</citation></ref></ref-list><sec id="s10" hwp:id="sec-25"><title hwp:id="title-30">Supplementary Methods: Mean to mode transformation function</title><p hwp:id="p-60">The typical distribution of eye fixation duration data in reading is ex-Gaussian (see <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-16" hwp:rel-id="F1">Figure 1c</xref>). For simplification, one can decompose the ex-Gaussian distribution in a normal and an exponential distribution. This decomposition is a simplification on a mathematical level since both the normal and exponential distributions can be modeled easily. Consequently, one can describe the central tendency of the ex-Gaussian distribution by three parameters: the mean and standard deviation of the normal distributed component and the exponential component (i.e., reflecting the skew of the ex-Gaussian distribution). The Î¼ relates to the mean of the normal distribution. The Ï refers to the standard deviation of the normal distribution. The Ï describes the rightward skew, i.e., representing the contribution of the exponential distribution.</p><p hwp:id="p-61">In a frequency analysis one investigates if a reoccurring event, in our case a saccade, has a temporal structure. In Experiment 1, we showed that the mode of the ex-Gaussian distribution of the sampling durations (fixation plus saccade duration) indicates the most common sampling duration, which was found to be the adequate metric for the frequency estimation (i.e., by showing comparable frequency estimates based on mode fixation duration and power spectral estimation approaches but not when using the mean fixation duration). In the eye-tracking literature on reading, it is more typical to report mean fixation durations. Reporting mean not mode fixation durations is a central problem of the current meta-analysis. Accordingly, we developed the mean-to-mode transformation function described here. With this function, we transform the mean fixation durations extracted from papers into mode values.</p><p hwp:id="p-62">We implemented the mean-to-mode transformation function in three steps: (i) We fit ex-Gaussian distributions (i.e., by decomposition methods) to existing empirical datasets. (ii) We used fitted ex-Gaussian parameters (Î¼: mean of the normal distribution; Ï: standard deviation of the normal distribution; Ï: exponential component describing the rightward skew) to simulate new, informed, ex-Gaussian distributions to derive a transformation function. (iii) We optimize the transformation function to increase transformation accuracy.</p><sec id="s10a" hwp:id="sec-26"><label>(i)</label><title hwp:id="title-31">Ex-Gaussian fitting to existing empirical datasets</title><p hwp:id="p-63">First, we fitted the three ex-Gaussian parameters to 29 empirical datasets containing fixation durations (11 published studies, i.e., three German studies from our lab, (1â3), and multiple English studies (4â10) for which datasets were openly available) using the <italic toggle="yes">mexgauss</italic> function from the <italic toggle="yes">retimes</italic> package in <italic toggle="yes">R</italic> (11). <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure S1a</xref> shows two empirical and the respective simulated distribution, including the mean and mode of the distribution, exemplarily. Henceforth, we jointly refer to these 29 datasets as the âsimulation dataâ. Combined we now obtained the exact mean and mode of 29 datasets as well as the Î¼, Ï, Ï for each dataset. Note, the main selection criteria for the datasets used in the present study was availability and accessibility of the raw fixation duration values.</p></sec><sec id="s10b" hwp:id="sec-27"><label>(ii)</label><title hwp:id="title-32">Using fitted ex-Gaussian parameters to simulate new informed ex-Gaussian distributions</title><p hwp:id="p-64">With the fitted ex-Gaussian parameters, we estimated, in a next step, three robust linear regression models (rlm function in R from the MASS package; (12)). One for each of the ex-Gaussian parameters (Î¼, Ï, Ï) to predict the mean fixation duration (Î¼: 0.40, SE = 0.09, t = 4.6; Ï: 0.14, SE = 0.09, t = 1.5; Ï: 0.60, SE = 0.09, t = 6.7). <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure S1b</xref> shows the relationships of each parameter to the means from each study.</p><p hwp:id="p-65">Now, we can simulate realistic ex-Gaussian distributions (with 500 samples) for any mean value with the <italic toggle="yes">exGAUS</italic> function from the <italic toggle="yes">gamlss</italic>.<italic toggle="yes">dist</italic> package in R (13). These distributions can be realized by the fitted linear regression coefficients (intercept, beta weight), which describe the relationship of each of the three ex-Gaussian parameter estimates to the mean of the dataset (see <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure S1b</xref>). For example, one can go to the graphics and see that with a mean fixation duration of 200 ms one can obtain a Î¼ value of 140, a Ï value of 30 and a Ï value of 40. Having a value to each of the ex-Gaussian parameters, one can simulate an ex-Gaussian distribution. This simulation then allows us to estimate the mode of the distribution, in our case around 150 ms. As a consequence, one can directly relate the mode of 150 ms to the mean value of 200 ms.</p><p hwp:id="p-66">To reduce estimation noise and increase robustness against outliers, we sampled ex-Gaussian distributions, not only for the 29 datasets available but for the whole range of mean fixation durations (149 and 397 ms) from the meta-analysis. From these 248 simulated ex-Gaussian distributions, we estimated the mode values relating each mean to a mode value. Finally, these related mean and mode values allowed us to generate a generalized mean-to-mode transformation function by only one linear regression (e.g., blue line in <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Fig. S1d</xref>). Note, to realize the function in a generalized way, we on purpose neglected the specific experimental manipulations of the different studies of our simulation data.</p></sec><sec id="s10c" hwp:id="sec-28"><label>(iii)</label><title hwp:id="title-33">Optimizing the transformation function</title><p hwp:id="p-67">For initial quality control, we used the fitted linear regression coefficients (intercept, beta weight) from the transformation function to transform the mean fixation duration of each of the 29 simulation datasets into a simulated mode. Since we were also able to measure the mode of these datasets were able to compare the simulated to the measured modes for each dataset. In <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Figure S1c</xref>, Level 1, we present the residual errors of the 29 simulated modes, relative to the measured modes. The negative relationship between the measured mode and the residuals (i.e., simulated minus measured mode) indicated a systematic overestimation for low measured modes and underestimation for high measured modes. This is likely caused by imprecisions in the ex-Gaussian fitting procedure. To account for this systematic error, we corrected the simulated modes by a sequential procedure. First, we described the error by a linear model. This model is then used to predict the over/underestimation of a given mode. The prediction is then used to correct the simulate mode values. This correction procedure was applied two times.</p><p hwp:id="p-68"><xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Figure S1c</xref>, Level 3, shows that after this sequential correction procedure, the final transformation function does not include a systematic error that one would expect to be present in the residuals. <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-7" hwp:rel-id="F7">Figure S1d</xref>, accordingly, shows the final, i.e., corrected, transformation-function, for all possible 248 mean fixation durations. <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-7-8" hwp:rel-id="F7">Figure S1e</xref> shows the final quality check from the simulation dataset showing the relationship of the measured and simulated modes. Despite some unsystematic noise, this optimized transformation function showed a near-perfect beta of 0.95 (SE = 0.23; t = 4.1).</p><fig id="figS1" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6 xref-fig-7-7 xref-fig-7-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;391896v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1.</label><caption hwp:id="caption-8"><p hwp:id="p-69">Development of a transformation function for converting mean fixation durations into simulated mode values. In order to estimate the predominant, i.e., mode, fixation duration from a single mean value extracted from a published empirical study, we implemented the following procedure: (a) First, an ex-Gaussian function was fitted separately to empirical fixation duration distributions of each of 29 independent datasets (left panel; see Materials and Methods), and we extracted the parameters Î¼, Ï, Ï describing the fitted ex-Gaussian distribution (right panel). (b) Second, across these 29 datasets, the relationship between the ex-Gaussian parameters and the empirical study means were described by linear models, separately for each parameter. The intercepts and beta-weights resulting from these linear models, for each ex-Gauss parameter, were then used to simulate an ex-Gaussian for each of the 29 empirical mean fixation durations, so that we could compare the empirical and simulated ex-Gaussian distributions. (c) Residuals for the mode estimation showed a systematic error, i.e., an overestimation for low modes and underestimation for high modes (upper panel / Level 1). We accounted for this systematic error, sequentially, by two linear models describing the error; see lower panel / Level 3 for residuals after accounting for the estimation error. (d) Performance of the final mean-to-mode transformation function (blue line) demonstrated for 248 fixation durations, i.e., for each millisecond within the range of the meta-analysis (149 â 397 ms). (e) Performance of the final version of the mean-to-mode transformation function, as demonstrated by the relationship between empirically measured and simulated modes from the 29 datasets used for establishing the transformation function.</p></caption><graphic xlink:href="391896v3_figS1" position="float" orientation="portrait" hwp:id="graphic-8"/></fig></sec></sec><ref-list hwp:id="ref-list-2"><title hwp:id="title-34">References</title><ref id="Sc1" hwp:id="ref-64"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Gagl B"><surname>Gagl</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawelka S"><surname>Hawelka</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hutzler F"><surname>Hutzler</surname> <given-names>F</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-63">A similar correction mechanism in slow and fluent readers after suboptimal landing positions</article-title>. <source hwp:id="source-64">Frontiers in Human Neuroscience</source> <volume>8</volume>:<fpage>355</fpage>.</citation></ref><ref id="Sc2" hwp:id="ref-65"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Gagl B"><surname>Gagl</surname> <given-names>B</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-64">Blue hypertext is a good design decision: no perceptual disadvantage in reading and successful highlighting of relevant information</article-title>. <source hwp:id="source-65">PeerJ</source> <volume>4</volume>:<fpage>e2467</fpage>.</citation></ref><ref id="Sc3" hwp:id="ref-66"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Gagl B"><surname>Gagl</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hawelka S"><surname>Hawelka</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Richlan F"><surname>Richlan</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schuster S"><surname>Schuster</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hutzler F"><surname>Hutzler</surname> <given-names>F</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-65">Parafoveal preprocessing in reading revisited: Evidence from a novel preview manipulation</article-title>. <source hwp:id="source-66">Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>40</volume>(<issue>2</issue>):<fpage>588</fpage>â<lpage>595</lpage>.</citation></ref><ref id="Sc4" hwp:id="ref-67"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Abbott MJ"><surname>Abbott</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Angele B"><surname>Angele</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ahn YD"><surname>Ahn</surname> <given-names>YD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-66">Skipping syntactically illegal the previews: the role of predictability</article-title>. <source hwp:id="source-67">J Exp Psychol Learn Mem Cogn</source> <volume>41</volume>(<issue>6</issue>):<fpage>1703</fpage>â<lpage>1714</lpage>.</citation></ref><ref id="Sc5" hwp:id="ref-68"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Angele B"><surname>Angele</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-67">Processing the in the parafovea: are articles skipped automatically?</article-title> <source hwp:id="source-68">J Exp Psychol Learn Mem Cogn</source> <volume>39</volume>(<issue>2</issue>):<fpage>649</fpage>â<lpage>662</lpage>.</citation></ref><ref id="Sc6" hwp:id="ref-69"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Angele B"><surname>Angele</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tran R"><surname>Tran</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-68">Parafoveal-foveal Overlap Can Facilitate Ongoing Word Identification During Reading: Evidence from Eye Movements</article-title>. <source hwp:id="source-69">J Exp Psychol Hum Percept Perform</source> <volume>39</volume>(<issue>2</issue>):<fpage>526</fpage>â<lpage>538</lpage>.</citation></ref><ref id="Sc7" hwp:id="ref-70"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schotter ER"><surname>Schotter</surname> <given-names>ER</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-69">Semantic preview benefit in reading English: The effect of initial letter capitalization</article-title>. <source hwp:id="source-70">Journal of Experimental Psychology: Human Perception and Performance</source> <volume>40</volume>(<issue>4</issue>):<fpage>1617</fpage>â<lpage>1628</lpage>.</citation></ref><ref id="Sc8" hwp:id="ref-71"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yang J"><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schuett S"><surname>Schuett</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Slattery TJ"><surname>Slattery</surname> <given-names>TJ</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-70">The effect of foveal and parafoveal masks on the eye movements of older and younger readers</article-title>. <source hwp:id="source-71">Psychology and Aging</source> <volume>29</volume>(<issue>2</issue>):<fpage>205</fpage>â<lpage>212</lpage>.</citation></ref><ref id="Sc9" hwp:id="ref-72"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Schotter ER"><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bicknell K"><surname>Bicknell</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Howard I"><surname>Howard</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Levy R"><surname>Levy</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-71">Task effects reveal cognitive flexibility responding to frequency and predictability: Evidence from eye movements in reading and proofreading</article-title>. <source hwp:id="source-72">Cognition</source> <volume>131</volume>(<issue>1</issue>):<fpage>1</fpage>â<lpage>27</lpage>.</citation></ref><ref id="Sc10" hwp:id="ref-73"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="391896v3.73" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Schotter ER"><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tran R"><surname>Tran</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rayner K"><surname>Rayner</surname> <given-names>K</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-72">Donât Believe What You Read (Only Once) Comprehension Is Supported by Regressions During Reading</article-title>. <source hwp:id="source-73">Psychological Science</source>: 0956797614531148.</citation></ref><ref id="Sc11" hwp:id="ref-74"><label>11.</label><citation publication-type="website" citation-type="web" ref:id="391896v3.74" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-74"><collab hwp:id="collab-1">retimes package | R Documentation Available at: //<ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.rdocumentation.org/packages/retimes/versions/0.1-2" ext-link-type="uri" xlink:href="https://www.rdocumentation.org/packages/retimes/versions/0.1-2" hwp:id="ext-link-8">https://www.rdocumentation.org/packages/retimes/versions/0.1-2</ext-link> [</collab>Accessed <date-in-citation content-type="access-date">January 30, 2019</date-in-citation>].</citation></ref><ref id="Sc12" hwp:id="ref-75"><label>12.</label><citation publication-type="website" citation-type="web" ref:id="391896v3.75" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Venables WN"><surname>Venables</surname> <given-names>WN</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ripley BD"><surname>Ripley</surname> <given-names>BD</given-names></string-name> (<year>2002</year>) <source hwp:id="source-74">Modern Applied Statistics with S (Springer)</source> Available at: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.springer.com/de/book/9780387954578" ext-link-type="uri" xlink:href="http://www.springer.com/de/book/9780387954578" hwp:id="ext-link-9">www.springer.com/de/book/9780387954578</ext-link> [Accessed <date-in-citation content-type="access-date">December 6, 2017</date-in-citation>].</citation></ref><ref id="Sc13" hwp:id="ref-76"><label>13.</label><citation publication-type="website" citation-type="web" ref:id="391896v3.76" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-76"><collab hwp:id="collab-2">gamlss | for statistical modelling</collab> Available at: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.gamlss.com/" ext-link-type="uri" xlink:href="https://www.gamlss.com/" hwp:id="ext-link-10">https://www.gamlss.com/</ext-link> [Accessed <date-in-citation content-type="access-date">January 30, 2019</date-in-citation>].</citation></ref></ref-list><sec id="s11" hwp:id="sec-29"><title hwp:id="title-35">Supplementary Methods: Small talk interview</title><p hwp:id="p-70">The full list of 22 questions from the âsmall talkâ interview we conducted in German.</p><list list-type="order" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-71">Wie viele PrÃ¼fungen hast du jetzt im Semester?</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-72">Warum studierst du? (ich frage meistens auch noch so allgemeiner, was genau sie studieren, wieso sie sich das rausgesucht haben, was ihnen daran SpaÃ macht)</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-73">Hast du dein Wohnort durch das Studium gewechselt?</p></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-74">Wie ist die Wohnungssituation fÃ¼r dich in Frankfurt?</p></list-item><list-item hwp:id="list-item-5"><p hwp:id="p-75">Hast du schon frÃ¼her eine Ausbildung/Studium gemacht?</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-76">Hast du schon mal ein Auslandsaufenthalt gemacht?</p></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-77">Hast du eine Zweitsprache? Welche Sprachen sprichst du?</p></list-item><list-item hwp:id="list-item-8"><p hwp:id="p-78">Machst du irgendein Sport?</p></list-item><list-item hwp:id="list-item-9"><p hwp:id="p-79">Spielst du irgendwelche Computerspiele / hast frÃ¼her gespielt?</p></list-item><list-item hwp:id="list-item-10"><p hwp:id="p-80">Was sind deine Hobbys / Interessen?</p></list-item><list-item hwp:id="list-item-11"><p hwp:id="p-81">Was hast du am Wochenende gemacht? (wenn sie sich nicht erinnern kÃ¶nnen frage ich was sie fÃ¼r das kommende Wochenende vorhaben)</p></list-item><list-item hwp:id="list-item-12"><p hwp:id="p-82">In welchen LÃ¤ndern warst du schon?</p></list-item><list-item hwp:id="list-item-13"><p hwp:id="p-83">Was ist dein Lieblingsessen? Was isst du gerne?</p></list-item><list-item hwp:id="list-item-14"><p hwp:id="p-84">Wie findest du das Wetter im Moment so?</p></list-item><list-item hwp:id="list-item-15"><p hwp:id="p-85">Was ist deine Lieblingsjahreszeit?</p></list-item><list-item hwp:id="list-item-16"><p hwp:id="p-86">Machst du irgendein Nebenjob?</p></list-item><list-item hwp:id="list-item-17"><p hwp:id="p-87">Hast du fÃ¼r den Sommer / die Weihnachtsferien etwas vor?</p></list-item><list-item hwp:id="list-item-18"><p hwp:id="p-88">Isst du gerne in der Mensa?</p></list-item><list-item hwp:id="list-item-19"><p hwp:id="p-89">Wo kommst du eigentlich her?</p></list-item><list-item hwp:id="list-item-20"><p hwp:id="p-90">Was machst du im Studium im Moment so inhaltlich?</p></list-item><list-item hwp:id="list-item-21"><p hwp:id="p-91">Was hast du nach dem Studium vor?</p></list-item><list-item hwp:id="list-item-22"><p hwp:id="p-92">Was war dein letzter Kinofilm / Fernsehfilm / Serie, die du geguckt hast?</p></list-item></list></sec></back></article>
