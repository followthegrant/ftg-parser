<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/151811</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;151811</article-id><article-id pub-id-type="other" hwp:sub-type="slug">151811</article-id><article-id pub-id-type="other" hwp:sub-type="tag">151811</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">A Guide to Robust Statistical Methods in Neuroscience</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label> Corresponding author: <email hwp:id="email-1">rwilcox@usc.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2524-2976</contrib-id><name name-style="western" hwp:sortable="Wilcox Rand R."><surname>Wilcox</surname><given-names>Rand R.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-2524-2976"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0006-8729</contrib-id><name name-style="western" hwp:sortable="Rousselet Guillaume A."><surname>Rousselet</surname><given-names>Guillaume A.</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-0006-8729"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Dept. of Psychology, University of Southern California</institution>, Los Angeles, CA 90089-1061, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Institute of Neuroscience and Psychology, College of Medical, Veterinary and Life Sciences, University of Glasgow</institution>, 58 Hillhead Street, G12 8QB, Glasgow, <country>UK</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-06-20T13:54:00-07:00">
    <day>20</day><month>6</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-06-20T13:54:00-07:00">
    <day>20</day><month>6</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-06-20T14:01:33-07:00">
    <day>20</day><month>6</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-06-20T14:01:33-07:00">
    <day>20</day><month>6</month><year>2017</year>
  </pub-date><elocation-id>151811</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-06-19"><day>19</day><month>6</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2017-06-19"><day>19</day><month>6</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-06-20"><day>20</day><month>6</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by/4.0/</ext-link></p></license></permissions><self-uri xlink:href="151811.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/151811v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="151811.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/151811v1/151811v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/151811v1/151811v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">ABSTRACT</title><p hwp:id="p-2">There is a vast array of new and improved methods for comparing groups and studying associations that offer the potential for substantially increasing power, providing improved control over the probability of a Type I error, and yielding a deeper and more nuanced understanding of neuroscience data. These new techniques effectively deal with four insights into when and why conventional methods can be unsatisfactory. But for the non-statistician, the vast array of new and improved techniques for comparing groups and studying associations can seem daunting, simply because there are so many new methods that are now available. The paper briefly reviews when and why conventional methods can have relatively low power and yield misleading results. The main goal is to suggest some general guidelines regarding when, how and why certain modern techniques might be used.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">Non-normality</kwd><kwd hwp:id="kwd-2">heteroscedasticity</kwd><kwd hwp:id="kwd-3">skewed distributions</kwd><kwd hwp:id="kwd-4">outliers</kwd><kwd hwp:id="kwd-5">curvature.</kwd></kwd-group><counts><page-count count="49"/></counts></article-meta></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-3">Introduction</title><p hwp:id="p-3">The typical introductory statistics course covers classic methods for comparing groups (e.g., Students t-test, the ANOVA F test and the Wilcoxon–Mann–Whitney test) and studying associations (e.g., Pearsons correlation and least squares regression). The two-sample Students t-test and the ANOVA F test assume that sampling is from normal distributions and that the population variances are identical, which is generally known as the homoscedasticity assumption. When testing hypotheses based on the least squares regression estimator or Pearsons correlation, similar assumptions are made. (<xref ref-type="sec" rid="s2b" hwp:id="xref-sec-4-1" hwp:rel-id="sec-4">Section 2.2</xref> elaborates on the details.) An issue of fundamental importance is whether violating these assumptions can have a serious detrimental impact on two key properties of a statistical test: the probability of a false positive, also known as a Type I error, and power, the probability of detecting true differences among groups and a true association among two or more variables. There is the related issue of whether conventional methods provide enough detail regarding how groups differ as well as the nature of true association.</p><p hwp:id="p-4">There are a variety of relatively well-known techniques for dealing with non-normality and unequal variances. For example, use a rank based method. However, by modern standards, these methods are relatively ineffective for reasons reviewed in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-7-1" hwp:rel-id="sec-7">section 3</xref>. More effective techniques are indicated in <xref ref-type="sec" rid="s4" hwp:id="xref-sec-14-1" hwp:rel-id="sec-14">section 4</xref>.</p><p hwp:id="p-5">The good news is that when comparing groups that have non-normal but identical distributions, control over the Type I error probability is, in general, reasonably good when using conventional techniques. But if the groups differ, there is now a vast literature indicating that under general conditions, power can be relatively poor. In practical terms, important differences among groups might be missed (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">b</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">c</xref>). Even when the normality assumption is true, but the population variances differ (called heteroscedasticity), power can be adversely impacted when using the ANOVA F.</p><p hwp:id="p-6">Similar concerns arise when dealing with regression. Conventional methods, including rank-based techniques, perform well, in terms of controlling the probability of a Type I error, when there is no association. But when there is an association, conventional methods, including rank-based techniques (e.g., Spearmans rho and Kendalls tau) can have a relatively low probability of detecting an association relative to modern methods developed during the last thirty years.</p><p hwp:id="p-7">Practical concerns regarding conventional methods stem from four major insights (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-2" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-2" hwp:rel-id="ref-62">c</xref>). These insights can be briefly summarized as follows.
<list list-type="bullet" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-8">The central limit and skewed distributions: much larger sample sizes might be needed to assume normality than is generally recognized.</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-9">There is now a deeper understanding of the role of outliers and how to deal with them. Some seemingly obvious strategies for dealing with outliers, based on standard training, are known to be highly unsatisfactory for reasons outlined later in the paper.</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-10">There is a substantial literature indicating that methods that assume homoscedasticity (equal variances) can yield inaccurate results when in fact there is heteroscedasticity, even when the sample sizes are quite large.</p></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-11">When dealing with regression, curvature refers to situations where the regression line is not straight. There is now considerable evidence that curvature is a much more serious concern than is generally recognized.</p></list-item></list></p><p hwp:id="p-12">Robust methods are typically thought of as methods that provide good control over the probability of a Type I error. But today they deal with much broader issues. In particular, they are designed to deal with the problems associated with skewed distributions, outliers, heteroscedasticity and curvature that were outlined above.</p><p hwp:id="p-13">One of the more fundamental goals among robust methods is to develop techniques that are not overly sensitive to very small changes in a distribution. For instance, a slight departure from normality should not destroy power. This rules out any method based on the mean and variance (e.g., <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Staudte &amp; Sheather, 1990</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-3" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-2" hwp:rel-id="ref-61">b</xref>). <xref ref-type="sec" rid="s2c" hwp:id="xref-sec-5-1" hwp:rel-id="sec-5">Section 2.3</xref> illustrates this point.</p><p hwp:id="p-14">Many modern robust methods are designed to have nearly the same amount of power as conventional methods under normality, but they continue to have relatively high power under slight departures from normality where conventional techniques based on means perform poorly. There are other fundamental goals, some of which are relevant regardless of how large the sample sizes might be. But an effective description of these goals goes beyond the scope of this paper. For present purposes, the focus is on achieving relatively high power.</p><p hwp:id="p-15">Another point that should be stressed has to do with standard power analyses. A common goal is to justify some choice for the sample sizes prior to obtaining any data. Note that in effect, the goal is to address a statistical issue without any data. Typically this is done by assuming normality and homoscedasticity, which in turn can suggest that relatively small sample sizes provide adequate power when using means. A practical concern is that violating either of these two assumptions can have a tremendous impact on power when attention is focused exclusively on comparing means. <xref ref-type="sec" rid="s2a" hwp:id="xref-sec-3-1" hwp:rel-id="sec-3">Section 2.1</xref> illustrates this concern when dealing with measures of central tendency. Similar concerns arise when dealing with least squares regression and Pearson’s correlation. These concerns can be mitigated by using recently developed robust methods summarized here as well as in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-4" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-3" hwp:rel-id="ref-62">c</xref>).</p><p hwp:id="p-16">There is now a vast array of new and improved methods that effectively deal with known concerns associated with classic techniques (e.g., <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Maronna et al., 2006</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Heritier et al., 2009</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-5" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-3" hwp:rel-id="ref-61">b</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-4" hwp:rel-id="ref-62">c</xref>). They include substantially improved methods for dealing with all four of the major insights previously listed. Perhaps more importantly, they can provide a deeper, more accurate and more nuanced understanding of data as will be illustrated in <xref ref-type="sec" rid="s5" hwp:id="xref-sec-23-1" hwp:rel-id="sec-23">section 5</xref>.</p><p hwp:id="p-17">For books focused on the mathematical foundation of modern robust methods, see <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Hampel et al. (1986)</xref>, <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Huber and Ronchetti (2009)</xref>, <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Maronna et al. (2006)</xref>, and <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">Staudte and Sheather (1990)</xref>. For books focused on applying robust methods, see <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Heritier et al. (2009)</xref> and <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-6" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-5" hwp:rel-id="ref-62">c</xref>). From an applied point of view, the difficulty is not finding a method that effectively deals with violations of standard assumptions. Rather, for the non-statistician, there is the difficulty of navigating through the many alternative techniques that might be used. This paper is an attempt to deal with this issue by providing a general guide regarding when and how modern robust methods might be used when comparing two or more groups. When dealing with regression, all of the concerns associated with conventional methods for comparing groups remain and new concerns are introduced. A few issues related to regression and correlations are covered here, but it is stressed that there are many other modern advances that have practical value. Readers interested in regression are referred to <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-7" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-6" hwp:rel-id="ref-62">c</xref>).</p><p hwp:id="p-18">A few general points should be stressed. First, if robust methods, such as modern methods based on the median described later in this paper, give very similar results to conventional methods based on means, this is reassuring that conventional methods based on the mean are performing relatively well in terms of Type I errors and power. But when they differ, there is doubt about the validity of conventional techniques. In a given situation, conventional methods might perform well in terms of controlling the Type I error probability and providing reasonably high power. But the best that can be said is that there are general conditions where conventional methods do indeed yield inaccurate inferences. A particular concern is that they can suffer from relatively low power in situations where more modern methods have relatively high power. More details are provided in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-7-2" hwp:rel-id="sec-7">sections 3</xref> and <xref ref-type="sec" rid="s4" hwp:id="xref-sec-14-2" hwp:rel-id="sec-14">4</xref>.</p><p hwp:id="p-19">Second, the choice of method can make a substantial difference in our understanding of data. One reason is that modern methods provide alternative and interesting perspectives that more conventional methods do not address. A complication is that there is no single method that dominates in terms of power or providing a deep understanding of how groups compare. The same is true when dealing with regression and measures of association. The reality is that several methods might be needed to address even what appears as a simple problem, for instance comparing two groups.</p><p hwp:id="p-20">There is, of course, the issue of controlling the probability of one or more Type I errors when multiple tests are performed. There are many modern improvements for dealing with this issue (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-8" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-7" hwp:rel-id="ref-62">c</xref>). And another strategy is to put more emphasis on exploratory studies. One could then deal with the risk of false positive results by conducting a confirmatory study aimed at determining whether significant results in an exploratory study can be replicated (<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Wagenmakers et al., 2012</xref>). Otherwise, there is the danger of missing important details regarding how groups compare. One of the main messages here is that despite the lack of a single method that dominates, certain guidelines can be offered regarding how to analyze data.</p><p hwp:id="p-21">Modern methods for plotting data can be invaluable as well (e.g., <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Rousselet, et al., 2017</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Rousselet et al., 2016</xref>; Weissgerber et al., 2016) In particular, they can provide important perspectives beyond the common strategy of using error bars. Complete details go beyond the scope of this paper, but <xref ref-type="sec" rid="s5" hwp:id="xref-sec-23-2" hwp:rel-id="sec-23">section 5</xref> illustrates some of the more effective plots that might be used.</p><p hwp:id="p-22">The paper is organized as follows. <xref ref-type="sec" rid="s2" hwp:id="xref-sec-2-1" hwp:rel-id="sec-2">Section 2</xref> briefly reviews when and why conventional methods can be highly unsatisfactory. This is necessary in order to appreciate modern technology and because standard training typically ignores these issues. Efforts to modernize basic training have been made (e.g., <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Field et al., 2012</xref>; <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-4" hwp:rel-id="ref-61">Wilcox, 2017b</xref>, c). And a 2016 special issue of the <italic toggle="yes">American Statistician</italic> (volume 69, number 4) was aimed at encouraging instructors to modernize their courses. (This special issue touches on a broader range of topics than those discussed here.) Some neuroscientists are trained in a manner that takes into account modern insights relevant to basic principles. But it is evident that most are not. <xref ref-type="sec" rid="s3" hwp:id="xref-sec-7-3" hwp:rel-id="sec-7">Section 3</xref> reviews the seemingly more obvious strategies aimed at salvaging standard techniques, the point being that by modern standards they are relatively ineffective and cannot be recommended. Moreover, certain strategies are not technically sound. <xref ref-type="sec" rid="s3" hwp:id="xref-sec-7-4" hwp:rel-id="sec-7">Section 3</xref> also provides an indication of how concerns regarding conventional methods are addressed using more modern techniques. <xref ref-type="sec" rid="s4" hwp:id="xref-sec-14-3" hwp:rel-id="sec-14">Section 4</xref> describes strategies for comparing two independent or dependent groups that take modern advances into account. Included are some methods aimed at comparing correlations as well as methods designed to determine which independent variables are most important. <xref ref-type="sec" rid="s5" hwp:id="xref-sec-23-3" hwp:rel-id="sec-23">Section 5</xref> illustrates modern methods using data from several studies.</p></sec><sec id="s2" hwp:id="sec-2" hwp:rev-id="xref-sec-2-1"><label>2</label><title hwp:id="title-4">Insights Regarding Conventional Methods</title><p hwp:id="p-23">This section elaborates on the concerns with conventional methods for comparing groups and studying associations stemming from the four insights previously indicated.</p><sec id="s2a" hwp:id="sec-3" hwp:rev-id="xref-sec-3-1 xref-sec-3-2"><label>2.1</label><title hwp:id="title-5">Skewed Distributions</title><p hwp:id="p-24">A skewed distribution simply refers to a distribution that is not symmetric about some central value. An example is shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1<bold>A</bold>.</xref> Such distributions occur naturally. An example relevant to the neurosciences is given in <xref ref-type="sec" rid="s5b" hwp:id="xref-sec-25-1" hwp:rel-id="sec-25">section 5.2</xref>. Skewed distributions are a much more serious problem for statistical inferences than once thought due to insights regarding the central limit theorem.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-25">Panel <bold>A</bold> illustrates an example of skewed distribution. Panel <bold>B</bold> illustrates the distribution of the sample mean under normality (the dashed line), <italic toggle="yes">n</italic> = 30, and the actual distribution based on a simulation. Each sample mean was computed based on 30 observations randomly sampled from the distribution shown in <bold>A.</bold> Panels <bold>C</bold> and <bold>D</bold> compare the theoretical <italic toggle="yes">T</italic> distribution with 29 degrees of freedom to distributions of 5000 <italic toggle="yes">T</italic> values. Again, the <italic toggle="yes">T</italic> values were computed from observations sampled from the distribution in <bold>A.</bold></p></caption><graphic xlink:href="151811_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-26">Consider the one-sample case. Conventional wisdom is that with a relatively small sample size, normality can be assumed under random sampling. An implicit assumption was that if the sample mean has, approximately, a normal distribution, then Students t-test will perform reasonably well. It is now known that this is not necessarily the case as illustrated, for example, in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-9" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-5" hwp:rel-id="ref-61">b</xref>).</p><p hwp:id="p-27">This point is illustrated here using a simulation that is performed in the following manner. Imagine that data are randomly sampled from the distribution shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1<bold>A</bold></xref> (a lognormal distribution) and the mean is computed based on a sample size of <italic toggle="yes">n</italic> = 30. Repeating this process 5000 times, the thick black line in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1<bold>B</bold></xref> shows a plot of the resulting sample means; the thin gray line is the plot of the means when sampling from a normal distribution instead. The distribution of <italic toggle="yes">T</italic> values for samples of <italic toggle="yes">n</italic> = 30 is indicated by the thick black line in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1<bold>C</bold></xref>; the thin gray line is the distribution of <italic toggle="yes">T</italic> values when sampling from a normal distribution. As can be seen, the actual <italic toggle="yes">T</italic> distribution extends out much further to the left compared to the distribution of <italic toggle="yes">T</italic> under normality. That is, in the current example, sampling from a skewed distribution leads to much more extreme values than expected by chance under normality, which in turn results in more false positive results than expected when the null hypothesis is true.</p><p hwp:id="p-28">Suppose the goal is to test some hypothesis at the 0.05 level. <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Bradley (1978)</xref> suggests that as a general guide, control over the probability of a Type I error is minimally satisfactory if the actual level is between 0.025 and 0.075. When we test at the 0.05 level, we expect 5% of the t-tests to be significant. However, when sampling from the skewed distribution considered here, this is not the case: the actual Type I error probability is approximately 0.111.</p><p hwp:id="p-29"><xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1<bold>D</bold></xref> shows the distribution of <italic toggle="yes">T</italic> when <italic toggle="yes">n</italic> = 100. Now the Type I error probability is approximately 0.082, again when testing at the 0.05 level. Based on Bradleys criterion, a sample size of about 130 or larger is required. Bradely (1978) goes on to suggest that ideally, the actual Type I error probability should be between 0.045 and 0.055. Now <italic toggle="yes">n</italic> = 600 is unsatisfactory; the actual level is approximately 0.057. With <italic toggle="yes">n</italic> = 700 the level is approximately 0.055.</p><p hwp:id="p-30">Before continuing, it is noted that the median belongs to the class of trimmed means, which refers to the strategy of trimming a specified proportion of the smallest and largest values and averaging the values that remain. For example, if n=10, 10% trimming means that the lowest and highest values are removed and the remaining data are averaged. Similarly, 20% trimming would remove the two smallest and two largest values. Based on conventional training, trimming might seem counterintuitive, but in some situations it can substantially increase our ability to control the Type I error probability, as illustrated next, and trimming can substantially increase power as well for reasons to be explained.</p><p hwp:id="p-31">First focus on controlling the probability of a Type I error. (<xref ref-type="sec" rid="s2c" hwp:id="xref-sec-5-2" hwp:rel-id="sec-5">Section 2.3</xref> illustrates one of the reasons methods based on means can have relatively low power.) <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref> illustrates the Type I error probability as a function of the sample size, when when using the mean, median and when sampling from the asymmetric (lognormal) distribution in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Figure 1<bold>A</bold></xref>. Inferences based on the 20% trimmed mean were made via the method derived by <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Tukey and McLaughlin (1963)</xref>. Inferences based on the median were made via the method derived by <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Hettmansperger and Sheather (2011)</xref>. (The software used to apply these latter two methods is contained in the R package described at the beginning of <xref ref-type="sec" rid="s4" hwp:id="xref-sec-14-4" hwp:rel-id="sec-14">section 4</xref>.) Also shown is the Type I error probability when sampling from a normal distribution. The gray area indicates Bradley’s criterion.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-32">Type I error probability as a function of sample size. The type I error probability was computed by running a simulation with 10,000 iterations. In each iteration, sample sizes from 10 to 500, in steps of 10, were drawn from a normal distribution and a lognormal distribution. For each combination of sample size and distribution, we applied a t-test on the mean, a test of the median, and a t-test on the 20% trimmed mean, all with alpha = 0.05. Depending on the test applied, the mean, median or 20% trimmed mean of the population sampled from was zero. The black horizontal line marks the expected 0.05 type I error probability. The gray area marks Bradley’s satisfactory range. When sampling from a normal distribution, all methods are close to the nominal 0.05 level, except the trimmed mean for very small sample sizes. When sampling is from a lognormal distribution, the mean and the trimmed mean give rise to too many false alarms for small sample sizes. The mean continues to give higher false positive rates than the other techniques even with n=500.</p></caption><graphic xlink:href="151811_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-33"><xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> illustrates the association between power and the sample size for the distributions used in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2</xref>. As can be seen, under normality, the sample mean is best, followed closely by the 20% trimmed mean. The median is least satisfactory when dealing with a normal distribution, as expected. However, for the asymmetric (lognormal) distribution, the median performs best and the mean performs very poorly.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-34">Power as a function of sample size. The probability of a true positive was computed by running a simulation with 10,000 iterations. In each iteration, sample sizes from 10 to 500, in steps of 10, were drawn from a normal distribution and the (lognormal) distribution shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Figure 1A</xref>. For each combination of sample size and distribution, we applied a t-test on the mean, a test of the median, and a t-test on the 20% trimmed mean, all with alpha = 0.05. Depending on the test applied, the mean, median or 20% trimmed mean of the population sampled from was 0.5. The black horizontal line marks the conventional 80% power threshold. When sampling from a normal distribution, all methods require less than 50 observations to achieve 80% power, and the mean appears to have higher power at lower sample size than the other methods. When sampling from a lognormal distribution, power drops dramatically for the mean but not for the median and the trimmed mean. Power for the median actually improves. The exact pattern of results depends on the effect size and the asymmetry of the distribution we sample from, so we strongly encourage readers to perform their own detailed power analyses.</p></caption><graphic xlink:href="151811_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-35">A feature of random samples taken from the distribution in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Figure 1<bold>A</bold></xref> is that the expected proportion of points declared an outlier is relatively small. For skewed distributions, as we move toward situations where outliers are more common, a sample size greater than 300 can be required to achieve reasonably good control over the Type I error probability. That is, control over the Type I error probability is a function of both the degree a distribution is skewed and the likelihood of encountering outliers. However, there are methods that perform reasonably well with small sample sizes as indicated in <xref ref-type="sec" rid="s4a" hwp:id="xref-sec-15-1" hwp:rel-id="sec-15">section 4.1</xref>.</p><p hwp:id="p-36">For symmetric distributions, where outliers tend to occur, the reverse can happen; the actual Type I error probability can be substantially less than the nominal level. This happens because outliers inflate the standard deviation, which in turn lowers the value of <italic toggle="yes">T</italic>, which in turn can negatively impact power. <xref ref-type="sec" rid="s2c" hwp:id="xref-sec-5-3" hwp:rel-id="sec-5">Section 2.3</xref> elaborates on this issue.</p><p hwp:id="p-37">In an important sense, outliers have a larger impact on the sample variance than the sample mean, which impacts the t-test. To illustrate this point, imagine the goal is to test <italic toggle="yes">H</italic><sub>0</sub>: <italic toggle="yes">μ</italic> = 1 based on the following values: 1, 1.5, 1.6, 1.8, 2, 2.2, 2.4, 2.7. Then <italic toggle="yes">T</italic> = 4.69, the p value is <italic toggle="yes">p</italic> = 0.002, and the 0.95 confidence interval = [1.45, 2.35]. Now, including the value 8, the mean increases from 1.9 to 2.58, suggesting at some level there is stronger evidence for rejecting the null hypothesis. However, this outlier increases the standard deviation from 0.54 to 2.1, and now <italic toggle="yes">T</italic> = 2.26 and <italic toggle="yes">p</italic> = 0.054. The 0.95 confidence interval = [–0.033, 3.19].</p><p hwp:id="p-38">Now consider the goal of comparing two independent or dependent groups. If the groups have identical distributions, then difference scores have a symmetric distribution and the probability of a Type I error is, in general, less than the nominal level when using conventional methods based on means. Now, in addition to outliers, differences in skewness create practical concerns when using Students t-test. Indeed, under general conditions, the two-sample Students t-test for independent groups is not even asymptotically correct, roughly because the standard error of the difference between the sample means is not estimated correctly (e.g., <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Cressie &amp; Whitford, 1986</xref>). Moreover, Students t-test can be biased. This means that the probability of rejecting the null hypothesis of equal means can be higher when the population means are equal, compared to situations where the population means differ. Roughly, this concern arises because the distribution of <italic toggle="yes">T</italic> can be skewed, and in fact the mean of <italic toggle="yes">T</italic> can differ from zero even though the null hypothesis is true. (For a more detailed explanation, see <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-8" hwp:rel-id="ref-62">Wilcox, 2017c</xref>, section 5.5.) Problems persist when Students t-test is replaced by <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Welchs (1938)</xref> method, which is designed to compare means in a manner that allows unequal variances. Put another way, if the goal is to test the hypothesis that two groups have identical distributions, conventional methods based on means perform well in terms of controlling the Type I error probability. But if the goal is to compare the population means, and if distributions differ, conventional methods can perform poorly.</p><p hwp:id="p-39">There are many techniques that perform well when dealing with skewed distributions in terms of controlling the Type I error probability, some of which are based on the usual sample median (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-10" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-9" hwp:rel-id="ref-62">c</xref>). Both theory and simulations indicate that as the amount of trimming increases, the ability to control over the probability of a Type I error increases as well. Moreover, for reasons to be explained, trimming can substantially increase power, a result that is not obvious based on conventional training. The optimal amount of trimming depends on the characteristics of the population distributions, which are unknown. Currently, the best that can be said is that the choice can make a substantial difference. The 20% trimmed has been studied extensively and often it provides a good compromise between the two extremes: no trimming (the mean) and the maximum amount of trimming (the median).</p><p hwp:id="p-40">In various situations, particularly important are inferential methods based on what are called bootstrap techniques. Two basic versions are the bootstrap-t and percentile bootstrap. Roughly, rather than assume normality, bootstrap-t methods perform a simulation using the observed data that yields an estimate of an appropriate critical value and a p-value. So values of <italic toggle="yes">T</italic> are generated as done in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Figure 1</xref>, only data are sampled, with replacement, from the observed data. In essence, bootstrap-t methods generate data-driven <italic toggle="yes">T</italic> distributions expected by chance if there were no effect. The percentile bootstrap proceeds in a similar manner, only when dealing with a trimmed mean, for example, the goal is to determine the distribution of the sample trimmed mean, which can then be used to compute a p-value and a confidence interval. When comparing two independent groups based on the usual sample median, if there are tied (duplicated) values, currently the only method that performs well in simulations, in terms of controlling the Type I error probability, is based on a percentile bootstrap method (cf. <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-10" hwp:rel-id="ref-62">Wilcox, 2017c</xref>, Table 5.3.). <xref ref-type="sec" rid="s4a" hwp:id="xref-sec-15-2" hwp:rel-id="sec-15">Section 4.1</xref> elaborates on how this method is performed.</p><p hwp:id="p-41">If the amount of trimming is close to zero, the bootstrap-t method is preferable to the percentile bootstrap method. But as the amount of trimming increases, at some point a percentile bootstrap method is preferable. This is the case with 20% trimming (e.g., Wilcox, 2017). It seems to be the case with 10% trimming as well, but a definitive study has not been made.</p><p hwp:id="p-42">Also, if the goal is to reflect the typical response, it is evident that the median or even a 20% trimmed mean might be more satisfactory. Using quantiles (percentiles) other than the median can be important as well, for reasons summarized in <xref ref-type="sec" rid="s4b" hwp:id="xref-sec-16-1" hwp:rel-id="sec-16">section 4.2</xref>.</p><p hwp:id="p-43">When comparing independent groups, modern improvements on the Wilcoxon–Mann– Whitney (WMW) test are another possibility, which are aimed at making inferences about the probability that a random observation from the first group is less than a random observation from the second. (More details are provided in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-7-5" hwp:rel-id="sec-7">section 3</xref>.) Additional possibilities are described in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-11" hwp:rel-id="ref-60">Wilcox (2017a)</xref>, some of which are illustrated in <xref ref-type="sec" rid="s4" hwp:id="xref-sec-14-5" hwp:rel-id="sec-14">section 4</xref> of this paper.</p><p hwp:id="p-44">In some situations, robust methods can have substantially higher power than any method based on means. But it is not being suggested that robust methods always have more power. This is not the case. Rather, the point is that power can depend crucially on the conjunction of which estimator is used (for instance the mean vs. the median), and how a confidence interval is built (for instance a parametric method or the percentile bootstrap). These choices are not trivial and must be taken into account when analyzing data.</p></sec><sec id="s2b" hwp:id="sec-4" hwp:rev-id="xref-sec-4-1"><label>2.2</label><title hwp:id="title-6">Heteroscedasticity</title><p hwp:id="p-45">It has been clear for some time that when using classic methods for comparing means, het-eroscedasticity (unequal population variances) is a serious concern (e.g., <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Brown &amp; Forsythe, 1974</xref>). Heteroscedasticity can impact both power and the Type I error probability. The basic reason is that, under general conditions, methods that assume homoscedasticity are using an incorrect estimate of the standard error when in fact there is heteroscedasticity. Indeed, there are concerns regardless of how large the sample size might be. Roughly, as we consider more and more complicated designs, heteroscedasticity becomes an increasing concern.</p><p hwp:id="p-46">When dealing with regression, homoscedasticity means that the variance of the dependent variable does not depend on the value of the independent variable. When dealing with age and depressive symptoms, for example, homoscedasticity means that the variation in measures of depressive symptoms at age 23 is the same at age 80 or any age in between as illustrated in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-47">Homoscedasticity and heteroscedasticity. Panel <bold>A</bold> illustrates homoscedasticity. The variance of the dependent variable is the same at any age. Panel <bold>B</bold> illustrates heteroscedasticity. The variance of the dependent variable can depend on the age of the participate.</p></caption><graphic xlink:href="151811_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-48">Independence implies homoscedasticity. So for this particular situation, classic methods associated with least squares regression, Pearson’s correlation, Kendall’s tau and Spearman’s rho are using a correct estimate of the standard error, which helps explain why they perform well in terms of Type I errors when there is no association. That is, when a homoscedastic method rejects, it is reasonable to conclude that there is an association, but in terms of inferring the nature of the association, these methods can perform poorly. Again, a practical concern is that when there is heteroscedasticity, homoscedastic methods use an incorrect estimate of the standard error, which can result in poor power and erroneous conclusions.</p><p hwp:id="p-49">A seemingly natural way of salvaging homoscedastic methods is to test the assumption that there is homoscedasticity. But six studies summarized in <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-11" hwp:rel-id="ref-62">Wilcox (2017c)</xref> found that this strategy is unsatisfactory. Presumably situations are encountered where this is not the case, but it is difficult and unclear how to determine when such situations are encountered.</p><p hwp:id="p-50">Methods that are designed to deal with heteroscedasticity have been developed and are easily applied using extant software. These techniques use a correct estimate of the standard error regardless of whether the homoscedasticity assumption is true. A general recommendation is to always use a heteroscedastic method given the goal of comparing measures of central tendency, or making inferences about regression parameters, as well as measures of association.</p></sec><sec id="s2c" hwp:id="sec-5" hwp:rev-id="xref-sec-5-1 xref-sec-5-2 xref-sec-5-3 xref-sec-5-4"><label>2.3</label><title hwp:id="title-7">Outliers</title><p hwp:id="p-51">Even small departures from normality can devastate power. The modern illustration of this fact stems from Tukey (1960) and is based on what is generally known as a mixed normal distribution. The mixed normal considered by Tukey means that with probability 0.9 an observation is sampled from a standard normal distribution; otherwise an observation is sampled from a normal distribution having mean zero and standard deviation 10. <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5<bold>A</bold></xref> shows a standard normal distribution and the mixed normal discussed by Tukey. Note that in the center of the distributions, the mixed normal is below the normal distribution. But for the two ends of the mixed normal distribution, the tails, the mixed normal lies above the normal distribution. For this reason, the mixed normal is often described as having heavy tails. In general, heavy-tailed distributions roughly refer to distributions where outliers are likely to occur.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-52"><bold>A.</bold> Density functions for the standard normal distribution (solid line) and the mixed normal distribution (dotted line). These distributions have an obvious similarity, 15 yet the variances are 1 and 10.9. <bold>B.</bold> Boxplots of means, medians and 20% trimmed means when sampling from a normal distribution. <bold>C.</bold> Boxplots of means, medians and 20% trimmed means when sampling from a mixed normal distribution. In panels B and C, each distribution has 10,000 values, and each of these values was obtained by computing the mean, median or trimmed mean of 30 randomly generated observations.</p></caption><graphic xlink:href="151811_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-53">Here is an important point. The standard normal has variance one, but the mixed normal has variance 10.9. That is, the population variance can be overly sensitive to slight changes on the tails of a distribution. Consequently, even slight departure from normality can result in relative poor power when using any method based on the mean.</p><p hwp:id="p-54">Put another way, samples from the mixed normal are more likely to result in outliers compared to samples from a standard normal distribution. As previously indicated, outliers inflate the sample variance, which can negatively impact power when using means. Another concern is that they can give a distorted and misleading summary regarding the bulk of the participants.</p><p hwp:id="p-55">The first indication that heavy-tailed distributions are a concern stems from a result derived by Laplace about two centuries ago. In modern terminology, he established that as we move from a normal distribution to a distribution more likely to generate outliers, the standard error of the usual sample median can be smaller than the standard error of the mean (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Hand, 1998</xref>). The first empirical evidence implying that outliers might be more common than what is expected under normality was reported by <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Bessel (1818)</xref>.</p><p hwp:id="p-56">To add perspective, we computed the mean, median and a 20% trimmed mean based on 30 observations generated from a standard normal distribution. (Again, a 20% trimmed mean removes the 20% lowest and highest values and averages the remaining data.) Then we repeated this process 10,000 times. Boxplots of the results are shown in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5B</xref>. Theory tells us that under normality the variation of the sample means is smaller than the variation among the 20% trimmed means and medians, and <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Figure 1B</xref> provides perspective on the extent this is the case.</p><p hwp:id="p-57">Now we repeat this process, only data are sampled from the mixed normal in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 5<bold>A</bold></xref>. <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5<bold>C</bold></xref> reports the results. As is evident, there is substantially less variation among the medians and 20% trimmed means. That is, despite trimming data, the standard errors of the median and 20% trimmed mean are substantially smaller, contrary to what might be expected based on standard training.</p><p hwp:id="p-58">Of course, a more important issue is whether the median or 20% trimmed mean ever have substantially smaller standard errors based on the data encountered in research. There are numerous illustrations that this is the case (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-12" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-6" hwp:rel-id="ref-61">b</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-12" hwp:rel-id="ref-62">c</xref>).</p><p hwp:id="p-59">There is the additional complication that the amount of trimming can substantially impact power, and the ideal amount of trimming, in terms of maximizing power, can depend crucially on the nature of the of the unknown distributions under investigation. The median performs best for the situation in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Figure 5<bold>C</bold></xref>, but situations are encountered where it trims too much, given the goal of minimizing the standard error. Roughly, a 20% trimmed mean competes reasonably well with the mean under normality. But as we move toward distributions that are more likely to generate outliers, at some point the median will have a smaller standard error than a 20% trimmed mean. Illustrations in <xref ref-type="sec" rid="s5" hwp:id="xref-sec-23-4" hwp:rel-id="sec-23">section 5</xref> demonstrate that this is a practical concern.</p><p hwp:id="p-60">It is not being suggested that the mere presence of outliers will necessarily result in higher power when using a 20% trimmed mean or median. But it is being argued that simply ignoring the potential impact of outliers can be a serious practical concern.</p><p hwp:id="p-61">In terms of controlling the Type I error probability, effective techniques are available for both the 20% trimmed mean and median. Because the choice between a 20% trimmed mean and median is not straightforward in terms of maximizing power, it is suggested that in exploratory studies, both of these estimators be considered.</p><p hwp:id="p-62">When dealing with least squares regression or Pearson’s correlation, again outliers are a serious concern. Indeed, even a single outlier might give a highly distorted sense about the association among the bulk of the participants under study. In particular, important associations might be missed. One of the more obvious ways of dealing with this issue is to switch to Kendall’s tau or Spearman’s rho. However, these measures of associations do not deal with all possible concerns related to outliers. For instance, two outliers, properly placed, can give a distorted sense about the association among the bulk of the data (e.g, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-7" hwp:rel-id="ref-61">Wilcox, 2017b</xref>, p. 239).</p><p hwp:id="p-63">A measure of association that deals with this issue is the skipped correlation where outliers are detected using a projection method, these points are removed, and Pearson’s correlation is computed using the remaining data. Complete details are summarized in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-13" hwp:rel-id="ref-60">Wilcox (2017a)</xref>. This particular skipped correlation can be computed with the R function scor and a confidence interval, that allows heteroscedasticity, can be computed with scorci. This function also reports a p-value when testing the hypothesis that the correlation is equal to zero. (See <xref ref-type="sec" rid="s3b" hwp:id="xref-sec-9-1" hwp:rel-id="sec-9">section 3.2</xref> for a description of common mistakes when testing hypotheses and outliers are removed.) Matlab code is available too (<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Pernet, Wilcox, &amp; Rousselet, 2012</xref>).</p></sec><sec id="s2d" hwp:id="sec-6"><label>2.4</label><title hwp:id="title-8">Curvature</title><p hwp:id="p-64">Typically, a regression line is assumed to be straight. In some situations, this approach seems to suffice. However, it cannot be stressed too strongly that there is a substantial literature indicating that this is not always the case. A vast array of new and improved nonparametric methods for dealing with curvature is now available, but complete details go beyond the scope of this paper. Here it is merely remarked that among the many nonparametric regression estimators that have been proposed, generally known as smoothers, two that seem to be particularly useful are <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Cleveland’s (1979)</xref> estimator, which can be applied via the R function lplot, and the running-interval smoother (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-14" hwp:rel-id="ref-60">Wilcox, 2017a</xref>), which can be applied with the R function rplot. Arguments can be made that other smoothers should be given serious consideration. Readers interested in these details are referred to <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-15" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-13" hwp:rel-id="ref-62">c</xref>).</p><p hwp:id="p-65">Cleveland’s smoother was initially designed to estimate the mean of the dependent variable given some value of the independent variable. The R function contains an option for dealing with outliers among the dependent variable, but it currently seems that the running-interval smoother is generally better for dealing with this issue. By default, the running-interval smoother estimates the 20% trimmed mean of the dependent variable, but any other measure of central tendency can be used via the argument est.</p><p hwp:id="p-66">A simple strategy for dealing with curvature is to include a quadratic term. Let <italic toggle="yes">X</italic> denote the independent variable. An even more general strategy is to include <italic toggle="yes">X<sup>a</sup></italic> in the model for some appropriate choice for the exponent <italic toggle="yes">a</italic>. But this strategy can be unsatisfactory as illustrated in <xref ref-type="sec" rid="s5d" hwp:id="xref-sec-27-1" hwp:rel-id="sec-27">section 5.4</xref> using data from a study dealing with fractional anisotropy and reading ability. In general, smoothers provide a more satisfactory approach.</p><p hwp:id="p-67">There are methods for testing the hypothesis that a regression line is straight (e.g., Wilcox, 2917a). However, failing to reject does not provide compelling evidence that it is safe to assume that indeed the regression line is straight. It is unclear when this approach has enough power to detect situations where curvature is an important practical concern. The best advice is to plot an estimate of the regression line using a smoother. If there is any indication that curvature might be an issue, use modern methods for dealing with curvature, many of which are summarized in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-16" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-14" hwp:rel-id="ref-62">c</xref>).</p></sec></sec><sec id="s3" hwp:id="sec-7" hwp:rev-id="xref-sec-7-1 xref-sec-7-2 xref-sec-7-3 xref-sec-7-4 xref-sec-7-5"><label>3</label><title hwp:id="title-9">Dealing with Violation of Assumptions</title><p hwp:id="p-68">Based on conventional training, there are some seemingly obvious strategies for dealing with the concerns reviewed in the previous section. But by modern standards, generally these strategies are relatively ineffective. This section summarizes strategies that perform poorly, followed by a brief description of modern methods that give improved results.</p><sec id="s3a" hwp:id="sec-8"><label>3.1</label><title hwp:id="title-10">Testing Assumptions</title><p hwp:id="p-69">A seemingly natural strategy is to test assumptions. In particular, test the hypothesis that distributions have a normal distribution and test the hypothesis that there is homoscedasticity. This approach generally fails, roughly because such tests do not have enough power to detect situations where violating these assumptions is a practical concern. That is, these tests can fail to detect situations that have an inordinately detrimental influence on statistical power and parameter estimation.</p><p hwp:id="p-70">For example, <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-17" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-15" hwp:rel-id="ref-62">c</xref>) lists six studies aimed at testing the homoscedasticity assumption with the goal of salvaging a method that assumes homoscedasticity (cf., <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Keselman et al., 2016</xref>). Briefly, these simulation studies generate data from a situation where it is known that homoscedastic methods perform poorly in terms of controlling the Type I error when there is heteroscedasticity. Then various methods for testing the homoscedasticity are performed, and if they reject, a heteroscedastic method is used instead. All six studies came to the same conclusion: this strategy is unsatisfactory. Presumably there are situations where this strategy is satisfactory, but it is unknown how to accurately determine whether this is the case based on the available data. In practical terms, all indications are that it is best to always use a heteroscedastic method when comparing measures of central tendency, and when dealing with regression as well as measures of association such as Pearson’s correlation.</p><p hwp:id="p-71">As for testing the normality assumption, for instance using the Kolmogorov–Smirnov test, currently a better strategy is to use a more modern method that performs about as well as conventional methods under normality, but which continues to perform relatively well in situations where standard techniques perform poorly. There are many such methods (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-18" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-16" hwp:rel-id="ref-62">c</xref>), some of which are outlined in <xref ref-type="sec" rid="s4" hwp:id="xref-sec-14-6" hwp:rel-id="sec-14">section 4</xref>. These methods can be applied using extant software as will be illustrated.</p></sec><sec id="s3b" hwp:id="sec-9" hwp:rev-id="xref-sec-9-1"><label>3.2</label><title hwp:id="title-11">Outliers: Two Common Mistakes</title><p hwp:id="p-72">There are two common mistakes regarding how to deal with outliers. The first is to search for outliers using the mean and standard deviation. For example, declare the value <italic toggle="yes">X</italic> and outlier if<disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1 xref-disp-formula-1-2 xref-disp-formula-1-3"><alternatives hwp:id="alternatives-1"><graphic xlink:href="151811_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives></disp-formula>
where <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="151811_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> and <italic toggle="yes">s</italic> are the usual sample mean and standard deviation, respectively. A problem with this strategy is that it suffers from masking, simply meaning that the very presence of outliers causes them to be missed (e.g, <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Rousseeuw &amp; Leroy, 1987</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-19" hwp:rel-id="ref-60">Wilcox, 2017a</xref>).</p><p hwp:id="p-73">Consider, for example, the values 1, 2, 2, 3, 4, 6, 100 and 100. The two last observations appear to be clear outliers, yet the rule given by <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">(1)</xref> fails to flag them as such. The reason is simple: the standard deviation of the sample is very large, at almost 45, because it is not robust to outliers.</p><p hwp:id="p-74">This is not to suggest that all outliers will be missed; this is not necessarily the case. The point is that multiple outliers might be missed that adversely affect any conventional method that might be used to compare means. Much more effective are the boxplot rule and the so-called MAD-median rule.</p><p hwp:id="p-75">The boxplot rule is applied as follows. Let <italic toggle="yes">q</italic><sub>1</sub> and <italic toggle="yes">q</italic><sub>2</sub> be estimates of the lower and upper quartiles, respectively. Then the value <italic toggle="yes">X</italic> is declared an outlier if <italic toggle="yes">X</italic> &lt; <italic toggle="yes">q</italic><sub>1</sub> – 1.5(<italic toggle="yes">q</italic><sub>2</sub> − <italic toggle="yes">q</italic><sub>1</sub>) or if <italic toggle="yes">X</italic> &gt; <italic toggle="yes">q</italic><sub>2</sub> + 1.5(<italic toggle="yes">q</italic><sub>2</sub> − <italic toggle="yes">q</italic><sub>1</sub>) As for the MAD-median rule, let <italic toggle="yes">X</italic><sub>1</sub>,…, <italic toggle="yes">X<sub>n</sub></italic> denote a random sample and let <italic toggle="yes">M</italic> be the usual sample median. MAD (the median absolute deviation to the median) is the median of the values |<italic toggle="yes">X</italic><sub>1</sub> – <italic toggle="yes">M</italic>|, … ,|<italic toggle="yes">X<sub>a</sub></italic> – <italic toggle="yes">M</italic>|. The MAD-median rule declares the value X an outlier if <disp-formula id="eqn2" hwp:id="disp-formula-2"><alternatives hwp:id="alternatives-3"><graphic xlink:href="151811_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives></disp-formula></p><p hwp:id="p-76">Under normality, it can be shown that MAD /0.6745 estimates the standard deviation, and of course <italic toggle="yes">M</italic> estimates the population mean. So the MAD-median rule is similar to using <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-2" hwp:rel-id="disp-formula-1">(1)</xref>, only rather than use a two-standard deviation rule, 2.24 is used instead.</p><p hwp:id="p-77">As an illustration, consider the values 1.85, 1.11, 1.11, 0.37, 0.37, 1.85, 71.53, and 71.53. The MAD-median rule detects the outliers: 71.53. But the rule given by <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-3" hwp:rel-id="disp-formula-1">(1)</xref> does not. The MAD-median rule is better than the boxplot rule in terms of avoiding masking.</p><p hwp:id="p-78">The second mistake is discarding outliers and applying some standard method for comparing means using the remaining data. This results in an incorrect estimate of the standard error, regardless how large the sample size might be. That is, an invalid test statistic is being used. Roughly, it can be shown that the remaining data are dependent, they are correlated, which invalidates the derivation of the standard error. Of course, if an argument can be made that a value is invalid, discarding it is reasonable and does not lead to technical issues. For instance, a straightforward case can be made if a measurement is outside physiological bounds, or if it follows a biologically non-plausible pattern over time, such as during an electrophysiological recording. But otherwise, the estimate of the standard error can be off by a factor of 2 (e.g., <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-8" hwp:rel-id="ref-61">Wilcox, 2017b</xref>), which is a serious practical issue. A simple way of dealing with this issue, when using a 20% trimmed mean or median, is to use a percentile bootstrap method. (With reasonably large sample sizes, alternatives to the percentile bootstrap method can be used, which are described in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-20" hwp:rel-id="ref-60">Wilcox 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-17" hwp:rel-id="ref-62">c</xref>). The main point here is that these methods are readily applied with the free software R, which is playing an increasing role in basic training. Some illustrations are given in <xref ref-type="sec" rid="s5" hwp:id="xref-sec-23-5" hwp:rel-id="sec-23">section 5</xref>.</p><p hwp:id="p-79">It is noted that when dealing with regression, outliers among the independent variables can be removed when testing hypotheses. But if outliers among the dependent variable are removed, conventional hypothesis testing techniques based on the least squares estimator are no longer valid, even when there is homoscedasticity. Again, the issue is that an incorrect estimate of the standard error is being used. When using robust regression estimators that deal with outliers among the dependent variable, again a percentile bootstrap method can be used to test hypotheses. Complete details are summarized in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-21" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-18" hwp:rel-id="ref-62">c</xref>). There are numerous regression estimators that effectively deal with outliers among the dependent variable, but a brief summary of the many details is impossible. The Theil and Sen estimator as well as the MM-estimator are relatively good choices, but arguments can be made that alternative estimators deserve serious consideration.</p></sec><sec id="s3c" hwp:id="sec-10"><label>3.3</label><title hwp:id="title-12">Transform the Data</title><p hwp:id="p-80">A common strategy for dealing with non-normality or heteroscedasticity is to transform the data. There are exceptions, but generally this approach is unsatisfactory for several reasons. First, the transformed data can again be skewed to the point that classic techniques perform poorly (e.g., <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-9" hwp:rel-id="ref-61">Wilcox, 2017b</xref>). Second, this simple approach does not deal with outliers in a satisfactory manner (e.g., <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Doksum &amp; Wong, 1983</xref>; <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Rasmussen, 1989</xref>). The number of outliers might decline, but it can remain the same and even increase. Currently, a much more satisfactory strategy is to use a modern robust method such as a bootstrap method in conjunction with a 20% trimmed mean or median. This approach also deals with heteroscedasticity in a very effective manner (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-22" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-19" hwp:rel-id="ref-62">c</xref>).</p><p hwp:id="p-81">Another concern is that a transformation changes the hypothesis being tested. In effect, transformations muddy the interpretation of any comparison because a transformation of the data also transforms the construct that it measures (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Grayson, 2004</xref>).</p></sec><sec id="s3d" hwp:id="sec-11"><label>3.4</label><title hwp:id="title-13">Use a Rank-Based Method</title><p hwp:id="p-82">Standard training suggests a simple way of dealing with non-normality: use a rank-based method such as the WMW test, the Kruskal–Wallis test and Friedman’s method. The first thing to stress is that under general conditions these methods are not designed to compare medians or other measures of central tendency. (For an illustration based on the Wilcoxon signed rank test, see <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-10" hwp:rel-id="ref-61">Wilcox, 2017b</xref>, p. 367. Also see <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Fagerland &amp; Sandvik, 2009</xref>.) Moreover, the derivation of these methods is based on the assumption that the groups have identical distributions. So in particular, homoscedasticity is assumed. In practical terms, if they reject, conclude that the distributions differ.</p><p hwp:id="p-83">But to get a more detailed understanding of how groups differ and by how much, alternative inferential techniques should be used in conjunction with plots such as those summarized by <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Rousselet, et al. (2017)</xref>. For example, use methods based on a trimmed mean or median.</p><p hwp:id="p-84">Many improved rank-based methods have been derived (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Brunner et al., 2002</xref>). But again, these methods are aimed at testing the hypothesis that groups have identical distributions. Important exceptions are the improvements on the WMW test (e.g., <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Cliff, 1996</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-23" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-11" hwp:rel-id="ref-61">b</xref>), which, as previously noted, are aimed at making inferences about the probability that a random observation from the first group is less than a random observation from the second.</p></sec><sec id="s3e" hwp:id="sec-12"><label>3.5</label><title hwp:id="title-14">Permutation Methods</title><p hwp:id="p-85">For completeness, it is noted that permutation methods have received some attention in the neuroscience literature (e.g., <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">Winkler et al., 2014</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Pernet et al., 2015</xref>). Briefly, this approach is well designed to test the hypothesis that two groups have identical distributions. But based on results reported by <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Boik (1987)</xref>, this approach cannot be recommended when comparing means. The same is true when comparing medians for reasons summarized by <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Romano (1990)</xref>. <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Chung and Romano (2013)</xref> summarize general theoretical concerns and limitations. They go on to suggest a modification of the standard permutation method, but at least in some situations the method is unsatisfactory (<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-20" hwp:rel-id="ref-62">Wilcox, 2017c</xref>, section 7.7). A deep understanding of when this modification performs well is in need of further study.</p></sec><sec id="s3f" hwp:id="sec-13"><label>3.6</label><title hwp:id="title-15">More Comments about the Median</title><p hwp:id="p-86">In terms of power, the mean is preferable over the median or 20% trimmed mean when dealing with symmetric distributions for which outliers are rare. If the distribution are skewed, the median and 20% trimmed mean can better reflect what is typical, and improved control over the Type I error probability can be achieved. When outliers occur, there is the possibility that the mean will have a much larger standard error than the median or 20% trimmed mean. Consequently, methods based on the mean might have relatively poor power. Note, however, that for skewed distributions, the difference between two means might be larger than the difference between the corresponding medians. Consequently, even when outliers are common, it is possible that a method based on the means will have more power. In terms of maximizing power, a crude rule is to use a 20% trimmed mean, but the seemingly more important point is that no method dominates. Focusing on a single measure of central tendency might result is missing an important difference. So again, exploratory studies can be vitally important.</p><p hwp:id="p-87">Even when there are tied values, it is now possible to get excellent control over the probability of a Type I error when using the usual sample median. For the one-sample case, this can be done with using a distribution free technique via the R function sintv2. Distribution free means that the actual Type I error probability can be determined exactly assuming random sampling only. When comparing two or more groups, currently the only known technique that performs well is the percentile bootstrap. Methods based on estimates of the standard error can perform poorly, even with large sample sizes. Also, when there are tied values, the distribution of the sample median does not necessarily converge to a normal distribution as the sample size increases. The very presence of tied values is not necessarily disastrous. But it is unclear how many tied values can be accommodated before disaster strikes. The percentile bootstrap method eliminates this concern.</p></sec></sec><sec id="s4" hwp:id="sec-14" hwp:rev-id="xref-sec-14-1 xref-sec-14-2 xref-sec-14-3 xref-sec-14-4 xref-sec-14-5 xref-sec-14-6"><label>4</label><title hwp:id="title-16">Comparing Groups and Measures of Association</title><p hwp:id="p-88">This section elaborates on methods aimed at comparing groups and measures of association. First attention is focused on two independent groups. Comparing dependent groups is discussed in <xref ref-type="sec" rid="s4d" hwp:id="xref-sec-18-1" hwp:rel-id="sec-18">section 4.4</xref>. <xref ref-type="sec" rid="s4e" hwp:id="xref-sec-19-1" hwp:rel-id="sec-19">Section 4.5</xref> comments briefly on more complex designs. This is followed by a description of how to compare measures of association as well as an indication of modern advances related to the analysis of covariance. Included are indications of how to apply these methods using the free software R, which at the moment is easily the best software for applying modern methods. R is a vast and powerful software package. Certainly matlab could be used, but this would require writing hundreds of functions in order to compete with R. There are numerous books on R, but only a relatively small subset of the basic commands is needed to apply the functions described here. (See, for example, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-12" hwp:rel-id="ref-61">Wilcox, 2017b</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-21" hwp:rel-id="ref-62">c</xref>.)</p><p hwp:id="p-89">The R functions noted here are stored in the R package WRS, which can be installed as indicated at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://https://github.com/nicebread/WRS" ext-link-type="uri" xlink:href="http://https://github.com/nicebread/WRS" hwp:id="ext-link-2">https://github.com/nicebread/WRS</ext-link>. Alternatively, and seemingly easier, use the R command source on the file Rallfun-v33.txt, which can be downloaded from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dornsife.usc.edu/labs/rwilcox/software/" ext-link-type="uri" xlink:href="http://dornsife.usc.edu/labs/rwilcox/software/" hwp:id="ext-link-3">http://dornsife.usc.edu/labs/rwilcox/software/</ext-link>.</p><p hwp:id="p-90">All recommended methods deal with heteroscedasticity. When comparing groups and distributions differ in shape, these methods are generally better than classic methods for comparing means, which can perform poorly.</p><sec id="s4a" hwp:id="sec-15" hwp:rev-id="xref-sec-15-1 xref-sec-15-2 xref-sec-15-3"><label>4.1</label><title hwp:id="title-17">Dealing with Small Sample Sizes</title><p hwp:id="p-91">This section focuses on the common situation in the neuroscience where the sample sizes are relatively small. When the sample sizes are very small, say less than or equal ten and greater than four, conventional methods based on means are satisfactory in terms of Type I errors when the null hypothesis is that the groups have identical distributions. If the goal is to control the probability of a Type I error when the null hypothesis is that groups have equal means, extant methods can be unsatisfactory. And as previously noted, methods based on means can have poor power relative to alternative techniques.</p><p hwp:id="p-92">Many of the more effective methods are based in part on the percentile bootstrap method. Consider, for example, the goal of comparing the medians of two independent groups. Let <italic toggle="yes">M<sub>x</sub></italic> and <italic toggle="yes">M<sub>y</sub></italic> be the sample medians for the two groups being compared and let <italic toggle="yes">D</italic> = <italic toggle="yes">M<sub>x</sub></italic> –<italic toggle="yes">M<sub>y</sub>.</italic> The basic strategy is to perform a simulation based on the observed data with the goal of approximating the distribution of <italic toggle="yes">D</italic>, which can then be used to compute a p-value as well as a confidence interval.</p><p hwp:id="p-93">Let <italic toggle="yes">n</italic> and <italic toggle="yes">m</italic> denote the sample sizes for the first and second group, respectively. The percentile bootstrap method proceeds as follows. For the first group, randomly sample with replacement <italic toggle="yes">n</italic> observations. This yields what is generally called a bootstrap sample. For the second group, randomly sample with replacement <italic toggle="yes">m</italic> observations. Next, based on these bootstrap samples, compute the sample medians, say <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="151811_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="151811_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula>. Let <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="151811_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula>. Repeating this process many times, a p-value (and confidence interval) can be computed based on the proportion of times <italic toggle="yes">D</italic><sup>*</sup> &lt; 0 as well as the proportion of times <italic toggle="yes">D</italic><sup>*</sup> = 0. (More precise details are given in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-24" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-22" hwp:rel-id="ref-62">c</xref>.) This method has been found to provide reasonably good control over the probability of a Type I error when both <italic toggle="yes">n</italic> and <italic toggle="yes">m</italic> are greater than equal to five. The R function medpb2 performs this technique.</p><p hwp:id="p-94">The method just described performs very well compared to alternative techniques. In fact, regardless of how large the sample sizes might be, the percentile bootstrap method is the only known method that continues to perform reasonably well when comparing medians and there are duplicated values. (Also see <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-25" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="sec" rid="s5c" hwp:id="xref-sec-26-1" hwp:rel-id="sec-26">section 5.3</xref>.)</p><p hwp:id="p-95">For the special case where the goal is to compare means, there is no method that provides reasonably accurate control over the Type I error probability for a relatively broad range of situations. In fairness, situations can be created where means perform well and indeed have higher power than methods based on a 20% trimmed mean or median. When dealing with perfectly symmetric distributions where outliers are unlikely to occur, methods based on means, and that allow heteroscedasticity, perform relatively well. But with small sample sizes, there is no satisfactory diagnostic tool indicating whether distributions satisfy these two conditions in an adequate manner. Generally, using means comes with the relatively high risk of poor control over the Type I error probability and relatively poor power.</p><p hwp:id="p-96">Switching to a 20% trimmed mean, the method derived by <xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Yuen (1974)</xref> performs fairly well even when the smallest sample size is six (cf. <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Özdemir et al., 2013</xref>). It can be applied with the R function yuen. (Yuen’s method reduces to Welch’s method for comparing means when there is no trimming.) When the smallest sample size is five, it can be unsatisfactory in situations where the percentile bootstrap method, used in conjunction with the median, continues to perform reasonably well. A rough rule is that the ability to control the Type I error probability improves as the amount of trimming increases. With small sample sizes, and when the goal is to compare means, it is unknown how to control the Type I error probability reasonably well over a reasonably broad range of situations.</p><p hwp:id="p-97">Another approach is to focus on <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>), the probability that a randomly sample observation from the first group is less than a randomly sample observation from the second group. This strategy is based in part on an estimate of the distribution of <italic toggle="yes">D</italic> = <italic toggle="yes">X</italic> – <italic toggle="yes">Y</italic>, the distribution of all pairwise differences between observations in each group.</p><p hwp:id="p-98">To illustrate this point, let <italic toggle="yes">X</italic><sub>1</sub>, … , <italic toggle="yes">X<sub>n</sub></italic> and <italic toggle="yes">Y</italic><sub>1</sub>,… , <italic toggle="yes">Y<sub>m</sub></italic> be random samples of size <italic toggle="yes">n</italic> and <italic toggle="yes">m</italic>, respectively. Let <italic toggle="yes">D<sub>ik</sub></italic> = <italic toggle="yes">X<sub>i</sub></italic> – <italic toggle="yes">Y<sub>k</sub></italic> (i = 1, … <italic toggle="yes">n</italic>; <italic toggle="yes">k</italic> = 1, … , <italic toggle="yes">m</italic>). Then the usual estimate of <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; Y) is simply the proportion of <italic toggle="yes">D<sub>ik</sub></italic> values less than zero. For instance, if <italic toggle="yes">X</italic> = (1, 2, 3) and <italic toggle="yes">Y</italic> = (1, 2.5, 4), then <italic toggle="yes">D</italic> = (0.0, 1.0, 2.0, -1.5, -0.5, 0.5, -3.0, -2.0, -1.0) and the estimate of <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>) is 4/9, the proportion of <italic toggle="yes">D</italic> values less than zero.</p><p hwp:id="p-99">Let <italic toggle="yes">μ<sub>x</sub>, μ<sub>y</sub></italic> and <italic toggle="yes"><sub>μD</sub></italic> denote the population means associated with <italic toggle="yes">X, Y</italic> and <italic toggle="yes">D</italic>, respectively. From basic principles, <italic toggle="yes">μ<sub>x</sub></italic> – <italic toggle="yes">μ<sub>y</sub></italic> = <italic toggle="yes">μ<sub>D</sub>.</italic> That is, the difference between two means is the same as the mean of all pairwise differences. However, let <italic toggle="yes">θ<sub>x</sub></italic>, <italic toggle="yes">θ<sub>y</sub></italic> and <italic toggle="yes">θ<sub>D</sub></italic> denote the population medians associated with <italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic> and <italic toggle="yes">D</italic>, respectively. For symmetric distributions, <italic toggle="yes">θ<sub>x</sub></italic> – <italic toggle="yes">θ<sub>y</sub></italic> = <italic toggle="yes">θ<sub>D</sub></italic>, but otherwise it is generally the case that <italic toggle="yes">θ<sub>x</sub></italic> – <italic toggle="yes">θ<sub>y</sub></italic> ≠ <italic toggle="yes">θ<sub>D</sub>.</italic> In other words, the difference between medians is typically not the same as the median of all pairwise differences. The same is true when using any amount of trimming greater than zero. Roughly, <italic toggle="yes">θ<sub>x</sub></italic> and <italic toggle="yes">θ<sub>y</sub></italic> reflect the typical response for each group, while <italic toggle="yes">θ<sub>D</sub></italic> reflects the typical difference between two randomly sampled participants, one from each group. Although less known, the second perspective can be instructive in many situations. For instance, in a clinical setting in which we want to know what effect to expect when randomly sampling a patient and a control participant.</p><p hwp:id="p-100">If two groups do not differ in any manner, <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>) = 0.5. Consequently, a basic goal is testing<disp-formula id="eqn3" hwp:id="disp-formula-3" hwp:rev-id="xref-disp-formula-3-1"><alternatives hwp:id="alternatives-7"><graphic xlink:href="151811_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives></disp-formula></p><p hwp:id="p-101">If this hypothesis is rejected, this indicates that it is reasonable to make a decision about whether <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>) is less than or greater than 0.5 It is readily verified that this is the same as testing<disp-formula id="eqn4" hwp:id="disp-formula-4" hwp:rev-id="xref-disp-formula-4-1"><alternatives hwp:id="alternatives-8"><graphic xlink:href="151811_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives></disp-formula></p><p hwp:id="p-102">An appeal of <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>) is that it is easily understood by non-statisticians, and it has practical importance for reasons summarized, among others, by <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Cliff (1996)</xref>, <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Ruscio (2008)</xref> and <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Newcombe (2006)</xref>. The Wilcoxon–Mann–Whitney (WMW) test is based on an estimate <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>). However, it is unsatisfactory in terms of making inferences about this probability because the estimate of the standard error assumes that the distributions are identical. If the distributions differ, an incorrect estimate of the standard error is being used. More modern methods deal with this issue. The method derived by <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">Cliff (1996)</xref> for testing <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-3-1" hwp:rel-id="disp-formula-3">(3)</xref> performs relatively well with small sample sizes and can be applied via the R function cidv2. (We are not aware of any commercial software package that contains this method.)</p><p hwp:id="p-103">However, there is the complication that for skewed distributions, differences among the means, for example, can be substantially smaller as well as substantially larger than differences among 20% trimmed means or medians. That is, regardless of how large the sample sizes might be, power can be substantially impacted by which measure of central tendency is used.</p></sec><sec id="s4b" hwp:id="sec-16" hwp:rev-id="xref-sec-16-1 xref-sec-16-2 xref-sec-16-3 xref-sec-16-4"><label>4.2</label><title hwp:id="title-18">Comparing Quantiles</title><p hwp:id="p-104">Rather than compare groups based on a single measure of central tendency, typically the mean, another approach is to compare multiple quantiles. For example, compare the quartiles, or all of the deciles, or even all quantiles. This provides more detail about where and how the two distributions differ (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">Rousselet, Pernet &amp; Wilcox, 2017</xref>). For example, the typical participants might not differ very much based on the medians, but the reverse might be true among low scoring individuals.</p><p hwp:id="p-105">First consider the goal of comparing all quantiles in a manner that controls the probability of one or more Type I errors among all the tests that are performed. Assuming random sampling only, <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Doksum and Sievers (1976)</xref> derived such a method that can be applied via the R function sband. The method is based on a generalization of the Kolmogorov–Smirnov test. A negative feature is that power can be adversely affected when there are tied values. And when the goal is to compare the more extreme quantiles, again power might be relatively low. A way of reducing these concerns is to compare the deciles using a percentile bootstrap method in conjunction with the quantile estimator derived by <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Harrell and Davis (1982)</xref>. This is easily done with the R functions qcomhd.</p><p hwp:id="p-106">Note that if the distributions associated with <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> do not differ, then <italic toggle="yes">D</italic> = <italic toggle="yes">X</italic> − <italic toggle="yes">Y</italic> will have a symmetric distribution about zero. Let <italic toggle="yes">x<sub>q</sub></italic> be the qth quantile of <italic toggle="yes">D</italic>, 0 &lt; <italic toggle="yes">q</italic> &lt; 0.5. In particular, it will be the case that <italic toggle="yes">x<sub>q</sub></italic> + <italic toggle="yes">x</italic><sub>1−<italic toggle="yes">q</italic></sub> = 0 when <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> have identical distributions. The median (2nd quartile) will be zero, and, for instance, the sum of the 3rd quartile (0.75 quantile) and the 1st quartile (0.25 quantile) will be zero. So this sum provides yet another perspective on how distributions differ (see illustrations in <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-4" hwp:rel-id="ref-45">Rousselet, Pernet &amp; Wilcox, 2017</xref>).</p><p hwp:id="p-107">Imagine, for example, that an experimental group is compared to a control group based on some measure of depressive symptoms. If <italic toggle="yes">x</italic><sub>0.25</sub> = −4 and <italic toggle="yes">x</italic><sub>0.75</sub> = 6, then for a single randomly sampled observation from each group, there is a sense in which the experimental treatment outweighs no treatment, because positive differences (beneficial effect) tend to be larger than negative differences (detrimental effect). The hypothesis <disp-formula id="eqn5" hwp:id="disp-formula-5"><alternatives hwp:id="alternatives-9"><graphic xlink:href="151811_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives></disp-formula> can be tested with the R function cbmhd. A confidence interval is returned as well. Current results indicate that the method provides reasonably accurate control over the Type I error probability when <italic toggle="yes">q</italic> = 0.25 and the sample sizes are greater than or equal to ten. For <italic toggle="yes">q</italic> = 0.1, sample sizes greater than or equal to twenty should be used (<xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Wilcox, 2012</xref>).</p></sec><sec id="s4c" hwp:id="sec-17" hwp:rev-id="xref-sec-17-1"><label>4.3</label><title hwp:id="title-19">Eliminate Outliers and Average the Remaining Values</title><p hwp:id="p-108">Rather than use means, trimmed means or the median, another approach is to use an estimator that down weights or eliminates outliers. For example, use the MAD-median to search for outliers, remove any that are found and average the remaining values. This is generally known as a modified one-step M-estimator (MOM). This approach might seem preferable to using a trimmed mean or median because trimming can eliminate points that are not outliers. But this issue is far from simple. Indeed, there are indications that when testing hypotheses, the expectation is that using a trimmed mean or median will perform better in terms of Type I errors and power (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-26" hwp:rel-id="ref-60">Wilcox, 2017a</xref>). However, there are exceptions: no single estimator dominates.</p><p hwp:id="p-109">As previously noted, an invalid strategy is to eliminate extreme values and apply conventional methods for means based on the remaining data, because the wrong standard error is used. Switching to a percentile bootstrap deals with this issue when using MOM as well as related estimators. The R function pb2gen applies this method.</p></sec><sec id="s4d" hwp:id="sec-18" hwp:rev-id="xref-sec-18-1"><label>4.4</label><title hwp:id="title-20">Comparing Dependent Variables</title><p hwp:id="p-110">Next, consider the goal of comparing two dependent variables. That is, the variables might be correlated. Based on the random sample (<italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">Y</italic><sub>1</sub>), … , (<italic toggle="yes">X<sub>n</sub></italic>, <italic toggle="yes">Y<sub>n</sub></italic>), let <italic toggle="yes">D<sub>i</sub></italic> = <italic toggle="yes">X<sub>i</sub></italic> – <italic toggle="yes">Y<sub>i</sub></italic> (<italic toggle="yes">i</italic> = 1, … , <italic toggle="yes">n</italic>). Even when <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> are correlated, <italic toggle="yes">μ<sub>x</sub></italic> – <italic toggle="yes">μ<sub>y</sub></italic> = <italic toggle="yes">μ<sub>D</sub></italic>, the difference between the population means is equal to the mean of the difference scores. But under general conditions this is not the case when working with trimmed means. When dealing with medians, for example, it is generally the case that <italic toggle="yes">θ<sub>x</sub></italic> – <italic toggle="yes">θ<sub>y</sub></italic> ≠ <italic toggle="yes">θ<sub>D</sub>.</italic></p><p hwp:id="p-111">If the distribution of <italic toggle="yes">D</italic> is symmetric and light-tailed (outliers are relatively rare), the paired t-test performs reasonably well. As we move toward a skewed distribution, at some point this is no longer the case for reasons summarized in <xref ref-type="sec" rid="s2a" hwp:id="xref-sec-3-2" hwp:rel-id="sec-3">section 2.1</xref>. Moreover, power and control over the probability of a Type I error are also a function of the likelihood of encountering outliers.</p><p hwp:id="p-112">There is a method for computing a confidence interval for <italic toggle="yes">θ<sub>D</sub></italic> for which the probability of a Type I error can be determined exactly assuming random sampling only (e.g., Hettmansperger &amp; McKean, 1998). In practice, a slight modification of this method is recommended that was derived by <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Hettmansperger and Sheather (1986)</xref>. So when sample sizes are very small, this method performs very well in terms of controlling the probability of a Type I error. And in general, it is an excellent method for making inferences about <italic toggle="yes">θ<sub>D</sub>.</italic> The method can be applied via the R function sintv2.</p><p hwp:id="p-113">As for trimmed means, with the focus still on <italic toggle="yes">D</italic>, a percentile bootstrap method can be used via the R function trimpb or wmcppb. Again, with 20% trimming, reasonably good control over the Type I error probability can be achieved. With <italic toggle="yes">n</italic> = 20, the percentile bootstrap method is better than the non-bootstrap method derived by <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">Tukey and McLaughlin (1963)</xref>. With large enough sample sizes the Tukey–McLaughlin method can be used in lieu of the percentile bootstrap method via the R function trimci, but it is unclear just how large the sample size must be.</p><p hwp:id="p-114">In some situations, there might be interest in comparing measures of central tendencies associated with the marginal distributions rather than the difference scores. Imagine, for example, participants consist of married couples. One issue might be the typical difference between a husband and his wife, in which case difference scores would be used. Another issue might be how the typical male compares to the typical female. So now the goal would be to test <italic toggle="yes">H</italic><sub>0</sub>: <italic toggle="yes">θ<sub>x</sub></italic> = <italic toggle="yes">θ<sub>y</sub></italic>, rather than <italic toggle="yes">H</italic><sub>0</sub>: <italic toggle="yes">θ<sub>D</sub></italic> = 0. The R function dmeppb tests the first of these hypotheses and performs relatively well, even when there are tied values. If the goal is to compare the marginal trimmed means, rather than make inferences about the trimmed mean of the difference scores, use the R function dtrimpb, or use wmcppb and set the argument dif=FALSE. When dealing with a moderately large sample size, the R function yuend can be used instead, but there is no clear indication just how large the sample size must be. A collection of quantiles can be compared with Dqcomhd and all of the quantiles can be compared via the function lband.</p><p hwp:id="p-115">Yet another approach is to use the classic sign test, which is aimed at making inferences about <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>). As is evident, this probability provides a useful perspective on the nature of the difference between the two dependent variables under study beyond simply comparing measures of central tendency. The R function signt performs the sign test, which by default uses the method derived by <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Agresti and Coull (1998)</xref>. If the goal is to ensure that the confidence interval has probability coverage at least 1 – <italic toggle="yes">α</italic>, rather than approximately equal to 1 – α, the <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Schilling and Doi (2014)</xref> method can be used by setting the argument SD=TRUE when using the R function signt. In contrast to the Schilling and Doi method, p-values can be computed when using the Agresti and Coull technique. Another negative feature of the Schilling and Doi method is that execution time can be extremely high even with a moderately large sample size.</p><p hwp:id="p-116">A criticism of the sign test is that its power might be lower than the Wilcoxon signed rank test. However, this issue is not straightforward. Moreover, the sign test can reject in situations where other conventional methods do not. Again, which method has the highest power depends on the characteristics of the unknown distributions generating the data. Also, in contrast to the sign test, the Wilcoxon signed rank test provides no insight into the nature of any difference that might exist without making rather restrictive assumptions about the underlying distributions. In particular, under general conditions, it does not compare medians or some other measure of central tendency as previously noted.</p></sec><sec id="s4e" hwp:id="sec-19" hwp:rev-id="xref-sec-19-1"><label>4.5</label><title hwp:id="title-21">More Complex Designs</title><p hwp:id="p-117">It is noted that when dealing with a one-way or higher ANOVA design, violations of the normality and homoscedasticity assumptions, associated with classic methods for means, become an even more serious issue in terms of both Type I error probabilities and power. Robust methods have been derived (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-27" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-23" hwp:rel-id="ref-62">c</xref>), but the many details go beyond the scope of this paper. However, a few points are worth stressing.</p><p hwp:id="p-118">Momentarily assume normality and homoscedasticity. Another important insight has to do with the role of the ANOVA F test versus post-hoc multiple comparison procedures such as the Tukey–Kramer method. In terms of controlling the probability of one or more Type I errors, is it necessary to first reject with the ANOVA F test? The answer is an unequivocal no. With equal sample sizes, the Tukey–Kramer method provides exact control. But if it is used only after the ANOVA F test rejects, this is no longer the case; it is lower than the nominal level (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Bernhardson, 1975</xref>). For unequal sample sizes, the probability of one or more Type I errors is less than or equal to the nominal level when using the Tukey–Kramer method. But if it is used only after the ANOVA F test rejects, it is even lower, which can negatively impact power. More generally, if an experiment aims to test specific hypotheses involving subsets of conditions, there is no obligation to first perform an ANOVA: the analyses should focus directly on the comparisons of interest using, for instance, the functions for linear contrasts listed below.</p><p hwp:id="p-119">Now consider non-normality and heteroscedasticity. When performing all pairwise com-parisons, for example, most modern methods are designed to control the the probability of one or more Type I errors without first performing a robust analog of the ANOVA F test. There are, however, situations where a robust analog of the ANOVA F test can help increase power (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-28" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, section 7.4).</p><p hwp:id="p-120">For a one-way design where the goal is to compare all pairs of groups, a percentile bootstrap method can be used via the R function linconpb. A non-bootstrap method is performed by lincon. For medians, use medpb. For an extension of Cliff’s method, use cidmul. Methods and corresponding R functions for both two-way and three-way designs, including techniques for dependent groups, are available as well; see <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-29" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-24" hwp:rel-id="ref-62">c</xref>).</p></sec><sec id="s4f" hwp:id="sec-20"><label>4.6</label><title hwp:id="title-22">Comparing Independent Correlations and Regression Slopes</title><p hwp:id="p-121">Next, consider two independent groups where for each group there is interest in the strength of the association between two variables. A common goal is to test the hypothesis that the strength of association is the same for both groups.</p><p hwp:id="p-122">Let <italic toggle="yes">ρ<sub>j</sub></italic> (<italic toggle="yes">j</italic> = 1, 2) be Pearson’s correlation for the <italic toggle="yes">j</italic>th group and consider the goal of testing<disp-formula id="eqn6" hwp:id="disp-formula-6" hwp:rev-id="xref-disp-formula-6-1 xref-disp-formula-6-2"><alternatives hwp:id="alternatives-10"><graphic xlink:href="151811_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives></disp-formula></p><p hwp:id="p-123">Various methods for accomplishing this goal are known to be unsatisfactory (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Wilcox, 2009</xref>). For example, one might use Fisher’s r-to-z transformation, but it follows immediately from results in <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Duncan and Layard (1973)</xref> that this approach performs poorly under general condi-tions. Methods that assume homoscedasticity, as depicted in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>, can be unsatisfactory as well. As previously noted, when there is an association (the variables are dependent), and in particular there is heteroscedasticity, a practical concern is that the wrong standard error is being used when testing hypotheses about the slopes. That is, the derivation of the test statistic is valid when there is no association; independence implies homoscedasticity. But under general conditions it is invalid when there is heteroscedasticity. This concern extends to inferences made about Pearson’s correlation.</p><p hwp:id="p-124">There are two methods that perform relatively well in terms of controlling the Type I error probability. The first is based on a modification of the basic percentile bootstrap method. Imagine that <xref ref-type="disp-formula" rid="eqn6" hwp:id="xref-disp-formula-6-1" hwp:rel-id="disp-formula-6">(6)</xref> is rejected if the confidence interval for <italic toggle="yes">ρ</italic><sub>1</sub> – <italic toggle="yes">ρ</italic><sub>2</sub> does not contain zero. So the Type I error probability depends on the width of this confidence interval. If it is too short, the actual Type I error probability will exceed 0.05. With small sample sizes this is exactly what happens when using the basic percentile bootstrap method. The modification consists of widening the confidence interval for <italic toggle="yes">ρ</italic><sub>1</sub> – <italic toggle="yes">ρ</italic><sub>2</sub> when the sample size is small. The amount it is widened depends on the sample sizes. The method can be applied via the R function twopcor. A limitation is that this method can be used only when the Type I error is 0.05 and it does not yield a p-value.</p><p hwp:id="p-125">The second approach is to use a method that estimates the standard error in a manner that deals with heteroscedasticity. When dealing with the slope of the least squares regression line, several methods are now available for getting valid estimates of the standard error when there is heteroscedasticity (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-30" hwp:rel-id="ref-60">Wilcox, 2017a</xref>). One of these is called the HC4 estimator, which can be used to test <xref ref-type="disp-formula" rid="eqn6" hwp:id="xref-disp-formula-6-2" hwp:rel-id="disp-formula-6">(6)</xref> via the R function twohc4cor.</p><p hwp:id="p-126">As previously noted, Pearson’s correlation is not robust: even a single outlier might substantially impact its value giving a distorted sense of the strength of the association among the bulk of the points. Switching to Kendall’s tau or Spearman’s rho, now a basic percentile bootstrap method can be used to compare two independent groups, in a manner that allows heteroscedasticity, via the R function twocor. As noted in <xref ref-type="sec" rid="s2c" hwp:id="xref-sec-5-4" hwp:rel-id="sec-5">section 2.3</xref>, the skipped correlation can be used via the R function scorci.</p><p hwp:id="p-127">The slopes of regression lines can be compared as well using methods that allow heteroscedasticity. For least squares regression, use the R function ols2ci. For robust regression estimators, use reg2ci.</p></sec><sec id="s4g" hwp:id="sec-21"><label>4.7</label><title hwp:id="title-23">Comparing Correlations, the Overlapping Case</title><p hwp:id="p-128">Now consider a single dependent variable <italic toggle="yes">Y</italic> and two independent variables, <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub>. A common and fundamental goal is understanding the relative importance of <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub> in terms of their association with <italic toggle="yes">Y.</italic> A typical mistake in neuroscience is to perform two separate tests of associations, one between <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">Y</italic>, another between <italic toggle="yes">X</italic><sub>2</sub> and <italic toggle="yes">Y</italic>, without explicitly comparing the association strengths between the independent variables (<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Nieuwenhuis et al. 2011</xref>). For instance, reporting that one test is significant, and the other is not, cannot be used to conclude that the associations themselves differ. A common example would be when an association is estimated between each of two brain measurements and a behavioural outcome.</p><p hwp:id="p-129">Their are many methods for estimating which independent variable is more important, many of which are known to be unsatisfactory (e.g., <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-25" hwp:rel-id="ref-62">Wilcox, 2017c</xref>, section 6.13). Stepwise regression is among the unsatisfactory techniques for reasons summarized by <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Montgomery and Peck (1992</xref>, Section 7.2.3) as well as <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Derksen and Keselman (1992)</xref>. Regardless of their relative merits, a practical limitation is that they do not reflect the strength of empirical evidence that the most important independent variable has been chosen. One strategy is to test <italic toggle="yes">H</italic><sub>0</sub>: <italic toggle="yes">ρ</italic><sub>1</sub> = <italic toggle="yes">ρ</italic><sub>2</sub>, where now <italic toggle="yes">ρ<sub>j</sub></italic> (<italic toggle="yes">j</italic> = 1, 2) is the correlation between <italic toggle="yes">Y</italic> and <italic toggle="yes">X<sub>j</sub>.</italic> This can be done with the R function TWOpov. When dealing with robust correlations, use the function twoDcorR.</p><p hwp:id="p-130">However, a criticism of this approach is that it does not take into account the nature of the association when both independent variables are included in the model. This is a concern because the strength of the association between <italic toggle="yes">Y</italic> and <italic toggle="yes">X</italic><sub>1</sub> can depend on whether <italic toggle="yes">X</italic><sub>2</sub> is included in the model as illustrated in <xref ref-type="sec" rid="s5d" hwp:id="xref-sec-27-2" hwp:rel-id="sec-27">section 5.4</xref>. There is now a robust method for testing the hypothesis that there is no difference in the association strength when both <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub> are included in the model. (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-31" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">2016</xref>). Heteroscedasticity is allowed. If, for example, there are three independent variables, one can test the hypothesis that the strength of the association for the first two independent variables is equal to the strength of the association for the third independent variable. The method can be applied with the R function regIVcom. A modification and extension of the method has been derived when there is curvature (Wilcox, in press), but it is limited to two independent variables.</p></sec><sec id="s4h" hwp:id="sec-22"><label>4.8</label><title hwp:id="title-24">ANCOVA</title><p hwp:id="p-131">The simplest version of the analysis of covariance (ANCOVA) consists of comparing the regression lines associated with two independent groups when there is a single independent variable. The classic method makes several restrictive assumptions: the regression lines are parallel, for each regression line there is homoscedasticity, the variance of the dependent variable is the same for both groups, normality, and a straight regression line provides an adequate approximation of the true association. Violating any of these assumptions is a serious practical concern. Violating two or more of these assumptions makes matters worse. There is now vast array of more modern methods that deal with violations of all of these assumptions <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-32" hwp:rel-id="ref-60">Wilcox (2017a</xref>, chapter 12). These newer techniques can substantially increase power compared to the classic ANCOVA technique, and perhaps more importantly they can provide a deeper and more accurate understanding of how the groups compare. But the many details go beyond the scope of this paper.</p><p hwp:id="p-132">As noted in the introduction, curvature is a more serious concern than is generally recognized. One strategy, as a partial check on the presence of curvature, is to simply plot the regression lines associated with two groups using the R functions lplot2g and rplot2g. When using these functions, as well as related functions, it can be vitally important to check on the impact of removing outliers among the independent variables. This is easily done with functions mentioned here by setting the argument xout=TRUE. If these plots suggest that curvature might be an issue, consider the R functions ancova and ancdet. This latter function applies method TAP in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-33" hwp:rel-id="ref-60">Wilcox (2017a</xref>, section 12.2.4) and can provide more detailed information about where and how two regression lines differ compared to the function ancova. These functions are based on methods that allow heteroscedasticity, non-normality, and they eliminate the classic assumption that the regression lines are parallel. For two independent variables, see <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-34" hwp:rel-id="ref-60">Wilcox (2017a</xref>, section 12.4). If there is evidence that curvature is not an issue, again there are very effective methods that allow heteroscedasticity as well as non-normality (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-35" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, section 12.1).</p></sec></sec><sec id="s5" hwp:id="sec-23" hwp:rev-id="xref-sec-23-1 xref-sec-23-2 xref-sec-23-3 xref-sec-23-4 xref-sec-23-5"><label>5</label><title hwp:id="title-25">Some Illustrations</title><p hwp:id="p-133">Using data from several studies, this section illustrates modern methods and how they contrast. Extant results suggest that robust methods have a relatively high likelihood of maxi-mizing power, but as previously stressed, no single method dominates in terms of maximizing power. Another goal in this section is to underscore the suggestion that multiple perspectives can be important. More complete descriptions of the results, as well as the R code that was used, are available on figshare (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">Wilcox &amp; Rousselet, 2017</xref>). The figshare reproducibility package also contains a more systematic assessment of type I error and power in the one-sample case. (See notebook <monospace>power_onesample.pdf</monospace>).</p><sec id="s5a" hwp:id="sec-24"><label>5.1</label><title hwp:id="title-26">Spatial Acuity for Pain</title><p hwp:id="p-134">The first illustration stems from <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Mancini et al. (2014)</xref> who report results aimed at providing a whole-body mapping of spatial acuity for pain. (Also see <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Mancini, 2016</xref>.) Here the focus is on their second experiment. Briefly, spatial acuity was assessed by measuring 2-point discrimination (2PD) thresholds for both pain and touch in 11 body territories. One goal was to compare touch measures taken at different body parts: forehead, shoulder, forearm, hand, back and thigh. Plots of the data are shown in <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6A</xref> for the six body parts. The sample size is <italic toggle="yes">n</italic> = 10.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-6"><p hwp:id="p-135">Data from <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">Mancini et al. 2014</xref>. Panel A shows the marginal distributions of thresholds at locations FH = forehead, S = shoulder, FA = forearm, H = hand, B = back and T = thigh. Individual participants (n = 10) are shown as colored disks and lines. The medians across participants are shown in black. Panel B shows the distributions of all pairwise differences between the conditions shown in A. In each stripchart (1D scatterplot), the black horizontal line marks the median of the differences.</p></caption><graphic xlink:href="151811_fig6" position="float" orientation="portrait" hwp:id="graphic-12"/></fig><p hwp:id="p-136">Their analyses were based on the ANOVA F test, followed by paired t-tests when the ANOVA F test was significant. Their significant results indicate that the distributions differ, but because the ANOVA F test is not a robust method when comparing means, there is some doubt about the nature of the differences. So one goal is to determine in which situations robust methods give similar results. And for the non-significant results, there is the issue of whether an important difference was missed due to using the ANOVA F test and Student’s t-test.</p><p hwp:id="p-137">First we describe a situation where robust methods based a median and a 20% trimmed mean give reasonably similar results. Comparing foot and thigh pain measures based on Student’s t-test, the 0.95 confidence interval = [−0.112, 0.894] and the p-value is 0.112. For a 20% trimmed mean the 0.95 confidence interval is [−0.032, 0.778] and the p-value is 0.096. As for the median, the corresponding results were [−0.085, 0.75] and 0.062.</p><p hwp:id="p-138">Next, all pairwise comparisons, based on touch, were performed for the following body parts: forehead, shoulder, forearm, hand, back and thigh. <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6B</xref> shows a plot of the difference scores for each pair of body parts. The probability of one or more Type I errors was controlled using an improvement on the Bonferroni method derived by <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Hochberg (1988)</xref>. The simple strategy of using paired t-tests if the ANOVA F rejects does not control the probability of one or more Type I errors. If paired t-tests are used without controlling the probability of one or more Type I errors, as done by Mancini et al., 14 of the 15 hypotheses are rejected. If the probability of one or more Type I errors is controlled using Hochberg’s method, the following results were obtained. Comparing means via the R function wmcp (and the argument tr=0), 10 of the 15 hypotheses were rejected. Using medians via the R function dmedpb, 11 were rejected. As for the 20% trimmed mean, using the R function wmcppb, now 13 are rejected illustrating the point made earlier that the choice of method can make a practical difference.</p><p hwp:id="p-139">It is noted that in various situations, using the sign test, the estimate of <italic toggle="yes">P</italic>(<italic toggle="yes">X</italic> &lt; <italic toggle="yes">Y</italic>) was equal to one, which provides a useful perspective beyond using a mean or median.</p></sec><sec id="s5b" hwp:id="sec-25" hwp:rev-id="xref-sec-25-1"><label>5.2</label><title hwp:id="title-27">Receptive Fields in Early Visual Cortex</title><p hwp:id="p-140">The next illustrations are based on data analyzed by <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Talebi and Baker (2016)</xref> and presented in their <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure 9</xref>. The goal was to estimate visual receptive field (RF) models of neurones from a cat’s visual cortex using natural image stimuli. The authors provided a rich quantification of the neurones’ responses and demonstrated the existence of three functionally distinct categories of simple cells. The total sample size is 212.</p><p hwp:id="p-141">There are three measures: latency, duration, and a direction selectivity index (dsi). For each of these measures there are three independent categories: nonoriented (nonOri) cells (n=101), expansive oriented (expOri) cells (n=48) and compressive oriented (compOri) cells (n=63).</p><p hwp:id="p-142">First focus on latency. Talebi et al. used Student’s t-test to compare means. Comparing the means for nonOri and expOri, no significant difference is found. But an issue is whether Student’s t-test might be missing an important difference. The plot of the distributions shown in the top row of <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7<bold>A</bold></xref>, which was created with the R function g5plot, provides a partial check on this possibility. As is evident, the distributions for nonOri and expOri are very similar suggesting that no method will yield a significant result. Using error bars is less convincing because important differences might exist when focusing instead on the median, 20% trimmed mean, or some other aspect of the distributions.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-7"><p hwp:id="p-143">Response latencies A and durations B from cells recorded by <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-2" hwp:rel-id="ref-50">Talebi and Baker (2016)</xref>. Row 1: Estimated distributions of latency and duration measures for three categories: nonOri, expOri, and compOri. Rows 2 and 3 show shift functions based on comparisons between the deciles of two groups. The deciles of one group are on the x-axis; the difference between the deciles of the two groups is on the y-axis. The black dotted line is the difference between deciles, plotted as function of the deciles in one group. The grey dotted lines mark the 95% bootstrap confidence interval, which is also highlighted by the grey shaded area. Row 2: Differences between deciles for nonOri versus expOri, plotted as a function of the deciles for nonOri. Row 3: Differences between deciles for nonOri versus compOri, plotted as a function of the deciles for nonOri.</p></caption><graphic xlink:href="151811_fig7" position="float" orientation="portrait" hwp:id="graphic-13"/></fig><p hwp:id="p-144">As noted in <xref ref-type="sec" rid="s4b" hwp:id="xref-sec-16-2" hwp:rel-id="sec-16">section 4.2</xref>, comparing the deciles can provide a more detailed understanding of how groups differ. The second row of <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7</xref> shows the estimated difference between the deciles for the nonOri group versus the expOri group. The vertical dotted line indicates the median. Also shown are confidence intervals (computed via the R function qcomhd) having, approximately, simultaneous probability coverage equal to 0.95. That is, the probability of one or more Type I errors is approximately 0.05. In this particular case, differences between the deciles are more pronounced as we move from the lower to the upper deciles, but again no significant differences are found.</p><p hwp:id="p-145">The third row shows the difference between the deciles for nonOri versus compOri. Again the magnitude of the differences becomes more pronounced moving from low to high measures. Now all of the deciles differ significantly except the 0.1 quantiles.</p><p hwp:id="p-146">Next, consider durations in column B of <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7</xref>. Comparing nonOri to expOri using means, 20% trimmed means and medians, the corresponding p-values are 0.001, 0.005 and 0.009. Even when controlling the probability of one or more Type I errors using Hochberg’s method, all three reject at the 0.01 level. So a method based means rejects at the 0.01 level, but this merely indicates that the distributions differ in some manner. To provide an indication that the groups differ in terms of some measure of central tendency, using 20% trimmed means and medians is more satisfactory. The plot in row 2 of <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Figure 7B</xref> confirms an overall shift between the two distributions, and suggests a more specific pattern, with increasing differences in the deciles beyond the median.</p><p hwp:id="p-147">Comparing nonOri to compOri, significant results were again obtained using means, 20% trimmed means and medians, the largest p-value is p=0.005. In contrast, qcomhd indicates a significant difference for all of the deciles excluding the 0.1 quantile. As can be seen from the last row in <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Figure 7B</xref>, once more the magnitude of the differences between the deciles increases as we move from the lower to the upper deciles. Again, this function provides a more detailed understanding of where and how the distributions differ significantly.</p></sec><sec id="s5c" hwp:id="sec-26" hwp:rev-id="xref-sec-26-1"><label>5.3</label><title hwp:id="title-28">Mild Traumatic Brain Injury</title><p hwp:id="p-148">The illustrations in this section stem from a study dealing with mild traumatic brain injury (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Almeida-Suhett et al., 2014</xref>). Briefly, 5-6 week old male, Sprague–Dawley rats received a mild controlled cortical impact (CCI) injury. The dependent variable used here is the stereologically estimated total number of GAD-67-positive cells in the basolateral amygdala (BLA). Measures 1 and 7 days after surgery were compared to the sham-treated control group that received a craniotomy, but no CCI injury. A portion of their analyses focused on the ipsilateral sides of the BLA. Boxplots of the data are shown in <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref>. The sample sizes are 13, 9 and 8, respectively.</p><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8:</label><caption hwp:id="caption-8"><p hwp:id="p-149">Boxplots for the contralateral and ipsilateral sides of the BLA</p></caption><graphic xlink:href="151811_fig8" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><p hwp:id="p-150">Almeida-Suhett et al. compared means using an ANOVA F test followed by Bonferroni post-hoc test. Comparing both day 1 and day 7 measures to the sham group based on Student’s t-test, the p-values are 0.0375 and <italic toggle="yes">&lt;</italic> 0.001, respectively. So, if the Bonferroni method is used, the day 1 group does not differ significantly from the sham group when testing at the 0.05 level. However, using Hochberg’s improvement on the Bonferroni method, now the reverse decision is made.</p><p hwp:id="p-151">Here, the day 1 group was compared again to the sham group based a percentile bootstrap method for comparing both 20% trimmed means and medians, as well as Cliff’s improvement on the WMW test. The corresponding p-values are 0.024, 0.086 and 0.040. If 20% trimmed means are compared instead with Yuen’s method, the p-value is p=0.079, but due to the relatively small sample sizes, a percentile bootstrap would be expected to provide more accurate control over the Type I error probability. The main point here is that the choice between Yuen and a percentile bootstrap method can make a practical difference. The boxplots suggest that sampling is from distributions that are unlikely to generate outliers, in which case a method based on the usual sample median might have relatively low power. When outliers are rare, a way of comparing medians that might have more power is to use instead the Harrell–Davis estimator mentioned in <xref ref-type="sec" rid="s4b" hwp:id="xref-sec-16-3" hwp:rel-id="sec-16">section 4.2</xref> in conjunction with a percentile bootstrap method. Now p=0.049. Also, testing <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-4-1" hwp:rel-id="disp-formula-4">(4)</xref> with the R function wmwpb, p=0.031 So focusing on <italic toggle="yes">θ<sub>D</sub></italic>, the median of all pairwise differences, rather than the individual medians, can make a practical difference.</p><p hwp:id="p-152">In summary, when comparing the sham group to the Day 1 group, all of the methods that perform relatively well when sample sizes are small, described in <xref ref-type="sec" rid="s4a" hwp:id="xref-sec-15-3" hwp:rel-id="sec-15">section 4.1</xref>, reject at the 0.05 level except the percentile bootstrap method based on the usual sample median. Taken as a whole, the results suggest that measures for the sham group are typically higher than measures based on day 1 group. As for the day 7 data, now all of the methods used for the day 1 data have p-values less than or equal to 0.002.</p><p hwp:id="p-153">The same analyses were done using the contralateral sides of the BLA. Now the results were consistent with those based on means: none are significant for day 1. As for the day 7 measures, both conventional and robust methods indicate significant results.</p></sec><sec id="s5d" hwp:id="sec-27" hwp:rev-id="xref-sec-27-1 xref-sec-27-2"><label>5.4</label><title hwp:id="title-29">Fractional Anisotropy and Reading Ability</title><p hwp:id="p-154">The next illustrations are based on data dealing with reading skills and structural brain development (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Houston et al., 2014</xref>). The general goal was to investigate maturational volume changes in brain reading regions and their association with performance on reading measures. The statistical methods used were not made explicit. Presumably they were least squares regression or Pearson’s correlation coupled with the usual Student’s t-tests. The ages of the participants ranged between 6 and 16. After eliminating missing values, the sample size is <italic toggle="yes">n</italic> = 53. (It is unclear how Houston et al. dealt with missing values.)</p><p hwp:id="p-155">As previously indicated, when dealing with regression, it is prudent to begin with a smoother as a partial check on whether assuming a straight regression line appears to be reasonable. Simultaneously, the potential impact of outliers needs to be considered. In exploratory studies, it is suggested that results based on both Cleveland’s smoother and the running interval smoother be examined. (Quantile regression smoothers are another option that can be very useful; use the R function qsm.)</p><p hwp:id="p-156">Here we begin by using the R function lplot (Cleveland’s smoother) to estimate the regression line when the goal is to estimate the mean of the dependent variable for some given value of an independent variable. <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Figure 9<bold>A</bold></xref> shows the estimated regression line when using age to predict left corticospinal measures (CST.L). <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Figure 9<bold>B</bold></xref> shows the estimate when a GORT fluency (GORT.FL) measure of reading ability (<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Wiederhold et al., 2001</xref>) is taken to be the dependent variable. The shaded areas indicate a 0.95 confidence region that contains the true regression line. In these two situations, assuming a straight regression line seems like a reasonable approximation of the true regression line.</p><fig id="fig9" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3 xref-fig-9-4 xref-fig-9-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;151811v1/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9:</label><caption hwp:id="caption-9"><p hwp:id="p-157">Non-parametric estimate of the regression line for predicting GORT.FL with CST.L</p></caption><graphic xlink:href="151811_fig9" position="float" orientation="portrait" hwp:id="graphic-15"/></fig><p hwp:id="p-158"><xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-4" hwp:rel-id="F9">Figure 9<bold>C</bold></xref> shows the estimated regression line for predicting GORT.FL with CST.L. Note the dip in the regression line. One possibility is that the dip reflects the true regression line, but another explanation is that it is due to outliers among the dependent variable. (The R function outmgv indicates that the upper GORT.FL values are outliers.) Switching to the running interval smoother (via the R function rplot), which uses a 20% trimmed mean to estimate the typical GORT.FL value, now the regression line appears to be quite straight. (For details, see the figshare file mentioned at the beginning of this section.)</p><p hwp:id="p-159">The presence of curvature can depend on which variable is taken to be the independent variable. <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-5" hwp:rel-id="F9">Figure 9<bold>D</bold></xref> illustrates this point by using CST.L as the dependent variable and GORT.FL as the independent variable, in contrast to 9<bold>C</bold>. Note how the line increases and then curves down.</p><p hwp:id="p-160">Using instead the running-interval smoother, where the goal is to estimate the 20% trimmed mean of the dependent variable, now there appears to be a distinct bend approximately at GORT.FL=70. For GORT.FL less than 70 the regression line appears to quite straight and the slope was found to be significant (p=0.007) based on the R function regci, which uses the robust Theil–Sen estimator by default. (It is designed to estimate the median of the dependent variable.) For GORT.FL greater than 70, again the regression line is reasonably straight, but now the slope is negative and does not differ significantly from zero (p=0.27). Moreover, testing the hypothesis that these two slopes are equal (via the R function reg2ci), p=0.027, which provides additional evidence that there is curvature. So a more robust smoother suggests that there is a positive association up to about GORT.FL=70, after which the association is much weaker and possibly nonexistent.</p><p hwp:id="p-161">A common strategy for dealing with curvature is to include a quadratic term in the regression model. More generally, one might try to straighten a regression by replacing the independent variable <italic toggle="yes">X</italic> with <italic toggle="yes">X<sup>a</sup></italic> for some suitable choice for <italic toggle="yes">a</italic>. However, for the situation at hand, the half slope ratio method (e.g., <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-36" hwp:rel-id="ref-60">Wilcox, 2017a</xref>, 11.4) does not support this strategy. It currently seems that smoothers provide a more satisfactory approach to dealing with curvature.</p><p hwp:id="p-162">Now consider the goal of determining whether age or CST.L is more important when GORT.FL is the dependent variable. A plot of the regression surface when both independent variables are used to predict GORT.FL suggests that the regression surface is well approximated by a plane. (Details are in the figshare document.) Testing the hypothesis that the two independent variables are equally important via the R function regIVcom indicates that age is more important. (This function uses the Theil–Sen estimator by default.) Moreover, the ratio of the strength of the individual associations is 53. Using least squares regression instead (via regIVcom but with the argument regfun=ols), again age is more important and now the ratio of the strength of the individual associations is 9.85.</p><p hwp:id="p-163">Another measure of reading ability that was used is the Woodcock–Johnson (WJ) basic reading skills composite index (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Woodcock, et al., 2001</xref>). Here we consider the extent the GORT (raw) rate score is more important than age when predicting the the WJ word attack (raw) score. Pearson’s correlation for the word attack score and age is 0.68 (p <italic toggle="yes">&lt;</italic> 0.001). The correlation between the GORT rate score and the word attack score is 0.79 (p <italic toggle="yes">&lt;</italic> 0.001). Comparing these correlations via the R function twohc4cor, no significant difference is found at the 0.05 level (p=0.11). But when both of these independent variables are entered into the model, and again the Theil–Sen regression estimator is used, a significant result is obtained (p&lt; 0.001): the GORT rate score is estimated to be more important. In addition, the strength of the association between age and the word attack score is estimated to be close to zero. Using instead least squares regression, p = 0.02 and the correlation between age and WJ word attack score drops to 0.012. So both methods indicate that the GORT rate score is more important, with the result based on a robust regression estimator providing more compelling evidence that this is the case. This illustrates a point made earlier that the relative importance of the independent variables can depend on which independent variables are included in the model.</p></sec></sec><sec id="s6" hwp:id="sec-28"><label>6</label><title hwp:id="title-30">A Suggested Guide</title><p hwp:id="p-164">While no single method is always best, the following guide is suggested when comparing groups or studying associations.
<list list-type="bullet" hwp:id="list-2"><list-item hwp:id="list-item-5"><p hwp:id="p-165">Plot the data. Error bars are popular, but they are limited regarding the information they convey, regardless of whether they are based on the standard deviation or an estimate of the standard error. Better are scatterplots, boxplots or violin plots. For small sample sizes, scatterplots should be the default. If the sample sizes are not too small, plots of the distributions can be very useful, but there is no agreed upon guideline regarding just how large the sample size should be. For the moment, we suggest checking both boxplots and plots of the distributions when the sample size is <italic toggle="yes">n</italic> ≥ 30 with the goal of getting different perspectives on the nature of the distributions. It is suggested that kernel density estimators, rather than histograms, be used for reasons illustrated in <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-13" hwp:rel-id="ref-61">Wilcox (2017b)</xref>. (The R functions akerd and g5plot use a kernel density estimator.) For discrete data, where the number of possible values for the outcome variable is relatively small, also consider a plot of the relative frequencies. The R function splot is one possibility. When comparing two groups consider the R function splotg2. For more details regarding plots, see <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Weissgerber et al. (2015)</xref> and <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-5" hwp:rel-id="ref-45">Rousselet et al. (2017)</xref>.</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-166">For very small sample sizes, say less than or equal to ten, consider the methods in <xref ref-type="sec" rid="s4b" hwp:id="xref-sec-16-4" hwp:rel-id="sec-16">sections 4.2</xref> and <xref ref-type="sec" rid="s4c" hwp:id="xref-sec-17-1" hwp:rel-id="sec-17">4.3</xref>.</p></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-167">Use a method that allows heteroscedasticity. If the homoscedasticity assumption is true, in general little is lost when using a heteroscedastic method. But as the degree of heteroscedasticity increases, at some point methods that allow heteroscedasticity can make a practical difference in terms of both Type I errors and power. Put more broadly, avoid methods that use an incorrect estimate of the standard error when groups differ or when dealing with regression and there is an association. These methods include t-tests and ANOVA F tests on means, the WMW test, as well as conventional methods for making inferences about measures of association and the parameters of the usual linear regression model.</p></list-item><list-item hwp:id="list-item-8"><p hwp:id="p-168">Be aware of the limitations of methods based on means: they have a relatively high risk of poor control over the Type I error probability, as well as poor power. Another possible concern is that when dealing with skewed distributions, the mean might be an unsatisfactory summary of the typical response. Results based on means are not necessarily inaccurate, but relative to other methods that might be used, there are serious practical concerns that are difficult to address. Importantly, when using means, even a significant result can yield a relatively inaccurate and unrevealing sense of how distributions differ, and a non-significant result cannot be used to conclude that distributions do not differ.</p></list-item><list-item hwp:id="list-item-9"><p hwp:id="p-169">As a useful alternative to comparisons based on means, consider using a shift function or some other method for comparing multiple quantiles. Sometimes these methods can provide a deeper understanding of where and how groups differ that has practical value as illustrated in <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Figure 7</xref>. There is even the possibility that they yield significant results when methods based on means, trimmed means and medians do not. For discrete data, where the variables have a limited number of possible values, consider the R function binband. (See <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-26" hwp:rel-id="ref-62">Wilcox, 2017c</xref>, section 12.1.17; or <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-37" hwp:rel-id="ref-60">Wilcox, 2017a</xref> section 5.8.5.)</p></list-item><list-item hwp:id="list-item-10"><p hwp:id="p-170">When checking for outliers, use a method that avoids masking. This eliminates any method based on the mean and variance. <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-38" hwp:rel-id="ref-60">Wilcox (2017a</xref>, section 6.4) summarizes methods designed for multivariate data. (The R functions outpro and outmgv use methods that perform relatively well.)</p></list-item><list-item hwp:id="list-item-11"><p hwp:id="p-171">Be aware that the choice of method can make a substantial difference. For example, highly non-significant results can become significant when switching from a method based on the mean to a 20% trimmed mean or median. The reverse can happen where methods based on means are significant but robust methods are not. In this latter situation, the reason might be that confidence intervals based on means are highly inaccurate. Resolving whether this is the case is difficult at best based on current technology. Consequently, it is prudent to consider the robust methods outlined in this paper.</p></list-item><list-item hwp:id="list-item-12"><p hwp:id="p-172">When dealing with regression or measures of association, use modern methods for checking on the impact of outliers. When using regression estimators, dealing with outliers among the independent variables is straightforward via the R functions mentioned here: set the argument xout=TRUE. As for outliers among the dependent variable, use some robust regression estimator. The Theil–Sen estimator is relatively good, but arguments can be made for using certain extensions and alternative techniques. When there are one or two independent variables, and the sample size is not too small, check the plots returned by smoothers. This can be done with the R functions rplot and lplot. Other possibilities and their relative merits are summarized in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-39" hwp:rel-id="ref-60">Wilcox (2017a)</xref>.</p></list-item></list></p></sec><sec id="s7" hwp:id="sec-29"><label>7</label><title hwp:id="title-31">Concluding Remarks</title><p hwp:id="p-173">It is not being suggested that methods based on means should be completely abandoned or have no practical value. Instead, complete reliance on conventional methods can result in a superficial, misleading and relatively uninformative understanding of how groups compare. In addition, they might provide poor control over the Type I error probability and power. Similar concerns plague least squares regression and Pearson’s correlation.</p><p hwp:id="p-174">When a method fails to reject, this leaves open the issue of whether a significant result was missed due to the method used. From this perspective, multiple tests can be informative. However, there are two competing goals that need to be considered. The first is that when testing multiple hypotheses, this can increase the probability of one or more Type I errors. From basic principles, if, for example, five tests are performed at the 0.05 level, and all five hypotheses are true, the expected number of Type I errors is 0.25. But the more common focus is on the probability of one or more Type I errors rather than the expected number of Type I errors. The probability of one or more Type I errors will be greater than 0.05, but by how much is difficult to determine exactly due to the dependence among the tests that are performed. Improvements on the Bonferroni method deal with this issue (e.g., <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Hochberg, 1988</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Hommel, 1988</xref>) and are readily implemented via the R function p.adjust. But the more tests that are performed, such adjustments come at the cost of lower power. Simultaneously, ignoring multiple perspectives runs the risk of not achieving a deep understanding of how groups compare. Also, as noted in the previous section, if methods based on means are used, it is prudent to check the extent robust methods give similar results.</p><p hwp:id="p-175">An important issue not discussed here is robust measures of effect size. When both conventional and robust methods reject, the method used to characterize how groups differ can be crucial. Cohen’s <italic toggle="yes">d</italic>, for example, is not robust simply because it is based on the means and variances. Robust measures of effect size are summarized in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-40" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-27" hwp:rel-id="ref-62">c</xref>). Currently, efforts are being made to extend and generalize these measures.</p><p hwp:id="p-176">Another important issue is the implication of modern insights in terms of the massive number of published papers using conventional methods. These insights do not necessarily imply that these results are incorrect. There are conditions where classic methods perform reasonably well. But there is a clear possibility that misleading results were obtained in some situations. One of the main concerns is whether important differences or associations have been missed. Some of the illustrations in <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-41" hwp:rel-id="ref-60">Wilcox (2017a</xref>, <xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-14" hwp:rel-id="ref-61">b</xref>, <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-28" hwp:rel-id="ref-62">c</xref>), for example, reanalyzed data from studies dealing with regression where the simple act of removing outliers among the independent variable resulted in a highly non-significant result becoming significant at the 0.05 level. As illustrated here, non-significant results can become significant when using a more modern method for comparing measures of central tendency. There is also the possibility that a few outliers can result in a large Pearson correlation when in fact there is little or no association among the bulk of the data (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Rousselet &amp; Pernet 2012</xref>). <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-29" hwp:rel-id="ref-62">Wilcox (2017c)</xref> mentions one unpublished study where this occurred. So the issue is not whether modern robust methods can make a practical difference. Rather, the issue is how often this occurs.</p><p hwp:id="p-177">Finally, there is much more to modern methods beyond the techniques and issues described here (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-42" hwp:rel-id="ref-60">Wilcox, 2017a</xref>). Included are additional methods for studying associations as well as substantially better techniques for dealing with ANCOVA. As mentioned in the introduction, there are introductory textbooks that include the basics of modern advances and insights. But the difficult task of modernizing basic training for the typical neuroscientist remains.</p></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-32">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Agresti A."><surname>Agresti</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Coull B. A."><surname>Coull</surname>, <given-names>B. A.</given-names></string-name> (<year>1998</year>). <article-title hwp:id="article-title-2">Approximate is better than “exact” for interval estimation of binomial proportions</article-title>. <source hwp:id="source-1">American Statistician</source>, <volume>52</volume>, <fpage>119</fpage>–<lpage>126</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Almeida-Suhett C.P."><surname>Almeida-Suhett</surname>, <given-names>C.P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prager E. M."><surname>Prager</surname>, <given-names>E. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pidoplichko V."><surname>Pidoplichko</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Figueiredo T. H."><surname>Figueiredo</surname>, <given-names>T. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marini A M."><surname>Marini</surname>, <given-names>A M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Z."><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eiden L.E."><surname>Eiden</surname>, <given-names>L.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Braga M."><surname>Braga</surname>, <given-names>M.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-3">Reduced GABAergic Inhibition in the Basolateral Amygdala and the Development of Anxiety-Like Behaviors after Mild Traumatic Brain Injury</article-title>. <source hwp:id="source-2">PLoS ONE</source> <volume>9</volume>(<issue>7</issue>): <fpage>e102627</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0102627</pub-id>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bessel F. W."><surname>Bessel</surname>, <given-names>F. W.</given-names></string-name> (<year>1818</year>). <source hwp:id="source-3">Fundamenta Astronomiae pro anno MDCCLV deducta ex observationibus viri incomparabilis James Bradley in specula astronomica Grenovicensi per annos 1750-1762 institutis</source>. <publisher-loc>Königsberg</publisher-loc>: <publisher-name>Friedrich Nicolovius</publisher-name>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bernhardson C."><surname>Bernhardson</surname>, <given-names>C.</given-names></string-name> (<year>1975</year>). <article-title hwp:id="article-title-4">Type I error rates when multiple comparison procedures follow a significant F test of ANOVA</article-title>. <source hwp:id="source-4">Biometrics</source>, <volume>31</volume>, <fpage>719</fpage>–<lpage>724</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Boik R. J."><surname>Boik</surname>, <given-names>R. J.</given-names></string-name> (<year>1987</year>). <article-title hwp:id="article-title-5">The Fisher-Pitman permutation test: A non-robust alternative to the normal theory <italic toggle="yes">F</italic> test when variances are heterogeneous</article-title>. <source hwp:id="source-5">British Journal of Mathematical and Statistical Psychology</source>, <volume>40</volume>, <fpage>26</fpage>–<lpage>42</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Bradley J. V."><surname>Bradley</surname>, <given-names>J. V.</given-names></string-name> (<year>1978</year>) <article-title hwp:id="article-title-6">Robustness?</article-title> <source hwp:id="source-6">British Journal of Mathematical and Statistical Psychology</source>, <volume>31</volume>, <fpage>144</fpage>–<lpage>152</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Brown M. B."><surname>Brown</surname>, <given-names>M. B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Forsythe A."><surname>Forsythe</surname>, <given-names>A.</given-names></string-name> (<year>1974</year>). <article-title hwp:id="article-title-7">The small sample behavior of some statistics which test the equality of several means</article-title>. <source hwp:id="source-7">Technometrics</source>, <volume>16</volume>, <fpage>129</fpage>–<lpage>132</lpage></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Brunner E."><surname>Brunner</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Domhof S."><surname>Domhof</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Langer F."><surname>Langer</surname>, <given-names>F.</given-names></string-name> (<year>2002</year>). <source hwp:id="source-8">Nonparametric Analysis of Longitudinal Data in Factorial Experiments</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Chung E."><surname>Chung</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Romano J. P."><surname>Romano</surname> <given-names>J. P.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-8">Exact and asymptotically robust permutation tests</article-title>. <source hwp:id="source-9">Annals of Statistics</source>, <volume>41</volume>, <fpage>484</fpage>–<lpage>507</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Cleveland W. S."><surname>Cleveland</surname>, <given-names>W. S.</given-names></string-name> (<year>1979</year>). <article-title hwp:id="article-title-9">Robust locally weighted regression and smoothing scatterplots</article-title>. <source hwp:id="source-10">Journal of the American Statistical Association</source>, <volume>74</volume>, <fpage>829</fpage>–<lpage>836</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3"><citation publication-type="book" citation-type="book" ref:id="151811v1.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Cliff N."><surname>Cliff</surname>, <given-names>N.</given-names></string-name> (<year>1996</year>). <source hwp:id="source-11">Ordinal Methods for Behavioral Data Analysis</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Cressie N. A. C."><surname>Cressie</surname>, <given-names>N. A. C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Whitford H. J."><surname>Whitford</surname>, <given-names>H. J.</given-names></string-name> (<year>1986</year>). <article-title hwp:id="article-title-10">How to use the two sample t-test</article-title>. <source hwp:id="source-12">Biometrical Journal</source>, <volume>28</volume>, <fpage>131</fpage>–<lpage>148</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Derksen S."><surname>Derksen</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Keselman H. J."><surname>Keselman</surname>, <given-names>H. J.</given-names></string-name> (<year>1992</year>). <article-title hwp:id="article-title-11">Backward, forward and stepwise automated subset selection algorithms: Frequency of obtaining authentic and noise variables</article-title>. <source hwp:id="source-13">British Journal of Mathematical and Statistical Psychology</source>, <volume>45</volume>, <fpage>265</fpage>–<lpage>282</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Doksum K. A."><surname>Doksum</surname>, <given-names>K. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sievers G. L."><surname>Sievers</surname>, <given-names>G. L.</given-names></string-name> (<year>1976</year>). <article-title hwp:id="article-title-12">Plotting with confidence: graphical comparisons of two populations</article-title>. <source hwp:id="source-14">Biometrika</source>, <volume>63</volume>, <fpage>421</fpage>–<lpage>434</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Doksum K. A."><surname>Doksum</surname>, <given-names>K. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wong C.-W."><surname>Wong</surname>, <given-names>C.-W.</given-names></string-name> (<year>1983</year>). <article-title hwp:id="article-title-13">Statistical tests based on transformed data</article-title>. <source hwp:id="source-15">Journal of the American Statistical Association</source>, <volume>78</volume>, <fpage>411</fpage>–<lpage>417</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Duncan G. T."><surname>Duncan</surname>, <given-names>G. T.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Layard M. W."><surname>Layard</surname>, <given-names>M. W.</given-names></string-name> (<year>1973</year>). <article-title hwp:id="article-title-14">A Monte-Carlo study of asymptotically robust tests for correlation</article-title>. <source hwp:id="source-16">Biometrika</source>, <volume>60</volume>, <fpage>551</fpage>–<lpage>558</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Fagerland M. W."><surname>Fagerland</surname>, <given-names>M. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Leiv Sandvik L."><surname>Leiv Sandvik</surname>, <given-names>L.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-15">The Wilcoxon-Mann-Whitney test under scrutiny</article-title>. <source hwp:id="source-17">Statistics in medicine</source>, <volume>28</volume>, <fpage>1487</fpage>–<lpage>1497</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Field A."><surname>Field</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Miles J."><surname>Miles</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Field Z."><surname>Field</surname>, <given-names>Z.</given-names></string-name> (<year>2012</year>). <source hwp:id="source-18">Discovering Statistics Using R.</source> <publisher-loc>Thousand Oaks</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Grayson D."><surname>Grayson</surname>, <given-names>D.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-16">Some myths and legends in quantitative psychology</article-title>. <source hwp:id="source-19">Understanding Statistics</source>, <volume>3</volume>, <fpage>101</fpage>–<lpage>134</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Hampel F. R."><surname>Hampel</surname>, <given-names>F. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ronchetti E. M."><surname>Ronchetti</surname>, <given-names>E. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rousseeuw P. J."><surname>Rousseeuw</surname>, <given-names>P. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Stahel W. A."><surname>Stahel</surname>, <given-names>W. A.</given-names></string-name> (<year>1986</year>) <source hwp:id="source-20">Robust Statistics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Hand A."><surname>Hand</surname>, <given-names>A.</given-names></string-name> (<year>1998</year>). <source hwp:id="source-21">A History of Mathematical Statistics from 1750 to 1930</source> <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name></citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Harrell F. E."><surname>Harrell</surname>, <given-names>F. E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Davis C. E."><surname>Davis</surname>, <given-names>C. E.</given-names></string-name> (<year>1982</year>). <article-title hwp:id="article-title-17">A new distribution-free quantile estimator</article-title>. <source hwp:id="source-22">Biometrika</source>, <volume>69</volume>, <fpage>635</fpage>–<lpage>640</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><citation publication-type="book" citation-type="book" ref:id="151811v1.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Heritier S."><surname>Heritier</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cantoni E"><surname>Cantoni</surname>, <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Copt S."><surname>Copt</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Victoria-Feser M.-P."><surname>Victoria-Feser</surname>, <given-names>M.-P.</given-names></string-name> (<year>2009</year>). <source hwp:id="source-23">Robust Methods in Biostatistics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Hettmansperger T. P."><surname>Hettmansperger</surname>, <given-names>T. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McKean J. W."><surname>McKean</surname>, <given-names>J. W.</given-names></string-name> (<year>2011</year>). <source hwp:id="source-24">Robust Nonparametric Statistical Methods, 2nd Ed.</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Arnold</publisher-name>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Hettmansperger T. P."><surname>Hettmansperger</surname>, <given-names>T. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sheather S. J."><surname>Sheather</surname>, <given-names>S. J.</given-names></string-name> (<year>1986</year>). <article-title hwp:id="article-title-18">Confidence interval based on interpolated order statistics</article-title>. <source hwp:id="source-25">Statistical Probability Letters</source>, <volume>4</volume>, <fpage>75</fpage>–<lpage>79</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Hochberg Y."><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name> (<year>1988</year>). <article-title hwp:id="article-title-19">A sharper Bonferroni procedure for multiple tests of significance</article-title>. <source hwp:id="source-26">Biometrika</source>, <volume>75</volume>, <fpage>800</fpage>–<lpage>802</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Hommel G."><surname>Hommel</surname>, <given-names>G.</given-names></string-name> (<year>1988</year>). <article-title hwp:id="article-title-20">A stagewise rejective multiple test procedure based on a modified Bonferroni test</article-title>. <source hwp:id="source-27">Biometrika</source>, <volume>75</volume>, <fpage>383</fpage>–<lpage>386</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Houston S. M."><surname>Houston</surname>, <given-names>S. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lebel C."><surname>Lebel</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Katzir T."><surname>Katzir</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Manis F. R."><surname>Manis</surname>, <given-names>F. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kan E."><surname>Kan</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rodriguez G. R."><surname>Rodriguez</surname>, <given-names>G. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sowell E. R."><surname>Sowell</surname>, <given-names>E. R.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-21">Reading skill and structural brain development</article-title>. <source hwp:id="source-28">Neuroreport</source>, <volume>25</volume>, <fpage>347</fpage>–<lpage>352</lpage>. doi:<pub-id pub-id-type="doi">10.1097/WNR.0000000000000121</pub-id>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Huber P. J."><surname>Huber</surname>, <given-names>P. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Ronchetti E."><surname>Ronchetti</surname>, <given-names>E.</given-names></string-name> (<year>2009</year>). <source hwp:id="source-29">Robust Statistics</source>, <edition>2nd Ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Keselman H. J."><surname>Keselman</surname>, <given-names>H. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Othman A."><surname>Othman</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-22">Generalized linear model analyses for treatment group equality when data are non-normal</article-title> <source hwp:id="source-30">Journal of Modern and Applied Statistical Methods</source>, <volume>15</volume>, <fpage>32</fpage>–<lpage>61</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="website" citation-type="web" ref:id="151811v1.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Mancini F"><surname>Mancini</surname>, <given-names>F</given-names></string-name>.(<year>2016</year>): <article-title hwp:id="article-title-23">ANA mancini14 data.zip. figshare</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://https://doi.org/10.6084/m9.figshare.3427766.v" ext-link-type="uri" xlink:href="http://https://doi.org/10.6084/m9.figshare.3427766.v" hwp:id="ext-link-4">https://doi.org/10.6084/m9.figshare.3427766.v</ext-link></citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Mancini F."><surname>Mancini</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bauleo A."><surname>Bauleo</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cole J."><surname>Cole</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lui F."><surname>Lui</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Porro C.A."><surname>Porro</surname>, <given-names>C.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haggard P."><surname>Haggard</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Iannetti G. D."><surname>Iannetti</surname>, <given-names>G. D.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-24">Whole-body mapping of spatial acuity for pain and touch</article-title>. <source hwp:id="source-31">Annals of Neurology</source>, <volume>75</volume> <fpage>917</fpage>–<lpage>924</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><citation publication-type="book" citation-type="book" ref:id="151811v1.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Maronna R. A."><surname>Maronna</surname>, <given-names>R. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martin D. R."><surname>Martin</surname>, <given-names>D. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Yohai V. J."><surname>Yohai</surname>, <given-names>V. J.</given-names></string-name> (<year>2006</year>). <source hwp:id="source-32">Robust Statistics: Theory and Methods</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Montgomery D. C."><surname>Montgomery</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Peck E. A."><surname>Peck</surname>, <given-names>E. A.</given-names></string-name> (<year>1992</year>). <source hwp:id="source-33">Introduction to Linear Regression Analysis</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Newcombe R. G."><surname>Newcombe</surname> <given-names>R. G.</given-names></string-name> (<year>2006</year>) <article-title hwp:id="article-title-25">Confidence intervals for an effect size measure based on the Mann-Whitney statistic. Part 1: General issues and tail-area-based methods</article-title>. <source hwp:id="source-34">Statistics in Medicine</source>, <volume>25</volume>, <fpage>543</fpage>–<lpage>557</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Nieuwenhuis S."><surname>Nieuwenhuis</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Forstmann B.U."><surname>Forstmann</surname>, <given-names>B.U.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wagenmakers E.J."><surname>Wagenmakers</surname>, <given-names>E.J.</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-26">Erroneous analyses of interactions in neuroscience: a problem of significance</article-title>. <source hwp:id="source-35">Nat Neurosci</source>, <volume>14</volume>, <fpage>1105</fpage>–<lpage>1107</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Özdemir A. F."><surname>Özdemir</surname>, <given-names>A. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Yildiztepe E."><surname>Yildiztepe</surname>, <given-names>E.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-27">Comparing measures of location: Some small-sample results when distributions differ in skewness and kurtosis under heterogeneity of variances</article-title> <source hwp:id="source-36">Communications in Statistics–Simulation and Computations</source>, <volume>42</volume> <fpage>407</fpage>–<lpage>424</lpage></citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Pernet C.R."><surname>Pernet</surname>, <given-names>C.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilcox R."><surname>Wilcox</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rousselet G.A."><surname>Rousselet</surname>, <given-names>G.A.</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-28">Robust correlation analyses: false positive and power validation using a new open source matlab toolbox</article-title>. <source hwp:id="source-37">Frontiers in Quantitative Psychology and Measurement</source>. DOI= <pub-id pub-id-type="doi">10.3389/fpsyg.2012.00606</pub-id></citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Pernet C. R."><surname>Pernet</surname>, <given-names>C. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Latinus M."><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichols T. E."><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rousselet G. A."><surname>Rousselet</surname>, <given-names>G. A.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-29">Cluster-based computational methods for mass univariate analyses of event-related brain potentials/fields: A simulation study</article-title>. <source hwp:id="source-38">Journal of Neuroscience Methods</source>, <volume>250</volume>, <fpage>85</fpage>–<lpage>93</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Rasmussen J. L."><surname>Rasmussen</surname>, <given-names>J. L.</given-names></string-name> (<year>1989</year>). <article-title hwp:id="article-title-30">Data transformation, Type I error rate and power</article-title>. <source hwp:id="source-39">British Journal of Mathematical and Statistical Psychology</source>, <volume>42</volume>, <fpage>203</fpage>–<lpage>211</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Romano J. P."><surname>Romano</surname>, <given-names>J. P.</given-names></string-name> (<year>1990</year>). <article-title hwp:id="article-title-31">On the behavior of randomization tests without a group invariance assumption</article-title>. <source hwp:id="source-40">Journal of the American Statistical Association</source> <volume>85</volume>, <fpage>686</fpage>–<lpage>692</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Rousseeuw P. J."><surname>Rousseeuw</surname>, <given-names>P. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Leroy A. M."><surname>Leroy</surname>, <given-names>A. M.</given-names></string-name> (<year>1987</year>). <source hwp:id="source-41">Robust Regression &amp; Outlier Detection</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.43" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Rousselet G.A."><surname>Rousselet</surname>, <given-names>G.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foxe J.J."><surname>Foxe</surname>, <given-names>J.J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bolam J.P."><surname>Bolam</surname>, <given-names>J.P.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-32">A few simple steps to improve the description of group results in neuroscience</article-title>. <source hwp:id="source-42">European Journal of Neuroscience</source>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Rousselet G.A."><surname>Rousselet</surname>, <given-names>G.A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Pernet C.R."><surname>Pernet</surname>, <given-names>C.R.</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-33">Improving standards in brain-behavior correlation analyses</article-title>. <source hwp:id="source-43">Frontiers in human neuroscience</source>, <volume>6</volume>, <fpage>119</fpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3 xref-ref-45-4 xref-ref-45-5"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Rousselet G.A."><surname>Rousselet</surname>, <given-names>G.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pernet C.R."><surname>Pernet</surname>, <given-names>C.R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wilcox R.R."><surname>Wilcox</surname>, <given-names>R.R.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-34">Beyond differences in means: robust graphical methods to compare two groups in neuroscience</article-title>. <source hwp:id="source-44">European Journal of Neuroscience</source></citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Ruscio J."><surname>Ruscio</surname>, <given-names>J.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-35">A probability-based measure of effect size: Robustness to base rates and other factors</article-title>. <source hwp:id="source-45">Psychological Methods</source>, <volume>13</volume>, <fpage>19</fpage>–<lpage>30</lpage>.</citation></ref><ref id="c47" hwp:id="ref-47"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Ruscio J."><surname>Ruscio</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mullen T."><surname>Mullen</surname>, <given-names>T.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-36">Confidence intervals for the probability of superiority effect size measure and the area under a receiver operating characteristic curve</article-title>. <source hwp:id="source-46">Multivariate Behavioral Research</source>, <volume>47</volume>, <fpage>201</fpage>–<lpage>223</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Schilling M."><surname>Schilling</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Doi J."><surname>Doi</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-37">A Coverage Probability Approach to Finding an Optimal Binomial Confidence Procedure</article-title>. <source hwp:id="source-47">American Statistician</source>, <volume>68</volume>, <fpage>133</fpage>–<lpage>145</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2"><citation publication-type="book" citation-type="book" ref:id="151811v1.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Staudte R. G."><surname>Staudte</surname>, <given-names>R. G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sheather S. J."><surname>Sheather</surname>, <given-names>S. J.</given-names></string-name> (<year>1990</year>). <source hwp:id="source-48">Robust Estimation and Testing</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1 xref-ref-50-2"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Talebi V."><surname>Talebi</surname>, <given-names>V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Baker C.L. Jr"><surname>Baker</surname>, <given-names>C.L.</given-names>, <suffix>Jr</suffix></string-name>. (<year>2016</year>) <article-title hwp:id="article-title-38">Categorically distinct types of receptive fields in early visual cortex</article-title>. <source hwp:id="source-49">Journal of Neurophysiology</source>, <volume>115</volume>, <fpage>2556</fpage>–<lpage>2576</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Tukey J. W."><surname>Tukey</surname>, <given-names>J. W.</given-names></string-name> (<year>1991</year>). <article-title hwp:id="article-title-39">The philosophy of multiple comparisons</article-title>. <source hwp:id="source-50">Statistical Science</source>, <volume>6</volume>, <fpage>100</fpage>–<lpage>116</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Tukey J. W."><surname>Tukey</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McLaughlin D. H."><surname>McLaughlin</surname> <given-names>D. H.</given-names></string-name> (<year>1963</year>). <article-title hwp:id="article-title-40">Less vulnerable confidence and significance procedures for location based on a single sample: Trimming/Winsorization 1</article-title>. <source hwp:id="source-51">Sankhya A</source>, <volume>25</volume> <fpage>331</fpage>–<lpage>352</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Wagenmakers E.J."><surname>Wagenmakers</surname>, <given-names>E.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wetzels R."><surname>Wetzels</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Borsboom D."><surname>Borsboom</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van der Maas H.L."><surname>van der Maas</surname>, <given-names>H.L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kievit R.A."><surname>Kievit</surname>, <given-names>R.A.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-41">An Agenda for Purely Confirmatory Research</article-title>. <source hwp:id="source-52">Perspectives in Psychological Science</source>, <volume>7</volume>, <fpage>632</fpage>–<lpage>638</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Welch B. L."><surname>Welch</surname>, <given-names>B. L.</given-names></string-name> (<year>1938</year>). <article-title hwp:id="article-title-42">The significance of the difference between two means when the population variances are unequal</article-title>. <source hwp:id="source-53">Biometrika</source>, <volume>29</volume>, <fpage>350</fpage>–<lpage>362</lpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Weissgerber T.L."><surname>Weissgerber</surname>, <given-names>T.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Milic N.M."><surname>Milic</surname>, <given-names>N.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winham S.J."><surname>Winham</surname>, <given-names>S.J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Garovic V.D."><surname>Garovic</surname>, <given-names>V.D.</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-43">Beyond bar and line graphs: time for a new data presentation paradigm</article-title>. <source hwp:id="source-54">PLoS Biol</source>, <volume>13</volume>, <fpage>e1002128</fpage>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.56" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Wiederhold J. L."><surname>Wiederhold</surname>, <given-names>J. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bryan B. R."><surname>Bryan</surname>, <given-names>B. R.</given-names></string-name> (<year>2001</year>). <source hwp:id="source-55">Gray Oral Reading Test</source>. <edition>Fourth Edition</edition>. <publisher-loc>San Antonio, TX</publisher-loc>: <publisher-name>PSYCHCORP</publisher-name>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-44">Comparing Pearson correlations: Dealing with heteroscedasticity and non-normality</article-title>. <source hwp:id="source-56">Communications in Statistics–Simulation and Computation</source>, <volume>38</volume>, <fpage>2220</fpage>–<lpage>2234</lpage></citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-45">Comparing two independent groups via a quantile generalization of the Wilcoxon–Mann–Whitney test</article-title>. <source hwp:id="source-57">Journal of Modern and Applied Statistical Methods</source>, <volume>11</volume>, <fpage>296</fpage>–<lpage>302</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.59" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-46">Robust regression: an inferential method for determining which independent variables are most important</article-title>. <source hwp:id="source-58">Journal of Applied Statistics</source> <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.tandfonline.com/eprint/6sZMRAfGqmxErZRTpmRj/full" ext-link-type="uri" xlink:href="http://www.tandfonline.com/eprint/6sZMRAfGqmxErZRTpmRj/full" hwp:id="ext-link-5">http://www.tandfonline.com/eprint/6sZMRAfGqmxErZRTpmRj/full</ext-link> <ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1080/02664763.2016.1268105" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02664763.2016.1268105" hwp:id="ext-link-6">http://dx.doi.org/10.1080/02664763.2016.1268105</ext-link></citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1 xref-ref-60-2 xref-ref-60-3 xref-ref-60-4 xref-ref-60-5 xref-ref-60-6 xref-ref-60-7 xref-ref-60-8 xref-ref-60-9 xref-ref-60-10 xref-ref-60-11 xref-ref-60-12 xref-ref-60-13 xref-ref-60-14 xref-ref-60-15 xref-ref-60-16 xref-ref-60-17 xref-ref-60-18 xref-ref-60-19 xref-ref-60-20 xref-ref-60-21 xref-ref-60-22 xref-ref-60-23 xref-ref-60-24 xref-ref-60-25 xref-ref-60-26 xref-ref-60-27 xref-ref-60-28 xref-ref-60-29 xref-ref-60-30 xref-ref-60-31 xref-ref-60-32 xref-ref-60-33 xref-ref-60-34 xref-ref-60-35 xref-ref-60-36 xref-ref-60-37 xref-ref-60-38 xref-ref-60-39 xref-ref-60-40 xref-ref-60-41 xref-ref-60-42"><citation publication-type="book" citation-type="book" ref:id="151811v1.60" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2017a</year>). <source hwp:id="source-59">Introduction to Robust Estimation and Hypothesis Testing</source> <edition>4th Ed</edition>. <publisher-loc>San Diego</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1 xref-ref-61-2 xref-ref-61-3 xref-ref-61-4 xref-ref-61-5 xref-ref-61-6 xref-ref-61-7 xref-ref-61-8 xref-ref-61-9 xref-ref-61-10 xref-ref-61-11 xref-ref-61-12 xref-ref-61-13 xref-ref-61-14"><citation publication-type="book" citation-type="book" ref:id="151811v1.61" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2017b</year>) <source hwp:id="source-60">Understanding and Applying Basic Statistical Methods Using R</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1 xref-ref-62-2 xref-ref-62-3 xref-ref-62-4 xref-ref-62-5 xref-ref-62-6 xref-ref-62-7 xref-ref-62-8 xref-ref-62-9 xref-ref-62-10 xref-ref-62-11 xref-ref-62-12 xref-ref-62-13 xref-ref-62-14 xref-ref-62-15 xref-ref-62-16 xref-ref-62-17 xref-ref-62-18 xref-ref-62-19 xref-ref-62-20 xref-ref-62-21 xref-ref-62-22 xref-ref-62-23 xref-ref-62-24 xref-ref-62-25 xref-ref-62-26 xref-ref-62-27 xref-ref-62-28 xref-ref-62-29"><citation publication-type="book" citation-type="book" ref:id="151811v1.62" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (<year>2017c</year>). <source hwp:id="source-61">Modern Statistics for the Social and Behavioral Sciences: A Practical Introduction</source>. <edition>2nd Ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Chapman &amp; Hall/CRC press</publisher-name>.</citation></ref><ref id="c63" hwp:id="ref-63"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.63" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> (in press). <article-title hwp:id="article-title-47">An inferential method for determining which of two independent variables is most important when there is curvature</article-title>. <source hwp:id="source-62">Journal of Modern and Applied Statistical Methods</source>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><citation publication-type="website" citation-type="web" ref:id="151811v1.64" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Wilcox R. R."><surname>Wilcox</surname>, <given-names>R. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rousselet G.A"><surname>Rousselet</surname>, <given-names>G.A</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-48">A guide to robust statistical methods: Illustrations using R</article-title> <ext-link l:rel="related" l:ref-type="uri" l:ref="http://https://doi.org/10.6084/m9.figshare.5114275" ext-link-type="uri" xlink:href="http://https://doi.org/10.6084/m9.figshare.5114275" hwp:id="ext-link-7">https://doi.org/10.6084/m9.figshare.5114275</ext-link>.</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Winkler A. M."><surname>Winkler</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ridgwaym G. R."><surname>Ridgwaym</surname> <given-names>G. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Webster M. A."><surname>Webster</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith S. M."><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Nichols T. M."><surname>Nichols</surname>, <given-names>T. M.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-49">Permutation inference for the general linear model</article-title>. <source hwp:id="source-63">NeuroImage</source>, <volume>92</volume>, <fpage>381</fpage>–<lpage>397</lpage></citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><citation publication-type="book" citation-type="book" ref:id="151811v1.66" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Woodcock R.W."><surname>Woodcock</surname>, <given-names>R.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGrew K.S."><surname>McGrew</surname>, <given-names>K.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mather N."><surname>Mather</surname>, <given-names>N.</given-names></string-name> (<year>2001</year>) <source hwp:id="source-64">Woodcock Johnson-III Tests of Achievement</source> (<publisher-name>Riverside Publishing</publisher-name>, <publisher-loc>Itaska, IL</publisher-loc>).</citation></ref><ref id="c67" hwp:id="ref-67"><citation publication-type="website" citation-type="web" ref:id="151811v1.67" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Yeatman J. D."><surname>Yeatman</surname>, <given-names>J. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dougherty R. F."><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ben-Shachar M."><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wandell B. A."><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2012</year>). <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.pnas.org/cgi/doi/10.1073/pnas.1206792109" ext-link-type="uri" xlink:href="http://www.pnas.org/cgi/doi/10.1073/pnas.1206792109" hwp:id="ext-link-8">www.pnas.org/cgi/doi/10.1073/pnas.1206792109</ext-link>.</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><citation publication-type="journal" citation-type="journal" ref:id="151811v1.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Yuen K. K."><surname>Yuen</surname>, <given-names>K. K.</given-names></string-name> (<year>1974</year>). <article-title hwp:id="article-title-50">The two sample trimmed t for unequal population variances</article-title>. <source hwp:id="source-65">Biometrika</source>, <volume>61</volume>, <fpage>165</fpage>–<lpage>170</lpage>.</citation></ref></ref-list></back></article>
