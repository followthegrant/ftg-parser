<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/132647</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;132647</article-id><article-id pub-id-type="other" hwp:sub-type="slug">132647</article-id><article-id pub-id-type="other" hwp:sub-type="tag">132647</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Confirmatory Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Genetics" hwp:journal="biorxiv"><subject>Genetics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Mutual information estimation for transcriptional regulatory network inference</article-title></title-group><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Ish-Horowicz Jonathan"><surname>Ish-Horowicz</surname><given-names>Jonathan</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Reid John"><surname>Reid</surname><given-names>John</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">MRC Biostatistics Unit, University of Cambridge</institution></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-05-01T13:45:23-07:00">
    <day>1</day><month>5</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-05-01T13:45:23-07:00">
    <day>1</day><month>5</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-05-01T14:05:18-07:00">
    <day>1</day><month>5</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-05-01T14:05:18-07:00">
    <day>1</day><month>5</month><year>2017</year>
  </pub-date><elocation-id>132647</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-05-01"><day>01</day><month>5</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-05-01"><day>01</day><month>5</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="132647.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/132647v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="132647.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/132647v1/132647v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/132647v1/132647v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Mutual information-based network inference algorithms are an important tool in the reverse-engineering of transcriptional regulatory networks, but all rely on estimates of the mutual information between the expression of pairs of genes. Various methods exist to compute estimates of the mutual information, but none have been firmly established as optimal for network inference. The performance of 9 mutual information estimation methods are compared using three popular network inference algorithms: CLR, MRNET and ARACNE. The performance of the estimators is compared on one synthetic and two real datasets. For estimators that discretise data, the effect of discretisation parameters are also studied in detail. Implementations of 5 estimators are provided in parallelised C++ with an R interface. These are faster than alternative implementations, with reductions in computation time up to a factor of 3,500.</p><sec hwp:id="sec-1"><title hwp:id="title-2">Results</title><p hwp:id="p-3">The B-spline estimator consistently performs well on real and synthetic datasets. CLR was found to be the best performing inference algorithm, corroborating previous results indicating that it is the state of the art mutual inference algorithm. It is also found to be robust to the mutual information estimation method and their parameters. Furthermore, when using an estimator that discretises expression data, using <italic toggle="yes">N</italic><sup>1/3</sup> bins for <italic toggle="yes">N</italic> samples gives the most accurate inferred network. This contradicts previous findings that suggested using <italic toggle="yes">N</italic><sup>1/2</sup> bins.</p></sec></abstract><counts><page-count count="45"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-2"><label>1</label><title hwp:id="title-3">Introduction</title><sec id="s1a" hwp:id="sec-3"><label>1.1</label><title hwp:id="title-4">Background</title><p hwp:id="p-4">Network inference is the reverse-engineering of transcriptional regulatory networks from high-throughput expression data. Transcriptional regulatory networks are the first level of a hierarchy of regulatory mechanisms that operate in cells and control gene expression. Proteins called transcription factors either activate or repress the transcription of a gene. Since these transcription factors are themselves the products of the expression of another gene, dependence between the expression of two genes suggests a regulatory link between them. This is the central assumption of network inference: that dependence between the expression of two genes is due to a functional relationship. Network inference is a highly underdetermined problem. There are approximately 4,500 genes in the popular model organism <italic toggle="yes">Escherichia coli</italic> (<italic toggle="yes">E. coli</italic>). This corresponds to over 10 million pairwise interactions which must be estimated from samples that typically number less than 1000. Several unsupervised approaches have been developed to tackle this problem, with different classes of methods proving effective at predicting certain network motifs. It has been shown that combining predictions from different classes of network inference algorithm significantly improves the accuracy of inferred networks [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>]. Examples of the techniques utilised by network inference algorithms include feature selection [<xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>] and Bayesian networks [<xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]. Another class of methods utilise mutual information, an information-theoretic quantity that captures dependence between two random variables. As well as network inference, mutual information is used in biology as a distance measure for gene expression clustering [<xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]. It is often preferred to linear correlation measures due to its ability to capture nonlinear dependencies.</p><p hwp:id="p-5">Mutual information-based network inference algorithms have several advantages. Specifically, they are well-suited to the direct, model-free inference of large-scale regulatory networks. Since they only consider pairwise interactions they are computationally affordable even for large numbers of genes [<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>]. Furthermore, considering pairwise interactions involves estimating bivariate probability distributions, meaning that they perform relatively well with low numbers of samples [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>, <xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>]. However, mutual information-based methods are unable to predict the type or direction of a regulatory relationship, unlike feature selection-based methods.</p></sec><sec id="s1b" hwp:id="sec-4"><label>1.2</label><title hwp:id="title-5">Motivation and Aims</title><p hwp:id="p-6">Estimating mutual information is known to be difficult [<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>] and various methods exist for computing an estimate. This study examines the effect of mutual information estimators on the accuracy of the inferred network. Three mutual information-based network inference algorithms, CLR [<xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>], MRNET [<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>] and ARACNE [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref>] are used to infer the network from a matrix of gene expression values. These algorithms produce a score for each gene pair, the ranking of which indicates the confidence of an regulatory link between the two genes. A single network is then produced by thresholding these scores. This score is derived from the mutual information between the expression profiles of every gene pair, and so the method used to estimate mutual information can have a significant effect on the accuracy of the inferred network. Further details on the network inference algorithms can be found in <xref ref-type="sec" rid="s5b" hwp:id="xref-sec-38-1" hwp:rel-id="sec-38">Section S2</xref>.</p><p hwp:id="p-7">CLR was the best-performing mutual information-based inference algorithm in the DREAM5 Network Inference Challenge [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">1</xref>] and has been used in the development of antibiotics [<xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>] and microbial fuel cells [<xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>]. ARACNE is another popular method that has been used in the study of brain tumours [<xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>] and the sequencing of the wheat genome [<xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]. MRNET has been shown to be competitive with CLR and ARACNE [<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>].</p><p hwp:id="p-8">This study compares the performance of CLR, MRNET and ARACNE when used with nine of the most popular mutual information estimators. These are the Maximum Likelihood, Miller-Madow, Chao-Shen, Shrinkage, B-spline, Kernel Density, <italic toggle="yes">k</italic>-nearest-neighbour, Pearson correlation and Spearman correlation estimators. Of these estimators, the first four are histogram-based and hence require data to discretised/binned. This study also investigates the use of the Bayesian Blocks algorithm in network inference. Bayesian Blocks was developed by Scargle et al. to model astronomical time series data using piecewise constant functions [<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>] and can be used to discretise continuous data into bins whose number and width are chosen to optimally fit data. The increased sophistication of Bayesian Blocks over simpler alternatives such as equal width bins should lead to more accurate estimates of probability distributions. It has previously been applied to network inference and was found to improve the accuracy of inferred networks [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>].</p><p hwp:id="p-9">The estimators will be compared using one synthetic and two real datasets from the DREAM5 Network Inference Challenge [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-3" hwp:rel-id="ref-1">1</xref>]. The synthetic dataset was generated with GeneNetWeaver [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>] and is labelled as <italic toggle="yes">in silico</italic>, while the real datasets are from <italic toggle="yes">E. coli</italic> and <italic toggle="yes">Saccharomyces cerevisiae</italic> (<italic toggle="yes">S. cerevisiae</italic>). Further details on the datasets are given in <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref> and <xref ref-type="sec" rid="s5c" hwp:id="xref-sec-42-1" hwp:rel-id="sec-42">Section S3</xref>.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-1"><p hwp:id="p-10">The properties of the datasets used in study [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-4" hwp:rel-id="ref-1">1</xref>]. For each dataset a subset of the genes are designated as potential regulators and the accuracy of inferred networks are only evaluated using the interactions of those genes. <italic toggle="yes">Proportion of links</italic> refers to the fraction of this subset of interactions that are regulatory links in the gold standard network. For the real datasets this is likely to be an underestimate. These values demonstrate the large class imbalance that is typical for network inference. For more detailed information on the data used in this work see <xref ref-type="sec" rid="s5c" hwp:id="xref-sec-42-2" hwp:rel-id="sec-42">Section S3</xref>.</p></caption><graphic xlink:href="132647_tbl1" position="float" orientation="portrait" hwp:id="graphic-1"/></table-wrap></sec><sec id="s1c" hwp:id="sec-5"><label>1.3</label><title hwp:id="title-6">Structure and contributions</title><p hwp:id="p-11">The following subsection reviews existing studies on this topic. <xref rid="s2" ref-type="sec" hwp:id="xref-sec-7-1" hwp:rel-id="sec-7">Section 2</xref> outlines the process of network inference, the mathematical definition of mutual information and discusses methods of mutual information estimation. <xref rid="s3" ref-type="sec" hwp:id="xref-sec-11-1" hwp:rel-id="sec-11">Section 3</xref> compares the performance of these mutual information estimators for network inference using datasets from the DREAM5 Network Inference Challenge [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-5" hwp:rel-id="ref-1">1</xref>]. Detailed descriptions of the mutual information estimators, the network inference algorithms and the performance metrics used in this and other studies on this topic can be found in the Supplementary Material.</p><p hwp:id="p-12">The contributions of this work are as follows:
<list list-type="bullet" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-13">This is the first evaluation of mutual information estimation using the three most popular mutual information-based network inference algorithms.</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-14">This is the first comprehensive evaluation of the parameters of the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators to include Bayesian Blocks.</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-15">This is the first evaluation of mutual information estimation for network inference to use the area under precision-recall curve (AUPRC) as the evaluation metric, as is recommended for the evaluation of classification problems with a large class imbalance [<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>–<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>].</p></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-16">This is the first systematic evaluation of the effect of Bayesian Blocks in network inference.</p></list-item><list-item hwp:id="list-item-5"><p hwp:id="p-17">Implementations of 5 estimators are provided in parallelised C++ with an R interface. These are faster than alternative implementations, with reductions in computation time up to a factor of 3,500.</p></list-item></list></p></sec><sec id="s1d" hwp:id="sec-6"><label>1.4</label><title hwp:id="title-7">Review of existing studies</title><p hwp:id="p-18">Several existing studies examine mutual information estimation for network inference. However, the inference algorithms, mutual information estimators and performance metrics vary between them. The most recent study by Kurt et al. examined the performance of 11 mutual information estimators, including all 9 estimators used in this study [<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>]. This study included raw Pearson and Spearman correlation values as the additional estimators. For the histogram-based estimators, equal width and equal frequency binning was used. This study used the network inference algorithms RELNET, ARACNE and C3NET. The approaches were evaluated using two synthetic datasets generated by SynTRen [<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>], an alternative tool to GeneNetWeaver, and two real datasets. The real datasets were from <italic toggle="yes">E. coli</italic> and <italic toggle="yes">S. cerevisiae</italic>. The authors found that the B-spline estimator and those based on the linear Pearson and correlation led to the most accurate inferred network on all the datasets.</p><p hwp:id="p-19">The C3NET algorithm [<xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>] does not produce a ranking of scores for each gene pair. Instead, it uses a heuristic approach to produce a single network. It is therefore not possible to evaluate its predictions using the metric used in this study, AUPRC. Instead the authors use the F-score. The evaluation metrics used in this study and others are discussed in <xref ref-type="sec" rid="s5d" hwp:id="xref-sec-43-1" hwp:rel-id="sec-43">Section S4</xref>. The RELNET (Relevance Network) algorithm ranks gene pairs by the mutual information between their expressions, with a higher rank corresponding to a more confident prediction of an edge [<xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>]. It had the lowest prediction accuracy for all mutual information estimators and so is not included in this study. For reasons why this simplistic approach is not effective see <xref ref-type="sec" rid="s5b" hwp:id="xref-sec-38-2" hwp:rel-id="sec-38">Section S2</xref>.</p><p hwp:id="p-20">A study by Olsen et al. [<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>] compared five mutual information estimators (Maximum Likelihood, Miller-Madow, Shrinkage, Pearson correlation and Spearman correlation) with CLR, MRNET and ARACNE. The evaluation metrics were F-scores and areas under receiver operating characteristic (ROC) curve. The authors report that CLR and MRNET outperform ARACNE, and that the Pearson and Spearman correlation estimators perform best on real data. In addition, the Miller-Madow estimator performs well on synthetic data. Olsen et al. also find that equal frequency binning is preferable to equal width binning.</p><p hwp:id="p-21">A comprehensive review of the existing literature on mutual information estimation is was completed by Walters-Williams and Li [<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>]. While this is not directly applied to network inference it is still a valuable survey and reports that the <italic toggle="yes">k</italic>-NN and Kernel Density estimators outperform the Maximum Likelihood estimator. They also report that the B-spline estimator provides equivalent performance to the Kernel Density and <italic toggle="yes">k</italic>-nearest-neighbour estimators in a much smaller computation time.</p></sec></sec><sec id="s2" hwp:id="sec-7" hwp:rev-id="xref-sec-7-1"><label>2</label><title hwp:id="title-8">Methods</title><sec id="s2a" hwp:id="sec-8" hwp:rev-id="xref-sec-8-1 xref-sec-8-2"><label>2.1</label><title hwp:id="title-9">Problem formulation and workflow</title><p hwp:id="p-22">Starting with a matrix of gene expression values <italic toggle="yes">X ∈</italic> ℛ<sup><italic toggle="yes">N × p</italic></sup>, the aim of network inference is to infer a graph <italic toggle="yes">&#x01d4a2;</italic> = (<italic toggle="yes">V, E</italic>), where each node <italic toggle="yes">g ∈ V</italic> represents a single gene and each edge <italic toggle="yes">e ∈ E</italic>, represents a regulatory link between two genes.</p><p hwp:id="p-23">From <italic toggle="yes">X</italic>, a mutual information estimator computes a symmetric matrix of mutual information values of all gene pairs, whose <italic toggle="yes">ij</italic>-th element is the mutual information between columns <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> of <italic toggle="yes">X</italic>. The network inference algorithm then computes a score for each gene pair from these mutual information values. These scores reflect the confidence that there is an edge between two genes. To produce a single inferred network from these edge scores we choose a threshold and consider all scores above that threshold as edges, with all scores below designated as non-edges. This means that the scores themselves are not analysed, only their ranks. The inferred network is then evaluated against the true network and the precision and recall are calculated (see <xref ref-type="sec" rid="s5d" hwp:id="xref-sec-43-2" hwp:rel-id="sec-43">Section S4</xref> for more details). This procedure is illustrated in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>. This is typically repeated for all possible thresholds, resulting in a list of precision-recall values. These values are plotted and the area underneath the resulting curve is used to quantify the accuracy of the predictions.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-2"><p hwp:id="p-24">The network inference workflow for mutual information-based inference algorithms. Typically this process will be repeated using all possible thresholds, from which a precision-recall curve is plotted. The area under this curve is then used to assess the accuracy of the inferred network.</p></caption><graphic xlink:href="132647_fig1" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2b" hwp:id="sec-9"><label>2.2</label><title hwp:id="title-10">Mutual Information</title><p hwp:id="p-25">Mutual information is an information-theoretic quantity that quantifies the dependence between two random variables. For two continuous random variables <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, their mutual information is defined as
<disp-formula id="eqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="132647_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
where <italic toggle="yes">p</italic>(<italic toggle="yes">x, y</italic>) is the joint probability distribution of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> and <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>) and <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>) are the marginal probability distribution of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> respectively. If <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> are discrete variables taking values in the sets <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> respectively this definition becomes
<disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="132647_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
</p><p hwp:id="p-26">This definition can be equivalently expressed in terms of the (information theoretic) entropy as
<disp-formula id="eqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="132647_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
where <italic toggle="yes">H</italic>(<italic toggle="yes">x</italic>) and <italic toggle="yes">H</italic>(<italic toggle="yes">y</italic>) are the marginal entropies defined by
<disp-formula id="eqn4" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="132647_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives>
</disp-formula>
and <italic toggle="yes">H</italic>(<italic toggle="yes">x, y</italic>) is the joint entropy defined by
<disp-formula id="eqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="132647_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
</p><p hwp:id="p-27">In practice, is common to use (3)-(5) to estimate <italic toggle="yes">I</italic>(<italic toggle="yes">x, y</italic>) if the mutual information estimator involves binning the expression data. In both (1) and (2) the base of the logarithm determines the units of <italic toggle="yes">I</italic>. Using log<sub>2</sub> leads to a quantity measured in bits and the natural logarithm leads to a quantity measured in nats.</p><p hwp:id="p-28">From (1), <italic toggle="yes">I</italic>(<italic toggle="yes">x, y</italic>) is symmetric and takes values in [0<italic toggle="yes">, ∞</italic>). It is zero if and only if <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> are strictly independent, which is to say that <italic toggle="yes">p</italic>(<italic toggle="yes">x, y</italic>) = <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>) <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>). This enables the mutual information to capture non-linear dependencies between variables, unlike linear measures such as the Pearson correlation coefficient.</p><p hwp:id="p-29">If <italic toggle="yes">p</italic>(<italic toggle="yes">x, y</italic>) is bivariate normal then the <italic toggle="yes">I</italic>(<italic toggle="yes">x, y</italic>) can be formulated exactly in terms of Pearson’s correlation coefficient
<disp-formula id="eqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="132647_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
where cov is the covariance of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> and <italic toggle="yes">σ</italic><sub><italic toggle="yes">x</italic></sub><italic toggle="yes">, σ</italic><sub><italic toggle="yes">y</italic></sub> are the standard deviations of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> respectively. In this case
<disp-formula id="eqn7" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="132647_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
</p><p hwp:id="p-30">Another common approach uses Spearman’s correlation coefficient, where <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> are replaced by a ranking of their values.</p></sec><sec id="s2c" hwp:id="sec-10"><label>2.3</label><title hwp:id="title-11">Mutual information estimation</title><p hwp:id="p-31">An accurate estimate of the mutual information requires accurate estimates of the distributions <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>), <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>) and <italic toggle="yes">p</italic>(<italic toggle="yes">x, y</italic>) from a finite (and often small) number of samples. Different mutual information estimators are essentially different approaches to estimating these distributions and each performs well in different scenarios. These different performance properties make different estimators suited to different tasks.</p><p hwp:id="p-32">For example, many of the applications of mutual information come from machine learning, where mutual information can be used for feature selection [<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>] and independent component analysis [<xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>]. Mutual information is also used in medical image analysis to align two images to the same coordinate system [<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>]. These two machine learning applications are concerned with finding independent random variables, and hence the mutual information estimator must be accurate near values close to zero. Medical image registration requires finding a coordinate system that maximises mutual information between images, and so the estimator must be accurate for large values. This is the regime which is important for network inference, where we are concerned with detecting strong dependencies between the expression of two genes.</p><p hwp:id="p-33">One approach to estimating mutual information is to place the data into bins and use histograms to estimate the required distributions. This simple approach is the Maximum Likelihood estimator and is known to be negatively biased [<xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>]. The Miller-Madow estimator [<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>] adds a correction to correct this bias, while the Shrinkage estimator [<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>] uses a uniform distribution to regularise the empirical distribution. Since network inference produces a ranked list of scores from mutual information estimates a constant bias added to each value should not affect the inferred network. The performance of these histogram-based estimators depend heavily on an appropriate choice of origin, the number of bins and bin locations.</p><p hwp:id="p-34">The Chao-Shen estimator [<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>] was developed to estimate the diversity of biological species from small sample sizes using entropy. It is adapted to account for species that are not included in a specific sample, which corresponds to an empty bin. When using the Chao-Shen estimator for network inference, continuous expression data is binned using parameters that are decided by the user. Since the user controls the binning of expression data it is not clear how the Chao-Shen estimator’s design motivations are applicable for network inference, but it is included as it widely used and has a straightforward implementation.</p><p hwp:id="p-35">Binning continuous data is a hard operation that is sensitive to noise. The B-spline, Kernel Density and <italic toggle="yes">k</italic>-nearest-neighbour estimators use smoothing methods to obtain more accurate estimates of the distributions. The B-spline estimator [<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>] is an adaptation of the Maximum Likelihood estimator that allows data points to be placed in multiple bins, thus smoothing the distribution. This procedure also reduces the dependence of the result on the choice of origin and the bin locations. The Kernel Density estimator [<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>] estimates the distributions using kernel functions rather than rectangular bins. Finally, the <italic toggle="yes">k</italic>-nearest-neighbour estimator [<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>] estimates the distribution using distances to nearest neighbours. Two additional estimators were included that use the Spearman and Pearson correlation with (7) to calculate the mutual information.</p><p hwp:id="p-36">The parameters of the various estimators are shown in <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>. Further information on the estimators and their parameters is given in <xref ref-type="sec" rid="s5a" hwp:id="xref-sec-28-1" hwp:rel-id="sec-28">Section S1</xref>.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2 xref-table-wrap-2-3 xref-table-wrap-2-4 xref-table-wrap-2-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-3"><p hwp:id="p-37">The parameters of the different mutual information estimators. See <xref ref-type="sec" rid="s5a" hwp:id="xref-sec-28-2" hwp:rel-id="sec-28">Section S1</xref> for further details on the estimators and their parameters. The parameters of the Kernel Density estimator were not investigated.</p></caption><graphic xlink:href="132647_tbl2" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap></sec></sec><sec id="s3" hwp:id="sec-11" hwp:rev-id="xref-sec-11-1"><label>3</label><title hwp:id="title-12">Results</title><sec id="s3a" hwp:id="sec-12" hwp:rev-id="xref-sec-12-1 xref-sec-12-2"><label>3.1</label><title hwp:id="title-13">Comparing mutual information estimators</title><p hwp:id="p-38">A parameter study, which is discussed in <xref rid="s3b" ref-type="sec" hwp:id="xref-sec-16-1" hwp:rel-id="sec-16">Section 3.2</xref>, attempted to identify parameters that led to large AUPRC values across all the datasets. Separate parameters were identified for each inference algorithm. These are displayed in <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref>. As the best performing inference algorithm, CLR will be the focus of this discussion.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3:</label><caption hwp:id="caption-4"><p hwp:id="p-39">The parameters that maximise the AUPRC for each combination of inference algorithm and mutual information estimator. These parameters were chosen following the investigation whose results are presented in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-1" hwp:rel-id="sec-44">Section S5</xref>. How these parameters were chosen is discussed in <xref rid="s3b" ref-type="sec" hwp:id="xref-sec-16-2" hwp:rel-id="sec-16">Section 3.2.</xref> The parameters of the Kernel Density estimator were not investigated and the only the values suggested by the authors of [<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">34</xref>] were used. The Spearman and Pearson correlation estimators do not have parameters.</p></caption><graphic xlink:href="132647_tbl3" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap><sec id="s3a1" hwp:id="sec-13"><label>3.1.1</label><title hwp:id="title-14"><italic toggle="yes">in silico</italic> dataset</title><p hwp:id="p-40">The AUPRC values for the <italic toggle="yes">in silico</italic> dataset are shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>. CLR gives the largest AUPRC values while ARACNE gives the lowest. This is the case for all three datasets. Furthermore, the B-spline estimator gives the largest AUPRC for both CLR and MRNET. For MRNET the benefit of using the B-spline estimator means that it performs better than CLR with many of the mutual information estimators. This is also true of MRNET with the Kernel Density estimator, but the AUPRC is lower than using MRNET with B-spline.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-5"><p hwp:id="p-41">The resulting area under precision-recall curve (AUPRC) when using the different mutual information estimators with each of the inference algorithms on the <italic toggle="yes">in silico</italic> dataset. The parameters for each estimator are those which were found to maximise the AUPRC following the investigation described in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-2" hwp:rel-id="sec-44">Section S5</xref>. The results of this investigation can be found in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-3" hwp:rel-id="sec-44">Section S5</xref>.</p></caption><graphic xlink:href="132647_fig2" position="float" orientation="portrait" hwp:id="graphic-12"/></fig></sec><sec id="s3a2" hwp:id="sec-14"><label>3.1.2</label><title hwp:id="title-15"><italic toggle="yes">E. coli</italic> dataset</title><p hwp:id="p-42">The AUPRC for the different mutual information estimators is shown in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>. The AUPRC values are lower for the <italic toggle="yes">E. coli</italic> dataset than for the <italic toggle="yes">in silico</italic> dataset, which reflects the added difficulty of inferring a network and obtaining an accurate gold standard for real biological systems (see <xref rid="s4a" ref-type="sec" hwp:id="xref-sec-20-1" hwp:rel-id="sec-20">Section 4.1</xref> for discussion on this topic). For this dataset there is not an estimator that performs significantly better than the others. However, the Kernel Density estimator performs significantly worse than the other estimators for all three inference algorithms, and MRNET in particular.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-6"><p hwp:id="p-43">The resulting area under precision-recall curve (AUPRC) when using the different mutual information estimators with each of the inference algorithms on the <italic toggle="yes">E. coli</italic> dataset. The parameters for each estimator are those which were found to maximise the AUPRC following the investigation described in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-4" hwp:rel-id="sec-44">Section S5</xref>. The results of this investigation can be found in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-5" hwp:rel-id="sec-44">Section S5</xref>.</p></caption><graphic xlink:href="132647_fig3" position="float" orientation="portrait" hwp:id="graphic-13"/></fig></sec><sec id="s3a3" hwp:id="sec-15"><label>3.1.3</label><title hwp:id="title-16"><italic toggle="yes">S. cerevisiae</italic> dataset</title><p hwp:id="p-44">The AUPRC for the different mutual information estimators is shown in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>. For this dataset the variation in AUPRC between the estimators is negligible. The parameter study also found negligible variation in AUPRC with the parameters of all the estimators whose parameters were investigated (not the Kernel Density estimator). Possible explanations are discussed in <xref rid="s4b" ref-type="sec" hwp:id="xref-sec-25-1" hwp:rel-id="sec-25">Section 4.2</xref>.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-7"><p hwp:id="p-45">The resulting area under precision-recall curve (AUPRC) when using the different mutual information estimators with each of the inference algorithms on the <italic toggle="yes">S. cerevisiae</italic> dataset. The parameters for each estimator are those which were found to maximise the AUPRC following the investigation described in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-6" hwp:rel-id="sec-44">Section S5</xref>. The results of this investigation can be found in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-7" hwp:rel-id="sec-44">Section S5</xref>.</p></caption><graphic xlink:href="132647_fig4" position="float" orientation="portrait" hwp:id="graphic-14"/></fig></sec></sec><sec id="s3b" hwp:id="sec-16" hwp:rev-id="xref-sec-16-1 xref-sec-16-2"><label>3.2</label><title hwp:id="title-17">Mutual information estimator parameters</title><p hwp:id="p-46">Part of this study examined estimator parameters with the aim of identifying optimal parameters that maximise the AUPRC. The full results can be found in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-8" hwp:rel-id="sec-44">Section S5</xref>.</p><p hwp:id="p-47">It was found in [<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">7</xref>] that the optimal parameters of the Maximum Likelihood, Miller-Madow and Shrinkage estimators are different for CLR, MRNET and ARACNE. Therefore it is expected that the optimal parameters will be different for the three inference algorithms. If possible, parameters were chosen that gave large AUPRC values on all three datasets. If parameters led to good performance on just the <italic toggle="yes">in silico</italic> and <italic toggle="yes">E. coli</italic> datasets then they were chosen. The <italic toggle="yes">E. coli</italic> dataset was given preference as its AUPRC results were found to vary more with parameters than the <italic toggle="yes">S. cerevisiae</italic> results.</p><p hwp:id="p-48">The parameters that maximise the AUPRC when using CLR are shown in <xref rid="tbl4" ref-type="table" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Table 4</xref>. CLR was included here as it was the best-performing inference algorithm on all three datasets (see <xref rid="s3a" ref-type="sec" hwp:id="xref-sec-12-1" hwp:rel-id="sec-12">Section 3.1</xref>), but equivalent results for MRNET and ARACNE are available in <xref ref-type="sec" rid="s5f" hwp:id="xref-sec-55-1" hwp:rel-id="sec-55">Section S6</xref>. <xref rid="tbl4" ref-type="table" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">Table 4</xref> shows that there is more consistency within datasets than within estimators.</p><table-wrap id="tbl4" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBL4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tbl4</object-id><label>Table 4:</label><caption hwp:id="caption-8"><p hwp:id="p-49">The mutual information estimator parameters that maximise the AUPRC when using the CLR network inference algorithm. The Spearman and Pearson correlation estimators do not have parameters, while the parameters of the Kernel Density estimator were not investigated. The parameters of each estimator are shown in <xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2</xref> and described in detail in <xref ref-type="sec" rid="s5a" hwp:id="xref-sec-28-3" hwp:rel-id="sec-28">Section S1</xref>.</p></caption><graphic xlink:href="132647_tbl4" position="float" orientation="portrait" hwp:id="graphic-15"/></table-wrap><sec id="s3b1" hwp:id="sec-17"><label>3.2.1</label><title hwp:id="title-18">Number of bins for histogram-based estimators</title><p hwp:id="p-50">This parameter study identified the number of bins as the key parameter of the Maximum Likeli-hood, Miller-Madow, Chao-Shen and Shrinkage estimators. Common choices are <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="132647_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> bins or <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="132647_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> bins, which lead to the same number of bins for all genes. Another option is the Freedman-Diaconis rule [<xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>], which uses bins with size 2 <italic toggle="yes">·</italic><inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="132647_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula>, where IQR is the inter-quartile range of the expression profile of a gene [<xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">36</xref>]. Note that this leads to different numbers of bins for each gene. The Freedman-Diaconis rule resulted in low AUPRC values and its results are not included here. See the <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-9" hwp:rel-id="sec-44">Section S5</xref> for the full results. Since the Bayesian Blocks algorithm chooses the number of bins automatically, another option is to use the number of bins chosen by Bayesian Blocks but with equal frequency or equal width bins.</p><p hwp:id="p-51">Both Bayesian Blocks and Freedman-Diaconis bins choose the number of bins for each gene, resulting in a distribution. These distributions are shown for each dataset in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>. The Freedman-Diaconis rule produces results that are approximately centred at <italic toggle="yes">N</italic><sup>1/2</sup>. For Bayesian Blocks the resulting distribution is closer to <italic toggle="yes">N</italic><sup>1/3</sup> and also has a smaller variance than the corresponding distribution for the Freedman-Diaconis rule.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-9"><p hwp:id="p-52">The resulting number of bins for each gene in the three datasets. The Freedman-Diaconis rule and Bayesian Blocks choose different numbers of bins for each gene, while the using <italic toggle="yes">N</italic><sup>1/2</sup> or <italic toggle="yes">N</italic><sup>1/3</sup> bins gives the same number for all genes. 16 genes with large numbers of Freedman-Diaconis bins have been excluded from the <italic toggle="yes">in silico</italic> plot for aesthetic reasons.</p></caption><graphic xlink:href="132647_fig5" position="float" orientation="portrait" hwp:id="graphic-16"/></fig><p hwp:id="p-53">It is expected that the AUPRC from <italic toggle="yes">N</italic><sup>1/3</sup> or the number of bins from Bayesian Blocks should give a similar AUPRC as they result in a similar number of bins (see <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5</xref>). This is observed in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figures 6</xref> and <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">7</xref>, which show that these two bin number rules give a larger AUPRC than <italic toggle="yes">N</italic><sup>1/2</sup> bins for the <italic toggle="yes">in silico</italic> and <italic toggle="yes">E. coli</italic> datasets. The relative increase in AUPRC is smaller for the <italic toggle="yes">E. coli</italic> dataset than <italic toggle="yes">in silico</italic>. These results also show that the AUPRC is more robust to the Binning Method than the number of bins. For the <italic toggle="yes">S. cerevisiae</italic> dataset, whose results are shown in <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref>, there is not a large variation in AUPRC between the different bin number rules. The lack of variation in AUPRC between estimators and parameters for the <italic toggle="yes">S. cerevisiae</italic> dataset will be discussed in <xref rid="s4b" ref-type="sec" hwp:id="xref-sec-25-2" hwp:rel-id="sec-25">Section 4.2</xref>.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-10"><p hwp:id="p-54">The AUPRC for the <italic toggle="yes">in silico</italic> dataset with the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. An individual plot shows the AUPRC when using a single inference algorithm and a specific number of bins with the four estimators. Each grouping of bars represents a single estimator for the three binning methods. Each column shows results for a single inference algorithm and each row shows results for a single number of bins. Note that when using the Bayesian Blocks binning method the number of bins is chosen automatically, hence the AUPRC values within columns are the same for the same MI estimator, but are included in all plots for ease of comparison. The Freedman-Diaconis rule has been removed from this plot as its AUPRC were significantly lower than the other number of bins but are included in <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Figure S2</xref>.</p></caption><graphic xlink:href="132647_fig6" position="float" orientation="portrait" hwp:id="graphic-17"/></fig><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-11"><p hwp:id="p-55">The AUPRC for the <italic toggle="yes">E. coli</italic> dataset with the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. An individual plot shows the AUPRC when using a single inference algorithm and a specific number of bins with the four estimators. Each grouping of bars represents a single estimator for the three binning methods. Each column shows results for a single inference algorithm and each row shows results for a single number of bins. Note that when using the Bayesian Blocks binning method the number of bins is chosen automatically, hence the AUPRC values within columns are the same for the same MI estimator, but are included in all plots for ease of comparison. The Freedman-Diaconis rule has been removed from this plot as its AUPRC were significantly lower than the other number of bins but are included in <xref ref-type="fig" rid="figS5" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Figure S5</xref></p></caption><graphic xlink:href="132647_fig7" position="float" orientation="portrait" hwp:id="graphic-18"/></fig><fig id="fig8" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8:</label><caption hwp:id="caption-12"><p hwp:id="p-56">The AUPRC for the <italic toggle="yes">S. cerevisiae</italic> dataset with the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. An individual plot shows the AUPRC when using a single inference algorithm and a specific number of bins with the four estimators. Each grouping of bars represents a single estimator for the three binning methods. Each column shows results for a single inference algorithm and each row shows results for a single number of bins. Note that when using the Bayesian Blocks binning method the number of bins is chosen automatically, hence the AUPRC values within columns are the same for the same MI estimator, but are included in all plots for ease of comparison. The Freedman-Diaconis rule has been removed from this plot as its AUPRC were significantly lower than the other number of bins but are included in <xref ref-type="fig" rid="figS8" hwp:id="xref-fig-18-1" hwp:rel-id="F18">Figure S8</xref></p></caption><graphic xlink:href="132647_fig8" position="float" orientation="portrait" hwp:id="graphic-19"/></fig><p hwp:id="p-57">These findings indicate that using the number of bins found by Bayesian Blocks improves the accuracy of inferred networks, even if the bin locations found by Bayesian Blocks are not used. A simpler alternative is to use <italic toggle="yes">N</italic><sup>1/3</sup> bins, which is typically close to the number of bins used by Bayesian Blocks.</p></sec></sec><sec id="s3c" hwp:id="sec-18"><label>3.3</label><title hwp:id="title-19">Computation times</title><p hwp:id="p-58">The computation time of each estimator is clearly an important practical consideration. The number of gene pairs is <italic toggle="yes">&#x01d4aa;</italic> (<italic toggle="yes">p</italic><sup>2</sup>) for <italic toggle="yes">p</italic> genes and linear in the number of samples. <xref rid="tbl5" ref-type="table" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Table 5</xref> shows the computation time of several implementations of the mutual information estimators used in this study. In producing this work several estimators were implemented using parallel C++ with an R interface and are available in the R package <italic toggle="yes">fastGeneMI</italic>. The package can be downloaded at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://bitbucket.org/Jonathan-Ish-Horowicz/fastgenemi/src" ext-link-type="uri" xlink:href="https://bitbucket.org/Jonathan-Ish-Horowicz/fastgenemi/src" hwp:id="ext-link-2">https://bitbucket.org/Jonathan-Ish-Horowicz/fastgenemi/src</ext-link>. <xref rid="tbl5" ref-type="table" hwp:id="xref-table-wrap-5-2" hwp:rel-id="T5">Table 5</xref> also includes the computation times when using alternative implementations that compute mutual information estimates from a matrix of continuous expression values. Another R package, “synRNASeqNet,” enables the calculation of mutual information estimates from binned data only and so was not included. Note that using the Bayesian Blocks binning method with the datasets from this study requires additional computation time of order 100 seconds.</p><table-wrap id="tbl5" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1 xref-table-wrap-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBL5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tbl5</object-id><label>Table 5:</label><caption hwp:id="caption-13"><p hwp:id="p-59">Time to compute the mutual information between the expression of all gene pairs for the <italic toggle="yes">E. coli</italic> dataset (4,297 genes, 805 samples) with different implementations of the estimators used in this study. Results are given in seconds, with the lowest time in bold text. In brackets is the computation time relative to the fastest implementation for that estimator. Computations were performed by 16 2.6 GHz Intel Sandy Bridge cores. If an implementation allows parallel computation then all 16 cores were used. n/a indicates that a package does not provide an implementation for that estimator.</p></caption><graphic xlink:href="132647_tbl5" position="float" orientation="portrait" hwp:id="graphic-20"/></table-wrap><p hwp:id="p-60">The histogram-based estimators have the shortest computation times, followed by the B-spline estimator. The computation time of the B-spline estimator is an order of magnitude larger. The Kernel Density and <italic toggle="yes">k</italic>-nearest-neighbour estimators have computation times two orders of magnitude larger than the histogram-based methods.</p></sec></sec><sec id="s4" hwp:id="sec-19"><label>4</label><title hwp:id="title-20">Discussion</title><sec id="s4a" hwp:id="sec-20" hwp:rev-id="xref-sec-20-1"><sec id="s4a1" hwp:id="sec-21"><title hwp:id="title-21">CLR was the best performing inference algorithm</title><p hwp:id="p-61">The results in <xref rid="s3a" ref-type="sec" hwp:id="xref-sec-12-2" hwp:rel-id="sec-12">Section 3.1</xref> provide further evidence that CLR is the state of the art mutual-information based inference algorithm [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-6" hwp:rel-id="ref-1">1</xref>, <xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-4" hwp:rel-id="ref-7">7</xref>]. Furthermore, it has been shown here to be robust to both the choice of estimator and the estimator parameters. This was evident across all the estimators whose parameters were investigated (see <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-10" hwp:rel-id="sec-44">Section S5</xref>). MRNET is able to match the performance of CLR, but this requires specific choices of estimator and parameters.</p></sec><sec id="s4a2" hwp:id="sec-22"><title hwp:id="title-22">The B-spline estimator was the best performing mutual information estimator</title><p hwp:id="p-62">We recommend using the B-spline estimator, supporting the findings of [<xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>, <xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>]. The AUPRC from the B-spline was among the largest for the <italic toggle="yes">in silico</italic> and <italic toggle="yes">E. coli</italic> datasets for each inference algorithm used in this study. The B-spline estimator performed less well on the <italic toggle="yes">S. cerevisiae</italic> dataset, however the variation in AUPRC for this dataset was smaller than the other two. The B-spline computation time is also significantly lower than for other smoothing estimators (Kernel Density and <italic toggle="yes">k</italic>-nearest-neighbour). The computation time is longer than the other histogram-based estimators, but these lead to lower AUPRC values. This may be because the binning of data is a hard operation and is thus extremely sensitive to noise and the locations of the bins. The B-spline estimator reduces this sensitivity by placing samples in multiple bins. The B-spline estimator was also found to be robust to changes in its parameters (see <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Figures S3</xref>, <xref ref-type="fig" rid="figS6" hwp:id="xref-fig-16-1" hwp:rel-id="F16">S6</xref> and <xref ref-type="fig" rid="figS9" hwp:id="xref-fig-19-1" hwp:rel-id="F19">S9</xref>).</p></sec><sec id="s4a3" hwp:id="sec-23"><title hwp:id="title-23">When using an estimator that bins expression data, use <italic toggle="yes">N</italic><sup>1/3</sup> bins for <italic toggle="yes">N</italic> samples</title><p hwp:id="p-63">These results indicate that when using a mutual information estimator that bins the data, using <italic toggle="yes">N</italic><sup>1/3</sup> bins is the optimal choice. This contradicts a previous finding by Kurt et al. who reported that <italic toggle="yes">N</italic><sup>1/2</sup> bins led to the most accurate inferred network [<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-3" hwp:rel-id="ref-22">22</xref>]. These differing results may be because that study used a F-score as the performance metric. We believe that AUPRC is a more appropriate metric for network inference and therefore that <italic toggle="yes">N</italic><sup>1/3</sup> is a superior choice of bin number. As discussed in <xref rid="s2a" ref-type="sec" hwp:id="xref-sec-8-1" hwp:rel-id="sec-8">Section 2.1</xref>, CLR, MRNET and ARACNE produce a list of edges that are ranked by their corresponding scores. To produce a single network from this ranked list a threshold is chosen, and any edge with a score greater than the threshold is designated as an edge. The F-score can only be evaluated for a single network and is therefore highly dependent on the choice of threshold. The AUPRC is computed using all possible thresholds by definition and so removes this dependency.</p></sec><sec id="s4a4" hwp:id="sec-24"><title hwp:id="title-24">Bayesian Blocks did not improve the accuracy of the inferred networks on the real datasets</title><p hwp:id="p-64">For the <italic toggle="yes">in silico</italic> dataset, using Bayesian Blocks bins led to larger AUPRC values than equal frequency or equal width bins. However, this was not the case for either of the real dataset. A possible explanation for this difference lies in the different processing of the synthetic and real expression data. The <italic toggle="yes">in silico</italic> expression data are raw values as simulated by GeneNetWeaver, while the real expression data have undergone microarray normalisation followed by a log transform. The expression of a single gene of the real datasets is therefore more normal than a gene from the synthetic dataset, which has a large number of zeros. This can be seen by comparing <xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figures 9</xref> and <xref rid="fig10" ref-type="fig" hwp:id="xref-fig-10-1" hwp:rel-id="F10">10</xref>. <xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Figure 9</xref> shows a concentration of values at zero. Bayesian Blocks represents these values using a narrow bin with a high frequency density, but equal frequency and equal width bins do not represent this feature of the data well. Real expression data (<xref rid="fig10" ref-type="fig" hwp:id="xref-fig-10-2" hwp:rel-id="F10">Figure 10</xref>) does not have this feature, and the three binning methods give qualitatively similar bins. For more information on how the expression data was prepared see the Supplementary Material of [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-7" hwp:rel-id="ref-1">1</xref>].</p><fig id="fig9" position="float" fig-type="figure" orientation="portrait" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3 xref-fig-9-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9:</label><caption hwp:id="caption-14"><p hwp:id="p-65">Histograms of the expression of the gadE gene (index 126) of the <italic toggle="yes">in silico</italic> dataset using the three binning methods. For the equal width and equal frequency methods the number of bins were chosen to match those used in the Bayesian Blocks case. For the unprocessed synthetic data there are are large number of zeros which are best represented by the Bayesian Blocks bins.</p></caption><graphic xlink:href="132647_fig9" position="float" orientation="portrait" hwp:id="graphic-21"/></fig><fig id="fig10" position="float" fig-type="figure" orientation="portrait" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIG10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">fig10</object-id><label>Figure 10:</label><caption hwp:id="caption-15"><p hwp:id="p-66">Histograms of the expression of the secE gene (index 1263) of the <italic toggle="yes">E. coli</italic> dataset using the three binning methods. For the equal width and equal frequency methods the number of bins were chosen to match those used in the Bayesian Blocks case. The three binning methods give more similar results for the processed real data than for the synthetic data (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Figure 9</xref>).</p></caption><graphic xlink:href="132647_fig10" position="float" orientation="portrait" hwp:id="graphic-22"/></fig><p hwp:id="p-67">For all the datasets the number of bins found by Bayesian Blocks was close to <italic toggle="yes">N</italic><sup>1/3</sup>. Furthermore, since the recommended inference algorithm-mutual information estimator combination (CLR-B-spline) is robust to the number of bins, we recommend using <italic toggle="yes">N</italic><sup>1/3</sup> bins.</p></sec></sec><sec id="s4b" hwp:id="sec-25" hwp:rev-id="xref-sec-25-1 xref-sec-25-2"><label>4.1</label><title hwp:id="title-25">The relationship between synthetic and real expression data</title><p hwp:id="p-68">These results raise questions about the relationship between synthetic and real expression data. Synthetic expression data generators are valuable and widely used tools for benchmarking network inference algorithms. However, the AUPRC values on the <italic toggle="yes">E. coli</italic> and <italic toggle="yes">S. cerevisiae</italic> datasets were significantly lower than on the <italic toggle="yes">in silico</italic> dataset. Furthermore, the AUPRC values for the <italic toggle="yes">in silico</italic> exhibited greater sensitivity to both the choice of estimator and estimator parameters.</p><p hwp:id="p-69">There are many causes of the decrease in AUPRC when moving from a synthetic to a real datasets. Measuring expression using DNA microarrays requires a set of complex steps, each of which introduce noise. Each of these steps involves several choices that are also known to impact on the measured expression values, such as the choice of normalisation procedure and the sample procedure [<xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>]. Although synthetic datasets allow the user to control additive noise it is not clear what is an appropriate noise amplitude to choose. It is also unlikely that additive noise is a good approximation of the noise resulting from a series of complex experimental techniques.</p><p hwp:id="p-70">An additional source of the discrepancy in AURPC between the real and synthetic data concerns the gold standard network. While the synthetic gold standard is known exactly, the real gold standard networks contain many false negatives. For example, the DREAM5 Network Inference Challenge identified 53 <italic toggle="yes">E. coli</italic> potentially novel interactions using a consensus of predictions by a range of inference algorithms. These were edges that were not present in the gold standard networks but were confidently predicted as edges by a number of inference algorithms. Of these interactions, 23 (43%) were subsequently verified experimentally [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-8" hwp:rel-id="ref-1">1</xref>]. Furthermore, since the networks used by GeneNetWeaver are sub-networks of real biological networks they are intrinsically biased against undiscovered regulatory links. That is to say, their potential to aid in the discovery of novel regulatory interactions is limited since their network topologies are based on incomplete biological knowledge. These comments do not aim to denigrate their use in the development of novel inference algorithms, simply to note that there remains a significant discrepancy between the inference of synthetic and real transcriptional regulatory networks.</p></sec><sec id="s4c" hwp:id="sec-26"><label>4.2</label><title hwp:id="title-26">Comparing the results of the <italic toggle="yes">E. coli</italic> and <italic toggle="yes">S. cerevisiae</italic> datasets</title><p hwp:id="p-71">A similar trend was observed between the two real datasets. The <italic toggle="yes">E. coli</italic> dataset AUPRC statistics were larger and more dependent on the estimator and its parameters than the <italic toggle="yes">S. cerevisiae</italic> dataset results. One cause may be that this dataset contains 563 samples while the <italic toggle="yes">E. coli</italic> dataset contain 805 samples. The lower number of samples makes it more difficult to estimate the probability distributions required for an estimate of the mutual information, and so it may be that all of the estimators are producing inaccurate mutual information estimates.</p><p hwp:id="p-72">There may also be a biological explanation for the discrepancy in results between the two real datasets. <italic toggle="yes">E. coli</italic> is a prokaryote and is therefore less complex than the eukaryotic <italic toggle="yes">S. cerevisiae</italic>. The added complexity of the <italic toggle="yes">S. cerevisiae</italic> transcriptional regulatory network is most likely a primary cause of the lower AURPC results. The lack of variation of AURPC for the <italic toggle="yes">S. cerevisiae</italic> dataset could suggest that mutual information-based inference algorithms have difficulty inferring its transcriptional regulatory network. However, the AUPRC values found in [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-9" hwp:rel-id="ref-1">1</xref>] are essentially constant across several other types of inference algorithm. This suggests either that the currently available approaches are fundamentally flawed, or that the <italic toggle="yes">S. cerevisiae</italic> gold standard is significantly more incomplete than that of <italic toggle="yes">E. coli</italic>. Specifically, the integration of heterogeneous data sources may be required for an accurate reconstruction of the <italic toggle="yes">S. cerevisiae</italic> transcriptional regulatory network [<xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>]. Another possible explanation is that a larger part of the transcriptional regulation in eukaryotes is performed by complexes of transcription factors and other proteins, which are impossible to identify using only pairwise interactions.</p></sec></sec></body><back><sec id="s5" sec-type="supplementary-material" hwp:id="sec-27"><title hwp:id="title-27">Supplementary Material</title><sec id="s5a" hwp:id="sec-28" hwp:rev-id="xref-sec-28-1 xref-sec-28-2 xref-sec-28-3 xref-sec-28-4 xref-sec-28-5"><label>S1</label><title hwp:id="title-28">Descriptions of Estimators</title><p hwp:id="p-73">This section consists of a detailed description of the MI estimators used in this study. As mentioned above, to estimate <italic toggle="yes">I</italic>(<italic toggle="yes">x, y</italic>) requires estimates of the marginal distributions <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>), <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>) and the joint distribution <italic toggle="yes">p</italic>(<italic toggle="yes">x, y</italic>). Estimates of these distributions will be denoted with as <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="132647_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula>.</p><p hwp:id="p-74">The Maximum Likelihood, Miller-Madow, Chao-Shen, Shrinkage and B-spline estimators were implemented in parallel C++ with an R interface. These implementations are available in the R package <italic toggle="yes">fastGeneMI</italic>. The package can be downloaded from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://bitbucket.org/Jonathan-Ish-Horowicz/fastgenemi/src" ext-link-type="uri" xlink:href="https://bitbucket.org/Jonathan-Ish-Horowicz/fastgenemi/src" hwp:id="ext-link-3">https://bitbucket.org/Jonathan-Ish-Horowicz/fastgenemi/src</ext-link>. The <italic toggle="yes">depEst</italic> implementation [<xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>] of the Kernel Density estimator was used with default parameters throughout. The <italic toggle="yes">k</italic>-nearest-neighbour estimator implementation from the <italic toggle="yes">parmigene</italic> R package was used [<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>].</p><sec id="s5a1" hwp:id="sec-29"><label>S1.1</label><title hwp:id="title-29">Maximum Likelihood Estimator</title><p hwp:id="p-75">The first histogram-based method is the Maximum Likelihood estimator, which discretises the continuous expression data into bins then approximates the marginal probability distributions using
<disp-formula id="eqn8" hwp:id="disp-formula-8">
<alternatives hwp:id="alternatives-12"><graphic xlink:href="132647_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-23"/></alternatives>
</disp-formula>
where <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> are the range of values covered by the <italic toggle="yes">i</italic>-th bin, <italic toggle="yes">N</italic><sub><italic toggle="yes">i</italic></sub> is the number of samples that fall into bin <italic toggle="yes">i</italic> and <italic toggle="yes">N</italic> = Σ<sub><italic toggle="yes">i</italic></sub> <italic toggle="yes">N</italic><sub><italic toggle="yes">i</italic></sub> is the total number of samples. Similarly, the estimate of the joint distribution is
<disp-formula id="eqn9" hwp:id="disp-formula-9">
<alternatives hwp:id="alternatives-13"><graphic xlink:href="132647_eqn9.gif" position="float" orientation="portrait" hwp:id="graphic-24"/></alternatives>
</disp-formula>
where <italic toggle="yes">y</italic><sub><italic toggle="yes">j</italic></sub> is the range of the <italic toggle="yes">j</italic>-th bin in <italic toggle="yes">y</italic>, <italic toggle="yes">N</italic><sub><italic toggle="yes">ij</italic></sub> are the number of samples in the <italic toggle="yes">i</italic>-th bin for <italic toggle="yes">x</italic> and the <italic toggle="yes">j</italic>-th bin for <italic toggle="yes">y</italic> and <italic toggle="yes">N</italic> = Σ<sub><italic toggle="yes">i</italic></sub> Σ<sub><italic toggle="yes">j</italic></sub> <italic toggle="yes">N</italic><sub><italic toggle="yes">ij</italic></sub>. The Maximum Likelihood entropy estimates are then given by
<disp-formula id="eqn10" hwp:id="disp-formula-10">
<alternatives hwp:id="alternatives-14"><graphic xlink:href="132647_eqn10.gif" position="float" orientation="portrait" hwp:id="graphic-25"/></alternatives>
</disp-formula>
and
<disp-formula id="eqn11" hwp:id="disp-formula-11">
<alternatives hwp:id="alternatives-15"><graphic xlink:href="132647_eqn11.gif" position="float" orientation="portrait" hwp:id="graphic-26"/></alternatives>
</disp-formula>
</p><p hwp:id="p-76">A mutual information estimate is then calculated using (3). (8) and (9) are often called “empirical distributions. The Maximum Likelihood estimator is known to be negatively biased, especially for small sample sizes [<xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">30</xref>]. It is also known as the “Empirical,” “Plug-in” or “Naive” estimator.</p></sec><sec id="s5a2" hwp:id="sec-30"><label>S1.2</label><title hwp:id="title-30">Miller-Madow Estimator</title><p hwp:id="p-77">Introduced in [<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">31</xref>] by Miller, this estimator introduces an additional term to the entropy estimates and (11) to correct for the negative bias of the Maximum Likelihood estimator. The entropy estimates are
<disp-formula id="eqn12" hwp:id="disp-formula-12">
<alternatives hwp:id="alternatives-16"><graphic xlink:href="132647_eqn12.gif" position="float" orientation="portrait" hwp:id="graphic-27"/></alternatives>
</disp-formula>
and
<disp-formula id="eqn13" hwp:id="disp-formula-13">
<alternatives hwp:id="alternatives-17"><graphic xlink:href="132647_eqn13.gif" position="float" orientation="portrait" hwp:id="graphic-28"/></alternatives>
</disp-formula>
</p><p hwp:id="p-78">Where <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-18"><inline-graphic xlink:href="132647_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> is the number of non-empty bins. A mutual information estimate is then found using (3).</p></sec><sec id="s5a3" hwp:id="sec-31"><label>S1.3</label><title hwp:id="title-31">Chao-Shen Estimator</title><p hwp:id="p-79">This estimator, proposed in [<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">33</xref>] by Chao and Shen, was developed for the estimation of the diversity of biological species via their entropy. In this context each bin corresponds to a species. This entropy estimator attempts to correct for the fact that some species may not be included in a single set of samples from a population. This is equivalent to a bin being empty.</p><p hwp:id="p-80">The first component is a Horvitz-Thompson estimator, which attempts to correct for varying proportions of observations within strata in a stratified sample of a target population [<xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>]. If <italic toggle="yes">N</italic> samples have been drawn from a population then the probability that bin <italic toggle="yes">i</italic> has been included is <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-19"><inline-graphic xlink:href="132647_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula>. This probability is inverse weighted for each bin when calculating the entropy.</p><p hwp:id="p-81">The second component of this estimator is a Good-Turing correction, which attempts to account for previously empty bins [<xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>]. An approximate form of the correction is
<disp-formula id="eqn14" hwp:id="disp-formula-14">
<alternatives hwp:id="alternatives-20"><graphic xlink:href="132647_eqn14.gif" position="float" orientation="portrait" hwp:id="graphic-29"/></alternatives>
</disp-formula>
where <italic toggle="yes">f</italic><sub>1</sub> is the number of bins with a single count. Here we are assuming that the number of empty bins is the fraction of the bins with a single count. This correction is multiplied by all the estimates of the distributions in (4) and (5).</p><p hwp:id="p-82">Together, these give the Chao-Shen entropy estimators
<disp-formula id="eqn15" hwp:id="disp-formula-15">
<alternatives hwp:id="alternatives-21"><graphic xlink:href="132647_eqn15.gif" position="float" orientation="portrait" hwp:id="graphic-30"/></alternatives>
</disp-formula>
and
<disp-formula id="eqn16" hwp:id="disp-formula-16">
<alternatives hwp:id="alternatives-22"><graphic xlink:href="132647_eqn16.gif" position="float" orientation="portrait" hwp:id="graphic-31"/></alternatives>
</disp-formula>
from which we can compute a mutual information estimate using (3).</p><p hwp:id="p-83">For continuous gene expression data we choose the number and width of the bins, meaning that we control the number of empty bins. Therefore the motivation of the Chao-Shen estimator, which was developed for discrete data, is not directly applicable in the context of network inference.</p></sec><sec id="s5a4" hwp:id="sec-32"><label>S1.4</label><title hwp:id="title-32">Shrinkage Estimator</title><p hwp:id="p-84">This estimator, by Hausser and Strimmer [<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">32</xref>], attempts to prevent overfitting using a convex sum of the empirical estimate of the distribution and a “target” distribution. For marginals, this “shrinkage” distribution is given by
<disp-formula id="eqn17" hwp:id="disp-formula-17">
<alternatives hwp:id="alternatives-23"><graphic xlink:href="132647_eqn17.gif" position="float" orientation="portrait" hwp:id="graphic-32"/></alternatives>
</disp-formula>
where <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub> is the target distribution and <italic toggle="yes">λ</italic><sub>M</sub> <italic toggle="yes">∈</italic> [0, 1] controls the relative weight of the two distributions. The target distribution is chosen to be the uniform distribution, 1/number of bins, which maximises the entropy. <italic toggle="yes">λ</italic><sub>M</sub> is given by
<disp-formula id="eqn18" hwp:id="disp-formula-18">
<alternatives hwp:id="alternatives-24"><graphic xlink:href="132647_eqn18.gif" position="float" orientation="portrait" hwp:id="graphic-33"/></alternatives>
</disp-formula>
and is truncated to fall in [0,1]. The joint “shrinkage” distribution is
<disp-formula id="eqn19" hwp:id="disp-formula-19">
<alternatives hwp:id="alternatives-25"><graphic xlink:href="132647_eqn19.gif" position="float" orientation="portrait" hwp:id="graphic-34"/></alternatives>
</disp-formula>
where
<disp-formula id="eqn20" hwp:id="disp-formula-20">
<alternatives hwp:id="alternatives-26"><graphic xlink:href="132647_eqn20.gif" position="float" orientation="portrait" hwp:id="graphic-35"/></alternatives>
</disp-formula>
where the target distribution is once again 1/number of bins and <italic toggle="yes">λ</italic><sub>J</sub> <italic toggle="yes">∈</italic> [0, 1]. The shrinkage entropy estimates are
<disp-formula id="eqn21" hwp:id="disp-formula-21">
<alternatives hwp:id="alternatives-27"><graphic xlink:href="132647_eqn21.gif" position="float" orientation="portrait" hwp:id="graphic-36"/></alternatives>
</disp-formula>
and
<disp-formula id="eqn22" hwp:id="disp-formula-22">
<alternatives hwp:id="alternatives-28"><graphic xlink:href="132647_eqn22.gif" position="float" orientation="portrait" hwp:id="graphic-37"/></alternatives>
</disp-formula>
from which a mutual information estimate can be computed using (3).</p></sec><sec id="s5a5" hwp:id="sec-33"><label>S1.5</label><title hwp:id="title-33">B-Spline Estimator</title><p hwp:id="p-85">This method, proposed by Daub et al. in [<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">8</xref>], is a modification of the Maximum Likelihood estimator that places samples in multiple bins, thus smoothing the distribution. This smoothing is achieved using B-splines.</p><p hwp:id="p-86">A B-spline is a piecewise polynomial function, with each polynomial being a linear combination of so-called “basis functions.” In a B-spline of order <italic toggle="yes">k</italic> these basis functions are polynomials with degree <italic toggle="yes">k -</italic> 1. The piecewise polynomials are joined at the elements of a <italic toggle="yes">knot vector t</italic>, which has <italic toggle="yes">k</italic> + <italic toggle="yes">m</italic> elements when smoothing <italic toggle="yes">m</italic> bins using a B-spline of order <italic toggle="yes">k</italic>. The elements of the knot vector are given by
<disp-formula id="eqn23" hwp:id="disp-formula-23">
<alternatives hwp:id="alternatives-29"><graphic xlink:href="132647_eqn23.gif" position="float" orientation="portrait" hwp:id="graphic-38"/></alternatives>
</disp-formula>
</p><p hwp:id="p-87">The knot vector is non-decreasing and fully determines the B-spline basis functions. For a B-spline of order <italic toggle="yes">k</italic> the basis functions are defined between <italic toggle="yes">k</italic> consecutive knots. The basis functions are determined recursively from <italic toggle="yes">k</italic> = 0 until the desired spline order is reached. For <italic toggle="yes">z ∈</italic> [0<italic toggle="yes">, m-k</italic>+1], the range of the knot vector, the <italic toggle="yes">m</italic> basis functions (one basis function per bin) are given by the Cox-de Boor recursion formula [<xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>]:
<disp-formula id="eqn24" hwp:id="disp-formula-24">
<alternatives hwp:id="alternatives-30"><graphic xlink:href="132647_eqn24.gif" position="float" orientation="portrait" hwp:id="graphic-39"/></alternatives>
</disp-formula>
<disp-formula id="eqn25" hwp:id="disp-formula-25">
<alternatives hwp:id="alternatives-31"><graphic xlink:href="132647_eqn25.gif" position="float" orientation="portrait" hwp:id="graphic-40"/></alternatives>
</disp-formula>
</p><p hwp:id="p-88">For <italic toggle="yes">k</italic> = 1 the basis functions are step functions that are 1 within a bin and 0 elsewhere, so any sample is placed only in the bin that covers its position. In fact, a B-spline of order <italic toggle="yes">k</italic> places each sample into <italic toggle="yes">k</italic> bins, meaning that a B-spline estimator with <italic toggle="yes">k</italic> = 1 is equivalent to the Maximum Likelihood estimator. This is because there are <italic toggle="yes">k</italic> basis functions defined at each value of <italic toggle="yes">z</italic>, and each basis function corresponds to a single bin. Since the sum of the basis functions at any <italic toggle="yes">z</italic> is 1 we can use them to place a each point into <italic toggle="yes">k</italic> bins based on its <italic toggle="yes">z</italic>-value. This requires that we transform the expression values of a single gene to the range of the knot vector using
<disp-formula id="eqn26" hwp:id="disp-formula-26">
<alternatives hwp:id="alternatives-32"><graphic xlink:href="132647_eqn26.gif" position="float" orientation="portrait" hwp:id="graphic-41"/></alternatives>
</disp-formula>
where <italic toggle="yes">x</italic> is the expression of a single gene and <italic toggle="yes">x</italic><sub><italic toggle="yes">max</italic></sub> and <italic toggle="yes">x</italic><sub><italic toggle="yes">min</italic></sub> are its maximum and minimum expression values.</p><p hwp:id="p-89">Evaluating the B-spline for single gene gives a matrix <italic toggle="yes">A</italic><sup><italic toggle="yes">x</italic></sup> <italic toggle="yes">∈ ℛ</italic><sup><italic toggle="yes">N ×m</italic></sup> whose <italic toggle="yes">ij</italic>-th element is the weighting of sample <italic toggle="yes">i</italic> in bin <italic toggle="yes">j</italic>. We then compute the marginal probability of this gene using
<disp-formula id="eqn27" hwp:id="disp-formula-27">
<alternatives hwp:id="alternatives-33"><graphic xlink:href="132647_eqn27.gif" position="float" orientation="portrait" hwp:id="graphic-42"/></alternatives>
</disp-formula>
</p><p hwp:id="p-90">After obtaining the marginal distribution for another gene with expression profile <italic toggle="yes">y</italic> and obtaining <italic toggle="yes">A</italic><sup><italic toggle="yes">y</italic></sup> we can compute the joint probability distribution using
<disp-formula id="eqn28" hwp:id="disp-formula-28">
<alternatives hwp:id="alternatives-34"><graphic xlink:href="132647_eqn28.gif" position="float" orientation="portrait" hwp:id="graphic-43"/></alternatives>
</disp-formula>
</p><p hwp:id="p-91">Then we compute the mutual information of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> using (3) - (5).</p></sec><sec id="s5a6" hwp:id="sec-34"><label>S1.6</label><title hwp:id="title-34">Kernel Density Estimator</title><p hwp:id="p-92">Proposed by Moon et al. in [<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-3" hwp:rel-id="ref-34">34</xref>], this estimator replaces rectangular histogram bins with kernels when estimating the probability distributions. The estimate of the probability density at expression <italic toggle="yes">x</italic> is given by
<disp-formula id="eqn29" hwp:id="disp-formula-29">
<alternatives hwp:id="alternatives-35"><graphic xlink:href="132647_eqn29.gif" position="float" orientation="portrait" hwp:id="graphic-44"/></alternatives>
</disp-formula>
where <italic toggle="yes">K</italic>(<italic toggle="yes">u</italic><sub><italic toggle="yes">k</italic></sub>) is the kernel function. This is a function of
<disp-formula id="eqn30" hwp:id="disp-formula-30">
<alternatives hwp:id="alternatives-36"><graphic xlink:href="132647_eqn30.gif" position="float" orientation="portrait" hwp:id="graphic-45"/></alternatives>
</disp-formula>
where <italic toggle="yes">x</italic><sub><italic toggle="yes">k</italic></sub> is a measured expression value, <italic toggle="yes">S</italic> is the covariance matrix between the dimensions of <italic toggle="yes">x</italic><sub><italic toggle="yes">k</italic></sub> and <italic toggle="yes">h</italic> is the kernel bandwidth. Note that for this estimator <italic toggle="yes">x</italic> is a vector of expression values at which we can sample the probability distribution and is not restricted to the measured expression values. In the original paper by Moon et al. the grid positions were chosen to be the locations of the data, but more recent implementations use a uniform grid over a range of [<italic toggle="yes">x</italic><sub><italic toggle="yes">min</italic></sub> <italic toggle="yes">-</italic> 1.5<italic toggle="yes">h, x</italic><sub><italic toggle="yes">max</italic></sub> +1.5<italic toggle="yes">h</italic>], where <italic toggle="yes">x</italic><sub><italic toggle="yes">min</italic></sub> and <italic toggle="yes">x</italic><sub><italic toggle="yes">max</italic></sub> are the minimum and maximum measured expression values.</p><p hwp:id="p-93">For marginal distributions <italic toggle="yes">x</italic> and <italic toggle="yes">x</italic><sub><italic toggle="yes">k</italic></sub> are scalars and <italic toggle="yes">S</italic> is the variance of the expression profile whose density is being estimated. For joint distributions, <italic toggle="yes">x</italic> and <italic toggle="yes">x</italic><sub><italic toggle="yes">k</italic></sub> are 2-D vectors and <italic toggle="yes">S</italic> is the 2x2 covariance matrix of the gene pair whose joint density is being estimated.</p><p hwp:id="p-94">Using a <italic toggle="yes">u</italic><sub><italic toggle="yes">k</italic></sub> of this form is known as the Fukunaga method [<xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>], where the data has been linearly transformed to have a unit covariance matrix. This is equivalent to a whitening transformation of
<disp-formula id="eqn31" hwp:id="disp-formula-31">
<alternatives hwp:id="alternatives-37"><graphic xlink:href="132647_eqn31.gif" position="float" orientation="portrait" hwp:id="graphic-46"/></alternatives>
</disp-formula>
where Λ and <italic toggle="yes">V</italic> are the eigenvalue and eigenvector matrices of the sample covariance matrix of <italic toggle="yes">x</italic>.</p><p hwp:id="p-95">To find the mutual information between two genes with expression profiles <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> the density estimates <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-38"><inline-graphic xlink:href="132647_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-39"><inline-graphic xlink:href="132647_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> sampled on the grid described above. The mutual information is then calculated using (2), where the sums are now over all grid positions.</p><p hwp:id="p-96">This estimator has two parameters: the kernel function and the bandwidth. A common choice is the normalised Gaussian kernel:
<disp-formula id="eqn32" hwp:id="disp-formula-32">
<alternatives hwp:id="alternatives-40"><graphic xlink:href="132647_eqn32.gif" position="float" orientation="portrait" hwp:id="graphic-47"/></alternatives>
</disp-formula>
where <italic toggle="yes">d</italic> is the dimension of the density being estimated (1 for marginal distributions and 2 for joint distributions). The choice of <italic toggle="yes">h</italic> is more important than the choice of kernel when estimating probability densities. In the original paper, Moon et al. suggest using the optimal Gaussian bandwidth according to Silverman [<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>]:
<disp-formula id="eqn33" hwp:id="disp-formula-33">
<alternatives hwp:id="alternatives-41"><graphic xlink:href="132647_eqn33.gif" position="float" orientation="portrait" hwp:id="graphic-48"/></alternatives>
</disp-formula>
</p><p hwp:id="p-97">This choice minimises the mean squared error in <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-42"><inline-graphic xlink:href="132647_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> if the true distribution is multivariate Gaussian and was found to give to comparable mutual information estimates to data-driven (and computationally expensive) bandwidth selection methods.</p><p hwp:id="p-98">When performing density estimation for visualisation it is common to log-transform non-negative data to obtain a density estimate that is also non-negative. This should not affect the mutual information estimate due to the invariance of mutual information under homeomorphisms [<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">35</xref>], but this study will investigate numerically the effect of the log-transformation on the mutual information estimate. It is also common to normalise the expression of each gene across all the samples to variance 1.</p></sec><sec id="s5a7" hwp:id="sec-35"><label>S1.7</label><title hwp:id="title-35"><italic toggle="yes">k</italic>-Nearest-Neighbour Estimator</title><p hwp:id="p-99">This approach utilises previous work by Kozachenko and Leonenko on estimating probability distributions and entropy from nearest neighbour distances [<xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>], which was itself based on work by Vasicek [<xref rid="c48" ref-type="bibr" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>]. Kraskov et al extended the analysis to produce a mutual information estimator [<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-3" hwp:rel-id="ref-35">35</xref>].</p><p hwp:id="p-100">For two expression profiles <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, we make the pair <italic toggle="yes">z</italic> = (<italic toggle="yes">x, y</italic>). For this space we use the maximum norm
<disp-formula id="eqn34" hwp:id="disp-formula-34">
<alternatives hwp:id="alternatives-43"><graphic xlink:href="132647_eqn34.gif" position="float" orientation="portrait" hwp:id="graphic-49"/></alternatives>
</disp-formula>
and compute <italic toggle="yes">ε</italic>(<italic toggle="yes">i</italic>)/2, the distance of <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub> = (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, y</italic><sub><italic toggle="yes">i</italic></sub>)<italic toggle="yes">, i</italic> = 1<italic toggle="yes">, …, N</italic> to its <italic toggle="yes">k</italic>-th nearest neighbour. Using (34),
<disp-formula id="eqn35" hwp:id="disp-formula-35">
<alternatives hwp:id="alternatives-44"><graphic xlink:href="132647_eqn35.gif" position="float" orientation="portrait" hwp:id="graphic-50"/></alternatives>
</disp-formula>
where <italic toggle="yes">ε</italic><sub><italic toggle="yes">x</italic></sub>(<italic toggle="yes">i</italic>) and <italic toggle="yes">ε</italic><sub><italic toggle="yes">x</italic></sub>(<italic toggle="yes">i</italic>) are the projections of <italic toggle="yes">ε</italic>(<italic toggle="yes">i</italic>) onto the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> directions. We then count the number of samples with ∥ <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> <italic toggle="yes">- x</italic><sub><italic toggle="yes">j</italic></sub> ∥ <italic toggle="yes">&lt; ε</italic><sub><italic toggle="yes">x</italic></sub>(<italic toggle="yes">i</italic>)/2 for all other samples <italic toggle="yes">j</italic>, and label it <italic toggle="yes">n</italic><sub><italic toggle="yes">x</italic></sub>(<italic toggle="yes">i</italic>). Similarly, we count the number of samples with ∥ <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub> <italic toggle="yes">- y</italic><sub><italic toggle="yes">j</italic></sub> <italic toggle="yes">∥ &lt; ε</italic><sub><italic toggle="yes">y</italic></sub>(<italic toggle="yes">i</italic>)/2 and label it <italic toggle="yes">n</italic><sub><italic toggle="yes">y</italic></sub>(<italic toggle="yes">i</italic>). Finally, the estimate for the mutual information between <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> is
<disp-formula id="eqn36" hwp:id="disp-formula-36">
<alternatives hwp:id="alternatives-45"><graphic xlink:href="132647_eqn36.gif" position="float" orientation="portrait" hwp:id="graphic-51"/></alternatives>
</disp-formula>
where <italic toggle="yes">ψ</italic>(<italic toggle="yes">x</italic>) is the digamma function satisfying
<disp-formula id="eqn37" hwp:id="disp-formula-37">
<alternatives hwp:id="alternatives-46"><graphic xlink:href="132647_eqn37.gif" position="float" orientation="portrait" hwp:id="graphic-52"/></alternatives>
</disp-formula>
and Γ(<italic toggle="yes">x</italic>) is the gamma function.</p><p hwp:id="p-101">This version of the <italic toggle="yes">k</italic>-NN estimator, also known as the KSG estimator after the authors of [<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-4" hwp:rel-id="ref-35">35</xref>], has relatively few theoretical results but it has been shown that the KSG estimator struggles to detect stronger relationships [<xref rid="c49" ref-type="bibr" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>]. This may make it inappropriate for gene network inference, where we are primarily concerned with detecting the strongest relationships between expression profiles.</p><p hwp:id="p-102">Implementations of this estimator add a small random noise (<italic toggle="yes">∼</italic> 10<sup>-9</sup>) to break ties between nearest neighbour distances. The investigation presented in <xref ref-type="sec" rid="s5e" hwp:id="xref-sec-44-11" hwp:rel-id="sec-44">Section S5</xref> found that the AUPRC only changes beyond its 5th decimal place for noise with maximum amplitudes of 10<sup>-9</sup>, 10<sup>-10</sup> and 10<sup>-11</sup>.</p></sec><sec id="s5a8" hwp:id="sec-36"><label>S1.8</label><title hwp:id="title-36">Parameters of the Histogram-based methods</title><p hwp:id="p-103"><xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-3" hwp:rel-id="T2">Table 2</xref> shows the parameters of each estimator. Further details on the parameters of the histogram-based estimators, the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators is presented below.</p><sec id="s5a8a" hwp:id="sec-37"><label>S1.8.1</label><title hwp:id="title-37">Binning method</title><p hwp:id="p-104">For the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators there are three binning methods available:</p><list list-type="order" hwp:id="list-2"><list-item hwp:id="list-item-6"><p hwp:id="p-105">Equal width</p></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-106">Equal frequency</p></list-item><list-item hwp:id="list-item-8"><p hwp:id="p-107">Bayesian Blocks</p></list-item></list><p hwp:id="p-108">Histograms of the expression of the same gene are shown in <xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-4" hwp:rel-id="F9">Figure 9</xref> using the three binning methods.</p><p hwp:id="p-109">For equal width binning with <italic toggle="yes">n</italic> bins we divide the range of the data into bins with width (max - min)<italic toggle="yes">/n</italic>. Equal frequency binning uses <italic toggle="yes">n</italic> bins such that each has an equal frequency, which is equivalent to the area of the resulting histogram bar.</p><p hwp:id="p-110">An alternative to these methods is the Bayesian Blocks algorithm, which was developed by Scargle et al. to model astronomical time series data using piecewise constant functions [<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref>]. One application of the Bayesian Blocks algorithm is to place the times of discrete events into bins, where both the number and locations of the bins are chosen to best fit the data. This is equivalent to creating a histogram that is an optimal representation of data. This is achieved by optimising a fitness function, which can be any measure that quantifies how well a constant function fits the data in a bin. This work uses the default fitness function for this application,
<disp-formula id="eqn38" hwp:id="disp-formula-38">
<alternatives hwp:id="alternatives-47"><graphic xlink:href="132647_eqn38.gif" position="float" orientation="portrait" hwp:id="graphic-53"/></alternatives>
</disp-formula>
where <italic toggle="yes">k</italic> is the block index, <italic toggle="yes">N</italic><sup>(<italic toggle="yes">k</italic>)</sup> is the number of counts in block <italic toggle="yes">k</italic>, <italic toggle="yes">T</italic><sup>(<italic toggle="yes">k</italic>)</sup> is the length of the block and <italic toggle="yes">λ</italic> is the single parameter of the piecewise constant model.</p><p hwp:id="p-111">The B-spline estimator can only be used with equal width bins.</p></sec></sec></sec><sec id="s5b" hwp:id="sec-38" hwp:rev-id="xref-sec-38-1 xref-sec-38-2"><label>S2</label><title hwp:id="title-38">Mutual Information Inference Algorithms</title><p hwp:id="p-112">Three mutual information-based network inference algorithms will be used in this work. They are CLR, MRNET and ARACNE. As described in <xref rid="s2a" ref-type="sec" hwp:id="xref-sec-8-2" hwp:rel-id="sec-8">Section 2.1</xref>, each inference algorithm takes a symmetric matrix of mutual information value as an input. From this matrix the algorithm computes a list of scores, which are ranked in descending order. If a score between two genes has a high rank this reflects a confident prediction of an edge between them.</p><p hwp:id="p-113">The implementations of the inference algorithms were taken from the R package <italic toggle="yes">minet</italic> [<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>].</p><p hwp:id="p-114">The procedure by which the scores are computed is designed in order to remove indirect connections. Examples of network motifs that contain indirect connections are illustrated in <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Figure S1</xref>.</p><fig id="figS1" position="float" fig-type="figure" orientation="portrait" hwp:id="F11" hwp:rev-id="xref-fig-11-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1:</label><caption hwp:id="caption-16"><p hwp:id="p-115">Two examples of network motifs that can cause systematic errors in network inference algorithms. This is due to dependence between the expression of two genes that are only indirectly connected in the true regulatory network. Network inference algorithms attempt to remove these indirect connections. In (a) there will be dependence between the expression of genes A and C, while in (b) there will be dependence between the expression of genes B and C.</p></caption><graphic xlink:href="132647_figS1" position="float" orientation="portrait" hwp:id="graphic-54"/></fig><sec id="s5b1" hwp:id="sec-39"><label>S2.1</label><title hwp:id="title-39">CLR</title><p hwp:id="p-116">The Context Likelihood of Relatedness (CLR) algorithm is the most recent of the three inference algorithms and was develoed by Faith et al. in 2007 [<xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>]. It considers the mutual information between the expression levels of a target gene and a second gene in the context of the distribution of the mutual information between the target gene and all other genes. The score of gene <italic toggle="yes">i</italic> with gene <italic toggle="yes">j</italic> is given by
<disp-formula id="eqn39" hwp:id="disp-formula-39">
<alternatives hwp:id="alternatives-48"><graphic xlink:href="132647_eqn39.gif" position="float" orientation="portrait" hwp:id="graphic-55"/></alternatives>
</disp-formula>
where <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, x</italic><sub><italic toggle="yes">j</italic></sub> are the expressions of genes <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> and <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, σ</italic><sub><italic toggle="yes">i</italic></sub> are the mean and standard deviation of the distribution <italic toggle="yes">I</italic>(<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, x</italic><sub><italic toggle="yes">k</italic></sub>)<italic toggle="yes">, k</italic> = 1<italic toggle="yes">, …, N, k ≠ i</italic>. The final score between genes <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> is then given <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-49"><inline-graphic xlink:href="132647_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula>.</p></sec><sec id="s5b2" hwp:id="sec-40"><label>S2.2</label><title hwp:id="title-40">MRNET</title><p hwp:id="p-117">MRNET is based on maximum relevance/minimum redundancy (MRMR), an information-theoretic feature selection technique that is common in machine learning [<xref rid="c50" ref-type="bibr" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>]. “Maximum relevance” refers to choosing features that have a high mutual information with the target variable. “Minimum redundancy” means that these features are chosen such that the mutual information between them is as low as possible. Olsen et al. applied MRMR in a network inference context, in which the target is the expression of gene <italic toggle="yes">i</italic> and the features are the expression levels of all other genes. The steps of the algorithm are as follows:
<list list-type="order" hwp:id="list-3"><list-item hwp:id="list-item-9"><p hwp:id="p-118">Select the expression profile <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> which has the largest mutual information with the target expression profile <italic toggle="yes">y</italic>.</p></list-item><list-item hwp:id="list-item-10"><p hwp:id="p-119">Select the next expression profile <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub> as the one that maximises
<disp-formula id="ueqn1" hwp:id="disp-formula-40">
<alternatives hwp:id="alternatives-50"><graphic xlink:href="132647_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-56"/></alternatives>
</disp-formula>
where <italic toggle="yes">X</italic><sub><italic toggle="yes">s</italic></sub> is the set of previously selected expression profiles.</p></list-item><list-item hwp:id="list-item-11"><p hwp:id="p-120">The score between genes <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> is given by <italic toggle="yes">s</italic><sub><italic toggle="yes">ij</italic></sub> = max(<italic toggle="yes">s</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, s</italic><sub><italic toggle="yes">j</italic></sub>).</p></list-item></list></p></sec><sec id="s5b3" hwp:id="sec-41"><label>S2.3</label><title hwp:id="title-41">ARACNE</title><p hwp:id="p-121">The Algorithm for the Reconstruction of Accurate Cellular Networks was proposed by Margolin et al. in 2006 [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-3" hwp:rel-id="ref-6">6</xref>]. It starts by computing the mutual information between all the pairs of genes. These values are then filtered using a threshold that corresponds to a <italic toggle="yes">p</italic>-value in the null hypothesis of two independent genes. It then attempts to remove indirect connections using the Data Processing Inequality [<xref rid="c51" ref-type="bibr" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>], which states that if an interaction between genes <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> depends on <italic toggle="yes">k</italic> then
<disp-formula id="eqn40" hwp:id="disp-formula-41">
<alternatives hwp:id="alternatives-51"><graphic xlink:href="132647_eqn40.gif" position="float" orientation="portrait" hwp:id="graphic-57"/></alternatives>
</disp-formula>
</p><p hwp:id="p-122">So for each gene triplet <italic toggle="yes">i</italic>, <italic toggle="yes">j</italic> and <italic toggle="yes">k</italic>, the lowest mutual information of <italic toggle="yes">I</italic>(<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, x</italic><sub><italic toggle="yes">j</italic></sub>), <italic toggle="yes">I</italic>(<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">, x</italic><sub><italic toggle="yes">k</italic></sub>) and <italic toggle="yes">I</italic>(<italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub><italic toggle="yes">, x</italic><sub><italic toggle="yes">k</italic></sub>) can only come from an indirect connection and so the corresponding edge is removed (its score is set to zero).</p></sec></sec><sec id="s5c" hwp:id="sec-42" hwp:rev-id="xref-sec-42-1 xref-sec-42-2"><label>S3</label><title hwp:id="title-42">Data</title><p hwp:id="p-123">This study will use the <italic toggle="yes">in silico</italic>, <italic toggle="yes">e.coli</italic> and <italic toggle="yes">s.cerevisiae</italic> datasets from the DREAM5 Network Inference Challenge [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-10" hwp:rel-id="ref-1">1</xref>]. The name <italic toggle="yes">in silico</italic> refers to the fact that this dataset has been simulated using GeneNetWeaver, a tool that uses ordinary differential equation models to simulate the expression of biologically inspired regulatory networks [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>]. The <italic toggle="yes">e.coli</italic> and <italic toggle="yes">s.cerevisiae</italic> datasets use experimental data.</p><p hwp:id="p-124">Each dataset consists of an expression matrix and a “gold standard” network, against which predictions are evaluated. The <italic toggle="yes">in silico</italic> expression matrix contains the raw values as simulated by GeneNetWeaver. The other two expression matrices have undergone microarray normalisation followed by a log transform. Further details on the three datasets is displayed in <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>.</p><p hwp:id="p-125">The gold standard is only known for the <italic toggle="yes">in silico</italic> dataset. For the <italic toggle="yes">e.coli</italic> dataset was been constructed from RegulonDB, a database of experimentally verified transcriptional interactions [<xref rid="c52" ref-type="bibr" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]. The gold standard for the <italic toggle="yes">s.cerevisiae</italic> dataset was constructed following the reanalysis of ChIP-chip data [<xref rid="c53" ref-type="bibr" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>]. The gold standards for the non-synthetic datasets are among the best in the field, but are almost certainly incomplete. Only interactions with strong experimental verification are included as edges in the network, so there may be many false negatives. Furthermore, only a subset of the genes are marked as potential regulators, and so the evaluation of an inferred network only occurs using edges between the regulators and other genes.</p><p hwp:id="p-126">Detailed information on the source of all three datasets and the gold standard networks is available in the Supplementary Material of [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-11" hwp:rel-id="ref-1">1</xref>].</p></sec><sec id="s5d" hwp:id="sec-43" hwp:rev-id="xref-sec-43-1 xref-sec-43-2"><label>S4</label><title hwp:id="title-43">Evaluating network predictions - precision and recall</title><p hwp:id="p-127">Network inference is a two-class classification task, where each pair of genes is classified as having an edge between them or not. Receiver operating characteristic (ROC) curves are a common method by which to evaluate such a classifier.</p><p hwp:id="p-128">A predicted edge can be a true positive (TP), false positive (FP), true negative (TN) or false negative (FN). A ROC curve plots the true positive rate
<disp-formula id="eqn41" hwp:id="disp-formula-42">
<alternatives hwp:id="alternatives-52"><graphic xlink:href="132647_eqn41.gif" position="float" orientation="portrait" hwp:id="graphic-58"/></alternatives>
</disp-formula>
against the false positive rate
<disp-formula id="eqn42" hwp:id="disp-formula-43">
<alternatives hwp:id="alternatives-53"><graphic xlink:href="132647_eqn42.gif" position="float" orientation="portrait" hwp:id="graphic-59"/></alternatives>
</disp-formula>
for various thresholds and evaluate the areas under the resulting curve (AUROC). A perfect classifier has area 1, while a random classifier has area <sup><inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-54"><inline-graphic xlink:href="132647_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula></sup>. However, ROC curves are known to be potentially misleading in classification tasks with large class imbalances, which is the case in network inference as well as many other biological classification problems [<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref>, <xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>]. Therefore we have a large number of true negatives that are of little interest when we evaluate our predicted networks.</p><p hwp:id="p-129">As an alternative we use precision-recall curves, where
<disp-formula id="eqn43" hwp:id="disp-formula-44">
<alternatives hwp:id="alternatives-55"><graphic xlink:href="132647_eqn43.gif" position="float" orientation="portrait" hwp:id="graphic-60"/></alternatives>
</disp-formula>
and Recall is equivalent to the true positive rate. Neither precision nor recall consider the number of true negatives. Similarly to a ROC curve, we plot the precision and recall for all thresholds. A more accurate classifier has a larger area under a precision-recall curve (AUPRC), which has a maximum value of 1 for a perfect classifier.</p><p hwp:id="p-130">The F- or F1-score is twice the harmonic mean of the precision and recall and is given by
<disp-formula id="eqn44" hwp:id="disp-formula-45">
<alternatives hwp:id="alternatives-56"><graphic xlink:href="132647_eqn44.gif" position="float" orientation="portrait" hwp:id="graphic-61"/></alternatives>
</disp-formula>
</p><p hwp:id="p-131">F-scores take values in [0, 1] and are close to 1 if both the precision and recall are close to 1, and close to 0 otherwise.</p><p hwp:id="p-132">The area under precision recall curve was computed using the <italic toggle="yes">PRROC</italic> R package [<xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref>].</p></sec><sec id="s5e" hwp:id="sec-44" hwp:rev-id="xref-sec-44-1 xref-sec-44-2 xref-sec-44-3 xref-sec-44-4 xref-sec-44-5 xref-sec-44-6 xref-sec-44-7 xref-sec-44-8 xref-sec-44-9 xref-sec-44-10 xref-sec-44-11"><label>S5</label><title hwp:id="title-44">Finding the best parameters for each estimator</title><p hwp:id="p-133">This section will identify the best parameters for the Maximum Likelihood, Miller-Madow, Chao-Shen, Shrinkage, B-spline and <italic toggle="yes">k</italic>-nearest-neighbours estimators. To recap, for the Maxiumum Like-lihood, Miller-Madow, Chao-Shen and Shrinkage estimators these are the number of bins and the binning method, unless the Bayesian Blocks binning method is used, in which case the number of bins are selected automatically. For the B-spline method we must choose the number of bins and the spline order. For the <italic toggle="yes">k</italic>-nearest-neighbour estimator we choose <italic toggle="yes">k</italic> and the amplitude of the noise added to each sample to break ties. The Kernel Density estimator has parameters (bandwidth and kernel function) but these were not investigated.</p><sec id="s5e1" hwp:id="sec-45"><label>S5.1</label><title hwp:id="title-45"><italic toggle="yes">In silico</italic> data</title><sec id="s5e1a" hwp:id="sec-46"><label>S5.1.1</label><title hwp:id="title-46">Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators</title><p hwp:id="p-134">First we compare the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. These estimators have been grouped together as they share the same parameters: binning method and number of bins. These results are shown in <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-12-2" hwp:rel-id="F12">Figure S2</xref>. This work is not a comparison of inference algorithms, however it is worth noting that CLR (plots a, d, g and j in <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Figure S2</xref>) consistently has the largest AUPRC while ARACNE (plots c, f, i and l) has the smallest. This pattern is observed across all the mutual information estimators and datasets used in this study and will not be commented on again.</p><p hwp:id="p-135">Using Bayesian Blocks increases the AUPRC for all 4 estimators when using CLR or ARACNE. For MRNET, Bayesian Blocks is the optimal parameter choice for the Miller-Madow and Shrinkage estimators.</p><p hwp:id="p-136">For all of these estimators using <italic toggle="yes">N</italic><sup>1/3</sup> bins is preferable to <italic toggle="yes">N</italic><sup>1/2</sup> bins, but both of these choices give lower AUPRCs than Bayesian Blocks. Of these two choices, <italic toggle="yes">N</italic><sup>1/3</sup> gives a larger AUPRC. This is probably due to <italic toggle="yes">N</italic><sup>1/3</sup> being closer to the number of bins used by Bayesian Blocks than <italic toggle="yes">N</italic><sup>1/2</sup>, as shown in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 5</xref>. The Freedman-Diaconis rule leads to a far lower AUPRC than the other choices of bin number for all the estimators and inference algorithms.</p><p hwp:id="p-137">Since the Bayesian Blocks algorithm chooses the number of bins automatically, this improvement may be due to a more appropriate number of bins rather than the positioning of the bins. Plots g, h and i of <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-12-4" hwp:rel-id="F12">Figure S2</xref> show the AUPRC when using Equal Width and Equal Frequency bins with the number of bins chosen by Bayesian Blocks. These plots show that the locations of the bins also have a positive impact on the AUPRC, but that the majority of the increase in AUPRC when using Bayesian Blocks is from choosing an optimal number of bins.</p><p hwp:id="p-138">From these results we can conclude that using Bayesian Blocks improves the performance of inference algorithms on this dataset. Furthermore, the number of bins is a more important parameter than the binning method for this dataset, in that if the number of bins is chosen “well,” then all the estimators are robust to the choice between equal frequency and equal width bins. Finally, using <italic toggle="yes">N</italic><sup>1/2</sup> equal width bins with the shrinkage estimator results in a low AUPRC for all three inference algorithms.</p></sec><sec id="s5e1b" hwp:id="sec-47"><label>S5.1.2</label><title hwp:id="title-47">B-spline estimator</title><p hwp:id="p-139"><xref ref-type="fig" rid="figS3" hwp:id="xref-fig-13-2" hwp:rel-id="F13">Figure S3</xref> shows the AUPRC when using the B-spline estimator on the <italic toggle="yes">in silico</italic> dataset with various spline orders and bin numbers. For all three inference algorithms increasing the spline order generally increases the AUPRC, however for some choices of the number of bins this is not the case. For example, when using MRNET with 10 bins the AUPRC decreases as the spline order increases. For ARACNE the increase in AUPRC with spline order is smaller than for CLR or MRNET, and is barely noticeable.</p><p hwp:id="p-140">Over the three inference algorithms and the various spline orders using <italic toggle="yes">N</italic><sup>1/3</sup> gives either the largest or almost largest AUPRC. For this dataset the integer value of <italic toggle="yes">N</italic><sup>1/3</sup> is 9, which is close to the recommended value of 10. Accordingly, the AUPRC between the two parameter choices are very similar. Using the same number of bins as Bayesian Blocks leads to a slightly lower AUPRC, and using <italic toggle="yes">N</italic><sup>1/2</sup> bins is significantly worse. Once again, using the Freedman-Diaconis rule leads to a much lower AUPRC than any other parameter choice.</p><p hwp:id="p-141">It is also interesting to note the increase in AUPRC for MRNET when using the B-spline estimator. When using the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators the AUPRC of MRNET was significantly below CLR, however the difference is much smaller when using the B-spline estimator.</p></sec><sec id="s5e1c" hwp:id="sec-48"><label>S5.1.3</label><title hwp:id="title-48"><italic toggle="yes">k</italic>-Nearest-Neighbour estimator</title><p hwp:id="p-142"><xref ref-type="fig" rid="figS4" hwp:id="xref-fig-14-1" hwp:rel-id="F14">Figure S4</xref> shows the results for the <italic toggle="yes">k</italic>-NN estimator for a range of values of <italic toggle="yes">k</italic>. For both CLR and MRNET the AUPRC increases from <italic toggle="yes">k</italic> = 2 to <italic toggle="yes">k</italic> 10 and is approximately constant for larger values of <italic toggle="yes">k</italic>. For ARACNE this increase in AUPRC stops for <italic toggle="yes">k ≥</italic> 5. For CLR, MRNET and ARACNE the largest AUPRC is when <italic toggle="yes">k</italic> = 13, 14 and 14 respectively.</p></sec></sec><sec id="s5e2" hwp:id="sec-49"><label>S5.2</label><title hwp:id="title-49"><italic toggle="yes">E. coli</italic> data</title><sec id="s5e2a" hwp:id="sec-50"><label>S5.2.1</label><title hwp:id="title-50">Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators</title><p hwp:id="p-143">The AUPRC values for the <italic toggle="yes">E. coli</italic> dataset when using the Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators are shown in <xref ref-type="fig" rid="figS5" hwp:id="xref-fig-15-2" hwp:rel-id="F15">Figure S5</xref>. The AUPRC values are lower for this dataset than for the <italic toggle="yes">in silico</italic> dataset, which reflects the added difficulty of both inferring a network and obtaining an accurate gold standard for real biological systems. The variation in AUPRC between estimators, binning methods and bin numbers is also smaller for <italic toggle="yes">E. coli</italic> than for <italic toggle="yes">in silico</italic>.</p><p hwp:id="p-144">Unlike for the <italic toggle="yes">in silico</italic> dataset, using Bayesian Blocks does not increase the AURPC for any of the mutual information estimator-inference algorithm combinations. However, the inference algorithms are more robust to the choice of bin number and binning method.</p><p hwp:id="p-145">Equal width bins are preferable for all the estimators when using CLR and all but the Chao-Shen estimator for MRNET. For ARACNE, equal frequency bins give a larger AUPRC for the Maximum Likelihood, Miller-Madow and Shrinkage estimators.</p><p hwp:id="p-146">For CLR, using <italic toggle="yes">N</italic><sup>1/3</sup> bins (which are similar values, as shown by <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5</xref>) gives the highest AUPRC for all of these estimators. MRNET follows the same trend but the Chao-Shen estimator has the largest AUPRC when used with the same number of bins as Bayesian Blocks. For ARACNE, the Chao-Shen and Shrinkage estimators perform better when used with <italic toggle="yes">N</italic><sup>1/2</sup> bins, while with the Maximum Likelihood and Chao-Shen estimators are best used with <italic toggle="yes">N</italic><sup>1/3</sup> and the Bayesian Blocks number of bins respectively.</p><p hwp:id="p-147">There are specific parameter choices that lead to low AUPRC values. The Freedman-Diaconis rule leads to the lowest AUPRC. However, this difference is negligible when using CLR or ARACNE with the Chao-Shen and Shrinkage estimators (plots j and l in <xref ref-type="fig" rid="figS5" hwp:id="xref-fig-15-3" hwp:rel-id="F15">Figure S5</xref>).</p></sec><sec id="s5e2b" hwp:id="sec-51"><label>S5.2.2</label><title hwp:id="title-51">B-spline estimator</title><p hwp:id="p-148"><xref ref-type="fig" rid="figS6" hwp:id="xref-fig-16-2" hwp:rel-id="F16">Figure S6</xref> shows the AUPRC when using the B-spline estimator on the <italic toggle="yes">E. coli</italic> dataset with various spline orders and bin numbers. Unlike for the <italic toggle="yes">in silico</italic> dataset, the AUPRC does not increase with spline order. When varying the number of bins the change in AUPRC is <italic toggle="yes">∼</italic> 0.001, except when using MRNET with the Freedman-Diaconis rule. In this case the AUPRC is significantly lower than all choices of bin number.</p><p hwp:id="p-149">Yet again, CLR performs best with ARACNE performing worst, however, now the Freedman-Diaconis rule only leads to low AUPRC values when using MRNET. For both CLR and ARACNE the number of bins does not strongly affect the AUPRC.</p></sec><sec id="s5e2c" hwp:id="sec-52"><label>S5.2.3</label><title hwp:id="title-52"><italic toggle="yes">k</italic>-Nearest-Neighbour Estimator</title><p hwp:id="p-150">The same trends are apparent for the <italic toggle="yes">E. coli</italic> data as for the <italic toggle="yes">in silico</italic> dataset, however the difference between low and high values of <italic toggle="yes">k</italic> for CLR and MRNET are now smaller than for the <italic toggle="yes">in silico</italic> data.</p></sec></sec><sec id="s5e3" hwp:id="sec-53"><label>S5.3</label><title hwp:id="title-53"><italic toggle="yes">S. cerevisiae</italic> data</title><sec id="s5e3a" hwp:id="sec-54"><label>S5.3.1</label><title hwp:id="title-54">All estimators</title><p hwp:id="p-151">The AUPRC values for the various estimators are shown in <xref ref-type="fig" rid="figS8" hwp:id="xref-fig-18-2" hwp:rel-id="F18">Figures S8</xref>, <xref ref-type="fig" rid="figS9" hwp:id="xref-fig-19-2" hwp:rel-id="F19">S9</xref> and <xref ref-type="fig" rid="figS10" hwp:id="xref-fig-20-1" hwp:rel-id="F20">S10</xref>. This dataset has the lowest AUPRC values andn there is very little variation in AUPRC between different parameters or estimators. The variation between inference algorithms is also lower than for the <italic toggle="yes">in silico</italic> and <italic toggle="yes">E. coli</italic> datasets.</p></sec></sec><sec id="s5f" hwp:id="sec-55" hwp:rev-id="xref-sec-55-1"><label>S6</label><title hwp:id="title-55">Best estimator parameters by inference algorithm - MRNET and ARACNE</title><p hwp:id="p-152">Tables S1 and S2 show the estimator parameters that maximised the AUPRC when using MRNET and ARACNE respectively.</p><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Table S1:</label><caption hwp:id="caption-17"><p hwp:id="p-153">The mutual information estimator parameters that maximise the AUPRC when using the MRNET network inference algorithm. The parameters of each estimator are shown in <xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-4" hwp:rel-id="T2">Table 2</xref> and described in detail in <xref ref-type="sec" rid="s5a" hwp:id="xref-sec-28-4" hwp:rel-id="sec-28">Section S1</xref>. The parameters of the Kernel Density estimator were not investigated and the only the values suggested by the authors of [<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-4" hwp:rel-id="ref-34">34</xref>] were used. The Spearman and Pearson correlation estimators do not have parameters.</p></caption><graphic xlink:href="132647_tblS2" position="float" orientation="portrait" hwp:id="graphic-62"/></table-wrap><table-wrap id="tblS2" orientation="portrait" position="float" hwp:id="T7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/TBLS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tblS2</object-id><label>Table S2:</label><caption hwp:id="caption-18"><p hwp:id="p-154">The mutual information estimator parameters that maximise the AUPRC when using the ARACNE network inference algorithm. The parameters of each estimator are shown in <xref ref-type="table" rid="tbl2" hwp:id="xref-table-wrap-2-5" hwp:rel-id="T2">Table 2</xref> and described in detail in <xref ref-type="sec" rid="s5a" hwp:id="xref-sec-28-5" hwp:rel-id="sec-28">Section S1</xref>. The parameters of the Kernel Density estimator were not investigated and the only the values suggested by the authors of [<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-5" hwp:rel-id="ref-34">34</xref>] were used. The Spearman and Pearson correlation estimators do not have parameters.</p></caption><graphic xlink:href="132647_tblS1" position="float" orientation="portrait" hwp:id="graphic-63"/></table-wrap><fig id="figS2" position="float" fig-type="figure" orientation="portrait" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3 xref-fig-12-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Figure S2:</label><caption hwp:id="caption-19"><p hwp:id="p-155">The AUPRC for the <italic toggle="yes">in silico</italic> dataset with Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. An individual plot shows the AUPRC when using a single inference algorithm and a specific number of bins with the four estimators. Each grouping of bars represents a single estimator for the three binning methods. Each column shows results for a single inference algorithm and each row shows results for a single number of bins. Note that when using the Bayesian Blocks binning method the number of bins is chosen automatically, hence the AUPRC values within columns are the same for the same MI estimator, but are included in all plots for ease of comparison.</p></caption><graphic xlink:href="132647_figS2" position="float" orientation="portrait" hwp:id="graphic-64"/></fig><fig id="figS3" position="float" fig-type="figure" orientation="portrait" hwp:id="F13" hwp:rev-id="xref-fig-13-1 xref-fig-13-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Figure S3:</label><caption hwp:id="caption-20"><p hwp:id="p-156">The AUPRC results for the <italic toggle="yes">in silico</italic> dataset with the B-spline estimator. Each plot shows the AUPRC results for a single inference algorithm, with the results being grouped by spline order.</p></caption><graphic xlink:href="132647_figS3" position="float" orientation="portrait" hwp:id="graphic-65"/></fig><fig id="figS4" position="float" fig-type="figure" orientation="portrait" hwp:id="F14" hwp:rev-id="xref-fig-14-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figS4</object-id><label>Figure S4:</label><caption hwp:id="caption-21"><p hwp:id="p-157">The AUPRC results when using the <italic toggle="yes">k</italic>-Nearest-Neighbour estimator on the <italic toggle="yes">in silico</italic> dataset. Each plot contains the AUPRC results for a single inference algorithm and each bar represents the AUPRC when using <italic toggle="yes">k</italic> nearest neighbours to estimate the mutual information.</p></caption><graphic xlink:href="132647_figS4" position="float" orientation="portrait" hwp:id="graphic-66"/></fig><fig id="figS5" position="float" fig-type="figure" orientation="portrait" hwp:id="F15" hwp:rev-id="xref-fig-15-1 xref-fig-15-2 xref-fig-15-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figS5</object-id><label>Figure S5:</label><caption hwp:id="caption-22"><p hwp:id="p-158">The AUPRC for the <italic toggle="yes">E. coli</italic> dataset with Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. An individual plot shows the AUPRC when using a single inference algorithm and a specific number of bins with the four estimators. Each grouping of bars represents a single estimator for the three binning methods. Each column shows results for a single inference algorithm and each row shows results for a single number of bins. Note that when using the Bayesian Blocks binning method the number of bins is chosen automatically, hence the AUPRC values within columns are the same for the same MI estimator, but are included in all plots for ease of comparison.</p></caption><graphic xlink:href="132647_figS5" position="float" orientation="portrait" hwp:id="graphic-67"/></fig><fig id="figS6" position="float" fig-type="figure" orientation="portrait" hwp:id="F16" hwp:rev-id="xref-fig-16-1 xref-fig-16-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F16</object-id><object-id pub-id-type="publisher-id">figS6</object-id><label>Figure S6:</label><caption hwp:id="caption-23"><p hwp:id="p-159">The AUPRC results for the <italic toggle="yes">E. coli</italic> dataset with the B-spline estimator. Each plot shows the AUPRC results for a single inference algorithm, with the results being grouped by spline order.</p></caption><graphic xlink:href="132647_figS6" position="float" orientation="portrait" hwp:id="graphic-68"/></fig><fig id="figS7" position="float" fig-type="figure" orientation="portrait" hwp:id="F17"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F17</object-id><object-id pub-id-type="publisher-id">figS7</object-id><label>Figure S7:</label><caption hwp:id="caption-24"><p hwp:id="p-160">The AUPRC results when using the <italic toggle="yes">k</italic>-Nearest-Neighbour estimator on the <italic toggle="yes">E. coli</italic> dataset. Each plot contains the AUPRC results for a single inference algorithm and each bar represents the AUPRC when using <italic toggle="yes">k</italic> nearest neighbours to estimate the mutual information.</p></caption><graphic xlink:href="132647_figS7" position="float" orientation="portrait" hwp:id="graphic-69"/></fig><fig id="figS8" position="float" fig-type="figure" orientation="portrait" hwp:id="F18" hwp:rev-id="xref-fig-18-1 xref-fig-18-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F18</object-id><object-id pub-id-type="publisher-id">figS8</object-id><label>Figure S8:</label><caption hwp:id="caption-25"><p hwp:id="p-161">The AUPRC for the <italic toggle="yes">S. cerevisiae</italic> dataset with Maximum Likelihood, Miller-Madow, Chao-Shen and Shrinkage estimators. An individual plot shows the AUPRC when using a single inference algorithm and a specific number of bins with the four estimators. Each grouping of bars represents a single estimator for the three binning methods. Each column shows results for a single inference algorithm and each row shows results for a single number of bins. Note that when using the Bayesian Blocks binning method the number of bins is chosen automatically, hence the AUPRC values within columns are the same for the same MI estimator, but are included in all plots for ease of comparison.</p></caption><graphic xlink:href="132647_figS8" position="float" orientation="portrait" hwp:id="graphic-70"/></fig><fig id="figS9" position="float" fig-type="figure" orientation="portrait" hwp:id="F19" hwp:rev-id="xref-fig-19-1 xref-fig-19-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F19</object-id><object-id pub-id-type="publisher-id">figS9</object-id><label>Figure S9:</label><caption hwp:id="caption-26"><p hwp:id="p-162">The AUPRC results for the <italic toggle="yes">S. cerevisiae</italic> dataset with the B-spline estimator. Each plot shows the AUPRC results for a single inference algorithm, with the results being grouped by spline order.</p></caption><graphic xlink:href="132647_figS9" position="float" orientation="portrait" hwp:id="graphic-71"/></fig><fig id="figS10" position="float" fig-type="figure" orientation="portrait" hwp:id="F20" hwp:rev-id="xref-fig-20-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;132647v1/FIGS10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F20</object-id><object-id pub-id-type="publisher-id">figS10</object-id><label>Figure S10:</label><caption hwp:id="caption-27"><p hwp:id="p-163">The AUPRC results when using the <italic toggle="yes">k</italic>-Nearest-Neighbour estimator on the <italic toggle="yes">S. cerevisiae</italic> dataset. Each plot contains the AUPRC results for a single inference algorithm and each bar represents the AUPRC when using <italic toggle="yes">k</italic> nearest neighbours to estimate the mutual information.</p></caption><graphic xlink:href="132647_figS10" position="float" orientation="portrait" hwp:id="graphic-72"/></fig></sec></sec></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-56">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2 xref-ref-1-3 xref-ref-1-4 xref-ref-1-5 xref-ref-1-6 xref-ref-1-7 xref-ref-1-8 xref-ref-1-9 xref-ref-1-10 xref-ref-1-11"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Marbach D."><given-names>D.</given-names> <surname>Marbach</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-2">“Wisdom of crowds for robust gene network inference,”</article-title> <source hwp:id="source-1">Nature Methods</source>, vol. <volume>9</volume>, no. <issue>8</issue>, pp. <fpage>796</fpage>–<lpage>804</lpage>, <year>2012</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Haury A.-C."><given-names>A.-C.</given-names> <surname>Haury</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-3">“Tigress: trustful inference of gene regulation using stability selection,”</article-title> <source hwp:id="source-2">BMC Systems Biology</source>, vol. <volume>6</volume>, no. <issue>1</issue>, p. <fpage>145</fpage>, <year>2012</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Yu J."><given-names>J.</given-names> <surname>Yu</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-4">“Advances to bayesian network inference for generating causal networks from observational biological data,”</article-title> <source hwp:id="source-3">Bioinformatics</source>, vol. <volume>20</volume>, no. <issue>18</issue>, pp. <fpage>3594</fpage>–<lpage>3603</lpage>, <year>2004</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Priness I."><given-names>I.</given-names> <surname>Priness</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-5">“Evaluation of gene-expression clustering via mutual information distance measure,”</article-title> <source hwp:id="source-4">BMC Bioinformatics</source>, vol. <volume>8</volume>, no. <issue>1</issue>, p. <fpage>111</fpage>, <year>2007</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="De Smet R."><given-names>R.</given-names> <surname>De Smet</surname></string-name> and <string-name name-style="western" hwp:sortable="Marchal K."><given-names>K.</given-names> <surname>Marchal</surname></string-name>, <article-title hwp:id="article-title-6">“Advantages and limitations of current network inference methods,”</article-title> <source hwp:id="source-5">Nature Reviews Microbiology</source>, vol. <volume>8</volume>, no. <issue>10</issue>, pp. <fpage>717</fpage>–<lpage>729</lpage>, <year>2010</year>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2 xref-ref-6-3"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Margolin A. A."><given-names>A. A.</given-names> <surname>Margolin</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-7">“Aracne: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context,”</article-title> <source hwp:id="source-6">BMC Bioinformatics</source>, vol. <volume>7</volume>, no. <issue>1</issue>, p. <fpage>S7</fpage>, <year>2006</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3 xref-ref-7-4"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Olsen C."><given-names>C.</given-names> <surname>Olsen</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-8">“On the impact of entropy estimation on transcriptional regulatory network inference based on mutual information,”</article-title> <source hwp:id="source-7">EURASIP Journal on Bioinformatics and Systems Biology</source>, vol. <volume>2009</volume>, no. <issue>1</issue>, p. <fpage>308959</fpage>, <year>2008</year>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Daub C. O."><given-names>C. O.</given-names> <surname>Daub</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-9">“Estimating mutual information using b-spline functions–an improved similarity measure for analysing gene expression data,”</article-title> <source hwp:id="source-8">BMC Bioinformatics</source>, vol. <volume>5</volume>, no. <issue>1</issue>, p. <fpage>118</fpage>, <year>2004</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Faith J. J."><given-names>J. J.</given-names> <surname>Faith</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-10">“Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles,”</article-title> <source hwp:id="source-9">PLOS Biology</source>, vol. <volume>5</volume>, no. <issue>1</issue>, p. <fpage>e8</fpage>, <year>2007</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Meyer P. E."><given-names>P. E.</given-names> <surname>Meyer</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-11">“Information-theoretic inference of large transcriptional regulatory networks,”</article-title> <source hwp:id="source-10">EURASIP Journal on Bioinformatics and Systems Biology</source>, vol. <volume>2007</volume>, no. <issue>1</issue>, p. <fpage>79879</fpage>, <year>2007</year>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Kohanski M. A."><given-names>M. A.</given-names> <surname>Kohanski</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-12">“Mistranslation of membrane proteins and two-component system activation trigger antibiotic-mediated cell death,”</article-title> <source hwp:id="source-11">Cell</source>, vol. <volume>135</volume>, no. <issue>4</issue>, pp. <fpage>679</fpage>–<lpage>690</lpage>, <year>2008</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Fredrickson J. K."><given-names>J. K.</given-names> <surname>Fredrickson</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-13">“Towards environmental systems biology of shewanella,”</article-title> <source hwp:id="source-12">Nature Reviews Microbiology</source>, vol. <volume>6</volume>, no. <issue>8</issue>, pp. <fpage>592</fpage>–<lpage>603</lpage>, <year>2008</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Carro M. S."><given-names>M. S.</given-names> <surname>Carro</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-14">“The transcriptional network for mesenchymal transformation of brain tumours,”</article-title> <source hwp:id="source-13">Nature</source>, vol. <volume>463</volume>, no. <issue>7279</issue>, pp. <fpage>318</fpage>–<lpage>325</lpage>, <year>2010</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Jia J."><given-names>J.</given-names> <surname>Jia</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-15">“Aegilops tauschii draft genome sequence reveals a gene repertoire for wheat adaptation,”</article-title> <source hwp:id="source-14">Nature</source>, vol. <volume>496</volume>, no. <issue>7443</issue>, pp. <fpage>91</fpage>–<lpage>95</lpage>, <year>2013</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Madhamshettiwar P. B."><given-names>P. B.</given-names> <surname>Madhamshettiwar</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-16">“Gene regulatory network inference: evaluation and application to ovarian cancer allows the prioritization of drug targets,”</article-title> <source hwp:id="source-15">Genome Medicine</source>, vol. <volume>4</volume>, no. <issue>5</issue>, p. <fpage>41</fpage>, <year>2012</year>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Scargle J. D."><given-names>J. D.</given-names> <surname>Scargle</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-17">“Studies in astronomical time series analysis. vi. bayesian block representations,”</article-title> <source hwp:id="source-16">The Astrophysical Journal</source>, vol. <volume>764</volume>, no. <issue>2</issue>, p. <fpage>167</fpage>, <year>2013</year>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Chan T. E."><given-names>T. E.</given-names> <surname>Chan</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-18">“Network inference and hypotheses-generation from single-cell transcriptomic data using multivariate information measures,”</article-title> <source hwp:id="source-17">bioRxiv</source>, p. <fpage>082099</fpage>, <year>2016</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Schaffter T."><given-names>T.</given-names> <surname>Schaffter</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-19">“Genenetweaver: in silico benchmark generation and performance profiling of network inference methods,”</article-title> <source hwp:id="source-18">Bioinformatics</source>, vol. <volume>27</volume>, no. <issue>16</issue>, pp. <fpage>2263</fpage>–<lpage>2270</lpage>, <year>2011</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Davis J."><given-names>J.</given-names> <surname>Davis</surname></string-name> and <string-name name-style="western" hwp:sortable="Goadrich M."><given-names>M.</given-names> <surname>Goadrich</surname></string-name>, <article-title hwp:id="article-title-20">“The relationship between precision-recall and roc curves,”</article-title> in <source hwp:id="source-19">Proceedings of the 23rd international conference on Machine learning. ACM, 2006</source>, pp. <fpage>233</fpage>–<lpage>240</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Jeni L. A."><given-names>L. A.</given-names> <surname>Jeni</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-21">“Facing imbalanced data–recommendations for the use of performance metrics,”</article-title> in <source hwp:id="source-20">Affective Computing and Intelligent Interaction (ACII)</source>, <volume>2013</volume> <collab hwp:id="collab-1">Humaine Association Conference on. IEEE, 2013</collab>, pp. <fpage>245</fpage>–<lpage>251</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Saito T."><given-names>T.</given-names> <surname>Saito</surname></string-name> and <string-name name-style="western" hwp:sortable="Rehmsmeier M."><given-names>M.</given-names> <surname>Rehmsmeier</surname></string-name>, <article-title hwp:id="article-title-22">“The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets,”</article-title> <source hwp:id="source-21">PLOS ONE</source>, vol. <volume>10</volume>, no. <issue>3</issue>, p. <fpage>e0118432</fpage>, <year>2015</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2 xref-ref-22-3"><label>[22]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Kurt Z."><given-names>Z.</given-names> <surname>Kurt</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-23">“A comprehensive comparison of association estimators for gene network inference algorithms,”</article-title> <source hwp:id="source-22">Bioinformatics</source>, p. btu182, <year>2014</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Van den Bulcke T."><given-names>T.</given-names> <surname>Van den Bulcke</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-24">“Syntren: a generator of synthetic gene expression data for design and analysis of structure learning algorithms,”</article-title> <source hwp:id="source-23">BMC Bioinformatics</source>, vol. <volume>7</volume>, no. <issue>1</issue>, p. <fpage>43</fpage>, <year>2006</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Altay G."><given-names>G.</given-names> <surname>Altay</surname></string-name> and <string-name name-style="western" hwp:sortable="Emmert-Streib F."><given-names>F.</given-names> <surname>Emmert-Streib</surname></string-name>, <article-title hwp:id="article-title-25">“Inferring the conservative causal core of gene regulatory networks,”</article-title> <source hwp:id="source-24">BMC Systems Biology</source>, vol. <volume>4</volume>, no. <issue>1</issue>, p. <fpage>132</fpage>, <year>2010</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Butte A. J."><given-names>A. J.</given-names> <surname>Butte</surname></string-name> and <string-name name-style="western" hwp:sortable="Kohane I. S."><given-names>I. S.</given-names> <surname>Kohane</surname></string-name>, <article-title hwp:id="article-title-26">“Mutual information relevance networks: functional genomic clustering using pairwise entropy measurements,”</article-title> in <source hwp:id="source-25">Pac Symp Biocomput</source>, vol. <volume>5</volume>, no. <issue>415</issue>, <year>2000, p. 26</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Walters-Williams J."><given-names>J.</given-names> <surname>Walters-Williams</surname></string-name> and <string-name name-style="western" hwp:sortable="Li Y."><given-names>Y.</given-names> <surname>Li</surname></string-name>, <article-title hwp:id="article-title-27">“Estimation of mutual information: A survey,” in <italic toggle="yes">International Conference on Rough Sets and Knowledge Technology</italic></article-title>. <source hwp:id="source-26">Springer</source>, <year>2009</year>, pp. <fpage>389</fpage>–<lpage>396</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Vergara J. R."><given-names>J. R.</given-names> <surname>Vergara</surname></string-name> and <string-name name-style="western" hwp:sortable="Estévez P. A."><given-names>P. A.</given-names> <surname>Estévez</surname></string-name>, <article-title hwp:id="article-title-28">“A review of feature selection methods based on mutual information,”</article-title> <source hwp:id="source-27">Neural computing and applications</source>, vol. <volume>24</volume>, no. <issue>1</issue>, pp. <fpage>175</fpage>–<lpage>186</lpage>, <year>2014</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>[28]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Aapo Hyvärinen A."><given-names>A.</given-names> <surname>Aapo Hyvärinen</surname></string-name>, <article-title hwp:id="article-title-29">“Independent component analysis by minimization of mutual information,”</article-title> <source hwp:id="source-28">Helsinki University of Technology, Laboratory of Computer and Information Science, Finland, Report A</source>, vol. <volume>46</volume>, <year>1997</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Maes F."><given-names>F.</given-names> <surname>Maes</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-30">“Multimodality image registration by maximization of mutual information,”</article-title> <source hwp:id="source-29">IEEE Transactions on Medical Imaging</source>, vol. <volume>16</volume>, no. <issue>2</issue>, pp. <fpage>187</fpage>–<lpage>198</lpage>, <year>1997</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Basharin G. P."><given-names>G. P.</given-names> <surname>Basharin</surname></string-name>, <article-title hwp:id="article-title-31">“On a statistical estimate for the entropy of a sequence of independent random variables,”</article-title> <source hwp:id="source-30">Theory of Probability &amp; Its Applications</source>, vol. <volume>4</volume>, no. <issue>3</issue>, pp. <fpage>333</fpage>–<lpage>336</lpage>, <year>1959</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><label>[31]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Miller G. A."><given-names>G. A.</given-names> <surname>Miller</surname></string-name>, <article-title hwp:id="article-title-32">“Note on the bias of information estimates,”</article-title> <source hwp:id="source-31">Information theory in psychology: Problems and methods</source>, vol. <volume>2</volume>, no. <issue>95</issue>, p. <fpage>100</fpage>, <year>1955</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><label>[32]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Hausser J."><given-names>J.</given-names> <surname>Hausser</surname></string-name> and <string-name name-style="western" hwp:sortable="Strimmer K."><given-names>K.</given-names> <surname>Strimmer</surname></string-name>, <article-title hwp:id="article-title-33">“Entropy inference and the james-stein estimator, with application to nonlinear gene association networks,”</article-title> <source hwp:id="source-32">Journal of Machine Learning Research</source>, vol. <volume>10, no. Jul</volume>, pp. <fpage>1469</fpage>–<lpage>1484</lpage>, <year>2009</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Chao A."><given-names>A.</given-names> <surname>Chao</surname></string-name> and <string-name name-style="western" hwp:sortable="Shen T.-J."><given-names>T.-J.</given-names> <surname>Shen</surname></string-name>, <article-title hwp:id="article-title-34">“Nonparametric estimation of shannons index of diversity when there are unseen species in sample,”</article-title> <source hwp:id="source-33">Environmental and ecological statistics</source>, vol. <volume>10</volume>, no. <issue>4</issue>, pp. <fpage>429</fpage>–<lpage>443</lpage>, <year>2003</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2 xref-ref-34-3 xref-ref-34-4 xref-ref-34-5"><label>[34]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Moon Y.-I."><given-names>Y.-I.</given-names> <surname>Moon</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-35">“Estimation of mutual information using kernel density estimators,”</article-title> <source hwp:id="source-34">Physical Review E</source>, vol. <volume>52</volume>, no. <issue>3</issue>, p. <fpage>2318</fpage>, <year>1995</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2 xref-ref-35-3 xref-ref-35-4"><label>[35]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Kraskov A."><given-names>A.</given-names> <surname>Kraskov</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-36">“Estimating mutual information,”</article-title> <source hwp:id="source-35">Physical review E</source>, vol. <volume>69</volume>, no. <issue>6</issue>, p. <fpage>066138</fpage>, <year>2004</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2"><label>[36]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Freedman D."><given-names>D.</given-names> <surname>Freedman</surname></string-name> and <string-name name-style="western" hwp:sortable="Diaconis P."><given-names>P.</given-names> <surname>Diaconis</surname></string-name>, <article-title hwp:id="article-title-37">“On the histogram as a density estimator: L 2 theory,” Zeitschrift für</article-title> <source hwp:id="source-36">Wahrscheinlichkeitstheorie und verwandte Gebiete</source>, vol. <volume>57</volume>, no. <issue>4</issue>, pp. <fpage>453</fpage>–<lpage>476</lpage>, <year>1981</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>[37]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Meyer P. E."><given-names>P. E.</given-names> <surname>Meyer</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-38">“minet: Ar/bioconductor package for inferring large transcriptional networks using mutual information,”</article-title> <source hwp:id="source-37">BMC Bioinformatics</source>, vol. <volume>9</volume>, no. <issue>1</issue>, p. <fpage>461</fpage>, <year>2008</year>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>[38]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Altay G."><given-names>G.</given-names> <surname>Altay</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-39">“Depest: an r package of important dependency estimators for gene network inference algorithms,”</article-title> <source hwp:id="source-38">bioRxiv</source>, p. <fpage>102871</fpage>, <year>2017</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>[39]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Sales G."><given-names>G.</given-names> <surname>Sales</surname></string-name> and <string-name name-style="western" hwp:sortable="Romualdi C."><given-names>C.</given-names> <surname>Romualdi</surname></string-name>, <article-title hwp:id="article-title-40">“parmigenea parallel r package for mutual information estimation and gene network reconstruction,”</article-title> <source hwp:id="source-39">Bioinformatics</source>, vol. <volume>27</volume>, no. <issue>13</issue>, pp. <fpage>1876</fpage>–<lpage>1877</lpage>, <year>2011</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>[40]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="De Rinaldis E."><given-names>E.</given-names> <surname>De Rinaldis</surname></string-name> and <string-name name-style="western" hwp:sortable="Lahm A."><given-names>A.</given-names> <surname>Lahm</surname></string-name>, <article-title hwp:id="article-title-41">DNA microarrays: current applications</article-title>. <source hwp:id="source-40">Horizon Scientific Press</source>, <year>2007</year>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>[41]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Banf M."><given-names>M.</given-names> <surname>Banf</surname></string-name> and <string-name name-style="western" hwp:sortable="Rhee S. Y."><given-names>S. Y.</given-names> <surname>Rhee</surname></string-name>, <article-title hwp:id="article-title-42">“Enhancing gene regulatory network inference through data integration with markov random fields,”</article-title> <source hwp:id="source-41">Scientific Reports</source>, vol. <volume>7</volume>, <year>2017</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>[42]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Horvitz D. G."><given-names>D. G.</given-names> <surname>Horvitz</surname></string-name> and <string-name name-style="western" hwp:sortable="Thompson D. J."><given-names>D. J.</given-names> <surname>Thompson</surname></string-name>, <article-title hwp:id="article-title-43">“A generalization of sampling without replacement from a finite universe,”</article-title> <source hwp:id="source-42">Journal of the American statistical Association</source>, vol. <volume>47</volume>, no. <issue>260</issue>, pp. <fpage>663</fpage>–<lpage>685</lpage>, <year>1952</year>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>[43]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.43" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Good I. J."><given-names>I. J.</given-names> <surname>Good</surname></string-name>, <article-title hwp:id="article-title-44">“The population frequencies of species and the estimation of population parameters,”</article-title> <source hwp:id="source-43">Biometrika</source>, pp. <fpage>237</fpage>–<lpage>264</lpage>, <year>1953</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>[44]</label><citation publication-type="book" citation-type="book" ref:id="132647v1.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="De Boor C."><given-names>C.</given-names> <surname>De Boor</surname></string-name> and <string-name name-style="western" hwp:sortable="Boor D."><given-names>D.</given-names> <surname>Boor</surname></string-name>, <chapter-title><italic toggle="yes">A practical guide to splines</italic></chapter-title>. <publisher-name>Springer-Verlag New York</publisher-name>, <year>1978</year>, vol. <volume>27</volume>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>[45]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Keinosuke F."><given-names>F.</given-names> <surname>Keinosuke</surname></string-name>, <article-title hwp:id="article-title-45">“Introduction to statistical pattern recognition,”</article-title> <source hwp:id="source-44">Academic Press Inc</source>, <year>1990</year>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>[46]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Silverman B. W."><given-names>B. W.</given-names> <surname>Silverman</surname></string-name>, <article-title hwp:id="article-title-46">Density estimation for statistics and data analysis</article-title>. <source hwp:id="source-45">CRC press</source>, <year>1986</year>, vol. <volume>26</volume>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>[47]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Kozachenko L."><given-names>L.</given-names> <surname>Kozachenko</surname></string-name> and <string-name name-style="western" hwp:sortable="Leonenko N. N."><given-names>N. N.</given-names> <surname>Leonenko</surname></string-name>, <article-title hwp:id="article-title-47">“Sample estimate of the entropy of a random vector,”</article-title> <source hwp:id="source-46">Problemy Peredachi Informatsii</source>, vol. <volume>23</volume>, no. <issue>2</issue>, pp. <fpage>9</fpage>–<lpage>16</lpage>, <year>1987</year>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>[48]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.48" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Vasicek O."><given-names>O.</given-names> <surname>Vasicek</surname></string-name>, <article-title hwp:id="article-title-48">“A test for normality based on sample entropy,”</article-title> <source hwp:id="source-47">Journal of the Royal Statistical Society. Series B (Methodological)</source>, pp. <fpage>54</fpage>–<lpage>59</lpage>, <year>1976</year>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>[49]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Gao S."><given-names>S.</given-names> <surname>Gao</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-49">“Efficient estimation of mutual information for strongly dependent variables</article-title>.” in <source hwp:id="source-48">AISTATS</source>, <year>2015</year>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>[50]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Peng H."><given-names>H.</given-names> <surname>Peng</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-50">“Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,”</article-title> <source hwp:id="source-49">IEEE Transactions on pattern analysis and machine intelligence</source>, vol. <volume>27</volume>, no. <issue>8</issue>, pp. <fpage>1226</fpage>–<lpage>1238</lpage>, <year>2005</year>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>[51]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.51" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Beaudry N. J."><given-names>N. J.</given-names> <surname>Beaudry</surname></string-name> and <string-name name-style="western" hwp:sortable="Renner R."><given-names>R.</given-names> <surname>Renner</surname></string-name>, <article-title hwp:id="article-title-51">“An intuitive proof of the data processing inequality,”</article-title> <source hwp:id="source-50">arXiv preprint arXiv:1107.0740</source>, <year>2011</year>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>[52]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Gama-Castro S."><given-names>S.</given-names> <surname>Gama-Castro</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-52">“Regulondb version 7.0: transcriptional regulation of escherichia coli k-12 integrated within genetic sensory response units (gensor units),”</article-title> <source hwp:id="source-51">Nucleic acids research</source>, vol. <volume>39</volume>, no. suppl 1, pp. <fpage>D98</fpage>–<lpage>D105</lpage>, <year>2011</year>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>[53]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="MacIsaac K. D."><given-names>K. D.</given-names> <surname>MacIsaac</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-53">“An improved map of conserved regulatory sites for saccharomyces cerevisiae,”</article-title> <source hwp:id="source-52">BMC Bioinformatics</source>, vol. <volume>7</volume>, no. <issue>1</issue>, p. <fpage>113</fpage>, <year>2006</year>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>[54]</label><citation publication-type="journal" citation-type="journal" ref:id="132647v1.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Grau J."><given-names>J.</given-names> <surname>Grau</surname></string-name> <etal>et al.</etal>, <article-title hwp:id="article-title-54">“Prroc: computing and visualizing precision-recall and receiver operating characteristic curves in r,”</article-title> <source hwp:id="source-53">Bioinformatics</source>, p. <fpage>btv153</fpage>, <year>2015</year>.</citation></ref></ref-list></back></article>
