<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/083857</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;083857</article-id><article-id pub-id-type="other" hwp:sub-type="slug">083857</article-id><article-id pub-id-type="other" hwp:sub-type="tag">083857</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author E-mail: <email hwp:id="email-1">emr443@nyu.edu</email></corresp><fn id="n1" fn-type="equal" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>¶</label><p hwp:id="p-1">EMR and IM contributed equally to this work</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Russek Evan M."><surname>Russek</surname><given-names>Evan M.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><xref ref-type="author-notes" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">¶</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Momennejad Ida"><surname>Momennejad</surname><given-names>Ida</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="author-notes" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">¶</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Botvinick Matthew M."><surname>Botvinick</surname><given-names>Matthew M.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Gershman Samuel J."><surname>Gershman</surname><given-names>Samuel J.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Daw Nathaniel D."><surname>Daw</surname><given-names>Nathaniel D.</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Center for Neural Science, New York University</institution>, New York, NY, <country>United States of America</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2"><label>2</label><institution hwp:id="institution-2">Princeton Neuroscience Institute and Department of Psychology, Princeton University</institution>, Princeton, NJ, <country>United States of America</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Google DeepMind</institution>, London, <country>United Kingdom</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Department of Psychology and Center for Brain Science, Harvard University</institution>, Cambridge, MA, <country>United States of America</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2016-10-27T10:09:27-07:00">
    <day>27</day><month>10</month><year>2016</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-08-09T08:54:45-07:00">
    <day>9</day><month>8</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2016-10-27T10:25:16-07:00">
    <day>27</day><month>10</month><year>2016</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-08-09T11:21:36-07:00">
    <day>9</day><month>8</month><year>2017</year>
  </pub-date><elocation-id>083857</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2016-10-26"><day>26</day><month>10</month><year>2016</year></date>
<date date-type="rev-recd" hwp:start="2017-08-09"><day>09</day><month>8</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-08-09"><day>09</day><month>8</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license hwp:id="license-1"><p hwp:id="p-2">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author’s permission.</p></license></permissions><self-uri xlink:href="083857.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/083857v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="083857.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/083857v3/083857v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/083857v3/083857v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">Humans and animals are capable of evaluating actions by considering their long-run future rewards through a process described using model-based reinforcement learning (RL) algorithms. The mechanisms by which neural circuits perform the computations prescribed by model-based RL remain largely unknown; however, multiple lines of evidence suggest that neural circuits supporting model-based behavior are structurally homologous to and overlapping with those thought to carry out model-free temporal difference (TD) learning. Here, we lay out a family of approaches by which model-based computation may be built upon a core of TD learning. The foundation of this framework is the successor representation, a predictive state representation that, when combined with TD learning of value predictions, can produce a subset of the behaviors associated with model-based learning, while requiring less decision-time computation than dynamic programming. Using simulations, we delineate the precise behavioral capabilities enabled by evaluating actions using this approach, and compare them to those demonstrated by biological organisms. We then introduce two new algorithms that build upon the successor representation while progressively mitigating its limitations. Because this framework can account for the full range of observed putatively model-based behaviors while still utilizing a core TD framework, we suggest that it represents a neurally plausible family of mechanisms for model-based evaluation.</p><sec hwp:id="sec-1"><title hwp:id="title-2">Author Summary</title><p hwp:id="p-4">According to standard models, when confronted with a choice, animals and humans rely on two separate, distinct processes to come to a decision. One process deliberatively evaluates the consequences of each candidate action and is thought to underlie the ability to flexibly come up with novel plans. The other process gradually increases the propensity to perform behaviors that were previously successful and is thought to underlie automatically executed, habitual reflexes. Although computational principles and animal behavior support this dichotomy, at the neural level, there is little evidence supporting a clean segregation. For instance, although dopamine — famously implicated in drug addiction and Parkinson’s disease — currently only has a well-defined role in the automatic process, evidence suggests that it also plays a role in the deliberative process. In this work, we present a computational framework for resolving this mismatch. We show that the types of behaviors associated with either process could result from a common learning mechanism applied to different strategies for how populations of neurons could represent candidate actions. In addition to demonstrating that this account can produce the full range of flexible behavior observed in the empirical literature, we suggest experiments that could detect the various approaches within this framework.</p></sec></abstract><counts><page-count count="68"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-2"><title hwp:id="title-3">Introduction</title><p hwp:id="p-5">A key question in both neuroscience and psychology is how the brain evaluates candidate actions in complex, sequential decision tasks. In principle, computing an action’s expected long-run cumulative future reward (or <italic toggle="yes">value</italic>) requires averaging rewards over the many future state trajectories that might follow the action. In practice, the exact computation of such expectations by dynamic programming or tree search methods may be prohibitively expensive, and it is widely believed that the brain simplifies the computations occurring at decision time, in part by relying on “cached” (pre-computed) long-run value estimates [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>].</p><p hwp:id="p-6">Such caching of values is the hallmark of prominent temporal difference (TD) learning theories, according to which prediction errors reported by phasic dopamine responses update these cached variables [<xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>–<xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]. This, in turn, provides a neuro-computational account of inflexible stimulus-response habits, due to the fact that TD learning cannot update cached values in response to distal changes in reward (e.g., following reward devaluation). The computationally cheap but inflexible “model-free” nature of TD learning, which relies only on trial-and-error interactions with the environment, contrasts with the flexible but computationally expensive “model-based” nature of dynamic programming and tree search methods, which rely on an explicit internal model of the environment. Due to the complementary properties of model-free and model-based value computation, it has been suggested that the brain makes use of both in the form of parallel RL systems that compete for control of behavior [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">1</xref>].</p><p hwp:id="p-7">Although flexible choice behavior seems to demonstrate that humans and animals may use model-based evaluation in some circumstances, very little is known about how this is actually implemented in the brain, or indeed to what extent the behavioral phenomena that have been taken to suggest model-based learning might arise from some simpler approximation. In this article, we explore a family of algorithms that capture a range of such approximations and, we argue, provide a promising set of candidates for the neural foundations supporting such learning.</p><p hwp:id="p-8">Our proposals are motivated by multiple suggestive, but also somewhat counterintuitive, lines of evidence, which suggest that the dopamine system and its key targets are implicated not just in the model-free behaviors that theory endows them with, but also in the more flexible choice adjustments that seem to reflect model-based learning [<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>–<xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>]. This is perplexing for the standard account because typical model-based algorithms such as value iteration do not make use of a TD prediction error for long-run reward of the sort associated with dopamine. Instead, they store internal variables (specifically, predictions about immediate “one-step” rather than long-run consequences of actions), which require different update rules and error signals [<xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>, <xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>]. Additionally, at choice time, such algorithms require computations that are structurally different than those typically prescribed to the dopaminergic-striatal circuitry [<xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>].</p><p hwp:id="p-9">In this article, we revisit and extend the <italic toggle="yes">successor representation</italic> (SR) [<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>,<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>; see also <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>-<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>], a predictive state representation that can endow TD learning with some aspects of model-based value computation, such as flexible adjustment following reward devaluation. That the SR, when combined with TD learning, can produce such flexible behavior makes it a promising candidate for a neural foundation for model-based learning, which (because it is built on top of a TD foundation) can also explain dopaminergic involvement in model-based learning. However, this approach, in its original form, results in behavioral inflexibilities that could serve as empirical signatures of it, but also make it inadequate for fully explaining organisms’ planning capacities. In particular, the SR simplifies planning by caching a set of intermediate quantities, expectations about cumulative future state occupancies. For this reason, unlike model-based learning, it is incapable of solving many tasks that require adjusting to changes in contingencies between actions and their long-run consequences (e.g. [<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>,<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]).</p><p hwp:id="p-10">In this article, we explore a family of algorithms based around the SR, and introduce two new variants that mitigate its limitations. In particular, we examine algorithms in which the SR is updated either by computation at choice time or off-line by replayed experience, both of which help to ameliorate its problems with caching. These approaches can each account for human and animal behavior in a wide range of planning tasks, suggest connections with other models of learning and dopamine, and make empirically testable predictions. Overall, these approaches represent a family of plausible and computationally efficient hypothetical mechanisms for the full range of flexible behaviors associated with model-based learning, and could provide a clear theoretical foundation for future experimental work.</p><p hwp:id="p-11">The article is organized as follows. In the remainder of this introduction, we review the formalism of reinforcement learning in a Markov decision process (MDP) and use this framework to delineate the problem of model-based flexibility arising from model-free circuitry and elucidate how the SR offers a potential solution to this problem. In the results section, we use simulations to demonstrate the precise behavioral limitations induced by computing values using the SR, as originally described, and evaluate these limitations with respect to the behavioral literature. We then introduce two new versions of the SR that progressively mitigate these limitations, and again simulate their expected consequences in terms of flexible or inflexible choice behavior.</p><sec id="s1a" hwp:id="sec-3"><title hwp:id="title-4">Formalism: Model-based and model-free reinforcement learning</title><p hwp:id="p-12">Here, we briefly review the formalism of reinforcement learning in a Markov decision process (MDP), which provides the foundation for our simulations (see [<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>] or [<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">23</xref>] for a fuller presentation).</p><p hwp:id="p-13">An MDP is defined by a set of states, a set of actions, a reward function <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>)over state/action pairs, and a state transition distribution, <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>′ |<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), where <italic toggle="yes">a</italic> denotes the chosen action. States and rewards occur sequentially according to these one-step functions, driven by a series of actions; the goal is to learn to choose a probabilistic policy over actions, denoted by <italic toggle="yes">π</italic>, that maximizes the value function, <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>), defined as the expected cumulative discounted reward:
<disp-formula id="ueqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="083857_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-1"/></alternatives>
</disp-formula></p><p hwp:id="p-14">Here, <italic toggle="yes">γ</italic> is a parameter controlling temporal discounting. The value function can also be defined recursively as the sum of the immediate reward of the action chosen in that state, <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), and the value of its successor state <italic toggle="yes">s</italic>’, averaged over possible actions, <italic toggle="yes">a</italic>, and transitions that would occur if the agent chose according to <italic toggle="yes">π</italic>:
<disp-formula id="eqn1" hwp:id="disp-formula-2" hwp:rev-id="xref-disp-formula-2-1 xref-disp-formula-2-2 xref-disp-formula-2-3 xref-disp-formula-2-4 xref-disp-formula-2-5 xref-disp-formula-2-6 xref-disp-formula-2-7 xref-disp-formula-2-8 xref-disp-formula-2-9 xref-disp-formula-2-10 xref-disp-formula-2-11 xref-disp-formula-2-12">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="083857_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
</p><p hwp:id="p-15">The value function under the optimal policy is given by:
<disp-formula id="eqn2" hwp:id="disp-formula-3" hwp:rev-id="xref-disp-formula-3-1 xref-disp-formula-3-2 xref-disp-formula-3-3 xref-disp-formula-3-4 xref-disp-formula-3-5 xref-disp-formula-3-6 xref-disp-formula-3-7 xref-disp-formula-3-8 xref-disp-formula-3-9 xref-disp-formula-3-10 xref-disp-formula-3-11 xref-disp-formula-3-12 xref-disp-formula-3-13">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="083857_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
</p><p hwp:id="p-16">Knowledge of the value function can help to guide choices. For instance, we can define the state-action value function as the value of choosing action <italic toggle="yes">a</italic> and following <italic toggle="yes">π</italic> thereafter:
<disp-formula id="eqn3" hwp:id="disp-formula-4" hwp:rev-id="xref-disp-formula-4-1 xref-disp-formula-4-2 xref-disp-formula-4-3 xref-disp-formula-4-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="083857_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
</p><p hwp:id="p-17">Then at any state one could choose the action that maximizes <italic toggle="yes">Q</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>). (Formally this defines a new policy, which is as good or better than the baseline policy <italic toggle="yes">π</italic>; analogously, <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-1" hwp:rel-id="disp-formula-3">equation 2</xref> can be used to define the optimal state-action value function, the maximization of which selects optimal actions.) Note that it is possible to write a recursive definition for <italic toggle="yes">Q</italic> in the same manner as <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-1" hwp:rel-id="disp-formula-2">equation 1</xref>, and work directly with the state-action values, rather than deriving them indirectly from <italic toggle="yes">V</italic>.</p><p hwp:id="p-18">For expositional simplicity, in this article, we work instead with <italic toggle="yes">V</italic> wherever possible (mainly because this is easier to depict visually, and simplifies the notation), and accordingly we assume in our simulations that the agent derives <italic toggle="yes">Q</italic> using <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-4-1" hwp:rel-id="disp-formula-4">equation 3</xref> for guiding choices. To be concrete, in a spatial “gridworld” task of the sort we simulate, this amounts to computing a value function <italic toggle="yes">V</italic> over locations <italic toggle="yes">s</italic>, and using it to derive <italic toggle="yes">Q</italic> (the value of actions <italic toggle="yes">a</italic> heading in each of the four cardinal directions) by examining <italic toggle="yes">V</italic> for each adjacent state. Although this simplifies bookkeeping for this class of tasks, <italic toggle="yes">this is not intended as a substantive claim</italic>. Indeed, the last algorithm we propose will work directly with <italic toggle="yes">Q</italic> values, and the others can easily be re-expressed in this form.</p><p hwp:id="p-19">The problem of reinforcement learning is then reduced to learning to predict the value function <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>) or <italic toggle="yes">V</italic>*(<italic toggle="yes">s</italic>). There are two main families of approaches. Model-based algorithms learn to estimate the one-step transition and reward functions, <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic> ′ |<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) and <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), from which it is possible to compute <italic toggle="yes">V</italic>* (or <italic toggle="yes">Q</italic>*) using <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-2" hwp:rel-id="disp-formula-3">equation 2</xref>. This typically involves unrolling the recursion in <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-3" hwp:rel-id="disp-formula-3">equation 2</xref> into a series of nested sums, an algorithm known as value iteration. The alternative, model-free, approach exemplified by TD learning bypasses estimating the one-step model. Instead, it directly updates a cached estimate of the value function itself. In particular, following a transition <italic toggle="yes">s</italic> → <italic toggle="yes">s</italic> ′ initiated by action <italic toggle="yes">a</italic>, a reward prediction error, <italic toggle="yes">δ</italic>, is calculated and used to update <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>):
<disp-formula id="eqn4" hwp:id="disp-formula-5" hwp:rev-id="xref-disp-formula-5-1 xref-disp-formula-5-2 xref-disp-formula-5-3 xref-disp-formula-5-4 xref-disp-formula-5-5 xref-disp-formula-5-6">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="083857_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
where <italic toggle="yes">α</italic><sub><italic toggle="yes">TD</italic></sub> is a learning rate parameter.</p><p hwp:id="p-20">The TD update rule is derived from the recursion in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-2" hwp:rel-id="disp-formula-2">equation 1</xref>: each step iteratively pushes the left hand side of the equation, <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>), closer to <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>)+ <italic toggle="yes">γV</italic>(<italic toggle="yes">s</italic>′), which is a one-sample estimate of the right hand side.</p><p hwp:id="p-21">Finally, analogous sample-based updates may also be conducted offline (e.g., between steps of actual, “online” experience). This is a key insight of Sutton’s Dyna architecture [<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref>] (see also [<xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>]). The approach, like TD, caches estimates of <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>). Here TD learning is supplemented by additional offline updates. Specifically, samples consisting of a transition and reward triggered by a state-action (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">s</italic>’) are generated either from a learned one-step model’s probability distributions <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>′ |<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) and <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), or instead simply replayed, model-free, from stored episodes of previously experienced transitions. For each sample, <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>) is then updated according to <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-5-1" hwp:rel-id="disp-formula-5">equation (4)</xref>. Given sufficient iterations of sampling between steps of real experience, this approach can substitute for explicit value iteration and produce estimates at each step comparable to model-based approaches that more directly solve <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-3" hwp:rel-id="disp-formula-2">equations 1</xref> or <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-4" hwp:rel-id="disp-formula-3">2</xref>.</p><p hwp:id="p-22">A further distinction, which will become important later, is that between <italic toggle="yes">on-policy</italic> methods, based on <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-4" hwp:rel-id="disp-formula-2">equation 1</xref>, and <italic toggle="yes">off-policy</italic> methods, based on <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-5" hwp:rel-id="disp-formula-3">equation 2</xref>. On-policy methods estimate a policy-dependent value function <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>), whereas off-policy methods directly estimate the optimal value function <italic toggle="yes">V</italic>*(<italic toggle="yes">s</italic>). Typically, model-based methods are off-policy (since having learned a one-step model it is possible to use <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-6" hwp:rel-id="disp-formula-3">equation 2</xref> to directly compute the optimal policy); whereas different TD learning variants can be either on- or off-policy.</p></sec><sec id="s1b" hwp:id="sec-4"><title hwp:id="title-5">Model-free learning in the brain</title><p hwp:id="p-23">Due to the similarity between the phasic responses of midbrain dopamine neurons, and the TD prediction error <italic toggle="yes">δ</italic> (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-5-2" hwp:rel-id="disp-formula-5">equation 4</xref>), it has long been suggested that this system implements TD learning [<xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>,<xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>]. More specifically (e.g. [<xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">4</xref>]; <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig 1a</xref>) it has been suggested that values <italic toggle="yes">V</italic> or <italic toggle="yes">Q</italic> are associated with the firing of medium spiny neurons in striatum [<xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>,<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>], as a function of an input state (or state-action) representation carried by their afferent neurons in frontal cortex, and that learning of the value function is driven by dopamine-mediated adjustment of the cortico-striatal synapses connecting these neurons. Selection among these striatal value representations would then drive action choice. Although not entirely uncontroversial, a great deal of evidence about dopamine and its targets supports this hypothesis (see [<xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>,<xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>] for fuller reviews).</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Fig. 1.</label><caption hwp:id="caption-1"><title hwp:id="title-6">Cortico-striatal loops and reinforcement learning.</title><p hwp:id="p-24">a) Canonical circuit for TD learning. A dopaminergic prediction error, signaled in substantia nigra pars compacta and ventral tegmental area, updates the value of cortically represented states and actions by modifying cortico-striatal synapses. Depending on value, represented in striatal medium spiny neurons (MSN), actions are passed through to basal-ganglia action systems. b) Results of rodent lesion studies. Lesions to a cortico-striatal loop passing through dorsomedial (DM) striatum prevent flexibly adjusting behavior following reward devaluation. This area receives input from ventromedial prefrontal cortex and projects, via globus pallidus, to dorsomedial nucleus of the thalamus. This loop is generally thought to implement model-based learning [<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>]. Lesions to cortico-striatal loop passing through dorsolateral (DL) striatum cause animals to maintain ability to flexibly adjust behavior following devaluation, despite over-training. This area receives input from sensory and motor areas of cortex and projects, via globus pallidus, to posterior nucleus of the thalamus. This loop is generally thought to implement model-free learning [<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">31</xref>]. In addition to receiving similar dopaminergic innervation from substantia nigra pars compacta (SnC), such loops are famously thought to be homologous to one another.</p></caption><graphic xlink:href="083857_fig1" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-25">Such theories provide a neural implementation of Thorndike’s early law of effect, the reinforcement principle according to which rewarded actions (here, those paired with positive prediction error) tend to be repeated [<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. However, the hypothesis that animals or humans rely exclusively on this principle to make decisions has long been known to be false, as demonstrated by a line of learning experiments whose basic logic traces to rodent spatial navigation experiments by Tolman [<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-3" hwp:rel-id="ref-22">22</xref>] (for modern variants, see [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>,<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>,<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>–<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>]).</p><p hwp:id="p-26">To facilitate simulation and analysis, we here frame the logic of these experiments in terms of “grid-world” spatial MDPs. When viewed as MDPs, Tolman’s experiments can be divided into two categories, which require subjects to adjust to either of two different sorts of local changes in the underlying MDP. Experience with these changes is staged so as to reveal whether they are relying on cached values or recomputing them from a representation of the full MDP.</p><p hwp:id="p-27">Accordingly, revaluation tasks, such as latent learning, reward devaluation, and sensory preconditioning, examine whether animals appropriately adjust behavior following changes in <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), such as a newly introduced reward (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig 2a</xref>). Analogously, contingency change (e.g., detour or contingency degradation) tasks examine whether animals appropriately adjust behavior following changes in <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>′ |<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), such as a blocked passageway (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig 2b</xref>). Model-free RL is insensitive to these manipulations because it caches cumulative expected rewards and requires additional learning to update the stored <italic toggle="yes">V</italic>. Conversely, model-based RL, which uses the one-step model directly to compute <italic toggle="yes">V</italic> at decision time, reacts immediately and flexibly to any experience that affects it. Note that the difference in behavior on these types of tasks predicted by the algorithms is categorical, and not a question of degree or learning speed. In particular, because of the representations they learn and update, model-based algorithms can make the correct choice following these manipulations without any further retraining (i.e. so long as they learn locally about the new contingency or value, they can immediately make appropriate choices in distal parts of the state space), whereas model-free algorithms cannot (in general, they must first experience trajectories starting from the test state and leading to the state with the changed value or transition contingency). Animals sometimes fail to correctly update behavior following revaluations, consistent with inflexible, model-free caching schemes [<xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>]. However, findings that in other circumstances animals can indeed flexibly adapt their behavior following such manipulations (without any further retraining – e.g. tested on the very first trial, or without feedback) has long been interpreted as evidence for their use of internal models, as in model-based RL or similar methods [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-3" hwp:rel-id="ref-1">1</xref>,<xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-4" hwp:rel-id="ref-22">22</xref>,<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>]. A key goal of this article is to interrogate this assumption, and to consider neural mechanisms that, despite falling short of full model-based RL, might support such behavioral flexibility.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Fig. 2.</label><caption hwp:id="caption-2"><title hwp:id="title-7">Grid-world representation of Tolman’s tasks.</title><p hwp:id="p-28">Dark grey positions represent maze boundaries. Light grey positions represent maze hallways. a) Latent learning: following a period of randomly exploring the maze (starting from S) the agent is notified that reward has been placed in position R. We examine whether the agent’s policy immediately updates to reflect the shortest path from S to R. b) Detour: after the agent learns to use the shortest path to reach a reward state R from state S, a barrier is placed in state B. After the agent is notified that state B is no longer accessible from its neighboring state, we examine whether its policy immediately updates to reflect the new shortest path to R from S.</p></caption><graphic xlink:href="083857_fig2" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s1c" hwp:id="sec-5"><title hwp:id="title-8">The puzzle of model-based learning and its neural substrates</title><p hwp:id="p-29">A further set of rodent lesion studies have used reward devaluation tasks to suggest that apparently model-based and model-free behaviors (i.e., behavior that is either flexible or insensitive following reward devaluation) depend on dissociable sets of brain areas (e.g. [<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>,<xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>]; <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig 1b</xref>). This led to the hypothesis (e.g., [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-4" hwp:rel-id="ref-1">1</xref>,<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-3" hwp:rel-id="ref-31">31</xref>,<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>]) that these two forms of reinforcement learning depend on competing systems in the brain—the dopaminergic TD system previously described, plus a second – less clearly understood – circuit supporting model-based behavior. But how is this latter computation carried out in the brain?</p><p hwp:id="p-30">A number of fairly abstract theories have been based around explicit computation of the state-action value based on some form of <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-4-2" hwp:rel-id="disp-formula-4">equation 3</xref>, e.g. by learning an estimate of the one-step transition function, <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>’|<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) and the immediate reward function <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) and using them iteratively to compute the future value by tree search, value iteration, or Bayesian inference [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-5" hwp:rel-id="ref-1">1</xref>,<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-2" hwp:rel-id="ref-39">39</xref>–<xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>]. These theories have not spoken in detail about the neural implementation of these computations, but an accompanying presumption has been that the model-based system does not rely on a dopaminergic prediction error signal. This is because the TD prediction error of <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-5-3" hwp:rel-id="disp-formula-5">equation 4</xref> (for <italic toggle="yes">γ</italic> &gt; 0, which is the parameter regime needed to explain phasic dopamine’s signature responses to the anticipation as well as receipt of reward [<xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>]) is specifically useful for directly learning long-run cumulative values <italic toggle="yes">V</italic>. In contrast, the idea of model-based learning is to <italic toggle="yes">derive</italic> these values iteratively by stringing together short-term predictions from a learned one-step model [<xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref>,<xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>]. Note that the prediction error normally thought to be reported by dopamine neurons is not appropriate here: the prediction error signal for updating the <italic toggle="yes">immediate</italic> reward model <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) is like <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-5-4" hwp:rel-id="disp-formula-5">equation 4</xref> but with <italic toggle="yes">γ</italic> = 0, which is not consistent with anticipatory phasic dopamine responses. (However, correlates of prediction errors for <italic toggle="yes">γ</italic> = 0 have been observed using human fMRI [<xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>]). Furthermore, the hypothesized process of adding these rewards up over anticipated trajectories at choice time, such as by value iteration or tree search, has no counterpart in model-free choice. Instead, learning from anticipatory TD errors stores complete long-run values (e.g., in corticostriatal synapses), requiring no further computation at choice time.</p><p hwp:id="p-31">However, neither the rodent lesion data nor another body of work studying the neural correlates of model-based learning in humans suggests such a clean differentiation between the dopaminergic-striatal circuit (supposed to support TD) and some other presumably non-dopaminergic substrate for model-based learning. Instead, lesions suggest each type of learning is supported by a different subregion of striatum, together with connected regions of cortex (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig 1b</xref>) and basal ganglia. This suggests that putatively model-based and model-free systems may map onto adjacent but structurally parallel cortico-basal ganglionic loops [<xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>], thus perhaps involving analogous (striatal) computations operating over distinct (cortical) input representations [<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>].</p><p hwp:id="p-32">Also contrary to a strict division between systems, both dorsomedial and dorsolateral striatal territories have similar interrelationships with dopamine [<xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>], though the involvement of their differential dopaminergic input in devaluation sensitivity has not been completely assessed [<xref rid="c48" ref-type="bibr" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>]. Research on humans’ learning in a two-step MDP (which has similar logic to devaluation studies) supports the causal involvement of dopamine in model-based learning [<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>–<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>]. Furthermore, dopaminergic recordings in rodents [<xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref>] (though see [<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">37</xref>]), and neuroimaging of prediction error signals in human striatum [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref>] suggest that these signals integrate model-based evaluations.</p><p hwp:id="p-33">Altogether, the research reviewed here supports the idea that model-based evaluations are at least partly supported by the same sort of dopaminergic-striatal circuit thought to support TD learning, though perhaps operating in separate cortico-striatal loops. This suggestion, if true, provides strong hints about the neural basis of model-based behavior. However, for the reasons discussed above, this also seems puzzlingly inconsistent with the abstract, textbook [<xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">24</xref>] picture of model-based learning by <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-4-3" hwp:rel-id="disp-formula-4">equation 3</xref>.</p><p hwp:id="p-34">Several more neurally explicit theories of some aspects of model-based computation have been advanced, which go some way toward resolving this tension by incorporating a dopaminergic component. Doya [<xref rid="c49" ref-type="bibr" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>] introduced a circuit by which projections via the cerebellum perform one step of forward state prediction, which activates a dopaminergic prediction error for the anticipated state. The candidate action can then be accepted or rejected by thresholding this anticipated prediction error against some aspiration level. It is unclear, however, how this one-step, serial approach can be generalized to tasks involving stochastic state transitions, direct comparison between multiple competing actions, or rewards accumulated over multiple steps (as in tasks like [<xref rid="c50" ref-type="bibr" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>]).</p><p hwp:id="p-35">A similar idea has arisen from recordings in spatial tasks, where the firing of place cells along trajectories ahead of the animal suggests a hippocampal basis for a similar (though multi-step) state anticipation process, potentially driving evaluation of these candidate states using learned reward values in ventral striatum [<xref rid="c51" ref-type="bibr" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>]. It is, however, unclear how this activity fits into a larger circuit for accumulation of these evaluations and comparison between options.</p><p hwp:id="p-36">Finally, another candidate approach is based on the Dyna framework discussed above. In this case, model-generated experience can be played back “off-line,” e.g. between trials or during rest. These ersatz experiences can, in turn, drive dopaminergic prediction errors and updating of striatal <italic toggle="yes">Q</italic> values using the same mechanisms as real experience. As noted above, given sufficient off-line replay, this can achieve the same effect as model-based planning; in particular, it can update <italic toggle="yes">Q</italic> values following revaluation and other manipulations [<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">27</xref>,<xref rid="c52" ref-type="bibr" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]. However, without a more traditional “on-line” planning component, this approach degrades (to that of basic, model-free <italic toggle="yes">Q</italic> learning) when there is insufficient time or resources for off-line replay, and when truly novel situations are encountered [<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-3" hwp:rel-id="ref-27">27</xref>].</p><p hwp:id="p-37">Here we propose and analyze a different family of approaches to these problems, which relate to the above proposals in that they incorporate elements of upstream predictive input to ventral striatum, and also of a different and more-flexible approach to offline updates. The proposed approach, based on the SR, builds even more directly on the standard TD learning model of dopaminergic-striatal circuitry.</p></sec><sec id="s1d" hwp:id="sec-6"><title hwp:id="title-9">The successor representation</title><p hwp:id="p-38">The research reviewed above suggests that flexible, seemingly model-based choices may be accomplished using computations that are homologous to those used in model-free RL. How can this be? In fact, it is known that evaluations with some features of model-based learning can result from TD learning over a different input representation. As shown by Dayan [<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>], <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-5" hwp:rel-id="disp-formula-2">equation 1</xref> can reformulated as:
<disp-formula id="eqn5" hwp:id="disp-formula-6" hwp:rev-id="xref-disp-formula-6-1">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="083857_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
</p><p hwp:id="p-39">Here, <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> is a matrix of expected cumulative discounted <italic toggle="yes">future state occupancies</italic>, measuring the cumulative time expected to be spent in each future state <italic toggle="yes">s</italic>′, if one were to start in some state <italic toggle="yes">s</italic> and follow policy <italic toggle="yes">π</italic> (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig 3</xref>):
<disp-formula id="eqn6" hwp:id="disp-formula-7" hwp:rev-id="xref-disp-formula-7-1">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="083857_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
where &#x01d540;(·) = 1 if its argument is true and 0 otherwise. Thus, this form rearranges the expectation over future trajectories in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-6" hwp:rel-id="disp-formula-2">equation 1</xref> by first computing expected occupancies for each state, then summing rewards obtained, via actions, in each state over these.</p><p hwp:id="p-40"><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> can also be used as a set of basis functions for TD learning of values. Specifically, we represent each state using a vector of features given by the corresponding row of <italic toggle="yes">M</italic> (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig 3</xref>), i.e. by the future occupancies expected for each state <italic toggle="yes">s</italic>′. Then we approximate <italic toggle="yes">V</italic> <sup><italic toggle="yes">π</italic></sup> <italic toggle="yes">s</italic> by some weighted combination of these features:
<disp-formula id="eqn7" hwp:id="disp-formula-8" hwp:rev-id="xref-disp-formula-8-1 xref-disp-formula-8-2 xref-disp-formula-8-3 xref-disp-formula-8-4">
<alternatives hwp:id="alternatives-8"><graphic xlink:href="083857_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula>
</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Fig. 3.</label><caption hwp:id="caption-3"><title hwp:id="title-10">Example state representations.</title><p hwp:id="p-41">a) Agent position (rodent image) in a maze whose hallways are indicated by grey. b) Punctate representation of the agent’s current state. Model-free behavior results from TD computation applied to this representation c,d) Possible successor representations of agent’s state. Model-based behavior may result from TD applied to this type of representation. The successor representation depends on the action selection policy the agent is expected to follow in future states. The figures show the representation of the current state under a random policy (c) versus a policy favoring rightward moves (d).</p></caption><graphic xlink:href="083857_fig3" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><p hwp:id="p-42">Comparing <xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-6-1" hwp:rel-id="disp-formula-6">equations 5</xref> and <xref ref-type="disp-formula" rid="eqn7" hwp:id="xref-disp-formula-8-1" hwp:rel-id="disp-formula-8">7</xref> demonstrates this approximation will be correct when the weight <italic toggle="yes">w</italic>(<italic toggle="yes">s</italic>′) for each successor state corresponds to its one-step reward, averaged over actions in <italic toggle="yes">s</italic>′, ∑<sub><italic toggle="yes">a</italic></sub> <italic toggle="yes">π(a</italic>|<italic toggle="yes">s</italic>′)<italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>′, <italic toggle="yes">a</italic>). One way to learn these weights is using standard TD learning (adapted for linear function approximation rather than the special case of a punctate state representation). In particular, following a transition <italic toggle="yes">s</italic> → <italic toggle="yes">s</italic>′, each index <italic toggle="yes">i</italic> of <italic toggle="yes">w</italic> is updated:
<disp-formula id="eqn8" hwp:id="disp-formula-9" hwp:rev-id="xref-disp-formula-9-1 xref-disp-formula-9-2 xref-disp-formula-9-3 xref-disp-formula-9-4 xref-disp-formula-9-5 xref-disp-formula-9-6">
<alternatives hwp:id="alternatives-9"><graphic xlink:href="083857_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula></p><p hwp:id="p-43">Here, <italic toggle="yes">δ</italic> is defined as in <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-5-5" hwp:rel-id="disp-formula-5">equation 4</xref>. Note that in the algorithms discussed below, the agent must estimate the successor matrix <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> from experience. If the feature matrix <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> were known and static, a simpler alternative to <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-1" hwp:rel-id="disp-formula-9">equation (8)</xref> for <italic toggle="yes">w</italic> would be to learn the one-step rewards by a delta rule on the immediate reward <italic toggle="yes">R</italic>. Since the successor representation is just a particular case of a linear feature vector for TD learning, the advantage of learning weights by the TD rule of <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-2" hwp:rel-id="disp-formula-9">equation 8</xref> is that weights learned this way will estimate value <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> for any feature matrix <italic toggle="yes">M</italic>, such as estimates of the successor matrix <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> prior to convergence (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Supplementary Figure 1</xref>).</p><p hwp:id="p-44">Altogether, this algorithm suggests a strategy for providing different inputs into a common dopaminergic/TD learning stage to produce different sorts of value predictions (see also [<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref>]). In particular, whereas model-free valuation may arise from TD mapping of a punctate representation of the current state (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig 3b</xref>) in sensory and motor cortex to values in dorsolateral striatum (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig 1b</xref>), at least some aspects of model-based valuation may arise by analogous TD mapping of the successor representation (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig 3c,d</xref>) in prefrontal cortex or hippocampus to values in dorsomedial striatum (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig 1b</xref>). This is possible because the successor matrix <italic toggle="yes">M</italic> has a predictive aspect reflecting knowledge of the state transitions <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>′ |<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), at least in terms of aggregate occupancy, separate from the state/action rewards <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>).</p><p hwp:id="p-45">This approach may thus offer a solution to how flexible, seemingly model-based choices can be implemented, and indeed can arise from the same dopaminergic-striatal circuitry that carries out model-free TD learning.</p><p hwp:id="p-46">What remains to be shown is whether algorithms based on this strategy – applying the SR as input to TD learning – can produce the full range of model-based behaviors. In the remainder of this paper, we simulate the behavior of such algorithms to explore this question.</p><p hwp:id="p-47">To simulate learning using the SR, we need to also simulate how the successor matrix <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> is itself produced from experience. <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> can be defined through a recursive equation that is directly analogous to <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-7" hwp:rel-id="disp-formula-2">equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-7" hwp:rel-id="disp-formula-3">2</xref>:
<disp-formula id="eqn9" hwp:id="disp-formula-10" hwp:rev-id="xref-disp-formula-10-1 xref-disp-formula-10-2 xref-disp-formula-10-3 xref-disp-formula-10-4 xref-disp-formula-10-5 xref-disp-formula-10-6 xref-disp-formula-10-7 xref-disp-formula-10-8">
<alternatives hwp:id="alternatives-10"><graphic xlink:href="083857_eqn9.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives>
</disp-formula>
where <bold>1</bold><sub><italic toggle="yes">s</italic></sub> is the vector of all zeros except for a 1 in the sth position and <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> is the one-step state transition matrix that is dependent on <italic toggle="yes">π</italic>, <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>, <italic toggle="yes">s</italic>′)=∑<sub><italic toggle="yes">a</italic></sub><italic toggle="yes">π</italic> (<italic toggle="yes">a</italic> |<italic toggle="yes">s</italic>) <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>′|<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>).</p><p hwp:id="p-48">Similar to how approaches to estimating <italic toggle="yes">V</italic> are derived from <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-8" hwp:rel-id="disp-formula-2">equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-8" hwp:rel-id="disp-formula-3">2</xref>, one could derive analogous approaches to estimating <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> from <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-1" hwp:rel-id="disp-formula-10">equation 9</xref>. Specifically, one could utilize a “model-based” approach that would learn <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> and use it iteratively to derive a solution for <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>. Alternatively, a TD learning approach could be taken to learn <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> directly, without use of a one-step model <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup>. (This approach is analogous to model-free TD methods for learning <italic toggle="yes">V</italic>, though it is arguably not really model-free since <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> is itself a sort of long-run transition model.) This TD learning approach would cache rows of M and update them after transitioning from their corresponding states, by moving the cached row closer to a one-sample estimate of the right hand side of <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-2" hwp:rel-id="disp-formula-10">equation 9</xref>. Lastly, such TD updates could also occur offline, using simulated or previously experienced samples. This approach for learning <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> would be comparable to the Dyna approach for learning <italic toggle="yes">V</italic>. The three models we consider below correspond to these three different possibilities.</p><p hwp:id="p-49">Finally, note that SR-based algorithms have favorable computational properties; in particular, at choice time, given <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> (e.g. if it is learned and cached rather than computed from a one-step model), SR can compute values <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> with a single dot product (e.g., a single layer of a linear neural network, <xref ref-type="disp-formula" rid="eqn7" hwp:id="xref-disp-formula-8-2" hwp:rel-id="disp-formula-8">equation 7</xref>), analogous to model-free TD algorithms. This is in contrast to the multiple steps of iterative computation required at choice time for computing value via <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-9" hwp:rel-id="disp-formula-2">equation 1</xref> in standard model-based approaches. This comes at the cost of storing the successor matrix <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>: if S is the number of states in the task, the SR matrix has a number of entries equal to <italic toggle="yes">S</italic><sup>2</sup>. Such entries of <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>can be stored as the (all-to-all) set of weights from a single layer of a neural network mapping input states to their successor representation.</p></sec></sec><sec id="s2" hwp:id="sec-7"><title hwp:id="title-11">Results</title><p hwp:id="p-50">In the following sections, we explore the behavioral consequences of each of these strategies. We structure the results as follows. For each learning method, we first present the algorithm. Then we present the results of simulations using that algorithm. The purpose of simulations is to verify our qualitative reasoning about the behavior of the algorithm and illustrate how the algorithm’s behavior compares to that of model-based dynamic programming methods. These simulations also suggest experiments that could be used to identify whether an animal or human were planning using such a strategy. Each task that we simulate is designed to be a categorical test the algorithm. Following some change in the task to which the agent must respond, some of the algorithms can arrive at the correct decision without additional experience, but other algorithms cannot. Such failures are due to the computational properties of the algorithms themselves and are thus parameter-independent. To ensure that this is the case, for each simulation presented in the results, we have verified that the qualitative result can be observed robustly under a wide range of parameter settings. In general, there are parameter settings under which models, which are demonstrated below to succeed in a given task, can be made to fail it. However, there are <italic toggle="yes">no parameter settings</italic> under which a model that is shown below to fail a given task will pass it (Supplementary Table 1).</p><p hwp:id="p-51">For each algorithm, we discuss its biological plausibility as well as how that algorithm’s performance lines up with that of animals.</p><statement hwp:id="statement-1"><label>Algorithm 1:</label><title hwp:id="title-12">The Original Successor Representation (SR-TD)</title><p hwp:id="p-52">The original SR [<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-3" hwp:rel-id="ref-15">15</xref>] (which we call SR-TD) constructed the future state occupancy predictions <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> using a TD learning approach. This approach caches rows of <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> and incrementally updates them after transitioning from their corresponding states. Specifically, following each state transition <italic toggle="yes">s</italic> → <italic toggle="yes">s</italic>′ each element of row <italic toggle="yes">s</italic> is updated as follows:
<disp-formula id="eqn10" hwp:id="disp-formula-11" hwp:rev-id="xref-disp-formula-11-1 xref-disp-formula-11-2 xref-disp-formula-11-3 xref-disp-formula-11-4">
<alternatives hwp:id="alternatives-11"><graphic xlink:href="083857_eqn10.gif" position="float" orientation="portrait" hwp:id="graphic-14"/></alternatives>
</disp-formula>
where <bold>1</bold><sub><italic toggle="yes">s</italic></sub> is the vector of all zeros except for a 1 in the sth position. <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>,:) is used as input to another TD learning stage, this time to learn the weights <italic toggle="yes">w</italic> for predicting expected future value from the state occupancy vector. To simulate SR-TD, we have the agent learn <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>and <italic toggle="yes">w</italic> in parallel, updating each (according to <xref ref-type="disp-formula" rid="eqn10" hwp:id="xref-disp-formula-11-1" hwp:rel-id="disp-formula-11">equations 10</xref> and <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-3" hwp:rel-id="disp-formula-9">8</xref>, respectively) at each transition; and sample actions according to an <italic toggle="yes">∈</italic>-greedy policy (see Methods).</p></statement><sec id="s2a" hwp:id="sec-8"><title hwp:id="title-13">Simulation Results</title><p hwp:id="p-53"><italic toggle="yes">SR-TD can solve some reward revaluation tasks</italic>. SR-TD is able to produce behavior analogous to model-based learning in some reward revaluation tasks that categorically defeat simple TD learning. To demonstrate this, we simulated the behavior of SR-TD in a grid-world version of Tolman’s <italic toggle="yes">latent learning</italic> task (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig 2a,b</xref>). The agent first explores the grid-world randomly, during which it learns the successor matrix <bold><italic toggle="yes">M</italic></bold> <sup><italic toggle="yes">π</italic></sup> corresponding to a random policy. Next, it learns that reward is available at position R (importantly, by being placed repeatedly at R and receiving reward, but not experiencing trajectories leading there from any other location). This experience induces prediction errors that cause the agent to update its weight vector, <bold><italic toggle="yes">w</italic></bold> in the position corresponding to the rewarded state. Finally, in a test probe, we allow the agent to form values by multiplying its current version of <bold><italic toggle="yes">M</italic></bold><sup><bold><italic toggle="yes">π</italic></bold></sup> with <bold><italic toggle="yes">w</italic></bold>, and measure whether (immediately on the first test trial following reward training) those values would produce a policy reflective of the shortest path from position S to R.</p><p hwp:id="p-54"><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig 4b</xref> shows SR-TD performance on a latent learning task: SR-TD can, without further learning, produce a new policy reflecting the shortest path to the rewarded location (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig 4b</xref>). As a comparison to SR-TD’s performance, we also simulated the behavior of a simpler foil algorithm that represents model-free (or, actually, limited model-based) performance. This algorithm applied TD learning updates to non-predictive, punctate, state representations to estimate <italic toggle="yes">V</italic>. As with the SR, we permitted this algorithm to convert state values to state-action values by using a single-step of model-based look-ahead. Although this algorithm’s performance is representative of the failure of fully model-free algorithms at solving these revaluation tasks, we designed it to go beyond a vanilla model-free TD algorithm by allowing a single-step of model-based lookahead. This is analogous to a limited sort of model-based learning that has been suggested previously to be implemented by the basal ganglia and cerebellum [<xref rid="c49" ref-type="bibr" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">49</xref>]. <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig 4a</xref> shows that this algorithm cannot solve latent learning problems: it learns nothing about paths around the maze from the reward training, and would have to discover the path to the new reward from scratch by additional exploration.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Fig. 4.</label><caption hwp:id="caption-4"><title hwp:id="title-14">Behavior of SR-TD:</title><p hwp:id="p-55">a) One-step of model-based lookahead combined with TD learning applied to punctate representations cannot solve latent learning tasks. b) SR-TD can solve some latent learning tasks. For both a) and b) median value function (grayscale) and implied policy (arrows) are shown immediately after the agent learns about reward in latent learning task. c) SR-TD can only update predicted future state occupancies following direct experience with states and their multi-step successors. For instance, if SR-TD were to learn that s’’ no longer follows s’, it would not be able to infer that state s’’ no longer follows state s. Whether animals make this sort of inference is tested in the detour task. d) SR-TD cannot solve detour problems. Median value function (grayscale) and implied policy (arrows) are shown after SR-TD encounters barrier in detour task. SR-TD fails to update decision policy to reflect the new shortest path.</p></caption><graphic xlink:href="083857_fig4" position="float" orientation="portrait" hwp:id="graphic-15"/></fig><p hwp:id="p-56"><italic toggle="yes">SR-TD cannot solve transition revaluation tasks</italic>. However, SR-TD is limited in its ability to react correctly to other seemingly similar manipulations. Because <bold><italic toggle="yes">M</italic></bold><sup><bold><italic toggle="yes">π</italic></bold></sup> reflects long-run cumulative state occupancies, rather than the individual one-step transition distribution, <bold><italic toggle="yes">P</italic></bold>(<bold><italic toggle="yes">s</italic></bold>’ <bold>|<italic toggle="yes">s</italic></bold>, <bold><italic toggle="yes">a</italic></bold>), SR-TD cannot adjust its valuations to local changes in the transitions without first updating <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold>at different locations. This inflexibility prevents SR-TD from flexibly adjusting value estimates after learning about changes in transition structure (“transition revaluation”; <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig 4b</xref>). Consider a grid-world version of Tolman’s detour task (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Figs 2b</xref> and 4d). Here, following an exploration period during which the agent is able to form an estimate of <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> under a random policy, the agent is trained to seek reward at R, starting from S. Later, a blockade is introduced at B. Again, the agent is allowed to experience this change only <italic toggle="yes">locally</italic>, by repeatedly being dropped in the state next to the barrier, attempting the action that leads to it and learning that the states to the right are no longer accessible from this state. This experience causes the agent to update <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> for the state immediately next to the barrier. However, despite this update, the rows of <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> corresponding to the states that lie along a path between the start state and the state next to the barrier remain unchanged. From <xref ref-type="disp-formula" rid="eqn10" hwp:id="xref-disp-formula-11-2" hwp:rel-id="disp-formula-11">equation 10</xref>, it can be seen that these updates can only occur from direct experience, i.e., a series of new trajectories starting at these states that encounter the barricade. SR-TD fails to reduce the value of these states (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig 4d</xref>), and thus would approach the barricade rather than taking a detour on the first visit back to S. As shown in supplemental materials, the depth-limited model-free algorithm also fails this test (Supplementary Table 1). A fully model-based algorithm (not shown) does make the correct choice in this case.</p></sec><sec id="s2b" hwp:id="sec-9"><title hwp:id="title-15">Interim Discussion</title><sec id="s2b1" hwp:id="sec-10"><title hwp:id="title-16">Biological Plausibility</title><p hwp:id="p-57">The reward learning stage of this rule (learning weights <bold><italic toggle="yes">w</italic></bold> to map <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic></bold>,:) to <bold><italic toggle="yes">V</italic></bold><sup><bold><italic toggle="yes">π</italic></bold></sup> (<bold><italic toggle="yes">s</italic></bold>)) is the standard dopaminergic TD rule, <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-4" hwp:rel-id="disp-formula-9">equation 8</xref>, operating over a new input. The update rule for that input, <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic></bold>,: <bold>)</bold>, is also based on a TD learning rule, but here applied to learning to predict cumulative future state occupancies. This uses a vector-valued error signal to update an entire row of <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> at each step. Crucially, despite the functional similarity between this rule and the TD update prescribed to dopamine, we do not suggest that dopamine carries this second error signal. Neurally, this sort of learning might, instead, be implemented using Hebbian associative learning between adjacent consecutive states [<xref rid="c53" ref-type="bibr" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>], with decaying eligibility traces (like TD(1)) to capture longer-run dependencies. Lastly, although we have defined the successor representation over tabular representations of states, is also possible to combine the SR with function approximation and distributed representations in order to reduce its dimensionality [<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">21</xref>,<xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref>].</p></sec><sec id="s2b2" hwp:id="sec-11"><title hwp:id="title-17">Behavioral Adequacy</title><p hwp:id="p-58">SR-TD is capable of solving some reward revaluation experiments. For similar reasons, SR-TD can solve sensory preconditioning (e.g. [<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">35</xref>]) and reward devaluation tasks (e.g. [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-3" hwp:rel-id="ref-6">6</xref>,<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-4" hwp:rel-id="ref-27">27</xref>,<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">33</xref>,<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>]), both of which turn on an analogous ability to update behavior when state transition probabilities are held constant but reward values are changed. Evidence for model-based behavior in animals and humans has typically come from these types of tasks, suggesting that SR-TD could underlie a good proportion of behavior considered to be model-based. However, SR-TD is incapable of solving seemingly analogous tasks that require replanning under a transition rather than a reward change. Because there is at least some evidence from the early literature [<xref rid="c55" ref-type="bibr" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref>] that animals can adapt correctly to such detour situations, we suggest that this inflexibility prevents SR-TD, on its own, from being a plausible mechanism for the full repertoire of model-based behavior.</p><statement hwp:id="statement-2"><label>Algorithm 2:</label><title hwp:id="title-18">Dynamic recomputation of the successor representation (SR-MB)</title><p hwp:id="p-59">Here, we explore a novel “model-based” approach, SR-MB, for constructing the expected state occupancy vector <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>,:). SR-MB learns a one-step transition model, <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> and uses it, at decision time, to derive a solution to <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-3" hwp:rel-id="disp-formula-10">equation 9</xref>. One key constraint on a model-based implementation suggested by the data is that the computation should be staged in a way consistent with the architecture suggested by <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig 2a</xref>. Specifically, the TD architecture in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig 2a</xref> suggests that, because the states are represented in cortex (or hippocampus) and weights (which capture information about rewards) and value are represented in downstream cortico-striatal synapses and medium spiny striatal neurons, information about <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) and <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>) should not be used in the online construction of states. For the SR approach, this implies that <italic toggle="yes">M</italic> be constructed without using direct knowledge of <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) or <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>). As we see below, this serial architecture – a cortical state-prediction stage providing input for a subcortical reward-prediction stage – if true, would impose interesting limitations on the resulting behavior.</p><p hwp:id="p-60">To construct <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>,:), SR-MB first learns the one-step state transition matrix <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup>, implemented in our simulations through separate learning of <italic toggle="yes">P</italic> (<italic toggle="yes">s</italic>′ | <italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) as well as <italic toggle="yes">π</italic> (<italic toggle="yes">a</italic>|<italic toggle="yes">s</italic>), the agent’s previously expressed decision policy (see Methods). Prior to each decision, <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> is used to compute a solution to <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-4" hwp:rel-id="disp-formula-10">equation 9</xref>. This solution can be expressed in either of two forms. A given row, <italic toggle="yes">s</italic>, of <italic toggle="yes">M</italic> can be computed individually as the sum of n-step transition probabilities starting from state <italic toggle="yes">s</italic>:
<disp-formula id="eqn11" hwp:id="disp-formula-12" hwp:rev-id="xref-disp-formula-12-1 xref-disp-formula-12-2">
<alternatives hwp:id="alternatives-12"><graphic xlink:href="083857_eqn11.gif" position="float" orientation="portrait" hwp:id="graphic-16"/></alternatives>
</disp-formula>
</p><p hwp:id="p-61">Alternatively, matrix inversion can be used to solve for the entire successor matrix at once:
<disp-formula id="eqn12" hwp:id="disp-formula-13" hwp:rev-id="xref-disp-formula-13-1 xref-disp-formula-13-2 xref-disp-formula-13-3 xref-disp-formula-13-4">
<alternatives hwp:id="alternatives-13"><graphic xlink:href="083857_eqn12.gif" position="float" orientation="portrait" hwp:id="graphic-17"/></alternatives>
</disp-formula>
</p><p hwp:id="p-62">To implement SR-MB, we use <xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-13-1" hwp:rel-id="disp-formula-13">equation 12</xref>. However, this is not a mechanistic commitment of the model, since <xref ref-type="disp-formula" rid="eqn11" hwp:id="xref-disp-formula-12-1" hwp:rel-id="disp-formula-12">equation 11</xref> is equivalent.</p><p hwp:id="p-63">Given <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>, SR-MB learns the reward prediction weights <italic toggle="yes">w</italic> and forms <italic toggle="yes">V</italic> and <italic toggle="yes">Q</italic> values in the same way as SR-TD.</p><p hwp:id="p-64">Note finally that this scheme is similar to solving <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-10" hwp:rel-id="disp-formula-2">equation 1</xref> for on-policy values <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> by value iteration, except that the sums are rearranged to put state prediction upstream of reward prediction, as per <xref ref-type="disp-formula" rid="eqn7" hwp:id="xref-disp-formula-8-3" hwp:rel-id="disp-formula-8">equation 7</xref> and in line with the neural architecture of <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Figure 2a</xref>. The max operator in <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-9" hwp:rel-id="disp-formula-3">equation 2</xref> prevents a similar rearrangement that would allow this scheme to be used for off-policy optimal values <italic toggle="yes">V</italic>* (<xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-10" hwp:rel-id="disp-formula-3">equation 2</xref>), as discussed below. The restriction to on-policy values <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> is the major empirical signature of this version of the model.</p></statement></sec></sec><sec id="s2c" hwp:id="sec-12"><title hwp:id="title-19">Simulation Results</title><sec id="s2c1" hwp:id="sec-13"><title hwp:id="title-20">SR-MB can solve transition revaluation tasks</title><p hwp:id="p-65">Using an updated <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> to recompute <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> at decision time ensures that behavior is sensitive to changes in the transition structure. We demonstrate this by showing that unlike SR-TD, SR-MB successfully solves Tolman’s detour task in addition to latent learning. In the detour task, after being dropped in the state next to the barrier, SR-MB updates its estimate of <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>’|<italic toggle="yes">sa</italic>) for the <italic toggle="yes">sa</italic> leading into the barrier. This new <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>’|<italic toggle="yes">sa</italic>) is then combined with it’s estimate of <italic toggle="yes">π</italic> (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) (learned through prior experience including initial random exploration of the maze and then trials starting in S and ending in R) to compute <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup>. The row of <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> corresponding to the state next to the barrier at this point now no longer predicts transitioning into the barrier state. When the agent then recomputes <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> by <xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-13-2" hwp:rel-id="disp-formula-13">equation 12</xref>, using the updated <italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup>, rows of <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> corresponding to the path between the start state and the barrier no longer predict future occupancy of states on the other side of the barrier. When <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>is then used to compute <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup>, the values immediately (on the first test trial after the barrier is encountered) result in a policy reflective of the new shortest path (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig 5a</xref>.)</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Fig. 5.</label><caption hwp:id="caption-5"><title hwp:id="title-21">Behavior of SR-MB.</title><p hwp:id="p-66">a) SR-MB can solve the detour task. Median value function (grayscale) and implied policy (arrows) after SR-MB encounters barrier. b) SR-MB determines successor states relative to a cached policy. If SR-MB learned from previous behavior that it will always select action a1, the value of s would become insensitive to changes in reward at s2’. C) Novel “policy” revaluation task. After a phase of random exploration, we place a reward in location R1. The agent completes a series of trials that alternatively start from locations S1 and S2 and end when R1 is reached. We then place a larger reward in location R2 and record the agent’s value function and implied policy upon encountering it. d) SR-MB cannot solve the novel “policy” revaluation task. Median value function and implied policy recorded immediately after SR-MB learns about reward placed in location R2. Notice that if the agent were to start from location S1, its policy would suboptimally lead it to the smaller reward at R1.</p></caption><graphic xlink:href="083857_fig5" position="float" orientation="portrait" hwp:id="graphic-18"/></fig></sec><sec id="s2c2" hwp:id="sec-14"><title hwp:id="title-22">SR-MB is limited by policy dependence</title><p hwp:id="p-67">SR-MB is able to solve both tasks that have been used as examples of planning in animals and humans. We thus sought to determine whether there were any tasks, perhaps not yet explored in the empirical literature, that could differentiate it from approaches that utilize “full” model-based value iteration. A key feature of SR-MB, as well as SR-TD, is that it computes <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> with respect to a policy <bold><italic toggle="yes">π</italic></bold>. For SR-MB, <bold><italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup></bold> is computed using <bold><italic toggle="yes">π</italic></bold>(<bold><italic toggle="yes">a</italic></bold> |<bold><italic toggle="yes">s</italic></bold>), which is learned through observation of previous actions. Because <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> is policy dependent, so are the value estimates that it produces. SR-TD and SR-MB are thus “on-policy” methods – their estimates of <bold><italic toggle="yes">V</italic></bold><sup><bold><italic toggle="yes">π</italic></bold></sup> can be compared to the estimates of a traditional model-based approach used to solve <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-11" hwp:rel-id="disp-formula-2">equation 1</xref>. A key limitation of “on-policy” methods is that their value estimates are made with respect to the policy under which learning occurred. This is an important limitation, as we will see below, because new learning about parts of the MDP can moot the previously learned policy <bold><italic toggle="yes">π</italic></bold> (and hence invalidate the associated successor matrix and values).</p><p hwp:id="p-68">For the successor representation strategy, this limitation could be bypassed if we could compute <bold><italic toggle="yes">M</italic></bold>* – an “off-policy” successor matrix based on the successor states expected under the <italic toggle="yes">optimal</italic> policy – which would, in turn provide the state input for solving for <bold><italic toggle="yes">V</italic></bold>*, the optimal long-run values. However, given the architectural constraints we have suggested, computing <bold><italic toggle="yes">M</italic></bold>* is not straightforward. In particular, defining <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-5" hwp:rel-id="disp-formula-10">equation 9</xref> with respect to the optimal policy would require replacing <bold><italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup></bold> with <italic toggle="yes">T*</italic>: the one-step transition matrix corresponding to the optimal policy, <bold><italic toggle="yes">T</italic></bold>*(<bold><italic toggle="yes">s</italic></bold>, <bold><italic toggle="yes">s</italic></bold>’) = <bold><italic toggle="yes">P</italic></bold>(<bold><italic toggle="yes">s</italic></bold>’|<bold><italic toggle="yes">s</italic></bold>, <bold><italic toggle="yes">a</italic></bold>*), where <bold><italic toggle="yes">a</italic></bold>* is the action in state <bold><italic toggle="yes">s</italic></bold> that maximizes future rewards. Computing <bold><italic toggle="yes">a</italic></bold>* online, however, would require access to <bold><italic toggle="yes">R</italic></bold>(<bold><italic toggle="yes">s</italic></bold>, <bold><italic toggle="yes">a</italic></bold>), which would violate the suggested serial staging of the computation, that <bold><italic toggle="yes">M</italic></bold> be constructed without using reward information. Policy dependence of value predictions thus defines both the major limitation as well as the empirical signature of SR-MB.</p></sec><sec id="s2c3" hwp:id="sec-15"><title hwp:id="title-23">SR-MB cannot solve novel policy revaluation tasks</title><p hwp:id="p-69">What are the behavioral implications of SR-MB estimating values using the policy that was expressed during learning? Consider a situation where state <bold><italic toggle="yes">s</italic></bold>’ can be reached from state <bold><italic toggle="yes">s</italic></bold> using action <bold><italic toggle="yes">a</italic></bold>, but SR-MB learned from past behavior that <bold><italic toggle="yes">π</italic></bold>(<bold><italic toggle="yes">a</italic></bold>|<bold><italic toggle="yes">s</italic></bold>) is near 0 (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig 5b</xref>). Then it will not include rewards at <bold><italic toggle="yes">s</italic></bold>’ in the value of <bold><italic toggle="yes">s</italic></bold>, even if separately learning (say, by visiting <bold><italic toggle="yes">s</italic></bold>’ but not in a trajectory starting from <bold><italic toggle="yes">s</italic></bold>) that <bold><italic toggle="yes">s</italic></bold>’ is the most rewarding state reachable from <bold><italic toggle="yes">s</italic></bold>. In other words, caching of the policy at <bold><italic toggle="yes">s</italic></bold> blinds the agent (without further exploration and relearning) to changes in the reward function that should change that policy. Value iteration based on <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-11" hwp:rel-id="disp-formula-3">equation 2</xref> does not have this limitation because the max operation would look ahead to the reward at <bold><italic toggle="yes">s</italic></bold>’ to determine whether it should be included.</p><p hwp:id="p-70">These considerations suggest a novel revaluation task (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig 5c</xref>). Here, SR-MB first performs many trials where R1 is rewarded, starting from both S1 as well as a new start position, S2. This experience causes the agent to learn a policy <bold><italic toggle="yes">π</italic></bold>(<bold><italic toggle="yes">s</italic></bold>, <bold><italic toggle="yes">a</italic></bold>) that reflects moves away from the bottom right corner of the maze. Next, a larger reward is introduced at R2. Despite having learned about this reward (by starting at R2 and updating <bold><italic toggle="yes">w</italic></bold> at the state corresponding to this location), because computation of <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> utilizes <bold><italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup></bold>, which reflects moves away from the bottom right corner of the maze, <bold><italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup></bold> does not predict the future occupancy of state R2 from any state along a path starting at S1. Because of this, the values of these states do not update to include the newly updated parts of the weight vector corresponding to position R2. Thus, despite the higher reward in R2, the agent would choose to head toward R1 from S1, due to caching of the incorrect policy in <bold><italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup></bold> (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig 5d</xref>). Thus, this task defeats SR-MB (as well as, shown in supplemental materials, the depth limited model-free planners and SR-TD), though it can be solved by standard model-based learning using <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-12" hwp:rel-id="disp-formula-3">equation 2</xref>.</p></sec></sec><sec id="s2d" hwp:id="sec-16"><title hwp:id="title-24">Interim Discussion</title><sec id="s2d1" hwp:id="sec-17"><title hwp:id="title-25">Biological Plausibility</title><p hwp:id="p-71">SR-MB requires the brain to compute <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>,:) from <italic toggle="yes">T</italic> <sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>,:) for a particular state s under evaluation. Although in our simulations, we used <xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-13-3" hwp:rel-id="disp-formula-13">equation 12</xref> to compute the entire successor matrix at once, this is not a mechanistic commitment of the model. For instance, recurrent neural networks offer a simple way to compute <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>,:) based on spreading activation implementing <xref ref-type="disp-formula" rid="eqn11" hwp:id="xref-disp-formula-12-2" hwp:rel-id="disp-formula-12">equation 11</xref>. Consider a network with one node for each state and the weight between node <italic toggle="yes">s</italic> and node <italic toggle="yes">s</italic>’ set to <italic toggle="yes">γT</italic> <sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>, <italic toggle="yes">s</italic>′). If node <italic toggle="yes">s</italic> is activated, then at each successive time step, the network activity will represent each successive component of the sum. Indeed, this model arose in the early connectionist literature [<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref>].</p><p hwp:id="p-72">Alternatively, it is also well established that recurrent neural networks can perform matrix inversion by relaxing to an attractor [<xref rid="c56" ref-type="bibr" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref>], making a computation based on <xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-13-4" hwp:rel-id="disp-formula-13">equation 12</xref> plausible as well.</p><p hwp:id="p-73">A final mechanism for computing <italic toggle="yes">M</italic> from <italic toggle="yes">T</italic> would involve sampling transitions from <italic toggle="yes">T</italic> offline and using them to iteratively update <italic toggle="yes">M</italic> according to <xref ref-type="disp-formula" rid="eqn10" hwp:id="xref-disp-formula-11-3" hwp:rel-id="disp-formula-11">equation 10</xref>, the SR-TD update. In the following section, we explore how an approach based on this idea, when carried out over state-actions, can be used to solve the “off-policy” planning problem as well.</p></sec><sec id="s2d2" hwp:id="sec-18"><title hwp:id="title-26">Behavioral Adequacy</title><p hwp:id="p-74">SR-MB produces the two behaviors that are considered signatures of planning in the empirical literature: immediate adjustment of decision policy following learned changes in either reward structure (latent learning) or transition structure (detour problem). The novel policy revaluation task demonstrates that SR-MB still produces errors that could in principle be behaviorally detectable, but have not been exercised by standard experimental tasks [<xref rid="c57" ref-type="bibr" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref>].</p><statement hwp:id="statement-3"><label>Algorithm 3: Off-policy experience resampling (SR-Dyna)</label><p hwp:id="p-75">Here we introduce a third approach towards solving <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-6" hwp:rel-id="disp-formula-10">equation 9</xref>, SR-Dyna, which can be compared to Sutton’s Dyna approach [<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>] for solving <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-12" hwp:rel-id="disp-formula-2">equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-3-13" hwp:rel-id="disp-formula-3">2</xref>. Akin to how Dyna replays experienced transitions offline to update estimates of <italic toggle="yes">V</italic>(<italic toggle="yes">s</italic>), SR-Dyna replays experienced transitions to update the successor matrix. When this approach is combined with an ‘off-policy’ update rule, similar to Q learning, to update the successor matrix offline, it is capable of solving the off-policy planning problem. Utilizing this type of update, however, requires us to work with a state-action version of the successor representation, <italic toggle="yes">H</italic>, which can be used directly to form <italic toggle="yes">Q</italic> values [<xref rid="c58" ref-type="bibr" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>,<xref rid="c59" ref-type="bibr" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref>]. The key idea here is to define future occupancy not over states but over state/action pairs, <italic toggle="yes">sa</italic>. Analogous to <xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-4-4" hwp:rel-id="disp-formula-4">equation 3</xref>, <italic toggle="yes">Q</italic> <sup><italic toggle="yes">π</italic></sup> can then be expressed:
<disp-formula id="eqn13" hwp:id="disp-formula-14" hwp:rev-id="xref-disp-formula-14-1">
<alternatives hwp:id="alternatives-14"><graphic xlink:href="083857_eqn13.gif" position="float" orientation="portrait" hwp:id="graphic-19"/></alternatives>
</disp-formula>
</p><p hwp:id="p-76"><italic toggle="yes">H</italic> is a matrix of expected cumulative discounted future state-action visitations, i.e. given that you are starting with state <italic toggle="yes">s</italic> and action <italic toggle="yes">a</italic>, the cumulative (discounted) expected number of times you will encounter each other state/action pair:
<disp-formula id="eqn14" hwp:id="disp-formula-15" hwp:rev-id="xref-disp-formula-15-1">
<alternatives hwp:id="alternatives-15"><graphic xlink:href="083857_eqn14.gif" position="float" orientation="portrait" hwp:id="graphic-20"/></alternatives>
</disp-formula>
</p><p hwp:id="p-77"><italic toggle="yes">H</italic> can then be used as a linear basis for learning <italic toggle="yes">Q</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>), using the SARSA TD algorithm to learn a weight for each column of <italic toggle="yes">H</italic>. In particular, when state-action <italic toggle="yes">s</italic>’<italic toggle="yes">a</italic>’ is performed after state action <italic toggle="yes">sa</italic>, a prediction error is calculated and used to update w:
<disp-formula id="eqn15" hwp:id="disp-formula-16" hwp:rev-id="xref-disp-formula-16-1">
<alternatives hwp:id="alternatives-16"><graphic xlink:href="083857_eqn15.gif" position="float" orientation="portrait" hwp:id="graphic-21"/></alternatives>
</disp-formula>
</p><p hwp:id="p-78">Like <italic toggle="yes">M</italic>, <italic toggle="yes">H</italic> can be defined recursively:
<disp-formula id="eqn16" hwp:id="disp-formula-17">
<alternatives hwp:id="alternatives-17"><graphic xlink:href="083857_eqn16.gif" position="float" orientation="portrait" hwp:id="graphic-22"/></alternatives>
</disp-formula>
where <italic toggle="yes">T</italic> <sup><italic toggle="yes">π</italic></sup> is the one-step state-action transition matrix, <italic toggle="yes">T</italic> <sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">sa</italic>, <italic toggle="yes">s</italic>′<italic toggle="yes">a</italic> ′) = ∑<sub><italic toggle="yes">s</italic>′</sub> ∑<sub><italic toggle="yes">a</italic> ′</sub> <italic toggle="yes">P</italic>(<italic toggle="yes">s</italic>′ |s, <italic toggle="yes">a</italic>) <italic toggle="yes">π</italic>(<italic toggle="yes">a</italic> ′|<italic toggle="yes">s</italic>′). As with SR-TD, this recursion can be used to derive a TD-like update rule by which an estimate of <italic toggle="yes">H</italic> can be iteratively updated:
<disp-formula id="eqn17" hwp:id="disp-formula-18">
<alternatives hwp:id="alternatives-18"><graphic xlink:href="083857_eqn17.gif" position="float" orientation="portrait" hwp:id="graphic-23"/></alternatives>
</disp-formula>
</p><p hwp:id="p-79">As with SR-MB, it is also possible to derive <bold><italic toggle="yes">H</italic></bold> from <bold><italic toggle="yes">T</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">sa</italic></bold>, <bold><italic toggle="yes">s</italic></bold>′ <bold><italic toggle="yes">a</italic></bold>′<bold>)</bold> using an explicit “model-based” solution analogous to <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-7" hwp:rel-id="disp-formula-10">equation 9</xref>. However, here, we investigate the approach of updating <bold><italic toggle="yes">H</italic></bold> off-line (e.g., between trials or during rest periods) using replay of experienced trajectories (e.g. [<xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref>]). The key assumption we make is that this off-line replay can sequentially activate both the state and reward (cortical and basal ganglia) stages of <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Figure 1b</xref>, giving rise to an off-policy update of <italic toggle="yes">H</italic> with respect to the policy <bold><italic toggle="yes">π</italic></bold>* that is optimal given the current rewards. By comparison, as articulated above, we assumed such policy maximization was not possible when computing the successor representation <bold><italic toggle="yes">M</italic></bold> on-line for SR-MB, since this entire computation was supposed to happen in cortex at decision time, upstream of the striatal reward learning stage.</p><p hwp:id="p-80">Following each transition, SR-Dyna stores the sample (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>, <italic toggle="yes">s</italic>’). Then in between decisions, SR-Dyna randomly selects (with a recency weighted bias) <italic toggle="yes">k</italic> samples (with replacement). For each sample, it updates H as follows:
<disp-formula id="eqn18" hwp:id="disp-formula-19">
<alternatives hwp:id="alternatives-19"><graphic xlink:href="083857_eqn18.gif" position="float" orientation="portrait" hwp:id="graphic-24"/></alternatives>
</disp-formula>
where
<disp-formula id="ueqn2" hwp:id="disp-formula-20">
<alternatives hwp:id="alternatives-20"><graphic xlink:href="083857_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-25"/></alternatives>
</disp-formula></p><p hwp:id="p-81">That is, when <italic toggle="yes">H</italic> updates from previously experienced samples, it performs an off-policy update using the best action it could have chosen, rather than the one it actually chose.</p></statement></sec></sec></sec><sec id="s3" hwp:id="sec-19"><title hwp:id="title-27">Results</title><sec id="s3a" hwp:id="sec-20"><title hwp:id="title-28">SR-Dyna can solve policy revaluation tasks</title><p hwp:id="p-82">Given sufficient sampling (large enough <bold><italic toggle="yes">k</italic></bold>), this off-policy updating not only permits SR-Dyna to solve the detour task, but also to solve the novel policy revaluation task (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Fig 6a</xref>). In the policy revaluation task, after the agent is introduced to the new reward in R2, and updates <bold><italic toggle="yes">w</italic></bold>, we permit it to draw 10,000 random samples from memory and perform an update for each. With each sample drawn, SR-Dyna partially adjusts the predictions of future state occupancies from a given state action <bold><italic toggle="yes">sa</italic></bold>, so that they become closer to the predictions of future state occupancies from <bold><italic toggle="yes">s</italic></bold>’<bold><italic toggle="yes">a</italic></bold>*, where <bold><italic toggle="yes">a</italic></bold>* is chosen with consideration of the newly updated <bold><italic toggle="yes">w</italic></bold>. Such updates thus allow SR-Dyna to re-learn a new version of <bold><italic toggle="yes">H</italic><sup><italic toggle="yes">π</italic></sup></bold>, corresponding to the policy that would result from repeated choices under the updated <bold><italic toggle="yes">w</italic></bold>. Once updated, <bold><italic toggle="yes">H</italic><sup><italic toggle="yes">π</italic></sup></bold> comes to reflect prediction of future states reflective of a policy that moves towards the new highest reward in the bottom right of the maze. When the updated <bold><italic toggle="yes">H</italic><sup><italic toggle="yes">π</italic></sup></bold> is then used to compute new values (on the first test trial following the policy retraining), those values result in a policy that would bring the agent from S to the new highest reward in R2 along the shortest path. This simulation demonstrates that SR-Dyna can thus produce behavior identical to “full” model-based value iteration in this task (as well as the other revaluation tasks previously simulated, as shown below in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6</xref>). However, it has the potential advantage that updating can take place fully off-line and thus offload computation to situations that may be ecologically convenient such as sleep or wakeful rest.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Fig. 6.</label><caption hwp:id="caption-6"><title hwp:id="title-29">Comparison of SR-Dyna and Dyna-Q</title><p hwp:id="p-83">Median value function (grayscale) and implied policy after each algorithm (row) learns about relevant change in each of the 3 tasks (column). Both SR-Dyna (a) and Dyna-Q (b) can solve all 3 tasks when a sufficient number of samples backed up. c) Without a sufficient number of samples, SR-Dyna can still solve the latent learning task. d) Without a sufficient number of samples, Dyna-Q cannot solve any of the 3 tasks.</p></caption><graphic xlink:href="083857_fig6" position="float" orientation="portrait" hwp:id="graphic-26"/></fig></sec><sec id="s3b" hwp:id="sec-21"><title hwp:id="title-30">Time constraints can distinguish SR-Dyna from Dyna-Q</title><p hwp:id="p-84">SR-Dyna is capable of behaving equivalently to dynamic programming and tree-search in that it can solve transition and policy revaluation tasks. We were thus interested in how it could be differentiated experimentally. Sutton’s original Dyna algorithm (Dyna-Q) differs from value iteration and tree search in that its ability to pass revaluation experiments is dependent on having enough time offline to perform sufficient number of sample backups. Given enough time between decisions, sufficient replay can occur and Dyna-Q can pass any type of revaluation task (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Fig 6b</xref>). In contrast, without sufficient offline replay, Dyna-Q degrades to a model-free agent and it cannot pass any revaluation task (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Fig 6d</xref>). SR-Dyna is similarly differentiated from tree-search and value iteration in that its flexibility depends on a completing a sufficient number of sample backups offline. As demonstrated above, given a sufficient number of backups, SR-Dyna can pass any type of revaluation (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Fig 6a</xref>). Without sufficient replay, its performance degrades to that of SR-TD – it can pass reward revaluation but fails transition and policy revaluation (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Fig 6c</xref>). SR-Dyna is thus differentiated from Dyna-Q in that, unlike Dyna-Q, without sufficient replay, it can still pass reward revaluation; that is, it retains a certain degree of “model-based” flexibility even in the degraded case. These predictions could be tested with experimental designs aimed at preventing replay by manipulating the length of rest periods or the difficulty of distractor tasks [<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-5" hwp:rel-id="ref-27">27</xref>].</p><p hwp:id="p-85">Time constraints or distractor tasks at decision time can also disambiguate the different algorithms. Tree-search and value iteration take time and effort at decision time, whereas SR-Dyna can support rapid action selection by inspecting its lookup table.</p></sec></sec><sec id="s4" hwp:id="sec-22"><title hwp:id="title-31">Interim Discussion</title><sec id="s4a" hwp:id="sec-23"><title hwp:id="title-32">Biological plausibility</title><p hwp:id="p-86">Following real experience, SR-Dyna uses a similar update rule as SR-TD, yet uses it to operate over state-actions rather than states. This is plausible given that the same Hebbian learning principles could operate over cortical or hippocampal representations of state/action conjunctions just as well as they could over states.</p><p hwp:id="p-87">As with Dyna-Q, SR-Dyna dovetails nicely with neural evidence about memory replay. Specifically, the widely demonstrated phenomenon of reactivation during rest or sleep of sequences of hippocampal activity seen during prior experiences [<xref rid="c61" ref-type="bibr" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref>,<xref rid="c62" ref-type="bibr" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">62</xref>], seems well suited to support the sort of off-line updates imagined by both Dyna approaches. (Although we have not simulated realistic hippocampal replay dynamics here, the Dyna approaches can learn from experiences replayed in arbitrary order.) The successor matrix updated by SR-Dyna might itself exist in the recurrent connections of hippocampal neurons [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>], though another intriguing possibility is that it is instead stored in prefrontal cortex (as in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Figure 1b</xref>). This second possibility lines up neatly with complementary system theories in the memory literature, according to which such hippocampal replay plays a role in post-encoding consolidation of memories by restructuring how information is represented across neocortical networks [<xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">63</xref>,<xref rid="c64" ref-type="bibr" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">64</xref>]. Such a connection should be explored in future research.</p></sec><sec id="s4b" hwp:id="sec-24"><title hwp:id="title-33">Behavioral adequacy</title><p hwp:id="p-88">Given sufficient replay, SR-Dyna is capable of producing behavior as flexible as that of full model-based value iteration. As with Dyna-Q, practical applications of SR-Dyna in larger environments will require developing sophisticated methods for selecting which samples to replay (e.g. [<xref rid="c65" ref-type="bibr" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">65</xref>]). We intend to develop such methods in future work.</p></sec></sec><sec id="s5" hwp:id="sec-25"><title hwp:id="title-34">Discussion</title><p hwp:id="p-89">Despite evidence that animals engage in flexible behaviors suggestive of model-based planning, we have little knowledge of how these computations are actually performed in the brain. Indeed, what evidence we have – particularly concerning the involvement of dopamine in these computations – seems difficult to reconcile with the standard abstract computational picture of planning by tree search using a learned model. We have here proposed variants of the SR that can address this question, serving as empirically consistent mechanisms for some or indeed all of the behaviors associated with model-based learning. Moreover, these are each built as utilizing a common TD learning stage for reward expectancies, allowing them to fit within the systems-level picture suggested by rodent lesion studies, and also explaining the involvement of dopamine in model-based valuation. In particular, they each envision how model-based learning could arise from the same dopaminergic TD learning associated with simple model-free learning, operating over a different and more elaborated cortical input representation.</p><sec id="s5a" hwp:id="sec-26"><title hwp:id="title-35">Accounting for TD circuitry in apparently model-based behavior</title><p hwp:id="p-90">More specifically, our motivation to develop this approach was based on three related sets of findings in the empirical literature. The first are that lesions to dorsomedial striatum prevent animals from adjusting preferences following reward revaluation [<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">5</xref>]. In contrast, lesions to neighboring dorsolateral striatum cause rats to maintain devaluation sensitivity, even following overtraining [<xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">38</xref>]. In the framework presented here, neurons in dorsomedial striatum could represent values derived by applying TD learning to the successor representation and neurons in dorsolateral striatum could represent values derived by applying TD to tabular representations. Lesions to dorsomedial striatum would thus force the animal to work with values in dorsolateral striatum, derived from tabular representations and thus not sensitive to devaluation. In contrast, lesions to dorsolateral striatum would cause the brain to work with values derived from the SR, which are devaluation-sensitive.</p><p hwp:id="p-91">The second set of findings include several reports that the phasic DA response (or analogous prediction error related BOLD signals in humans) tracks apparently model-based information [<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-4" hwp:rel-id="ref-6">6</xref>,<xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">11</xref>]. We have focused our simulations on choice behavior, and have not presented our theories’ analogous predictions about the responses of neurons, such as DA cells, thought to signal decision variables. However, whenever the SR algorithms’ expectations about action values incorporate “model-based” information (such as latent learning, <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig 4a</xref>) neural signals related to those predictions and to prediction errors would be similarly informed. Thus the theories predict systematic expectancy-related effects in the modeled dopamine response, tracking the differences in choice preference relative to the standard “model-free” accounts, which are blind to reward contingencies in these tasks.</p><p hwp:id="p-92">A third distinct set of findings also speaks to a relationship between dopamine and model-based learning. These are reports that several measures of dopaminergic efficiency (both causal and correlational) track the degree to which human subjects engage in model-based decision strategies in both multistep reward revaluation tasks and multiplayer games [<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>–<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref>,<xref rid="c66" ref-type="bibr" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">66</xref>]. One possibility is that these effects reflect strengthened vs. weakened phasic dopaminergic signaling, which in our model controls reward learning for SR-based “model-based” estimates in dorsomedial striatum. However, this account does not explain the specificity of these effects to measures of putative model-based (vs. model-free) learning. These effects may instead be related to functions of dopamine other than prediction error signaling, such as <italic toggle="yes">tonic</italic> dopamine’s involvement supporting working memory [<xref rid="c67" ref-type="bibr" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">67</xref>] or its hypothesized role controlling the allocation of cognitive effort [<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-3" hwp:rel-id="ref-39">39</xref>,<xref rid="c68" ref-type="bibr" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">68</xref>,<xref rid="c69" ref-type="bibr" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">69</xref>].</p></sec><sec id="s5b" hwp:id="sec-27"><title hwp:id="title-36">Other potential explanations</title><p hwp:id="p-93">The framework outlined in this paper is not the only direction toward a neurobiologically explicit theory of putatively model-based behavior, nor even the only suggestion explaining the involvement of dopamine. As discussed above and pointed out in [<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-6" hwp:rel-id="ref-27">27</xref>], Sutton’s original Dyna algorithm – in which experience replayed offline is used to update action values <italic toggle="yes">V</italic> or <italic toggle="yes">Q</italic> directly – offers another avenue by which seemingly model-based flexibility can be built on the foundation of the standard prediction error model of dopamine. This is a promising piece of the puzzle, but exclusive reliance on replay to underpin all behavioral flexibility seems unrealistic. Among our innovations here is to suggest that replay can also be used to learn and update a successor representation, which then confers many of the other advantages of model-based learning (such as flexibility in the face of reward devaluation) without the dependence on further replay to replan. Furthermore, the addition of SR to the Dyna framework explains a number of phenomena that replay, on its own, does not. For instance, given that Dyna-Q works with a single set of cached Q values, updated through both experience and replay, it is not clear how it could, on its own, explain the apparent segregation of revaluation sensitive and insensitive value estimates in dorsomedial and dorsolateral striatum [<xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">5</xref>,<xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-3" hwp:rel-id="ref-38">38</xref>].</p><p hwp:id="p-94">Another potential solution to some of the puzzles motivating this work is that dopamine could have a role in action selection, as part of a circuit for partially model-based action evaluation [<xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">47</xref>]. According to this idea, dopamine neurons could compute a prediction error measuring the difference between the value of the current state and the future value of a predicted successor state, caused by a given candidate action. The size of this prediction error could then determine whether the action is performed. This mechanism would endow the brain with a single step of model-based prediction. However, it is not straightforward how this sort of approach could underlie model-based learning in tasks requiring more than a single step of prediction, and accordingly our simulations (see <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Figure 4a</xref> and supplemental materials) show that it cannot solve any of the revaluation tasks considered here, which all probe for deeper search through the state space. A recent study provided convincing behavioral evidence that humans sometimes simplify model-based action selection by combining just one step of state prediction with cached successor state values [<xref rid="c70" ref-type="bibr" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">70</xref>]. Yet this same study along with others [<xref rid="c50" ref-type="bibr" hwp:id="xref-ref-50-2" hwp:rel-id="ref-50">50</xref>] have also provided evidence that humans can plan through more than one step and thus are not confined to this approximation. It is also not straightforward how this sort of mechanism could endow model-based predictions in cases where stochasticity requires consideration of “trees” of possible future states.</p><p hwp:id="p-95">Nevertheless, by elucidating a more general framework in which a predictive state representation may feed into downstream dopaminergic reward learning, we view our framework as fleshing out the spirit of this suggestion while also addressing these issues. We similarly realize other conceptual suggestions in the literature suggesting that more flexible model-based like behavior may arise not through tree-search like planning, but rather by applying model-free RL to more sophisticated state representations [<xref rid="c71" ref-type="bibr" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">71</xref>]. In a more specific application of this idea, [<xref rid="c72" ref-type="bibr" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">72</xref>] demonstrated that a sophisticated representation that includes reward history can produce model-based like behavior in the two-step reward revaluation task. The successor representation adds to this work by clarifying for any task’s transition structure, the precise representation that can be used to generate model-based behavior.</p><p hwp:id="p-96">Relatedly, because it places model-based state prediction in the input to a standard TD learning circuit, our framework could easily be extended to include several modules with inputs corresponding to several different types or granularities of models: for instance, varying degrees of temporal abstraction corresponding to different time discount factors in <xref ref-type="disp-formula" rid="eqn6" hwp:id="xref-disp-formula-7-1" hwp:rel-id="disp-formula-7">equation 6</xref> [<xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">44</xref>,<xref rid="c73" ref-type="bibr" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">73</xref>]. This would parallel a number of other recent suggestions that different striatal loops model the world at different levels of hierarchical abstraction [<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">46</xref>,<xref rid="c74" ref-type="bibr" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">74</xref>], while also harmonizing the somewhat underspecified model-based evaluation process these theories assume with the predominant temporal difference account of striatal learning.</p></sec><sec id="s5c" hwp:id="sec-28"><title hwp:id="title-37">Multiplicity and Arbitration</title><p hwp:id="p-97">Although our presentation culminated with proposing an algorithm (SR-Dyna) that can in principle perform equivalently to full model-based learning using value iteration, this need not be the only goal and there need not be only a single answer. The behaviors associated with model-based learning may not have unitary sources in the brain but may instead be multiply determined. All of the algorithms we have considered are viable candidate pieces of a larger set of decision systems. Notably, the experiments we have highlighted as suggesting striatal or dopaminergic involvement in “model-based” learning and inspiring the present work all use extremely shallow planning problems (e.g. operant lever pressing, two-stimulus Pavlovian sequences, or two-step MDPs) together with reward revaluation designs. Even SR-TD is sufficient to explain these. It may well be that planning in other tasks, like chess, or in spatial mazes, is supported by entirely different circuits that really do implement something like tree search; or that they differentially require replay, like SR-Dyna. Also, although replay-based approaches go a long way, value computation at choice time using more traditional model-based approaches is likely needed at the very least to explain the ability to evaluate truly novel options (like the value of “tea jelly”; [<xref rid="c75" ref-type="bibr" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">75</xref>]) using semantic knowledge. Some evidence that rodents may use more than just replay to compute values, even in spatial tasks, comes from findings that the prevalence of sharp-wave-ripples, a putative sign of replay, is inversely related to the prevalence of vicarious trial and error behaviors, a process thought to be involved in decision-time value computation, potentially by standard MB dynamic programming or alternatively SR-MB [<xref rid="c76" ref-type="bibr" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">76</xref>].</p><p hwp:id="p-98">Relatedly, if the brain might cache both endpoint decision variables like <italic toggle="yes">Q</italic>, or their precursors like <italic toggle="yes">M</italic>, update either or both with off-line replay, and optionally engage in further model-based recomputation at choice time, then the arbitration or control question of how the brain prioritizes all this computation to harvest rewards most effectively and efficiently becomes substantially more complicated than previously considered. The prioritization of replay – which memories to replay when – becomes particularly important. The particular ordering and dynamics of replay are also outside our modeling here: in order to focus our investigation on the simplest behavioral predictions of SR-Dyna, we chose the simplest, naive sampling scheme in which the agent replays a single state-action-state transition with uniformly random probability. This sampling strategy is not a mechanistic commitment (nor of course does it reflect the dynamics of realistic hippocampal replay trajectories), and we expect that like Dyna-Q, SR-Dyna would work even more efficiently given more sophisticated replay prioritization schemes. In this regard, we expect that the prioritization of replay, like the arbitration between model-based vs model-free tradeoffs [<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-6" hwp:rel-id="ref-1">1</xref>,<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-4" hwp:rel-id="ref-39">39</xref>,<xref rid="c77" ref-type="bibr" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">77</xref>], might operate according to the principles of efficient cost-benefit management. We expect that in addition to more typical observed patterns of replay, such a scheme may be able to explain cases where the replayed sequences are not a simple reflection of the animal’s current policy [<xref rid="c78" ref-type="bibr" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">78</xref>]. The current model is robust to differences in replay but would need to be extended with a more principled and detailed replay model to address these questions.</p></sec><sec id="s5d" hwp:id="sec-29"><title hwp:id="title-38">Future Experimental Work</title><p hwp:id="p-99">With simulations, we have presented experiments that could be used to elicit recognizable behavior form the different algorithms proposed here. Although we ruled out the simplest approach, SR-TD, due to its inflexibility, it is worth more carefully considering the evidence against it. The main counterexamples to SR-TD are transition revaluation and detour tasks. Apart from the classic work of Tolman and Honzik [<xref rid="c55" ref-type="bibr" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">55</xref>], the original results of which are actually quite mixed (see [<xref rid="c79" ref-type="bibr" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">79</xref>]), there is surprisingly little evidence to go on. A number of different studies have shown that healthy animals will normally choose the shortest alternative route after learning about a blockade preventing a previously preferred route (e.g. [<xref rid="c80" ref-type="bibr" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">80</xref>–<xref rid="c82" ref-type="bibr" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">82</xref>]). However, in these studies, the animal learns about the blockade after starting from the maze starting location. Thus, unlike in our simulations in which the animal learns about the blockade in isolation, animals in these tasks would have the opportunity to learn from direct experience that maze locations leading up to the blockade are no longer followed by maze locations further along the previously preferred path. Such tasks could thus potentially be solved by SR-TD. Studies that show that animals will take a shortcut to a goal that is discovered along a preferred path present a somewhat cleaner test for SR-TD [<xref rid="c83" ref-type="bibr" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">83</xref>,<xref rid="c84" ref-type="bibr" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">84</xref>]; however it is often difficult to interpret a potential role of exploration or visual (rather than cognitive map) guidance in the resulting behavior. Work in humans, however, seems to more clearly suggest an ability to solve detour tasks without re-learning [<xref rid="c85" ref-type="bibr" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">85</xref>]. Simon and Daw [<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-3" hwp:rel-id="ref-23">23</xref>] for instance directly assessed SR-TD’s fit to human subjects’ choice adjustments in a changing spatial maze, and found it fit poorly relative to traditional model-based learning.</p><p hwp:id="p-100">Overall, additional careful work that measures how animals respond to transition changes, learned in isolation, is needed. Whereas Tolman’s other early reward revaluation experiments (latent learning) have been conceptually replicated in many modern, non-spatial tasks like instrumental reward devaluation and sensory preconditioning, the same is not true of detours. Indeed, the modern operant task that is often presented as analogous to detours, so-called instrumental contingency degradation (e.g., [<xref rid="c86" ref-type="bibr" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">86</xref>]), is not functionally equivalent. In such tasks, the association between an action and its outcome is degraded through introduction of background rewards. However, because the information about the changed contingency is not presented separately from the rest of the experience about actions and their rewards, unlike all the other tests discussed here, contingency degradation as it has been studied in instrumental conditioning can actually be solved by a simple model-free learner that re-learns the new action values. The puzzle here is actually not how animals can solve the task, but why they should ever fail to solve it. This has thus led to a critique not of model-based but of model-free learning theories [<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-3" hwp:rel-id="ref-46">46</xref>].</p><p hwp:id="p-101">In any case, the modeling considerations proposed here suggest that more careful laboratory work on “transition revaluation” type changes to detect use of SR-TD, is warranted. Similarly, “policy revaluations” along the lines of that in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig 5</xref> would be useful to detect to what extent planning along the lines of SR-MB is contributing. Finally, although SR-Dyna in principle can perform model-based value computation, this depends on sufficient replay. The SR-Dyna hypothesis suggests the testable prediction that behavior should degrade to SR-TD under conditions when replay can contribute less. A number of experiments in the rodent literature have explored the behavioral deficits that result from interrupting sharp-wave ripples (events in which hippocampal replay is known to occur). Such manipulations have been shown to produce behavioral deficits that are consistent with the SR-Dyna hypothesis, yet not exclusively predicted by it. For instance, two studies found that suppression of hippocampal sharp-wave ripples during rest slows down acquisition of the correct behavioral policy in spatial learning tasks in which the task environment is static [<xref rid="c87" ref-type="bibr" hwp:id="xref-ref-87-1" hwp:rel-id="ref-87">87</xref>,<xref rid="c88" ref-type="bibr" hwp:id="xref-ref-88-1" hwp:rel-id="ref-88">88</xref>]. These results are consistent with the notion that the purpose of replay is to provide additional experience, which is used to update some representation relevant to learning. However, these results are not specifically diagnostic of the successor matrix. For instance, preventing replay would slow down policy acquisition for Dyna-Q as well as SR-Dyna (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Supplementary Figure 2</xref>).</p><p hwp:id="p-102">Another study found a more specific effect of suppressing hippocampal sharp wave ripples during performance of a task. Here, the manipulation caused learning deficits selective for a subset of trials in which animals faced a hidden-state problem. In particular, these were trials in which animals had to choose which direction to turn depending on the events of the previous trial [<xref rid="c89" ref-type="bibr" hwp:id="xref-ref-89-1" hwp:rel-id="ref-89">89</xref>]. Such tasks constitute hidden-state problems, in that the “state” required to make the correct choice cannot be deduced entirely from an animal’s immediate sensory experience. In RL terms, to solve these problems, the animals must construct an augmented internal state, distinct from a simple representation of the immediate sensory situation [<xref rid="c90" ref-type="bibr" hwp:id="xref-ref-90-1" hwp:rel-id="ref-90">90</xref>]. One interpretation of the experimental result is that blocking replay interfered with this internal state construction process.</p><p hwp:id="p-103">This result resonates with our SR-Dyna proposal, which also posits that replay is involved in constructing the mapping between the sensory state and a different, augmented internal representation of it: the SR. However, our model as currently specified augments the state space with predictive features to support model-based flexibility, and does not currently address other sorts of elaborations of the state input that have been used in other work to facilitate learning in situations with hidden state or other uncertain sensory input [<xref rid="c91" ref-type="bibr" hwp:id="xref-ref-91-1" hwp:rel-id="ref-91">91</xref>–<xref rid="c93" ref-type="bibr" hwp:id="xref-ref-93-1" hwp:rel-id="ref-93">93</xref>]. Fully understanding these results therefore requires augmenting our model to address hidden state as well as state prediction. In fact, these two functions may be closely related: a number of approaches to the hidden state problem in the computational RL literature address it using predictive representations that are related to the SR [<xref rid="c94" ref-type="bibr" hwp:id="xref-ref-94-1" hwp:rel-id="ref-94">94</xref>,<xref rid="c95" ref-type="bibr" hwp:id="xref-ref-95-1" hwp:rel-id="ref-95">95</xref>].</p><p hwp:id="p-104">In human studies, factors like the duration of off-task rest periods and presence of distractor tasks during such periods have been manipulated to extend, limit, or interfere with replay. Some evidence suggests that distractor tasks at decision time have no effect on reward revaluation [<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-7" hwp:rel-id="ref-27">27</xref>], consistent with SR-Dyna. Other recent work has demonstrated that humans benefit from additional pre-decision time in revaluation tasks that closely resemble “policy revaluation” [<xref rid="c96" ref-type="bibr" hwp:id="xref-ref-96-1" hwp:rel-id="ref-96">96</xref>] and that this benefit recruits a network including the prefrontal cortex and basal ganglia. Such work is consistent with the predictions of both Dyna-Q as well as SR-Dyna accounts of value updating presented here.</p><p hwp:id="p-105">Overall, future work will need to combine such manipulations of replay, with the three revaluation tasks imagined in this paper and demonstrate differences in the effects of manipulations on reward versus transition and policy revaluations. We have recently demonstrated, though without attempting to manipulate replay, that humans are worse at adjusting behavior following transition and policy revaluations compared to reward revaluations, suggesting that they may at least partially use either an SR-TD or SR-Dyna (with limited sample backups) strategy for evaluation [<xref rid="c57" ref-type="bibr" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">57</xref>].</p></sec><sec id="s5e" hwp:id="sec-30"><title hwp:id="title-39">Neural substrate of the successor representation</title><p hwp:id="p-106">We have suggested, on the basis of rodent lesion studies, that the SR may be encoded in parts of prefrontal cortex that project to dorosomedial striatum. However, we should note that recent work has also implicated the hippocampus as a potential site of the SR. Specifically, a state-state version of the SR can explain some properties of hippocampal place cells [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>,<xref rid="c97" ref-type="bibr" hwp:id="xref-ref-97-1" hwp:rel-id="ref-97">97</xref>] as well as fMRI measures of the representation of visual stimuli in tasks where such stimuli are presented sequentially [<xref rid="c98" ref-type="bibr" hwp:id="xref-ref-98-1" hwp:rel-id="ref-98">98</xref>,<xref rid="c99" ref-type="bibr" hwp:id="xref-ref-99-1" hwp:rel-id="ref-99">99</xref>]. This work has largely built on ideas of the hippocampus in general as a site of cognitive map [<xref rid="c100" ref-type="bibr" hwp:id="xref-ref-100-1" hwp:rel-id="ref-100">100</xref>] as well as prior suggestions that hippocampal place cells may in fact encode the transition structure of the environment [<xref rid="c101" ref-type="bibr" hwp:id="xref-ref-101-1" hwp:rel-id="ref-101">101</xref>] and that such transition information may make them ideal basis functions for TD learning [<xref rid="c102" ref-type="bibr" hwp:id="xref-ref-102-1" hwp:rel-id="ref-102">102</xref>] If a state version of the SR exists in the hippocampus, we think it is reasonable that value weights would be leaned by neurons connecting the hippocampus to ventral striatum, in the same TD manner discussed in this paper.</p><p hwp:id="p-107">However, we also think a case can be made for the prefrontal cortex as another candidate basis for an SR. (These proposals are in no way mutually exclusive.) In addition to the rodent lesion evidence reviewed in the introduction of this paper, the prefrontal cortex shares many cognitivemap properties observed in the hippocampus [<xref rid="c103" ref-type="bibr" hwp:id="xref-ref-103-1" hwp:rel-id="ref-103">103</xref>] and has been suggested to be the basis of state representations for reinforcement learning [<xref rid="c91" ref-type="bibr" hwp:id="xref-ref-91-2" hwp:rel-id="ref-91">91</xref>,<xref rid="c104" ref-type="bibr" hwp:id="xref-ref-104-1" hwp:rel-id="ref-104">104</xref>]. A number of human studies have demonstrated the PFC’s role in the representation of prospective goals [<xref rid="c105" ref-type="bibr" hwp:id="xref-ref-105-1" hwp:rel-id="ref-105">105</xref>,<xref rid="c106" ref-type="bibr" hwp:id="xref-ref-106-1" hwp:rel-id="ref-106">106</xref>]. Furthermore, unlike the hippocampus, parts of the prefrontal cortex appear to be involved in action representation in addition to state representation [<xref rid="c107" ref-type="bibr" hwp:id="xref-ref-107-1" hwp:rel-id="ref-107">107</xref>], thus making it a candidate to hold a potential state-action version of the successor matrix. Overall, further experimental work will be required to determine whether either or indeed both these areas serves as the basis for the successor representation, and what specific roles they play in learning and representation.</p></sec><sec id="s5f" hwp:id="sec-31"><title hwp:id="title-40">Connection to other cognitive processes</title><p hwp:id="p-108">Finally, the SR may contribute to a number of other cognitive processes. Above we noted that there is evidence that areas of medial temporal lobe seem to encode predictive representations. In line with this, it has been noted that there is a close correspondence between the update rule used by SR-TD and update rules in the temporal context model of memory [<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]. Also, recent approaches to reinforcement learning in the brain have advocated for a hierarchical approach in which punctate actions are supplemented by temporally abstract policies [<xref rid="c108" ref-type="bibr" hwp:id="xref-ref-108-1" hwp:rel-id="ref-108">108</xref>]. In this context, it has been suggested that the SR may be useful for discovering useful temporal abstractions by identifying bottlenecks in the state space that can then be used to organize states and action into a hierarchy [<xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-3" hwp:rel-id="ref-18">18</xref>,<xref rid="c109" ref-type="bibr" hwp:id="xref-ref-109-1" hwp:rel-id="ref-109">109</xref>]. The efficacy of the SR for model-based RL opens the possibility that the brain accomplishes planning, action chunking, and grouping episodic memories using a common mechanism.</p><p hwp:id="p-109">Overall, this article has laid out a family of candidate mechanistic hypotheses for explaining the full range of behaviors typically associated with model-based learning, while connecting them with the circuitry for model-free learning as currently understood. In addition to the transition and policy revaluation behavioral experiments suggested above, future neuroimaging work could seek evidence for these hypotheses. Specifically, failures to flexibly update decision policies that are caused by caching of either the successor representation (as in SR-TD or SR-Dyna with insufficient replay) or a decision policy (as in SR-MB) should be accompanied by neural markers of non-updated future state occupancy predictions. Such neural markers could be identified using representational similarity analysis (e.g. [<xref rid="c110" ref-type="bibr" hwp:id="xref-ref-110-1" hwp:rel-id="ref-110">110</xref>]), cross-stimulus suppression (e.g. [<xref rid="c111" ref-type="bibr" hwp:id="xref-ref-111-1" hwp:rel-id="ref-111">111</xref>]) or through use of category specific, decodable, visual stimuli (e.g. [<xref rid="c112" ref-type="bibr" hwp:id="xref-ref-112-1" hwp:rel-id="ref-112">112</xref>]). Similar work in experimental animals such as rodents (e.g. [<xref rid="c113" ref-type="bibr" hwp:id="xref-ref-113-1" hwp:rel-id="ref-113">113</xref>]) could use the full range of invasive tools to trace the inputs to dorsomedial vs. dorsolateral striatum, so as to examine the information represented there and how it changes following the various sorts of revaluation manipulations discussed here. As has been the case for model-free learning, the emergence of an increasingly clear and quantitative taxonomy of different candidate algorithms is likely to guide this work and help to elucidate the neural basis of model-based learning.</p></sec></sec><sec id="s6" hwp:id="sec-32"><title hwp:id="title-41">Methods</title><sec id="s6a" hwp:id="sec-33"><title hwp:id="title-42">General Simulation Methods</title><p hwp:id="p-110">All simulations were carried out in 10x10 (N = 100 states) grid-worlds in which the agent could move in any of the four cardinal directions, unless a wall blocked such a movement. States with rewards contained a single action. Upon selecting that action, the agent received the reward and was taken to a terminal state. Each task was simulated with each algorithm 500 times. For each simulation, we recorded the agent’s value function at certain points. For SR-Dyna, which worked with action values rather than state values, the state value function was computed as the max action value available in that state. Figures display the median value, for each state, over the 500 runs. To determine the implied policy for the median value function, we computed, for each state, which accessible successor state had the maximum median value.</p></sec><sec id="s6b" hwp:id="sec-34"><title hwp:id="title-43">Specific Task Procedures</title><sec id="s6b1" hwp:id="sec-35"><title hwp:id="title-44">Latent learning task</title><p hwp:id="p-111">The latent learning task was simulated in the grid-world environment shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2a</xref>. Starting from position S, the agent first took 25000 steps exploring the maze. After exploration, the reward in position R1 was raised to 10. To learn about the reward, the agent completed a single step, starting from position R1, 20 times. We then recorded the state value function.</p></sec><sec id="s6b2" hwp:id="sec-36"><title hwp:id="title-45">Detour task</title><p hwp:id="p-112">The detour task was simulated using the grid-world environment shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2b</xref>. Starting from position S, the agent first took 10000 steps exploring the maze. The reward in position R was then increased to 10. The agent then completed 5 trials, starting from position S that ended when the reward was reached. A wall was then added to in position B. To learn about the wall, the agent completed a single step, starting from the position immediately left of the wall, 40 times. We then recorded the state value function.</p></sec><sec id="s6b3" hwp:id="sec-37"><title hwp:id="title-46">Novel revaluation task</title><p hwp:id="p-113">The novel revaluation task was simulated using the environment in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Fig. 5c</xref>. The agent first completed the entire latent learning task. After successfully reaching position R1 from position S, the agent then completed 20 trials. Each trial alternately started at S1 or S2 and ended when the agent reached position R1. We then set the reward in position R2 to 20. To learn about the new reward, the agent completed one step, starting from position R2, 20 times. We then recorded the state value function.</p></sec></sec><sec id="s6c" hwp:id="sec-38"><title hwp:id="title-47">Additional Details on Algorithms</title><sec id="s6c1" hwp:id="sec-39"><title hwp:id="title-48">One-step look-ahead</title><p hwp:id="p-114">The one-step look-ahead model stored an estimate of state-value function <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> <italic toggle="yes">s</italic>. At the beginning of each simulation <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> was initilzled to <bold>0</bold>. Following each transition <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup> was updated according to <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-5-6" hwp:rel-id="disp-formula-5">equation 4</xref>. Prior to each choice, Q-values for each action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic> were then computed as <italic toggle="yes">Q</italic><sup><italic toggle="yes">π</italic></sup> (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) = <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>’) where <italic toggle="yes">s</italic>’ is the state that deterministically follows action a in state s. Note that leaving <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) out of this equation works because rewards are paired exclusively with actions in terminal states (and thus <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) for non-terminal actions is 0).</p></sec><sec id="s6c2" hwp:id="sec-40"><title hwp:id="title-49">Original successor representation (SR-TD)</title><p hwp:id="p-115">SR-TD computed <italic toggle="yes">V</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>) using two structures: the successor matrix, <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup>(<italic toggle="yes">s</italic>, <italic toggle="yes">s</italic>’) and a weight vector, <italic toggle="yes">w</italic>(<italic toggle="yes">s</italic>). At the beginning of each simulation, <italic toggle="yes">M</italic><sup><italic toggle="yes">π</italic></sup> was initialized as an identity matrix; however, rows corresponding to terminal states were set to 0. The weight vector was initialized as <italic toggle="yes">w</italic> = <bold>0</bold>. Following each transition, <italic toggle="yes">M</italic> and <italic toggle="yes">w</italic> were updated using <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-5" hwp:rel-id="disp-formula-9">equations (8)</xref> and <xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-10-8" hwp:rel-id="disp-formula-10">(9)</xref>. In implementing the update in <xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-9-6" hwp:rel-id="disp-formula-9">equation 8</xref>, each element of the feature vector, <italic toggle="yes">M</italic>(<italic toggle="yes">s</italic>,:), was scaled by <italic toggle="yes">M</italic>(<italic toggle="yes">s</italic>,:)* <italic toggle="yes">M</italic> (<italic toggle="yes">s</italic>,:) <sup><italic toggle="yes">T</italic></sup>. This scaling permits the weight learning rate parameter to maintain a consistent interpretation as proportional step-size. Prior to each choice, <italic toggle="yes">V</italic> was computed using <xref ref-type="disp-formula" rid="eqn7" hwp:id="xref-disp-formula-8-4" hwp:rel-id="disp-formula-8">equation (7)</xref>. Q-values for each action were computed the same as for the one-step look-ahead model.</p></sec><sec id="s6c3" hwp:id="sec-41"><title hwp:id="title-50">Recomputation of the successor matrix (SR-MB)</title><p hwp:id="p-116">This algorithm starts each task with a basic knowledge of the ‘physics’ of grid-world: which successor state, <italic toggle="yes">s</italic>’, would follow each action <italic toggle="yes">sa</italic> in a situation in which <italic toggle="yes">sa</italic> is available (e.g. not blocked by a wall). It also stores and updates, for each state <italic toggle="yes">s</italic>, <italic toggle="yes">A</italic><sub><italic toggle="yes">s</italic></sub>, the set of actions currently available in state <italic toggle="yes">s</italic> as well as a policy <italic toggle="yes">π</italic>(<italic toggle="yes">a</italic>|<italic toggle="yes">s</italic>), which stores the probability of selecting action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic> (as learned from the agent’s own previous choices). <italic toggle="yes">A</italic><sub><italic toggle="yes">s</italic></sub> was initialized to reflect all four cardinal actions being available in each state. Each row <italic toggle="yes">π</italic> were initialized as a uniform distribution over state-actions, <italic toggle="yes">π</italic>(<italic toggle="yes">a</italic>|<italic toggle="yes">s</italic>) = 0.25.</p><p hwp:id="p-117">After performing action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic> and transitioning to state <italic toggle="yes">s</italic>’, <italic toggle="yes">A</italic><sub><italic toggle="yes">s’</italic></sub> was updated to reflect which actions are available in state <italic toggle="yes">s</italic>’ and <italic toggle="yes">π</italic> is updated using a delta rule:
<disp-formula id="ueqn3" hwp:id="disp-formula-21">
<alternatives hwp:id="alternatives-21"><graphic xlink:href="083857_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-27"/></alternatives>
</disp-formula>
where <italic toggle="yes">α</italic><sub><italic toggle="yes">π</italic></sub> is a free parameter.</p><p hwp:id="p-118">Prior to each choice, the model computed each row, s, of one-step transition matrix <italic toggle="yes">T</italic> <sup><italic toggle="yes">π</italic></sup> as follows:
<disp-formula id="ueqn4" hwp:id="disp-formula-22">
<alternatives hwp:id="alternatives-22"><graphic xlink:href="083857_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-28"/></alternatives>
</disp-formula>
where <bold>1</bold><sub><italic toggle="yes">s</italic></sub> is a vector zeros of length <italic toggle="yes">S</italic> with a 1 in position corresponding to state <italic toggle="yes">s</italic>’ and <italic toggle="yes">s</italic>’ is the state to which action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic> deterministically leads. <italic toggle="yes">T</italic> <sup><italic toggle="yes">π</italic></sup> was then used to compute <italic toggle="yes">M</italic> using <xref ref-type="disp-formula" rid="eqn10" hwp:id="xref-disp-formula-11-4" hwp:rel-id="disp-formula-11">equation 10</xref>. Computation of <italic toggle="yes">V</italic> and <italic toggle="yes">Q</italic> was then the same as in SR-TD.</p></sec><sec id="s6c4" hwp:id="sec-42"><title hwp:id="title-51">Episodic replay algorithm (SR-Dyna)</title><p hwp:id="p-119">This algorithm computed <italic toggle="yes">Q</italic>(<italic toggle="yes">sa</italic>) using two structures: a state-action successor matrix, <italic toggle="yes">H</italic>(<italic toggle="yes">sa</italic>, <italic toggle="yes">s</italic>’<italic toggle="yes">a</italic>’) and weight vector <italic toggle="yes">w</italic>(<italic toggle="yes">sa</italic>). At the beginning of each simulation, the successor matrix <italic toggle="yes">H</italic> was initialized to an identity matrix; however rows corresponding to terminal states were set to <bold>0</bold>. The weight vector was initialized to <italic toggle="yes">w</italic> = <bold>0</bold>. The algorithm also stored every sample (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>, <italic toggle="yes">s</italic>’). After performing action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic> and transitioning to state <italic toggle="yes">s</italic>’ the sample (<italic toggle="yes">s</italic>’, <italic toggle="yes">a</italic>’, <italic toggle="yes">s</italic>’) was stored, and <italic toggle="yes">H</italic> and <italic toggle="yes">w</italic> were updated according to <xref ref-type="disp-formula" rid="eqn13" hwp:id="xref-disp-formula-14-1" hwp:rel-id="disp-formula-14">equations (13)</xref> and <xref ref-type="disp-formula" rid="eqn14" hwp:id="xref-disp-formula-15-1" hwp:rel-id="disp-formula-15">(14)</xref>. Following each step, we also selected 10 one-step samples (according to recency weighted probabilities with replacement) from the stored history, and replayed each to update H according to <xref ref-type="disp-formula" rid="eqn15" hwp:id="xref-disp-formula-16-1" hwp:rel-id="disp-formula-16">equation 15</xref>. Following transitions in which a learned change occurred to either the reward function or available actions, <italic toggle="yes">k</italic> one-step samples were selected and used to update the model, where k was set to 10 in the insufficient replay condition and to 10000 in the sufficient replay condition. Samples were drawn by first selecting a state-action, <italic toggle="yes">sa</italic>, from a uniform distribution. A sample then drawn from the set of experienced samples, initiated in <italic toggle="yes">sa,</italic> according to an in, initiated from sa was then selected according to an exponential distribution with <italic toggle="yes">λ</italic> = 1/5.</p></sec><sec id="s6c5" hwp:id="sec-43"><title hwp:id="title-52">Dyna-Q</title><p hwp:id="p-120">This algorithm stored <italic toggle="yes">Q</italic>(<italic toggle="yes">sa</italic>). At the beginning of each simulation, <italic toggle="yes">Q</italic> was initialized to <bold>0.</bold> The algorithm also stored every experienced sample (<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">s</italic>’). After performing action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic>, experiencing reward r, and transitioning to state <italic toggle="yes">s</italic>’ the sample (<italic toggle="yes">s</italic>’, <italic toggle="yes">a</italic>’, <italic toggle="yes">r</italic>, <italic toggle="yes">s</italic>’) was stored and Q was updated according to the Q-learning prediction error:
<disp-formula id="ueqn5" hwp:id="disp-formula-23">
<alternatives hwp:id="alternatives-23"><graphic xlink:href="083857_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-29"/></alternatives>
</disp-formula></p><p hwp:id="p-121">Following each step, as with SR-Dya, we also selected 10 one-step samples (according to recency weighted probabilities with replacement) from the stored history, and replayed each to update <italic toggle="yes">Q</italic> according to equation the update above. Following transitions in which a learned change occurred to either the reward function or available actions, <italic toggle="yes">k</italic> one-step samples were selected and used to update the model, where k was set to 10 in the insufficient replay condition and to 10000 in the sufficient replay condition. Samples were drawn the same way as in SR-Dyna.</p></sec><sec id="s6c6" hwp:id="sec-44"><title hwp:id="title-53">Parameters</title><p hwp:id="p-122">All algorithms converted Q-values to actions using an <italic toggle="yes">∈</italic>-greedy policy which selects the highest-valued action with probability 1 – <italic toggle="yes">∈</italic>, and chooses randomly with probability <italic toggle="yes">∈</italic>. Parameters for all models and each simulation were varied and we observed that the qualitative results can be observed under a wide range of parameter settings (Supplementary Table 1). For the figures in the results section, the following parameters were used. For all models, <italic toggle="yes">∈</italic> was set to 0.1. In addition, all models used a discount parameter <italic toggle="yes">γ</italic> = 0.95. The three SR models used a weight learning rate parameter <italic toggle="yes">α</italic><sub><italic toggle="yes">w</italic></sub> = 0.3. Model Dyna-Q used a learning rate <italic toggle="yes">α</italic><sub><italic toggle="yes">Q</italic></sub>= 0.3. In addition to these parameters, SR-TD and SR-Dyna used a successor-matrix learning rate of <italic toggle="yes">α</italic><sub><italic toggle="yes">sr</italic></sub>= 0.3 and SR-MB used a policy learning rate of <italic toggle="yes">α</italic><sub><italic toggle="yes">π</italic></sub>= 0.1.</p></sec></sec></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-54">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2 xref-ref-1-3 xref-ref-1-4 xref-ref-1-5 xref-ref-1-6"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niv Y"><surname>Niv</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-2">Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source hwp:id="source-1">Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1560</pub-id></citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Houk JC"><surname>Houk</surname> <given-names>JC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adams JL"><surname>Adams</surname> <given-names>JL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barto a C."><surname>Barto</surname> <given-names>a C.</given-names></string-name> <article-title hwp:id="article-title-3">A model of how the basal ganglia generates and uses neural signals that predict reinforcement</article-title>. <source hwp:id="source-2">Model Inf Process Basal Ganglia</source>. <year>1995</year>; <fpage>249</fpage>–<lpage>270</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Montague PR"><surname>Montague</surname> <given-names>PR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P"><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sejnowski TJ"><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name>. <article-title hwp:id="article-title-4">A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source hwp:id="source-3">J Neurosci</source>. <year>1996</year>;<volume>16</volume>: <fpage>1936</fpage>–<lpage>1947</lpage>. doi:<pub-id pub-id-type="doi">10.1.1.156.635</pub-id></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Frank MJ"><surname>Frank</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seeberger LC"><surname>Seeberger</surname> <given-names>LC</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Reilly RC"><surname>O’Reilly</surname> <given-names>RC</given-names></string-name>. <article-title hwp:id="article-title-5">By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism</article-title>. <source hwp:id="source-4">Science (80-</source>). <year>2004</year>;<volume>306</volume>: <fpage>1940</fpage>–<lpage>1943</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1102941</pub-id></citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Yin HH"><surname>Yin</surname> <given-names>HH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ostlund SB"><surname>Ostlund</surname> <given-names>SB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knowlton BJ"><surname>Knowlton</surname> <given-names>BJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>. <article-title hwp:id="article-title-6">The role of the dorsomedial striatum in instrumental conditioning</article-title>. <source hwp:id="source-5">Eur J Neurosci</source>. <year>2005</year>;<volume>22</volume>: <fpage>513</fpage>–<lpage>23</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1460- 9568.2005.04218.x</pub-id></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2 xref-ref-6-3 xref-ref-6-4"><label>6.</label><citation publication-type="other" citation-type="journal" ref:id="083857v3.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour B"><surname>Seymour</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P"><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Raymond J."><surname>Raymond</surname> <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-7">Model-based influences on humans’ choices and striatal prediction errors</article-title>. <year>2011</year>;<volume>69</volume>: <fpage>1204</fpage>–<lpage>1215</lpage>. doi: 10.1016/j.neuron.2011.02.027.Model-based</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Wunderlich K"><surname>Wunderlich</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smittenaar P"><surname>Smittenaar</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name>. <article-title hwp:id="article-title-8">Dopamine Enhances Model-Based over ModelFree Choice Behavior</article-title>. <source hwp:id="source-6">Neuron</source>. <year>2012</year>;<volume>75</volume>: <fpage>418</fpage>–<lpage>424</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.042</pub-id></citation></ref><ref id="c8" hwp:id="ref-8"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Doll BB"><surname>Doll</surname> <given-names>BB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bath KG"><surname>Bath</surname> <given-names>KG</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frank MJ"><surname>Frank</surname> <given-names>MJ</given-names></string-name>. <article-title hwp:id="article-title-9">Variability in Dopamine Genes Dissociates Model-Based and Model-Free Reinforcement Learning</article-title>. <source hwp:id="source-7">J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>1211</fpage>–<lpage>1222</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUR0SCI.1901-15.2016</pub-id></citation></ref><ref id="c9" hwp:id="ref-9"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Deserno L"><surname>Deserno</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huys QJM"><surname>Huys</surname> <given-names>QJM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boehme R"><surname>Boehme</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Buchert R"><surname>Buchert</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heinze H-J"><surname>Heinze</surname> <given-names>H-J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grace AA"><surname>Grace</surname> <given-names>AA</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-10">Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making</article-title>. <source hwp:id="source-8">Proc Natl Acad Sci U S A. 2015</source>; <volume>112</volume>: <fpage>1595</fpage>–<lpage>600</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1417219112</pub-id></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Sharp ME"><surname>Sharp</surname> <given-names>ME</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foerde K"><surname>Foerde</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname> <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-11">Dopamine selectively remediates “model-based” reward learning: A computational approach</article-title>. <source hwp:id="source-9">Brain</source>. <year>2015</year>; <volume>139</volume>: <fpage>355</fpage>–<lpage>364</lpage>. doi:<pub-id pub-id-type="doi">10.1093/brain/awv347</pub-id></citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Sadacca BF"><surname>Sadacca</surname> <given-names>BF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones JL"><surname>Jones</surname> <given-names>JL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schoenbaum G."><surname>Schoenbaum</surname> <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-12">Midbrain dopamine neurons compute inferred and cached value prediction errors in a common framework</article-title>. <source hwp:id="source-10">Elife</source>. <year>2016</year>;<volume>5</volume>: <fpage>1</fpage>–<lpage>13</lpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.13665</pub-id></citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Glascher J"><surname>Glascher</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N"><surname>Daw</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P"><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="versus O’Doherty JP. States"><given-names>O’Doherty JP. States</given-names> <surname>versus</surname></string-name> rewards: <string-name name-style="western" hwp:sortable="model-based Dissociable neural prediction error signals underlying"><given-names>Dissociable neural prediction error signals underlying</given-names> <surname>model-based</surname></string-name> and <string-name name-style="western" hwp:sortable="learning model-free reinforcement"><given-names>model-free reinforcement</given-names> <surname>learning</surname></string-name>. <source hwp:id="source-11">Neuron. Elsevier Ltd</source>; <year>2010</year>;<volume>66</volume>: <fpage>585</fpage>–<lpage>595</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.016</pub-id></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Doherty JP"><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>. <article-title hwp:id="article-title-13">Multiple Forms of Value Learning and the Function of Dopamine BT - Neuroeconomics: Decision Making and the Brain</article-title>. <source hwp:id="source-12">Neuroeconomics Decision Making and the Brain</source>. <year>2008</year>. pp. <fpage>367</fpage>–<lpage>387</lpage>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://books.google.com/books?hl=en&amp;lr=&amp;id=g0QPLzBXDEMC&amp;oi=fnd&amp;pg=PA367&amp;dq=balleine+neuroeconomics&amp;ots=i9afuLQDYl&amp;sig=usxp3lfOydDCxVhoXJXa_IFCPLU" ext-link-type="uri" xlink:href="http://books.google.com/books?hl=en&amp;lr=&amp;id=g0QPLzBXDEMC&amp;oi=fnd&amp;pg=PA367&amp;dq=balleine+neuroeconomics&amp;ots=i9afuLQDYl&amp;sig=usxp3lfOydDCxVhoXJXa_IFCPLU" hwp:id="ext-link-1">http://books.google.com/books?hl=en&amp;lr=&amp;id=g0QPLzBXDEMC&amp;oi=fnd&amp;pg=PA367&amp;dq=balleine+neuroeconomics&amp;ots=i9afuLQDYl&amp;sig=usxp3lfOydDCxVhoXJXa_IFCPLU</ext-link></citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-14">The algorithmic anatomy of model-based evaluation</article-title>. <source hwp:id="source-13">Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130478-.</fpage> doi:<pub-id pub-id-type="doi">10.1098/rstb.2013.0478</pub-id></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2 xref-ref-15-3"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-15">Improving Generalisation for Temporal Difference Learning: The Successor Representation</article-title>. <source hwp:id="source-14">Neural Comput</source>. <year>1993</year>;<volume>5</volume>: <fpage>613</fpage>–<lpage>624</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-16">Motivated Reinforcement Learning</article-title>. <source hwp:id="source-15">Adv Neural Inf Process Syst</source>. <year>2002</year>;</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Sutton RS"><surname>Sutton</surname> <given-names>RS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pinette B."><surname>Pinette</surname> <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-17">The learning of world models by connectionist networks</article-title>. <source hwp:id="source-16">Proceedings of the Seventh Annual Conference of the Cognitive Science Society</source>. <year>1985</year>. pp. <fpage>54</fpage>–<lpage>64</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2 xref-ref-18-3"><label>18.</label><citation publication-type="website" citation-type="web" ref:id="083857v3.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Stachenfeld KL"><surname>Stachenfeld</surname> <given-names>KL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>. <article-title hwp:id="article-title-18">Design Principles of the Hippocampal Cognitive Map. Adv Neural Inf Process Syst 27. 2014</article-title>; <fpage>1</fpage>–<lpage>9</lpage>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://web.mit.edu/sjgershm/www/Stachenfeld14.pdf%5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map" ext-link-type="uri" xlink:href="http://web.mit.edu/sjgershm/www/Stachenfeld14.pdf%5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map" hwp:id="ext-link-2">http://web.mit.edu/sjgershm/www/Stachenfeld14.pdf%5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map</ext-link></citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moore CD"><surname>Moore</surname> <given-names>CD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Todd MT"><surname>Todd</surname> <given-names>MT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norman K a."><surname>Norman</surname> <given-names>K a.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sederberg PB"><surname>Sederberg</surname> <given-names>PB</given-names></string-name>. <article-title hwp:id="article-title-19">The Successor Representation and Temporal Context</article-title>. <source hwp:id="source-17">Neural Comput</source>. <year>2012</year>;<volume>24</volume>: <fpage>1553</fpage>–<lpage>1568</lpage>. doi: 10.1162/NECO_a_00282</citation></ref><ref id="c20" hwp:id="ref-20"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Suri RE"><surname>Suri</surname> <given-names>RE</given-names></string-name>. <article-title hwp:id="article-title-20">Anticipatory responses of dopamine neurons and cortical neurons reproduced by internal model</article-title>. <source hwp:id="source-18">Exp Brain Res</source>. <year>2001</year>;<volume>140</volume>: <fpage>234</fpage>–<lpage>240</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s002210100814</pub-id></citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3"><label>21.</label><citation publication-type="other" citation-type="journal" ref:id="083857v3.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Barreto A"><surname>Barreto</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Munos R"><surname>Munos</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schaul T"><surname>Schaul</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Silver D."><surname>Silver</surname> <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-21">Successor Features for Transfer in Reinforcement Learning</article-title>. arXiv Prepr. <year>2016</year>;1606.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2 xref-ref-22-3 xref-ref-22-4"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Tolman EC"><surname>Tolman</surname> <given-names>EC</given-names></string-name>. <article-title hwp:id="article-title-22">Cognitive maps in rats and men</article-title>. <source hwp:id="source-19">Psychol Rev</source>. <year>1948</year>;<volume>55</volume>: <fpage>189</fpage>–<lpage>208</lpage>. doi: 10.1037/h0061626</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2 xref-ref-23-3"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Simon DA"><surname>Simon</surname> <given-names>DA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title hwp:id="article-title-23">Neural correlates of forward planning in a spatial decision task in humans</article-title>. <source hwp:id="source-20">J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>5526</fpage>–<lpage>5539</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4647-10.2011</pub-id></citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2"><label>24.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Sutton RS"><surname>Sutton</surname> <given-names>RS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barto AG"><surname>Barto</surname> <given-names>AG</given-names></string-name>. <chapter-title>Reinforcement learning</chapter-title>: <source hwp:id="source-21">an introduction</source>. <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>25.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tobler PN"><surname>Tobler</surname> <given-names>PN</given-names></string-name>. <chapter-title>Value Learning through Reinforcement</chapter-title>. In: <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Glimcher PW"><surname>Glimcher</surname> <given-names>PW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fehr E"><surname>Fehr</surname> <given-names>E</given-names></string-name></person-group>, editors. <source hwp:id="source-22">Neuroeconomics</source>. <edition>2nd ed</edition>. <publisher-name>London: Elsevier</publisher-name>; <year>2014</year>. pp. <fpage>283</fpage>–<lpage>298</lpage>. doi:<pub-id pub-id-type="doi">10.1016/B978-0-12-416008-8.00015-2</pub-id></citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Sutton RS"><surname>Sutton</surname> <given-names>RS</given-names></string-name>. <article-title hwp:id="article-title-24">Dyna, an integrated architecture for learning, planning, and reacting</article-title>. <source hwp:id="source-23">ACM SIGART Bull</source>. <year>1991</year>;<volume>2</volume>: <fpage>160</fpage>–<lpage>163</lpage>. doi:<pub-id pub-id-type="doi">10.1145/122344.122377</pub-id></citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2 xref-ref-27-3 xref-ref-27-4 xref-ref-27-5 xref-ref-27-6 xref-ref-27-7"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Markman AB"><surname>Markman</surname> <given-names>AB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Otto AR"><surname>Otto</surname> <given-names>AR</given-names></string-name>. <article-title hwp:id="article-title-25">Retrospective revaluation in sequential decision making: a tale of two systems</article-title>. <source hwp:id="source-24">J Exp Psychol Gen</source>. <year>2014</year>;<volume>143</volume>: <fpage>182</fpage>–<lpage>94</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0030844</pub-id></citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Samejima K"><surname>Samejima</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ueda Y"><surname>Ueda</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doya K"><surname>Doya</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kimura M."><surname>Kimura</surname> <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-26">Representation of Action-Specific Reward Values in the Striatum</article-title>. <source hwp:id="source-25">Science (80-</source>). <year>2005</year>;<fpage>310</fpage>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://science.sciencemag.org/content/310/5752/1337" ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/310/5752/1337" hwp:id="ext-link-3">http://science.sciencemag.org/content/310/5752/1337</ext-link></citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Lau B"><surname>Lau</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glimcher PW"><surname>Glimcher</surname> <given-names>PW</given-names></string-name>. <article-title hwp:id="article-title-27">Value Representations in the Primate Striatum during Matching Behavior</article-title>. <source hwp:id="source-26">Neuron</source>. <year>2008</year>;<volume>58</volume>: <fpage>451</fpage>–<lpage>463</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.021</pub-id></citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.30" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Glimcher PW"><surname>Glimcher</surname> <given-names>PW</given-names></string-name>. <article-title hwp:id="article-title-28">Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</article-title>. <source hwp:id="source-27">Proc Natl Acad Sci U S A. 2011</source>; <volume>108 Suppl</volume>: <fpage>15647</fpage>–<lpage>54</lpage>. doi: 10.1073/pnas.1014269108</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2 xref-ref-31-3"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Doherty JP"><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>. <article-title hwp:id="article-title-29">Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action</article-title>. <source hwp:id="source-28">Neuropsychopharmacology</source>. <year>2010</year>;<volume>35</volume>: <fpage>48</fpage>–<lpage>69</lpage>. doi:<pub-id pub-id-type="doi">10.1038/npp.2009.131</pub-id></citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>32.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Thorndike EL"><surname>Thorndike</surname> <given-names>EL</given-names></string-name>, <chapter-title>Jelliffe. Animal Intelligence. Experimental Studies</chapter-title>. <source hwp:id="source-29">The Journal of Nervous and Mental Disease</source>. <publisher-name>Transaction Publishers</publisher-name>; <year>1912</year>. p. <fpage>357</fpage>. doi:<pub-id pub-id-type="doi">10.1097/00005053-191205000-00016</pub-id></citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Camerer C"><surname>Camerer</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ho T-H."><surname>Ho</surname> <given-names>T-H.</given-names></string-name> <article-title hwp:id="article-title-30">Experience-Weighted Atttraction in Normal Form Games</article-title>. <source hwp:id="source-30">Econometrica</source>. <year>1999</year>;<volume>67</volume>: <fpage>827</fpage>–<lpage>874</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Dickinson A"><surname>Dickinson</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>. <article-title hwp:id="article-title-31">The role of learning in the operation of motivational systems</article-title>. In: <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Gallistel CR"><surname>Gallistel</surname> <given-names>CR</given-names></string-name></person-group>, editor. <source hwp:id="source-31">Steven’s handbook of experimental psychology: Learning, motivation and emotion. New York: John Wiley &amp; Sons</source>; <year>2002</year>. pp. <fpage>497</fpage>–<lpage>534</lpage>. doi:<pub-id pub-id-type="doi">10.1002/0471214426.pas0312</pub-id></citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2"><label>35.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Wimmer GE"><surname>Wimmer</surname> <given-names>GE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname> <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-32">Preference by association: how memory mechanisms in the hippocampus bias decisions</article-title>. <source hwp:id="source-32">Science (80-). American Association for the Advancement of Science</source>; <year>2012</year>;<volume>338</volume>: <fpage>270</fpage>–<lpage>3</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1223252</pub-id></citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Dickinson A."><surname>Dickinson</surname> <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-33">Actions and Habits: The Development of Behavioural Autonomy [Internet]</article-title>. <source hwp:id="source-33">Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>1985</year>. pp. <fpage>67</fpage>–<lpage>78</lpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.1985.0010</pub-id></citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Dickinson A"><surname>Dickinson</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine B."><surname>Balleine</surname> <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-34">Motivational control of goal-directed action</article-title>. <source hwp:id="source-34">Anim Learn Behav</source>. <year>1994</year>;<volume>22</volume>: <fpage>1</fpage>–<lpage>18</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03199951</pub-id></citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2 xref-ref-38-3"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Yin HH"><surname>Yin</surname> <given-names>HH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knowlton BJ"><surname>Knowlton</surname> <given-names>BJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>. <article-title hwp:id="article-title-35">Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning</article-title>. <source hwp:id="source-35">Eur J Neurosci</source>. <year>2004</year>;<volume>19</volume>: <fpage>181</fpage>–<lpage>189</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1460-9568.2004.03095.x</pub-id></citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1 xref-ref-39-2 xref-ref-39-3 xref-ref-39-4"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Keramati M"><surname>Keramati</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dezfouli A"><surname>Dezfouli</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Piray P."><surname>Piray</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-36">Speed/accuracy trade-off between the habitual and the goal-directed processes</article-title>. <source hwp:id="source-36">PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1002055</pub-id></citation></ref><ref id="c40" hwp:id="ref-40"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Pezzulo G"><surname>Pezzulo</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rigoli F"><surname>Rigoli</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chersi F."><surname>Chersi</surname> <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-37">The mixed instrumental controller: Using value of information to combine habitual choice and mental simulation</article-title>. <source hwp:id="source-37">Front Psychol</source>. <year>2013</year>;<volume>4</volume>: <fpage>1</fpage> doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2013.00092</pub-id></citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Solway A"><surname>Solway</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>. <article-title hwp:id="article-title-38">Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates</article-title>. <source hwp:id="source-38">Psychol Rev. NIH Public Access</source>; <year>2012</year>;<volume>119</volume>: <fpage>120</fpage>–<lpage>54</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0026435</pub-id></citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Schultz W"><surname>Schultz</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P"><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Montague PR"><surname>Montague</surname> <given-names>PR</given-names></string-name>. <article-title hwp:id="article-title-39">A neural substrate of prediction and reward</article-title>. <source hwp:id="source-39">Science (80-</source>). <year>1997</year>;<volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id></citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Doherty JP"><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>. <article-title hwp:id="article-title-40">Neuroeconomics</article-title>. <source hwp:id="source-40">Neuroeconomics. 1st</source> ed. <year>2009</year>. pp. <fpage>367</fpage>–<lpage>387</lpage>. doi:<pub-id pub-id-type="doi">10.1016/B978-0-12-374176-9.00024-5</pub-id></citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Tanaka SC"><surname>Tanaka</surname> <given-names>SC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doya K"><surname>Doya</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Okada G"><surname>Okada</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ueda K"><surname>Ueda</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Okamoto Y"><surname>Okamoto</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamawaki S."><surname>Yamawaki</surname> <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-41">Prediction of Immediate and Future Rewards Differentially Recruits Cortico-Basal Ganglia Loops</article-title>. <source hwp:id="source-41">Nature Neuroscience. Tokyo</source>; <year>2004</year>. pp. <fpage>887</fpage>–<lpage>893</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-4-431-55402-8_22</pub-id></citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Alexander GE"><surname>Alexander</surname> <given-names>GE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crutchner MD"><surname>Crutchner</surname> <given-names>MD</given-names></string-name>. <article-title hwp:id="article-title-42">Functional architecture of basal ganglia circuits: neural substrated of parallel processing</article-title>. <source hwp:id="source-42">Trends Neurosci</source>. <year>1990</year>; <volume>13</volume>: <fpage>266</fpage>–<lpage>271</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0166- 2236(90)90107-L</pub-id></citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2 xref-ref-46-3"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Dezfouli A"><surname>Dezfouli</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>. <article-title hwp:id="article-title-43">Habits, action sequences and reinforcement learning</article-title>. <source hwp:id="source-43">Eur J Neurosci</source>. <year>2012</year>;<volume>35</volume>: <fpage>1036</fpage>–<lpage>1051</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08050.x</pub-id></citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Haber SN"><surname>Haber</surname> <given-names>SN</given-names></string-name>. <article-title hwp:id="article-title-44">The primate basal ganglia: Parallel and integrative networks</article-title>. <source hwp:id="source-44">J Chem Neuroanat</source>. <year>2003</year>;<volume>26</volume>: <fpage>317</fpage>–<lpage>330</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jchemneu.2003.10.003</pub-id></citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Faure A"><surname>Faure</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haberland U"><surname>Haberland</surname> <given-names>U</given-names></string-name>, <string-name name-style="western" hwp:sortable="El Massioui N"><given-names>Massioui N</given-names> <surname>El</surname></string-name>. <article-title hwp:id="article-title-45">Lesion to the Nigrostriatal Dopamine System Disrupts Stimulus - Response Habit Formation</article-title>. <source hwp:id="source-45">J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>2771</fpage>–<lpage>2780</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUR0SCI.3894-04.2005</pub-id></citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2"><label>49.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname> <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-46">What are the Computations of the Cerebellum, the Basal Gangila, and the Cerebral Cortex?</article-title> <source hwp:id="source-46">Sci Technol</source>. <year>1999</year>;<volume>12</volume>: <fpage>1</fpage>–<lpage>48</lpage>. doi:<pub-id pub-id-type="doi">10.1016/s0893-6080(99)00046-5</pub-id></citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1 xref-ref-50-2"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Huys QJM"><surname>Huys</surname> <given-names>QJM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lally N"><surname>Lally</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Faulkner P"><surname>Faulkner</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eshel N"><surname>Eshel</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seifritz E"><surname>Seifritz</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-47">Interplay of approximate planning strategies</article-title>. <source hwp:id="source-47">Proc Natl Acad Sci U S A</source>. <year>2015</year>; <volume>112</volume>: <fpage>3098</fpage>–<lpage>103</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1414219112</pub-id></citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>51.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="van der Meer MAA"><surname>van der Meer</surname> <given-names>MAA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Redish AD"><surname>Redish</surname> <given-names>AD</given-names></string-name>. <article-title hwp:id="article-title-48">Expectancies in decision making, reinforcement learning, and ventral striatum</article-title>. <source hwp:id="source-48">Front Neurosci. Frontiers</source>; <year>2010</year>;<volume>3</volume>: <fpage>6</fpage>. doi:<pub-id pub-id-type="doi">10.3389/neuro.01.006.2010</pub-id></citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.52" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Ludvig EA"><surname>Ludvig</surname> <given-names>EA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mirian MS"><surname>Mirian</surname> <given-names>MS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kehoe EJ"><surname>Kehoe</surname> <given-names>EJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutton RS"><surname>Sutton</surname> <given-names>RS</given-names></string-name>. <article-title hwp:id="article-title-49">Associative learning from replayed experience</article-title>. <source hwp:id="source-49">bioRxiv</source>. <year>2017; doi</year>:<ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/100800" ext-link-type="uri" xlink:href="https://doi.org/10.1101/100800" hwp:id="ext-link-4">https://doi.org/10.1101/100800</ext-link></citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Rao RP"><surname>Rao</surname> <given-names>RP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sejnowski TJ"><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name>. <article-title hwp:id="article-title-50">Spike-timing-dependent Hebbian plasticity as temporal difference learning</article-title>. <source hwp:id="source-50">Neural Comput</source>. <year>2001</year>;<volume>13</volume>: <fpage>2221</fpage>–<lpage>2237</lpage>. doi:<pub-id pub-id-type="doi">10.1162/089976601750541787</pub-id></citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="other" citation-type="journal" ref:id="083857v3.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Gehring CA"><surname>Gehring</surname> <given-names>CA</given-names></string-name>. <article-title hwp:id="article-title-51">Approximate Linear Successor Representation. Reinforcement Learning Decision Making</article-title>. <year>2015</year>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf" ext-link-type="uri" xlink:href="http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf" hwp:id="ext-link-5">http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf</ext-link></citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2"><label>55.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.55" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Tolman EC"><surname>Tolman</surname> <given-names>EC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Honzik CH"><surname>Honzik</surname> <given-names>CH</given-names></string-name>. <chapter-title>Introduction and removal of reward, and maze performance in rats</chapter-title>. <publisher-name>Univ Calif Publ Psychol</publisher-name>. <year>1930</year>;</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>56.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.56" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Jang J"><surname>Jang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee S"><surname>Lee</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shin S."><surname>Shin</surname> <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-52">An optimization network for matrix inversion</article-title>. <source hwp:id="source-51">Neural Inf Process Syst</source>. <year>1988</year>; <fpage>397</fpage>–<lpage>401</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><label>57.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.57" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Momennejad I"><surname>Momennejad</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Russek EM"><surname>Russek</surname> <given-names>EM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cheong JH"><surname>Cheong</surname> <given-names>JH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N"><surname>Daw</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>. <article-title hwp:id="article-title-53">The successor representation in human reinforcement learning</article-title>. <source hwp:id="source-52">bioRxiv</source>. <year>2016; doi</year>:<ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/083824" ext-link-type="uri" xlink:href="https://doi.org/10.1101/083824" hwp:id="ext-link-6">https://doi.org/10.1101/083824</ext-link></citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>58.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Wang T"><surname>Wang</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bowlingm M"><surname>Bowlingm</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schuurmans D."><surname>Schuurmans</surname> <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-54">Dual representations for dynamic programming and reinforcement learning</article-title>. <collab hwp:id="collab-1">Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning</collab>, <source hwp:id="source-53">ADPRL</source> <year>2007</year>. 2007. pp. <fpage>44</fpage>–<lpage>51</lpage>. doi:<pub-id pub-id-type="doi">10.1109/ADPRL.2007.368168</pub-id></citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>59.</label><citation publication-type="website" citation-type="web" ref:id="083857v3.59" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="White LM"><surname>White</surname> <given-names>LM</given-names></string-name>. <article-title hwp:id="article-title-55">Temporal Difference Learning: Eligibility Traces and the Successor Representation for Actions [Internet]. University of Toronto</article-title>. <year>1995</year>. Available <ext-link l:rel="related" l:ref-type="uri" l:ref="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.4525&amp;rep=rep1&amp;t" ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.4525&amp;rep=rep1&amp;t" hwp:id="ext-link-7">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.4525&amp;rep=rep1&amp;t</ext-link> ype=pdf</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>60.</label><citation publication-type="other" citation-type="journal" ref:id="083857v3.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Blundell C"><surname>Blundell</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uria B"><surname>Uria</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pritzel A"><surname>Pritzel</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Y"><surname>Li</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ruderman A"><surname>Ruderman</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leibo JZ"><surname>Leibo</surname> <given-names>JZ</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-56">Model-Free Episodic Control</article-title>. arXiv:<pub-id pub-id-type="arxiv">160604460v1</pub-id> [statML]. <year>2016</year>; <fpage>1</fpage>–<lpage>12</lpage>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><label>61.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.61" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Wilson M"><surname>Wilson</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="McNaughton B."><surname>McNaughton</surname> <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-57">Reactivation of hippocampal ensemble memories during sleep</article-title>. <source hwp:id="source-54">Science (80-</source>). <year>1994</year>;<fpage>265</fpage>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><label>62.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Kudrimoti HS"><surname>Kudrimoti</surname> <given-names>HS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barnes CA"><surname>Barnes</surname> <given-names>CA</given-names></string-name>, <string-name name-style="western" hwp:sortable="McNaughton BL"><surname>McNaughton</surname> <given-names>BL</given-names></string-name>. <article-title hwp:id="article-title-58">Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics</article-title>. <source hwp:id="source-55">J Neurosci</source>. <year>1999</year>;<volume>19</volume>: <fpage>4090</fpage>–<lpage>101</lpage>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.ncbi.nlm.nih.gov/pubmed/10234037" ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10234037" hwp:id="ext-link-8">http://www.ncbi.nlm.nih.gov/pubmed/10234037</ext-link></citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><label>63.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="McClelland JL"><surname>McClelland</surname> <given-names>JL</given-names></string-name>, <string-name name-style="western" hwp:sortable="McNaughton BL"><surname>McNaughton</surname> <given-names>BL</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Reilly RC"><surname>O’Reilly</surname> <given-names>RC</given-names></string-name>. <article-title hwp:id="article-title-59">Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title>. <source hwp:id="source-56">Psychol Rev</source>. <year>1995</year>;<volume>102</volume>: <fpage>419</fpage>–<lpage>457</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id></citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><label>64.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Buzsaki G."><surname>Buzsaki</surname> <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-60">Two-stage model of memory trace formation: A role for “noisy” brain states</article-title>. <source hwp:id="source-57">Neuroscience</source>. <year>1989</year>;<volume>31</volume>: <fpage>551</fpage>–<lpage>570</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0306-4522(89)90423-5</pub-id></citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><label>65.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Moore AW"><surname>Moore</surname> <given-names>AW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Atkeson CG"><surname>Atkeson</surname> <given-names>CG</given-names></string-name>. <article-title hwp:id="article-title-61">Prioritized Sweeping: Reinforcement Learning with Less Data and Less Time</article-title>. <source hwp:id="source-58">Mach Learn</source>. <year>1993</year>;<volume>13</volume>: <fpage>103</fpage>–<lpage>130</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1022635613229</pub-id></citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><label>66.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.66" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Set E"><surname>Set</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Saez I"><surname>Saez</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhu L"><surname>Zhu</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Houser DE"><surname>Houser</surname> <given-names>DE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Myung N"><surname>Myung</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhong S"><surname>Zhong</surname> <given-names>S</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-62">Dissociable contribution of prefrontal and</article-title> <source hwp:id="source-59">striatal dopaminergic genes to learning in economic games</source>. doi: 10.1073/pnas.1316259111</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><label>67.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Durstewitz D"><surname>Durstewitz</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seamans JK"><surname>Seamans</surname> <given-names>JK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sejnowski TJ"><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name>. <article-title hwp:id="article-title-63">Neurocomputational models of working memory</article-title>. <source hwp:id="source-60">Nat Neurosci. Nature Publishing Group</source>; <year>2000</year>;<volume>3</volume>: <fpage>1184</fpage>–<lpage>1191</lpage>. doi:<pub-id pub-id-type="doi">10.1038/81460</pub-id></citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><label>68.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Niv Y"><surname>Niv</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Joel D"><surname>Joel</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-64">Tonic dopamine: Opportunity costs and the control of response vigor</article-title>. <source hwp:id="source-61">Psychopharmacology (Berl)</source>. <year>2007</year>;<volume>191</volume>: <fpage>507</fpage>–<lpage>520</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s00213-006-0502-4</pub-id></citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1"><label>69.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Boureau YL"><surname>Boureau</surname> <given-names>YL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sokol-Hessner P"><surname>Sokol-Hessner</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title hwp:id="article-title-65">Deciding How To Decide: Self-Control and Meta-Decision Making</article-title>. <source hwp:id="source-62">Trends Cogn Sci. Elsevier Ltd</source>; <year>2015</year>;<volume>19</volume>: <fpage>700</fpage>–<lpage>710</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2015.08.013</pub-id></citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><label>70.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Keramati M"><surname>Keramati</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smittenaar P"><surname>Smittenaar</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-66">Adaptive integration of habits into depth-limited planning defines a habitual-goal-directed spectrum</article-title>. <source hwp:id="source-63">Proc Natl Acad Sci U S A. National Academy of Sciences</source>; <year>2016</year>;<volume>113</volume>: <fpage>12868</fpage>–<lpage>12873</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1609094113</pub-id></citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><label>71.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Hiroyuki N."><surname>Hiroyuki</surname> <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-67">Multiplexing signals in reinforcement learning with internal models and dopamine</article-title>. <source hwp:id="source-64">Curr Opin Neurobiol. Elsevier Ltd</source>; <year>2014</year>;<volume>25</volume>: <fpage>123</fpage>–<lpage>129</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2014.01.001</pub-id></citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><label>72.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Akam T"><surname>Akam</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Costa R"><surname>Costa</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-68">Simple Plans or Sophisticated Habits? State, Transition and Learning Interactions in the Two-Step Task</article-title>. <source hwp:id="source-65">PLoS Comput Biol</source>. <year>2015</year>; <volume>11</volume>: <fpage>1</fpage>–<lpage>25</lpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1004648</pub-id></citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><label>73.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Sutton RS"><surname>Sutton</surname> <given-names>RS</given-names></string-name>. <article-title hwp:id="article-title-69">TD Models: Modeling the world at a mixture of time scales</article-title>. <source hwp:id="source-66">Proceedings of the 12th Int Conf on Machine Learning</source>. <year>1995</year>. doi:<pub-id pub-id-type="doi">10.1017/CBO9781107415324.004</pub-id></citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><label>74.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dezfouli A"><surname>Dezfouli</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ito M"><surname>Ito</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname> <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-70">Hierarchical control of goal-directed action in the cortical-basal ganglia network</article-title>. <source hwp:id="source-67">Curr Opin Behav Sci. Elsevier Ltd</source>; <year>2015</year>;<volume>5</volume>: <fpage>1</fpage>–<lpage>7</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cobeha.2015.06.001</pub-id></citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><label>75.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Barron HC"><surname>Barron</surname> <given-names>HC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Behrens TEJ"><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>. <article-title hwp:id="article-title-71">Online evaluation of novel choices by simultaneous representation of multiple memories</article-title>. <source hwp:id="source-68">Nat Neurosci. Nature Publishing Group; 2013</source>; <volume>16</volume>: <fpage>1492</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3515</pub-id></citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><label>76.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Papale AE"><surname>Papale</surname> <given-names>AE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zielinski MC"><surname>Zielinski</surname> <given-names>MC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frank LM"><surname>Frank</surname> <given-names>LM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jadhav SP"><surname>Jadhav</surname> <given-names>SP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Redish AD"><surname>Redish</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Papale AE"><surname>Papale</surname> <given-names>AE</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-72">Interplay between Hippocampal Sharp-Wave-Ripple Events and Vicarious Trial and Error Behaviors in Report Interplay between Hippocampal Sharp-Wave-Ripple Events and Vicarious Trial and Error Behaviors in Decision Making</article-title>. <source hwp:id="source-69">Neuron. Elsevier Inc</source>.; <year>2016</year>;<volume>92</volume>: <fpage>975</fpage>–<lpage>982</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.028</pub-id></citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1"><label>77.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Lee SW"><surname>Lee</surname> <given-names>SW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shimojo S"><surname>Shimojo</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Doherty JP"><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>. <source hwp:id="source-70">Neural Computations Underlying Arbitration between Model-Based and Model-free Learning. Neuron. Elsevier Inc</source>.; <year>2014</year>;<volume>81</volume>: <fpage>687</fpage>–<lpage>699</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.028</pub-id></citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><label>78.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.78" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Gupta AS"><surname>Gupta</surname> <given-names>AS</given-names></string-name>, <string-name name-style="western" hwp:sortable="van der Meer MAA"><surname>van der Meer</surname> <given-names>MAA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Touretzky DS"><surname>Touretzky</surname> <given-names>DS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Redish AD"><surname>Redish</surname> <given-names>AD</given-names></string-name>. <article-title hwp:id="article-title-73">Hippocampal Replay Is Not a Simple Function of Experience</article-title>. <source hwp:id="source-71">Neuron</source>. <year>2010</year>;<volume>65</volume>: <fpage>695</fpage>–<lpage>705</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id></citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><label>79.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.79" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Tolman Ciancia F."><given-names>Ciancia F.</given-names> <surname>Tolman</surname></string-name> and Honzik (<year>1930</year>) <article-title hwp:id="article-title-74">revisited: or The mazes of psychology (1930-1980</article-title>). <source hwp:id="source-72">Psychol Rec</source>. 1991;<volume>41</volume>: <fpage>461</fpage>–<lpage>472</lpage>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tolman+and+honzik+(1930)+revisited+or+the+mazes+of+psychology+(1930-1980)#0" ext-link-type="uri" xlink:href="http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tolman+and+honzik+(1930)+revisited+or+the+mazes+of+psychology+(1930-1980)#0" hwp:id="ext-link-9">http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tolman+and+honzik+(1930)+revisited+or+the+mazes+of+psychology+(1930-1980)#0</ext-link></citation></ref><ref id="c80" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1"><label>80.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.80" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="Poucet B"><surname>Poucet</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thinus-Blanc C"><surname>Thinus-Blanc</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chapuis N."><surname>Chapuis</surname> <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-75">Route planning in cats, in relation to the visibility of the goal</article-title>. <source hwp:id="source-73">Anim Behav</source>. <year>1983</year>;<volume>31</volume>: <fpage>594</fpage>–<lpage>599</lpage>. doi:<pub-id pub-id-type="doi">10.1016/s0003-3472(83)80083-9</pub-id></citation></ref><ref id="c81" hwp:id="ref-81"><label>81.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.81" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Winocur G"><surname>Winocur</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moscovitch M"><surname>Moscovitch</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rosenbaum RS"><surname>Rosenbaum</surname> <given-names>RS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sekeres M."><surname>Sekeres</surname> <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-76">An investigation of the effects of hippocampal lesions in rats on pre- and postoperatively acquired spatial memory in a complex environment</article-title>. <source hwp:id="source-74">Hippocampus</source>. <year>2010</year>;<volume>20</volume>: <fpage>1350</fpage>–<lpage>1365</lpage>. doi:<pub-id pub-id-type="doi">10.1002/hipo.20721</pub-id></citation></ref><ref id="c82" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1"><label>82.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.82" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Jovalekic A"><surname>Jovalekic</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hayman R"><surname>Hayman</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Becares N"><surname>Becares</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reid H"><surname>Reid</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thomas G"><surname>Thomas</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilson J"><surname>Wilson</surname> <given-names>J</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-77">Horizontal biases in rats’ use of three-dimensional space</article-title>. <source hwp:id="source-75">Behav Brain Res</source>. <year>2011</year>;<volume>222</volume>: <fpage>279</fpage>–<lpage>288</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.bbr.2011.02.035</pub-id></citation></ref><ref id="c83" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1"><label>83.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.83" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Chapuis N"><surname>Chapuis</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Durup M"><surname>Durup</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thinus-Blanc C."><surname>Thinus-Blanc</surname> <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-78">The role of exploratory experience in a shortcut task by golden hamsters (&lt;i&gt;Mesocricetus auratus&lt;/i&gt;</article-title>). <source hwp:id="source-76">Learn Behav. 1987</source>; <volume>15</volume>: <fpage>174</fpage>–<lpage>178</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03204960</pub-id></citation></ref><ref id="c84" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1"><label>84.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Alvernhe A"><surname>Alvernhe</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Van Cauter T"><surname>Van Cauter</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Save E"><surname>Save</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poucet B."><surname>Poucet</surname> <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-79">Different CA1 and CA3 representations of novel routes in a shortcut situation</article-title>. <source hwp:id="source-77">J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>7324</fpage>–<lpage>33</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUR0SCI.1909-08.2008</pub-id></citation></ref><ref id="c85" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1"><label>85.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.85" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Spiers HJ"><surname>Spiers</surname> <given-names>HJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gilbert SJ"><surname>Gilbert</surname> <given-names>SJ</given-names></string-name>. <article-title hwp:id="article-title-80">Solving the detour problem in navigation: a model of prefrontal and hippocampal interactions</article-title>. <source hwp:id="source-78">Front Hum Neurosci</source>. <year>2015</year>;<volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2015.00125</pub-id></citation></ref><ref id="c86" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1"><label>86.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.86" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Corbit LH"><surname>Corbit</surname> <given-names>LH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ostlund SB"><surname>Ostlund</surname> <given-names>SB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balleine BW"><surname>Balleine</surname> <given-names>BW</given-names></string-name>. <article-title hwp:id="article-title-81">Sensitivity to instrumental contingency degradation is mediated by the entorhinal cortex and its efferents via the dorsal hippocampus</article-title>. <source hwp:id="source-79">J Neurosci</source>. <year>2002</year>;<volume>22</volume>: <fpage>10976</fpage>–<lpage>84</lpage>. doi:<pub-id pub-id-type="doi">22/24/10976 [pii]</pub-id></citation></ref><ref id="c87" hwp:id="ref-87" hwp:rev-id="xref-ref-87-1"><label>87.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.87" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-87"><string-name name-style="western" hwp:sortable="Girardeau G"><surname>Girardeau</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benchenane K"><surname>Benchenane</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wiener SI"><surname>Wiener</surname> <given-names>SI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Buzsaki G"><surname>Buzsaki</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zugaro MB"><surname>Zugaro</surname> <given-names>MB</given-names></string-name>. <article-title hwp:id="article-title-82">Selective suppression of hippocampal ripples impairs spatial memory</article-title>. <source hwp:id="source-80">Nat Neurosci. Nature Publishing Group</source>; <year>2009</year>;<volume>12</volume>: <fpage>1222</fpage>–<lpage>1223</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.2384</pub-id></citation></ref><ref id="c88" hwp:id="ref-88" hwp:rev-id="xref-ref-88-1"><label>88.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.88" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-88"><string-name name-style="western" hwp:sortable="Ego-Stengel V"><surname>Ego-Stengel</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilson MA"><surname>Wilson</surname> <given-names>MA</given-names></string-name>. <article-title hwp:id="article-title-83">Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat</article-title>. <source hwp:id="source-81">Hippocampus</source>. <year>2010</year>;<volume>20</volume>: <fpage>1</fpage>–<lpage>10</lpage>. doi:<pub-id pub-id-type="doi">10.1002/hipo.20707</pub-id></citation></ref><ref id="c89" hwp:id="ref-89" hwp:rev-id="xref-ref-89-1"><label>89.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.89" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-89"><string-name name-style="western" hwp:sortable="Jadhav SP"><surname>Jadhav</surname> <given-names>SP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kemere C"><surname>Kemere</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="German PW"><surname>German</surname> <given-names>PW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frank LM"><surname>Frank</surname> <given-names>LM</given-names></string-name>. <article-title hwp:id="article-title-84">Awake Hippocampal Sharp-Wave Ripples Support Spatial Memory</article-title>. <source hwp:id="source-82">Science (80-)</source>. <year>2012</year>;<volume>336</volume>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://science.sciencemag.org/content/336/6087/1454.long" ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/336/6087/1454.long" hwp:id="ext-link-10">http://science.sciencemag.org/content/336/6087/1454.long</ext-link></citation></ref><ref id="c90" hwp:id="ref-90" hwp:rev-id="xref-ref-90-1"><label>90.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.90" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-90"><string-name name-style="western" hwp:sortable="Khamassi M"><surname>Khamassi</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Humphries MD"><surname>Humphries</surname> <given-names>MD</given-names></string-name>. <article-title hwp:id="article-title-85">Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies</article-title>. <source hwp:id="source-83">Front Behav Neurosci</source>. <year>2012</year>;<volume>6</volume>: <fpage>1</fpage>–<lpage>19</lpage>. doi:<pub-id pub-id-type="doi">10.3389/fnbeh.2012.00079</pub-id></citation></ref><ref id="c91" hwp:id="ref-91" hwp:rev-id="xref-ref-91-1 xref-ref-91-2"><label>91.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.91" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-91"><string-name name-style="western" hwp:sortable="Wilson RC"><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Takahashi YK"><surname>Takahashi</surname> <given-names>YK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schoenbaum G"><surname>Schoenbaum</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niv Y."><surname>Niv</surname> <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-86">Orbitofrontal Cortex as a Cognitive Map of Task Space</article-title>. <source hwp:id="source-84">Neuron</source>. <year>2014</year>;<volume>81</volume>: <fpage>267</fpage>–<lpage>279</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.005</pub-id></citation></ref><ref id="c92" hwp:id="ref-92"><label>92.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.92" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-92"><string-name name-style="western" hwp:sortable="Nomoto K"><surname>Nomoto</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schultz W"><surname>Schultz</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Watanabe T"><surname>Watanabe</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sakagami M."><surname>Sakagami</surname> <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-87">Temporally extended dopamine responses to perceptually demanding reward-predictive stimuli</article-title>. <source hwp:id="source-85">J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>10692</fpage>–<lpage>10702</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4828-09.2010</pub-id></citation></ref><ref id="c93" hwp:id="ref-93" hwp:rev-id="xref-ref-93-1"><label>93.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.93" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-93"><string-name name-style="western" hwp:sortable="Dayan P"><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title hwp:id="article-title-88">Decision theory, reinforcement learning, and the brain</article-title>. <source hwp:id="source-86">Cogn Affect Behav Neurosci</source>. <year>2008</year>;<volume>8</volume>: <fpage>429</fpage>–<lpage>453</lpage>. doi:<pub-id pub-id-type="doi">10.3758/CABN.8.4.429</pub-id></citation></ref><ref id="c94" hwp:id="ref-94" hwp:rev-id="xref-ref-94-1"><label>94.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.94" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-94"><string-name name-style="western" hwp:sortable="Littman ML"><surname>Littman</surname> <given-names>ML</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutton RS"><surname>Sutton</surname> <given-names>RS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Singh S."><surname>Singh</surname> <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-89">Predictive Representations of State</article-title>. <source hwp:id="source-87">Neural Inf Process Syst</source>. <year>2001</year>;<volume>14</volume>: <fpage>1555</fpage>–<lpage>1561</lpage>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.2493&amp;rep=rep1&amp;type=pdf" ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.2493&amp;rep=rep1&amp;type=pdf" hwp:id="ext-link-11">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.2493&amp;rep=rep1&amp;type=pdf</ext-link></citation></ref><ref id="c95" hwp:id="ref-95" hwp:rev-id="xref-ref-95-1"><label>95.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.95" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-95"><string-name name-style="western" hwp:sortable="Schlegel M"><surname>Schlegel</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="White A"><surname>White</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="White M."><surname>White</surname> <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-90">Stable predictive representations with general value functions for continual learning</article-title>. <source hwp:id="source-88">Continual Learning and Deep Networks workshop at the Neural Information Processing System Conference</source>. <year>2017</year>. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://sites.ualberta.ca/∼amw8/cldl.pdf" ext-link-type="uri" xlink:href="https://sites.ualberta.ca/∼amw8/cldl.pdf" hwp:id="ext-link-12">https://sites.ualberta.ca/∼amw8/cldl.pdf</ext-link></citation></ref><ref id="c96" hwp:id="ref-96" hwp:rev-id="xref-ref-96-1"><label>96.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.96" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-96"><string-name name-style="western" hwp:sortable="Fermin ASR"><surname>Fermin</surname> <given-names>ASR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yoshida T"><surname>Yoshida</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yoshimoto J"><surname>Yoshimoto</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ito M"><surname>Ito</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tanaka SC"><surname>Tanaka</surname> <given-names>SC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doya K."><surname>Doya</surname> <given-names>K.</given-names></string-name> <chapter-title>Model-based action planning involves cortico-cerebellar and basal ganglia networks</chapter-title>. <source hwp:id="source-89">Sci Rep</source>. <publisher-name>Nature Publishing Group</publisher-name>; <year>2016</year>;<volume>6</volume>: <fpage>31378</fpage>. doi:<pub-id pub-id-type="doi">10.1038/srep31378</pub-id></citation></ref><ref id="c97" hwp:id="ref-97" hwp:rev-id="xref-ref-97-1"><label>97.</label><citation publication-type="other" citation-type="journal" ref:id="083857v3.97" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-97"><string-name name-style="western" hwp:sortable="Stachenfeld KL"><surname>Stachenfeld</surname> <given-names>KL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman SJ"><surname>Gershman</surname> <given-names>SJ</given-names></string-name>. <article-title hwp:id="article-title-91">The hippocampus as a predictive map</article-title>. <year>2016</year>;</citation></ref><ref id="c98" hwp:id="ref-98" hwp:rev-id="xref-ref-98-1"><label>98.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.98" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-98"><string-name name-style="western" hwp:sortable="Schapiro AC"><surname>Schapiro</surname> <given-names>AC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rogers TT"><surname>Rogers</surname> <given-names>TT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cordova NI"><surname>Cordova</surname> <given-names>NI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Turk-Browne NB"><surname>Turk-Browne</surname> <given-names>NB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>. <article-title hwp:id="article-title-92">Neural representations of events arise from temporal community structure</article-title>. <source hwp:id="source-90">Nat Neurosci. Nature Research</source>; <year>2013</year>;<volume>16</volume>: <fpage>486</fpage>–<lpage>92</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3331</pub-id></citation></ref><ref id="c99" hwp:id="ref-99" hwp:rev-id="xref-ref-99-1"><label>99.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.99" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-99"><string-name name-style="western" hwp:sortable="Garvert MM"><surname>Garvert</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Behrens TE"><surname>Behrens</surname> <given-names>TE</given-names></string-name>. <article-title hwp:id="article-title-93">A map of abstract relational knowledge in the human hippocampal-entorhinal cortex</article-title>. <source hwp:id="source-91">Elife</source>. <year>2017</year>;<volume>6</volume>: <fpage>1</fpage>–<lpage>18</lpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.17086</pub-id></citation></ref><ref id="c100" hwp:id="ref-100" hwp:rev-id="xref-ref-100-1"><label>100.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.100" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-100"><string-name name-style="western" hwp:sortable="O’Keefe J"><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nadel L."><surname>Nadel</surname> <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-94">The hippocampus as a cognitive map [Internet]</article-title>. <source hwp:id="source-92">Clarendon Press</source>; <year>1978. Available</year>: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://arizona.openrepository.com/arizona/handle/10150/620894" ext-link-type="uri" xlink:href="http://arizona.openrepository.com/arizona/handle/10150/620894" hwp:id="ext-link-13">http://arizona.openrepository.com/arizona/handle/10150/620894</ext-link></citation></ref><ref id="c101" hwp:id="ref-101" hwp:rev-id="xref-ref-101-1"><label>101.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.101" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-101"><string-name name-style="western" hwp:sortable="Gaussier P"><surname>Gaussier</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Revel A"><surname>Revel</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Banquet JP"><surname>Banquet</surname> <given-names>JP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Babeau V."><surname>Babeau</surname> <given-names>V.</given-names></string-name> <article-title hwp:id="article-title-95">From view cells and place cells to cognitive map learning: processing stages of the hippocampal system</article-title>. <source hwp:id="source-93">Biol Cybern. SpringerVerlag</source>; <year>2002</year>;<volume>86</volume>: <fpage>15</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s004220100269</pub-id></citation></ref><ref id="c102" hwp:id="ref-102" hwp:rev-id="xref-ref-102-1"><label>102.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.102" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-102"><string-name name-style="western" hwp:sortable="Gustafson NJ"><surname>Gustafson</surname> <given-names>NJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name name-style="western" hwp:sortable="Neymotin S"><surname>Neymotin</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Olypher A"><surname>Olypher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vayntrub Y."><surname>Vayntrub</surname> <given-names>Y.</given-names></string-name> <chapter-title>Grid Cells, Place Cells, and Geodesic Generalization for Spatial Reinforcement Learning. Kording KP, editor</chapter-title>. <source hwp:id="source-94">PLoS Comput Biol</source>. <publisher-name>The MIT Press</publisher-name>; <year>2011</year>;<volume>7</volume>: <fpage>e1002235</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1002235</pub-id></citation></ref><ref id="c103" hwp:id="ref-103" hwp:rev-id="xref-ref-103-1"><label>103.</label><citation publication-type="book" citation-type="book" ref:id="083857v3.103" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-103"><string-name name-style="western" hwp:sortable="Wikenheiser AM"><surname>Wikenheiser</surname> <given-names>AM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schoenbaum G."><surname>Schoenbaum</surname> <given-names>G.</given-names></string-name> <chapter-title>Over the river, through the woods: cognitive maps in the hippocampus and orbitofrontal cortex</chapter-title>. <source hwp:id="source-95">Nat Rev Neurosci</source>. <publisher-name>Nature Research</publisher-name>; <year>2016</year>;<volume>17</volume>: <fpage>513</fpage>–<lpage>523</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn.2016.56</pub-id></citation></ref><ref id="c104" hwp:id="ref-104" hwp:rev-id="xref-ref-104-1"><label>104.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.104" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-104"><string-name name-style="western" hwp:sortable="Schuck NW"><surname>Schuck</surname> <given-names>NW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cai B"><surname>Cai</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilson RC"><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Correspondence YN"><surname>Correspondence</surname> <given-names>YN</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cai MB"><surname>Cai</surname> <given-names>MB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niv Y."><surname>Niv</surname> <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-96">Human Orbitofrontal Cortex Represents a Cognitive Map of State Space</article-title>. <source hwp:id="source-96">Neuron. Elsevier Inc</source>; <year>2016</year>;<volume>91</volume>: <fpage>1402</fpage>–<lpage>1412</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.019</pub-id></citation></ref><ref id="c105" hwp:id="ref-105" hwp:rev-id="xref-ref-105-1"><label>105.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.105" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-105"><string-name name-style="western" hwp:sortable="Momennejad I"><surname>Momennejad</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haynes J-D."><surname>Haynes</surname> <given-names>J-D.</given-names></string-name> <article-title hwp:id="article-title-97">Human anterior prefrontal cortex encodes the “what” and “when” of future intentions</article-title>. <source hwp:id="source-97">Neuroimage</source>. <year>2012</year>;<volume>61</volume>: <fpage>139</fpage>–<lpage>148</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.079</pub-id></citation></ref><ref id="c106" hwp:id="ref-106" hwp:rev-id="xref-ref-106-1"><label>106.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.106" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-106"><string-name name-style="western" hwp:sortable="Momennejad I"><surname>Momennejad</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haynes J-D."><surname>Haynes</surname> <given-names>J-D.</given-names></string-name> <article-title hwp:id="article-title-98">Encoding of Prospective Tasks in the Human Prefrontal Cortex under Varying Task Loads</article-title>. <source hwp:id="source-98">J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>17342</fpage>–<lpage>17349</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.0492-13.2013</pub-id></citation></ref><ref id="c107" hwp:id="ref-107" hwp:rev-id="xref-ref-107-1"><label>107.</label><citation publication-type="other" citation-type="journal" ref:id="083857v3.107" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-107"><string-name name-style="western" hwp:sortable="Miller EK"><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cohen JD"><surname>Cohen</surname> <given-names>JD</given-names></string-name>. <article-title hwp:id="article-title-99">A N I NTEGRATIVE T HEORY OF P REFRONTAL C ORTEX F UNCTION</article-title>. <year>2001</year>; <fpage>167</fpage>–<lpage>202</lpage>.</citation></ref><ref id="c108" hwp:id="ref-108" hwp:rev-id="xref-ref-108-1"><label>108.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.108" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-108"><string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niv Y"><surname>Niv</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barto AC"><surname>Barto</surname> <given-names>AC</given-names></string-name>. <article-title hwp:id="article-title-100">Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective</article-title>. <source hwp:id="source-99">Cognition</source>. <year>2009</year>;<volume>113</volume>: <fpage>262</fpage>–<lpage>280</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cognition.2008.08.011</pub-id></citation></ref><ref id="c109" hwp:id="ref-109" hwp:rev-id="xref-ref-109-1"><label>109.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.109" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-109"><string-name name-style="western" hwp:sortable="Botvinick M"><surname>Botvinick</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weinstein A."><surname>Weinstein</surname> <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-101">Model-based hierarchical reinforcement learning and human action control</article-title>. <source hwp:id="source-100">Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130480-</fpage>. doi: 10.1098/rstb.2013.0480</citation></ref><ref id="c110" hwp:id="ref-110" hwp:rev-id="xref-ref-110-1"><label>110.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.110" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-110"><string-name name-style="western" hwp:sortable="Schapiro AC"><surname>Schapiro</surname> <given-names>AC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rogers TT"><surname>Rogers</surname> <given-names>TT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cordova NI"><surname>Cordova</surname> <given-names>NI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Turk-Browne NB"><surname>Turk-Browne</surname> <given-names>NB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Botvinick MM"><surname>Botvinick</surname> <given-names>MM</given-names></string-name>. <article-title hwp:id="article-title-102">Neural representations of events arise from temporal community structure</article-title>. <source hwp:id="source-101">Nat Publ Gr</source>. <year>2013</year>; <volume>16</volume>. doi:<pub-id pub-id-type="doi">10.1038/nn.3331</pub-id></citation></ref><ref id="c111" hwp:id="ref-111" hwp:rev-id="xref-ref-111-1"><label>111.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.111" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-111"><string-name name-style="western" hwp:sortable="Boorman ED"><surname>Boorman</surname> <given-names>ED</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rajendran VG"><surname>Rajendran</surname> <given-names>VG</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Reilly JX"><surname>O’Reilly</surname> <given-names>JX</given-names></string-name>, <string-name name-style="western" hwp:sortable="Behrens TE"><surname>Behrens</surname> <given-names>TE</given-names></string-name>. <article-title hwp:id="article-title-103">Two Anatomically and Computationally Distinct Learning Signals Predict Changes to Stimulus-Outcome Associations in Hippocampus</article-title>. <source hwp:id="source-102">Neuron. The Authors</source>; <year>2016</year>;<volume>89</volume>: <fpage>1343</fpage>–<lpage>1354</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.014</pub-id></citation></ref><ref id="c112" hwp:id="ref-112" hwp:rev-id="xref-ref-112-1"><label>112.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.112" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-112"><string-name name-style="western" hwp:sortable="Doll BB"><surname>Doll</surname> <given-names>BB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duncan KD"><surname>Duncan</surname> <given-names>KD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon DA"><surname>Simon</surname> <given-names>DA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D"><surname>Shohamy</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw ND"><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title hwp:id="article-title-104">Model-based choices involve prospective neural activity</article-title>. <source hwp:id="source-103">Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>767</fpage>–<lpage>772</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3981</pub-id></citation></ref><ref id="c113" hwp:id="ref-113" hwp:rev-id="xref-ref-113-1"><label>113.</label><citation publication-type="journal" citation-type="journal" ref:id="083857v3.113" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-113"><string-name name-style="western" hwp:sortable="Parker NF"><surname>Parker</surname> <given-names>NF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cameron CM"><surname>Cameron</surname> <given-names>CM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Taliaferro JP"><surname>Taliaferro</surname> <given-names>JP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee J"><surname>Lee</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Choi JY"><surname>Choi</surname> <given-names>JY</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davidson TJ"><surname>Davidson</surname> <given-names>TJ</given-names></string-name>, <etal>et al</etal>. <article-title hwp:id="article-title-105">Reward and choice encoding in terminals of midbrain dopamine neurons depends on striatal target</article-title>. <source hwp:id="source-104">Nat Neurosci</source>. <year>2016</year>;<fpage>19</fpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.4287</pub-id></citation></ref></ref-list><sec sec-type="supplementary-material" id="s7" hwp:id="sec-45"><title hwp:id="title-55">Supplementary Materials</title><fig id="figS1" position="float" fig-type="figure" orientation="portrait" hwp:id="F7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Supplementary Figure 1:</label><caption hwp:id="caption-7"><p hwp:id="p-123">Advantage of TD learning over direct reward learning of weights. a) Task environment. On each trial, the agent was placed in state S. Trials ended when the agent reached state R, which contained a reward value of 10. Unlike the latent learning task in the main text, in this task did not contain an exploratory period enabling the agent to learn the successor matrix prior to introduction of reward. b) Number of steps on each trial for agent learning weights using TD learning and agent learning weights using reward learning. Plotted lines show average over 500 runs. 95% confidence intervals are contained within line thickness. Parameters for each of the two algorithms were set to those that minimized the average number of total steps over 80 trials. Such parameters were found by grid search over <italic toggle="yes">α</italic><sub><italic toggle="yes">sr</italic></sub> ∈ [. 1,.3,.5,.<italic toggle="yes">7</italic>,.9], <italic toggle="yes">∈</italic> ∈ [0.1, 0.3, 0.5] and <italic toggle="yes">α</italic><sub><italic toggle="yes">w</italic></sub> ∈.[1,.3,.5,.<italic toggle="yes">7</italic>,.9]. Both algorithms learned the SR using the SR-TD update. The “TD Learning” algorithm updated weights using TD learning. The “Reward Learning” algorithm updated weights by delta-rule learning on the immediate reward function. Specifically, after performing action <italic toggle="yes">a</italic> in state <italic toggle="yes">s</italic> and receiving reward <italic toggle="yes">r</italic>, the following update was performed: <italic toggle="yes">w</italic>(<italic toggle="yes">s</italic>)← <italic toggle="yes">w</italic>(<italic toggle="yes">s</italic>)+ <italic toggle="yes">α</italic><sub><italic toggle="yes">w</italic></sub>(<italic toggle="yes">r</italic> – <italic toggle="yes">w</italic>(<italic toggle="yes">s</italic>)).</p></caption><graphic xlink:href="083857_figS1" position="float" orientation="portrait" hwp:id="graphic-30"/></fig><fig id="figS2" position="float" fig-type="figure" orientation="portrait" hwp:id="F8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Supplementary Figure 2:</label><caption hwp:id="caption-8"><p hwp:id="p-124">Preventing replay slows acquisition for both a) SR-Dyna and b) Dyna Q. Both algorithms under the two sampling settings were simulated on the task displayed in Supplementary <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Figure 1</xref>. Both a) and b) show number of steps on each trial for agent permitted to replay 20 samples between each decision and an agent not permitted to replay any samples. Plotted lines show average over 500 runs. 95% confidence intervals are contained within shaded region around lines. For each algorithm and sample setting, we chose parameters by a grid search in the following range: <italic toggle="yes">α</italic><sub><italic toggle="yes">sr</italic></sub> ∈ [. 1, .3, .5, .7, .9], ∈ ∈ [0.1, 0.3, 0.5], <italic toggle="yes">α</italic><sub><italic toggle="yes">w</italic></sub> ∈ [. 1, .3, .5, .7, .9], and <italic toggle="yes">α</italic><sub><italic toggle="yes">Q</italic></sub> ∈ [. 1, .3, .5, .7, .9].</p></caption><graphic xlink:href="083857_figS2" position="float" orientation="portrait" hwp:id="graphic-31"/></fig><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;083857v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-9"><title hwp:id="title-56">Robustness of simulation results to varying parameters</title><p hwp:id="p-125">Here, we display the results of simulating each task, using each algorithm under a wide variety of parameter settings. Each table below corresponds to a particular algorithm simulating a particular task. For a given parameter setting, the algorithm was simulated 500 times. A check indicates that the 500 run median value function produced by that parameter setting results in the optimal policy for the task. A cross indicates that it does not result in the optimal policy.</p><p hwp:id="p-126"><bold>Latent learning task</bold></p></caption><graphic xlink:href="083857_tbl1" position="float" orientation="portrait" hwp:id="graphic-32"/><graphic xlink:href="083857_tbl1a" position="float" orientation="portrait" hwp:id="graphic-33"/><graphic xlink:href="083857_tbl1b" position="float" orientation="portrait" hwp:id="graphic-34"/><graphic xlink:href="083857_tbl1c" position="float" orientation="portrait" hwp:id="graphic-35"/><graphic xlink:href="083857_tbl1d" position="float" orientation="portrait" hwp:id="graphic-36"/><graphic xlink:href="083857_tbl1e" position="float" orientation="portrait" hwp:id="graphic-37"/><graphic xlink:href="083857_tbl1f" position="float" orientation="portrait" hwp:id="graphic-38"/><graphic xlink:href="083857_tbl1g" position="float" orientation="portrait" hwp:id="graphic-39"/><graphic xlink:href="083857_tbl1h" position="float" orientation="portrait" hwp:id="graphic-40"/><graphic xlink:href="083857_tbl1i" position="float" orientation="portrait" hwp:id="graphic-41"/><graphic xlink:href="083857_tbl1j" position="float" orientation="portrait" hwp:id="graphic-42"/><graphic xlink:href="083857_tbl1k" position="float" orientation="portrait" hwp:id="graphic-43"/><graphic xlink:href="083857_tbl1l" position="float" orientation="portrait" hwp:id="graphic-44"/><graphic xlink:href="083857_tbl1m" position="float" orientation="portrait" hwp:id="graphic-45"/><graphic xlink:href="083857_tbl1n" position="float" orientation="portrait" hwp:id="graphic-46"/><graphic xlink:href="083857_tbl1o" position="float" orientation="portrait" hwp:id="graphic-47"/><graphic xlink:href="083857_tbl1p" position="float" orientation="portrait" hwp:id="graphic-48"/><graphic xlink:href="083857_tbl1q" position="float" orientation="portrait" hwp:id="graphic-49"/><graphic xlink:href="083857_tbl1r" position="float" orientation="portrait" hwp:id="graphic-50"/><graphic xlink:href="083857_tbl1s" position="float" orientation="portrait" hwp:id="graphic-51"/><graphic xlink:href="083857_tbl1t" position="float" orientation="portrait" hwp:id="graphic-52"/></table-wrap></sec></back></article>
