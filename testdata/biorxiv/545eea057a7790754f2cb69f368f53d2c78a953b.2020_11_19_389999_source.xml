<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2020.11.19.389999</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2020.11.19.389999</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2020.11.19.389999</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2020.11.19.389999</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2020.11.19.389999</article-id><article-version>1.4</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Animal Behavior and Cognition" hwp:journal="biorxiv"><subject>Animal Behavior and Cognition</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Deep neural network models reveal interplay of peripheral coding and stimulus statistics in pitch perception</article-title></title-group><author-notes hwp:id="author-notes-1"><fn id="n1" fn-type="others" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>*</label><p hwp:id="p-1">co-first authors</p></fn><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>#</label>Corresponding author; email: <email hwp:id="email-1">msaddler@mit.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2698-7275</contrib-id><name name-style="western" hwp:sortable="Saddler Mark R."><surname>Saddler</surname><given-names>Mark R.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">*</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">#</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-2698-7275"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Gonzalez Ray"><surname>Gonzalez</surname><given-names>Ray</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3965-2503</contrib-id><name name-style="western" hwp:sortable="McDermott Josh H."><surname>McDermott</surname><given-names>Josh H.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-3" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3965-2503"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">Department of Brain and Cognitive Sciences, MIT</institution></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3"><label>2</label><institution hwp:id="institution-2">McGovern Institute for Brain Research, MIT</institution></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2 xref-aff-3-3"><label>3</label><institution hwp:id="institution-3">Center for Brains, Minds and Machines, MIT</institution></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Program in Speech and Hearing Biosciences and Technology, Harvard University</institution></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2020-11-20T16:45:20-08:00">
    <day>20</day><month>11</month><year>2020</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-10-25T17:27:12-07:00">
    <day>25</day><month>10</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2020-11-20T16:50:14-08:00">
    <day>20</day><month>11</month><year>2020</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-10-25T17:32:50-07:00">
    <day>25</day><month>10</month><year>2021</year>
  </pub-date><elocation-id>2020.11.19.389999</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2020-11-19"><day>19</day><month>11</month><year>2020</year></date>
<date date-type="rev-recd" hwp:start="2021-10-25"><day>25</day><month>10</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-10-25"><day>25</day><month>10</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="389999.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2020.11.19.389999v4.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="389999.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2020.11.19.389999v4/2020.11.19.389999v4.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2020.11.19.389999v4/2020.11.19.389999v4.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">ABSTRACT</title><p hwp:id="p-3">Perception is thought to be shaped by the environments for which organisms are optimized. These influences are difficult to test in biological organisms but may be revealed by machine perceptual systems optimized under different conditions. We investigated environmental and physiological influences on pitch perception, whose properties are commonly linked to peripheral neural coding limits. We first trained artificial neural networks to estimate fundamental frequency from biologically faithful cochlear representations of natural sounds. The best-performing networks replicated many characteristics of human pitch judgments. To probe the origins of these characteristics, we then optimized networks given altered cochleae or sound statistics. Human-like behavior emerged only when cochleae had high temporal fidelity and when models were optimized for naturalistic sounds. The results suggest pitch perception is critically shaped by the constraints of natural environments in addition to those of the cochlea, illustrating the use of artificial neural networks to reveal underpinnings of behavior.</p></abstract><counts><page-count count="73"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-4">The authors have declared no competing interest.</p></notes><fn-group content-type="external-links" hwp:id="fn-group-1"><fn fn-type="dataset" hwp:id="fn-2"><p hwp:id="p-5">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/msaddler/pitchnet" ext-link-type="uri" xlink:href="https://github.com/msaddler/pitchnet" hwp:id="ext-link-2">https://github.com/msaddler/pitchnet</ext-link>
</p></fn><fn fn-type="dataset" hwp:id="fn-3"><p hwp:id="p-6">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/msaddler/bez2018model" ext-link-type="uri" xlink:href="https://github.com/msaddler/bez2018model" hwp:id="ext-link-3">https://github.com/msaddler/bez2018model</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">INTRODUCTION</title><p hwp:id="p-7">A key goal of perceptual science is to understand why sensory-driven behavior takes the form that it does. In some cases, it is natural to relate behavior to physiology, and in particular to the constraints imposed by sensory transduction. For instance, color discrimination is limited by the number of cone types in the retina<sup><xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref></sup>. Olfactory discrimination is similarly constrained by the receptor classes in the nose<sup><xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref></sup>. In other cases, behavior can be related to properties of environmental stimulation that are largely divorced from the constraints of peripheral transduction. For example, face recognition in humans is much better for upright faces, presumably because we predominantly encounter upright faces in our environment<sup><xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref></sup>.</p><p hwp:id="p-8">Understanding how physiological and environmental factors shape behavior is important both for fundamental scientific understanding and for practical applications such as sensory prostheses, the engineering of which might benefit from knowing how sensory encoding constrains behavior. Yet the constraints on behavior are often difficult to pin down. For instance, the auditory periphery encodes sound with exquisite temporal fidelity<sup><xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref></sup>, but the role of this information in hearing remains controversial<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>–<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref></sup>. Part of the challenge is that the requisite experiments – altering sensory receptors or environmental conditions during evolution or development, for instance – are practically difficult (and ethically unacceptable in humans).</p><p hwp:id="p-9">The constraints on behavior can sometimes instead be revealed by computational models. Ideal observer models, which optimally perform perceptual tasks given particular sensory inputs and sensory receptor responses, have been the method of choice for investigating such constraints<sup><xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref></sup>. While biological perceptual systems likely never reach optimal performance, in some cases humans share behavioral characteristics of ideal observers, suggesting that those behaviors are consequences of having been optimized under particular biological or environmental constraints<sup><xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>–<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref></sup>. Ideal observers provide a powerful framework for normative analysis, but for many real-world tasks, deriving provably optimal solutions is analytically intractable. The relevant sensory transduction properties are often prohibitively complicated, and the taskrelevant parameters of natural stimuli and environments are difficult to specify mathematically. An attractive alternative might be to collect many real-world stimuli and optimize a model to perform the task on these stimuli. Even if not fully optimal, such models might reveal consequences of optimization under constraints that could provide insights into behavior.</p><p hwp:id="p-10">In this paper, we explore whether contemporary “deep” artificial neural networks (DNNs) can be used in this way to gain normative insights about complex perceptual tasks. DNNs provide general-purpose architectures that can be optimized to perform challenging real-world tasks<sup><xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref></sup>. While DNNs are unlikely to fully achieve optimal performance, they might reveal the effects of optimizing a system under particular constraints<sup><xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>,<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref></sup>. Previous work has documented similarities between human and network behavior for neural networks trained on vision or hearing tasks<sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>–<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref></sup>. However, we know little about the extent to which human-DNN similarities depend on either biological constraints that are built into the model architecture or the sensory signals for which the models are optimized. By manipulating the properties of simulated sensory transduction processes and the stimuli on which the DNN is trained, we hoped to get insight into the origins of behaviors of interest.</p><p hwp:id="p-11">Here, we test this approach in the domain of pitch – traditionally conceived as the perceptual correlate of a sound’s fundamental frequency (F0)<sup><xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref></sup>. Pitch is believed to enable a wide range of auditory-driven behaviors, such as voice and melody recognition<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref></sup>, and has been the subject of a long history of work in psychology<sup><xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>–<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref></sup> and neuroscience<sup><xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>–<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref></sup>. Yet despite a wealth of data, the underlying computations and constraints that determine pitch perception remain debated<sup><xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref></sup>. In particular, controversy persists over the role of spike timing in the auditory nerve, for which a physiological extraction mechanism has remained elusive<sup><xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>,<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref></sup>. The role of cochlear frequency selectivity, which has also been proposed to constrain pitch discrimination, remains similarly debated<sup><xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">26</xref>,<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref></sup>. By contrast, little attention has been given to the possibility that pitch perception might instead or additionally be shaped by the constraints of estimating the F0 of natural sounds in natural environments.</p><p hwp:id="p-12">One factor limiting resolution of these debates is that previous models of pitch have generally not attained quantitatively accurate matches to human behavior<sup><xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>–<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>,<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>,<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>–<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref></sup>. Moreover, because most previous models have been mechanistic rather than normative, they do not speak to the potential adaptation of pitch perception to particular types of sounds or peripheral neural codes. Here we used DNNs in the role traditionally occupied by ideal observers, optimizing them to extract pitch information from peripheral neural representations of natural sounds. DNNs have become the method of choice for pitch tracking in engineering applications<sup><xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref></sup>, but have not been combined with realistic models of the peripheral auditory system, and have not been compared to human perception. We then tested the influence of peripheral auditory physiology and natural sound statistics on human pitch perception by manipulating them during model optimization. The results provide new evidence for the importance of peripheral phase locking in human pitch perception. However, they also indicate that the properties of pitch perception reflect adaptation to natural sound statistics, in that systems optimized for alternative stimulus statistics deviate substantially from human-like behavior.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">RESULTS</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Training task and stimuli</title><p hwp:id="p-13">We used supervised deep learning to build a model of pitch perception optimized for natural speech and music. DNNs were trained to estimate the F0 of short (50ms) segments of speech and musical instrument recordings, selected to have high periodicity and well-defined F0s. To emulate natural listening conditions, the speech and music clips were embedded in aperiodic background noise taken from YouTube soundtracks. The networks’ task was to classify each stimulus into one of 700 F0 classes (log-spaced between 80 Hz and 1000 Hz, bin width = 1/16 semitones = 0.36%F0). We generated a dataset of 2.1 million stimuli. Networks were trained using 80% of this dataset and the remaining 20% was used as a validation set to measure the success of the optimization.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Peripheral auditory model</title><p hwp:id="p-14">In our primary training condition, we hard-coded the input representation for our networks to be as faithful as possible to known peripheral auditory physiology. We used a detailed phenomenological model of the auditory nerve<sup><xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref></sup> to simulate peripheral representations of each stimulus (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1A</xref>). The input representations to our networks consisted of 100 simulated auditory nerve fibers. Each stimulus was represented as a 100-fiber by 1000-timestep array of instantaneous firing rates (sampled at 20 kHz).</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-15">Pitch model overview. <bold>(A)</bold> Schematic of model structure. DNNs were trained to estimate the F0 of speech and music sounds embedded in real-world background noise. Networks received simulated auditory nerve representations of acoustic stimuli as input. Green outlines depict the extent of example convolutional filter kernels in time and frequency (horizontal and vertical dimensions, respectively). <bold>(B)</bold> Simulated auditory nerve representation of a harmonic tone with a fundamental frequency (F0) of 200 Hz. The sound waveform is shown above and its power spectrum to the left. The waveform is periodic in time, with a period of 5ms. The spectrum is harmonic (i.e., containing multiples of the fundamental frequency). Network inputs were arrays of instantaneous auditory nerve firing rates (depicted in greyscale, with lighter hues indicating higher firing rates). Each row plots the firing rate of a frequency-tuned auditory nerve fiber, arranged in order of their place along the cochlea (with low frequencies at the bottom). Individual fibers phase-lock to low-numbered harmonics in the stimulus (lower portion of the nerve representation), or to the combination of high-numbered harmonics (upper portion). Time-averaged responses on the right show the pattern of nerve fiber excitation across the cochlear frequency axis (the “excitation pattern”). Low-numbered harmonics produce distinct peaks in the excitation pattern. <bold>(C)</bold> Schematics of six example DNN architectures trained to estimate F0. Network architectures varied in the number of layers, the number of units per layer, the extent of pooling between layers, and the size and shape of convolutional filter kernels <bold>(D)</bold> Summary of network architecture search. F0 classification performance on the validation set (noisy speech and instrument stimuli not seen during training) is shown as a function of training steps for all 400 networks trained. The highlighted curves correspond to the architectures depicted in A and C. The relatively low overall accuracy reflects the fine-grained F0 bins we used. <bold>(E)</bold> Histogram of accuracy, expressed as the median F0 error on the validation set, for all trained networks (F0 error in percent is more interpretable than the classification accuracy, the absolute value of which is dependent on the width of the F0 bins). <bold>(F)</bold> Confusion matrix for the best-performing network (depicted in A) tested on the validation set.</p></caption><graphic xlink:href="389999v4_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-16">An example simulated auditory nerve representation for a harmonic tone is shown in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1B</xref>. Theories of pitch have tended to gravitate toward one of the two axes of such representations: the frequency-to-place mapping along the cochlea’s length, or the time axis. However, it is visually apparent that the nerve representation of even this relatively simple sound is quite rich, with a variety of potential cues: phase locking to individual frequencies, phase shifts between these phase-locked responses, peaks in the time-averaged response (the “excitation” pattern) for low-numbered harmonics, and phase locking to the F0 for the higher-numbered harmonics. The DNN models have access to all of this information. Through optimization for the training task, the DNNs should learn to use whichever peripheral cues best allow them to extract F0.</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-7">Neural network architecture search</title><p hwp:id="p-17">The performance of an artificial neural network is influenced both by the particular weights that are learned during training and by the various parameters that define the architecture of the network<sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref></sup>. To obtain a high-performing model, we performed a large-scale random architecture search. Each architecture consisted of a feedforward series of layers instantiating linear convolution, nonlinear rectification, normalization, and pooling operations. Within this family, we trained 400 networks varying in their number of layers, number of units per layer, extent of pooling between layers, and the size and shape of convolutional filters (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1C</xref>).</p><p hwp:id="p-18">The different architectures produced a broad distribution of training task performances (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1D</xref>). In absolute terms accuracy was good – the median error was well below 1% (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1E</xref>), which is on par with good human F0 discrimination thresholds<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-3" hwp:rel-id="ref-25">25</xref></sup>. The vast majority of misclassifications fell within bins neighboring the true F0 or at an integer number octaves away (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1F</xref>), as in human pitch-matching judgments<sup><xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref></sup>.</p></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-8">Characteristics of pitch perception emerge in optimized DNNs</title><p hwp:id="p-19">Having obtained a model that can estimate F0 from natural sounds, we simulated a suite of well-known psychophysical experiments to assess whether the model replicated known properties of human pitch perception. Each experiment measures the effect of particular cues on pitch discrimination or estimation using synthetic tones (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2</xref>, left column), and produces an established result in human listeners (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2</xref>, center column). We tested the effect of these stimulus manipulations on our 10 best-performing network architectures. Given evidence for individual differences across different networks optimized for the same task<sup><xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref></sup>, most figures feature results averaged across the 10 best networks identified in our architecture search (which we collectively refer to as “the model”). Averaging across an ensemble of networks effectively allows us to marginalize over architectural hyperparameters and provide uncertainty estimates for our model’s results<sup><xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>,<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref></sup>. Individual results for the 10 networks are shown in <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Supplementary Fig. 1</xref>.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12 xref-fig-2-13 xref-fig-2-14 xref-fig-2-15 xref-fig-2-16 xref-fig-2-17 xref-fig-2-18 xref-fig-2-19 xref-fig-2-20 xref-fig-2-21 xref-fig-2-22 xref-fig-2-23 xref-fig-2-24 xref-fig-2-25 xref-fig-2-26 xref-fig-2-27 xref-fig-2-28 xref-fig-2-29 xref-fig-2-30 xref-fig-2-31 xref-fig-2-32 xref-fig-2-33 xref-fig-2-34 xref-fig-2-35"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-20">Pitch model validation: human and neural network psychophysics. Five classic experiments from the pitch psychoacoustics literature (<bold>A-E</bold>) were simulated on neural networks trained to estimate the F0 of natural sounds. Each row corresponds to a different experiment and contains (from left to right) a schematic of the experimental stimuli, results from human listeners (re-plotted from the original studies), and results from networks. Error bars indicate bootstrapped 95% confidence intervals around the mean of the 10 best network architectures ranked by F0 estimation performance on natural sounds (individual network results are shown in <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-10-2" hwp:rel-id="F10">Supplementary Fig. 1</xref>). <bold>(A)</bold> F0 discrimination thresholds for bandpass synthetic tones, as a function of lowest harmonic number and phase. Human listeners and networks discriminated pairs of sine-phase or random-phase harmonic tones with similar F0s. Stimuli were bandpass-filtered to control which harmonics were audible. <bold>(B)</bold> Perceived pitch of alternating-phase complex tones containing either low or high-numbered harmonics. Alternating-phase tones (i.e., with odd-numbered harmonics in sine phase and even-numbered harmonics in cosine phase) contain twice as many peaks in the waveform envelope as sine-phase tones with the same F0. Human listeners adjusted a sine-phase tone to match the pitch of the alternating-phase tone. Networks made F0 estimates for the alternating-phase tones directly. Histograms show distributions of pitch judgments as the ratio between the reported F0 and the stimulus F0. <bold>(C)</bold> Pitch of frequency-shifted complexes. Harmonic complexes (containing either low or high-numbered harmonics) were made inharmonic by shifting all component frequencies by the same number of Hz. Human listeners and networks reported the F0s they perceived for these stimuli (same experimental methods as in B). Shifts in the perceived F0 are shown as a function of the shift applied to the component frequencies. <bold>(D)</bold> Pitch of complexes with individually mistuned harmonics. Human listeners and networks reported the F0s they perceived for complex tones in which a single harmonic frequency was shifted (same experimental methods as in B). Shifts in the perceived F0 are shown as a function of the mistuning applied to seven different harmonics within the tone (harmonic numbers indicated in different colors at top of graphs). Note that the y-axis limits are different in the human and model graphs – they exhibit qualitative but not quantitative similarity. This could be because the networks are better able to isolate the contribution of the harmonic to the F0, whereas human listeners may sometimes erroneously be biased by the harmonic itself. <bold>(E)</bold> Frequency discrimination thresholds measured with pure tones and transposed tones. Transposed tones are high-frequency tones that are amplitude-modulated so as to instantiate the temporal cues from low-frequency pure tones at a higher-frequency place on the cochlea. Human and network listeners discriminated pairs of pure tones with similar frequencies and pairs of transposed tones with similar envelope frequencies.</p></caption><graphic xlink:href="389999v4_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-21">As shown in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2</xref>, the model (right column) qualitatively and in most cases quantitatively replicates the result of each of the five different experiments in humans (center column). We emphasize that none of the stimuli were included in the networks’ training set, and that the model was not fit to match human results in any way. These results collectively suggest that the model relies on similar cues as the human pitch system. We describe these results in turn.</p></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-9">Dependence on low-numbered harmonics</title><p hwp:id="p-22">First, human pitch discrimination is more accurate for stimuli containing low-numbered harmonics (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2A</xref>, center, solid line)<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-2" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>,<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>,<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-4" hwp:rel-id="ref-25">25</xref></sup>. This finding is often interpreted as evidence for the importance of “place” cues to pitch, which are only present for low-numbered harmonics (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1B</xref>, right). The model reproduced this effect, though the inflection point was somewhat lower than in human listeners: discrimination thresholds were low only for stimuli containing the fifth or lower harmonic (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig. 2A</xref>, right, solid line).</p></sec><sec id="s2f" hwp:id="sec-8"><title hwp:id="title-10">Phase effects are limited to high-numbered harmonics</title><p hwp:id="p-23">Second, human perception is affected by harmonic phases only for high-numbered harmonics. When harmonic phases are randomized, human discrimination thresholds are elevated for stimuli that lack low-numbered harmonics (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig. 2A</xref>, center, dashed vs. solid line)<sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-5" hwp:rel-id="ref-25">25</xref></sup>. In addition, when odd and even harmonics are summed in sine and cosine phase, respectively (“alternating phase”, a manipulation that doubles the number of peaks in the waveform’s temporal envelope; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2B</xref>, left), listeners report the pitch to be twice as high as the corresponding sine-phase complex, but only for high-numbered harmonics (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Fig. 2B</xref>, center)<sup><xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref></sup>. These results are typically thought to indicate use of temporal fluctuations in a sound’s envelope when cues for low-numbered harmonics are not available<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-3" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-3" hwp:rel-id="ref-22">22</xref>,<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-3" hwp:rel-id="ref-26">26</xref></sup>. The model replicates both effects (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2A&amp;B</xref>, right), indicating that it uses similar temporal cues to pitch as humans, and in similar conditions.</p></sec><sec id="s2g" hwp:id="sec-9"><title hwp:id="title-11">Pitch shifts for shifted low-numbered harmonics</title><p hwp:id="p-24">Third, frequency-shifted complex tones (in which all of the component frequencies have been shifted by the same number of Hz; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2C</xref>, left) produce linear shifts in the pitch reported by humans, but only if the tones contain low-numbered harmonics (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-11" hwp:rel-id="F2">Fig. 2C</xref>, center)<sup><xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref></sup>. The model’s F0 predictions for these stimuli resemble those measured from human listeners (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-12" hwp:rel-id="F2">Fig. 2C</xref>, right).</p><p hwp:id="p-25">Fourth, shifting individual harmonics in a complex tone (“mistuning”; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-13" hwp:rel-id="F2">Fig. 2D</xref>, left) can also produce pitch shifts in humans under certain conditions<sup><xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref></sup>: the mistuning must be small (effects are largest for 3-4% mistunings) and applied to a low-numbered harmonic (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-14" hwp:rel-id="F2">Fig. 2D</xref>, center). The model replicates this effect as well, although the size of the shift is smaller than that observed in humans (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-15" hwp:rel-id="F2">Fig. 2D</xref>, right).</p></sec><sec id="s2h" hwp:id="sec-10"><title hwp:id="title-12">Poor discrimination of transposed tones</title><p hwp:id="p-26">Fifth, “transposed tones” designed to instantiate the temporal cues from low frequencies at a higher-frequency place on the cochlea (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-16" hwp:rel-id="F2">Fig. 2E</xref>, left) elicit weak pitch percepts in humans and thus yield higher discrimination thresholds than pure tones (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-17" hwp:rel-id="F2">Fig. 2E</xref>, center)<sup><xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref></sup>. This finding is taken to indicate that to the extent that temporal cues to pitch matter perceptually, they must occur at the correct place on the cochlea. The model reproduced this effect: discrimination thresholds were worse for transposed tones than they are for pure tones (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-18" hwp:rel-id="F2">Fig. 2E</xref>, right).</p></sec><sec id="s2i" hwp:id="sec-11"><title hwp:id="title-13">DNNs with better F0 estimation show more human-like behavior</title><p hwp:id="p-27">To evaluate whether the human-model similarity evident in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-19" hwp:rel-id="F2">Fig. 2</xref> depends on having optimized the model architecture for F0 estimation of natural sounds, we simulated the full suite of psychophysical experiments on each of our 400 trained networks. These 400 networks varied in how well they estimated F0 for the validation set (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig. 1D&amp;E</xref>). For each psychophysical experiment and network, we quantified the similarity between human and network results with a correlation coefficient. We then compared this human-model similarity to each network’s performance on the validation set (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3A-E</xref>).</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-28">Network architectures producing better F0 estimation for natural sounds exhibit more human-like pitch behavior. <bold>A-E</bold> plot human-model similarity for all 400 architectures as a function of the accuracy of the trained architecture on the validation set (a set of stimuli distinct from the training dataset, but generated with the same procedure). The similarity between human and model results was quantified for each experiment as the correlation coefficient between analogous data points (see Methods). Pearson correlations between validation set accuracy and human-model similarity for each experiment are noted in the legends. Each graph (<bold>A-E)</bold> corresponds to one of the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-20" hwp:rel-id="F2">Fig. 2A-E</xref>): <bold>(A)</bold> F0 discrimination as a function of harmonic number and phase, <bold>(B)</bold> pitch estimation of alternating-phase stimuli, <bold>(C)</bold> pitch estimation of frequency-shifted complexes, <bold>(D)</bold> pitch estimation of complexes with individually mistuned harmonics, and <bold>(E)</bold> frequency discrimination with pure and transposed tones. <bold>(F)</bold> The results of the experiment from A (F0 discrimination thresholds as a function of lowest harmonic number and harmonic phase) measured from the 40 worst, middle, and best architectures ranked by F0 estimation performance on natural sounds (indicated with green patches in A). Lines plot means across the 40 networks. Error bars indicate 95% confidence intervals via bootstrapping across the 40 networks. Human F0 discrimination thresholds from the same experiment are re-plotted for comparison.</p></caption><graphic xlink:href="389999v4_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-29">For four of the five experiments (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3A</xref>-D), there was a significant positive correlation between training task performance and human-model similarity (p&lt;0.001 in each case). The transposed tones experiment (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3E</xref>) was the exception, as all networks similarly replicated the main human result regardless of their training task performance. We suspect this is because transposed tones cause patterns of peripheral stimulation that rarely occur for natural sounds. Thus, virtually any model that learns to associate naturally occurring peripheral cues with F0 will exhibit poor performance for transposed tones.</p><p hwp:id="p-30">To illustrate the effect of optimization for one experiment, <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3F</xref> displays the average F0 discrimination thresholds for each of the worst, middle, and best 10% of networks (sorted by performance on the validation set). It is visually apparent that top-performing networks exhibit more similar psychophysical behavior to humans than worseperforming networks. See <xref ref-type="fig" rid="figS2" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Supplementary Fig. 2</xref> for analogous results for the other four experiments from <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-21" hwp:rel-id="F2">Fig. 2</xref>. Overall, these results indicate that networks with better performance on the F0-estimation training task generally exhibit more human-like pitch behavior, consistent with the idea that these patterns of behavior are byproducts of optimization under natural constraints.</p><p hwp:id="p-31">Because the space of network architectures is large, it is a challenge to definitively associate particular network motifs with good performance and/or human-like behavior. However, we found that very shallow networks both performed poorly on the training task and exhibited less similarity with human behavior (<xref ref-type="fig" rid="figS3" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Supplementary Fig. 3</xref>). This result provides evidence that deep networks (with multiple hierarchical stages of processing) better account for human pitch behavior than relatively shallow networks.</p></sec><sec id="s2j" hwp:id="sec-12"><title hwp:id="title-14">Human-like behavior requires a biologically-constrained cochlea</title><p hwp:id="p-32">To test whether a biologically-constrained cochlear model was necessary for human-like pitch behavior, we trained networks to estimate F0 directly from sound waveforms (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4A</xref>). We replaced the cochlear model with a bank of 100 one-dimensional convolutional filters operating directly on the audio. The weights of these first-layer filters were optimized for the F0 estimation task along with the rest of the network.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6 xref-fig-4-7 xref-fig-4-8 xref-fig-4-9 xref-fig-4-10 xref-fig-4-11 xref-fig-4-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-33">Networks trained to estimate F0 directly from sound waveforms exhibit less human-like pitch behavior. <bold>(A)</bold> Schematic of model structure. Model architecture was identical to that depicted in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1A</xref>, except that the hardwired cochlear input representation was replaced by a layer of 1-dimensional convolutional filters operating directly on sound waveforms. The first-layer filter kernels were optimized for the F0 estimation task along with the rest of the network weights. We trained the 10 best networks from our architecture search with these learnable first-layer filters. <bold>(B)</bold> The best frequencies (sorted from lowest to highest) of the 100 learned filters for each of the 10 network architectures are plotted in magenta. For comparison, the best frequencies of the 100 cochlear filters in the hardwired peripheral model are plotted in black. <bold>(C)</bold> Effect of learned cochlear filters on network behavior in all five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-22" hwp:rel-id="F2">Fig. 2A-E</xref>): F0 discrimination as a function of harmonic number and phase (Expt. A), pitch estimation of alternating-phase stimuli (Expt. B), pitch estimation of frequency-shifted complexes (Expt. C), pitch estimation of complexes with individually mistuned harmonics (Expt. D), and frequency discrimination with pure and transposed tones (Expt. E). Lines plot means across the 10 networks; error bars plot 95% confidence intervals, obtained by bootstrapping across the 10 networks. <bold>(D)</bold> Comparison of human-model similarity metrics between networks trained with either the hardwired cochlear model (black) or the learned cochlear filters (magenta) for each psychophysical experiment. Asterisks indicate statistical significance of two-sample t-tests comparing the two cochlear model conditions: ***p&lt;0.001, *p=0.016. Error bars indicate 95% confidence intervals bootstrapped across the 10 network architectures.</p></caption><graphic xlink:href="389999v4_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-34">The learned filters deviated from those in the ear, with best frequencies tending to be lower than those of the hardwired peripheral model (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4B</xref>). Networks with learned cochlear filters also exhibited less human-like behavior than their counterparts with the fixed cochlear model (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig, 4C&amp;D</xref>). In particular, networks with learned cochlear filters showed little ability to extract pitch information from high-numbered harmonics. Discrimination thresholds for higher harmonics were poor (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4C</xref>, Expt. A) and networks did not exhibit phase effects (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig. 4C</xref>, Expt. A &amp; B). Accordingly, human-model similarity was substantially lower with learned cochlear filters for two of five psychophysical experiments (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig. 4D</xref>; Expt. A: t(18)= 5.23, p&lt;0.001, d=2.47; Expt. B: t(18)=12.69, p&lt;0.001, d=5.98). This result suggests that a human-like cochlear representation is necessary to obtain human-like behavior, but also that the F0 estimation task on its own is insufficient to produce a human-like cochlear representation, likely because the cochlea is shaped by many auditory tasks. Thus, the cochlea may be best considered as a constraint on pitch perception rather than the other way around.</p></sec><sec id="s2k" hwp:id="sec-13"><title hwp:id="title-15">Dependence of pitch behavior on the cochlea</title><p hwp:id="p-35">To gain insight into what aspects of the cochlea underlie the characteristics of pitch perception, we investigated how the model behavior depends on its peripheral input. Decades of research has sought to determine the aspects of peripheral auditory representations that underlie pitch judgments, but experimental research has been limited by the difficulty of manipulating properties of peripheral representations. We took advantage of the ability to perform experiments on the model that are not possible in biology, training networks with peripheral representations that were altered in various ways. To streamline presentation, we present results for a single psychophysical result that was particularly diagnostic: the effect of lowest harmonic number on F0 discrimination thresholds (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-23" hwp:rel-id="F2">Fig. 2A</xref>, solid line). Results for other experiments are generally congruent with the overall conclusions and are shown in Supplementary Figures. We first present experiments manipulating the fidelity of temporal coding, followed by experiments manipulating frequency selectivity along the cochlea’s length.</p></sec><sec id="s2l" hwp:id="sec-14"><title hwp:id="title-16">Human-like behavior depends critically on phase locking</title><p hwp:id="p-36">To investigate the role of temporal coding in the auditory periphery, we trained networks with alternative upper limits of auditory nerve phase locking. Phase locking is limited by biophysical properties of inner hair cell transduction<sup><xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">4</xref></sup>, which are impractical to alter in vivo but which can be modified in silico via the simulated inner hair cell’s lowpass filter<sup><xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">42</xref></sup>. We separately trained networks with lowpass cutoff frequencies of 50 Hz, 320 Hz, 1000 Hz, 3000 Hz (the nerve model’s default value, commonly presumed to roughly match that of the human auditory nerve), 6000 Hz, and 9000 Hz. With a cutoff frequency of 50 Hz, virtually all temporal structure in the peripheral representation of our stimuli was eliminated, meaning the network only had access to cues from the place of excitation along the cochlea (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 5A</xref>). As the cutoff frequency was increased, the network gained access to progressively finer-grained spike-timing information (in addition to the place cues). The 10 best-performing networks from the architecture search were retrained separately with each of these altered cochleae.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6 xref-fig-5-7 xref-fig-5-8 xref-fig-5-9 xref-fig-5-10 xref-fig-5-11 xref-fig-5-12 xref-fig-5-13 xref-fig-5-14 xref-fig-5-15 xref-fig-5-16"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><p hwp:id="p-37">Pitch perception is impaired in networks optimized with degraded spike timing in the auditory nerve. <bold>(A)</bold> Simulated auditory nerve representations of the same stimulus (harmonic tone with 200 Hz F0) under six configurations of the peripheral auditory model. Configurations differed in the cutoff frequency of the inner hair cell lowpass filter, which sets the upper limit of auditory nerve phase locking. The 3000 Hz setting is that normally used to model the human auditory system. As in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Fig. 1A</xref>, each peripheral representation is flanked by the stimulus power spectrum and the time-averaged cochlear excitation pattern. <bold>(B)</bold> Schematic of stimuli used to measure F0 discrimination thresholds as a function of lowest harmonic number. Gray level denotes amplitude. Two example trials are shown, with two different lowest harmonic numbers. <bold>(C)</bold> F0 discrimination thresholds as a function of lowest harmonic number measured from networks trained and tested with each of the six peripheral model configurations depicted in A. The best thresholds and the transition points from good to poor thresholds (defined as the lowest harmonic number for which thresholds first exceeded 1%) are re-plotted to the left of and below the main axes, respectively. Here and in E, lines plot means across the 10 networks; error bars plot 95% confidence intervals, obtained by bootstrapping across the 10 networks. <bold>(D)</bold> Schematic of stimuli used to measure frequency discrimination thresholds as a function of sound level. Gray level denotes amplitude. <bold>(E)</bold> Frequency discrimination thresholds as a function of sound level measured from human listeners (left) and from the same networks as C (right). Human thresholds, which are reported as a function of sensation level, are re-plotted from<sup><xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref></sup>.</p></caption><graphic xlink:href="389999v4_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-38">Reducing the upper limit of phase locking qualitatively changed the model’s psychophysical behavior and made it less human-like. As shown in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig. 5B&amp;C</xref>, F0 discrimination thresholds became worse, with the best threshold (the left-most data point, corresponding to a lowest harmonic number of 1) increasing as the cutoff was lowered (significantly worse for all three conditions: 1000 Hz, t(18)=4.39, p&lt;0.001, d=1.96; 320 Hz, t(18)=11.57, p&lt;0.001, d=5.17; 50 Hz, t(18)=9.30, p&lt;0.001, d=4.16; two-sample t-tests comparing to thresholds in the 3000 Hz condition). This in itself is not surprising, as it has long been known that phase locking enables better frequency discrimination than place information alone<sup><xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>,<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref></sup>. However, thresholds also showed a different dependence on harmonic number as the phase locking cutoff was lowered. Specifically, the transition from good to poor thresholds, here defined as the left-most point where thresholds exceeded 1%, was lower with degraded phase locking. This difference was significant for two of the three conditions (1000 Hz, t(18)=5.15, p&lt;0.001, d=2.30; 50 Hz, t(18)=10.10, p&lt;0.001, d=4.52; two-sample t-tests comparing to the 3000 Hz condition; the transition point was on average lower for the 320 Hz condition, but the results were more variable across architectures, and so the difference was not statistically significant). Increasing the cutoff to 6000 Hz or 9000 Hz had minimal effects on both of these features (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig. 5C</xref>), suggesting that superhuman temporal resolution would not continue to improve pitch perception (at least as assessed here). Discrimination thresholds for high-numbered harmonics were in fact slightly worse for increased cutoff frequencies. One explanation is that increasing the model’s access to fine timing information biases the learned strategy to rely more on this information, which is less useful for determining the F0 of stimuli containing only high-numbered harmonics. Overall, these results suggest that auditory nerve phase locking like that believed to be present in the human ear is critical for human-like pitch perception.</p><p hwp:id="p-39">A common criticism of place-based pitch models is that they fail to account for the robustness of pitch across sound level, because cochlear excitation patterns saturate at high levels<sup><xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-4" hwp:rel-id="ref-26">26</xref></sup>. Consistent with this idea, frequency discrimination thresholds (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig. 5D</xref>) measured from networks with lower phase locking cutoffs were less invariant to level than networks trained with normal spike-timing information (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig. 5E</xref>, right). Thresholds for models with limited phase locking became progressively worse for louder tones, unlike those for humans (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Fig. 5E</xref>, left)<sup><xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-2" hwp:rel-id="ref-50">50</xref></sup>. This effect produced an interaction between the effect of stimulus level and the phase locking cutoff on discrimination thresholds (F(13.80,149.08)=4.63, p&lt;0.001, <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="389999v4_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>), in addition to the main effect of the cutoff (F(5,54)=23.37, p&lt;0.001, <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="389999v4_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>; also evident in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-7" hwp:rel-id="F5">Fig. 5C</xref>). Similar effects were observed when thresholds were measured with complex tones (data not shown).</p><p hwp:id="p-40">To control for the possibility that the poor performance of the networks trained with lower phase locking cutoffs might be specific to the relatively small number of simulated auditory nerve fibers in the model, we generated an alternative representation for the 50 Hz cutoff condition, using 1000 nerve fibers and 100 timesteps (sampled at 2 kHz). We then trained and tested the 10 best-performing networks from our architecture search on these representations (transposing the nerve fiber and time dimensions to maintain the input size and thus be able to use the same network architecture). Increasing the number of simulated auditory nerve fibers by a full order of magnitude modestly improved thresholds but did not qualitatively change the results: networks without high-fidelity temporal information still exhibited abnormal F0 discrimination behavior. The 50 Hz condition results in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-8" hwp:rel-id="F5">Fig. 5C&amp;E</xref> are taken from the 1000 nerve fiber networks, as this seemed the most conservative comparison. Results for different numbers of nerve fibers are provided in <xref ref-type="fig" rid="figS4" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Supplementary Fig. 4</xref>.</p><p hwp:id="p-41">We simulated the full suite of psychophysical experiments on all networks with altered cochlear temporal resolution (<xref ref-type="fig" rid="figS5" hwp:id="xref-fig-14-1" hwp:rel-id="F14">Supplementary Fig. 5</xref>). Several other experimental results were also visibly different from those of humans in models with altered phase locking cutoffs (in particular, the alternating-phase and mistuned harmonics experiments). Overall, the results indicate that normal human pitch perception depends on phase locking up to 3000 Hz.</p></sec><sec id="s2m" hwp:id="sec-15"><title hwp:id="title-17">Human-like behavior depends less on cochlear filter bandwidths</title><p hwp:id="p-42">The role of cochlear frequency tuning in pitch perception has also been the source of longstanding debates<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-4" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-4" hwp:rel-id="ref-22">22</xref>,<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>,<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-2" hwp:rel-id="ref-48">48</xref>,<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>,<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">32</xref></sup>. Classic “place” theories of pitch postulate that F0 is inferred from the peaks and valleys in the excitation pattern. Contrary to this idea, we found that simply eliminating all excitation pattern cues (by separately re-scaling each frequency channel in the peripheral representation to have the same time-averaged response, without retraining the model) had almost no effect on network behavior (<xref ref-type="fig" rid="figS6" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Supplementary Fig. 6</xref>). This result suggests that F0 estimation does not require the excitation pattern per se, but it remains possible it that might still be constrained by the frequency tuning of the cochlea.</p><p hwp:id="p-43">To investigate the perceptual effects of cochlear frequency tuning, we trained networks with altered tuning. We first scaled cochlear filter bandwidths to be two times narrower and two times broader than those estimated for human listeners<sup><xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref></sup>. The effect of this manipulation is visually apparent in the width of nerve fiber tuning curves as well as in the number of harmonics that produce distinct peaks in the cochlear excitation patterns (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Fig. 6A</xref>).</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6 xref-fig-6-7 xref-fig-6-8 xref-fig-6-9 xref-fig-6-10 xref-fig-6-11 xref-fig-6-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-6"><p hwp:id="p-44">Cochlear frequency tuning has relatively little effect on pitch perception. <bold>(A)</bold> Cochlear filter bandwidths were scaled to be two times narrower or two times broader than those estimated for normalhearing humans. This manipulation is evident in the width of auditory nerve tuning curves measured from five individual fibers per condition (upper left panel). Tuning curves plot thresholds for each fiber as a function of pure tone frequency. Right and lower left panels show simulated auditory nerve representations of the same stimulus (harmonic tone with 200 Hz F0) for each bandwidth condition. Each peripheral representation is flanked by the stimulus power spectrum and the time-averaged auditory nerve excitation pattern. The excitation patterns are altered by changes in frequency selectivity, with coarser tuning yielding less pronounced peaks for individual harmonics, as expected. <bold>(B)</bold> Cochlear filters modeled on the human ear were replaced with a set of linearly spaced filters with constant bandwidths in Hz. Pure tone tuning curves measured with linearly spaced filters are much sharper than those estimated for humans at higher frequencies (left panel; note the logspaced frequency scale). The right panel shows the simulated auditory nerve representation of the stimulus from A with linearly spaced cochlear filters. In this condition, all harmonics are equally resolved by the cochlear filters and thus equally likely to produce peaks in the time-averaged excitation pattern. <bold>(C)</bold> Schematic of stimuli used to measure F0 discrimination thresholds. Gray level denotes amplitude. Two example trials are shown, with two different lowest harmonic numbers. <bold>(D)</bold> F0 discrimination thresholds as a function of lowest harmonic number, measured from networks trained and tested with each of the four peripheral model configurations depicted in A and B. The best thresholds and the transition points from good to poor thresholds (defined as the lowest harmonic number for which thresholds first exceeded 1%) are re-plotted to the left of and below the main axes, respectively. Lines plot means across the 10 networks; error bars indicate 95% confidence intervals bootstrapped across the 10 networks.</p></caption><graphic xlink:href="389999v4_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-45">We also modified the cochlear model to be linearly spaced (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Fig. 6B</xref>), uniformly distributing the characteristic frequencies of the model nerve fibers along the frequency axis and equating their filter bandwidths. Unlike a normal cochlea, which resolves only low-numbered harmonics, the linearly spaced alteration yielded a peripheral representation where all harmonics are equally resolved by the cochlear filters, providing another test of the role of frequency selectivity.</p><p hwp:id="p-46">Contrary to the notion that cochlear frequency selectivity strongly constrains pitch discrimination, networks trained with different cochlear bandwidths exhibit relatively similar F0 discrimination behavior (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Fig. 6C&amp;D</xref>). Broadening filters by a factor of two had no significant effect on the best thresholds (t(18)=0.40, p=0.69, t-test comparing thresholds when lowest harmonic number = 1 to the human tuning condition). Narrowing filters by a factor of two yielded an improvement in best thresholds that was statistically significant (t(18)=2.74, p=0.01, d=1.23) but very small (0.27% vs. 0.32% for the networks with normal human tuning). Linearly spaced cochlear filters also yielded best thresholds that were not significantly different from those for normal human tuning (t(18)=1.88, p=0.08). In addition, the dependence of thresholds on harmonic number was fairly similar in all cases (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Fig. 6D</xref>). The transition between good and poor thresholds occurred around the sixth harmonic irrespective of the cochlear bandwidths (not significantly different for any of the three altered tuning conditions: two times broader, t(18)=1.33, p=0.20; two times narrower, t(18)=1.00, p=0.33; linearly spaced, t(18)=0.37, p=0.71; t-tests comparing to the normal human tuning condition).</p><p hwp:id="p-47">All three models with altered cochlear filter bandwidths produced worse thresholds for stimuli containing only high-numbered harmonics (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Fig. 6D</xref>). This effect is expected for the narrower and linearly-spaced conditions (smaller bandwidths result in reduced envelope cues from beating of adjacent harmonics), but we do not have an explanation for why networks with broader filters also produced poorer thresholds. One possibility that we ruled out is overfitting of the network architectures to the human cochlear filter bandwidths; validation set accuracies were no worse with broader filters (t(18)=0.66, p=0.52). However, we note that all of the models exhibit what would be considered poor performance for stimuli containing only high harmonics (thresholds are at least an order of magnitude worse than they are for low harmonics), and are thus all generally consistent with human perception in this regime.</p><p hwp:id="p-48">We also simulated the full suite of psychophysical experiments from <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-24" hwp:rel-id="F2">Fig. 2</xref> on networks with altered frequency tuning. Most experimental results were robust to peripheral frequency tuning (<xref ref-type="fig" rid="figS7" hwp:id="xref-fig-16-1" hwp:rel-id="F16">Supplementary Fig. 7</xref>).</p></sec><sec id="s2n" hwp:id="sec-16"><title hwp:id="title-18">Dependence of pitch behavior on training set sound statistics</title><p hwp:id="p-49">In contrast to the widely debated roles of peripheral cues, the role of natural sound statistics in pitch has been little discussed throughout the history of hearing research. To investigate how optimization for natural sounds may have shaped pitch perception, we fixed the cochlear representation to its normal human settings and instead manipulated the characteristics of the sounds on which networks were trained.</p></sec><sec id="s2o" hwp:id="sec-17"><title hwp:id="title-19">Altered training set spectra produce altered behavior</title><p hwp:id="p-50">One salient property of speech and instrument sounds is that they typically have more energy at low frequencies than high frequencies (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. 7A</xref>, left column, black line). To test if this lowpass characteristic shapes pitch behavior, we trained networks on highpass-filtered versions of the same stimuli (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig. 7A</xref>, left column, orange line) and then measured their F0 discrimination thresholds (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Fig. 7B</xref>). For comparison, we performed the same experiment with lowpass-filtered sounds.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6 xref-fig-7-7 xref-fig-7-8 xref-fig-7-9 xref-fig-7-10 xref-fig-7-11 xref-fig-7-12 xref-fig-7-13 xref-fig-7-14 xref-fig-7-15"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-7"><p hwp:id="p-51">Pitch perception depends on training set sound statistics. <bold>(A)</bold> Average power spectrum of training stimuli under different training conditions. Networks were trained on datasets with lowpass- and highpass-filtered versions of the primary speech and music stimuli (column 1), as well as datasets of synthetic tones with spectral statistics either matched or anti-matched (see Methods) to those of the primary dataset (column 2), and datasets containing exclusively speech or music (column 3). Filtering indicated in column 1 was applied to the speech and music stimuli prior to their superposition on background noise. Grey shaded regions plot the average power spectrum of the background noise that pitch-evoking sounds were embedded in for training purposes. <bold>(B)</bold> Schematic of stimuli used to measure F0 discrimination thresholds as a function of lowest harmonic number. Two example trials are shown, with two different lowest harmonic numbers. <bold>(C)</bold> F0 discrimination thresholds as a function of lowest harmonic number, measured from networks trained on each dataset shown in A. Lines plot means across the 10 networks; error bars indicate 95% confidence intervals bootstrapped across the 10 networks.</p></caption><graphic xlink:href="389999v4_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-52">Thresholds measured from networks optimized for highpass sounds exhibited a much weaker dependence on harmonic number than if optimized for natural sounds (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Fig. 7C</xref>, left column). This difference produced an interaction between the effects of harmonic number and the training condition (F(2.16,38.85)=72.33, p&lt;0.001, <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="389999v4_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula>). By contrast, the dependence on harmonic number was accentuated for lowpass-filtered stimuli, again producing an interaction between the effects of harmonic number and the training condition (F(4.25,76.42)=30.81, p&lt;0.001, <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="389999v4_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula>).</p><p hwp:id="p-53">We also simulated the full suite of psychophysical experiments on these networks (<xref ref-type="fig" rid="figS8" hwp:id="xref-fig-17-1" hwp:rel-id="F17">Supplementary Fig. 8</xref>) and observed several other striking differences in their performance characteristics. In particular, networks optimized for highpass-filtered natural sounds exhibited better discrimination thresholds for transposed tones than pure tones (t(18)=9.92, p&lt;0.001, d=4.43, two-sided two-sample t-test comparing pure tone and transposed tone thresholds averaged across frequency), a complete reversal of the human result. These results illustrate that the properties of pitch perception are not strictly a function of the information available in the periphery – performance characteristics can depend strongly on the “environment” in which a system is optimized.</p></sec><sec id="s2p" hwp:id="sec-18"><title hwp:id="title-20">Natural spectral statistics account for human-like behavior</title><p hwp:id="p-54">To isolate the acoustic properties needed to reproduce human-like pitch behavior, we also trained networks on synthetic tones embedded in masking noise, with spectral statistics matched to those of the natural sound training set (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Fig. 7A</xref>, center column). Specifically, we fit multivariate Gaussians to the spectral envelopes of the speech/instrument sounds and the noise from the original training set, and synthesized stimuli with spectral envelopes sampled from these distributions. Although discrimination thresholds were overall somewhat better than when trained on natural sounds, the resulting network again exhibited human-like pitch characteristics (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Fig. 7C</xref>, center column, black line). Because the synthetic tones were constrained only by the mean and covariance of the spectral envelopes of our natural training data, the results suggest that such low-order spectral statistics capture much of the natural sound properties that matter for obtaining human-like pitch perception (see <xref ref-type="fig" rid="figS8" hwp:id="xref-fig-17-2" hwp:rel-id="F17">Supplementary Fig. 8</xref> for results on the full suite of psychophysical experiments).</p><p hwp:id="p-55">For comparison, we also trained networks on synthetic tones with spectral statistics that deviate considerably from speech and instrument sounds. We generated these “anti-matched” synthetic tones by multiplying the mean of the fitted multivariate Gaussian by negative one (see Methods) and sampling spectral envelopes from the resulting distribution. Training on the resulting highpass synthetic tones (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-7" hwp:rel-id="F7">Fig. 7A</xref>, center column, orange line) completely reversed the pattern of behavior seen in humans: discrimination thresholds were poor for stimuli containing low-numbered harmonics and good for stimuli containing only high-numbered harmonics (producing a negative correlation with human results: r=-0.98, p&lt;0.001, Pearson correlation) (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-8" hwp:rel-id="F7">Fig. 7C</xref>, center column, orange line). These results further illustrate that the dominance of low-numbered harmonics in human perception is not an inevitable consequence of cochlear transduction – good pitch perception is possible in domains where it is poor in humans, provided the system is trained to extract the relevant information.</p></sec><sec id="s2q" hwp:id="sec-19"><title hwp:id="title-21">Music-trained networks exhibit better pitch acuity</title><p hwp:id="p-56">We also trained networks separately using only speech or only music stimuli (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-9" hwp:rel-id="F7">Fig. 7A</xref>, right column). Consistent with the more accurate pitch discrimination found in human listeners with musical training<sup><xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref></sup>, networks optimized specifically for music have lower discrimination thresholds for stimuli with low-numbered harmonics (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-10" hwp:rel-id="F7">Fig. 7C</xref>, right column; t(18)=9.73, p&lt;0.001, d=4.35, two-sample t-test comparing left-most conditions – which produce the best thresholds – for speech and music training). As a test of whether this result could be explained by cochlear processing, we repeated this experiment on networks with learnable first-layer filters (as in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-7" hwp:rel-id="F4">Fig. 4A</xref>) and found that networks optimized specifically for music still produced lower absolute thresholds (<xref ref-type="fig" rid="figS9" hwp:id="xref-fig-18-1" hwp:rel-id="F18">Supplementary Fig. 9</xref>). This result likely reflects the greater similarity of the synthetic test tones (standardly used to assess pitch perception) to instrument notes compared to speech excerpts, the latter of which are less perfectly periodic over the stimulus duration.</p></sec><sec id="s2r" hwp:id="sec-20"><title hwp:id="title-22">Training set noise required for “missing fundamental” illusion</title><p hwp:id="p-57">One of the core challenges of hearing is the ubiquity of background noise. To investigate how pitch behavior may have been shaped by the need to hear in noise, we varied the level of the background noise in our training set. Networks trained in noisy environments (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Fig. 8</xref>, left) resembled humans in accurately inferring F0 even when the F0 was not physically present in the stimuli (thresholds for stimuli with lowest harmonic number between 2 and 5 were all under 1%). This “missing fundamental illusion” was progressively weakened in networks trained in higher SNRs (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Fig. 8</xref>, center and right), with discrimination thresholds sharply elevated when the lowest harmonic number exceeded two (F(2,27)=6.79, p&lt;0.01, <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="389999v4_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula>; main effect of training condition when comparing thresholds for lowest harmonic numbers between 2 and 5).</p><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3 xref-fig-8-4 xref-fig-8-5 xref-fig-8-6 xref-fig-8-7 xref-fig-8-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-8"><p hwp:id="p-58">Key characteristics of human pitch behavior only emerge in noisy training conditions. <bold>(A)</bold> Average power spectrum of training stimuli. Networks were trained on speech and music stimuli embedded in three different levels of background noise: high (column 1), low (column 2), and none (column 3). <bold>(B)</bold> Effect of training set noise level on network behavior in all five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-25" hwp:rel-id="F2">Fig. 2A-E</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). Lines plot means across the 10 networks; error bars indicate 95% confidence intervals bootstrapped across the 10 networks.</p></caption><graphic xlink:href="389999v4_fig8" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-59">Networks trained in noiseless environments also deviated from human behavior when tested on alternating-phase (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Fig. 8B</xref>, row 2) and frequency-shifted complexes (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-4" hwp:rel-id="F8">Fig. 8B</xref>, row 3), apparently ignoring high-numbered harmonics (correlations with human results were lower in both experiments; t(18)=9.08, p&lt;0.001, d=4.06 and t(18)=4.41, p&lt;0.001, d=1.97, comparing high vs. no training noise). Conversely, discrimination thresholds for pure tones (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-5" hwp:rel-id="F8">Fig. 8B</xref>, row 5) remained good (below 1%), as though the networks learned to focus primarily on the first harmonic. Collectively, these results suggest the ability to extract F0 information from high-numbered harmonics in part reflects an adaptation for hearing in noise.</p></sec><sec id="s2s" hwp:id="sec-21"><title hwp:id="title-23">Network neurophysiology</title><p hwp:id="p-60">Although our primary focus in this paper was to use DNNs to understand behavior in normative terms, we also examined whether the internal representations of our model might exhibit established neural phenomena.</p><p hwp:id="p-61">We simulated electrophysiology experiments on our best-performing network architecture by measuring time-averaged model unit activations to pure and complex tones varying in harmonic composition (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Fig. 9A</xref>). F0 tuning curves of units in different network layers (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Fig. 9B</xref>) illustrate a transition from frequency-tuned units in the first layer (relu_0, where units responded whenever a harmonic of a complex tone aligned with their pure-tone tuning) to complex tuning in intermediate layers (relu_2, relu_4, and fc_int) to unambiguous F0 tuning in the final layer (fc_top), where units responded selectively to specific F0s across different harmonic compositions. These latter units thus resemble pitch-selective neurons identified in primate auditory cortex<sup><xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref></sup> in which tuning to the F0 of missing-fundamental complexes aligns with pure tone tuning.</p><fig id="fig9" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3 xref-fig-9-4 xref-fig-9-5 xref-fig-9-6 xref-fig-9-7 xref-fig-9-8 xref-fig-9-9 xref-fig-9-10 xref-fig-9-11 xref-fig-9-12 xref-fig-9-13 xref-fig-9-14 xref-fig-9-15 xref-fig-9-16"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9.</label><caption hwp:id="caption-9"><p hwp:id="p-62">Network neurophysiology. Network activations were measured in response to pure tones and complex tones with four different harmonic compositions. <bold>(A)</bold> Left: Power spectra for stimuli with 200 Hz F0. Center: expected F0 tuning curves for an idealized frequency-tuned unit. The tuning curves are color-matched to the corresponding stimulus (e.g., black for pure tones and red for harmonics 6-14). A frequency-tuned unit should respond to pure tones near its preferred frequency (414 Hz) or to complex tones containing harmonics near its preferred frequency (e.g., when F0 = 212, 138, 103.5, or 82.8 Hz, i.e. 414/2, 414/3 or 414/4 Hz). Right: expected F0 tuning curves for an idealized F0-tuned unit. An F0-tuned unit should produce tuning curves that are robust to harmonic composition. The strength of a unit’s F0 tuning can thus be quantified as the mean correlation between the pure tone (frequency) tuning curve and each of the complex tone tuning curves. <bold>(B)</bold> F0 tuning curves measured from five representative units in each of five network layers. Units in the first layer (relu_0) seem to exhibit frequency tuning. Units in the last layer (fc_top) exhibit F0 tuning. <bold>(C)</bold> Left: Nominal F0 tuning curves were measured for complex tones made inharmonic by jittering component frequencies. Center: Such curves are shown for one example unit in the network’s last layer. Unlike for harmonic tones, the tuning curves for tones with different frequency compositions do not align. Right: The overall F0 tuning of a network layer was computed by averaging the F0 tuning strength across all units in the layer. A unit’s F0 tuning strength was quantified as the mean correlation between the pure tone (frequency) tuning curve and each of the complex tone tuning curves. For each of our 10 best network architectures, overall F0 tuning (computed separately using either harmonic or inharmonic complex tones) is plotted as a function of network layer. Network units become progressively more F0-tuned deeper into the networks, but only for harmonic tones. <bold>(D)</bold> Left: Population responses of pitch-selective units in marmoset auditory cortex, human auditory cortex, and our model’s output layer, plotted as a function of lowest harmonic number. Marmoset single-unit recordings were made from 3 animals and error bars indicate SEM across 50 neurons (re-plotted from <sup><xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref></sup>). Center: Human fMRI responses to harmonic tones, as a function of their lowest harmonic number. Data were collected from 13 participants and error bars indicate within-subject SEM (re-plotted from <sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">29</xref></sup>). Responses were measured from a functional region of interest defined by a contrast between harmonic tones and frequency-matched noise. Responses were measured in independent data (to avoid double dipping). Right: Network unit activations to harmonic tones as a function of lowest harmonic number. Activations were averaged across all units in the final fully connected layer of our 10 best network architectures (error bars indicate 95% confidence intervals bootstrapped across the 10 best network architectures).</p></caption><graphic xlink:href="389999v4_fig9" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-63">We quantified the F0 tuning of individual units by measuring the correlation between pure tone and complex tone tuning curves. High correlations between tuning curves indicate F0 tuning invariant to harmonic composition. In each of the 10 best-performing networks, units became progressively more F0-tuned deeper into the network (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Fig. 9C</xref>, right, solid symbols). Critically, this result depended on the harmonicity of the tones. When we repeated the analysis with complex tones made inharmonic by jittering component frequencies<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref></sup> (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-4" hwp:rel-id="F9">Fig. 9C</xref>, left), network units no longer showed F0 tuning (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-5" hwp:rel-id="F9">Fig. 9B</xref>, center) and the dependence on network layer was eliminated (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-6" hwp:rel-id="F9">Fig. 9C</xref>, right, open symbols). In this respect the units exhibit a signature of human F0-based pitch, which is also disrupted by inharmonicity<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-3" hwp:rel-id="ref-20">20</xref>,<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref></sup>, and of pitch-tuned neurons in non-human primates<sup><xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref></sup>.</p><p hwp:id="p-64">To compare the population tuning to that observed in the auditory system, we also measured unit activations to harmonic complexes as a function of the lowest harmonic in the stimulus. The F0-tuned units in our model’s final layer responded more strongly when stimuli contained low-numbered harmonics (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-7" hwp:rel-id="F9">Fig. 9D</xref>, right; main effect of lowest harmonic number on mean activation, F(1.99, 17.91)=134.69, p&lt;0.001, <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="389999v4_inline5a.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula>). This result mirrors the response characteristics of pitch-selective neurons (measured with single-unit electrophysiology) in marmoset auditory cortex (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-8" hwp:rel-id="F9">Fig. 9D</xref>, left)<sup><xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-3" hwp:rel-id="ref-28">28</xref></sup> and pitch-selective voxels (measured with fMRI) in human auditory cortex (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-9" hwp:rel-id="F9">Fig. 9D</xref>, center)<sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-3" hwp:rel-id="ref-29">29</xref></sup>.</p></sec></sec><sec id="s3" hwp:id="sec-22"><title hwp:id="title-24">DISCUSSION</title><p hwp:id="p-65">We developed a model of pitch perception by optimizing artificial neural networks to estimate the fundamental frequency of their acoustic input. The networks were trained on simulated auditory nerve representations of speech and music embedded in background noise. The best-performing networks closely replicated human pitch judgments in simulated psychophysical experiments despite never being trained on the psychophysical stimuli. To investigate which aspects of the auditory periphery and acoustical environment contribute to human-like pitch behavior, we optimized networks with altered cochleae and sound statistics. Lowering the upper-limit of phase locking in the auditory nerve yielded models with behavior unlike that of humans: F0 discrimination was substantially worse than in humans and had a distinct dependence on stimulus characteristics. Model behavior was substantially less sensitive to changes in cochlear frequency tuning. However, the results were also strongly dependent on the sound statistics the model was optimized for. Optimizing for stimuli with unnatural spectra, or without concurrent background noise yielded behavior qualitatively different from that of humans. The results suggest that the characteristics of human pitch perception reflect the demands of estimating the fundamental frequency of natural sounds, in natural conditions, given a human cochlea.</p><p hwp:id="p-66">Our model innovates on prior work in pitch perception in two main respects. First, the model was optimized to achieve accurate pitch estimation in realistic conditions. By contrast, most previous pitch models have instantiated particular mechanistic or algorithmic hypotheses<sup><xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">33</xref>–<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">37</xref>,<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-6" hwp:rel-id="ref-25">25</xref>,<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">38</xref>–<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">40</xref></sup>. Our model’s initial stages incorporated detailed simulations of the auditory nerve, but the rest of the model was free to implement any of a wide set of strategies that optimized performance. Optimization enabled us to test normative explanations of pitch perception that have previously been neglected. Second, the model achieved reasonable quantitative matches to human pitch behavior. This match to behavior allowed strong tests of the role of different elements of peripheral coding in the auditory nerve. Prior work attempted to derive optimal decoders of frequency from the auditory nerve<sup><xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">49</xref>,<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref></sup>, but was unable to assess pitch perception (i.e., F0 estimation) due to the added complexity of this task.</p><p hwp:id="p-67">Both of these innovations were enabled by contemporary “deep” neural networks. For our purposes, DNNs instantiate general-purpose functions that can be optimized to perform a training task. They learn to use task-relevant information present in the sensory input, and avoid the need for hand-designed methods to extract such information. This generality is important for achieving good performance on real-world tasks. Hand-designed models, or simpler model classes, would likely not provide human-level performance. For instance, we found that very shallow networks both produced worse overall performance, and a poorer match to human behavior (<xref ref-type="fig" rid="figS3" hwp:id="xref-fig-12-2" hwp:rel-id="F12">Supplementary Fig. 3</xref>).</p><p hwp:id="p-68">Although mechanistic explanations of pitch perception are widely discussed<sup><xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-3" hwp:rel-id="ref-33">33</xref>–<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-3" hwp:rel-id="ref-38">38</xref>,<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-3" hwp:rel-id="ref-40">40</xref></sup>, there have been few attempts to explain pitch in normative terms. But like other aspects of perception, pitch is plausibly the outcome of an optimization process (realized through some combination of evolution and development) that produces good performance under natural conditions. We found evidence that these natural conditions have a large influence on the nature of pitch perception, in that human-like behavior emerged only in models optimized for naturalistic sounds heard in naturalistic conditions (with background noise).</p><p hwp:id="p-69">In particular, the demands of extracting the F0 of natural sounds appear to explain one of the signature characteristics of human pitch perception: the dependence on low-numbered harmonics. This characteristic has traditionally been proposed to reflect limitations of cochlear filtering, with filter bandwidths determining the frequencies that can be resolved in a harmonic sound<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-5" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-5" hwp:rel-id="ref-22">22</xref>,<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">52</xref>,<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-3" hwp:rel-id="ref-48">48</xref></sup>. However, we found that the dependence on harmonic number could be fully reversed for sufficiently unnatural sound training sets (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-11" hwp:rel-id="F7">Fig. 7C</xref>). Moreover, the dependence was stable across changes in cochlear filter bandwidths (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Fig. 6C</xref>). These results suggest that pitch characteristics primarily reflect the constraints of natural sound statistics (specifically, lowpass power spectra) coupled with the high temporal fidelity of the auditory nerve. In the language of machine learning, discrimination thresholds appear to partly be a function of the match between the test stimuli and the training set (i.e., the sensory signals a perceptual system was optimized for). Our results suggest that this match is critical to explaining many of the well-known features of pitch perception.</p><p hwp:id="p-70">A second influence of the natural environment was evident when we eliminated background noise from the training set (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-6" hwp:rel-id="F8">Fig. 8</xref>). Networks trained without background noise did not extract F0 information from high-numbered harmonics, relying entirely on the lowest-numbered harmonics. Such a strategy evidently works well for idealized environments (where the lowest harmonics are never masked by noise), but not for realistic environments containing noise, and diverges from the strategy employed by human listeners. This result suggests that pitch is also in part a consequence of needing to hear in noise, and is consistent with evidence that human pitch perception is highly noise-robust<sup><xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref></sup>. Together, these two results suggest that explanations of pitch perception cannot be separated from the natural environment.</p><p hwp:id="p-71">The approach we propose here contrasts with prior work that derived optimal strategies for psychophysical tasks on synthetic stimuli<sup><xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>,<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-3" hwp:rel-id="ref-49">49</xref>,<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">9</xref>,<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref></sup>. Although human listeners often improve on such tasks with practice, there is not much reason to expect humans to approach optimal behavior for arbitrary tasks and stimuli (because these do not drive natural selection, or learning during development). By contrast, it is plausible that humans are near-optimal for important tasks in the natural environment, and that the consequences of this optimization will be evident in patterns of psychophysical performance, as we found here.</p><p hwp:id="p-72">Debates over pitch mechanisms have historically been couched in terms of the two axes of the cochlear representation: place and time. Place models analyze the signature of harmonic frequency spectra in the excitation pattern along the length of the cochlea<sup><xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>,<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref></sup>, whereas temporal models quantify signatures of periodicity in temporal patterns of spikes<sup><xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-4" hwp:rel-id="ref-33">33</xref>,<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-3" hwp:rel-id="ref-37">37</xref></sup>. Our model makes no distinction between place and time per se, using whatever information in the cochlear representation is useful for the training task. However, we were able to assess its dependence on peripheral resolution in place and time by altering the simulated cochlea. These manipulations provided evidence that fine-grained peripheral timing is critical for normal pitch perception (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-9" hwp:rel-id="F5">Fig. 5C&amp;E</xref>), and that fine-grained place-based frequency tuning is less so (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-7" hwp:rel-id="F6">Fig. 6</xref>). Some degree of cochlear frequency selectivity is likely critical to enabling phase locking to low-numbered harmonics, but such effects evidently do not depend sensitively on tuning bandwidth. These conclusions were enabled by combining a realistic model of the auditory periphery with task-optimized neural networks.</p><p hwp:id="p-73">Our model is consistent with most available pitch perception data, but it is not perfect. For instance, the inflection point in the graph of <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-26" hwp:rel-id="F2">Fig. 2A</xref> occurs at a somewhat lower harmonic number in the model than in humans. Given the evidence presented here that pitch perception reflects the stimulus statistics a system is optimized for, some discrepancies might be expected from the training set, which (due to the limitations of available corpora) consisted entirely of speech and musical instrument sounds, and omitted other types of natural sounds that are periodic in time. The range of F0s we trained on was similarly limited by available audio data sets, and prevents us from making predictions about the perception of very high frequencies<sup><xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref></sup>. The uniform distributions over sound level and SNR in our training dataset were also not matched in a principled way to the natural world. Discrepancies may also reflect shortcomings of our F0 estimation task (which used only 50ms clips) or peripheral model, which although state-of-the-art and relatively well validated, is imperfect (e.g., peripheral representations consisted of firing rates rather than spikes).</p><p hwp:id="p-74">We note that the ear itself is the product of evolution and thus likely itself reflects properties of the natural environment<sup><xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref></sup>. We chose to train models on a fixed representation of the ear in part to address longstanding debates over the role of established features of peripheral neural coding on pitch perception. We view this approach as sensible on the grounds that the evolution of the cochlea was plausibly influenced by many different natural behaviors, such that it is more appropriately treated as a constraint on a model of pitch rather than a model stage to be derived along with the rest of the model. Consistent with this view, when we replaced the fixed peripheral model with a set of learnable filters operating directly on sound waveforms, networks exhibited less human-like pitch behavior (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-8" hwp:rel-id="F4">Fig. 4</xref>). This result suggests it could be fruitful to incorporate additional stages of peripheral physiology, which might similarly provide constraints on pitch perception.</p><p hwp:id="p-75">Our model shares many of the commonly-noted limitations of DNNs as models of the brain<sup><xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">62</xref>,<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">63</xref></sup>. Our optimization procedure is not a model of biological learning and/or evolution, but rather provides a way to obtain a system that is optimized for the training conditions given a particular peripheral representation of sound. Biological organisms are almost certainly not learning to estimate F0 from thousands of explicitly labeled examples, and in the case of pitch may leverage their vocal ability to produce harmonic stimuli to hone their perceptual mechanisms. These differences could cause the behavior of biological systems to deviate from optimized neural networks in some ways.</p><p hwp:id="p-76">The neural network architectures we used here are also far from fully consistent with biology, being only a coarse approximation to neural networks in the brain. Although similarities have been documented between trained neural network representations and brain representations<sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-3" hwp:rel-id="ref-16">16</xref>,<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref></sup>, and although we saw some such similarities ourselves in the network’s activations (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-10" hwp:rel-id="F9">Fig. 9C</xref>), the inconsistencies with biology could lead to behavioral differences compared to humans.</p><p hwp:id="p-77">And although our approach is inspired by classical ideal observer models, the model class and optimization methods likely bias the solutions to some extent, and are not provably optimal like classic ideal observer models. Nonetheless, the relatively good match to available data suggests that the optimization is sufficiently successful as to be useful for our purposes.</p><p hwp:id="p-78">The model developed here performs a single task – that of estimating the F0 of a short sound. Human pitch behavior is often substantially more complex, in part because information is conveyed by how the F0 changes over time, as in prosody<sup><xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">64</xref></sup> or melody<sup><xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">65</xref></sup>. In some cases relative pitch involves comparisons of the spectrum rather than the F0<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-4" hwp:rel-id="ref-20">20</xref>,<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">55</xref></sup> and/or can be biased by changes in the timbre of a sound<sup><xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">66</xref></sup>, for reasons that are not well understood. The framework used here could help to develop normative understanding of such effects, by incorporating more complicated tasks (e.g., involving speech or music) and then characterizing the pitch-related behavior that results. DNNs that perform more complex pitch tasks might also exhibit multiple stages of pitch representations that could provide insight into putative hierarchical stages of auditory cortex<sup><xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>,<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">67</xref>,<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-3" hwp:rel-id="ref-18">18</xref></sup>.</p><p hwp:id="p-79">The approach we used here has natural extensions to understanding other aspects of hearing<sup><xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">68</xref></sup>, in which similar questions about the roles of peripheral cues have remained unresolved. Our methods could also be extended to investigate hearing impairment, which can be simulated with alterations to standard models of the cochlea<sup><xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">69</xref></sup> and which often entails particular types of deficits in pitch perception<sup><xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">51</xref></sup>. Prostheses such as cochlear implants are another natural application of task-optimized modeling. Current implants restore some aspects of hearing relatively well, but pitch perception is not one of them<sup><xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">70</xref></sup>. Models optimized with different types of simulated electrical stimulation could clarify the patterns of behavior to expect. Models trained with either acoustically- or electrically-stimulated peripheral auditory representations (or combinations thereof) and then tested with electrically-stimulated input could yield insights into the variable outcomes of pediatric cochlear implantation. Similar approaches could be applied to study acclimatization to hearing aids in adults.</p><p hwp:id="p-80">There is also growing evidence for species differences in pitch perception<sup><xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">71</xref>,<xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">72</xref></sup>. Our approach could be used to relate species differences in perception to species differences in the cochlea<sup><xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">73</xref></sup> or to differences in the acoustic environment and/or tasks a species may be optimized for. While our results suggest that differences in cochlear filters alone are unlikely to explain differences in pitch perception abilities across species, they leave open the possibility that human pitch abilities reflect the demands of speech and music, which plausibly require humans to be more sensitive to small F0 differences than other species. This issue could be clarified by optimizing network representations for different auditory tasks.</p><p hwp:id="p-81">More generally, the results here illustrate how supervised machine learning enables normative analysis in domains where traditional ideal observers are intractable, an approach that is broadly applicable outside of pitch and audition.</p></sec><sec id="s4" hwp:id="sec-23"><title hwp:id="title-25">METHODS</title><sec id="s4a" hwp:id="sec-24"><title hwp:id="title-26">Natural sounds training dataset - overview</title><p hwp:id="p-82">The main training set consisted of 50ms excerpts of speech and musical instruments. This duration was chosen to enable accurate pitch perception in human listeners<sup><xref ref-type="bibr" rid="c74" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">74</xref></sup>, but to be short enough that the F0 would be relatively stable even in natural sounds such as speech that have time-varying F0s. The F0 label for a training example was estimated from a “clean” speech or music excerpt. These excerpts were then superimposed on natural background noise. Overall stimulus presentation levels were drawn uniformly between 30 dB SPL and 90 dB SPL. All training stimuli were sampled at 32 kHz.</p></sec><sec id="s4b" hwp:id="sec-25"><title hwp:id="title-27">Speech and music training excerpts</title><p hwp:id="p-83">We used STRAIGHT<sup><xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">75</xref></sup> to compute time-varying F0 and periodicity traces for sounds in several large corpora of recorded speech and instrumental music: Spoken Wikipedia Corpora (SWC)<sup><xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">76</xref></sup>, Wall Street Journal (WSJ), CMU Kids Corpus, CSLU Kids Speech, NSynth<sup><xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">77</xref></sup>, and RWC Music Database. STRAIGHT provides accurate estimates of the F0 provided the background noise is low, as it was in each of the corpora. Musical instrument recordings were notes from the chromatic scale, and thus were spaced roughly in semitones. To ensure that sounds would span a continuous range of F0s, we randomly pitch-shifted each instrumental music recording by a small amount (up to ±3% F0, via resampling).</p><p hwp:id="p-84">Source libraries were constructed for each corpus by extracting all highly periodic (time-averaged periodicity level &gt; 0.8) and non-overlapping 50ms segments from each recording. We then generated our natural sounds training dataset by sampling segments with replacement from these source libraries to uniformly populate 700 logspaced F0 bins between 80 Hz and 1000 Hz (bin width = 1/16 semitones = 0.36% F0). Segments were assigned to bins according to their time-averaged F0. The resulting training dataset consisted of 3000 exemplars per F0 bin for a total of 2.1 million exemplars. The relative contribution of each corpus to the final dataset was constrained both by the number of segments per F0 bin available in each source library (the higher the F0, the harder it is to find speech clips) and the goal of using audio from many different speakers, instruments, and corpora. The composition we settled on is:
<list list-type="bullet" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-85">F0 bins between 80 Hz and 320 Hz</p><list list-type="bullet" hwp:id="list-2"><list-item hwp:id="list-item-2"><p hwp:id="p-86">50% instrumental music (1000 NSynth and 500 RWC clips per bin),</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-87">50% adult speech (1000 SWC and 500 WSJ clips per bin)</p></list-item></list></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-88">F0 bins between 320 Hz and 450 Hz</p><list list-type="bullet" hwp:id="list-3"><list-item hwp:id="list-item-5"><p hwp:id="p-89">50% instrumental music (1000 NSynth and 500 RWC clips per bin)</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-90">50% child speech (750 CSLU and 750 CMU clips per bin)</p></list-item></list></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-91">F0 bins between 450 Hz and 1000 Hz</p><list list-type="bullet" hwp:id="list-4"><list-item hwp:id="list-item-8"><p hwp:id="p-92">100% instrumental music (2500 NSynth and 500 RWC clips per bin)</p></list-item></list></list-item></list></p></sec><sec id="s4c" hwp:id="sec-26"><title hwp:id="title-28">Background noise for training data</title><p hwp:id="p-93">To make the F0 estimation task more difficult and to simulate naturalistic listening conditions, each speech or instrument excerpt in the training dataset was embedded in natural background noise. The signal-to-noise ratio for each training example was drawn uniformly between −10 dB and +10 dB. Noise source clips were taken from a subset of the AudioSet corpus<sup><xref ref-type="bibr" rid="c78" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">78</xref></sup>, screened to remove nonstationary sounds (e.g., speech or music). The screening procedure involved measuring auditory texture statistics (envelope means, correlations, and modulation power in and across cochlear frequency channels)<sup><xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">79</xref></sup> from all recordings, and discarding segments over which these statistics were not stable in time, as in previous studies<sup><xref ref-type="bibr" rid="c80" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">80</xref></sup>. To ensure the F0 estimation task remained well defined for the noisy stimuli, background noise clips were also screened for periodicity by computing their autocorrelation functions. Noise clips with peaks greater than 0.8 at lags greater than 1ms in their normalized autocorrelation function were excluded.</p></sec><sec id="s4d" hwp:id="sec-27"><title hwp:id="title-29">Peripheral auditory model</title><p hwp:id="p-94">The Bruce et al. (2018) auditory nerve model was used to simulate the peripheral auditory representation of every stimulus. This model was chosen because it captures many of the complex response properties of auditory nerve fibers and has been extensively validated against electrophysiological data from cats<sup><xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-2" hwp:rel-id="ref-69">69</xref>,<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-3" hwp:rel-id="ref-42">42</xref></sup>. Stages of peripheral signal processing in the model include: a fixed middle-ear filter, a nonlinear cochlear filter bank to simulate level-dependent frequency tuning of the basilar membrane, inner and outer hair cell transduction functions, and a synaptic vesicle release/re-docking model of the synapse between inner hair cells and auditory nerve fibers. Although the model’s responses have only been directly compared to recordings made in nonhuman animals, some model parameters have been inferred for humans (such as the bandwidths of cochlear filters) on the basis of behavioral and otoacoustic measurements<sup><xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-2" hwp:rel-id="ref-53">53</xref></sup>.</p><p hwp:id="p-95">Because the majority of auditory nerve fibers, especially those linked to feedforward projections to higher auditory centers, have high spontaneous firing rates<sup><xref ref-type="bibr" rid="c81" hwp:id="xref-ref-81-1" hwp:rel-id="ref-81">81</xref>,<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">82</xref></sup>, we used exclusively high spontaneous rate fibers (70 spikes/s) as the input to our model. To control for the possibility that spontaneous auditory nerve fiber activity could influence pitch behavior (for instance, at conversational speech levels, firing rates of high spontaneous rate fibers are typically saturated, which may degrade excitation pattern cues to F0) we additionally trained and tested the 10 best-performing networks from the architecture search using exclusively low spontaneous rate fibers (0.1 spikes/s). The average results for these networks are shown in <xref ref-type="fig" rid="figS10" hwp:id="xref-fig-19-1" hwp:rel-id="F19">Supplementary Fig. 10</xref>. We found that psychophysical behavior was qualitatively unaffected by nerve fiber spontaneous rate. These results suggested to us that high spontaneous rate fibers were sufficient to yield human-like pitch behavior, so we exclusively used high spontaneous rate fibers in all other experiments.</p><p hwp:id="p-96">In most cases the input to the neural network models consisted of the instantaneous firing rate responses of 100 auditory nerve fibers with characteristic frequencies spaced uniformly on an ERB-number scale<sup><xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">83</xref></sup> between 125 Hz and 14000 Hz. Firing rates were used to approximate the information that would be available in a moderate group of spiking nerve fibers receiving input from the same inner hair cell. The use of 100 frequency channels primarily reflects computational constraints (CPU time for simulating peripheral representations, storage costs, and GPU memory for training), but we note that this number is similar to that used in other auditory models with cochlear frontends<sup><xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">84</xref></sup>. We confirmed that increasing the number of channels by a factor of 10 had little effect on the behavioral results from our main natural sound training condition (<xref ref-type="fig" rid="figS4" hwp:id="xref-fig-13-2" hwp:rel-id="F13">Supplementary Figs. 4</xref> and <xref ref-type="fig" rid="figS5" hwp:id="xref-fig-14-2" hwp:rel-id="F14">5</xref>), and given that 100 channels was sufficient to obtain model thresholds on par with those of humans, it appears that there is little benefit to additional channels for the task we studied.</p><p hwp:id="p-97">To prevent the stimuli being dominated by sound onset/offset effects, each stimulus was padded with 100ms of the original waveform before being passed through the nerve model. The resulting 150ms auditory nerve responses were resampled to 20 kHz. The middle 50ms was then excerpted, leaving a 100-fiber by 1000-timestep array of instantaneous firing rates that constituted the input to the neural networks.</p></sec><sec id="s4e" hwp:id="sec-28"><title hwp:id="title-30">Deep neural network models - overview</title><p hwp:id="p-98">The 100-by-1000 simulated auditory nerve representations were passed into deep convolutional neural networks, each consisting of a series of feedforward layers. These layers were hierarchically organized and instantiated one of a number of simple operations: linear convolution, pointwise nonlinear rectification, weighted average pooling, batch normalization, linear transformation, dropout regularization, and softmax classification.</p><p hwp:id="p-99">The last layer of each network performed F0 classification. We opted to use classification with narrow F0 bins rather than regression in order to soften the assumption that output F0 distributions for a stimulus should be unimodal. For example, an octave error would incur a very large penalty under standard regression loss functions (e.g., L1 or L2), which measure the distance between the predicted and target F0. Classification loss functions, such as the softmax cross-entropy used here, penalize all misclassifications equally. In preliminary work, we found classification networks were empirically easier to train than regression networks and yielded smaller median F0 errors.</p><p hwp:id="p-100">The precision of the network’s F0 estimate is limited by the bin width of the output layer (and by the precision of the training set labels). We chose a bin width of 1/16 semitones (0.36%). We found empirically that the median F0 estimation error increased for bins wider than this value, and did not improve for narrower bins (<xref ref-type="fig" rid="figS11" hwp:id="xref-fig-20-1" hwp:rel-id="F20">Supplementary Fig. 11A</xref>). This performance asymptote could reflect the limits of the F0 labels the network was trained on. As it happened, with this bin width of 1/16 of a semitone it was possible to attain discrimination thresholds for synthetic tones that were on par with the best thresholds typically measured in human listeners (~0.1-0.4%)<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-6" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-4" hwp:rel-id="ref-48">48</xref></sup> for some model architectures and auditory nerve settings. Discrimination thresholds were worse for wider classification bins (<xref ref-type="fig" rid="figS11" hwp:id="xref-fig-20-2" hwp:rel-id="F20">Supplementary Fig. 11B</xref>). We otherwise observed no qualitative change in the pattern of psychophysical results as the bin width was changed. The bin width might thus be considered analogous to decision noise that is sometimes added to models to match human performance (though our choice of bin width appears near-optimal for the dataset we worked with). We note that discrimination thresholds for synthetic tones were also plausibly limited by the similarity of the tones to the training data.</p></sec><sec id="s4f" hwp:id="sec-29"><title hwp:id="title-31">Definitions of constituent neural network operations</title><sec id="s4f1" hwp:id="sec-30"><title hwp:id="title-32">Convolutional layer</title><p hwp:id="p-101">A convolutional layer implements the convolution of a bank of <italic toggle="yes">N<sub>k</sub></italic> two-dimensional linear filter kernels with an input <italic toggle="yes">X</italic>. Convolution performs the same operation at each point in the input, which for the 2D auditory nerve representations we used entails convolving in both time and frequency. Convolution in time is a natural choice for models of sensory systems as their input has translation-invariant temporal statistics. Because translation invariance does not hold for the frequency dimension, convolution in frequency is less obviously a natural model constraint. However, many types of sound signals are well described by approximate translation invariance in local neighborhoods of the frequency domain, and classical auditory models can often be described as imposing convolution in frequency<sup><xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">85</xref>,<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-2" hwp:rel-id="ref-84">84</xref></sup>. Moreover, imposing convolution greatly reduces the number of parameters to be learned. We have empirically found that auditory neural network models often train more readily when convolution in frequency is imposed, suggesting that it is a useful form of model regularization.</p><p hwp:id="p-102">The input is a three-dimensional array with shape [<italic toggle="yes">M<sub>f</sub>, M<sub>t</sub>, M<sub>k</sub></italic>]. For the first convolutional layer in our networks, the input shape was [100,1000,1], corresponding to 100 frequency bins (nerve fibers), 1000 timesteps, and a placeholder 1 in the filter kernel dimension.</p><p hwp:id="p-103">A convolutional layer is defined by five parameters:
<list list-type="order" hwp:id="list-5"><list-item hwp:id="list-item-9"><p hwp:id="p-104"><italic toggle="yes">h</italic> : height of the convolutional filter kernels (number of filter taps in the frequency dimension)</p></list-item><list-item hwp:id="list-item-10"><p hwp:id="p-105"><italic toggle="yes">w</italic> : width of the convolutional filter kernels (number of filter taps in the time dimension)</p></list-item><list-item hwp:id="list-item-11"><p hwp:id="p-106"><italic toggle="yes">N<sub>k</sub></italic> : number of different convolutional filter kernels</p></list-item><list-item hwp:id="list-item-12"><p hwp:id="p-107"><italic toggle="yes">W</italic> : Trainable weights for each of the <italic toggle="yes">N<sub>k</sub></italic> filter kernels; <italic toggle="yes">W</italic> has shape [<italic toggle="yes">h, w, M<sub>k</sub>, N<sub>k</sub></italic>]</p></list-item><list-item hwp:id="list-item-13"><p hwp:id="p-108"><italic toggle="yes">B</italic> : Trainable bias vector with shape [<italic toggle="yes">N<sub>k</sub></italic>]</p></list-item></list></p><p hwp:id="p-109">The output of the convolutional layer <italic toggle="yes">Y</italic> has shape [<italic toggle="yes">N<sub>f</sub>, N<sub>t</sub>, N<sub>k</sub></italic>] and is given by:
<disp-formula hwp:id="disp-formula-1"><alternatives hwp:id="alternatives-7"><graphic xlink:href="389999v4_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives></disp-formula>
where ⊙ denotes pointwise multiplication and ⌊·/·⌋ denotes integer division. Convolutional layers all used a stride of 1 (i.e., non-strided convolution) and “valid” padding, meaning filters were only applied at positions where every element of the kernel overlapped the input. Due to this boundary handling, the frequency and time dimensions of the output were smaller than those of the input: <italic toggle="yes">N<sub>f</sub></italic> = <italic toggle="yes">M<sub>f</sub></italic> – <italic toggle="yes">h</italic> + 1 and <italic toggle="yes">N<sub>t</sub></italic> = <italic toggle="yes">M<sub>t</sub></italic> – <italic toggle="yes">w</italic> + 1.</p></sec><sec id="s4f2" hwp:id="sec-31"><title hwp:id="title-33">Pointwise nonlinear rectification</title><p hwp:id="p-110">To learn a nonlinear function, a neural network must contain nonlinear operations. We incorporate nonlinearity via the rectified linear unit (ReLU) activation function, which is applied pointwise to every element <italic toggle="yes">x</italic> in some input <italic toggle="yes">X</italic>:
<disp-formula hwp:id="disp-formula-2"><alternatives hwp:id="alternatives-8"><graphic xlink:href="389999v4_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives></disp-formula></p></sec><sec id="s4f3" hwp:id="sec-32"><title hwp:id="title-34">Weighted average pooling</title><p hwp:id="p-111">Pooling operations reduce the dimensionality of inputs by aggregating information across adjacent frequency and time bins. To reduce aliasing in our networks (which would otherwise occur from downsampling without first lowpass-filtering), we used weighted average pooling with Hanning windows<sup><xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-2" hwp:rel-id="ref-62">62</xref></sup>. This pooling operation was implemented as the strided convolution of a two-dimensional (frequencyby-time) Hanning filter kernel <italic toggle="yes">H</italic> with an input <italic toggle="yes">X</italic>:
<disp-formula hwp:id="disp-formula-3"><alternatives hwp:id="alternatives-9"><graphic xlink:href="389999v4_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives></disp-formula>
where * denotes convolution and <italic toggle="yes">s<sub>f</sub></italic> and <italic toggle="yes">s<sub>t</sub></italic> indicate the stride length in frequency and time, respectively. The Hanning window <italic toggle="yes">H</italic> had a stride-dependent shape [<italic toggle="yes">h<sub>f</sub>, h<sub>t</sub></italic>], where
<disp-formula hwp:id="disp-formula-4"><alternatives hwp:id="alternatives-10"><graphic xlink:href="389999v4_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives></disp-formula></p><p hwp:id="p-112">For an inputs with shape [<italic toggle="yes">N<sub>f</sub>, N<sub>t</sub>, N<sub>k</sub></italic>], the shape of the output <italic toggle="yes">Y</italic> is [<italic toggle="yes">N<sub>f</sub>/ s<sub>f</sub>, N<sub>t</sub>/ s<sub>t</sub>, N<sub>k</sub></italic>]. Note that when either <italic toggle="yes">s<sub>f</sub></italic> or <italic toggle="yes">s<sub>t</sub></italic> is set to 1, there is no pooling along the corresponding dimension.</p></sec><sec id="s4f4" hwp:id="sec-33"><title hwp:id="title-35">Batch normalization</title><p hwp:id="p-113">Batch normalization is an operation that normalizes its inputs in a pointwise manner using running statistics computed from every batch of training data. Normalizing activations between layers greatly improves DNN training efficiency by reducing the risk of exploding and vanishing gradients: small changes to network parameters in one layer are less likely to be amplified in successive layers if they are separately normalized. For every batch of inputs <italic toggle="yes">B</italic> during training, the pointwise batch mean (<italic toggle="yes">μ<sub>B</sub></italic>) and batch variance <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="389999v4_inline6.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> are computed and then used to normalize each input <italic toggle="yes">X<sub>b</sub></italic> ∈ <italic toggle="yes">B</italic>:
<disp-formula hwp:id="disp-formula-5"><alternatives hwp:id="alternatives-12"><graphic xlink:href="389999v4_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-14"/></alternatives></disp-formula>
where all operations are applied pointwise, <italic toggle="yes">ϵ</italic> = 0.001 to prevent division by zero, and <italic toggle="yes">γ</italic> and <italic toggle="yes">β</italic> are learnable scale and offset parameters. Throughout training, single-batch statistics are used to update the running mean (<italic toggle="yes">μ<sub>total</sub></italic>) and variance <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="389999v4_inline7.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula>. During evaluation mode, <italic toggle="yes">X<sub>b,normaiized</sub></italic> is computed using <italic toggle="yes">μ<sub>total</sub></italic> and <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="389999v4_inline8.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> in place of <italic toggle="yes">μ<sub>B</sub></italic> and <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="389999v4_inline9.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula>.</p></sec><sec id="s4f5" hwp:id="sec-34"><title hwp:id="title-36">Fully connected layer</title><p hwp:id="p-114">A fully connected (or dense) layer applies a linear transformation to its input without any notion of localized frequency or time. An input <italic toggle="yes">X</italic> with shape [<italic toggle="yes">N<sub>f</sub>, N<sub>t</sub>, N<sub>k</sub></italic>], is first reshaped to a vector <italic toggle="yes">X<sub>flat</sub></italic> with shape [<italic toggle="yes">N<sub>f</sub></italic> · <italic toggle="yes">N<sub>t</sub></italic> · <italic toggle="yes">N<sub>k</sub></italic>]. Then, <italic toggle="yes">X<sub>flat</sub></italic> is linearly transformed to give an output <italic toggle="yes">Y</italic> with shape [<italic toggle="yes">N<sub>out</sub></italic>]:
<disp-formula hwp:id="disp-formula-6"><alternatives hwp:id="alternatives-16"><graphic xlink:href="389999v4_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-15"/></alternatives></disp-formula>
where <italic toggle="yes">B</italic> is a bias vector with shape [<italic toggle="yes">N<sub>out</sub></italic>] and <italic toggle="yes">W</italic> is a weight matrix with shape [<italic toggle="yes">N<sub>out</sub>, N<sub>in</sub></italic>]. The values of <italic toggle="yes">B</italic> and <italic toggle="yes">W</italic> are learned during the optimization procedure.</p></sec><sec id="s4f6" hwp:id="sec-35"><title hwp:id="title-37">Dropout regularization</title><p hwp:id="p-115">The dropout operation receives as input a vector <italic toggle="yes">X</italic> with shape [<italic toggle="yes">N<sub>in</sub></italic>] and randomly selects a fraction (<italic toggle="yes">r</italic>) of its values to set to zero. The remaining values are scaled by 1/(1 – <italic toggle="yes">r</italic>), so that the expected sum over all outputs is equal to the expected sum over all inputs. The <italic toggle="yes">r</italic> · <italic toggle="yes">N<sub>in</sub></italic> positions in <italic toggle="yes">X</italic> that get set to zero are chosen at random for every new batch of data. Dropout is commonly used to reduce overfitting in artificial neural networks. It can be thought of as a form of model averaging across the exponentially many sub-networks generated by zeroing-out different combinations of units. All of our networks contained exactly one dropout operation immediately preceding the final fully connected layer. We used a dropout rate of 50% during both training and evaluation.</p></sec><sec id="s4f7" hwp:id="sec-36"><title hwp:id="title-38">Softmax classifier</title><p hwp:id="p-116">The final operation of every network is a softmax activation function, which receives as input a vector <italic toggle="yes">X</italic> of length <italic toggle="yes">N<sub>classes</sub></italic> (equal to the number of output classes; 700 in our case). The input vector is passed through a normalized exponential function to produce a vector <italic toggle="yes">Y</italic> of the same length:
<disp-formula hwp:id="disp-formula-7"><alternatives hwp:id="alternatives-17"><graphic xlink:href="389999v4_ueqn7.gif" position="float" orientation="portrait" hwp:id="graphic-16"/></alternatives></disp-formula></p><p hwp:id="p-117">The values of the output vector are all greater than zero and sum to one. <italic toggle="yes">Y</italic> can be interpreted as a probability distribution over F0 classes for the given input sound.</p></sec></sec><sec id="s4g" hwp:id="sec-37"><title hwp:id="title-39">Model optimization - architecture search</title><p hwp:id="p-118">All of our DNN architectures had the general form of one to eight convolutional layers plus one to two fully connected layers. Each convolutional layer was always immediately followed by three successive operations: ReLU activation function, weighed average pooling, and batch normalization. Fully connected layers were always situated at the end of the network, after the last convolution-ReLU-pooling-normalization block. The final fully connected layer was always immediately followed by the softmax classifier. For architectures with two fully connected layers, the first fully connected layer was followed by a ReLU activation function and a batch normalization operation. In our analyses, we sometimes grouped networks by their number of convolutional layers (e.g., single vs. multi-convolutional-layer networks; <xref ref-type="fig" rid="figS3" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Supplementary Fig. 3</xref>) regardless of the number of fully connected layers. When we refer to network “activations” in a given convolutional layer (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-11" hwp:rel-id="F9">Fig. 9</xref>), we always mean the outputs of the ReLU activation function immediately following that convolutional layer.</p><p hwp:id="p-119">Within the family of models considered, we generated 400 distinct DNN architectures by randomly sampling from a large space of hyperparameters. The number of convolutional layers was first uniformly drawn from 1 to 8. Within each layer, the number and dimensions of convolutional filter kernels were then sampled based on the size of the layer’s input. The number of filter kernels in the first layer was 16, 32, or 64 (each sampled with probability=1/3). The number of kernels in each successive layer could increase by a factor of 2 (probability=1/2), stay the same (probability=1/3), or decrease by a factor of 2 (probability=1/6) relative to the previous layer. Frequency dimensions of the filter kernels were integers sampled uniformly between 1 and <italic toggle="yes">N<sub>f</sub></italic> /2, where <italic toggle="yes">N<sub>f</sub></italic> is the frequency dimension of the layer’s input. Time dimensions of the filter kernels were integers sampled uniformly between <italic toggle="yes">N<sub>t</sub></italic>/20 and <italic toggle="yes">N<sub>t</sub></italic>/2, where <italic toggle="yes">N<sub>t</sub></italic> is the time dimension of the layer’s input. These sampling ranges tended to produce rectangular filters (longer in the time dimension than the frequency dimension), especially in the early layers. We felt this was a reasonable design choice given the rectangular dimensions of the input (100- by-1000, frequency-by-time). To limit the memory footprint of the generated DNNs, we imposed 16 and 1024 as lower and upper bounds on the number of kernels in a single layer and capped the frequency × time area of convolutional filter kernels at 256.</p><p hwp:id="p-120">The stride lengths for the weighted average pooling operations after each convolutional layer were also sampled from distributions. Pooling stride lengths were drawn uniformly between 1 and 4 for the frequency dimension and 1 and 8 for the time dimension. The existence (probability=1/2) and size (128, 256, 512, or 1024 units) of a penultimate fully connected layer were also randomly sampled. The final fully connected layer always contained 700 units to support classification into the 700 F0 bins.</p></sec><sec id="s4h" hwp:id="sec-38"><title hwp:id="title-40">Model optimization - network training</title><p hwp:id="p-121">All 400 network architectures were trained to classify F0 of our natural sounds dataset via stochastic gradient descent with gradients computed via back-propagation. We used a batch size of 64 and the ADAM optimizer with a learning rate of 0.0001. Network weights were trained using 80% of the dataset and the remaining 20% was held-out as a validation set. Performance on the validation set was measured every 5000 training steps and, to reduce overfitting, training was stopped once classification accuracy stopped increasing by at least 0.5% every 5000 training steps. Training was also stopped for networks that failed to achieve 5% classification accuracy after 10000 training steps. Each network was able to reach these early-stopping criteria in less than 48 hours when trained on a single NVIDIA Tesla V100 GPU.</p><p hwp:id="p-122">To ensure conclusions were not based on the idiosyncrasies of any single DNN architecture, we selected the 10 architectures that produced the highest validation set accuracies to use as our model experimental “participants” (collectively referred to as “the model”). We re-trained all 10 architectures for each manipulation of the peripheral auditory model (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-9" hwp:rel-id="F4">Figs. 4</xref>-<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-8" hwp:rel-id="F6">6</xref>) and the training set sound statistics (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-12" hwp:rel-id="F7">Figs. 7</xref> and <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-7" hwp:rel-id="F8">8</xref>). The 10 different network architectures are described in <xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Supplementary Table 1</xref>.</p></sec><sec id="s4i" hwp:id="sec-39"><title hwp:id="title-41">Network psychophysics - overview</title><p hwp:id="p-123">To investigate network pitch behavior, we simulated a set of classic psychophysical experiments on all trained networks. The general procedure was to (1) pass each experimental stimulus through a network, (2) compute F0 discrimination thresholds or shifts in the “perceived” F0 (depending on the experiment) from network predictions, and (3) compare network results to published data from human listeners tested on the same stimulus manipulations. We selected five psychophysical experiments. These experiments are denoted A through E in the following sections to align with <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-27" hwp:rel-id="F2">Fig. 2</xref> (which contains schematics of the stimulus manipulations in each experiment). We attempted to reproduce stimuli from these studies as closely as possible, though some modifications were necessary (e.g., all stimuli were truncated to 50ms to accommodate the input length for the networks). Because the cost of running experiments on networks is negligible, networks were tested on many more (by 1 to 3 orders of magnitude) stimuli than were human participants. Human data from the original studies was obtained either directly from the original authors (Experiments A-B) or by extracting data points from published figures (Experiments C-E) using Engauge Digitizer (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://markummitchell.github.io/engauge-digitizer" ext-link-type="uri" xlink:href="http://markummitchell.github.io/engauge-digitizer" hwp:id="ext-link-4">http://markummitchell.github.io/engauge-digitizer</ext-link>). In most cases, individual subject data was not available (the original studies were performed 17 to 36 years prior to this work), so we report only across-subject means and do not include error bars for human data.</p></sec><sec id="s4j" hwp:id="sec-40"><title hwp:id="title-42">Experiment A: effect of harmonic number and phase on pitch discrimination</title><p hwp:id="p-124">Experiment A reproduced the stimulus manipulation of Bernstein and Oxenham (2005) to measure F0 discrimination thresholds as a function of lowest harmonic number and phase.</p><sec id="s4j1" hwp:id="sec-41"><title hwp:id="title-43">Stimuli</title><p hwp:id="p-125">Stimuli were harmonic complex tones, bandpass-filtered and embedded in masking noise to control the lowest audible harmonic, and whose harmonics were in sine or random phase. In the original study, the bandpass filter was kept fixed while the F0 was roved to set the lowest harmonic number. Here, to measure thresholds at many combinations of F0 and lowest harmonic number, we roved both the F0 and the location of the filter. We took the 4<sup>th</sup>-order Butterworth filter (2500 to 3500 Hz −3 dB passband) described in the original study and translated its frequency response along the frequency axis to set the lowest audible harmonic for a given stimulus. Before filtering, the level of each individual harmonic was set to 48.3 dB SPL, which corresponds to 15 dB above the masked thresholds of the original study’s normal-hearing participants. After filtering, harmonic tones were embedded in modified uniform masking noise<sup><xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-5" hwp:rel-id="ref-48">48</xref></sup>, which has a spectrum that is flat (15 dB/Hz SPL) below 600 Hz and rolls off at 2 dB/octave above 600 Hz. This noise was designed to ensure that only harmonics within the filter’s −15 dB passband were audible.</p></sec><sec id="s4j2" hwp:id="sec-42"><title hwp:id="title-44">Human experiment</title><p hwp:id="p-126">The human F0 discrimination thresholds (previously published by Bernstein and Oxenham) were measured from 5 normal-hearing participants (3 female) between the ages of 18 and 21 years old, all self-described amateur musicians with at least 5 years of experience<sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-7" hwp:rel-id="ref-25">25</xref></sup>. Each participant completed 4 adaptive tracks per condition (where a condition had a particular lowest harmonic number and either random or sine phase). Bernstein and Oxenham (2005) reported very similar F0 discrimination thresholds for two different spectral conditions (“low spectrum” with 2500 to 3500 Hz filter passband and “high spectrum” with 5000 to 7000 Hz filter passband). To simplify presentation and because our network experiment measured average thresholds across a wide range of bandpass filter positions, here we report their human data averaged across spectral condition.</p></sec><sec id="s4j3" hwp:id="sec-43"><title hwp:id="title-45">Model experiment</title><p hwp:id="p-127">The F0 discrimination experiment we ran on each network had 600 conditions corresponding to all combinations of 2 harmonic phases (sine or random), 30 lowest harmonic numbers (<italic toggle="yes">n<sub>low</sub></italic> = 1,2,3 …30), and 10 reference F0s (<italic toggle="yes">F</italic><sub>0<italic toggle="yes">,ref</italic></sub>) spaced uniformly on a logarithmic scale between 100 and 300 Hz. Within each condition, each network was evaluated on 121 stimuli with slightly different F0s (within ±6% of <italic toggle="yes">F</italic><sub>0,<italic toggle="yes">ref</italic></sub>) but the same bandpass filter. The filter was positioned such that the low frequency cutoff of its −15 dB passband was equal to the frequency of the lowest harmonic for that condition and F0 (<italic toggle="yes">n<sub>low</sub></italic> × <italic toggle="yes">F</italic><sub>0,<italic toggle="yes">ref</italic></sub>). On the grounds that human listeners likely employ a strong prior that stimuli should have fairly similar F0s within single trials of a pitch discrimination experiment, we limited network F0 predictions to fall within a one-octave range (centered at <italic toggle="yes">F</italic><sub>0<italic toggle="yes">ref</italic></sub>). We simulated a two-alternative forced choice paradigm by making all 7,260 possible pairwise comparisons between the 121 stimuli for a condition. In each trial, we asked if the network predicted a higher F0 for the stimulus in the pair with the higher F0 (i.e., if the network correctly identified which of two stimuli had a higher F0). A small random noise term was used to break ties when the network predicted the same F0 for both stimuli. We next constructed a psychometric function by plotting the percentage of correct trials as a function of %F0 difference between two stimuli. We then averaged psychometric functions across the 10 reference F0s with the same harmonic phase and lowest harmonic number. Network thresholds were thus based on 1210 stimuli (72,600 pairwise F0 discriminations) per condition. Normal cumulative distribution functions were fit to the 60 resulting psychometric functions (2 phase conditions x 30 lowest harmonic numbers). To match human F0 discrimination thresholds, which were measured with a 2-down-1-up adaptive algorithm, we defined the network F0 discrimination threshold as the F0 difference (in percent, capped at 100%) that yielded 70.7% of trials correct.</p></sec><sec id="s4j4" hwp:id="sec-44"><title hwp:id="title-46">Human-model comparison</title><p hwp:id="p-128">We quantified the similarity between human and network F0 discrimination thresholds as the correlation between vectors of analogous data points. The network vector contained 60 F0 discrimination thresholds, one for each combination of phase and lowest harmonic number. To get a human vector with 60 analogous F0 discrimination thresholds, we a) linearly interpolated the human data between lowest harmonic numbers and b) assumed that F0 discrimination thresholds were constant for lowest harmonic numbers between 1 and 5 (supported by other published data<sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-4" hwp:rel-id="ref-29">29</xref>,<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-6" hwp:rel-id="ref-48">48</xref></sup>). We then computed the Pearson correlation coefficient between log-transformed vectors of human and network thresholds.</p></sec></sec><sec id="s4k" hwp:id="sec-45"><title hwp:id="title-47">Experiment B: pitch of alternating-phase harmonic complexes</title><p hwp:id="p-129">Experiment B reproduced the stimulus manipulation of Shackleton and Carlyon (1994) to test if our networks exhibited pitch-doubling for alternating-phase harmonic stimuli.</p><sec id="s4k1" hwp:id="sec-46"><title hwp:id="title-48">Stimuli</title><p hwp:id="p-130">Stimuli consisted of consecutive harmonics (each presented at 50 dB SPL) summed together in alternating sine/cosine phase: odd-numbered harmonics in sine phase (0° offset between frequency components) and even-numbered harmonics in cosine phase (90° offset, such that components align at their peaks). As in Experiment A, these harmonic tones were bandpass-filtered and embedded in masking noise to control which harmonics were audible. The original study used pink noise and analog filters. Here, we used modified uniform masking noise and digital Butterworth filters (designed to approximate the original passbands). We generated stimuli with three different 4<sup>th</sup>-order Butterworth filters specified by their −3 dB passbands: 125 to 625 Hz (“low harmonics”), 1375 to 1875 Hz (“mid harmonics”), and 3900 to 5400 Hz (“high harmonics”). The exact harmonic numbers that are audible in each of these passbands depends on the F0. The original study used stimuli with F0s near 62.5, 125, and 250 Hz (sometimes offset by ±4% from the nominal F0 to avoid stereotyped responses). The 62.5 Hz condition was excluded here because the lowest F0 our networks could report was 80 Hz. We generated 354 stimuli with F0s near 125 Hz (120-130 Hz) and 250 Hz (240-260 Hz), in both cases uniformly sampled on a logarithmic scale, for each filter condition (2124 stimuli in total).</p></sec><sec id="s4k2" hwp:id="sec-47"><title hwp:id="title-49">Human experiment</title><p hwp:id="p-131">In the original experiment of Shackleton and Carlyon (1994), participants adjusted the F0 of a sine-phase control tone to match the pitch of a given alternating-phase test stimulus. The matched F0 provides a proxy for the perceived F0 for the test stimulus. The previously published human data were obtained from 8 normal-hearing listeners who had a wide range of musical experience. Each participant made 18 pitch matches per condition.</p></sec><sec id="s4k3" hwp:id="sec-48"><title hwp:id="title-50">Model experiment</title><p hwp:id="p-132">To simulate the human paradigm in our model, we simply took the network’s F0 prediction (within a 3-octave range centered at the stimulus F0) as the “perceived” F0 of the alternating-phase test stimulus. For each stimulus, we computed the ratio of the predicted F0 to the stimulus F0. Histograms of these frequency ratios (bin width = 2%) were generated for each of the 6 conditions (3 filter conditions × 2 nominal F0s). To simplify presentation, histograms are only shown for 2 conditions: “low harmonics” and “high harmonics”, both with F0s near 125 Hz.</p></sec><sec id="s4k4" hwp:id="sec-49"><title hwp:id="title-51">Human-model comparison</title><p hwp:id="p-133">Shackleton and Carlyon (1994) constructed histograms from their pitch matching data, pooling responses across participants (144 pitch matches per histogram). We quantified the similarity between human and network responses by measuring linear correlations between human and network histograms for the same condition. Human histograms were first re-binned to have the same 2% bin width as network histograms. Pearson correlation coefficients were computed separately for each of the 6 conditions and then averaged across conditions to give a single number quantifying human-network similarity.</p></sec></sec><sec id="s4l" hwp:id="sec-50"><title hwp:id="title-52">Experiment C: pitch of frequency-shifted complexes</title><p hwp:id="p-134">Experiment C reproduced the stimulus manipulation of Moore and Moore (2003) to test if our networks exhibited pitch shifts for frequency-shifted complexes.</p><sec id="s4l1" hwp:id="sec-51"><title hwp:id="title-53">Stimuli</title><p hwp:id="p-135">Stimuli were modifications of harmonic complex tones with consecutive harmonic frequencies in cosine phase. We imposed three different F0-dependent spectral envelopes -- as described by Moore and Moore (2003) -- on the stimuli. The first, which we termed the “low harmonics” spectral envelope, had a flat 3-harmonicwide passband centered at the 5<sup>th</sup> harmonic. The second (termed “mid harmonics”) had a flat 5-harmonic-wide passband centered at the 11<sup>th</sup> harmonic. The third (termed “high harmonics”) had a flat 5-harmonic-wide passband centered at the 16<sup>th</sup> harmonic. All three of these spectral envelopes had sloping regions flanking the flat passband. Amplitudes (relative to the flat passband) at a given frequency <italic toggle="yes">F</italic> in the sloping regions were always given by (10<sup><italic toggle="yes">x</italic></sup> - 1)/9 where <italic toggle="yes">x</italic> = 1 - |(<italic toggle="yes">F</italic> – <italic toggle="yes">F<sub>e</sub></italic>)/1.5F0| and <italic toggle="yes">F<sub>e</sub></italic> is the edge of the flat region. The amplitude was set to zero for <italic toggle="yes">x</italic> ≤ 0.</p><p hwp:id="p-136">For a given F0 and (fixed) spectral envelope, we made stimuli inharmonic by shifting every component frequency by a common offset in Hz specified as a percentage of the F0. As a concrete example, consider a stimulus with F0 = 100 Hz and the “low harmonics” spectral envelope. This stimulus contains nonzero energy at 200, 300, 400, 500, 600, and 700 Hz. Frequency-shifting this harmonic tone by +8% of the F0 results in an inharmonic tone with energy at 208, 308, 408, 508, 608, and 708 Hz. For each of the three spectral envelopes, we generated stimuli with frequency component shifts of +0, +4, +8, +12, +16, +20, and +24 %F0. For each combination of spectral envelope and frequency component shift, we generated stimuli with 3917 nominal F0s spaced log-uniformly between 80 and 480 Hz (83,391 stimuli in total). These stimuli are a superset of those used in the human experiment, which measured shifts for three F0s (100, 200, and 400 Hz) and four component shifts (+0, +8, +16, +24 %F0). As in the original study, stimuli were presented at overall levels of 70 dB SPL.</p></sec><sec id="s4l2" hwp:id="sec-52"><title hwp:id="title-54">Human experiment</title><p hwp:id="p-137">Moore and Moore (2003) used a pitch matching paradigm to allow listeners to report the perceived F0s for frequency-shifted complex tones. 5 normalhearing listeners (all musically trained) between the ages of 19 and 31 years old participated in the study. Each participant made 108 pitch matches. Moore and Moore (2003) reported quantitatively similar patterns of pitch shifts for the three F0s tested (100, 200, and 400 Hz). To simplify presentation and because we used many more F0s in the network experiment, here we present their human data averaged across F0 conditions.</p></sec><sec id="s4l3" hwp:id="sec-53"><title hwp:id="title-55">Model experiment</title><p hwp:id="p-138">For the model experiment, we again took network F0 predictions for the 83,391 frequency-shifted complexes as the “perceived” F0s. F0 predictions were restricted to a one-octave range centered at the target F0 (the F0 of the stimulus before frequency-shifting). We summarize these values as shifts in the predicted F0, which are given by (<italic toggle="yes">F</italic>0<sub><italic toggle="yes">predicted</italic></sub> - <italic toggle="yes">F</italic>0<sub><italic toggle="yes">target</italic></sub>)/F0<sub><italic toggle="yes">target</italic></sub>. These shifts are reported as the median across all tested F0s and plotted as a function of component shift and spectral envelope. To simplify presentation, results are only shown for two spectral envelopes, “low harmonics” and “high harmonics”.</p></sec><sec id="s4l4" hwp:id="sec-54"><title hwp:id="title-56">Human-model comparison</title><p hwp:id="p-139">We quantified the similarity between human and network pitch shifts as the Pearson correlation coefficient between vectors of analogous data points. The network vector contained 21 median shifts, one for each combination of spectral envelope and component shift. To obtain a human vector with 21 analogous pitch shifts, we linearly interpolated the human data between component shifts.</p></sec></sec><sec id="s4m" hwp:id="sec-55"><title hwp:id="title-57">Experiment D: pitch of complexes with individually mistuned harmonics</title><p hwp:id="p-140">Experiment D reproduced the stimulus manipulation of Moore et al. (1985) to test if our networks exhibited pitch shifts for complexes with individually mistuned harmonics.</p><sec id="s4m1" hwp:id="sec-56"><title hwp:id="title-58">Stimuli</title><p hwp:id="p-141">Stimuli were modifications of harmonic complex tones containing 12 equalamplitude harmonics (60 dB SPL per component) in sine phase. We generated such tones with F0s near 100 Hz, 200 Hz, and 400 Hz (178 F0s uniformly spaced on a logarithmic scale within ±4% of each nominal F0). Stimuli were then made inharmonic by shifting the frequency of a single component at a time. We applied +0, +1, +2, +3, +4, +6, and +8 % frequency shifts to each of the following harmonic numbers: 1, 2, 3, 4, 5, 6, and 12. In total there were 178 stimuli in each of the 147 conditions (3 nominal F0s × 7 component shifts × 7 harmonic numbers).</p></sec><sec id="s4m2" hwp:id="sec-57"><title hwp:id="title-59">Human experiment</title><p hwp:id="p-142">Moore et al. (1985) used a pitch-matching paradigm in which participants adjusted the F0 of a comparison tone to match the perceived pitch of the complex with the mistuned harmonic. Three participants (all highly experienced in psychoacoustic tasks) completed the experiment. Participants each made 10 pitch matches per condition tested. Humans were tested on 126 of the 147 conditions in the model experiment (3 nominal F0s x 7 component shifts x 6 harmonic numbers) – the conditions with a harmonic number of 12 were not included.</p></sec><sec id="s4m3" hwp:id="sec-58"><title hwp:id="title-60">Model experiment</title><p hwp:id="p-143">For the model experiment, we used the procedure described for Experiment C to measure shifts in the network’s predicted F0 for all 26,166 stimuli. Shifts were averaged across similar F0s (within ±4% of the same nominal F0) and reported as a function of component shift and harmonic number. To simplify presentation, results are only shown for F0s near 200 Hz. Results were similar for F0s near 100 and 400 Hz.</p></sec><sec id="s4m4" hwp:id="sec-59"><title hwp:id="title-61">Human-model comparison</title><p hwp:id="p-144">We compared the network’s pattern of pitch shifts to those averaged across the three participants from Moore et al. (1985). Human-model similarity was again quantified as the Pearson correlation coefficient between vectors of analogous data points. The network vector contained 147 mean shift values corresponding to the 147 conditions. Though Moore et al. (1985) did not report pitch shifts for the 12<sup>th</sup> harmonic, they explicitly stated they were unable to measure significant shifts when harmonics above the 6<sup>th</sup> were shifted. We thus inferred pitch shifts were always equal to zero for the 12<sup>th</sup> harmonic when compiling the vector of 147 analogous pitch shifts. We included this condition because some networks exhibited pitch shifts for high-numbered harmonics and we wanted our similarity metric to be sensitive to this deviation from human behavior.</p></sec></sec><sec id="s4n" hwp:id="sec-60"><title hwp:id="title-62">Experiment E: frequency discrimination with pure and transposed tones</title><p hwp:id="p-145">Experiment E measured network discrimination thresholds for pure tones and transposed tones as described by Oxenham et al. (2004).</p><sec id="s4n1" hwp:id="sec-61"><title hwp:id="title-63">Stimuli</title><p hwp:id="p-146">Transposed tones were generated by multiplying a half-wave rectified low-frequency sinusoid (the “envelope”) with a high-frequency sinusoid (the “carrier”). Before multiplication, the envelope was lowpass filtered (4<sup>th</sup> order Butterworth filter) with a cutoff frequency equal to 20% of the carrier frequency. To match the original study, we used carrier frequencies of 4000, 6350, and 10,080 Hz. For each carrier frequency, we generated 6144 transposed tones with envelope frequencies spaced uniformly on a logarithmic scale between 80 and 320 Hz. We also generated 6144 pure tones with frequencies spanning the same range. All stimuli were presented at 70 dB SPL and embedded in the same modified uniform masking noise as Experiment A. The original study embedded only the transposed tones in lowpass-filtered noise to mask distortion products. To ensure that the noise would not produce differences in the model’s performance for the two types of stimuli, we included it for pure tones as well.</p></sec><sec id="s4n2" hwp:id="sec-62"><title hwp:id="title-64">Human experiment</title><p hwp:id="p-147">Oxenham et al. (2004) reported discrimination thresholds for these same 4 conditions (transposed tones with 3 different carrier frequencies + pure tones) at 5 reference frequencies between 55 and 320 Hz. Data was collected from 4 young (&lt;30 years old) adult participants who had at least 1 hour of training on the frequency discrimination task. Discrimination thresholds were based on 3 adaptive tracks per participant per condition.</p></sec><sec id="s4n3" hwp:id="sec-63"><title hwp:id="title-65">Model experiment</title><p hwp:id="p-148">The procedure for measuring network discrimination thresholds for pure tones was analogous to the one used in Experiment A. We first took network F0 predictions (within a one-octave range centered at the stimulus frequency) for all 6144 stimuli. We then simulated a two-alternative forced choice paradigm by making pairwise comparisons between predictions for stimuli with similar frequencies (within 2.7 semitones of 5 “reference frequencies” spaced log-uniformly between 80 and 320 Hz). For each pair of stimuli, we asked if the network correctly predicted a higher F0 for the stimulus with the higher frequency. From all trials at a given reference frequency, we constructed a psychometric function plotting the percentage of correct trials as a function of percent frequency difference between the two stimuli. Normal cumulative distribution functions were fit to each psychometric function and thresholds were defined as the percent frequency difference (capped at 100%) that yielded 70.7% correct. Each threshold was based on 233,586 pairwise discriminations made between 684 stimuli. The procedure for measuring thresholds with transposed tones was identical, except that the correct answer was determined by the envelope frequency rather than the carrier frequency. Thresholds were measured separately for transposed tones with different carrier frequencies. To simplify presentation, we show transposed tone thresholds averaged across carrier frequencies (results were similar for different carrier frequencies).</p></sec><sec id="s4n4" hwp:id="sec-64"><title hwp:id="title-66">Human-model comparison</title><p hwp:id="p-149">We again quantified human-network similarity as the Pearson correlation coefficient between vectors of analogous log-transformed discrimination thresholds. Both vectors contained 20 discrimination thresholds corresponding to 5 reference frequencies × 4 stimulus classes (transposed tones with 3 different carrier frequencies + pure tones). Human thresholds were linearly interpolated to estimate thresholds at the same reference frequencies used for networks. This step was necessary because our networks were not trained to make F0 predictions below 80 Hz.</p></sec></sec><sec id="s4o" hwp:id="sec-65"><title hwp:id="title-67">Effect of stimulus level on frequency discrimination</title><p hwp:id="p-150">To investigate how phase locking in the periphery contributes to the level-robustness of pitch perception, we measured pure tone frequency discrimination thresholds from our networks as a function of stimulus level (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-10" hwp:rel-id="F5">Fig. 5D</xref>).</p><sec id="s4o1" hwp:id="sec-66"><title hwp:id="title-68">Stimuli</title><p hwp:id="p-151">We generated pure tones at 6,144 frequencies spaced uniformly on a logarithmic scale between 200 and 800 Hz. Tones were embedded in the same modified uniform masking noise as Experiment A. The signal-to-noise ratio was fixed at 20 dB and the overall stimulus levels were varied between 10 and 100 dB SPL in increments of 10 dB.</p></sec><sec id="s4o2" hwp:id="sec-67"><title hwp:id="title-69">Human experiment</title><p hwp:id="p-152">Wier et al. (1977) reported frequency discrimination thresholds for pure tones in low-level broadband noise as a function of frequency and sensation level (i.e., the amount by which the stimulus is above its detection threshold). Thresholds were measured from four participants with at least 20 hours of training on the frequency discrimination task. Participants completed four or five 2-down-1-up adaptive tracks of 100 trials per condition. Stimuli were presented at five different sensation levels: 5, 10, 20, 40, and 80 dB relative to masked thresholds in 0 dB spectrum level noise (broadband, lowpass-filtered at 10000 Hz). We averaged the reported thresholds across four test frequencies (200, 400, 600, and 800 Hz) and re-plotted them as a function of sensation level in <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-11" hwp:rel-id="F5">Fig. 5E</xref>.</p></sec><sec id="s4o3" hwp:id="sec-68"><title hwp:id="title-70">Model experiment</title><p hwp:id="p-153">We used the same procedure used in Experiments A and E to measure frequency discrimination thresholds. The simulated frequency discrimination experiment considered all possible pairings of stimuli with similar frequencies (within 2.7 semitones). Reported discrimination thresholds were pooled across all tested frequencies (200 to 800 Hz).</p></sec><sec id="s4o4" hwp:id="sec-69"><title hwp:id="title-71">Human-model comparison</title><p hwp:id="p-154">Because the human results were reported in terms of sensation level rather than SPL, we did not compute a quantitative measure of the match between model and human results, and instead plot the results side-by-side for qualitative comparison.</p></sec></sec><sec id="s4p" hwp:id="sec-70"><title hwp:id="title-72">Auditory nerve manipulations - overview</title><p hwp:id="p-155">The general procedure for investigating the dependence of network behavior on aspects of the auditory nerve representation was to (1) modify the auditory nerve model, (2) retrain networks (starting from a random initialization) on modified auditory nerve representations of the same natural sounds dataset, and (3) simulate psychophysical experiments on the trained networks using modified auditory nerve representations of the same test stimuli. We used this approach to investigate whether a biologically-constrained cochlea is necessary to obtain human-like pitch behavior and to evaluate the dependence of network pitch behavior on both temporal and “place” information in the auditory nerve representation. The only exception to this general procedure was in the experiment that tested the effect of flattening the excitation pattern, which was performed on networks that were trained on normal auditory nerve representations (see below).</p></sec><sec id="s4q" hwp:id="sec-71"><title hwp:id="title-73">Replacing the hardwired cochlear model with learnable filters</title><p hwp:id="p-156">We replaced the hardwired auditory nerve model with a convolutional layer whose weights could be optimized alongside the rest of the DNN (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-10" hwp:rel-id="F4">Fig. 4</xref>). The convolutional layer consisted of 100 one-dimensional filter kernels (each with 801 taps) that operated directly on 32 kHz audio, applied using “valid” convolution. The audio input to the network was 75ms in duration such that the valid output of convolution was 50ms, as in the hardwired cochlear representation. Outputs from the 100 filters were stacked, halfwave rectified, and then resampled to 20 kHz, resulting in a first-layer representation with 100 “nerve fibers” and 1000 timesteps to match the size and temporal resolution of the hardwired cochlear representations. We separately trained the 10 best network architectures from the original architecture search with this learnable “cochlear” layer. The best frequency of a learned filter was determined after training from the maximum value of its transfer function.</p></sec><sec id="s4r" hwp:id="sec-72"><title hwp:id="title-74">Manipulating fine timing information in the auditory nerve</title><p hwp:id="p-157">We modified the upper frequency limit of phase locking in the auditory nerve by adjusting the cutoff frequency of the inner hair cell lowpass filter within the auditory nerve model. By default, the lowpass characteristics of the inner hair cell’s membrane potential are modeled as a 7<sup>th</sup> order filter with a cutoff frequency of 3000 Hz<sup><xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-4" hwp:rel-id="ref-42">42</xref></sup>. We trained and tested networks with this cutoff frequency set to 50, 250, 1000, 3000, 6000, and 9000 Hz. In each of these cases, the sampling rate of the peripheral representation used as input to the networks was 20 kHz so that spike-timing information would not be limited by the Nyquist frequency.</p><p hwp:id="p-158">When the inner hair cell cutoff frequency is set to 50 Hz, virtually all temporal information in the short-duration stimuli we used was eliminated, leaving only place information (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-12" hwp:rel-id="F5">Fig. 5A</xref>). To control for the possibility that the performance characteristics of networks trained on such representations could be limited by the number of model nerve fibers (set to 100 for most of our experiments), we repeated this manipulation with 1000 auditory nerve fibers (characteristic frequencies again spaced uniformly on an ERB-number scale between 125 Hz and 14000 Hz). To keep the network architecture constant, we reduced the sampling rate to 2 kHz (which for the 50 Hz hair cell cutoff preserved all stimulus-related information), yielding peripheral representations that were 1000-fiber by 100-timestep arrays of instantaneous firing rates. We then simply transposed the nerve fiber and time dimensions so that networks still operated on 100-by-1000 inputs, allowing us to use the same network architectures as in all other training conditions. Note that by transposing the input representation, we effectively changed the orientation of the convolutional filter kernels. Kernels that were previously long in the time dimension and short in the nerve fiber dimension became short in the time dimension and long in the nerve fiber dimension. We saw this as desirable as it allowed us to rule out the additional possibility that the performance characteristics of networks with lower limits of phase locking were due to convolutional kernel shapes that were optimized for input representations with high temporal fidelity and thus perhaps less suited for extracting place information (which requires pooling information across nerve fibers).</p><p hwp:id="p-159">To more closely examine how the performance with degraded phase locking (i.e., the 50 Hz inner hair cell cutoff frequency condition) might be limited by the number of model nerve fibers, we also generated peripheral representations with either 100, 250, or 500 nerve fibers (with characteristics frequencies uniformly spaced on an ERB-number scale between 125 Hz and 14000 Hz in each case). To keep the network’s input size fixed at 100-by-1000 (necessary to use the same network architecture), we transposed the input array, again using 100 timesteps instead of 1000 (sampled at 2 kHz), and upsampled the frequency (nerve fiber) dimension to 1000 via linear interpolation. In this way the input dimensionality was preserved across conditions even though the information was limited by the original number of nerve fibers. Median %F0 error on the validation set and discrimination thresholds and were measured for networks trained and tested with each of these peripheral representations (<xref ref-type="fig" rid="figS4" hwp:id="xref-fig-13-3" hwp:rel-id="F13">Supplementary Fig. 4</xref>).</p></sec><sec id="s4s" hwp:id="sec-73"><title hwp:id="title-75">Eliminating place cues by flattening the excitation pattern</title><p hwp:id="p-160">To test if our trained networks made use of peaks and valleys in the time-averaged excitation pattern (which provide “place” cues to F0), we tested networks on nerve representations with artificially flattened excitation patterns. Nerve representations were flattened by separately scaling each frequency channel of the nerve representation to have the same time-averaged response. Each row (nerve fiber) of the nerve representation was divided by its time-averaged firing rate and multiplied by the mean firing rate across all rows, yielding an excitation-flattened nerve representation with the same mean firing rate as the original. This manipulation was separately applied to each psychophysical stimulus.</p></sec><sec id="s4t" hwp:id="sec-74"><title hwp:id="title-76">Manipulating cochlear filter bandwidths</title><p hwp:id="p-161">Cochlear filter bandwidths in the auditory nerve model were set based on estimates of human frequency tuning from otoacoustic and behavioral experiments<sup><xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-3" hwp:rel-id="ref-53">53</xref></sup>. We modified the frequency tuning to be two times narrower and two times broader than these human estimates by scaling the filter bandwidths by 0.5 and 2.0, respectively.</p><p hwp:id="p-162">To investigate the importance of the frequency scaling found in the cochlea, we also generated a peripheral representation with linearly spaced cochlear filters. The characteristic frequencies of 100 model nerve fibers were linearly spaced between 125 Hz and 8125 Hz and the 10-dB-down bandwidth of each cochlear filter was set to 80 Hz. This bandwidth (which is approximately equal to that of a “human” model fiber with 400 Hz characteristic frequency) was chosen to be as narrow as possible without introducing frequency “gaps” between adjacent cochlear filters.</p><p hwp:id="p-163">To verify that these manipulations had the anticipated effects, we measured tuning curves (detection thresholds as a function of frequency) for simulated nerve fibers with characteristic frequencies of 250, 500, 1000, 2000, 4000 Hz (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-9" hwp:rel-id="F6">Fig. 6A&amp;B</xref>). Mean firing rate responses were computed for each fiber to 50ms pure tones with frequencies between 125 Hz and 8000 Hz. Thresholds were defined as the minimum sound level required to increase the fiber’s mean firing rate response 10% above its spontaneous rate (i.e., dB SPL required for 77 spike/s).</p></sec><sec id="s4u" hwp:id="sec-75"><title hwp:id="title-77">Sound statistics manipulations - overview</title><p hwp:id="p-164">The general procedure for investigating the dependence of network behavior on sound statistics was to (1) modify the sounds in training dataset, (2) retrain networks (starting from a random initialization) on auditory nerve representations of the modified training dataset, and (3) simulate psychophysical experiments on trained networks, always using the same test stimuli.</p></sec><sec id="s4v" hwp:id="sec-76"><title hwp:id="title-78">Training on filtered natural sounds</title><p hwp:id="p-165">We generated lowpass and highpass versions of our natural sounds training dataset by applying randomly-generated lowpass or highpass Butterworth filters to every speech and instrument sound excerpt. For the lowpass-filtered dataset, 3-dB-down filter cutoff frequencies were drawn uniformly on a logarithmic scale between 500 and 5000 Hz. For the highpass-filtered dataset, cutoff frequencies were drawn uniformly from a logarithmic scale between 1000 and 10000 Hz. The order of each filter was drawn uniformly from 1 to 5 and all filters were applied twice, once forward and once backwards, to eliminate phase shifts. Filtered speech and instrument sounds were then combined with the unmodified background noise signals used in the original dataset (SNRs drawn uniformly from −10 to +10 dB).</p></sec><sec id="s4w" hwp:id="sec-77"><title hwp:id="title-79">Training on spectrally matched and anti-matched synthetic tones</title><p hwp:id="p-166">To investigate the extent to which network pitch behavior could be explained by low-order spectral statistics of our natural sounds dataset, we generated a dataset of 2.1 million synthetic stimuli with spectral statistics matched to those measured from our primary dataset. STRAIGHT<sup><xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-2" hwp:rel-id="ref-75">75</xref></sup> was used to measure the spectral envelope (by averaging the estimated filter spectrogram across time) of every speech and instrument sound in our dataset. We then measured the mean and covariance of the first 13 Mel-frequency cepstral coefficients (MFCCs), defining a multivariate Gaussian. We sampled new spectral envelopes from this distribution by drawing MFCC coefficients and inverting them to produce a spectral envelope. These enveloped were imposed (via multiplication in the frequency domain) on harmonic complex tones with F0s sampled to uniformly populate the 700 log-spaced F0 bins in the network’s classification layer. Before envelope imposition, tones initially contained all harmonics up to 16 kHz in cosine phase, with equal amplitudes.</p><p hwp:id="p-167">To generate a synthetic dataset with spectral statistics that deviate considerably from those measured from our primary dataset, we simply multiplied the mean of the fitted multivariate Gaussian (a vector of 13 MFCCs) by negative one, which inverts the mean spectral envelope. Spectral envelopes sampled from the distribution defined by the negated mean (and unaltered covariance matrix) were imposed on 2.1 million harmonic complex tones to generate an “anti-matched” synthetic tones dataset.</p><p hwp:id="p-168">Both the matched and anti-matched synthetic tones were embedded in synthetic noise spectrally matched to the background noise in our primary natural sounds dataset. The procedure for synthesizing spectrally-matched noise was analogous to the one used to generate spectrally-matched tones, except that we estimated the spectral envelope using the power spectrum. We measured the power spectrum of every background noise clip in our primary dataset, computed the mean and covariance of the first 13 MFCCs, and imposed spectral envelopes sampled from the resulting multivariate Gaussian on white noise via multiplication in the frequency domain. Synthetic tones and noise were combined with SNRs drawn uniformly from −10 to +10 dB and overall stimulus presentation levels were drawn uniformly from 30 to 90 dB SPL.</p></sec><sec id="s4x" hwp:id="sec-78"><title hwp:id="title-80">Training on speech and music separately</title><p hwp:id="p-169">We generated speech-only and music-only training datasets by selectively sampling from the same source libraries used to populate the combined dataset. Due to the lack of speech clips in our source libraries with high F0s, we decided to limit both datasets to F0s between 80 and 450 Hz (spanning 480 of 700 F0 bins). This ensured that differences between networks trained on speech or music would not be due to differences in the F0 range. The composition of the speech-only dataset was:
<list list-type="bullet" hwp:id="list-6"><list-item hwp:id="list-item-14"><p hwp:id="p-170">F0 bins between 80 Hz and 320 Hz</p><list list-type="bullet" hwp:id="list-7"><list-item hwp:id="list-item-15"><p hwp:id="p-171">100% adult speech (2000 SWC and 1000 WSJ clips per bin)</p></list-item></list></list-item><list-item hwp:id="list-item-16"><p hwp:id="p-172">F0 bins between 320 Hz and 450 Hz</p><list list-type="bullet" hwp:id="list-8"><list-item hwp:id="list-item-17"><p hwp:id="p-173">100% child speech (1500 CSLU and 1500 CMU clips per bin)</p></list-item></list></list-item></list></p><p hwp:id="p-174">The composition of our music-only dataset was:
<list list-type="bullet" hwp:id="list-9"><list-item hwp:id="list-item-18"><p hwp:id="p-175">F0 bins between 80 Hz and 450 Hz</p><list list-type="bullet" hwp:id="list-10"><list-item hwp:id="list-item-19"><p hwp:id="p-176">100% instrumental music (2000 NSynth and 1000 RWC clips per bin)</p></list-item></list></list-item></list></p><p hwp:id="p-177">Stimuli in both datasets were added to background noise clips sampled from the same sources used for the combined dataset (SNRs drawn uniformly from −10 to +10 dB).</p></sec><sec id="s4y" hwp:id="sec-79"><title hwp:id="title-81">Training on natural sounds with reduced background noise</title><p hwp:id="p-178">To train networks in a low-noise environment, we regenerated our natural sounds training dataset with SNRs drawn uniformly from +10 to +30 dB rather than −10 to +10 dB. For the noiseless case, we entirely omitted the addition of background noise to the speech and instruments sounds before training. To ensure F0 discrimination thresholds measured from networks trained with reduced background noise would not be limited by masking noise in the psychophysical stimuli, we evaluated these networks on noiseless versions of the psychophysical stimuli (Experiments A and E). The amplitudes of harmonics that were masked by noise in the original stimuli (i.e., harmonics inaudible to human listeners in the original studies) were set to zero in the noiseless stimuli. When these networks were evaluated on psychophysical stimuli that did include masking noise, F0 discrimination behavior was qualitatively similar, but absolute thresholds were elevated relative to networks that were trained on the −10 to +10 dB SNR dataset.</p></sec><sec id="s4z" hwp:id="sec-80"><title hwp:id="title-82">Network neurophysiology</title><p hwp:id="p-179">We simulated electrophysiological recordings and functional imaging experiments on our trained networks by examining the internal activations of networks in response to stimuli. We treated units in the network layers as model “neurons” and looked at their tuning using their average activations across time to different stimuli. We measured tuning properties using equal-amplitude sine-phase harmonic complex tones (45 dB SPL per frequency component) in threshold equalizing noise (10 dB SPL per ERB). We used a total of 11,520 stimuli: 5 unique harmonic compositions (pure tones or successive harmonics 1-9, 2-10, 4-12, or 6-14) × 2304 unique F0s (logarithmically-spaced between 80 and 640 Hz). For each unit, we constructed an F0 tuning curve for each harmonic composition by averaging activations to stimuli within the same F0 classification bin (i.e., within 1/16 semitone bins). Tuning curves were normalized separately for each unit by dividing by the unit’s maximum response across the full stimulus set. Units that produced a response of zero to all of the test stimuli were excluded from analysis (&lt;1% of units).</p><p hwp:id="p-180">As a measure of the strength of F0 tuning in a single model unit, we computed the mean Pearson correlation coefficient between the pure tone (frequency) tuning curve and each of the complex tone tuning curves. A perfectly F0-tuned unit should selectively respond to pure and complex tones of a preferred F0 independent of harmonic composition (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-12" hwp:rel-id="F9">Fig. 9A</xref>), yielding a high mean correlation coefficient. We measured the F0 tuning correlation of each unit in each layer (all ReLU activations following convolutional layers and fully connected layers) of the 10 best-performing network architectures. The F0 tuning strength of a network layer was computed by averaging this metric across all units in the layer.</p><p hwp:id="p-181">To test if the observed F0 tuning depended on the harmonicity of the stimuli, we repeated this analysis with complex tones made inharmonic by randomly shifting component frequencies with a fixed jitter pattern<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-5" hwp:rel-id="ref-20">20</xref></sup>. The jitter pattern allowed for each individual component (after the F0 component) to be shifted by up to ±50% of the F0. Jitter values for each component were drawn uniformly from −50% to +50% with rejection sampling to ensure adjacent components were separated by at least 30 Hz to minimize salient differences in beating. Each component’s frequency was the original harmonic’s frequency plus the jitter value multiplied by the nominal F0. Like harmonic tones, inharmonic tones generated in this way have frequency components that increase in spacing as the nominal F0 is increased, but unlike harmonic tones they lack a fundamental frequency in the range of audible pitch (i.e., above ~30Hz<sup><xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">86</xref></sup>). Empirically, human perceptual signatures of F0-based pitch are disrupted by inharmonicity<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-6" hwp:rel-id="ref-20">20</xref>,<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-3" hwp:rel-id="ref-55">55</xref>,<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">57</xref></sup>, making it a way to distinguish human-like representations of F0 from coarser representations of frequency spacing. The same jitter pattern (i.e., the same mapping of nominal harmonic numbers to jitter value) was applied to all stimuli regardless of F0 and harmonic composition. As in the F0-tuning analysis with harmonic tones, we measured the average strength of nominal F0 tuning for each layer in the 10 networks. To ensure results were not unduly biased by a single random jitter pattern, the analysis was repeated five times with different random seeds. F0 tuning summary metrics reported in <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-13" hwp:rel-id="F9">Fig. 9C</xref> are averaged across these five random seeds.</p><p hwp:id="p-182">We measured population responses as a function of lowest harmonic number (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-14" hwp:rel-id="F9">Fig. 9D</xref>) using a superset of the harmonic complex tones used to measure F0 tuning. The dataset was expanded to include all complexes containing 9 successive harmonics, with lowest harmonic numbers 1 to 15 (e.g., 1-9, 2-10, 3-11, … 15-26). We first identified the best F0 (i.e., the F0 producing the largest normalized mean response across all lowest harmonic numbers) for each of the 700 units in each network’s final fully connected layer. We then constructed lowest-harmonic-number tuning curves by taking responses to stimuli at the best F0 with lowest harmonic numbers 1 to 15. These tuning curves were averaged across units to give the population response.</p><p hwp:id="p-183">We qualitatively compared the network’s population response to those of pitch-selective neurons in marmoset auditory cortex<sup><xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-4" hwp:rel-id="ref-28">28</xref></sup> and pitch-selective voxels in human auditory cortex<sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-5" hwp:rel-id="ref-29">29</xref></sup>. Bendor and Wang used single-unit electrophysiology to measure the spiking rates of 50 pitch-selective neurons (from 3 marmosets) in response to complexes containing 3 to 9 consecutive harmonics in either cosine or Schroeder phase. Recordings were repeated a minimum of 10 times per stimulus condition. Norman-Haignere and colleagues measured fMRI responses to bandpass-filtered sine-phase harmonic complex tones and frequency-matched noise. The 12 participants (4 male, 8 female, ages 21-28) were non-musicians with normal hearing. Pitch-selective voxels were defined as those whose responses were larger for complex tones than for frequency-matched noise. We re-plotted data extracted from figures in both published studies.</p></sec><sec id="s4za" hwp:id="sec-81"><title hwp:id="title-83">Statistics - analysis of human data</title><p hwp:id="p-184">Human data was scanned from original figures or provided by the authors of the original papers. We did not have access to data from individual human participants, and so did not plot error bars on the graphs of human results.</p></sec><sec id="s4zb" hwp:id="sec-82"><title hwp:id="title-84">Statistics - analysis of human-model comparison metrics</title><p hwp:id="p-185">Human-model comparison metrics were computed separately for each psychophysical experiment (as described in the above sections on each experiment and its analysis) and for each of the 400 networks trained in our architecture search. To test if networks with better performance on the F0 estimation training task produce better matches to human psychophysical behavior, we computed the Pearson correlation between validation set accuracies and human-model comparison metrics for each experiment (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3</xref>, right-most column).</p><p hwp:id="p-186">To test if “deep” networks (defined here as networks with more than one convolutional layer) tended to produce better performance on the training task than the networks with just one convolutional layer (<xref ref-type="fig" rid="figS3" hwp:id="xref-fig-12-4" hwp:rel-id="F12">Supplementary Fig. 3</xref>), we performed a Wilcoxon rank-sum test comparing the validation set accuracies of the 54 single-convolutional-layer networks to those of the other 346 networks. To test if the “deep” networks tended to produce better matches to human behavior we performed a Wilcoxon rank-sum test comparing the human-model similarity metrics of the 54 single-convolutional-layer networks to those of the other 346 networks. To obtain a single human-model similarity score per network for this test, we pooled metrics across the five main psychophysical experiments. This was accomplished by first rank ordering the human-model similarity metrics of all networks within experiments and then averaging ranks across experiments.</p><p hwp:id="p-187">These human-model similarity metrics were used to analyze two other experiments (in all others we used more fine-grained analysis of best thresholds and transition points, described below). To assess the statistical significance of changes in human-model similarity when networks were optimized with a learned “cochlea” (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-11" hwp:rel-id="F4">Fig. 4</xref>) or in the absence of background noise, we compared metrics measured from 10 networks trained per condition. The networks had 10 different architectures, corresponding to the 10 best-performing architectures identified in our search (<xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Supplementary Table 1</xref>). We performed two-sample t-tests (each sample containing results from the 10 independently trained networks) to compare human-model comparison metrics between training conditions. Effect sizes were quantified as Cohen’s d and reported for all such tests that indicated statistically significant differences. Because human-model similarity metrics were bounded between −1 and 1, we passed the metrics through an inverse normal cumulative distribution function before performing t-tests. All t-tests and ranksum tests were two-sided.</p></sec><sec id="s4zc" hwp:id="sec-83"><title hwp:id="title-85">Statistics - analysis of best thresholds and transition points</title><p hwp:id="p-188">One of the key signatures of human pitch perception is that listeners are very good at making fine F0 discriminations (thresholds typically below 1%) if and only if stimuli contain low-numbered harmonics. F0 discrimination thresholds increase by an order of magnitude for stimuli containing only higher-numbered harmonics<sup><xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-7" hwp:rel-id="ref-43">43</xref>,<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-7" hwp:rel-id="ref-48">48</xref></sup>. To assess the effect of altered cochlear input or training sound statistics (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-13" hwp:rel-id="F5">Figs. 5</xref>, <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-10" hwp:rel-id="F6">6</xref>, <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-13" hwp:rel-id="F7">7</xref>), we thus focused on two measures: first, the absolute F0 discrimination acuity of our model when all low-numbered harmonics were present (“best threshold”), and second, the harmonic number at which discrimination ability transitioned from good to poor (“transition point”). In each case we used two-sample t-tests, comparing either the F0 discrimination thresholds (log-transformed) for tones containing the first harmonic, or the lowest harmonic number where thresholds first exceeded 1%. In each case we compared results for networks with different auditory nerve models or training sets. To quantify effect sizes, Cohen’s d is reported for all two-sample t-tests that indicated statistically significant differences.</p></sec><sec id="s4zd" hwp:id="sec-84"><title hwp:id="title-86">Statistics - ANOVAs on discrimination thresholds</title><p hwp:id="p-189">We performed analyses of variance (ANOVAs) on log-transformed F0 discrimination thresholds to help satisfy the assumptions of equal variance and normality (normality was evaluated by eye). Mixed model ANOVAs were performed with training conditions (peripheral model and training set manipulations) as between-subject factors and psychophysical stimulus parameters (lowest harmonic number and stimulus presentation level) as within-subject factors. The specific pairings of these different factors were: stimulus presentation level vs. auditory nerve phase locking cutoff (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-14" hwp:rel-id="F5">Fig. 5E</xref>), lowest harmonic number vs. training set spectral statistics (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-14" hwp:rel-id="F7">Fig. 7C</xref>), and lowest harmonic number vs. training set noise level (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-8" hwp:rel-id="F8">Fig. 8</xref>). We also performed a repeated-measures ANOVA to test for a main effect of lowest harmonic number on population responses in the network’s last layer (<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-15" hwp:rel-id="F9">Fig. 9C</xref>). F-statistics, p-values, and <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-18"><inline-graphic xlink:href="389999v4_inline10.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> are reported for main effects and interactions of interest. Greenhouse-Geisser corrections were applied in all cases where Mauchly’s test indicated the assumption of sphericity had been violated.</p></sec></sec></body><back><sec hwp:id="sec-85"><title hwp:id="title-87">DATA AVAILABILITY</title><p hwp:id="p-190">The Wall Street Journal (LDC93S6A), CMU Kids Corpus (LDC97S63), and CSLU Kids Speech (LDC2007S18) audio datasets used in this study are available from the Linguistic Data Consortium (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.ldc.upenn.edu" ext-link-type="uri" xlink:href="https://www.ldc.upenn.edu" hwp:id="ext-link-5">https://www.ldc.upenn.edu</ext-link>). The Spoken Wikipedia Corpora audio dataset is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://nats.gitlab.io/swc/" ext-link-type="uri" xlink:href="https://nats.gitlab.io/swc/" hwp:id="ext-link-6">https://nats.gitlab.io/swc/</ext-link>. The RWC Music Database is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://staff.aist.go.jp/m.goto/RWC-MDB/" ext-link-type="uri" xlink:href="https://staff.aist.go.jp/m.goto/RWC-MDB/" hwp:id="ext-link-7">https://staff.aist.go.jp/m.goto/RWC-MDB/</ext-link>. The pitch datasets we compiled from these publicly available corpora, along with the psychophysical test stimulus sets, are available at: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/msaddler/pitchnet" ext-link-type="uri" xlink:href="https://github.com/msaddler/pitchnet" hwp:id="ext-link-8">https://github.com/msaddler/pitchnet</ext-link>.</p></sec><sec hwp:id="sec-86"><title hwp:id="title-88">CODE AVAILABILITY</title><p hwp:id="p-191">Source code for the Bruce et al. (2018) auditory nerve model is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.ece.mcmaster.ca/~ibruce/zbcANmodel/zbcANmodel.htm" ext-link-type="uri" xlink:href="https://www.ece.mcmaster.ca/~ibruce/zbcANmodel/zbcANmodel.htm" hwp:id="ext-link-9">https://www.ece.mcmaster.ca/~ibruce/zbcANmodel/zbcANmodel.htm</ext-link>. We developed a Python wrapper around the model, which supports flexible manipulation of cochlear filter bandwidths and the upper limit of phase locking. This wrapper is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/msaddler/bez2018model" ext-link-type="uri" xlink:href="https://github.com/msaddler/bez2018model" hwp:id="ext-link-10">https://github.com/msaddler/bez2018model</ext-link>. Code to implement and analyze our deep neural network pitch models (including trained network weights) is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/msaddler/pitchnet" ext-link-type="uri" xlink:href="https://github.com/msaddler/pitchnet" hwp:id="ext-link-11">https://github.com/msaddler/pitchnet</ext-link>.</p></sec><ack hwp:id="ack-1"><title hwp:id="title-89">ACKNOWLEDGMENTS</title><p hwp:id="p-192">We thank Alex Durango and Jenelle Feather for providing the AudioSet background noise stimuli, Jenelle Feather and Andrew Francl for contributions to a shared codebase used in this project, John Cohn for assistance with computing resources, Josh Bernstein, Andrew Oxenham, Trevor Shackleton, and Bob Carlyon for sharing human psychophysics data and stimuli, Sam Norman-Haignere and the McDermott lab for helpful feedback on the manuscript, and Ian Bruce, Laurel Carney, Bertrand Delgutte, Oded Barzelay, Brian Moore, and Hideki Kawahara for helpful discussions. Work supported by NSF grant BCS-1634050 and NIH grant R01DC017970.</p></ack><sec hwp:id="sec-87"><title hwp:id="title-90">AUTHOR CONTRIBUTIONS</title><p hwp:id="p-193">All authors conceived the project and designed the experiments. R.G. ran and analyzed pilot versions of the experiments. M.R.S. ran the experiments described in the paper, analyzed the data, and made the figures. M.R.S. and J.H.M. drafted the manuscript. All authors edited the manuscript.</p></sec><sec hwp:id="sec-88"><title hwp:id="title-91">COMPETING INTERESTS</title><p hwp:id="p-194">The authors declare no competing interests.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-92">REFERENCES</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="book" citation-type="book" ref:id="2020.11.19.389999v4.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Wandell B. A."><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> <source hwp:id="source-1">Foundations of Vision</source>. (<publisher-name>Sinauer Associates</publisher-name>, <year>1995</year>).</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Hildebrand J. G."><surname>Hildebrand</surname>, <given-names>J. G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Shepherd G. M."><surname>Shepherd</surname>, <given-names>G. M.</given-names></string-name> <article-title hwp:id="article-title-2">Mechanisms of olfactory discrimination: converging evidence for common principles across phyla</article-title>. <source hwp:id="source-2">Annu. Rev. Neurosci</source>. <volume>20</volume>, <fpage>595</fpage>–<lpage>631</lpage> (<year>1997</year>).</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Yin R. K."><surname>Yin</surname>, <given-names>R. K.</given-names></string-name> <article-title hwp:id="article-title-3">Looking at upside-down faces</article-title>. <source hwp:id="source-3">J. Exp. Psychol</source>. <volume>81</volume>, <fpage>141</fpage>–<lpage>145</lpage> (<year>1969</year>).</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Palmer A. R."><surname>Palmer</surname>, <given-names>A. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Russell I. J."><surname>Russell</surname>, <given-names>I. J.</given-names></string-name> <article-title hwp:id="article-title-4">Phase-locking in the cochlear nerve of the guinea-pig and its relation to the receptor potential of inner hair-cells</article-title>. <source hwp:id="source-4">Hear. Res</source>. <volume>24</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>1986</year>).</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Attneave F."><surname>Attneave</surname>, <given-names>F.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Olson R. K."><surname>Olson</surname>, <given-names>R. K.</given-names></string-name> <article-title hwp:id="article-title-5">Pitch as a medium: a new approach to psychophysical scaling</article-title>. <source hwp:id="source-5">Am. J. Psychol</source>. <volume>84</volume>, <fpage>147</fpage>–<lpage>166</lpage> (<year>1971</year>).</citation></ref><ref id="c6" hwp:id="ref-6"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Javel E."><surname>Javel</surname>, <given-names>E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mott J. B."><surname>Mott</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-6">Physiological and psychophysical correlates of temporal processes in hearing</article-title>. <source hwp:id="source-6">Hear. Res</source>. <volume>34</volume>, <fpage>275</fpage>–<lpage>294</lpage> (<year>1988</year>).</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Jacoby N."><surname>Jacoby</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-7">Universal and non-universal features of musical pitch perception revealed by singing</article-title>. <source hwp:id="source-7">Curr. Biol</source>. <volume>29</volume>, <fpage>3229</fpage>–<lpage>3243.e12</lpage> (<year>2019</year>).</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Geisler W. S."><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> <article-title hwp:id="article-title-8">Contributions of ideal observer theory to vision research</article-title>. <source hwp:id="source-8">Vision Res</source>. <volume>51</volume>, <fpage>771</fpage>–<lpage>781</lpage> (<year>2011</year>).</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Heinz M. G."><surname>Heinz</surname>, <given-names>M. G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Colburn H. S."><surname>Colburn</surname>, <given-names>H. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Carney L. H."><surname>Carney</surname>, <given-names>L. H.</given-names></string-name> <article-title hwp:id="article-title-9">Evaluating auditory performance limits: I. One-parameter discrimination using a computational model for the auditory nerve</article-title>. <source hwp:id="source-9">Neural Comput</source>. <volume>13</volume>, <fpage>2273</fpage>–<lpage>2316</lpage> (<year>2001</year>).</citation></ref><ref id="c10" hwp:id="ref-10"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Weiss Y."><surname>Weiss</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli E. P."><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Adelson E. H."><surname>Adelson</surname>, <given-names>E. H.</given-names></string-name> <article-title hwp:id="article-title-10">Motion illusions as optimal percepts</article-title>. <source hwp:id="source-10">Nat. Neurosci</source>. <volume>5</volume>, <fpage>598</fpage>–<lpage>604</lpage> (<year>2002</year>).</citation></ref><ref id="c11" hwp:id="ref-11"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Burge J."><surname>Burge</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Geisler W. S."><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> <article-title hwp:id="article-title-11">Optimal defocus estimation in individual natural images</article-title>. <source hwp:id="source-11">Proc. Natl. Acad. Sci</source>. <volume>108</volume>, <fpage>16849</fpage>–<lpage>16854</lpage> (<year>2011</year>).</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Girshick A. R."><surname>Girshick</surname>, <given-names>A. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Landy M. S."><surname>Landy</surname>, <given-names>M. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Simoncelli E. P."><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> <article-title hwp:id="article-title-12">Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>. <source hwp:id="source-12">Nat. Neurosci</source>. <volume>14</volume>, <fpage>926</fpage>–<lpage>932</lpage> (<year>2011</year>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="LeCun Y."><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G."><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-13">Deep learning</article-title>. <source hwp:id="source-13">Nature</source> <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage> (<year>2015</year>).</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Kell A. J."><surname>Kell</surname>, <given-names>A. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-14">Deep neural network models of sensory systems: windows onto the role of task constraints</article-title>. <source hwp:id="source-14">Curr. Opin. Neurobiol</source>. <volume>55</volume>, <fpage>121</fpage>–<lpage>132</lpage> (<year>2019</year>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>15.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Francl A."><surname>Francl</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-15">Deep neural network models of sound localization reveal how perception is adapted to real-world environments</article-title>. <source hwp:id="source-15">Nat. Hum. Behav</source>. (In Press).</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2 xref-ref-16-3"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Yamins D. L. K."><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="DiCarlo J. J."><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> <article-title hwp:id="article-title-16">Using goal-driven deep learning models to understand sensory cortex</article-title>. <source hwp:id="source-16">Nat. Neurosci</source>. <volume>19</volume>, <fpage>356</fpage>–<lpage>365</lpage> (<year>2016</year>).</citation></ref><ref id="c17" hwp:id="ref-17"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Jozwik K. M."><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Storrs K. R."><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mur M."><surname>Mur</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-17">Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</article-title>. <source hwp:id="source-17">Front. Psychol</source>. <volume>8</volume>, (<year>2017</year>).</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2 xref-ref-18-3"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Kell A. J. E."><surname>Kell</surname>, <given-names>A. J. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamins D. L. K."><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shook E. N."><surname>Shook</surname>, <given-names>E. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norman-Haignere S. V."><surname>Norman-Haignere</surname>, <given-names>S. V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-18">A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title>. <source hwp:id="source-18">Neuron</source> <volume>98</volume>, <fpage>630</fpage>–<lpage>644.e16</lpage> (<year>2018</year>).</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2"><label>19.</label><citation publication-type="book" citation-type="book" ref:id="2020.11.19.389999v4.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="de Cheveigné A."><surname>de Cheveigné</surname>, <given-names>A.</given-names></string-name> <chapter-title>Pitch Perception Models</chapter-title>. in <source hwp:id="source-19">Pitch: Neural Coding and Perception</source> (eds. <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Plack C. J."><surname>Plack</surname>, <given-names>C. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fay R. R."><surname>Fay</surname>, <given-names>R. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Popper A. N."><surname>Popper</surname>, <given-names>A. N.</given-names></string-name></person-group>) <fpage>169</fpage>–<lpage>233</lpage> (<publisher-name>Springer</publisher-name>, <year>2005</year>). doi:<pub-id pub-id-type="doi">10.1007/0-387-28958-5_6</pub-id>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2 xref-ref-20-3 xref-ref-20-4 xref-ref-20-5 xref-ref-20-6"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="McPherson M. J."><surname>McPherson</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-19">Diversity in pitch perception revealed by task dependence</article-title>. <source hwp:id="source-20">Nat. Hum. Behav</source>. <volume>2</volume>, <fpage>52</fpage>–<lpage>66</lpage> (<year>2018</year>).</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Moore B. C. J."><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glasberg B. R."><surname>Glasberg</surname>, <given-names>B. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Peters R. W."><surname>Peters</surname>, <given-names>R. W.</given-names></string-name> <article-title hwp:id="article-title-20">Relative dominance of individual partials in determining the pitch of complex tones</article-title>. <source hwp:id="source-21">J. Acoust. Soc. Am</source>. <volume>77</volume>, <fpage>1853</fpage>–<lpage>1860</lpage> (<year>1985</year>).</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2 xref-ref-22-3 xref-ref-22-4 xref-ref-22-5"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Shackleton T. M."><surname>Shackleton</surname>, <given-names>T. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Carlyon R. P."><surname>Carlyon</surname>, <given-names>R. P.</given-names></string-name> <article-title hwp:id="article-title-21">The role of resolved and unresolved harmonics in pitch perception and frequency modulation discrimination</article-title>. <source hwp:id="source-22">J. Acoust. Soc. Am</source>. <volume>95</volume>, <fpage>3529</fpage>–<lpage>3540</lpage> (<year>1994</year>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Moore G. A."><surname>Moore</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Moore B. C. J."><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name> <article-title hwp:id="article-title-22">Perception of the low pitch of frequency-shifted complexes</article-title>. <source hwp:id="source-23">J. Acoust. Soc. Am</source>. <volume>113</volume>, <fpage>977</fpage>–<lpage>985</lpage> (<year>2003</year>).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bernstein J. G. W."><surname>Bernstein</surname>, <given-names>J. G. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Penagos H."><surname>Penagos</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-23">Correct tonotopic representation is necessary for complex pitch perception</article-title>. <source hwp:id="source-24">Proc. Natl. Acad. Sci</source>. <volume>101</volume>, <fpage>1421</fpage>–<lpage>1425</lpage> (<year>2004</year>).</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2 xref-ref-25-3 xref-ref-25-4 xref-ref-25-5 xref-ref-25-6 xref-ref-25-7"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Bernstein J. G. W."><surname>Bernstein</surname>, <given-names>J. G. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-24">An autocorrelation model with place dependence to account for the effect of harmonic number on fundamental frequency discrimination</article-title>. <source hwp:id="source-25">J. Acoust. Soc. Am</source>. <volume>117</volume>, <fpage>3816</fpage>–<lpage>3831</lpage> (<year>2005</year>).</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2 xref-ref-26-3 xref-ref-26-4"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Cariani P. A."><surname>Cariani</surname>, <given-names>P. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Delgutte B."><surname>Delgutte</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-25">Neural correlates of the pitch of complex tones. I. Pitch and pitch salience</article-title>. <source hwp:id="source-26">J. Neurophysiol</source>. <volume>76</volume>, <fpage>1698</fpage>–<lpage>1716</lpage> (<year>1996</year>).</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Patterson R. D."><surname>Patterson</surname>, <given-names>R. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uppenkamp S."><surname>Uppenkamp</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnsrude I. S."><surname>Johnsrude</surname>, <given-names>I. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Griffiths T. D."><surname>Griffiths</surname>, <given-names>T. D.</given-names></string-name> <article-title hwp:id="article-title-26">The processing of temporal pitch and melody information in auditory cortex</article-title>. <source hwp:id="source-27">Neuron</source> <volume>36</volume>, <fpage>767</fpage>–<lpage>776</lpage> (<year>2002</year>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2 xref-ref-28-3 xref-ref-28-4"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Bendor D."><surname>Bendor</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wang X."><surname>Wang</surname>, <given-names>X.</given-names></string-name> <article-title hwp:id="article-title-27">The neuronal representation of pitch in primate auditory cortex</article-title>. <source hwp:id="source-28">Nature</source> <volume>436</volume>, <fpage>1161</fpage>–<lpage>1165</lpage> (<year>2005</year>).</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2 xref-ref-29-3 xref-ref-29-4 xref-ref-29-5"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Norman-Haignere S."><surname>Norman-Haignere</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kanwisher N."><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-28">Cortical pitch regions in humans respond primarily to resolved harmonics and are located in specific tonotopic regions of anterior auditory cortex</article-title>. <source hwp:id="source-29">J. Neurosci</source>. <volume>33</volume>, <fpage>19451</fpage>–<lpage>19469</lpage> (<year>2013</year>).</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="de Cheveigné A."><surname>de Cheveigné</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Pressnitzer D."><surname>Pressnitzer</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-29">The case of the missing delay lines: Synthetic delays obtained by cross-channel phase interaction</article-title>. <source hwp:id="source-30">J. Acoust. Soc. Am</source>. <volume>119</volume>, <fpage>3908</fpage>–<lpage>3918</lpage> (<year>2006</year>).</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Verschooten E."><surname>Verschooten</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-30">The upper frequency limit for the use of phase locking to code temporal fine structure in humans: A compilation of viewpoints</article-title>. <source hwp:id="source-31">Hear. Res</source>. <volume>377</volume>, <fpage>109</fpage>–<lpage>121</lpage> (<year>2019</year>).</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Mehta A. H."><surname>Mehta</surname>, <given-names>A. H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-31">Effect of lowest harmonic rank on fundamentalfrequency difference limens varies with fundamental frequency</article-title>. <source hwp:id="source-32">J. Acoust. Soc. Am</source>. <volume>147</volume>, <fpage>2314</fpage>–<lpage>2322</lpage> (<year>2020</year>).</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2 xref-ref-33-3 xref-ref-33-4"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Licklider J. C. R."><surname>Licklider</surname>, <given-names>J. C. R.</given-names></string-name> <article-title hwp:id="article-title-32">A duplex theory of pitch perception</article-title>. <source hwp:id="source-33">Experientia</source> <volume>7</volume>, <fpage>128</fpage>–<lpage>134</lpage> (<year>1951</year>).</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Goldstein J. L."><surname>Goldstein</surname>, <given-names>J. L.</given-names></string-name> <article-title hwp:id="article-title-33">An optimum processor theory for the central formation of the pitch of complex tones</article-title>. <source hwp:id="source-34">J. Acoust. Soc. Am</source>. <volume>54</volume>, <fpage>1496</fpage>–<lpage>1516</lpage> (<year>1973</year>).</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>35.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Terhardt E."><surname>Terhardt</surname>, <given-names>E.</given-names></string-name> <article-title hwp:id="article-title-34">Calculating virtual pitch</article-title>. <source hwp:id="source-35">Hear. Res</source>. <volume>1</volume>, <fpage>155</fpage>–<lpage>182</lpage> (<year>1979</year>).</citation></ref><ref id="c36" hwp:id="ref-36"><label>36.</label><citation publication-type="confproc" citation-type="confproc" ref:id="2020.11.19.389999v4.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Slaney M."><surname>Slaney</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Lyon R. F."><surname>Lyon</surname>, <given-names>R. F.</given-names></string-name> <article-title hwp:id="article-title-35">A perceptual pitch detector</article-title>. in <conf-name>International Conference on Acoustics, Speech, and Signal Processing</conf-name> <fpage>357</fpage>–<lpage>360</lpage> vol.<volume>1</volume> (<year>1990</year>). doi:<pub-id pub-id-type="doi">10.1109/ICASSP.1990.115684</pub-id>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2 xref-ref-37-3"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Meddis R."><surname>Meddis</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="O’Mard L."><surname>O’Mard</surname>, <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-36">A unitary model of pitch perception</article-title>. <source hwp:id="source-36">J. Acoust. Soc. Am</source>. <volume>102</volume>, <fpage>1811</fpage>–<lpage>1820</lpage> (<year>1997</year>).</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2 xref-ref-38-3"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Laudanski J."><surname>Laudanski</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zheng Y."><surname>Zheng</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Brette R."><surname>Brette</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-37">A structural theory of pitch</article-title>. <source hwp:id="source-37">eNeuro</source> <volume>1</volume>, (<year>2014</year>).</citation></ref><ref id="c39" hwp:id="ref-39"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Ahmad N."><surname>Ahmad</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Higgins I."><surname>Higgins</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walker K. M. M."><surname>Walker</surname>, <given-names>K. M. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Stringer S. M."><surname>Stringer</surname>, <given-names>S. M.</given-names></string-name> <article-title hwp:id="article-title-38">Harmonic training and the formation of pitch representation in a neural network model of the auditory brain</article-title>. <source hwp:id="source-38">Front. Comput. Neurosci</source>. <volume>10</volume>, (<year>2016</year>).</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2 xref-ref-40-3"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Barzelay O."><surname>Barzelay</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Furst M."><surname>Furst</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Barak O."><surname>Barak</surname>, <given-names>O.</given-names></string-name> <article-title hwp:id="article-title-39">A new approach to model pitch perception using sparse coding</article-title>. <source hwp:id="source-39">PLOS Comput. Biol</source>. <volume>13</volume>, <fpage>e1005338</fpage> (<year>2017</year>).</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="confproc" citation-type="confproc" ref:id="2020.11.19.389999v4.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Kim J. W."><surname>Kim</surname>, <given-names>J. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Salamon J."><surname>Salamon</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li P."><surname>Li</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bello J. P."><surname>Bello</surname>, <given-names>J. P.</given-names></string-name> <article-title hwp:id="article-title-40">CREPE: a convolutional representation for pitch estimation</article-title>. in <conf-name>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</conf-name> <fpage>161</fpage>–<lpage>165</lpage> (<year>2018</year>). doi:<pub-id pub-id-type="doi">10.1109/ICASSP.2018.8461329</pub-id>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2 xref-ref-42-3 xref-ref-42-4"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Bruce I. C."><surname>Bruce</surname>, <given-names>I. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erfani Y."><surname>Erfani</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zilany M. S. A."><surname>Zilany</surname>, <given-names>M. S. A.</given-names></string-name> <article-title hwp:id="article-title-41">A phenomenological model of the synapse between the inner hair cell and auditory nerve: implications of limited neurotransmitter release sites</article-title>. <source hwp:id="source-40">Hear. Res</source>. <volume>360</volume>, <fpage>40</fpage>–<lpage>54</lpage> (<year>2018</year>).</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1 xref-ref-43-2 xref-ref-43-3 xref-ref-43-4 xref-ref-43-5 xref-ref-43-6 xref-ref-43-7"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Houtsma A. J. M."><surname>Houtsma</surname>, <given-names>A. J. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Smurzynski J."><surname>Smurzynski</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-42">Pitch identification and discrimination for complex tones with many harmonics</article-title>. <source hwp:id="source-41">J. Acoust. Soc. Am</source>. <volume>87</volume>, <fpage>304</fpage>–<lpage>310</lpage> (<year>1990</year>).</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Moore B. C. J."><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glasberg B. R."><surname>Glasberg</surname>, <given-names>B. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Proctor G. M."><surname>Proctor</surname>, <given-names>G. M.</given-names></string-name> <article-title hwp:id="article-title-43">Accuracy of pitch matching for pure tones and for complex tones with overlapping or nonoverlapping harmonics</article-title>. <source hwp:id="source-42">J. Acoust. Soc. Am</source>. <volume>91</volume>, <fpage>3443</fpage>–<lpage>3450</lpage> (<year>1992</year>).</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Mehrer J."><surname>Mehrer</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Spoerer C. J."><surname>Spoerer</surname>, <given-names>C. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kietzmann T. C."><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name> <article-title hwp:id="article-title-44">Individual differences among deep neural network models</article-title>. <source hwp:id="source-43">Nat. Commun</source>. <volume>11</volume>, <fpage>5725</fpage> (<year>2020</year>).</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Lakshminarayanan B."><surname>Lakshminarayanan</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pritzel A."><surname>Pritzel</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Blundell C."><surname>Blundell</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-45">Simple and scalable predictive uncertainty estimation using deep ensembles</article-title>. in <source hwp:id="source-44">Advances in Neural Information Processing Systems</source> vol. <volume>30</volume> <fpage>6402</fpage>–<lpage>6413</lpage> (<year>2017</year>).</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>47.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Wilson A. G."><surname>Wilson</surname>, <given-names>A. G.</given-names></string-name> <article-title hwp:id="article-title-46">The case for Bayesian deep learning</article-title>. ArXiv<pub-id pub-id-type="arxiv">200110995</pub-id> Cs Stat (<year>2020</year>).</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1 xref-ref-48-2 xref-ref-48-3 xref-ref-48-4 xref-ref-48-5 xref-ref-48-6 xref-ref-48-7"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Bernstein J. G."><surname>Bernstein</surname>, <given-names>J. G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-47">Pitch discrimination of diotic and dichotic tone complexes: Harmonic resolvability or harmonic number?</article-title> <source hwp:id="source-45">J. Acoust. Soc. Am</source>. <volume>113</volume>, <fpage>3323</fpage>–<lpage>3334</lpage> (<year>2003</year>).</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2 xref-ref-49-3"><label>49.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Siebert W. M."><surname>Siebert</surname>, <given-names>W. M.</given-names></string-name> <article-title hwp:id="article-title-48">Frequency discrimination in the auditory system: place or periodicity mechanisms?</article-title> <source hwp:id="source-46">Proc. IEEE</source> <volume>58</volume>, <fpage>723</fpage>–<lpage>730</lpage> (<year>1970</year>).</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1 xref-ref-50-2"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Wier C. C."><surname>Wier</surname>, <given-names>C. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jesteadt W."><surname>Jesteadt</surname>, <given-names>W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Green D. M."><surname>Green</surname>, <given-names>D. M.</given-names></string-name> <article-title hwp:id="article-title-49">Frequency discrimination as a function of frequency and sensation level</article-title>. <source hwp:id="source-47">J. Acoust. Soc. Am</source>. <volume>61</volume>, <fpage>178</fpage>–<lpage>184</lpage> (<year>1977</year>).</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><label>51.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Arehart K. H."><surname>Arehart</surname>, <given-names>K. H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Burns E. M."><surname>Burns</surname>, <given-names>E. M.</given-names></string-name> <article-title hwp:id="article-title-50">A comparison of monotic and dichotic complex-tone pitch perception in listeners with hearing loss</article-title>. <source hwp:id="source-48">J. Acoust. Soc. Am</source>. <volume>106</volume>, <fpage>993</fpage>–<lpage>997</lpage> (<year>1999</year>).</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Bernstein J. G. W."><surname>Bernstein</surname>, <given-names>J. G. W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-51">The relationship between frequency selectivity and pitch discrimination: Sensorineural hearing loss</article-title>. <source hwp:id="source-49">J. Acoust. Soc. Am</source>. <volume>120</volume>, <fpage>3929</fpage>–<lpage>3945</lpage> (<year>2006</year>).</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1 xref-ref-53-2 xref-ref-53-3"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Shera C. A."><surname>Shera</surname>, <given-names>C. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Guinan J. J."><surname>Guinan</surname>, <given-names>J. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-52">Revised estimates of human cochlear tuning from otoacoustic and behavioral measurements</article-title>. <source hwp:id="source-50">Proc. Natl. Acad. Sci</source>. <volume>99</volume>, <fpage>3318</fpage>–<lpage>3323</lpage> (<year>2002</year>).</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Micheyl C."><surname>Micheyl</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Delhommeau K."><surname>Delhommeau</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perrot X."><surname>Perrot</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-53">Influence of musical and psychoacoustical training on pitch discrimination</article-title>. <source hwp:id="source-51">Hear. Res</source>. <volume>219</volume>, <fpage>36</fpage>–<lpage>47</lpage> (<year>2006</year>).</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2 xref-ref-55-3"><label>55.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="McPherson M. J."><surname>McPherson</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-54">Time-dependent discrimination advantages for harmonic sounds suggest efficient coding for memory</article-title>. <source hwp:id="source-52">Proc. Natl. Acad. Sci</source>. <volume>117</volume>, <fpage>32169</fpage>–<lpage>32180</lpage> (<year>2020</year>).</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>56.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Bendor D."><surname>Bendor</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Osmanski M. S."><surname>Osmanski</surname>, <given-names>M. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wang X."><surname>Wang</surname>, <given-names>X.</given-names></string-name> <article-title hwp:id="article-title-55">Dual-pitch processing mechanisms in primate auditory cortex</article-title>. <source hwp:id="source-53">J. Neurosci</source>. <volume>32</volume>, <fpage>16149</fpage>–<lpage>16161</lpage> (<year>2012</year>).</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><label>57.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.57" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="McPherson M. J."><surname>McPherson</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grace R. C."><surname>Grace</surname>, <given-names>R. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-56">Harmonicity aids hearing in noise</article-title>. <source hwp:id="source-54">Atten. Percept. Psychophys</source>. (In Press).</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>58.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Durlach N. I."><surname>Durlach</surname>, <given-names>N. I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Braida L. D."><surname>Braida</surname>, <given-names>L. D.</given-names></string-name> <article-title hwp:id="article-title-57">Intensity perception. I. Preliminary theory of intensity resolution</article-title>. <source hwp:id="source-55">J. Acoust. Soc. Am</source>. <volume>46</volume>, <fpage>372</fpage>–<lpage>383</lpage> (<year>1969</year>).</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>59.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Micheyl C."><surname>Micheyl</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schrater P. R."><surname>Schrater</surname>, <given-names>P. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-58">Auditory frequency and intensity discrimination explained using a cortical population rate code</article-title>. <source hwp:id="source-56">PLOS Comput. Biol</source>. <volume>9</volume>, <fpage>e1003336</fpage> (<year>2013</year>).</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>60.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Micheyl C."><surname>Micheyl</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Keebler M. V."><surname>Keebler</surname>, <given-names>M. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Loper A."><surname>Loper</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Santurette S."><surname>Santurette</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-59">Pitch perception beyond the traditional existence region of pitch</article-title>. <source hwp:id="source-57">Proc. Natl. Acad. Sci</source>. <volume>108</volume>, <fpage>7629</fpage>–<lpage>7634</lpage> (<year>2011</year>).</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><label>61.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Lewicki M. S."><surname>Lewicki</surname>, <given-names>M. S.</given-names></string-name> <article-title hwp:id="article-title-60">Efficient coding of natural sounds</article-title>. <source hwp:id="source-58">Nat. Neurosci</source>. <volume>5</volume>, <fpage>356</fpage>–<lpage>363</lpage> (<year>2002</year>).</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1 xref-ref-62-2"><label>62.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Feather J."><surname>Feather</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Durango A."><surname>Durango</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gonzalez R."><surname>Gonzalez</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J."><surname>McDermott</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-61">Metamers of neural networks reveal divergence from human perceptual systems</article-title>. in <source hwp:id="source-59">Advances in Neural Information Processing Systems</source> vol. <volume>32</volume> <fpage>10078</fpage>–<lpage>10089</lpage> (<year>2019</year>).</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><label>63.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Lindsay G."><surname>Lindsay</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-62">Convolutional neural networks as a model of the visual system: past, present, and future</article-title>. <source hwp:id="source-60">J. Cogn. Neurosci</source>. (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1162/jocn_a_01544</pub-id>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><label>64.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Tang C."><surname>Tang</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hamilton L. S."><surname>Hamilton</surname>, <given-names>L. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Chang E. F."><surname>Chang</surname>, <given-names>E. F.</given-names></string-name> <article-title hwp:id="article-title-63">Intonational speech prosody encoding in the human auditory cortex</article-title>. <source hwp:id="source-61">Science</source> <volume>357</volume>, <fpage>797</fpage>–<lpage>801</lpage> (<year>2017</year>).</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><label>65.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Dowling W. J."><surname>Dowling</surname>, <given-names>W. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fujitani D. S."><surname>Fujitani</surname>, <given-names>D. S.</given-names></string-name> <article-title hwp:id="article-title-64">Contour, interval, and pitch recognition in memory for melodies</article-title>. <source hwp:id="source-62">J. Acoust. Soc. Am</source>. <volume>49</volume>, <fpage>524</fpage>–<lpage>531</lpage> (<year>1971</year>).</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><label>66.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Allen E. J."><surname>Allen</surname>, <given-names>E. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oxenham A. J."><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-65">Symmetric interactions and interference between pitch and timbre</article-title>. <source hwp:id="source-63">J. Acoust. Soc. Am</source>. <volume>135</volume>, <fpage>1371</fpage>–<lpage>1379</lpage> (<year>2014</year>).</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><label>67.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Bizley J. K."><surname>Bizley</surname>, <given-names>J. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walker K. M. M."><surname>Walker</surname>, <given-names>K. M. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nodal F. R."><surname>Nodal</surname>, <given-names>F. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="King A. J."><surname>King</surname>, <given-names>A. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Schnupp J. W. H."><surname>Schnupp</surname>, <given-names>J. W. H.</given-names></string-name> <article-title hwp:id="article-title-66">Auditory cortex represents both pitch judgments and the corresponding acoustic cues</article-title>. <source hwp:id="source-64">Curr. Biol</source>. <volume>23</volume>, <fpage>620</fpage>–<lpage>625</lpage> (<year>2013</year>).</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><label>68.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Lorenzi C."><surname>Lorenzi</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gilbert G."><surname>Gilbert</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carn H."><surname>Carn</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garnier S."><surname>Garnier</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Moore B. C. J."><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name> <article-title hwp:id="article-title-67">Speech perception problems of the hearing impaired reflect inability to use temporal fine structure</article-title>. <source hwp:id="source-65">Proc. Natl. Acad. Sci</source>. <volume>103</volume>, <fpage>18866</fpage>–<lpage>18869</lpage> (<year>2006</year>).</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1 xref-ref-69-2"><label>69.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Zilany M. S. A."><surname>Zilany</surname>, <given-names>M. S. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bruce I. C."><surname>Bruce</surname>, <given-names>I. C.</given-names></string-name> <article-title hwp:id="article-title-68">Modeling auditory-nerve responses for high sound pressure levels in the normal and impaired auditory periphery</article-title>. <source hwp:id="source-66">J. Acoust. Soc. Am</source>. <volume>120</volume>, <fpage>1446</fpage>–<lpage>1466</lpage> (<year>2006</year>).</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><label>70.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Gfeller K."><surname>Gfeller</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-69">Accuracy of cochlear implant recipients on pitch perception, melody recognition, and speech reception in noise</article-title>: <source hwp:id="source-67">Ear Hear</source>. <volume>28</volume>, <fpage>412</fpage>–<lpage>423</lpage> (<year>2007</year>).</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><label>71.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Shofner W. P."><surname>Shofner</surname>, <given-names>W. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Chaney M."><surname>Chaney</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-70">Processing pitch in a nonhuman mammal (Chinchilla laniger)</article-title>. <source hwp:id="source-68">J. Comp. Psychol</source>. <volume>127</volume>, <fpage>142</fpage>–<lpage>153</lpage> (<year>2013</year>).</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><label>72.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Walker K. M."><surname>Walker</surname>, <given-names>K. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gonzalez R."><surname>Gonzalez</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kang J. Z."><surname>Kang</surname>, <given-names>J. Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="King A. J."><surname>King</surname>, <given-names>A. J.</given-names></string-name> <article-title hwp:id="article-title-71">Across-species differences in pitch perception are consistent with differences in cochlear filtering</article-title>. <source hwp:id="source-69">eLife</source> <volume>8</volume>, <fpage>e41626</fpage> (<year>2019</year>).</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><label>73.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Joris P. X."><surname>Joris</surname>, <given-names>P. X.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-72">Frequency selectivity in Old-World monkeys corroborates sharp cochlear tuning in humans</article-title>. <source hwp:id="source-70">Proc. Natl. Acad. Sci</source>. <volume>108</volume>, <fpage>17516</fpage>–<lpage>17520</lpage> (<year>2011</year>).</citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><label>74.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="White L. J."><surname>White</surname>, <given-names>L. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Plack C. J."><surname>Plack</surname>, <given-names>C. J.</given-names></string-name> <article-title hwp:id="article-title-73">Temporal processing of the pitch of complex tones</article-title>. <source hwp:id="source-71">J. Acoust. Soc. Am</source>. <volume>103</volume>, <fpage>2051</fpage>–<lpage>2063</lpage> (<year>1998</year>).</citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1 xref-ref-75-2"><label>75.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Kawahara H."><surname>Kawahara</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-74">Tandem-STRAIGHT: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation</article-title>. <source hwp:id="source-72">2008 IEEE Int. Conf. Acoust. Speech Signal Process</source>. <fpage>3933</fpage>–<lpage>3936</lpage> (<year>2008</year>) doi:<pub-id pub-id-type="doi">10.1109/icassp.2008.4518514</pub-id>.</citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><label>76.</label><citation publication-type="confproc" citation-type="confproc" ref:id="2020.11.19.389999v4.76" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Köhn A."><surname>Köhn</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stegen F."><surname>Stegen</surname>, <given-names>F.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Baumann T."><surname>Baumann</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-75">Mining the Spoken Wikipedia for speech data and beyond</article-title>. in <conf-name>Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</conf-name> <fpage>4644</fpage>–<lpage>4647</lpage> (<source hwp:id="source-73">European Language Resources Association (ELRA)</source>, <year>2016</year>).</citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1"><label>77.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Engel J."><surname>Engel</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-76">Neural audio synthesis of musical notes with WaveNet autoencoders</article-title>. ArXiv<pub-id pub-id-type="arxiv">170401279</pub-id> Cs (<year>2017</year>).</citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><label>78.</label><citation publication-type="other" citation-type="journal" ref:id="2020.11.19.389999v4.78" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Gemmeke J. F."><surname>Gemmeke</surname>, <given-names>J. F.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-77">Audio Set: an ontology and human-labeled dataset for audio events</article-title>. in <source hwp:id="source-74">Proc. IEEE ICASSP 2017</source> <fpage>776</fpage>–<lpage>780</lpage> (<year>2017</year>).</citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><label>79.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.79" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Simoncelli E. P."><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> <article-title hwp:id="article-title-78">Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title>. <source hwp:id="source-75">Neuron</source> <volume>71</volume>, <fpage>926</fpage>–<lpage>940</lpage> (<year>2011</year>).</citation></ref><ref id="c80" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1"><label>80.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.80" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="McWalter R."><surname>McWalter</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="McDermott J. H."><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name> <article-title hwp:id="article-title-79">Adaptive and selective time averaging of auditory scenes</article-title>. <source hwp:id="source-76">Curr. Biol</source>. <volume>28</volume>, <fpage>1405</fpage>–<lpage>1418.e10</lpage> (<year>2018</year>).</citation></ref><ref id="c81" hwp:id="ref-81" hwp:rev-id="xref-ref-81-1"><label>81.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.81" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Liberman M. C."><surname>Liberman</surname>, <given-names>M. C.</given-names></string-name> <article-title hwp:id="article-title-80">Central projections of auditory-nerve fibers of differing spontaneous rate. I. Anteroventral cochlear nucleus</article-title>. <source hwp:id="source-77">J. Comp. Neurol</source>. <volume>313</volume>, <fpage>240</fpage>–<lpage>258</lpage> (<year>1991</year>).</citation></ref><ref id="c82" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1"><label>82.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.82" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Carney L. H."><surname>Carney</surname>, <given-names>L. H.</given-names></string-name> <article-title hwp:id="article-title-81">Supra-threshold hearing and fluctuation profiles: implications for sensorineural and hidden hearing loss</article-title>. <source hwp:id="source-78">J. Assoc. Res. Otolaryngol</source>. <volume>19</volume>, <fpage>331</fpage>–<lpage>352</lpage> (<year>2018</year>).</citation></ref><ref id="c83" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1"><label>83.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.83" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Glasberg B. R."><surname>Glasberg</surname>, <given-names>B. R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Moore B. C. J."><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name> <article-title hwp:id="article-title-82">Derivation of auditory filter shapes from notched-noise data</article-title>. <source hwp:id="source-79">Hear. Res</source>. <volume>47</volume>, <fpage>103</fpage>–<lpage>138</lpage> (<year>1990</year>).</citation></ref><ref id="c84" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1 xref-ref-84-2"><label>84.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Chi T."><surname>Chi</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ru P."><surname>Ru</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Shamma S. A."><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name> <article-title hwp:id="article-title-83">Multiresolution spectrotemporal analysis of complex sounds</article-title>. <source hwp:id="source-80">J. Acoust. Soc. Am</source>. <volume>118</volume>, <fpage>887</fpage>–<lpage>906</lpage> (<year>2005</year>).</citation></ref><ref id="c85" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1"><label>85.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.85" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Dau T."><surname>Dau</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kollmeier B."><surname>Kollmeier</surname>, <given-names>B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kohlrausch A."><surname>Kohlrausch</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-84">Modeling auditory processing of amplitude modulation. II. Spectral and temporal integration</article-title>. <source hwp:id="source-81">J. Acoust. Soc. Am</source>. <volume>102</volume>, <fpage>2906</fpage>–<lpage>2919</lpage> (<year>1997</year>).</citation></ref><ref id="c86" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1"><label>86.</label><citation publication-type="journal" citation-type="journal" ref:id="2020.11.19.389999v4.86" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Pressnitzer D."><surname>Pressnitzer</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Patterson R. D."><surname>Patterson</surname>, <given-names>R. D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Krumbholz K."><surname>Krumbholz</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-85">The lower limit of melodic pitch</article-title>. <source hwp:id="source-82">J. Acoust. Soc. Am</source>. <volume>109</volume>, <fpage>2074</fpage>–<lpage>2084</lpage> (<year>2001</year>).</citation></ref></ref-list><sec hwp:id="sec-89"><title hwp:id="title-93">SUPPLEMENTARY INFORMATION</title><fig id="figS1" position="float" orientation="portrait" fig-type="figure" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2 xref-fig-10-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Supplementary Figure 1.</label><caption hwp:id="caption-10"><p hwp:id="p-195">Pitch behavior of the 10 best network architectures ranked by F0 estimation performance on natural sounds. Each column shows results from a single neural network architecture (depicted at the top). Detailed descriptions of each architecture are provided in <xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Supplementary Table 1</xref>. The five rows in this grid correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-28" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5).</p></caption><graphic xlink:href="389999v4_figS1" position="float" orientation="portrait" hwp:id="graphic-17"/></fig><fig id="figS2" position="float" orientation="portrait" fig-type="figure" hwp:id="F11" hwp:rev-id="xref-fig-11-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Supplementary Figure 2.</label><caption hwp:id="caption-11"><p hwp:id="p-196">Network architectures producing better F0 estimation for natural sounds exhibit more human-like pitch behavior. The five rows in this grid correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-29" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). First three columns show results measured from the 40 worst, middle, and best-performing network architectures (out of the 400 architectures trained in our architecture search, ranked by F0 estimation performance on natural sounds), respectively. Error bars plot bootstrapped 95% confidence intervals around the mean across the 40 networks. Human results (reproduced from <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Fig. 1</xref>) are shown in column 4. Column 5 contains scatter plots of human-model behavioral similarity (quantified as a correlation between the results of each model and that of humans) vs. validation set accuracy for all trained networks (reproduced from <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Fig. 3</xref>). Pearson correlations between validation set accuracy and human-model similarity for each experiment are noted in the legends.</p></caption><graphic xlink:href="389999v4_figS2" position="float" orientation="portrait" hwp:id="graphic-18"/></fig><fig id="figS3" position="float" orientation="portrait" fig-type="figure" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3 xref-fig-12-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Supplementary Figure 3.</label><caption hwp:id="caption-12"><p hwp:id="p-197">Deep networks better account for human psychophysical behavior than networks with just one convolutional layer. Of the 400 randomly-generated networks we considered, 54 contained only one convolutional layer. These 54 single-convolutional-layer networks produced lower validation set accuracies (z=7.31, p&lt;0.001, Wilcoxon rank-sum test), shown on the x-axis of the graphs in the right column. The single-convolutional-layer networks also produce less human-like psychophysical results. The five rows in this grid correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-30" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). In column 1, network psychophysical results are shown averaged across the top 10% (34 of 346) of networks (ranked by validation set performance) containing more than one convolutional layer. In column 2, results are shown averaged across the top 10% (5 of 54) of networks containing just one convolutional layer. Human results are shown in column 3. Column 4 contains scatter plots of human-model behavioral similarity (quantified as a correlation between the results of each model and that of humans) vs. validation set accuracy for all trained networks. Black and magenta data points correspond to multi-convolutional-layer and single-convolutional-layer networks, respectively. Median data points are included for each group to illustrate how single-layer networks generally both performed worse on the F0 estimation task and produced poorer matches to human behavior. Error bars plot bootstrapped 95% confidence intervals around the mean across architectures. When pooled across all five experiments, human-model similarity was lower for single-convolutional-layer networks than the remaining 346 multi-convolutional-layer networks (z=9.24, p&lt;0.001, two-sided Wilcoxon rank-sum test). Moreover, the top 40% of networks ranked according to overall human-model similarity consisted entirely of multi-convolutional-layer networks.</p></caption><graphic xlink:href="389999v4_figS3" position="float" orientation="portrait" hwp:id="graphic-19"/></fig><fig id="figS4" position="float" orientation="portrait" fig-type="figure" hwp:id="F13" hwp:rev-id="xref-fig-13-1 xref-fig-13-2 xref-fig-13-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figS4</object-id><label>Supplementary Figure 4.</label><caption hwp:id="caption-13"><p hwp:id="p-198">Effect of number of auditory nerve fibers on F0 estimation error and discrimination thresholds measured from networks without spike-timing information. <bold>(a)</bold> Median F0 estimation error on the natural sounds validation set for 10 networks trained and tested with five different peripheral model configurations varying in the number of input auditory nerve fibers (ANFs) and in whether the frequency and time dimensions were “transposed” (indicated by superscript T). The network architectures were constrained to take a 100-by-1000 array as input. The default input representation was 100 frequency channels (auditory nerve fibers) by 1000 timesteps. A transposed input representation consists of 1000 frequency channels by 100 timesteps. To manipulate the number of nerve fibers while keeping the size of the input representation fixed, transposed peripheral representations with fewer than 1000 nerve fibers were upsampled to 1000 (via linear interpolation) along the frequency (auditory nerve fiber) dimension. Time was sampled at 2 kHz to yield 100 timesteps. For all networks included in the plot, the IHC filter cutoff frequency was set to 50 Hz to eliminate all phase-locked temporal information (see <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-15" hwp:rel-id="F5">Fig. 5</xref>). <bold>(b)</bold> F0 discrimination thresholds as a function of lowest harmonic number of synthetic tones, measured from the same networks as <bold>(a)</bold>. The best thresholds are re-plotted to the left of the main axes. Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS4" position="float" orientation="portrait" hwp:id="graphic-20"/></fig><fig id="figS5" position="float" orientation="portrait" fig-type="figure" hwp:id="F14" hwp:rev-id="xref-fig-14-1 xref-fig-14-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figS5</object-id><label>Supplementary Figure 5.</label><caption hwp:id="caption-14"><p hwp:id="p-199">Effects of phase locking cutoff on network pitch behavior. Columns correspond to networks trained and tested with seven different configurations of the peripheral auditory model. Configurations differed in the upper frequency limit of auditory nerve phase locking (inner hair cell lowpass filter cutoff) and the number of auditory nerve fibers (ANFs) (see <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-16" hwp:rel-id="F5">Fig. 5</xref>). Rows correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-31" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS5" position="float" orientation="portrait" hwp:id="graphic-21"/></fig><fig id="figS6" position="float" orientation="portrait" fig-type="figure" hwp:id="F15" hwp:rev-id="xref-fig-15-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figS6</object-id><label>Supplementary Figure 6.</label><caption hwp:id="caption-15"><p hwp:id="p-200">Networks do not rely on place cues to F0 in the excitation pattern to produce human-like pitch behavior. <bold>(a)</bold> Simulated peripheral representations of the same stimulus (harmonic tone with 200 Hz F0) with unmodified (left column) or flattened (right column) time-averaged excitation patterns. The peaks and valleys in the unmodified excitation pattern, which provide place cues to F0, were eliminated by separately scaling each frequency channel of the nerve representation to have the same time-averaged response. <bold>(b)</bold> Psychophysical results from the best-performing network architecture tested on auditory nerve representations with either unmodified or flattened excitation patterns. In both cases, the model was trained on auditory nerve representations with unmodified excitation patterns. Rows correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-32" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), frequency-shifted complexes (row 3), and complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS6" position="float" orientation="portrait" hwp:id="graphic-22"/></fig><fig id="figS7" position="float" orientation="portrait" fig-type="figure" hwp:id="F16" hwp:rev-id="xref-fig-16-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F16</object-id><object-id pub-id-type="publisher-id">figS7</object-id><label>Supplementary Figure 7.</label><caption hwp:id="caption-16"><p hwp:id="p-201">Effects of altered cochlear frequency selectivity on network pitch behavior. The four columns correspond to networks trained and tested with four different settings of cochlear frequency selectivity: linearly spaced cochlear filters with constant bandwidths (column 1), cochlear filters with bandwidths two times narrower than those estimated for normal hearing-humans but normally spaced (i.e., evenly spaced on an ERB scale, to best approximate the spacing believed to characterize the ear) (column 2), normally spaced cochlear filters with bandwidths matched to those of normal-hearing humans (column 3), and cochlear filters with two times broader bandwidths but normally spaced (column 4) (see <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-11" hwp:rel-id="F6">Fig. 6</xref>). Rows correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-33" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures. Psychophysical results were qualitatively robust to changes in peripheral frequency tuning. The main exceptions were the effects of harmonic phase, which were reduced for the linearly spaced models (correlations between human and model results for the phase randomization and alternating phase experiments were lower in the linearly spaced condition than in the human tuning condition; phase randomization: t(18)=3.13, p&lt;0.01, d=1.40; alternating phase: t(18)=6.50, p&lt;0.001, d=2.91; two-sided two-sample t-tests). These results are to be expected because the sharp tuning of the linearly spaced filters (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-12" hwp:rel-id="F6">Fig. 6B</xref>) results in less interaction between adjacent harmonics, which is believed to drive phase effects.</p></caption><graphic xlink:href="389999v4_figS7" position="float" orientation="portrait" hwp:id="graphic-23"/></fig><fig id="figS8" position="float" orientation="portrait" fig-type="figure" hwp:id="F17" hwp:rev-id="xref-fig-17-1 xref-fig-17-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F17</object-id><object-id pub-id-type="publisher-id">figS8</object-id><label>Supplementary Figure 8.</label><caption hwp:id="caption-17"><p hwp:id="p-202">Effects of training set sound statistics on network pitch behavior. Columns correspond to different training datasets (described in column titles). Rows correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-34" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), pitch estimation of frequency-shifted complexes (row 3), pitch estimation of complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). Results are shown for the best-performing network architecture, averaged across 10 instances of the architecture trained from different random initializations. Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS8" position="float" orientation="portrait" hwp:id="graphic-24"/></fig><fig id="figS9" position="float" orientation="portrait" fig-type="figure" hwp:id="F18" hwp:rev-id="xref-fig-18-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F18</object-id><object-id pub-id-type="publisher-id">figS9</object-id><label>Supplementary Figure 9.</label><caption hwp:id="caption-18"><p hwp:id="p-203">F0 discrimination thresholds as a function of lowest harmonic number, measured from networks trained separately on speech-only and music-only datasets. <bold>(a)</bold> Results from networks trained on simulated auditory nerve representations produced by a fixed peripheral auditory model (reproduced from <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-15" hwp:rel-id="F7">Fig. 7C</xref>). <bold>(b)</bold> Results from networks trained directly on sound waveforms (first-layer “cochlear” filters were learned alongside the rest of the network weights; see <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-12" hwp:rel-id="F4">Fig. 4</xref>). Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS9" position="float" orientation="portrait" hwp:id="graphic-25"/></fig><fig id="figS10" position="float" orientation="portrait" fig-type="figure" hwp:id="F19" hwp:rev-id="xref-fig-19-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F19</object-id><object-id pub-id-type="publisher-id">figS10</object-id><label>Supplementary Figure 10.</label><caption hwp:id="caption-19"><p hwp:id="p-204">Effects of auditory nerve fiber type (high vs. low spontaneous rate) on network pitch behavior. <bold>(a)</bold> Simulated peripheral representations of the same stimulus (harmonic tone with 200 Hz F0) with high (70 spikes/s; left column) and low (0.1 spikes/s; right column) spontaneous rate auditory nerve fibers. <bold>(b)</bold> Psychophysical results from the best-performing network architecture trained and tested with each of the two different nerve fiber types. Rows correspond to the five main psychophysical experiments (see <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-35" hwp:rel-id="F2">Fig. 2a-e</xref>): F0 discrimination as a function of harmonic number and phase (row 1), pitch estimation of alternating-phase stimuli (row 2), frequency-shifted complexes (row 3), and complexes with individually mistuned harmonics (row 4), and frequency discrimination with pure and transposed tones (row 5). Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS10" position="float" orientation="portrait" hwp:id="graphic-26"/></fig><fig id="figS11" position="float" orientation="portrait" fig-type="figure" hwp:id="F20" hwp:rev-id="xref-fig-20-1 xref-fig-20-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/FIGS11</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F20</object-id><object-id pub-id-type="publisher-id">figS11</object-id><label>Supplementary Figure 11.</label><caption hwp:id="caption-20"><p hwp:id="p-205">Effects of F0 classification bin width on F0 estimation error and discrimination thresholds. <bold>(a)</bold> Median F0 estimation error on the natural sounds validation set for 10 networks trained and tested with five different F0 classification bin widths. <bold>(b)</bold> F0 discrimination thresholds as a function of lowest harmonic number of synthetic tones, measured from the same networks as <bold>a</bold>. The best thresholds are re-plotted to the left of the main axes. Error bars plot bootstrapped 95% confidence intervals around the mean across the 10 best network architectures.</p></caption><graphic xlink:href="389999v4_figS11" position="float" orientation="portrait" hwp:id="graphic-27"/></fig><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2020.11.19.389999v4/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Supplementary Table 1.</label><caption hwp:id="caption-21"><p hwp:id="p-206">Details of 10 best network architectures. Columns correspond to 10 distinct convolutional neural network architectures (the 10 best-performing networks identified in our random architecture search). Rows include descriptors of constituent network operations. Grey horizontal bands group operations by convolutional layer. With two exceptions, all results figures present results averaged across all 10 architectures. The two exceptions are <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-16" hwp:rel-id="F9">Fig. 9a</xref>, which features only the best-performing architecture (arch_0191), and <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-10-3" hwp:rel-id="F10">Supplementary Fig. 1</xref>, which displays results separately for each of these 10 architectures. Legend:
<list list-type="bullet" hwp:id="list-11"><list-item hwp:id="list-item-20"><p hwp:id="p-207"><italic toggle="yes">conv</italic> [<italic toggle="yes">h, w, k</italic>] : convolutional layer with <italic toggle="yes">h</italic> = kernel height (frequency dimension), <italic toggle="yes">w</italic> = kernel width (time dimension), and <italic toggle="yes">k</italic> = number of kernels</p></list-item><list-item hwp:id="list-item-21"><p hwp:id="p-208"><italic toggle="yes">relu</italic> [<italic toggle="yes">N<sub>f</sub>, N<sub>t</sub>, N<sub>k</sub></italic>]: rectified linear unit activation function operating on inputs with the specified shape (<italic toggle="yes">N<sub>f</sub></italic> = frequency dimension, <italic toggle="yes">N<sub>t</sub></italic> = time dimension, and <italic toggle="yes">N<sub>k</sub></italic> = kernel dimension)</p></list-item><list-item hwp:id="list-item-22"><p hwp:id="p-209"><italic toggle="yes">pool</italic> [<italic toggle="yes">s<sub>f</sub>, s<sub>t</sub></italic>]: weighted averaged pooling operation with stride in the frequency dimension and stride <italic toggle="yes">s<sub>t</sub></italic> in the time dimension</p></list-item><list-item hwp:id="list-item-23"><p hwp:id="p-210"><italic toggle="yes">norm</italic> [<italic toggle="yes">N<sub>f</sub>, N<sub>t</sub>, N<sub>k</sub></italic>]: batch normalization operating on inputs with the specified shape (<italic toggle="yes">N<sub>f</sub></italic> = frequency dimension, <italic toggle="yes">N<sub>t</sub></italic> = time dimension, and <italic toggle="yes">N<sub>k</sub></italic> = kernel dimension)</p></list-item><list-item hwp:id="list-item-24"><p hwp:id="p-211"><italic toggle="yes">f<sub>C</sub></italic> [<italic toggle="yes">N</italic>] : fully-connected layer with <italic toggle="yes">N</italic> units</p></list-item><list-item hwp:id="list-item-25"><p hwp:id="p-212"><italic toggle="yes">dropout</italic> : dropout regularization with 50% dropout rate</p></list-item></list></p></caption><graphic xlink:href="389999v4_tblS1" position="float" orientation="portrait" hwp:id="graphic-28"/></table-wrap></sec></back></article>
