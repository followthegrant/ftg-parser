<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/191858</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;191858</article-id><article-id pub-id-type="other" hwp:sub-type="slug">191858</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">191858</article-id><article-id pub-id-type="other" hwp:sub-type="tag">191858</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">MARS-Net: Deep learning-based segmentation pipeline for profiling cellular morphodynamics from multiple types of live cell microscopy</article-title></title-group><author-notes hwp:id="author-notes-1"><fn id="n1" hwp:id="fn-1"><label>*</label><p hwp:id="p-1">These authors equally contributed to this work.</p></fn><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1 xref-corresp-1-2"><label>§</label>Correspondence: <email hwp:id="email-1">kwonmoo.lee@childrens.harvard.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Jang Junbong"><surname>Jang</surname><given-names>Junbong</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Wang Chuangqi"><surname>Wang</surname><given-names>Chuangqi</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">§</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Zhang Xitong"><surname>Zhang</surname><given-names>Xitong</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Choi Hee June"><surname>Choi</surname><given-names>Hee June</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Pan Xiang"><surname>Pan</surname><given-names>Xiang</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Lin Bolun"><surname>Lin</surname><given-names>Bolun</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-2" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><name name-style="western" hwp:sortable="Yu Yudong"><surname>Yu</surname><given-names>Yudong</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref></contrib><contrib contrib-type="author" hwp:id="contrib-8"><name name-style="western" hwp:sortable="Whittle Carly"><surname>Whittle</surname><given-names>Carly</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-9"><name name-style="western" hwp:sortable="Ryan Madison"><surname>Ryan</surname><given-names>Madison</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-6" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-10"><name name-style="western" hwp:sortable="Chen Yenyu"><surname>Chen</surname><given-names>Yenyu</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-7" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-11"><name name-style="western" hwp:sortable="Lee Kwonmoo"><surname>Lee</surname><given-names>Kwonmoo</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-8" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-4" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-2" hwp:rel-id="corresp-1">§</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5 xref-aff-1-6 xref-aff-1-7 xref-aff-1-8"><label>1</label><institution hwp:id="institution-1">Department of Biomedical Engineering, Worcester Polytechnic Institute</institution>, Worcester, MA 01609, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3 xref-aff-2-4"><label>2</label><institution hwp:id="institution-2">Vascular Biology Program, Boston Children’s Hospital</institution>, Boston, MA 02115, <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2"><label>3</label><institution hwp:id="institution-3">Department of Surgery, Harvard Medical School</institution>, Boston, MA 02115, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1 xref-aff-4-2"><label>4</label><institution hwp:id="institution-4">Department of Computer Science, Worcester Polytechnic Institute</institution>, Worcester, MA 01609, <country>USA</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Robotics Engineering Program, Worcester Polytechnic Institute</institution>, Worcester, MA 01609, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-09-21T11:19:16-07:00">
    <day>21</day><month>9</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-04-05T20:30:14-07:00">
    <day>5</day><month>4</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-09-21T11:25:14-07:00">
    <day>21</day><month>9</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-04-05T20:33:46-07:00">
    <day>5</day><month>4</month><year>2021</year>
  </pub-date><elocation-id>191858</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-09-20"><day>20</day><month>9</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2021-04-05"><day>05</day><month>4</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-04-05"><day>05</day><month>4</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="191858.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/191858v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="191858.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/191858v3/191858v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/191858v3/191858v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">Quantitative studies of cellular morphodynamics rely on extracting leading-edge velocity time-series based on accurate cell segmentation from live cell imaging. However, live cell imaging has numerous challenging issues about accurate edge localization. Here, we develop a deep learning-based pipeline, termed MARS-Net (Multiple-microscopy- type-based Accurate and Robust Segmentation Network), that utilizes transfer learning and the datasets from multiple types of microscopy to localize cell edges with high accuracy, allowing quantitative profiling of cellular morphodynamics. For effective training with the datasets from multiple types of live cell microscopy, we integrated the pretrained VGG-19 encoder with U-Net decoder and added dropout layers. Using this structure, we were able to train one neural network model that can accurately segment various live cell movies from phase contrast, spinning disk confocal, and total internal reflection fluorescence microscopes. Intriguingly, MARS-Net produced more accurate edge localization than the neural network models trained with single microscopy type datasets, whereas the standard U-Net could not increase the overall accuracy. We expect that MARS-Net can accelerate the studies of cellular morphodynamics by providing accurate segmentation of challenging live cell images.</p></abstract><counts><page-count count="49"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-4">The authors have declared no competing interest.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">Introduction</title><p hwp:id="p-5">Live cell imaging is a fundamental tool to study the changes of cellular morphology (morphodynamics), which is involved in cancer metastasis, immune responses, and stem cell differentiation, among others <sup><xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>–<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref></sup>. Cellular morphodynamics is governed by protrusion and retraction of the leading edges of cells, driven by cytoskeleton and adhesion processes<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>, <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref></sup>. Due to their phenotypic heterogeneity, computational image analysis in conjunction with machine learning has been employed to understand and characterize cellular morphodynamics<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>–<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref></sup>.</p><p hwp:id="p-6">Quantitative studies of cellular morphodynamics rely on extracting leading-edge velocity time-series. Therefore, accurate and consistent edge segmentation at every frame of live cell movie is necessary. Fluorescence microscopes can acquire high contrast cellular images by introducing fluorescently tagged molecules, particularly for fixed cells. Fluorescence imaging, however, causes phototoxicity to live cells, which makes researchers limit light illumination and total image acquisition time. These make fluorescence live cell images noisy, low contrast, and low-throughput. Selection of cells of low-level expression of fluorescent proteins and photobleaching further degrades the image quality<sup><xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref></sup>. Therefore, having reliable segmentation from live cell images is a significant issue. The alternative to fluorescence microscopy is a label-free phase contrast microscopy that minimizes phototoxicity in the cell. But phase contrast images contain halo and shade-off artifacts, incurring a significant challenge for reliable cell segmentation<sup><xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>–<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref></sup>.</p><p hwp:id="p-7">Although there exist numerous conventional segmentation methods, including Otsu method<sup><xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref></sup>, Canny Detector<sup><xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref></sup>, active contour or snake-based method<sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref></sup>, and PMI method based on mutual Information<sup><xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref></sup>, they often rely on a few mathematical assumptions which tend to be broken in live cell imaging conditions, rendering insufficient accuracy for cell- edge detection for the analysis of cellular morphodynamics. Supervised learning with a deep learning model can overcome such problems. Among deep learning models, Convolutional Neural Network (CNN) excels in pattern recognition in images by learning complex features directly from the input images using its hierarchical structure<sup><xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref></sup>. CNN has achieved great success in image classification<sup><xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>–<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref></sup> and segmentation<sup><xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>–<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref></sup> and demonstrated promising results in static and live cell images<sup><xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>, 29–<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref></sup> <sup><xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref></sup>. Particularly, U-Net<sup><xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">24</xref></sup> is the most widely adopted CNN-based structure for image segmentation. Some of the cell image segmentation applications are easily accessible even for users without much computational resources and coding skills<sup><xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">35</xref>, <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>, <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref></sup>. Despite these successes, the deep learning-based segmentation on live cell imaging has not been extensively tested for cellular morphodynamics studies. Moreover, models trained on their specific datasets do not generalize to different datasets, and training a new model requires large training datasets. Also, high fidelity training sets are required for training an accurate segmentation model, substantially increasing the cost of data labeling.</p><p hwp:id="p-8">To increase the segmentation accuracy and overcome training data shortage for morphodynamic profiling, we developed a deep learning framework, termed MARS-Net (Multiple-microscopy-type-based Accurate and Robust Segmentation Network), which learns robust image features for accurate segmentation using the datasets from multiple types of microscopy. We reasoned that the cross-modal features learned from images of multiple types of microscopy could achieve more accurate and robust edge localization than the features from the single type of microscopy images. Therefore, we combined training data from live cell movies of migrating PtK1 cells independently taken by different microscopy techniques such as phase contrast, spinning disk confocal (SDC), and total internal reflection fluorescence (TIRF) microscopes.</p><p hwp:id="p-9">In this pipeline, we used the U-Net<sup><xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-3" hwp:rel-id="ref-24">24</xref></sup> based structure, which is CNN comprised of encoder, decoder, and skip connections in between them for segmentation. To achieve high edge localization accuracy with the datasets from multiple types of microscopy, we incorporated the transfer learning technique that initializes the weights of the network with those of the same network trained on ImageNet<sup><xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref></sup> for the image recognition task. This has been applied in many deep learning segmentation models (FCN<sup><xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">23</xref></sup>, DeepEdge<sup><xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref></sup>, TernausNetV2<sup><xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref></sup>) and classification tasks<sup><xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>–<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref></sup> to achieve high performance with a limited dataset. We replaced the U-Net encoder with one of the image classification networks such as VGG16/VGG19<sup><xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref></sup>, ResNet50V2<sup><xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref></sup>, and EfficientNetB7<sup><xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref></sup> and used the initial weights from the ImageNet<sup><xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-2" hwp:rel-id="ref-39">39</xref></sup> training. Among them, the pretrained VGG19 encoder coupled with U-Net decoder (VGG19-U-Net) segmented the boundary of the cell with the highest accuracy. Dropout<sup><xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref></sup> layers were added to the model (VGG19D-U-Net) as a regularization method to prevent overfitting and boost the performance further. MARS- Net (VGG19D-U-Net trained on the images from multiple types of microscopy) was able to segment cell boundary more accurately than the model trained on single-microscopy- type data, whereas U-Net could not gain significant performance benefit from training on the data from multiple types of microscopy. Also, we demonstrate that MARS-Net enables more reliable quantitative analyses of cellular morphodynamics compared to the single- microscopy-type model.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Results</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5"><bold>Overview of the computational pipeline</bold></title><p hwp:id="p-10">We prepare the ground truth masks from live cell images semi-automatically using our labeling tool (<bold><xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1a</xref></bold>). The images and the corresponding ground truth masks are preprocessed (see Methods for details), and they are used to train the deep neural networks for segmentation (<bold><xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1b</xref></bold>). The trained neural network generates a segmentation of the cell boundary, which can be used for morphodynamic profiling developed by Danuser’s group<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">5</xref></sup> (<bold><xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1c</xref></bold>). It measures local velocities of the cell boundary throughout the movie and summarizes local velocities for every probing spatial window and time frame. Since this quantification method is sensitive to pixel-level segmentation errors, accurate edge localization is necessary.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><title hwp:id="title-6">Overview of the computational pipeline.</title><p hwp:id="p-11">(<bold>a</bold>) Labeling Tool, (<bold>b</bold>) Deep Learning for Segmentation (MARS-Net), and (<bold>c</bold>) Quantification of Cellular Morphodynamics. White Bars: 32.5 μm.</p></caption><graphic xlink:href="191858v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-12">Deep learning requires a large training dataset, and labeling many frames per live cell movie by hand can take several hours or days. Also, there is an inconsistency in the quality of labeled images depending on the labeler’s experience. Therefore, we created the cell labeling tool to reduce human-labor by automating most labeling procedures and reproduce accurate and consistent labels. A systematic approach to create labels promotes reliable training and evaluation of the deep learning model<sup><xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>, <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref></sup>. The labeling tool takes the input image through series of image processing operations as follows. In the edge extraction step (<bold><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2a</xref></bold>), the input image is blurred using Gaussian, Bilateral, and Guided blurring operations, and the Canny edge detector<sup><xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref></sup> extracts edges from the blurred images. Three extracted edge images are combined to one edge image by adding their pixel intensity values at each coordinate. The errors such as fragmented edges and incorrect edge detection are inherent problems of conventional segmentation methods, so the users must correct the output for further processing. In the post processing step, edge images are converted into binarized segmented images, and any floating artifacts or noisy edges are removed from the edge images (<bold><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2b</xref></bold>). When running the labeling tool, users have to specify which side of the edge is foreground and background and adjust two hyper-parameters based on the input image characteristics. The hyper- parameters are kernel size for blurring operations and hysteresis thresholding min-max value for detecting edges.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><title hwp:id="title-7">Deep learning architecture.</title><p hwp:id="p-13">(<bold>a-b</bold>) Workflow in the labeling tool comprising edge detection step (<bold>a</bold>) and post-processing step (<bold>b</bold>). (<bold>c</bold>) The deep neural network, VGG19D-U- Net, for segmentation of cell edges. The number of filters in each convolutional block is shown underneath each convolutional block. Light violet lines indicate which features from the encoder are concatenated with which up-sampled features in the decoder.</p></caption><graphic xlink:href="191858v3_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-14">After the training sets are prepared, we trained the deep neural network, VGG19D-U-Net, which is a fully convolutional network with VGG19 encoder, U-Net decoder, and dropout layers (<bold><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2c</xref></bold>). VGG-19 encoder contains five convolutional blocks, each of which contains one max-pooling layer and multiple convolutional layers with a depth of 64-128- 256-512-512. The first convolutional block does not have a max-pooling layer. U-Net decoder has four deconvolutional blocks comprised of one up-sampling layer that concatenates with the encoded features and two convolutional layers with the depth of 512-256-128-64. Dropout layers are added after each max-pooling and up-sampling layer. The first dropout layer is set to drop 25% of the incoming units, and the rest of the dropout layers are set to drop 50% of the incoming units.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-8">Segmentation of phase contrast live cell movies using VGG19D-U-Net</title><p hwp:id="p-15">We first tested VGG19D-U-Net with a dataset from a single type of microscopy, which is five live cell movies of migrating PtK1 cells acquired by a phase contrast microscope for 200 frames at 5 sec/frame. The segmentation accuracy was measured by precision, recall, and F1 score of edge localization<sup><xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref></sup> (see Methods for details) since edge localization is our main criterion for evaluation. The Wilcoxon signed-rank test was used to test the statistical significance of performance difference, unless otherwise specified. We trained models on six different numbers of training frames (1,2,6,10,22,34) from each movie. The specified number of frames were randomly selected from each live cell movie as the training data. We used the leave-one-movie-out cross validation, in which one movie is selected for testing, and the other movies are used for training.</p><p hwp:id="p-16">We trained the segmentation architectures with various pretrained models integrated with U-Net decoder: VGG16, VGG19, ResNet50V2, and EfficientNetB7<sup><xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref></sup>. As demonstrated in the learning curve (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3a</xref></bold>), VGG19D-U-Net converged to lower validation loss than U- Net while their training losses are the same, suggesting less overfitting in VGG19D-U-Net than U-Net. Among different encoder models, VGG19D-U-Net yielded the highest F1 score across the different numbers of training frames (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3b</xref></bold>). Even with additional training frames, U-Net without pretraining could not surpass any other models trained on the equivalent number of frames. Notably, the F1 score of VGG19D-U-Net trained on one frame per movie is higher than U-Net trained on 34 frames per movie by 0.014 (0.929 vs 0.915). Overall, the F1 scores of all models tend to increase as more training frames were added, but their F1 scores plateaued as the number of training frames increased. When models were trained with ten frames per movie (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3d-f</xref></bold>), F1 score of VGG19D-U-Net was significantly higher than the next best model VGG16D-U-Net by 0.003 (0.937 vs 0.934) with p=4.69x10<sup>-6</sup>. These results demonstrate the importance of transfer learning and dropout layers for accurate segmentation of the live cell image regardless of the size of the training dataset.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5 xref-fig-3-6 xref-fig-3-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><title hwp:id="title-9">Performance comparison of the models trained on the phase contrast microscopy dataset.</title><p hwp:id="p-17">(<bold>a</bold>) Learning curves of U-Net and VGG19D-U-Net. Solid lines are average training loss, and dotted lines are average validation loss. (<bold>b</bold>) Average F1 scores of models trained on different numbers of frames per movie. (<bold>c</bold>) Training efficiency of seven different models in terms of model size, training time, and segmentation accuracy. The name of the model and its number of parameters in italics are written on the bubble. The size of a bubble is proportional to the number of parameters in the model. (<bold>d</bold>-<bold>f</bold>) Average F1, Precision and Recall of models. Suffix D denotes Dropout and suffix U-Net was omitted from the model names for brevity in the legend. The tests of significance by Wilcoxon signed-rank test with p &gt;= 0.05 are indicated by ns, p &lt; 0.05 are indicated by *, p &lt; 0.0001 are indicated by **. Error bars: 95% confidence intervals of the bootstrap mean.</p></caption><graphic xlink:href="191858v3_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-18">The model size, training time, and performance of the architectures trained on 34 frames per movie were summarized (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3c</xref></bold>). The EfficientNetB7-U-Net was the deepest network with the most parameters (71.1M) and took the longest time (4.7 hours) to train on average. ResNet50V2-U-Net took the least amount of time (0.81 hours) to train but had a lower F1 than VGG16-U-Net and VGG19-U-Net. The training time among U-Net, VGG16-U-Net, and VGG19-U-Net was similar, but VGG16-U-Net and VGG19-U-net have higher F1 than U-Net. Adding dropout layers to VGG16 or VGG19 encoders (VGG16D or VGG19D) makes the model more accurate without any additional parameters but requires longer training time. Since our criterion for the best model is the high F1 score, not model size or training time, VGG19D-U-Net with the highest F1 score (0.943) was chosen as the segmentation model in our pipeline.</p><p hwp:id="p-19">We also visually confirmed that VGG19D-U-Net localized the cell boundary more accurately than U-Net (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4</xref></bold>). VGG19D-U-Net finds the cell body regardless of the halo effect, unlike U-Net (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4a</xref> inset3</bold>). Also, U-Net incorrectly segmented background as the cell body (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4b</xref> inset1</bold>) or segmented cell body as the background (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4b</xref> inset2</bold>). In the progression of the segmented cell boundary throughout the movie (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig. 4c</xref></bold>), inaccurate segmentation of U-Net in multiple frames accumulated as indicated by the white dashed box in the upper left and center of the images. In contrast, VGG19D-U-Net produced a smooth transition of the cell boundary.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><title hwp:id="title-10">Visualization of segmentation results from U-Net and VGG19D-U-Net trained on the phase contrast microscopy dataset.</title><p hwp:id="p-20">(<bold>a-b</bold>) Edges extracted from the ground truth mask and predictions from U-Net and VGG19D-U-Net are overlaid on the first frame of the movie. Each edge is represented by one of three primary colors. Overlap of two or more edges is represented by the combination of those colors. (<bold>c</bold>) Progression of cell edges segmented by U-Net and VGG19D-U-Net overlaid on the first frame of the movie (blue, 0s; red, 1000s time points). Bars: 32.5 μm.</p></caption><graphic xlink:href="191858v3_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-21">We further investigated the roles of individual components in our VGG19D-U-Net structure (<bold><xref ref-type="fig" rid="figs1" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Supplementary Fig. 1a-c</xref></bold>). The segmentation accuracy of models in terms of F1, precision, and recall has a similar trend, so we refer to them collectively as the performance. U-Net, VGG16-U-Net, and VGG19-U-Net without pretraining have relatively similar performance. But pretraining VGG16-U-Net and VGG19-U-Net and adding dropout layers significantly increased their performance while having the same depth. Adding batch normalization layers to the VGG16-U-Net, and VGG19-U-Net reduced their performance. Also, combining a structured form of dropout for convolutional networks, DropBlock<sup><xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref></sup>, and batch normalization layers (VGG19DB-U-Net) which resembles SD- UNet<sup><xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref></sup>, resulted in significantly lower performance than VGG19D-U-Net. The performance of VGG19DB-U-Net might have been low due to the variance shift that occurs when using both dropout and batch normalization layers<sup><xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref></sup>.</p><p hwp:id="p-22">Different sizes of the cropped patch are also investigated (<bold><xref ref-type="fig" rid="figs1" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Supplementary Fig. 1d-f</xref></bold>). The size indicated here is the size of the image patches, so the size of the ground truth mask patches is smaller because our network crops out the output image (see Methods for details). The model trained on patches of size 96x96 had similar performance to other models trained on bigger patches. The models trained on 128x128, 192x192, and 256x256 patches had almost identical precision (0.969). However, if the size of a patch was reduced to 80x80 or 64x64, the performance of the model decreased significantly. These results suggest that the features relevant to detecting cellular boundary exist in the patch size of 128x128 even though it lacks contextual information of the entire cell body. Since training on smaller patches has the benefits of reducing memory usage and training on more diverse patches using the same computational resources, we used the patch size of 128x128 in our pipeline.</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-11">Segmentation of live cell movies from a single type of fluorescence microscopy using VGG19D-U-Net</title><p hwp:id="p-23">In this section, we tested the segmentation accuracy of VGG19D-U-Net using fluorescence live cell movies. The training sets consisted of five live cell movies of PtK1 cells expressing GFP-mDia1 using a spinning disk confocal (SDC) microscope for 200 frames at 5 sec/frame, and six live cell movies of PtK1 cells expressing paxillin-HaloTag- TMR acquired by a Total Internal Reflection Fluorescence (TIRF) microscope for 200 frames at 5 sec/frame. These live cell images are very challenging for the segmentation using conventional intensity-thresholding methods. The SDC images are highly noisy and low contrast because the cells expressed low levels of GFP-mDia1. Although the TIRF images have higher contrast and less noise than SDC images, they have other technical challenges as follows: i) high-intensity signals of paxillin accumulated in focal adhesions make edge segmentation difficult, particularly for intensity threshold-based methods, ii) the nonuniform light illumination of a TIRF microscope incurs additional issues for the segmentation, iii) the leading edge of cells could transiently lift up and leave the thin TIRF illumination, resulting in less visible cell edges.</p><p hwp:id="p-24">To prepare the reliable segmentation training sets for the SDC images, we also expressed SNAP-tag-actin and label it by TMR (SNAP-tag-TMR-actin) and performed the multiplexed imaging together with GFP-mDia1. The images in the channel of SNAP-tag- TMR-actin have good contrast along the cell boundary. Therefore, conventional image thresholding was applied to SNAP-tag-TMR-actin images, and the resulting binary masks were used as ground truth labels for the SDC datasets. To make more reliable ground truth masks for the TIRF images, the fluorescence images of the same cells were also taken using standard widefield illumination. We used our labeling tool to label the cell edges in the widefield images, which served as ground truth labels for the TIRF datasets.</p><p hwp:id="p-25">We trained U-Net and VGG19D-U-Net on the fluorescence SDC and TIRF datasets separately, and evaluated their performance by leave-one-movie-out cross validation. During training (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 5a,d</xref></bold>), VGG19D-U-Net converged to lower validation loss than U-Net while U-Net overfitted as epoch increased, as demonstrated by the increase of difference between training and validation loss. Across the different number of training frames, VGG19D-U-Net yielded a higher F1 score than U-Net on SDC datasets (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig. 5b</xref></bold>). For one of the movies, U-Net performed considerably worse than VGG19D-U-Net, indicated by the faint line with the least F1 score of about 0.2. Even though the average F1 score seems to stay consistent as the number of training frames increased in the graph, VGG19D-U-Net trained on 34 frames per movie had a greater average F1 score than the same model trained on one frame per movie by 0.028 (0.891 vs 0.863). When models were trained on ten training frames per movie, VGG19D-U-Net had a greater F1 than U- Net by 0.115 (0.866 vs 0.751) with p=2.95x10<sup>-</sup><sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref></sup>. Also, the distribution of evaluated frames in the F1 score (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig. 5c</xref></bold>) showed that all frames which were evaluated as 0 in F1 score when segmented by U-Net, had higher F1 scores when segmented by VGG19D. The superior performance of VGG19D-U-Net compared to U-Net is consistent with the results on phase contrast datasets.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6 xref-fig-5-7 xref-fig-5-8 xref-fig-5-9 xref-fig-5-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><title hwp:id="title-12">Performance comparison of U-Net and VGG19D-U-Net trained on fluorescence microscopy datasets.</title><p hwp:id="p-26">(<bold>a-c, g</bold>) Models trained on SDC datasets and (<bold>d-f, h</bold>) Models trained on TIRF datasets. (<bold>a,d</bold>) Learning curves of U-Net and VGG19-U-Net. Solid lines are average training loss, and dotted lines are average validation loss. (<bold>b, e</bold>) Average F1 scores of models trained on varying numbers of training frames. Lighter lines represent individual test set results in the leave-one-movie-out cross validation, and darker and thicker lines represent the average of all test set results. Error bars: 95% confidence intervals of the bootstrap mean. (<bold>c, f</bold>) The distribution of F1 score in violin plot and box plot in black with a median indicated by the white circle. Number of evaluated frames are (<bold>c</bold>) n=1000 and (<bold>d</bold>) n=132. The tests of significance by Wilcoxon signed-rank test with p &lt; 0.05 are indicated by * and p &lt; 0.0001 are indicated by **. <bold>(g-h)</bold> Visualization of edges extracted from ground truth masks and predictions from U-Net and VGG19D-U- Net overlaid on the first frame of the movie. Each edge is represented by one of three primary colors. Overlap of two or more edges is represented by the combination of those colors. (<bold>g</bold>) Bar: 7.2 μm. (<bold>h</bold>) Bar: 6.5 μm.</p></caption><graphic xlink:href="191858v3_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-27">On TIRF datasets (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig. 5e</xref></bold>), U-Net initially surpassed VGG19D-U-Net when trained on 1 or 2 frames per movie, but they converged to similar average F1 scores as the number of training frames increased. When models were trained on two frames per movie, VGG19D-U-Net had a lower average F1 score than U-Net by 0.02 (0.840 vs 0.860) with p=2.88x10<sup>-7</sup>. But when models were trained on ten training frames per movie, VGG19D- U-Net had a marginally lower average F1 score than U-Net by only 0.002 (0.871-0.873) with p=0.002. This similarity is also reflected in the distributions of the evaluated frames of both models (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig. 5f</xref></bold>).</p><p hwp:id="p-28">The visual inspection of edges segmented by U-Net and VGG19D-U-Net demonstrated that VGG19D-U-Net performed well on all SDC datasets, while U-Net failed to segment one of the SDC datasets (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Fig. 5g</xref></bold>). Both U-Net and VGG19D-U-Net did not perform well on one of the TIRF datasets, shown by the mismatch between the ground truth edge and segmented edges from U-Net and VGG19D-U-Net (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-7" hwp:rel-id="F5">Fig. 5h</xref></bold>). The mismatch is because of the limited illumination of the cell boundary that is hard to detect even for human eyes. The ground truth mask was created using the same cell images by widefield illumination, so a portion of the cell edge may be lifted from the surface and away from the thin illumination of the TIRF microscope.</p></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-13">Training of VGG19D-U-Net on the datasets from multiple types of microscopy</title><p hwp:id="p-29">We established that VGG19D-U-Net outperformed U-Net when they were trained with the phase contrast and the SDC datasets, respectively. Now, we prepare the training set from multiple types of microscopy by combining the previous data (phase contrast, SDC, and TIRF microscopy) and train VGG19D-U-Net on them to create MARS-Net. Same as the evaluation for the single-microscopy-type models, leave-one-movie-out cross validation was used. When this training strategy was applied, multiple-microscopy-type VGG19D- U-Net (VGG19D-U-Net<sup>M</sup>, MARS-Net) converged to lower validation loss than multiple- microscopy-type U-Net (U-Net<sup>M</sup>) without overfitting (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Fig. 6a</xref></bold>). VGG19D-U-Net<sup>M</sup> had a significantly higher F1 than single-microscopy-type VGG19D-U-Net (VGG19D-U-Net<sup>S</sup>) by 0.028 (0.904 vs 0.876), whereas there was not a significant difference in F1 between U- Net<sup>M</sup> and single-microscopy-type U-Net (U-Net<sup>S</sup>) (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Fig. 6b</xref></bold>). The distribution of the evaluated frames (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Fig. 6c</xref></bold>) from VGG19D-U-Net<sup>M</sup> also contained fewer outliers than other models.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6 xref-fig-6-7 xref-fig-6-8 xref-fig-6-9 xref-fig-6-10 xref-fig-6-11 xref-fig-6-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-6"><title hwp:id="title-14">Comparison of single-microscopy-type and multiple-microscopy-type training using U-Net and VGG19D-U-Net.</title><p hwp:id="p-30">(<bold>a</bold>) Learning curves of U-Net<sup>M</sup> and VGG19- U-Net<sup>M</sup>. Solid lines are average training loss, and dotted lines are average validation loss. (<bold>b</bold>) Average F1 scores across all datasets. “Single” represents single-microscopy- type, “Multi” represents multiple-microscopy-type, and “VGG19D” represents the VGG19D-U-Net. The statistical significance by Wilcoxon signed-rank test with p &gt;= 0.05 are indicated by ns, p &lt;0.05 are indicated by *, and p &lt; 0.001 are indicated by **. Error bars: 95% confidence intervals of the bootstrap mean. (<bold>c</bold>) The distribution of F1 scores in the violin plot and the box plot in black with a median indicated by the white circle. The statistical significances are not shown since they are indicated in (<bold>b</bold>). The number of evaluated frames is n=335. Every fifth frame in the movie is sampled to gather about 21 frames from each movie. (<bold>d</bold>) Close-up views of the segmentation results on all three microscopies: phase contrast, SDC and TIRF. Edges extracted from ground truth masks and predictions are overlaid on their corresponding original image. Each edge is represented by one of three primary colors. Overlap of two or more edges is represented by the combination of those colors. (<bold>e-g</bold>) Class activation map of the single-microscopy- type and multiple-microscopy-type VGG19D-U-Net with respect to the ground truth edge. The last layer in the third and fourth block of the encoder is visualized for one randomly chosen frame in the phase contrast (<bold>e</bold>) live cell movie in order from left to right. The last layer in the fifth block of the encoder is visualized for one randomly chosen frame in two of the SDC (<bold>f</bold>) and TIRF (<bold>g</bold>) live cell movies. In the heatmap, the value 0 means no activation, and 1 means the highest activation. The green line represents the ground truth edge of the cell body.</p></caption><graphic xlink:href="191858v3_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-31">When the performance was averaged per microscopy type (<bold><xref rid="figs2" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Supplementary Fig. 2 a-c</xref></bold>), VGG19D-U-Net<sup>M</sup> had a higher F1 than VGG19D-U-Net<sup>S</sup> for every microscopy type; It had marginally significantly higher F1 in the phase contrast dataset by 0.003 (0.933 vs 0.930) with p=0.039 (the paired sample t-test was used since their differences in F1 are normally distributed according to the Lilliefors test (p=0.062)). Also, VGG19D-U-Net<sup>M</sup> significantly improved F1 than VGG19D-U-Net<sup>S</sup> in the SDC dataset by 0.05 (0.911 vs 0.861) with p=2.61x10<sup>-</sup><sup><xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">51</xref></sup> and in the TIRF dataset by 0.029 (0.878 vs 0.849) with p=0.012 by Wilcoxon signed-rank test. While U-Net<sup>M</sup> marginally improved F1 than U-Net<sup>S</sup> in the SDC dataset by 0.012 (0.767 vs 0.755) with p=1.39x10<sup>-</sup><sup><xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-3" hwp:rel-id="ref-16">16</xref></sup> and not significantly improved F1 in the TIRF datasets with p=0.068, U-Net<sup>M</sup> significantly reduced F1 for the phase contrast dataset by (0.884 vs 0.898) with p=1.43x10<sup>-5</sup>. The distributions of the evaluated frames (<bold><xref ref-type="fig" rid="figs2" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Supplementary Fig. 2d-f</xref></bold>, <bold><xref ref-type="fig" rid="figs3" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Supplementary Fig. 3</xref></bold>) show that MARS-Net can accurately segment many frames that VGG19D-U-Net<sup>S</sup> could not handle in the SDC and TIRF datasets.</p><p hwp:id="p-32">In addition, the performance of MARS-Net was similar or greater than U-Net<sup>M</sup> on all microscopy types. Unlike VGG19D-U-Net<sup>S</sup>, which had a significantly lower F1 than U- Net<sup>S</sup> in the TIRF dataset, the difference of F1 between MARS-Net and U-Net<sup>M</sup> was not significant (p=0.637). The distributions of the evaluated frames (<bold><xref ref-type="fig" rid="figs2" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Supplementary Fig. 2d-f</xref></bold>, <bold><xref ref-type="fig" rid="figs3" hwp:id="xref-fig-10-2" hwp:rel-id="F10">Supplementary Fig. 3</xref></bold>) show that MARS-Net can accurately segment outlier frames that U-Net<sup>M</sup> could not handle in the phase contrast and SDC datasets. These results demonstrate that our deep learning architecture, VGG19D-U-Net was more effective in learning the cross-modal features from the datasets of multiple types of microscopy and generalizing to unseen datasets than U-Net.</p><p hwp:id="p-33">We also visually confirmed the performance between VGG19D-U-Net<sup>S</sup> and MARS-Net in edge localization of all three microscopies (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Fig. 6d</xref></bold>, <bold><xref ref-type="fig" rid="figs2" hwp:id="xref-fig-9-4" hwp:rel-id="F9">Supplementary Fig. 2g-i</xref></bold>). In most cases, both accurately localizes the edge and overlaps with the ground truth illustrated by white lines. However, in the cases that VGG19D-Net<sup>S</sup> made inaccurate edge localization, MARS-Net accurately localized the ground truth edge, shown as pink lines. Even for one of the TIRF movies that VGG19D-U-Net<sup>S</sup> struggled in the previous section (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-8" hwp:rel-id="F5">Fig. 5h</xref> inset2</bold>), MARS-Net can localize edges more accurately (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Fig. 6d</xref> inset5</bold>). Taken together, VGG19D-U-Net can be trained with live cell images from multiple types of microscopy and produce more accurate and robust segmentation than the single- microscopy-type model.</p><p hwp:id="p-34">To understand the effect of multiple-microscopy-type training on VGG19D-U-Net, we made the class activation maps of the convolutional layers in the encoder using SEG- GRAD-CAM<sup><xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref></sup> (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Fig. 6e-g</xref></bold>). The class activation map shows which pixels in the original image positively influence the feature maps in the convolutional layer to segment the cell boundary pixels. In the encoder of VGG19D-U-Net comprised of five blocks of convolutional layers, the last layers in each block are visualized. In phase contrast images, the activation map from the third and fourth block showed consistent differences in activated features between single and multiple-microscopy-type models (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-7" hwp:rel-id="F6">Fig. 6e</xref></bold>). In the third block, the multiple-microscopy-type model utilized features both on the outside and inside of the edge, while the multiple-microscopy-type model mainly utilized features on the outside of the edge exclusively for segmenting cell boundary. The activated regions from the multiple-microscopy-type model are associated with the brightness outside the cell boundary due to the halo effect in phase contrast microscopy. Also, in the fourth block, the multiple-microscopy-type model mainly utilized features inside the cell boundary, while the multiple-microscopy-type model utilized features along the cell boundary. These results illustrate that how changing the training dataset influence the same deep learning model to utilize different features in the image for segmentation.</p><p hwp:id="p-35">Complete arrangement of class activation maps from first to last blocks in the encoder for all live cell movies (<bold><xref ref-type="fig" rid="figs5" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Supplementary Fig. 5</xref>-<xref rid="figs7" ref-type="fig" hwp:id="xref-fig-14-1" hwp:rel-id="F14">7</xref></bold>) demonstrates that the earlier block spots low-level, fine-grained features, and the last block spots the entire cell body. Based on this observation, the multiple-microscopy-type models could spot the whole cell body in fluorescence images (SDC and TIRF), while the multiple-microscopy-type model could only find a portion of the cell body for some of the movies (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-8" hwp:rel-id="F6">Fig. 6f-g</xref></bold>). In total, the multiple- microscopy-type model could not find the entire cell body for 4 out of 11 fluorescence movies, while the multiple-microscopy-type model correctly found cell bodies in all fluorescence movies. These results suggest that the cross-modal features learned from the dataset from multiple types of microscopy are more effective than the multiple- microscopy-type dataset.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6 xref-fig-7-7 xref-fig-7-8 xref-fig-7-9 xref-fig-7-10 xref-fig-7-11 xref-fig-7-12"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-7"><title hwp:id="title-15">Morphodynamic profiling of the segmented movies by single- microscopy-type U-Net and MARS-Net.</title><p hwp:id="p-36">(<bold>a</bold>, <bold>c</bold>, <bold>e</bold>) Protrusion velocity maps made from U-Net<sup>S</sup> and MARS-Net. The black ellipses emphasize some of the erroneous regions (<bold>a,c</bold>) from U-Net<sup>S</sup> and the improved protrusion patterns from MARS-Net (<bold>e</bold>). (<bold>b</bold>) Overlay of the large regions of error/noise containing at least 10 pixels in the protrusion maps of the phase contrast movie (<bold>a</bold>) from U-Net<sup>S</sup> and MARS-Net. (<bold>d</bold>) Comparison of overall noise present in protrusion maps of the SDC movie (<bold>c</bold>) from U-Net<sup>S</sup> and MARS-Net. (<bold>f</bold>) Overlay of the large protrusion areas containing at least 10 pixels in the protrusion map of the TIRF movie (<bold>e</bold>) from U-Net<sup>S</sup> and MARS-Net.</p></caption><graphic xlink:href="191858v3_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-16">Quantitative Profiling of Cellular Morphodynamics</title><p hwp:id="p-37">After segmenting phase contrast, SDC and TIRF live cell movies by U-Net<sup>S</sup> and MARS- Net, we quantified local protrusion velocities to see how MARS-net improves cellular morphodynamics profiling<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">5</xref></sup> over the standard U-Net. The protrusion maps of phase contrast movies segmented by MARS-Net contain less errors or noise than the protrusion maps by U-Net<sup>S</sup> (<bold><xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. 7a</xref></bold>). We define the velocity errors as the dramatic change in protrusion or retraction velocity within a few frames due to the segmentation error, indicated by alternating red and blue color. To corroborate these visual observations, we located the errorneous parts in the protrusion map by thresholding the noise images from each protrusion map (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig. 7b</xref></bold>, <bold><xref ref-type="fig" rid="figs8" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Supplementary Fig. 8</xref></bold>) (see Methods for details). For the phase contrast movie, when pixels in large errorneous regions are counted, U-Net<sup>S</sup> produced more errors than MARS-Net by (567 vs 307). In the SDC movie, U-Net<sup>S</sup> not only produced more errorneous regions than MARS-Net but also produced more background noise (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Fig. 7c</xref></bold>). When all magnitude of noise were counted and plotted against their frequency, U-Net<sup>S</sup> produced more noise than MARS-Net (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Fig. 7d</xref></bold>). For the TIRF movie, instead of reducing the noise, MARS-Net exhibited stronger patterns of protruding cell boundary than U-Net<sup>S</sup>, illustrated by long columns of red in the black ellipses (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Fig. 7e</xref></bold>). For the quantitative comparison, we thresholded the noise-filtered protrusion maps (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Fig. 7f</xref></bold>) . When the pixels in large regions of the protrusive regions were counted, MARS-Net produced more protrusion than U-Net<sup>S</sup> by (1013 vs 582). Strongly protruding edges have low contrast since they are lifted upward and away from the TIRF illumination. Our analysis demonstrates that MARS-Net is capable of detecting these edges, allowing more accurate morphodynamic profiling. Taken together, considering protrusion maps can be used to identify phenotypes from subcellular movement<sup><xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref></sup>, both error/noise reduction and protrusion enhancement of the morphodynamics pattern from the accurate segmentation of MARS-Net will benefit further analysis.</p></sec></sec><sec id="s3" hwp:id="sec-8"><title hwp:id="title-17">Discussion</title><p hwp:id="p-38">Our transfer learning approach employing VGG19 and dropout layers is shown to be superior to conventional U-Net for segmenting live cell time-lapse images in both single- microscopy-type and multiple-microscopy-type training. ImageNet pretrained VGG19-U- Net has been effective in the segmentation of medical images<sup><xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>, <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref></sup>. Also, VGG19 encoder were better than other encoders with deeper layers, Res50V2 and EfficientNetB7. This may be because ResNetV2 and EfficientNetB7 reduce the input spatial dimension by half in their first convolutional layers, while VGG19 encoder preserves the input spatial dimension with convolutional layers that does convolution with padding. Since our objective is to segment cell boundary accurately, retaining low-level features in the first convolutional layers that can identify edges<sup><xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref></sup> are crucial for the localization of cell boundary. Moreover, the labeled training images required to get satisfactory segmentation results on the dataset from multiple microscopy types are only 1% (two frames per movie of 200 frames) in the leave-one-movie-out cross validation, which can facilitate cell biologists to adopt MARS-Net in their research.</p><p hwp:id="p-39">Cell images from different microscopies can have drastically different image qualities and distribution of intensities, so training on them together could have degraded the performance by confusing the model instead. In the similar work done by CellPose<sup><xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-3" hwp:rel-id="ref-35">35</xref></sup>, multiple types of cell images were combined to build one generalist model. Its generalist model had a similar segmentation accuracy to the specialist model trained on one type of cell image when they were evaluated on the specialist dataset. On the contrary, training on live cell movies of three different microscopes (phase contrast, SDC, TIRF), we were able to significantly enhance the segmentation model by extracting effective features for cell edges across different microscopy data instead of overfitting on single microscopy data. Remarkably, although three types of live cell images employed in this study are very difficult to be segmented by conventional algorithms, the cross-modal features synergistically learned by MARS-Net were able to successfully detect extremely low contrast cell edges that could not be detected by the single-microscopy-type model due to the noise and the limited TIRF illumination (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-9" hwp:rel-id="F6">Figure 6d</xref>, inset5 and 6</bold>)</p><p hwp:id="p-40">Through transfer learning with ImageNet pretrained weights, the deep learning model reuses diverse features learned from millions of images on the Internet<sup><xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-3" hwp:rel-id="ref-39">39</xref></sup>. This benefits the model to become invariant to various imaging conditions such as brightness, contrast, and camera resolution. Similarly, multiple-microscopy-type training benefits VGG19D-U- Net to become invariant to the imaging modality and attempt to create a robust model that identifies cell boundaries with semantic understanding, as shown by our activation maps. Another benefit of multiple-microscopy-type training is that it reduces the need to create new training datasets because the training dataset in one microscopy can be reused to analyze the dataset in another microscopy. This is consistent with the previous study demonstrating that multi-fidelity data was used to increase the size of the training set and improved the performance of the deep learning model in material science research<sup><xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref></sup>.</p><p hwp:id="p-41">Morphodynamic profiling has been usually done with high-contrast fluorescence live cell images amenable for standard threshold-based segmentation methods, limiting the throughput of the analysis pipeline. Particularly, phase contrast microscopy images had not been used due to the segmentation issues. Since phase contrast microscopy does not require expensive optical components and fluorescence labeling, MARS-Net, in conjunction with phase contrast microscopy, can substantially accelerate quantitative studies of cellular morphodynamics.</p></sec><sec id="s4" hwp:id="sec-9"><title hwp:id="title-18">Methods</title><sec id="s4a" hwp:id="sec-10"><title hwp:id="title-19">Data Collection</title><p hwp:id="p-42">Cell culture and live cell imaging procedures were followed according to the previous studies<sup><xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref></sup>. PtK1 cells were cultured in Ham’s F12 medium (Invitrogen) supplemented with 10% FBS, 0.1 mg ml<sup>−1</sup> streptomycin, and 100 U ml<sup>−1</sup> penicillin. Cells were then imaged at 5 second intervals for 1000 seconds using 0.45 NA Super Plan Fluor ELWD 20X ADM objective for phase contrast imaging and 60X, 1.4 NA Plan Apochromat objective for fluorescence spinning disk confocal imaging, 1.49NA Apochromat TIRF 100X for fluorescence TIRF imaging.</p><p hwp:id="p-43">PtK1 cells were transfected with the DNA constructs of GFP-mDia1 and SNAP-tag-actin or paxilin-HaloTag using Neon transfection system (Invitrogen) according to the manufacturer’s instructions (1 pulse, 1400 V, 20 ms) and were grown on acid-washed glass #1.5 coverslips for 2 days before imaging. Prior to imaging, expressed SNAP-tag- actin or paxillin-HaloTag proteins were labeled with SNAP-tag-TMR (New England BioLabs) or HaloTag-TMR (Promega) ligands, respectively according to the manufacturers’ instructions. All imaging was performed in imaging medium (Leibovitz’s L- 15 without phenol red, Invitrogen) supplemented with 10% fetal bovine serum (FBS), 0.1 mg ml<sup>−1</sup> streptomycin, 100 U ml<sup>−1</sup> penicillin, 0.45% glucose, 1.0 U ml<sup>−1</sup> Oxyrase (Oxyrase Inc.) and 10 mM Lactate. PtK1 cells were acquired from Gaudenz Danuser lab. They were routinely tested for mycoplasma contamination.</p><p hwp:id="p-44">All microscopy except for TIRF microscopy (described elsewhere<sup><xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref></sup>) was performed using the set up as follows: Nikon Ti-E inverted motorized microscope (including motorized focus, objective nosepiece, fluorescence filter turret, and condenser turret) with integrated Perfect Focus System, Yokogawa CSU-X1 spinning disk confocal head with a manual emission filter wheel with Spectral Applied Research Borealis modification, Spectral Applied Research custom laser merge module (LMM-7) with AOTF and solid state 445 nm (200 mW), 488 nm (200 mW), 514 nm (150 mW), 561 nm (200 mW), and 637 nm (140 mW) lasers, Semrock 405/488/561/647 and 442/514/647 dichroic mirrors, Ludl encoded XY stage, Ludl piezo Z sample holder for high speed optical sectioning, Prior fast transmitted and epi-fluorescence light path shutters, Hamamatsu Flash 4.0 LT sCMOS camera, 37 °C microscope incubator enclosure with 5% CO2 delivery (In Vivo), Molecular Devices MetaMorph v7.7, TMC vibration-isolation table.</p></sec><sec id="s4b" hwp:id="sec-11"><title hwp:id="title-20">Dataset</title><p hwp:id="p-45">The live cell movies used for training and evaluation of our pipeline are as follows.
<list list-type="simple" hwp:id="list-1"><list-item hwp:id="list-item-1"><label>•</label><p hwp:id="p-46">Five movies of label-free migrating PtK1 cells by a phase contrast microscope</p></list-item><list-item hwp:id="list-item-2"><label>•</label><p hwp:id="p-47">Five dual-color movies of PtK1 cells expressing GFP-mDia1 and SNAP-tag-actin by a Spinning Disk Confocal (SDC) microscope.</p></list-item><list-item hwp:id="list-item-3"><label>•</label><p hwp:id="p-48">Six movies of PtK1 cells expressing paxillin-HaloTag-TMR, a marker of cell-matrix adhesions by a Total Internal Reflection Fluorescence (TIRF) microscope</p></list-item></list>
Each live cell movie contains 200 frames, and about 40 frames per movie were labeled by our labeling tool for each phase contrast movie. For each SDC movie, all 200 frames were labeled by thresholding actin images. For each TIRF movie, 22 frames were labeled using the images from standard widefield fluorescence microscopy images and our labeling tool. Overall, 202 frames from phase contrast, 1000 frames from SDC, and 132 frames from TIRF movies are labeled to train and test our pipeline. The pixel size is 325nm for phase contrast datasets, 72nm for SDC datasets and 65nm for TIRF datasets.</p><p hwp:id="p-49">The ground truth masks for phase contrast and TIRF images are labeled using our labeling tool (<bold><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2a-b</xref></bold>). SDC images have the corresponding high contrast images of SNAP-tag-TMR-actin with good contrast along the cell boundary. Therefore, ground truth masks for SDC images can be labeled by applying denoising and thresholding without human intervention. The non-local means method implemented in ImageJ for denoising (sigma=15 and smoothing_factor=1) was applied to each SNAP-tag-TMR-actin image. Then, thresholding was applied to all frames with an optimal threshold determined by visually checking the generated masks and re-adjusting the threshold until the generated masks align the best with the cell boundary. The generated binary masks were used as the ground-truth for GFP-mDia1 fluorescence videos.</p></sec><sec id="s4c" hwp:id="sec-12"><title hwp:id="title-21">Training Dataset Preparation</title><p hwp:id="p-50">Before training the deep learning model, frames and their ground truth mask are processed for training and testing. Six different numbers of training frames (1,2,6,10,22,34) are randomly selected from each live cell movie except for the test set movie to determine the adequate number of training frames to train the model. The chosen frames become part of the training/validation set. Then, 200 patches of 128X128 pixels are randomly cropped from each frame and its corresponding ground truth mask. The cropping is necessary to reduce memory and computational requirement and to ensure that the size of the input image works in our model. 60% of the cropped images are from the boundary of cytoplasm illustrated by red boxes, and 20% are from inside, and other 20% are from outside of the cytoplasm illustrated by blue boxes in <bold><xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1b</xref></bold>. Both cropped images and masks are in grayscale and called patches. Only patch sizes in multiple of 16 are allowed because other sizes cause a mismatch of spatial size between encoded features and decoded features when concatenating them in U-Net structure.</p><p hwp:id="p-51">Patches are augmented to negate the effect of small training size and improve performance. The augmentation methods include random rotation within 50 degrees, width, and height shift within 10% of the image’s width and height, shear in counter- clockwise direction within 36 degrees, zoom in or out randomly within 10% of the image size, and horizontal and vertical flips of the image. The original image’s reflection replaces the portion of the augmented image outside the boundary of the original image. The default number of augmented patches is 6400. For instance, the total number of patches in training and validation sets given two frames from each live cell movie in leave-one- movie-out cross validation is 8000 (2 x 4 x 200 + 6400). Then, patches are randomly split into training and validation sets with a ratio of 80:20.</p><p hwp:id="p-52">Image patches are preprocessed to facilitate the deep learning model training. For phase contrast and SDC datasets, all image patches from one movie are standardized based on the mean µ and the standard deviation δ of pixel values of the cropped and augmented patches in that movie. In this way, the distribution of pixel values per movie has the mean and standard deviation equal to zero and one, respectively. Image patches from the TIRF dataset have poor contrast, so they are preprocessed differently from phase contrast or SDC datasets. After the mean µ and the standard deviation δ of pixel values of a TIRF movie are calculated, the pixel values <italic toggle="yes">x</italic><sub><italic toggle="yes">i,j</italic></sub> are replaced with the following values when they are less than µ − 2δ or greater than µ + 3δ.
<disp-formula hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="191858v3_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
In the end, the min-max normalization was applied to rescale the pixel ranges to [0, 1].
<disp-formula hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="191858v3_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
For prediction, images and masks are not cropped or augmented, but the same standardization or preprocessing steps are applied based on the microscopy type.</p></sec><sec id="s4d" hwp:id="sec-13"><title hwp:id="title-22">Neural Network Architecture</title><p hwp:id="p-53">All models mentioned in this paper are based on the same U-Net structure comprised of encoder and decoder. The original U-Net encoder was replaced by other encoders such as VGG16, VGG19, ResNet50V2, and EfficientNetB7, but the same U-Net decoder was used without any modification. Weights of the standard U-Net encoder were randomly initialized, and encoders in other models were pretrained with ImageNet provided by Keras. Every model had four skip connections that concatenate encoded features with decoded features.</p><p hwp:id="p-54">The convolutional filter size is 3x3, and the zero-padding in each convolutional layer of U-Net and VGG models yields the feature map with the same spatial size after convolution. The size of the input patch is 128x128, and the size of the max-pooling and up-sampling filter is both 2x2. The same size of max-pooling and up-sampling filters make the max- pooled feature map and the up-sampled feature map at the same hierarchical level to have the same spatial size. The last layer of the network crops the 128x128 output by 30 pixels on all sides to get the segmented image of size 68x68. Cropping is necessary to eliminate the boundary effects. Without cropping, the segmented image is hazy along the image boundary, lowering the segmentation model’s accuracy.</p><p hwp:id="p-55">In the prediction step, every frame of the movie in the test set is segmented by the trained model. The image is not cropped into 128x128 pixel patches, but its width and height are padded with its reflection by about 30 pixels. Then, the padded regions are removed from the predicted binary mask to avoid boundary effects.</p></sec><sec id="s4e" hwp:id="sec-14"><title hwp:id="title-23">Neural Network Training Settings</title><p hwp:id="p-56">For a fair comparison of models, each model’s hyperparameters were configured the same as follows: Adam<sup><xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">62</xref></sup> optimizer with learning rate=10<sup>-5</sup>, batch size=64, input size=128, output size=68, early stopping patience=3. In the decoder and encoder that is not pretrained, the kernel weights were initialized with Glorot uniform<sup><xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">63</xref></sup>, and the bias weights were initialized with zeros. The pretrained models were fine-tuned without freezing any weights.</p><p hwp:id="p-57">The binary cross-entropy was used as a loss function for training. To avoid overfitting, we used the early stopping, so training stopped when the validation loss did not decrease during the three consecutive epochs. For the phase contrast and SDC datasets, early stopping patience was 3, and the maximum epoch was 100. For the TIRF dataset, early stopping patience was 10, and the maximum epoch was 300. We used default parameters in the Keras for other parameters. The neural network training was performed using TensorFlow<sup><xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">64</xref></sup> 2.3 on RTX Titan GPU with CUDA 10.1 for multiple-microscopy-type phase contrast models and multiple-microscopy-type models and TensorFlow 1.15 on GTX 1080Ti GPU with CUDA 10.0 for multiple-microscopy-type SDC models and multiple-microscopy-type TIRF models.</p></sec><sec id="s4f" hwp:id="sec-15"><title hwp:id="title-24">The Number of Training Frames</title><p hwp:id="p-58">The number of training frames per movie in leave-one-movie-out cross validation (F<sub>C</sub>) and the total number of training frames (F<sub>T</sub>) used to train the model in the results section are described here.
<list list-type="simple" hwp:id="list-2"><list-item hwp:id="list-item-4"><label>•</label><p hwp:id="p-59">F<sub>C</sub> = 34, F<sub>T</sub> = 136 (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3c</xref></bold>)</p></list-item><list-item hwp:id="list-item-5"><label>•</label><p hwp:id="p-60">F<sub>C</sub> = 10, F<sub>T</sub> = 40 (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-6" hwp:rel-id="F3">Fig. 3a,d-f</xref> , <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-9" hwp:rel-id="F5">Fig. 5a,c,g</xref>, <xref ref-type="fig" rid="figs1" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Supplementary Fig. 1</xref></bold>)</p></list-item><list-item hwp:id="list-item-6"><label>•</label><p hwp:id="p-61">F<sub>C</sub> = 10, F<sub>T</sub> = 50 (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-10" hwp:rel-id="F5">Fig. 5d,f,h</xref></bold>)</p></list-item><list-item hwp:id="list-item-7"><label>•</label><p hwp:id="p-62">F<sub>C</sub> = 2, F<sub>T</sub> = 8 (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig. 4</xref></bold>)</p></list-item><list-item hwp:id="list-item-8"><label>•</label><p hwp:id="p-63">F<sub>C</sub> = 2, F<sub>T</sub> = 30 (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-10" hwp:rel-id="F6">Fig. 6a</xref></bold>)</p></list-item><list-item hwp:id="list-item-9"><label>•</label><p hwp:id="p-64">F<sub>C</sub> = 2, and F<sub>T</sub> = 30 for multiple-microscopy-type models, F<sub>T</sub> = 8 for single- microscopy-type model trained on phase contrast or SDC datasets or F<sub>T</sub> = 10 for single-microscopy-type model trained on TIRF dataset (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-11" hwp:rel-id="F6">Fig. 6b-g</xref>, <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-7" hwp:rel-id="F7">Fig. 7</xref> <xref ref-type="fig" rid="figs2" hwp:id="xref-fig-9-5" hwp:rel-id="F9">Supplementary Fig. 2</xref><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-8" hwp:rel-id="F7">-7</xref></bold>).</p></list-item></list>
</p></sec><sec id="s4g" hwp:id="sec-16"><title hwp:id="title-25">Cross Validations</title><p hwp:id="p-65">To rigorously test our deep learning model’s generalizability and reproducibility, we evaluated every model by leave-one-movie-out cross validation. It is the same as leave- one-subject-out cross validation<sup><xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">65</xref></sup> but with the subject replaced by the live cell movie. We set aside one movie as a test set, and the rest of the movies are used for training and validation. Frames in the same live cell movie have little difference in image features, but there is a distinctive visual difference even among the live cell movies taken by the same microscopy. Therefore, we consider frames in the same live cell movie to be independent and identically distributed (i.i.d) and live cell movies to be out of distribution (o.o.d). The leave-one-movie-out cross validation ensures that our model is assessed on the o.o.d test set to prevent shortcut learning<sup><xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">66</xref></sup>. For instance, given five live cell movies called A, B, C, D, and E, movie E is set aside as a test set, and movies A, B, C, and D become training/validation sets. In the subsequent validation, movie D is set aside as a test set, and movies A, B, C, and E become training/validation sets. This process is repeated until every movie is set aside as the test set once. Then, the test performance measures are averaged.</p><p hwp:id="p-66">Our segmentation pipeline trained on the same dataset can yield different segmentation results due to random selection of frames, random cropping, and random train/validation set split. In order to reduce the variations caused by them, the leave-one-movie-out cross validation is repeated five times for single-microscopy-type phase contrast models in <bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-7" hwp:rel-id="F3">Fig. 3</xref></bold>, and <bold><xref ref-type="fig" rid="figs1" hwp:id="xref-fig-8-4" hwp:rel-id="F8">Supplementary Fig. 1a-c</xref></bold>. Frames and patches were randomly selected and randomly split into training and validation set in each repetition.</p></sec><sec id="s4h" hwp:id="sec-17"><title hwp:id="title-26">Evaluation Metrics</title><p hwp:id="p-67">Precision, recall, and F1 score between ground truth edges and segmented edges are calculated by the edge correspondence algorithm in the Berkeley Segmentation Benchmark<sup><xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">52</xref>, <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">67</xref></sup>, with the search radii (Phase Contrast: 3 pixels; SDC: 5 pixels, TIRF: 5 pixels). The package performs bipartite matching of two edge images by iteratively matching how edge pixels in one image corresponds to edge pixels in another image. For instance, an edge pixel in the first image is counted as a match if there is an edge pixel in the second image within the search radii of the target pixel.</p><p hwp:id="p-68">Before evaluation, both ground truth masks and segmentation by the models are image processed. The image processing steps include thresholding the grayscale images into binary images with a threshold value of 0.5, given intensity values ranging from 0 to 1, filling small holes, and extracting edge by the canny edge detector. Since images are binarized before evaluation, the intensity value of each pixel in the image is either 0 or 1. The match between ground truth pixels and segmented pixels of intensity 1 are true positives (<italic toggle="yes">tp</italic>). The segmented pixels of intensity 1 that do not match with ground truth pixels are false positives (<italic toggle="yes">fp</italic>). And the ground truth pixels of intensity 1 that do not match with segmented pixels are false negatives (<italic toggle="yes">fn</italic>). True negatives, which are the match between ground truth pixel of intensity 0 and segmented pixel of intensity 0, are ignored. After counting <italic toggle="yes">tp</italic>, <italic toggle="yes">fp</italic>, <italic toggle="yes">and fn</italic> in an image, the metrics are calculated as follows.
<disp-formula hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="191858v3_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula>
For each metric, every segmented frame is evaluated, and evaluated frames are bootstrapped with 1000 replicates to calculate bootstrap mean and 95% confidence interval. The image processing and evaluation are done using MATLAB R2019b.</p></sec><sec id="s4i" hwp:id="sec-18"><title hwp:id="title-27">Profiling of Cellular Morphodynamics</title><p hwp:id="p-69">The steps taken to perform quantitative profiling of cellular morphodynamics<sup><xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-5" hwp:rel-id="ref-5">5</xref></sup> on segmented movies is described in detail (<bold><xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1c</xref></bold>). The live cell movie is cropped as illustrated by the dashed white rectangle, and the velocity of the cell along its boundary is estimated based on the difference of segmented area in the previous and present frames. Then, the estimated velocity is grouped into rectangular blocks called "window" to get a smoother estimate of the velocity. At the local sampling step, the outermost band of windows along the boundary of the cell is sampled to draw a protrusion activity map showing the velocity at each window number and frame number. Inner bands inside the cell are ignored. The size of each window is 6 pixels, or 1.95μm for phase contrast dataset, 7 pixels, or 504nm for SDC, and 8 pixels, or 520nm for TIRF datasets.</p><p hwp:id="p-70">The signal in the protrusion map is found by the cubic smoothing spline interpolation of the protrusion maps with the smoothing parameter (p=0.7). The error/noise is calculated by subtracting the original protrusion map with the spline filtered protrusion map. To ignore the regions of error/noise and protrusion of small magnitudes, thresholding operation set magnitude lower than 3μm/min to zero for the phase contrast movie and lower than 2μm/min to zero for the SDC and the TIRF movies. Then, small connected regions of error/noise or protrusion signal containing less than 10 pixels are removed (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-9" hwp:rel-id="F7">Fig. 7b,f</xref></bold> and <bold><xref ref-type="fig" rid="figs8" hwp:id="xref-fig-15-2" hwp:rel-id="F15">Supplementary Fig. 8</xref></bold>) to highlight the large error from U-Net<sup>S</sup> or protrusion signal from MARS-Net that facilitates further analysis. For the SDC dataset (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-10" hwp:rel-id="F7">Fig. 7d</xref></bold>), the histogram of all error magnitude without thresholding was plotted against its log frequency as a line graph.</p></sec><sec id="s4j" hwp:id="sec-19"><title hwp:id="title-28">Class Activation Map</title><p hwp:id="p-71">The technique called SEG-GRAD-CAM<sup><xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">57</xref></sup> visualizes the feature maps that are positively associated with the increase in the intensity of output pixels. Unlike GRAD-CAM<sup><xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">68</xref></sup>, which is designed for classifiers that output a vector, SEG-GRAD-CAM can explain the decision of the segmentation model that outputs a 2-dimensional matrix. Our region-of-interest is the cell boundary, so we visualized activation of feature maps with respect to the edge extracted from the ground truth mask.</p><p hwp:id="p-72">Let A<sup>k</sup> be the k<sup>th</sup> feature map in the filter. Among convolutional layers in the VGG19D-U- Net, we are interested in the last layer of each block in the encoder. The total number of feature maps from the first to last blocks are as follows: 64, 128, 256, and 512. The output of the model, <italic toggle="yes">y,</italic> only has one channel, and its value ranges from 0 to 1. <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> are indexes of the pixels that correspond to the cell boundary <italic toggle="yes">C</italic> in the output image <italic toggle="yes">y</italic>, and <italic toggle="yes">u</italic> and <italic toggle="yes">v</italic> are indexes of the spatial location in <italic toggle="yes">A<sup>k</sup></italic>. N is the total number of pixels in <italic toggle="yes">A<sup>k</sup></italic>. Then, the importance of the feature map at each spatial location can be computed as follows.
<disp-formula hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="191858v3_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula>
For every pixel of <italic toggle="yes">y</italic>, the gradients with respect to all pixels in the feature map are calculated by backpropagation and global average pooled across the spatial dimensions of <italic toggle="yes">A<sup>k</sup></italic>. The weight matrix <italic toggle="yes">ak</italic>, which the same spatial dimension as <italic toggle="yes">A<sup>k</sup></italic>, dot products with <italic toggle="yes">A<sup>k</sup></italic> to get the weighted sum of the feature maps.
<disp-formula hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="191858v3_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula>
The weighted sum of the feature maps or heatmap is spatially scaled up by bilinear interpolation to match the input image size. Scaling up is necessary because the heatmaps from different convolutional feature maps have different spatial sizes. Scaled- up heatmaps are overlaid with their corresponding ground truth edge and can be compared with each other. Finally, ReLU is applied to ignore the negative influence of the feature maps on the prediction of a cell boundary.
<disp-formula hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="191858v3_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives>
</disp-formula>
</p></sec></sec><sec id="s5" hwp:id="sec-20"><title hwp:id="title-29">Code availability statement</title><p hwp:id="p-73">The code for our deep learning-based segmentation pipeline and evaluation code is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://rc-gitlab.chboston.org/kleelab/MARS-Net" ext-link-type="uri" xlink:href="https://rc-gitlab.chboston.org/kleelab/MARS-Net" hwp:id="ext-link-2">https://rc-gitlab.chboston.org/kleelab/MARS-Net</ext-link></p></sec><sec id="s6" hwp:id="sec-21"><title hwp:id="title-30">Data availability statement</title><p hwp:id="p-74">The datasets used in the current study are available from the corresponding author on a reasonable request.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-31">Acknowledgments</title><p hwp:id="p-75">We thank Microsoft for providing us with Azure cloud computing resources (Microsoft Azure Research Award), and Boston Scientific for providing us with the gift for deep learning research. This work was supported by NIH grant GM122012 and GM133725.</p></ack><sec id="s7" hwp:id="sec-22"><title hwp:id="title-32">Author Contributions</title><p hwp:id="p-76">CW and XZ initiated the project. JJ and CW designed the pipeline. JJ wrote the final version of the manuscript and supplement. JJ, CW, XZ, BL, and YY wrote the code for the segmentation model. JJ built the labeling tool. HC performed the live cell imaging experiments. JJ, CW, HC, XP, MR, and YC prepared the training sets. JJ visualized feature maps using SEG-GRAD-CAM and profiled cellular morphodynamics from the segmented movies. KL coordinated the study and wrote the final version of the manuscript and supplement. All authors discussed the results of the study.</p></sec><sec id="s8" hwp:id="sec-23"><title hwp:id="title-33">Competing Interests</title><p hwp:id="p-77">The authors declare no competing financial or non-financial interests.</p></sec><sec id="s9" hwp:id="sec-24"><title hwp:id="title-34">Author Information</title><p hwp:id="p-78">Correspondence and requests for materials, data, and code should be addressed to KL (<email hwp:id="email-2">kwonmoo.lee@childrens.harvard.edu</email>).</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-35">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Hermans T. M."><surname>Hermans</surname>, <given-names>T. M.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-2">Motility efficiency and spatiotemporal synchronization in non- metastatic vs. metastatic breast cancer cells</article-title>. <source hwp:id="source-1">Integr Biol (Camb</source><italic toggle="yes">)</italic> <volume>5</volume>, <fpage>1464</fpage>–<lpage>1473</lpage>, doi:<pub-id pub-id-type="doi">10.1039/c3ib40144h</pub-id> (<year>2013</year>).</citation></ref><ref id="c2" hwp:id="ref-2"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Manak M. S."><surname>Manak</surname>, <given-names>M. S.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-3">Live-cell phenotypic-biomarker microfluidic assay for the risk stratification of cancer patients via machine learning</article-title>. <source hwp:id="source-2">Nat Biomed Eng</source> <volume>2</volume>, <fpage>761</fpage>–<lpage>772</lpage>, doi:<pub-id pub-id-type="doi">10.1038/s41551-018-0285-z</pub-id> (<year>2018</year>).</citation></ref><ref id="c3" hwp:id="ref-3"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Leithner A."><surname>Leithner</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-4">Diversified actin protrusions promote environmental exploration but are dispensable for locomotion of leukocytes</article-title>. <source hwp:id="source-3">Nat Cell Biol</source> <volume>18</volume>, <fpage>1253</fpage>–<lpage>1259</lpage>, doi:<pub-id pub-id-type="doi">10.1038/ncb3426</pub-id> (<year>2016</year>).</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Buggenthin F."><surname>Buggenthin</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-5">Prospective identification of hematopoietic lineage choice by deep learning</article-title>. <source hwp:id="source-4">Nat Methods</source> <volume>14</volume>, <fpage>403</fpage>–<lpage>406</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nmeth.4182</pub-id> (<year>2017</year>).</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4 xref-ref-5-5"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Machacek M."><surname>Machacek</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Danuser G"><surname>Danuser</surname>, <given-names>G</given-names></string-name>. <article-title hwp:id="article-title-6">Morphodynamic profiling of protrusion phenotypes</article-title>. <source hwp:id="source-5">Biophys J</source> <volume>90</volume>, <fpage>1439</fpage>–<lpage>1452</lpage>, doi:<pub-id pub-id-type="doi">10.1529/biophysj.105.070383</pub-id> (<year>2006</year>).</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Lee K."><surname>Lee</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-7">Functional hierarchy of redundant actin assembly factors revealed by fine- grained registration of intrinsic image fluctuations</article-title>. <source hwp:id="source-6">Cell Syst</source> <volume>1</volume>, <fpage>37</fpage>–<lpage>50</lpage>, doi:<pub-id pub-id-type="doi">10.1016/j.cels.2015.07.001</pub-id> (<year>2015</year>).</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Wang C."><surname>Wang</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-8">Deconvolution of subcellular protrusion heterogeneity and the underlying actin regulator dynamics from live cell imaging</article-title>. <source hwp:id="source-7">Nat Commun</source> <volume>9</volume>, <fpage>1688</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41467-018-04030-0</pub-id> (<year>2018</year>).</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Machacek M."><surname>Machacek</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-9">Coordination of Rho GTPase activities during cell protrusion</article-title>. <source hwp:id="source-8">Nature</source> <volume>461</volume>, <fpage>99</fpage>–<lpage>103</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nature08242</pub-id> (<year>2009</year>).</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Stephens D. J."><surname>Stephens</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Allan V. J"><surname>Allan</surname>, <given-names>V. J</given-names></string-name>. <article-title hwp:id="article-title-10">Light microscopy techniques for live cell imaging</article-title>. <source hwp:id="source-9">Science</source> <volume>300</volume>, <fpage>82</fpage>–<lpage>86</lpage> (<year>2003</year>).</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Li K."><surname>Li</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kanade T"><surname>Kanade</surname>, <given-names>T</given-names></string-name>. <article-title hwp:id="article-title-11">Nonnegative mixed-norm preconditioning for microscopy image segmentation</article-title>. <source hwp:id="source-10">International Conference on Information Processing in Medical Imaging</source>, <fpage>362</fpage>–<lpage>373</lpage> (<year>2009</year>).</citation></ref><ref id="c11" hwp:id="ref-11"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Ambühl M. E."><surname>Ambühl</surname>, <given-names>M. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brepsant C."><surname>Brepsant</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meister J. J."><surname>Meister</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Verkhovsky A. B."><surname>Verkhovsky</surname>, <given-names>A. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sbalzarini I. F"><surname>Sbalzarini</surname>, <given-names>I. F</given-names></string-name>. <article-title hwp:id="article-title-12">High- resolution cell outline segmentation and tracking from phase-contrast microscopy images</article-title>. <source hwp:id="source-11">Journal of microscopy</source> <volume>245</volume>, <fpage>161</fpage>–<lpage>170</lpage> (<year>2012</year>).</citation></ref><ref id="c12" hwp:id="ref-12"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Bensch R."><surname>Bensch</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Ronneberger O."><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name> <article-title hwp:id="article-title-13">Cell segmentation and tracking in phase contrast images using graph cut with asymmetric boundary costs</article-title>. <source hwp:id="source-12">2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)</source>, <fpage>1220</fpage>–<lpage>1223</lpage> (<year>2015</year>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Vicar T."><surname>Vicar</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-14">Cell segmentation methods for label-free contrast microscopy: review and comprehensive comparison</article-title>. <source hwp:id="source-13">BMC Bioinformatics</source> <volume>20</volume>, doi:<pub-id pub-id-type="doi">10.1186/s12859-019-2880-8</pub-id> (<year>2019</year>).</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Otsu N"><surname>Otsu</surname>, <given-names>N</given-names></string-name>. <article-title hwp:id="article-title-15">A threshold selection method from gray-level histograms</article-title>. <source hwp:id="source-14">IEEE transactions on systems, man, and cybernetics</source> <volume>9</volume>, <fpage>62</fpage>–<lpage>66</lpage> (<year>1979</year>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Canny J"><surname>Canny</surname>, <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-16">A computational approach to edge detection</article-title>. <source hwp:id="source-15">IEEE Transactions on pattern analysis and machine intelligence</source>, <fpage>679</fpage>–<lpage>698</lpage> (<year>1986</year>).</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2 xref-ref-16-3"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Chan T. F."><surname>Chan</surname>, <given-names>T. F.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Vese L. A"><surname>Vese</surname>, <given-names>L. A</given-names></string-name>. <article-title hwp:id="article-title-17">Active contours without edges</article-title>. <source hwp:id="source-16">IEEE Transactions on image processing</source> <volume>10</volume>, <fpage>266</fpage>–<lpage>277</lpage> (<year>2001</year>).</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Isola P."><surname>Isola</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zoran D."><surname>Zoran</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krishnan D."><surname>Krishnan</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Adelson E. H"><surname>Adelson</surname>, <given-names>E. H</given-names></string-name>. <article-title hwp:id="article-title-18">Crisp boundary detection using pointwise mutual information</article-title>. <source hwp:id="source-17">European Conference on Computer Vision</source>, <fpage>799</fpage>–<lpage>814</lpage> (<year>2014</year>).</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="LeCun Y."><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G."><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-19">Deep learning</article-title>. <source hwp:id="source-18">Nature</source> <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nature14539</pub-id> (<year>2015</year>).</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Krizhevsky A."><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G. E"><surname>Hinton</surname>, <given-names>G. E</given-names></string-name>. <article-title hwp:id="article-title-20">Imagenet classification with deep convolutional neural networks</article-title>. <source hwp:id="source-19">Advances in neural information processing systems</source>, <fpage>1097</fpage>-<lpage>1105</lpage> (<year>2012</year>).</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Simonyan K."><surname>Simonyan</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zisserman A"><surname>Zisserman</surname>, <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-21">Very deep convolutional networks for large-scale image recognition</article-title>. <source hwp:id="source-20">International Conference on Learning Representations</source> (<year>2015</year>).</citation></ref><ref id="c21" hwp:id="ref-21"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang X."><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ren S."><surname>Ren</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sun J"><surname>Sun</surname>, <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-22">Deep residual learning for image recognition</article-title>. <source hwp:id="source-21">Proceedings of the IEEE conference on computer vision and pattern recognition</source>, <fpage>770</fpage>–<lpage>778</lpage> (<year>2016</year>).</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang X."><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ren S."><surname>Ren</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sun J"><surname>Sun</surname>, <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-23">Identity Mappings in Deep Residual Networks</article-title>. <source hwp:id="source-22">arXiv</source>, doi:arXiv:1603.05027 (<year>2016</year>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Long J."><surname>Long</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shelhamer E."><surname>Shelhamer</surname>, <given-names>E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Darrell T"><surname>Darrell</surname>, <given-names>T</given-names></string-name>. <article-title hwp:id="article-title-24">Fully convolutional networks for semantic segmentation</article-title>. <source hwp:id="source-23">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>3431</fpage>-<lpage>3440</lpage> (<year>2015</year>).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2 xref-ref-24-3"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Ronneberger O."><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fischer P."><surname>Fischer</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Brox T"><surname>Brox</surname>, <given-names>T</given-names></string-name>. <article-title hwp:id="article-title-25">U-net: Convolutional networks for biomedical image segmentation</article-title>. <source hwp:id="source-24">International Conference on Medical Image Computing and Computer-Assisted Intervention</source>, <fpage>234</fpage>–<lpage>241</lpage> (<year>2015</year>).</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Bertasius G."><surname>Bertasius</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shi J."><surname>Shi</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Torresani L"><surname>Torresani</surname>, <given-names>L</given-names></string-name>. <article-title hwp:id="article-title-26">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</article-title>. <source hwp:id="source-25">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>4380</fpage>–<lpage>4389</lpage> (<year>2015</year>).</citation></ref><ref id="c26" hwp:id="ref-26"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Shen W."><surname>Shen</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang X."><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang Y."><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bai X."><surname>Bai</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zhang Z"><surname>Zhang</surname>, <given-names>Z</given-names></string-name>. <article-title hwp:id="article-title-27">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</article-title>. <source hwp:id="source-26">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>3982</fpage>-<lpage>3991</lpage> (<year>2015</year>).</citation></ref><ref id="c27" hwp:id="ref-27"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Badrinarayanan V."><surname>Badrinarayanan</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kendall A."><surname>Kendall</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Cipolla R"><surname>Cipolla</surname>, <given-names>R</given-names></string-name>. <article-title hwp:id="article-title-28">SegNet: A Deep Convolutional Encoder- Decoder Architecture for Image Segmentation</article-title>. <source hwp:id="source-27">IEEE Trans Pattern Anal Mach Intell</source> <volume>39</volume>, <fpage>2481</fpage>–<lpage>2495</lpage>, doi:<pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id> (<year>2017</year>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Ahmed I."><surname>Ahmed</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ahmad M."><surname>Ahmad</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khan F. A."><surname>Khan</surname>, <given-names>F. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Asif M"><surname>Asif</surname>, <given-names>M</given-names></string-name>. <article-title hwp:id="article-title-29">Comparison of Deep-Learning-Based Segmentation Models: Using Top View Person Images</article-title>. <source hwp:id="source-28">IEEE Access</source> <volume>8</volume>, <fpage>136361</fpage>–<lpage>136373</lpage>, doi:<pub-id pub-id-type="doi">10.1109/access.2020.3011406</pub-id> (<year>2020</year>).</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Falk T."><surname>Falk</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-30">U-Net: deep learning for cell counting, detection, and morphometry</article-title>. <source hwp:id="source-29">Nat Methods</source> <volume>16</volume>, <fpage>67</fpage>–<lpage>70</lpage>, doi:<pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id> (<year>2019</year>).</citation></ref><ref id="c30" hwp:id="ref-30"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Sadanandan S. K."><surname>Sadanandan</surname>, <given-names>S. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ranefall P."><surname>Ranefall</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Le Guyader S."><surname>Le Guyader</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Wahlby C"><surname>Wahlby</surname>, <given-names>C</given-names></string-name>. <article-title hwp:id="article-title-31">Automated Training of Deep Convolutional Neural Networks for Cell Segmentation</article-title>. <source hwp:id="source-30">Sci Rep</source> <volume>7</volume>, <fpage>7860</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41598-017-07599-6</pub-id> (<year>2017</year>).</citation></ref><ref id="c31" hwp:id="ref-31"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="McQuin C."><surname>McQuin</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-32">CellProfiler 3.0: Next-generation image processing for biology</article-title>. <source hwp:id="source-31">PLoS Biol</source> <volume>16</volume>, <issue>e2005970</issue>, doi:<pub-id pub-id-type="doi">10.1371/journal.pbio.2005970</pub-id> (<year>2018</year>).</citation></ref><ref id="c32" hwp:id="ref-32"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Moen E."><surname>Moen</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-33">Deep learning for cellular image analysis</article-title>. <source hwp:id="source-32">Nat Methods</source>, doi:<pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id> (<year>2019</year>).</citation></ref><ref id="c33" hwp:id="ref-33"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Chai X."><surname>Chai</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ba Q."><surname>Ba</surname>, <given-names>Q.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Yang G."><surname>Yang</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-34">Characterizing Robustness and Sensitivity of Convolutional Neural Networks in Segmentation of Fluorescence Microscopy Images</article-title>. <source hwp:id="source-33">2018 25th IEEE International Conference on Image Processing (ICIP)</source>, <fpage>3838</fpage>–<lpage>3842</lpage> (<year>2018</year>).</citation></ref><ref id="c34" hwp:id="ref-34"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Al-Kofahi Y."><surname>Al-Kofahi</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zaltsman A."><surname>Zaltsman</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Graves R."><surname>Graves</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marshall W."><surname>Marshall</surname>, <given-names>W.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rusu M"><surname>Rusu</surname>, <given-names>M</given-names></string-name>. <article-title hwp:id="article-title-35">A deep learning-based algorithm for 2-D cell segmentation in microscopy images</article-title>. <source hwp:id="source-34">BMC Bioinformatics</source> <volume>19</volume>, <issue>365</issue>, doi:<pub-id pub-id-type="doi">10.1186/s12859-018-2375-z</pub-id> (<year>2018</year>).</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2 xref-ref-35-3"><label>35.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Stringer C."><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang T."><surname>Wang</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Michaelos M."><surname>Michaelos</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Pachitariu M"><surname>Pachitariu</surname>, <given-names>M</given-names></string-name>. <article-title hwp:id="article-title-36">Cellpose: a generalist algorithm for cellular segmentation</article-title>. <source hwp:id="source-35">Nat Methods</source>, doi:<pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id> (<year>2020</year>).</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Van Valen D. A."><surname>Van Valen</surname>, <given-names>D. A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-37">Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments</article-title>. <source hwp:id="source-36">PLoS computational biology</source> <volume>12</volume>, <issue>e1005177</issue> (<year>2016</year>).</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>37.</label><citation publication-type="book" citation-type="book" ref:id="191858v3.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Chamier L. V."><surname>Chamier</surname>, <given-names>L. V.</given-names></string-name> <etal>et al.</etal> <chapter-title>ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy</chapter-title> (<publisher-name>Cold Spring Harbor Laboratory</publisher-name>, <year>2020</year>).</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>38.</label><citation publication-type="book" citation-type="book" ref:id="191858v3.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Gómez-De-Mariscal E."><surname>Gómez-De-Mariscal</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <chapter-title>DeepImageJ: A user-friendly plugin to run deep learning models in ImageJ</chapter-title> (<publisher-name>Cold Spring Harbor Laboratory</publisher-name>, <year>2019</year>).</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1 xref-ref-39-2 xref-ref-39-3"><label>39.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Deng J."><surname>Deng</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-38">in CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition</article-title>. (<source hwp:id="source-37">IEEE</source>).</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Iglovikov V."><surname>Iglovikov</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seferbekov S. S."><surname>Seferbekov</surname>, <given-names>S. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Buslaev A."><surname>Buslaev</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Shvets A"><surname>Shvets</surname>, <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-39">TernausNetV2: Fully Convolutional Network for Instance Segmentation</article-title>. <source hwp:id="source-38">CVPR Workshops</source>, <fpage>233</fpage>–<lpage>237</lpage> (<year>2018</year>).</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Yosinski J."><surname>Yosinski</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Clune J."><surname>Clune</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Lipson H"><surname>Lipson</surname>, <given-names>H</given-names></string-name>. <article-title hwp:id="article-title-40">How transferable are features in deep neural networks?</article-title> <source hwp:id="source-39">Advances in neural information processing systems</source>, <fpage>3320</fpage>–<lpage>3328</lpage> (<year>2014</year>).</citation></ref><ref id="c42" hwp:id="ref-42"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Razavian A. S."><surname>Razavian</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Azizpour H."><surname>Azizpour</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sullivan J."><surname>Sullivan</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Carlsson S"><surname>Carlsson</surname>, <given-names>S</given-names></string-name>. <article-title hwp:id="article-title-41">CNN features off-the-shelf: an astounding baseline for recognition</article-title>. <source hwp:id="source-40">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on</source>, <fpage>512</fpage>–<lpage>519</lpage> (<year>2014</year>).</citation></ref><ref id="c43" hwp:id="ref-43"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.43" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Donahue J."><surname>Donahue</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-42">Decaf: A deep convolutional activation feature for generic visual recognition</article-title>. <source hwp:id="source-41">International conference on machine learning</source>, <fpage>647</fpage>–<lpage>655</lpage> (<year>2014</year>).</citation></ref><ref id="c44" hwp:id="ref-44"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Oquab M."><surname>Oquab</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bottou L."><surname>Bottou</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Laptev I."><surname>Laptev</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Sivic J"><surname>Sivic</surname>, <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-43">Learning and transferring mid-level image representations using convolutional neural networks</article-title>. <source hwp:id="source-42">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</source>, <fpage>1717</fpage>–<lpage>1724</lpage> (<year>2014</year>).</citation></ref><ref id="c45" hwp:id="ref-45"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Zeiler M. D."><surname>Zeiler</surname>, <given-names>M. D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fergus R."><surname>Fergus</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-44">Visualizing and understanding convolutional networks</article-title>. <source hwp:id="source-43">European conference on computer vision</source>, <fpage>818</fpage>–<lpage>833</lpage> (<year>2014</year>).</citation></ref><ref id="c46" hwp:id="ref-46"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Choi J. Y."><surname>Choi</surname>, <given-names>J. Y.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-45">Multi-categorical deep learning neural network to classify retinal images: A pilot study employing small database</article-title>. <source hwp:id="source-44">PLoS One</source> <volume>12</volume>, <issue>e0187336</issue>, doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0187336</pub-id> (<year>2017</year>).</citation></ref><ref id="c47" hwp:id="ref-47"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Kim S. J."><surname>Kim</surname>, <given-names>S. J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-46">Deep transfer learning-based hologram classification for molecular diagnostics</article-title>. <source hwp:id="source-45">Sci Rep</source> <volume>8</volume>, <issue>17003</issue>, doi:<pub-id pub-id-type="doi">10.1038/s41598-018-35274-x</pub-id> (<year>2018</year>).</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.48" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Pratt L. Y"><surname>Pratt</surname>, <given-names>L. Y</given-names></string-name>. <article-title hwp:id="article-title-47">Discriminability-based transfer between neural networks</article-title>. <source hwp:id="source-46">Advances in neural information processing systems</source>, <fpage>204</fpage>–<lpage>211</lpage> (<year>1993</year>).</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>49.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Tan M."><surname>Tan</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Le Q. V"><surname>Le</surname>, <given-names>Q. V</given-names></string-name>. <article-title hwp:id="article-title-48">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</article-title>. <source hwp:id="source-47">ICML 2019</source>, doi:arxiv:1905.11946 (<year>2020</year>).</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Srivastavanitish N."><surname>Srivastavanitish</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hinton G."><surname>Hinton</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krizhevskykriz A."><surname>Krizhevskykriz</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutskeverilya I."><surname>Sutskeverilya</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Salakhutdinov R"><surname>Salakhutdinov</surname>, <given-names>R</given-names></string-name>. <article-title hwp:id="article-title-49">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</article-title>. <source hwp:id="source-48">Journal of Machine Learning Research</source> <volume>15</volume>, <fpage>1929</fpage>–<lpage>1958</lpage>, doi:<pub-id pub-id-type="doi">10.5555/2627435.2670313</pub-id> (<year>2014</year>).</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><label>51.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.51" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Bertram C. A."><surname>Bertram</surname>, <given-names>C. A.</given-names></string-name> <etal>et al.</etal> <fpage>204</fpage>–<lpage>213</lpage> (<publisher-name>Springer International Publishing</publisher-name>).</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Arbelaez P."><surname>Arbelaez</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maire M."><surname>Maire</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fowlkes C."><surname>Fowlkes</surname>, <given-names>C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Malik J"><surname>Malik</surname>, <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-50">Contour detection and hierarchical image segmentation</article-title>. <source hwp:id="source-49">IEEE transactions on pattern analysis and machine intelligence</source> <volume>33</volume>, <fpage>898</fpage>–<lpage>916</lpage> (<year>2010</year>).</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.53" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Baheti B."><surname>Baheti</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Innani S."><surname>Innani</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gajre S."><surname>Gajre</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Talbar S"><surname>Talbar</surname>, <given-names>S</given-names></string-name>. <article-title hwp:id="article-title-51">Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured Environment</article-title>. <source hwp:id="source-50">CVPR Workshops</source> (<year>2020</year>).</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.54" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Ghiasi G."><surname>Ghiasi</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin T.-Y."><surname>Lin</surname>, <given-names>T.-Y.</given-names></string-name> &amp; Quoc. <article-title hwp:id="article-title-52">DropBlock: A regularization method for convolutional networks</article-title>. <source hwp:id="source-51">arXiv pre-print server</source>, doi:arxiv:1810.12890 (<year>2018</year>).</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>55.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.55" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Guo C."><surname>Guo</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Szemenyei M."><surname>Szemenyei</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pei Y."><surname>Pei</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yi Y."><surname>Yi</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zhou W."><surname>Zhou</surname>, <given-names>W.</given-names></string-name> in 2019 <article-title hwp:id="article-title-53">IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE)</article-title> <fpage>439</fpage>–<lpage>444</lpage> (<year>2019</year>).</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>56.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.56" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Li X."><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen S."><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hu X."><surname>Hu</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Yang J."><surname>Yang</surname>, <given-names>J.</given-names></string-name> in 2019 <article-title hwp:id="article-title-54">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</article-title> <fpage>2677</fpage>–<lpage>2685</lpage> (<year>2019</year>).</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><label>57.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Vinogradova K."><surname>Vinogradova</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dibrov A."><surname>Dibrov</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Myers G"><surname>Myers</surname>, <given-names>G</given-names></string-name>. <article-title hwp:id="article-title-55">Towards Interpretable Semantic Segmentation via Gradient-Weighted Class Activation Mapping (Student Abstract)</article-title>. <source hwp:id="source-52">Proceedings of the AAAI Conference on Artificial Intelligence</source> <volume>34</volume>, <fpage>13943</fpage>–<lpage>13944</lpage>, doi:<pub-id pub-id-type="doi">10.1609/aaai.v34i10.7244</pub-id> (<year>2020</year>).</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>58.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.58" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Conze P.-H."><surname>Conze</surname>, <given-names>P.-H.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-56">Abdominal multi-organ segmentation with cascaded convolutional and adversarial deep networks</article-title>. <source hwp:id="source-53">arXiv pre-print server</source>, doi:arxiv:2001.09521 (<year>2020</year>).</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>59.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.59" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Jha D."><surname>Jha</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Michael Johansen"><surname>Michael</surname>, <given-names>Johansen</given-names></string-name>, <string-name name-style="western" hwp:sortable="Halvorsen D."><given-names>D.</given-names>, <surname>Halvorsen</surname></string-name>, <string-name name-style="western" hwp:sortable="&amp; Hvard P."><given-names>P.</given-names> <surname>&amp; Hvard</surname></string-name>. <article-title hwp:id="article-title-57">DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation</article-title>. <source hwp:id="source-54">arXiv pre-print server</source>, doi:arxiv:2006.04868 (<year>2020</year>).</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>60.</label><citation publication-type="book" citation-type="book" ref:id="191858v3.60" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Zeiler M. D."><surname>Zeiler</surname>, <given-names>M. D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fergus R."><surname>Fergus</surname>, <given-names>R.</given-names></string-name> <fpage>818</fpage>-<lpage>833</lpage> (<publisher-name>Springer International Publishing</publisher-name>, <year>2014)</year>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><label>61.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Chen C."><surname>Chen</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zuo Y."><surname>Zuo</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ye W."><surname>Ye</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li X."><surname>Li</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Ong S. P"><surname>Ong</surname>, <given-names>S. P</given-names></string-name>. <article-title hwp:id="article-title-58">Learning properties of ordered and disordered materials from multi-fidelity data</article-title>. <source hwp:id="source-55">Nature Computational Science</source> <volume>1</volume>, <fpage>46</fpage>–<lpage>53</lpage>, doi:<pub-id pub-id-type="doi">10.1038/s43588-020-00002-x</pub-id> (<year>2021</year>).</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><label>62.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.62" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Kingma D. P."><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Ba J."><surname>Ba</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-59">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</article-title>. <source hwp:id="source-56">ICLR</source> (<year>2015</year>).</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><label>63.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Glorot X."><surname>Glorot</surname>, <given-names>X.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bengio Y"><surname>Bengio</surname>, <given-names>Y</given-names></string-name>. <article-title hwp:id="article-title-60">Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</article-title>, <source hwp:id="source-57">JMLR Workshop and Conference Proceedings</source> <volume>9</volume>, <fpage>249</fpage>–<lpage>256</lpage> (<year>2010</year>).</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><label>64.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.64" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Abadi M. I."><surname>Abadi</surname>, <given-names>M. I.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-61">Tensorflow: A system for large-scale machine learning</article-title>. <fpage>265</fpage>--<lpage>283</lpage> (<year>2016</year>).</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><label>65.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Koul A."><surname>Koul</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Becchio C."><surname>Becchio</surname>, <given-names>C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Cavallo A"><surname>Cavallo</surname>, <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-62">Cross-Validation Approaches for Replicability in Psychology</article-title>. <source hwp:id="source-58">Frontiers in Psychology</source> <volume>9</volume>, doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2018.01117</pub-id> (<year>2018</year>).</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><label>66.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Geirhos R."><surname>Geirhos</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-63">Shortcut learning in deep neural networks</article-title>. <source hwp:id="source-59">Nature Machine Intelligence</source> <volume>2</volume>, <fpage>665</fpage>–<lpage>673</lpage>, doi:<pub-id pub-id-type="doi">10.1038/s42256-020-00257-z</pub-id> (<year>2020</year>).</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><label>67.</label><citation publication-type="journal" citation-type="journal" ref:id="191858v3.67" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Martin D. R."><surname>Martin</surname>, <given-names>D. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fowlkes C. C."><surname>Fowlkes</surname>, <given-names>C. C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Malik J"><surname>Malik</surname>, <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-64">Learning to detect natural image boundaries using local brightness, color, and texture cues</article-title>. <source hwp:id="source-60">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</source>, <fpage>530</fpage>–<lpage>549</lpage> (<year>2004</year>).</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><label>68.</label><citation publication-type="other" citation-type="journal" ref:id="191858v3.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Selvaraju R. R."><surname>Selvaraju</surname>, <given-names>R. R.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-65">Grad-CAM: Visual Explanations from Deep Networks via Gradient- Based Localization</article-title>. doi:<pub-id pub-id-type="doi">10.1109/iccv.2017.74</pub-id>.</citation></ref></ref-list><sec hwp:id="sec-25"><p hwp:id="p-79">
<fig id="figs1" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3 xref-fig-8-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figs1</object-id><label>Supplementary Figure 1.</label><caption hwp:id="caption-8"><title hwp:id="title-36">Effects of VGG variants and patch size on the models trained on phase contrast datasets</title><p hwp:id="p-80">(<bold>a-c</bold>) Average performance of U-Net and its variants with VGG encoders, pretraining, batch normalization or dropout layers. Model names suffixed by “NP” and U-Net are not pretrained, but other models are ImageNet pretrained. Suffix B denotes batch normalization and D denotes Dropout. (<bold>d-f</bold>) Average performance of ImageNet pretrained VGG19D-U-Net trained on different sizes of two- dimensional cropped image patches, ranging from 64x64 to 256x256 pixels. The tests of significance by Wilcoxon signed-rank test with p &gt;= 0.05 are indicated by ns, p &lt; 0.05 are indicated by * and p &lt; 0.0001 are indicated by **. Only the statistical tests of differences between adjacent bars are shown except for (<bold>b</bold>) to compare VGG19D-U-Net with the next best model, VGG19-U-Net. Error bars: 95% confidence intervals of the bootstrap mean.</p></caption><graphic xlink:href="191858v3_figs1" position="float" orientation="portrait" hwp:id="graphic-14"/></fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3 xref-fig-9-4 xref-fig-9-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figs2</object-id><label>Supplementary Figure 2.</label><caption hwp:id="caption-9"><title hwp:id="title-37">Comparisons of single-microscopy-type and multiple- microscopy-type training using U-Net and VGG19D-U-Net models.</title><p hwp:id="p-81">(<bold>a-c</bold>) Average F1 scores of models evaluated on phase contrast (<bold>a</bold>) , SDC (<bold>b</bold>) , and TIRF (<bold>c</bold>). (<bold>d-f</bold>) The distribution of F1 score in violin plot and box plot in black with a median indicated by the white circle for phase contrast (<bold>d</bold>) , SDC (<bold>e</bold>) , and TIRF (<bold>f</bold>) . The numbers of evaluated frames are n=202 for phase contrast, n=1000 for SDC, and n=132 for TIRF. The statistical significance by Wilcoxon signed-rank test with p &gt;= 0.05 are indicated by ns, p &lt;0.05 are indicated by *, and p &lt; 0.001 are indicated by **. The statistical significance by paired t-test with p &lt;0.05 is indicated by <sup>t</sup>*. Error bars: 95% confidence intervals of the bootstrap mean. (<bold>g-i</bold>) Visualization of segmentation results on three microscopies: phase contrast (<bold>g</bold>), SDC (<bold>h</bold>) , and TIRF (<bold>i</bold>) . These correspond to the close-up views shown in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-12" hwp:rel-id="F6">Figure 6</xref>. Edges extracted from ground truth masks and predictions are overlaid on the original image. Each edge is represented by one of three primary colors. Overlap of two or more edges is represented by the combination of those colors. (<bold>g</bold>) Bar: 32.5 μm. (<bold>h</bold>) Bar: 7.2 μm. (<bold>i</bold>) Bar: 6.5 μm,</p></caption><graphic xlink:href="191858v3_figs2" position="float" orientation="portrait" hwp:id="graphic-15"/></fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">figs3</object-id><label>Supplementary Figure 3.</label><caption hwp:id="caption-10"><title hwp:id="title-38">Comparisons of U-Net and VGG19D-U-Net models trained on single or multiple-microscopy-type dataset.</title><p hwp:id="p-82">(<bold>a-h</bold>) The distribution of precision and recall of U-Net and VGG19D-U-Net either trained on single-microscopy-type or multiple-microscopy- type datasets. The evaluated frames are sampled from all datasets (<bold>a-b</bold>) or phase contrast (<bold>c-d)</bold>, SDC (<bold>e-f</bold>) , or TIRF (<bold>g-h</bold>) dataset only. Within a violin plot, there is a box plot in black with a median indicated by the white circle. (n=335 for all datasets, n=202 for phase contrast, n=1000 for SDC, and n=132 for TIRF)</p></caption><graphic xlink:href="191858v3_figs3" position="float" orientation="portrait" hwp:id="graphic-16"/></fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure" hwp:id="F11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figs4</object-id><label>Supplementary Figure 4.</label><caption hwp:id="caption-11"><title hwp:id="title-39">Visualization of single-microscopy-type U-Net and MARS-Net.</title><p hwp:id="p-83">(<bold>a-f</bold>) Progression of the cell edges overlaid on the first frame of the movie using segmentation results from U-Net<sup>S</sup> (<bold>a,c,e</bold>) and MARS-Net (<bold>b,d,f</bold>). (<bold>a-b</bold>) Phase contrast live cell movie, Bars: 32.5 μm. (<bold>c-d</bold>) SDC live cell movie, Bars: 7.2 μm. (<bold>e-f</bold>) TIRF live cell movie, Bars: 6.5 μm. The approximate window numbers corresponding to window numbers in protrusion maps in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-11" hwp:rel-id="F7">Fig. 7</xref> are labeled in white color. (blue, 0s; red, 1000s time points).</p></caption><graphic xlink:href="191858v3_figs4" position="float" orientation="portrait" hwp:id="graphic-17"/></fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure" hwp:id="F12" hwp:rev-id="xref-fig-12-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figs5</object-id><label>Supplementary Figure 5.</label><caption hwp:id="caption-12"><title hwp:id="title-40">Class activation map of convolutional layers in the single-microscopy-type and multiple-microscopy-type VGG19D-U-Net with respect to the ground truth edges for the phase contrast dataset.</title><p hwp:id="p-84">From each live cell movie, one frame was randomly chosen for visualization. On the left, the original image of the corresponding frame is shown. The word ’S’ represents the single- microscopy-type dataset, and ’M’ represents the multiple-microscopy-type dataset. The class activation maps from the first to the fifth block in the encoder are organized in order from left to right. In the heatmap, the value 0 means no activation, and 1 means the highest activation. The green lines represent the ground truth edges.</p></caption><graphic xlink:href="191858v3_figs5" position="float" orientation="portrait" hwp:id="graphic-18"/></fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure" hwp:id="F13"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figs6</object-id><label>Supplementary Figure 6.</label><caption hwp:id="caption-13"><title hwp:id="title-41">Class activation map of convolutional layers in the single-microscopy-type and multiple-microscopy-type VGG19D-U-Net with respect to the ground truth edges for the SDC dataset.</title><p hwp:id="p-85">From each live cell movie, one frame was randomly chosen for visualization. On the left, the original image of the corresponding frame is shown. The word ’S’ represents the single-microscopy-type dataset, and ’M’ represents the multiple-microscopy-type dataset. The class activation maps from the first to the fifth block in the encoder are organized in order from left to right. In the heatmap, the value 0 means no activation, and 1 means the highest activation. The green lines represent the ground truth edges.</p></caption><graphic xlink:href="191858v3_figs6" position="float" orientation="portrait" hwp:id="graphic-19"/></fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure" hwp:id="F14" hwp:rev-id="xref-fig-14-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figs7</object-id><label>Supplementary Figure 7.</label><caption hwp:id="caption-14"><title hwp:id="title-42">Class activation map of convolutional layers in the single-microscopy-type and multiple-microscopy-type VGG19D-U-Net with respect to the ground truth edge for the TIRF dataset.</title><p hwp:id="p-86">From each live cell movie, one frame was randomly chosen for visualization. On the left, the original image of the corresponding frame is shown. The word ’S’ represents the single-microscopy-type dataset, and ’M’ represents the multiple-microscopy-type dataset. The class activation maps from the first to the fifth block in the encoder are organized in order from left to right. In the heatmap, the value 0 means no activation, and 1 means the highest activation. The green lines represent the ground truth edges.</p></caption><graphic xlink:href="191858v3_figs7" position="float" orientation="portrait" hwp:id="graphic-20"/></fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure" hwp:id="F15" hwp:rev-id="xref-fig-15-1 xref-fig-15-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;191858v3/FIGS8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figs8</object-id><label>Supplementary Figure 8.</label><caption hwp:id="caption-15"><title hwp:id="title-43">Threshold error/noise and protrusion in the protrusion maps of Phase Contrast, SDC, and TIRF live cell movies segmented by single-microscopy-type U-Net and MARS-Net.</title><p hwp:id="p-87">These are obtained from filtering and thresholding protrusion maps in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-12" hwp:rel-id="F7">Fig. 7</xref>. In each row, the same cropped region in the movie is profiled for cellular morphodynamics.</p></caption><graphic xlink:href="191858v3_figs8" position="float" orientation="portrait" hwp:id="graphic-21"/></fig>
</p></sec></back></article>
