<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/261271</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;261271</article-id><article-id pub-id-type="other" hwp:sub-type="slug">261271</article-id><article-id pub-id-type="other" hwp:sub-type="tag">261271</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Neural signatures of the processing of temporal patterns in sound</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Correspondence concerning this article should be addressed to Björn Herrmann, The Brain &amp; Mind Institute, The University of Western Ontario, London, Ontario, N6A 5B7, Canada. E-mail: <email hwp:id="email-1">herrmann.b@gmail.com</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6362-3043</contrib-id><name name-style="western" hwp:sortable="Herrmann Björn"><surname>Herrmann</surname><given-names>Björn</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-6362-3043"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Johnsrude Ingrid S."><surname>Johnsrude</surname><given-names>Ingrid S.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">The Brain and Mind Institute, The University of Western Ontario</institution>, N6A 3K7, London, ON, <country>Canada</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">School of Communication Sciences &amp; Disorders, The University of Western Ontario</institution>, N6A 5B7, London, ON, <country>Canada</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-02-07T05:00:31-08:00">
    <day>7</day><month>2</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-05-07T10:45:11-07:00">
    <day>7</day><month>5</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-02-07T05:06:21-08:00">
    <day>7</day><month>2</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-05-07T10:51:08-07:00">
    <day>7</day><month>5</month><year>2018</year>
  </pub-date><elocation-id>261271</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-02-06"><day>06</day><month>2</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-05-06"><day>06</day><month>5</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-05-07"><day>07</day><month>5</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license hwp:id="license-1"><p hwp:id="p-1">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="261271.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/261271v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="261271.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/261271v2/261271v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/261271v2/261271v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">The ability to detect regularities in sound (i.e., recurring structure) is critical for effective perception, enabling, for example, change detection and prediction. Two seemingly unconnected lines of research concern the neural operations involved in processing regularities: one investigates how neural activity synchronizes with temporal regularities (e.g., frequency modulation; FM) in sounds, whereas the other focuses on increases in sustained activity during stimulation with repeating tone-frequency patterns. In three electroencephalography studies with male and female human participants, we investigated whether neural synchronization and sustained neural activity are dissociable, or whether they are functionally interdependent. Experiment I demonstrated that neural activity synchronizes with temporal regularity (FM) in sounds, and that sustained activity increases concomitantly. In Experiment II, phase coherence of FM in sounds was parametrically varied. Although neural synchronization was more sensitive to changes in FM coherence, such changes led to a systematic modulation of both neural synchronization and sustained activity, with magnitude increasing as coherence increased. In Experiment III, participants either performed a duration categorization task on the sounds, or a visual object tracking task to distract attention. Neural synchronization was observed irrespective of task, whereas the sustained response was observed only when attention was on the auditory task, not under (visual) distraction. The results suggest that neural synchronization and sustained activity levels are functionally linked: both are sensitive to regularities in sounds. However, neural synchronization might reflect a more sensory-driven response to regularity, compared with sustained activity which may be influenced by attentional, contextual, or other experiential factors.</p><sec id="s1" hwp:id="sec-1"><title hwp:id="title-2">Significance statement</title><p hwp:id="p-3">Optimal perception requires that the auditory system detects regularities in sounds. Synchronized neural activity and increases in sustained neural activity both appear to index the detection of a regularity, but the functional interrelation of these two neural signatures is unknown. In three electroencephalography experiments, we measured both signatures concomitantly while listeners were presented with sounds containing frequency modulations that differed in their regularity. We observed that both neural signatures are sensitive to temporal regularity in sounds, although they functionally decouple when a listener is distracted by a demanding visual task. Our data suggest that neural synchronization reflects a more automatic response to regularity, compared with sustained activity which may be influenced by attentional, contextual, or other experiential factors.</p></sec></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-3">Keywords</title><kwd hwp:id="kwd-1">Neural synchronization</kwd><kwd hwp:id="kwd-2">temporal regularity</kwd><kwd hwp:id="kwd-3">stimulus statistics</kwd><kwd hwp:id="kwd-4">entrainment</kwd><kwd hwp:id="kwd-5">sustained activity</kwd><kwd hwp:id="kwd-6">electroencephalography</kwd></kwd-group><counts><page-count count="37"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Introduction</title><p hwp:id="p-4">Natural sound environments are rich in statistical regularities and perceptual systems are sensitive to these regularities (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Garrido et al., 2013</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">McDermott et al., 2013</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Herrmann and Johnsrude, 2018</xref>). A regularity may be defined as reoccurring structure in sounds, and may include, among others, repeated patterns of brief tones or of a sound’s frequency modulation. Detection of regularities is crucial for efficient auditory scene analysis (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Bendixen, 2014</xref>). It enables sensitivity to acoustic changes (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">Schröger, 2007</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">Winkler et al., 2009</xref>), allows generation of expectations about future sounds (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Barnes and Jones, 2000</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Jones et al., 2002</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Herrmann et al., 2016</xref>), and supports perception of speech (<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Idemaru and Holt, 2011</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Peelle and Davis, 2013</xref>; <xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Baese-Berk et al., 2014</xref>). In this paper, we investigate how regularity is represented in patterns of neural (electroencephalographic) activity.</p><p hwp:id="p-5">Prior investigations of the neural mechanisms supporting the processing of regularities in sounds have largely been conducted along two seemingly unrelated lines (but see (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Baldeweg, 2006</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-2" hwp:rel-id="ref-77">Winkler et al., 2009</xref>) for other measures). One line of research focuses on the propensity of neural oscillatory activity to synchronize with temporal regularities in sound environments (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Nozaradan et al., 2011</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Henry and Obleser, 2012</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Lakatos et al., 2013b</xref>; <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Costa-Faidella et al., 2017</xref>; <xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">ten Oever et al., 2017</xref>). Temporally regular sound stimulation drives neural activity into the same rhythm as the stimulus rhythm. Synchronization of neural activity is thought to provide a listener with a reference frame that can be used to predict future sounds (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Henry and Herrmann, 2014</xref>; <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Nobre and van Ede, 2018</xref>), which may, in turn, support perception of speech and music (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">Peelle and Davis, 2013</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Doelling and Poeppel, 2015</xref>).</p><p hwp:id="p-6">Another, more recent, line of research has revealed that the detection of repetition in a sequence of brief tones is marked by an increase in a sustained, low-frequency DC power offset in magneto-/electroencephalographic activity (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Southwell et al., 2017</xref>). The sustained response increase appears independent of a specific type of structure: It has been observed for repeating sequences of tones with random frequencies (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-2" hwp:rel-id="ref-66">Southwell et al., 2017</xref>), complex sounds made of isochronous tone sequences (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-2" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>), and coherent frequency changes in sounds made of broadband chords (<xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">Teki et al., 2016</xref>). Sustained response increases have been proposed to reflect a prediction-related increase in sensitivity when uncertainty is reduced (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-3" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Heilbron and Chait</xref>, in press).</p><p hwp:id="p-7">The relation between neural synchronization and the sustained response for the detection of regularities is unknown. Neural synchronization to low frequency (~2–6 Hz) temporal regularities is strongest in auditory cortex (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Herrmann et al., 2013</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Keitel et al., 2017</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Millman et al., 2017</xref>). Sustained response increases appear to be generated in a wide network of brain areas, including auditory cortex, hippocampus, parietal cortex, and inferior frontal gyrus (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-4" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-2" hwp:rel-id="ref-71">Teki et al., 2016</xref>). Neural synchronization in sensory areas may provide the means to recognize the temporal regularity, which then is further processed in higher-order brain regions. However, it is unclear how neural synchronization and sustained response interact functionally.</p><p hwp:id="p-8">In the current study, we conduct three electroencephalography (EEG) experiments to investigate the relation between neural synchronization and sustained neural activity. In Experiment I we test whether neural synchronization and sustained activity are simultaneously elicited by temporal regularity in sounds using a stimulation protocol similar to previous work (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-5" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-3" hwp:rel-id="ref-66">Southwell et al., 2017</xref>). In Experiment II, we parametrically manipulate the degree of temporal regularity in sounds – and thereby the degree of neural synchronization – in order to investigate whether sustained activity is differently sensitive to temporal regularity compared to neural synchronization. Finally, using a selective attention paradigm, Experiment III tests the extent to which neural populations supporting neural synchronization versus sustained neural activity are modulated by attention, and thus the extent to which they may be potentially dissociable.</p></sec><sec id="s3" hwp:id="sec-3"><title hwp:id="title-5">General methods</title><sec id="s3a" hwp:id="sec-4"><title hwp:id="title-6">Participants</title><p hwp:id="p-9">Participants were male and female students at the University of Western Ontario aged 17–29 years and with no reported neurological disease or hearing impairment. All participants had normal or corrected-to-normal vision and were naïve to the purposes of the experiment. Different participants were recruited for each of the three experiments. Participants gave written informed consent prior to the experiment, and either received course credit or were paid $5 CAD per half-hour for their participation. The experiments were conducted in accordance with the Declaration of Helsinki, the Canadian Tri-Council Policy Statement on Ethical Conduct for Research Involving Humans (TCPS2-2014), and were approved by the local Non-Medical Research Ethics Board of the University of Western Ontario (protocol ID: 106570).</p></sec><sec id="s3b" hwp:id="sec-5"><title hwp:id="title-7">Procedure</title><p hwp:id="p-10">All experimental procedures were carried out in a single-walled sound-attenuating booth (Eckel Industries). Sounds were presented via Sennheiser (HD 25-SP II) headphones and a Steinberg UR22 (Steinberg Media Technologies) external sound card. Stimulation was controlled by a PC (Windows 7, 64 bit) running Psychtoolbox in Matlab (R2015b).</p><p hwp:id="p-11">Prior to the experiment, the hearing threshold for a 1600-Hz pure tone (Experiment I) or a narrowband noise centered on 1414 Hz (Experiments II and III) was determined for each individual using a method-of-limits procedure. The thresholds were used to present sounds during the EEG experiments at 55 dB above the individual sensation level. Thresholds were obtained by presenting sounds of 12-s duration that changed (either decreased or increased) continuously in intensity at 5.4 dB/s. Participants indicated via button press when they could no longer hear the tone (intensity decrease) or when they started to hear the tone (intensity increase). The mean sound intensity at the time of the button press was noted for 6 decreasing sounds and 6 increasing sounds (decreasing and increasing sounds alternated), and these were averaged to determine the individual hearing threshold.</p></sec><sec id="s3c" hwp:id="sec-6"><title hwp:id="title-8">EEG recordings and preprocessing</title><p hwp:id="p-12">EEG signals were recorded at 1024-Hz sampling rate from 16 (Ag/Ag–Cl-electrodes) electrodes and additionally from left and right mastoids (BioSemi, Amsterdam, The Netherlands; 208 Hz low-pass filter) while participants listened to sounds that either did or did not contain a regularity (for details on individual conditions see the experimental procedures below). Electrodes were referenced to a monopolar reference feedback loop connecting a driven passive sensor and a common-mode-sense (CMS) active sensor, both located posterior on the scalp.</p><p hwp:id="p-13">Offline data analysis was carried out using MATLAB software (v7.14; MathWorks, Inc.). Line noise (60 Hz) was removed from the raw data using an elliptic filter. Data were re-referenced to the average mastoids, high-pass filtered at a cutoff of 0.7 Hz (2449 points, Hann window), and low-pass filtered at a cutoff of 22 Hz (211 points, Kaiser window). Data were down-sampled to 512 Hz and divided into epochs ranging from −1 to 5.8 s (Experiments I &amp; II) or from −0.5 to 4 s (Experiment III). Epochs were shorter in Experiment III compared to Experiments I &amp; II to accommodate differences in sound duration and trial structure (see below). Independent components analysis (runica method, (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Makeig et al., 1996</xref>); logistic infomax algorithm, (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Bell and Sejnowski, 1995</xref>); Fieldtrip implementation, (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Oostenveld et al., 2011</xref>), RRID:SCR_004849) was used to identify and remove activity related to blinks and horizontal eye movements. Epochs that exceeded a signal change of 200 μV in any electrode were excluded from analyses. This pipeline was used to investigate neural synchronization (see below).</p><p hwp:id="p-14">In order to investigate the sustained neural activity, the same pipeline was computed a second time, with the exception that high-pass filtering was omitted. Omission of the high-pass filter is necessary to investigate the sustained response, because the response is a very low-frequency signal reflecting a DC shift (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-6" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-4" hwp:rel-id="ref-66">Southwell et al., 2017</xref>). Activity related to blinks and horizontal eye movements was removed using the identified components from the high-pass filtered data.</p></sec><sec id="s3d" hwp:id="sec-7"><title hwp:id="title-9">EEG data analysis: Sustained response</title><p hwp:id="p-15">Single-trial time courses were averaged for each condition (details on the conditions in each experiment are presented below). Note that previous studies that investigated the sustained response used a root-mean-square calculation instead of calculating the average across trials (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-7" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-3" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-3" hwp:rel-id="ref-71">Teki et al., 2016</xref>). The root-mean-square calculation results in only positive values, whereas the calculation of the average preserves information about polarity. Previous work utilized the root-mean square calculation because magnetoencephalography was recorded and absolute polarity in these recording cannot be interpreted. However, the polarity of the EEG signal may provide information about neural mechanisms as a negative-going wave may indicate an excitable state (<xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">Whittingstall and Logothetis, 2009</xref>). We therefore did not calculate the root-mean-square for the current EEG recordings but instead used the average across trials.</p><p hwp:id="p-16">Separately for each electrode, response time courses were baseline corrected by subtracting the mean amplitude in the pre-stimulus time window (Experiments I &amp; II: −1 to 0 seconds; Experiment III: −0.5 to 0 seconds; differences in the length are related to experiment-specific trial structure; see below) from the amplitude at each time point. In order to investigate the sustained response, signals were averaged across a fronto-central-parietal electrode cluster (Fz, F3, F4, Cz, C3, C4, Pz, P3, P4) and, for each condition, the mean amplitude was calculated within the time window during which a regularity could occur in sounds: 2–4.8 s (Experiment I), 2.2–4.8 s (Experiment II), 1.6–4 s (Experiment III). Selection of the fronto-central-parietal electrode cluster was motivated by the spatially wide-spread sustained response across electrodes (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figures 1</xref>, <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">4</xref>, and <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">7</xref>) that is consistent with previous localizations of the sustained response in auditory cortex and higher-level brain regions (i.e., parietal cortex, hippocampus, inferior frontal gyrus) (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-8" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-4" hwp:rel-id="ref-71">Teki et al., 2016</xref>).</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><title hwp:id="title-10">Stimulus conditions and neural responses for Experiment I.</title><p hwp:id="p-17"><bold>A</bold>: Frequency of brief tones (dots) making up a 4.8-s sound. An example for each stimulus condition is shown. <bold>B</bold>: Response time courses and scalp topographies for each condition. The dotted vertical lines mark the time window of interest used for analysis (2–4.8 s). <bold>C</bold>: Mean response magnitude for the 2–4.8 s time window. Error bars reflect the standard error of the mean (removal of between-subject variance; (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Masson and Loftus, 2003</xref>). *p<sub>FDR</sub> ≤ 0.05. Other stimulus effects (i.e., among the three REG sequences) were not significant.</p></caption><graphic xlink:href="261271_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><title hwp:id="title-11">Results of the inter-trial phase coherence analysis for Experiment I.</title><p hwp:id="p-18">ITPC frequency spectrum (left) and ITPC values at the stimulus frequencies (right). Error bars reflect the standard error of the mean (removal of between-subject variance; (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">Masson and Loftus, 2003</xref>). *p &lt; 0.05. Other stimulus effects were not significant.</p></caption><graphic xlink:href="261271_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s3e" hwp:id="sec-8"><title hwp:id="title-12">EEG data analysis: Neural synchronization</title><p hwp:id="p-19">Neural synchronization was investigated using inter-trial phase coherence (ITPC) as a dependent measure (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Lachaux et al., 1999</xref>). To this end, a fast Fourier transform (including a Hann window taper and zero-padding) was calculated for each trial and channel for the time window during which a regularity could occur in sounds: 2–4.8 s (Experiment I), 2.2–4.8 s (Experiment II), 1.6–4 s (Experiment III). The resulting complex numbers were normalized by dividing each complex number by its magnitude. ITPC was then calculated as the absolute value of the mean normalized complex number across trials. ITPC values can take on values between 0 (no coherence) and 1 (maximum coherence). ITPC was calculated for frequencies ranging from 1–12 Hz. For statistical analyses, ITPC was averaged across the fronto-central-parietal electrode cluster (Fz, F3, F4, Cz, C3, C4, Pz, P3, P4).</p></sec><sec id="s3f" hwp:id="sec-9"><title hwp:id="title-13">Experimental design and statistical analysis</title><sec id="s3f1" hwp:id="sec-10"><title hwp:id="title-14">These experiments were not preregistered</title><p hwp:id="p-20">Details of the critical variables and statistical tests for each experiment can be found below. In general, experimental manipulations were within-subject factors. Differences between experimental conditions were thus assessed using paired-samples t-tests, one-sample t-tests, or via repeated-measures analysis of variance (rmANOVA). Whenever the assumption of sphericity was violated, Greenhouse–Geisser correction was applied (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Greenhouse and Geisser, 1959</xref>); we report the original degrees of freedom, the corrected p value, and the epsilon coefficient. False discovery rate (FDR) correction was applied to correct for multiple comparisons among post-hoc tests following an rmANOVA (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Genovese et al., 2002</xref>). Correlations between conditions are reported as Spearman correlations and repeated-measures correlations (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Bakdash and Marusich, 2017</xref>); using the ‘rmcorr’ implementation in Rstudio software; Version 1.1.419; <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.rstudio.com/" ext-link-type="uri" xlink:href="https://www.rstudio.com/" hwp:id="ext-link-1">https://www.rstudio.com/</ext-link>, RRID:SCR_000432). The repeated-measures correlation determines the common within-individual association for paired measures (two measures in Experiment I; four measures in Experiment II) for multiple individuals.</p><p hwp:id="p-21">Throughout the manuscript, effect sizes are provided as partial eta-squared (η<sub>p</sub><sup>2</sup>) when an analysis of variance (ANOVA) is reported and as r<sub>e</sub> (r<sub>equivalent</sub>) when a t-test is reported (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Rosenthal and Rubin, 2003</xref>). r<sub>e</sub> is equivalent to a Pearson product-moment correlation for two continuous variables, to a point-biserial correlation for one continuous and one dichotomous variable, and to the square root of partial η<sup>2</sup> for ANOVAs.</p></sec></sec></sec><sec id="s4" hwp:id="sec-11"><title hwp:id="title-15">Experiment I: Regularity detection – sustained activity and neural synchronization</title><p hwp:id="p-22">Experiment I aimed to replicate previous observations of a sustained response elicited by a repeating sequence of brief tones (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-9" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-5" hwp:rel-id="ref-66">Southwell et al., 2017</xref>), and, in addition, aimed to investigate whether neural synchronization co-occurs with changes in sustained activity levels.</p><sec id="s4a" hwp:id="sec-12"><title hwp:id="title-16">Participants</title><p hwp:id="p-23">Sixteen participants took part in Experiment I (mean age [standard deviation]: 18.6 ±2 years; 8 female).</p></sec><sec id="s4b" hwp:id="sec-13"><title hwp:id="title-17">Acoustic stimulation &amp; procedure</title><p hwp:id="p-24">Stimuli were similar to those used in previous studies (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-10" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-6" hwp:rel-id="ref-66">Southwell et al., 2017</xref>). Stimuli were 4.8-s long sequences of 0.04-s tones (0.007 s rise time; 0.007 s fall time; 120 tones per sound; no gap between tones). Each stimulus sequence comprised 12 sets of tones and each set was 10 tones (0.4 seconds) long. The frequency of a tone could take on one of 70 values ranging from 700 to 2500 Hz (logarithmically spaced).</p><p hwp:id="p-25">The selection of frequency values for each of the 12 sets depended on the stimulus condition. Four conditions were utilized: one condition contained no regularity (RAND), whereas the other three conditions (REG, REG2.5, and REG5) contained a regular pattern that started 2 seconds after sound onset. For the RAND condition (containing no regularity), 10 new frequency values were randomly selected for each of the 12 sets (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1A</xref>, top). In the REG condition (containing a regularity), 10 new frequency values were randomly selected for each of the first 5 sets (0-2 seconds; similar to RAND), and then 10 new random frequency values were selected and repeated for set 6 to 12, thereby creating a regularity (the serial order of tone frequencies was identical in these repeating sets; <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1A</xref>, second row). Selection of the 10 random frequency values (and their serial order), used to create the regularity, differed from trial to trial. This condition is similar to the sounds used in previous studies that investigated sustained neural activity (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-11" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-7" hwp:rel-id="ref-66">Southwell et al., 2017</xref>).</p><p hwp:id="p-26">In order to investigate neural synchronization and sustained neural activity concomitantly, two additional conditions were utilized. Although the tone sequence repeats in the REG condition, the frequencies of the brief tones did not relate to each other in any regular way and the serial order of tone frequencies differed from trial to trial. Hence, the REG sequences may not drive observable oscillatory neural patterns, because observation of neural synchronization (e.g., using inter-trial phase coherence; ITPC) requires that neural activity is driven with the same oscillatory stimulus pattern on each trial (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Henry and Obleser, 2012</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Henry et al., 2014</xref>). Accordingly, we created regularities in which the tone sequences not only repeated (as in REG) but the change in frequency from tone to tone created a sinusoidal oscillatory pattern, to which synchronization of neural activity could be analyzed. We utilized two different oscillatory rates in order to assess the generalization of our stimuli. In the REG2.5 condition, the 10 frequency values (repeated for sets 6 to 12) were selected such that their arrangement reflected a 2.5-Hz sinusoidal frequency modulation (FM) (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1A</xref>, third row). In the REG5 condition, the 10 frequency values (repeated for sets 6 to 12) were selected such that their arrangement reflected a 5-Hz sinusoidal FM (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1A</xref>, bottom). For conditions REG2.5 and REG5, the starting phase was similar across trials, so that, when phase coherence is calculated, we would not be canceling signals with different phases. The precise frequency values chosen differed from trial to trial to avoid sounds being too similar (holding FM frequency and phase constant).</p><p hwp:id="p-27">Throughout Experiment I, participants listened passively to 138 trials of each condition while watching a movie of their choice with subtitles and the sound muted on a battery-driven, portable DVD player. The experiment was divided into 6 blocks. During each block, 23 trials of each condition were randomly presented. Trials were separated by a 2-s inter-stimulus interval.</p></sec><sec id="s4c" hwp:id="sec-14"><title hwp:id="title-18">EEG analysis</title><p hwp:id="p-28">In order to investigate the sustained response, for each condition, the mean amplitude within the 2–4.8 second time window post stimulus onset and across the fronto-central-parietal electrode cluster was calculated. A one-way rmANOVA with the factor Condition (RAND, REG, REG2.5, REG5) was carried out using amplitude as a dependent measure.</p><p hwp:id="p-29">Neural synchronization was investigated using ITPC calculated for neural activity within the 2–4.8 second time window. ITPC was averaged across 0.2-Hz wide frequency windows centered on the frequency in the stimulus (2.5 Hz; 5 Hz), and averaged across the fronto-central-parietal electrode cluster. Separately for ITPC values at 2.5 Hz and ITPC values at 5 Hz, a one-way rmANOVA with the factor Condition (RAND, REG, REG2.5, REG5) was carried out.</p></sec><sec id="s4d" hwp:id="sec-15"><title hwp:id="title-19">Results</title><p hwp:id="p-30"><xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Figure 1B</xref> shows the response time courses and topographical distributions for each condition. The rmANOVA revealed a main effect of Condition (F<sub>3,45</sub> = 5.52, p = 0.003, η<sub>p</sub><sup>2</sup> = 0.269). Post hoc tests indicated that responses were more negative in the 2–4.8 second time window for REG (t<sub>15</sub> = −4.53, p<sub>FDR</sub> = 0.01, r<sub>e</sub> = 0.760), REG2.5 (t<sub>15</sub> = −3.39, p<sub>FDR</sub> &lt; 0.05, r<sub>e</sub> = 0.659), and REG5 (t<sub>15</sub> = −2.93, p<sub>FDR</sub> = 0.05, r<sub>e</sub> = 0.603) compared to the RAND condition (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Figure 1C</xref>). There were no statistically significant differences among the REG, REG2.5, and REG5 conditions (for all, p<sub>FDR</sub> &gt; 0.05). Scalp topographies indicate a wide distribution covering parietal electrodes as well as fronto-central electrodes.</p><p hwp:id="p-31">Neural synchronization was statistically assessed for the ITPC at 2.5 Hz and at 5 Hz. The rmANOVA for ITPC values at 2.5 Hz revealed a main effect of Condition (F<sub>3,45</sub> = 22.61, p = 9<sup>e−6</sup>, ε = 0.535, η<sub>p</sub><sup>2</sup> = 0.601). Post hoc tests showed significantly larger ITPC for REG2.5 (i.e., the regularity with a 2.5-Hz rhythm) compared to the RAND (t<sub>15</sub> = 4.73, p<sub>FDR</sub> = 0.001, r<sub>e</sub> = 0.774), REG (t<sub>15</sub> = 5.60, p<sub>FDR</sub> &lt; 0.001, r<sub>e</sub> = 0.823), and REG5 conditions (t<sub>15</sub> = 5.45, p<sub>FDR</sub> &lt; 0.001, re = 0.815).</p><p hwp:id="p-32">The rmANOVA for ITPC values at 5 Hz revealed a main effect of Condition (F<sub>3,45</sub> = 21.53, p = 4<sup>e−6</sup>, ε = 0.604, η<sub>p</sub><sup>2</sup> = 0.589). Post hoc tests showed significantly larger ITPC for REG5 compared to RAND (t<sub>15</sub> = 4.06, p<sub>FDR</sub> = 0.01, r<sub>e</sub> = 0.724) and REG (t<sub>15</sub> = 4.13, p<sub>FDR</sub> &lt; 0.01, r<sub>e</sub> = 0.729), but also significantly larger ITPC for REG2.5 compared to RAND (t<sub>15</sub> = 6.74, p<sub>FDR</sub> = 0.001, r<sub>e</sub> = 0.867) and REG (t<sub>15</sub> = 7.25, p<sub>FDR</sub> &lt; 0.001, r<sub>e</sub> = 0.882). The 5-Hz effect for the REG2.5 condition reflects an ITPC increase at the first harmonic frequency of 2.5 Hz. A large first harmonic in the EEG spectrum is a common observation for frequency-modulated sounds at low modulation rates (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Picton et al., 2003</xref>). Scalp topographies are consistent with neural sources in auditory cortex (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Näätänen and Picton, 1987</xref>; <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">Picton et al., 2003</xref>).</p><p hwp:id="p-33">Spearman correlations we calculated in order to investigate the relation between the sustained response effect for the REG condition (REG-minus-RAND) and the sustained response effect for the REG2.5 (REG2.5-minus-RAND) and the REG5 conditions (REG5-minus-RAND) (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>). We observed a positive correlation between the REG regularity effect and the REG2.5 regularity effect (r = 0.524, p = 0.040), whereas the correlation between the REG regularity effect and the REG5 regularity effect was not significant (r = 0.238, p = 0.329), but showed a similar trend.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-34">Spearman correlation between the sustained response effect for the REG condition versus the sustained response effect for the REG2.5 and REG5 conditions.</p></caption><graphic xlink:href="261271_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-35">Repeated-measures correlation analyses were calculated (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Bakdash and Marusich, 2017</xref>), in order to investigate the relation between the effect of regularity on sustained activity and ITPC for the REG2.5 and the REG5 conditions (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>). To this end, we entered the two paired measures (RAND and REG2.5; RAND and REG5) of ITPC and sustained activity into the analysis. We observed negative correlations, indicating that the effect of regularity on sustained response increased (i.e., became more negative) when the effect of ITPC increased, which was significant for the REG2.5 condition (r = −0.603, p = 0.010), but not for the REG5 condition (r = −0.336, p = 0.187).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><title hwp:id="title-20">Repeated-measures correlations.</title><p hwp:id="p-36">Correlation between sustained response and inter-trial phase coherence. Note that the plots display negative correlations, because an increase in sustained activity reflects a negative-going signal, whereas an increase in ITPC is positive-going. Observations from the same participant are displayed in the same color, with the corresponding lines showing the fit of the repeated-measures correlation for each participant.</p></caption><graphic xlink:href="261271_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s4e" hwp:id="sec-16"><title hwp:id="title-21">Summary</title><p hwp:id="p-37">The results of Experiment I replicate previous findings of a sustained response (here more negative) when a regularity occurs in a sound compared to when a sound contains no regularity (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-12" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-4" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-8" hwp:rel-id="ref-66">Southwell et al., 2017</xref>). In addition, Experiment I extends these previous reports by showing that, when the stimuli are constituted to allow observation of synchronization (REG2.5 &amp; REG5), both the sustained response and neural synchronization are observed. The magnitudes of these signals correlate within participants, suggesting that they are not independent. In Experiment II we introduce a within-subject, parametric manipulation of regularity in order to observe whether both ITPC and the sustained response scale with the degree of regularity.</p></sec></sec><sec id="s5" hwp:id="sec-17"><title hwp:id="title-22">Experiment II: Parametric modulation of neural synchronization and sustained activity</title><p hwp:id="p-38">In order to parametrically modulate the strength of neural synchronization, we made use of narrowband sounds with varying degrees of frequency modulation (FM). Previous work shows that neural activity synchronizes with FMs in sounds (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">John et al., 2001</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">Herrmann et al., 2013</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Henry et al., 2014</xref>) and manipulation of FM systematically modulates neural synchronization (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Boettcher et al., 2002</xref>).</p><sec id="s5a" hwp:id="sec-18"><title hwp:id="title-23">Participants</title><p hwp:id="p-39">Eighteen participants who did not participate in Experiment I participated in Experiment II (mean age [standard deviation]: 18.4 ±0.9 years; 10 female).</p></sec><sec id="s5b" hwp:id="sec-19"><title hwp:id="title-24">Acoustic stimulation &amp; procedure</title><p hwp:id="p-40">Stimuli were narrow-band noises made by adding 48 frequency-modulated pure tones (components). For each of the 48 components, a random mean carrier frequency (Cf) between 1200 Hz and 2000 Hz was selected (starting phase was random). The range over which the carrier frequency was modulated was defined as Cf ±Cf×0.25 (e.g., for a mean carrier frequency of 1200 Hz, the carrier frequency was modulated between 900 Hz and 1500 Hz). For each of the 48 components, the rate at which the carrier frequency was modulated was not fixed but changed randomly between 2 Hz and 5 Hz within each component (the randomization differed between components). The 48 components were summed in order to obtain a narrow-band noise sound. We henceforth refer to this stimulus as the S0 condition (an abbreviation for a stimulus with no congruent frequency components; see <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5A</xref>, top).</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6 xref-fig-5-7 xref-fig-5-8 xref-fig-5-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><title hwp:id="title-25">Stimulus conditions and neural responses for Experiment II.</title><p hwp:id="p-41"><bold>A</bold>: Examples of each of the sound conditions. Note that the representation of the auditory stimulus does not reflect the waveform of the narrow-band noise but instead the 48 frequency components (sound frequency on the y-axis). In S33, S67, and S100, 16, 32 or all 48 frequency components (respectively) become synchronized in phase at about 2.2 s after sound onset. <bold>B</bold>: Response time courses and scalp topographies for each condition. The dotted vertical lines mark the time window of interest used for analysis (2.2–4.8 s). <bold>C</bold>: Mean response magnitude for the 2.2–4.8 s time window. Condition labels: S0 – stimulus with 0% congruent frequency components; S33 – 33% congruent components; S67 – 67% congruent components; S100 – 100% congruent components. Error bars reflect the standard error of the mean (removal of between-subject variance; (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-3" hwp:rel-id="ref-47">Masson and Loftus, 2003</xref>). *p &lt; 0.05.</p></caption><graphic xlink:href="261271_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-42">Three additional conditions were created in which the degree of a sound’s FM phase coherence was manipulated. In the S33 condition, the FM rate of each of the 48 components changed randomly between 2 Hz and 5 Hz (similar to S0), but approximately 2.2 seconds after sound onset 16 out of the 48 frequency components (33%) synchronized in phase, and FM rate remained constant at 3.5 Hz for the rest of the sound (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5A</xref>, second row). In the S67 condition, 32 out of the 48 frequency components (67%) synchronized in phase, and FM rate remained constant at 3.5 Hz for the rest of the sound (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 5A</xref>, third row). Finally, in the S100 condition, all 48 frequency components (100%) aligned in phase at about 2.2 seconds after sound onset and FM rate remained constant at 3.5 Hz for the rest of the sound (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5A</xref>, bottom). Stimuli were created such that there was no transient change at the transition from nonaligned to phase-aligned frequency components.</p><p hwp:id="p-43">Throughout Experiment II, participants listened passively to 138 trials of each condition while watching a movie of their choice with subtitles and the sound muted on a battery-driven, portable DVD player. The experiment was divided into 6 blocks. During each block, 23 trials of each condition were randomly presented. Trials were separated by a 2 s inter-stimulus interval.</p></sec><sec id="s5c" hwp:id="sec-20"><title hwp:id="title-26">EEG analysis</title><p hwp:id="p-44">The sustained response was analyzed by calculating the mean amplitude within the 2.2–4.8 second time window post stimulus onset and across the fronto-central-parietal electrode cluster. For each participant, a linear function fit was used to relate FM phase coherence (0, 33, 66, 100%) to the sustained response. In order test whether the sustained response was sensitive to FM phase coherence, the slope of the linear function was tested against zero using a one-sample t-test.</p><p hwp:id="p-45">Inter-trial phase coherence (ITPC) was calculated for neural activity within the 2.2–4.8 second time window post stimulus onset. ITPC was averaged across a 0.2-Hz wide frequency window for two frequencies: one was centered on the stimulus’ modulation rate (3.5 Hz) and one on the first harmonic (7 Hz). We focused on these two frequencies as FM stimulation commonly leads to spectral peaks at the stimulation frequency and its first harmonic (<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-3" hwp:rel-id="ref-57">Picton et al., 2003</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Henry et al., 2017</xref>). ITPC was averaged across the fronto-central-parietal electrode cluster. Separately for each participant, and for ITPC values at 3.5 Hz and ITPC values at 7 Hz, a linear function fit was used to relate FM phase coherence (0, 33, 66, 100%) to ITPC. In order test whether ITPC was sensitive to FM phase coherence, the slope of the linear function was tested against zero using a one-sample t-test (separately for 3.5 Hz and 7 Hz).</p></sec><sec id="s5d" hwp:id="sec-21"><title hwp:id="title-27">Results</title><p hwp:id="p-46"><xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Figure 5B</xref> shows the response time courses and topographical distributions for each condition. We observed a significant negative linear trend (t<sub>17</sub> = −3.88, p = 0.001, r<sub>e</sub> = 0.686; <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Figure 5C</xref>), indicating that the sustained response increased in magnitude (i.e., became more negative) as a function of FM phase coherence.</p><p hwp:id="p-47"><xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref> shows the ITPC frequency spectrum and topographical distributions for each condition. For the 3.5 Hz frequency, we observed a significant linear trend (t<sub>17</sub> = 3.18, p = 0.005, r<sub>e</sub> = 0.611), indicating that ITPC increased with increasing FM phase coherence. No effect of FM phase coherence was observed for the 7-Hz frequency (t<sub>17</sub> = 1.10, p = 0.686, r<sub>e</sub> = 0.258).</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-6"><title hwp:id="title-28">Inter-trial phase coherence (ITPC) results for Experiment II.</title><p hwp:id="p-48">Condition labels: S0 – stimulus with 0% congruent frequency components; S33 – 33% congruent components; S67 – 67% congruent components; S100 – 100% congruent components. Error bars reflect the standard error of the mean (removal of between-subject variance; (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-4" hwp:rel-id="ref-47">Masson and Loftus, 2003</xref>). *p &lt; 0.05. n.s. – not significant</p></caption><graphic xlink:href="261271_fig6" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-49">In order to compare the magnitude of the sustained response modulation with the magnitude of the ITPC modulation, the sustained response data and the ITPC data were separately z-transformed. The z-transformation was obtained as follows: We calculated the average response “AR” across all conditions and participants. The AR was subtracted from the response of each condition and participant. The resulting value for each condition and participant was subsequently divided by the AR. The sustained response data were also sign-inverted to ensure that for both ITPC and sustained activity, more positive values mean a larger response. Linear functions were fit to the z-transformed data as a function of FM phase coherence (0, 33, 67, 100%), and the slopes were compared between sustained response data and ITPC data using a paired t-test. This analysis revealed that neural synchronization (ITPC) was more strongly modulated by the degree of FM phase coherence in a sound compared to the sustained response (t<sub>17</sub> = 2.45, p = 0.026, r<sub>e</sub> = 0.511); in other words, the slope was steeper. The weaker apparent relation between the sustained, low frequency response and regularity may be owing to the sustained response being more sensitive to biological noise such as sweating, movement etc. compared to ITPC, which is an amplitude-normalized index. In order to account, in part, for such differences, we recalculated the analysis by applying a rationalized arcsine transform (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Studebaker, 1985</xref>) to ITPC data before calculating the z-transformation. The rationalized arcsine transform linearizes indices that range from 0 to 1, such as ITPC, and makes them more Gaussian. Similar to the original analysis, ITPC was more strongly modulated by the FM phase coherence than was the sustained response (t17 = 3.62, p = 0.002, re = 0.660).</p><p hwp:id="p-50">Finally, we investigated whether the systematic modulation of the sustained response by FM coherence is correlated with the modulation of ITPC using a repeated-measures correlation (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-3" hwp:rel-id="ref-3">Bakdash and Marusich, 2017</xref>). To this end, we entered the four paired measures (S0, S33, S67, S100) of ITPC and sustained activity into the analysis. We observed a negative correlation, indicating that an increase in sustained response (i.e., more negative) was accompanied by an increase in ITPC, but this correlation was only marginally significant (r = −0.263, p = 0.053; <xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7</xref>).</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-7"><title hwp:id="title-29">Repeated-measures correlation between sustained response and inter-trial phase coherence.</title><p hwp:id="p-51">Note that the plots display a negative correlation, because an increase in sustained activity reflects a negative-going signal, whereas an increase in ITPC is positive-going. Observations from the same participant are displayed in the same color, with the corresponding lines showing the fit of the repeated-measures correlation for each participant.</p></caption><graphic xlink:href="261271_fig7" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s5e" hwp:id="sec-22"><title hwp:id="title-30">Summary</title><p hwp:id="p-52">The results of Experiment II show that parametrically manipulating the degree of frequency modulation in a sound leads to systematic changes in the magnitude of the sustained response and neural synchronization. The data also indicate that synchronization of neural activity is more sensitive to regularity imposed by a coherent frequency modulation in sounds compared to the sustained response. The data might thus indicate that the relation between neural synchronization and sustained activity levels is indirect, rather than that they are tightly linked. In order to investigate the relationship between the sustained response and neural synchronization further, we conducted Experiment III in which we utilized a selective attention task. If the sustained response and neural synchronization are differentially affected by attentional state, this would be evidence that the two neural systems are at least partially independent.</p></sec></sec><sec id="s6" hwp:id="sec-23"><title hwp:id="title-31">Experiment III: Effects of attention on sustained response and neural synchronization</title><p hwp:id="p-53">Experiment III aimed to investigate the degree to which attention affects the sustained response and neural synchronization. Participants either attended to sounds with and without regularity or ignored the sounds and attended to a visual multiple object tracking (MOT; (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Pylyshyn and Storm, 1988</xref>) task that requires continuous attentional focus (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Cavanagh and Alvarez, 2005</xref>; <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Alvarez and Franconeri, 2007</xref>; <xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Tombu and Seiffert, 2008</xref>; <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">Scholl, 2009</xref>) and is thus suitable for distraction over multiple seconds (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Masutomi et al., 2016</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Herrmann and Johnsrude, 2018</xref>).</p><sec id="s6a" hwp:id="sec-24"><title hwp:id="title-32">Participants</title><p hwp:id="p-54">Twenty-seven participants who did not participate in Experiments I &amp; II participated in Experiment III (mean age [standard deviation]: 20.3 ±3.3 years; 15 female). Three additional participants did not comply with the task instructions and were thus excluded.</p></sec><sec id="s6b" hwp:id="sec-25"><title hwp:id="title-33">Acoustic stimulation &amp; procedure</title><p hwp:id="p-55">Acoustic stimuli were narrow-band noises similar to the stimuli used in Experiment II. In one condition, none of the 48 frequency components were synchronized in modulation rate over time. In a second condition, 38 out of 48 frequency components (79%) became synchronized at about 1.6 seconds after sound onset. The sound’s duration varied between 4 and 5 seconds (which was related to the duration categorization task described below).</p><p hwp:id="p-56">During the experiment, participants sat in front of a Dell LCD computer screen (~70 cm away; 75 Hz repetition rate; 24-inch diagonal). On every trial, participants were concurrently presented with a visual moving dot display and an auditory stimulus (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8A</xref>). Each trial started with a 1.3-s stationary display of 16 dots (dot diameter: 1.2 cm [0.9°]) of which 10 were white (distractor dots) and 6 were red (target dots). Presentation of dots was constrained to a display frame of 20.6 cm width (15.6°) and 19.4 cm height (14.7°) centered on the screen and highlighted to the participants by a gray frame on a black background. A yellow fixation square (0.16 cm [0.12°]) was presented at the center of the display frame. After 1.3 s, all dots reverted to white and the dots started to move. Concurrently with the dot movement and for the duration of the dot movement, one of the two sound conditions was played to the participant via headphones. Dots never moved outside of the display frame and never overlapped during movements; dots moved approximately 3.7 cm/s (2.8°/s). After 4 or 5 seconds (equal number of trials of each), the sound ended, the dot display froze, and one dot was marked in green (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Figure 8A</xref>).</p><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3 xref-fig-8-4 xref-fig-8-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8:</label><caption hwp:id="caption-8"><title hwp:id="title-34">Experimental design and response time course for Experiment III.</title><p hwp:id="p-57"><bold>A</bold>: Shows the experimental design. Participants performed either an auditory duration categorization task (half of the blocks) or a visual multiple object tracking task (the other half of the blocks). Note that the representation of the auditory stimulus does not reflect the waveform of the narrow-band noise but instead the 48 frequency components similar to <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-7" hwp:rel-id="F5">Figure 5A</xref> top. <bold>B</bold>: Response time courses and scalp topographies for each condition. The dotted vertical lines mark the time window of interest used for analysis (1.6–4 s). Bar graphs show the mean response magnitude for the 1.6–4 s time window. Condition labels: A0 – attention to auditory task, 0% congruent stimulus components; A79 – attention to auditory task, 79% congruent stimulus components; V0 – attention to visual task, 0% congruent stimulus components; V79 – attention to visual task, 79% congruent stimulus components. Error bars reflect the standard error of the mean (removal of between-subject variance; (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-5" hwp:rel-id="ref-47">Masson and Loftus, 2003</xref>). *p &lt; 0.05.</p></caption><graphic xlink:href="261271_fig8" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-58">Participants either performed a visual multiple object tracking (MOT) task or an auditory duration categorization task. In the MOT task, participants were asked to track those dots over time that were marked in red (target dots) in the pre-movement 1.3-s time period. They were also asked to ignore any sounds played concurrently with the dot movements. Participants had to judge whether the green dot was one of the dots they had been asked to track. In half of the trials, the green dot was indeed one of the target dots. In the other half of the trials, the green dot was one of the distractor dots. In the auditory duration categorization task, participants were asked to indicate whether they heard a sound with a short or long duration. During the auditory task, participants were asked to look at the fixation point on the screen, but to ignore the visual stimulation (which included the same MOT stimuli). Prior to the EEG recordings, participants underwent two training blocks, one for the visual task and one for the auditory task, and were taught what “short” and “long” durations were for the auditory task.</p><p hwp:id="p-59">In order to target a d′ of about 2 (i.e., difficult but manageable) (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Macmillan and Creelman, 2004</xref>) and to control for task difficulty differences between the visual task and the auditory task across participants, slight adjustments from the described procedure were implemented. The precise stimulation parameters were as follows: For 15 participants, the stimulus sound (and dot movements) lasted either 4 s (short) or 5 s (long) and, when the green dot was a distractor dot, it was selected <italic toggle="yes">randomly</italic> out of the 10 distractor dots. For 8 participants, the stimulus sound (and dot movements) lasted either 4 s (short) or 5 s (long) and, when the green dot was a distractor, it was chosen to be the distractor dot that was located <italic toggle="yes">closest</italic> to a target dot, making the MOT task more difficult. For 4 participants, the stimulus sound (and dot movements) lasted either 4.1 s (short) or 4.9 s (long), making duration categorization more difficult (as in variant 1 described above, green distractor dots were selected <italic toggle="yes">randomly</italic>). Critically, the auditory and visual stimulation was the same for all participants for the first four seconds after sound onset and we restricted our data analyses to this common period.</p><p hwp:id="p-60">Participants performed 6 blocks of about 11–12 min each. During three blocks, participants performed the visual task; in the other three blocks they performed the auditory task. Task blocks alternated and starting block was counterbalanced across participants. Within each block, 36 sounds of each condition (0% congruent components and 79% congruent components) were pseudo-randomly presented. Overall, participants listened to 108 sounds of each condition, once while they attended to the sounds and performed the sound-duration categorization task and once while they attended to the visual stimulation and performed the MOT task. Henceforth we refer to sounds with 0% congruent components and sounds with 79% congruent components in the auditory attention task as A0 and A79, respectively. We refer to V0 and V79 for the same sounds presented during the visual attention task.</p></sec><sec id="s6c" hwp:id="sec-26"><title hwp:id="title-35">Behavioral analysis</title><p hwp:id="p-61">Behavioral data were analyzed using d′ (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">Macmillan and Creelman, 2004</xref>). For the visual task, when the participant responded “yes”, we counted the response as a hit if the green dot was indeed a target, and as a false alarm if it was not. For the auditory task, when the participant responded “long”, we counted the response as a hit if the sound duration was indeed long (5 s or 4.9 s) and as a false alarm if it was not.</p><p hwp:id="p-62">A few participants struggled with one or both of the tasks, exhibiting relatively poor performance. In order to ensure that we included only those participants who, we were confident, were attending to the instructed task most of the time, we excluded participants with poor task performance from subsequent analyses. Hence, participants with a d′ &lt; 1 (corresponding approximately to 0.7 proportion correct responses and lower; with chance level being 0.5), in either of the tasks, were excluded at this stage (N=6), leading to twenty-one participants that were included in the EEG analysis.</p></sec><sec id="s6d" hwp:id="sec-27"><title hwp:id="title-36">EEG analysis</title><p hwp:id="p-63">The sustained response was analyzed by calculating the mean amplitude within the 1.6–4 second time window and across the fronto-central-parietal electrode cluster. A two-way rmANOVA with the factors FM coherence (0% vs. 79% congruent frequency components) and Attention (auditory, visual) was carried out. In order to account for potential differences in task difficulty on the individual level between the auditory task and the visual task, the difference between the auditory d′ and the visual d′ was entered as a covariate of no interest into the analysis.</p><p hwp:id="p-64">Inter-trial phase coherence (ITPC) was calculated for the responses within the 1.6–4 second time window. ITPC was averaged across 0.2-Hz wide frequency windows centered on the stimulus’ modulation rate and the first harmonic (3.5 Hz; 7 Hz), and averaged across the fronto-central-parietal electrode cluster. Separately for ITPC values at 3.5 Hz and ITPC values at 7 Hz, a two-way rmANOVA with the factors FM coherence (0% vs. 79% congruent frequency components) and Attention (auditory, visual) was carried out, again entering the difference between the auditory d′ and the visual d′ as a covariate of no interest.</p></sec><sec id="s6e" hwp:id="sec-28"><title hwp:id="title-37">Results</title><p hwp:id="p-65">Behavioral performance measured as d′ did not differ significantly between the auditory duration categorization task and the visual MOT task (t20 = 1.28, p = 0.215, r<sub>e</sub> = 0.275; auditory d′ [±std]: 2.13 ±0.66; visual d′ [±std]: 1.90 ±0.60).</p><p hwp:id="p-66">EEG response time courses and scalp topographies are displayed in <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Figure 8B</xref>. The rmANOVA revealed a main effect of FM coherence (F<sub>1,19</sub> = 7.13, p = 0.015, η<sub>p</sub><sup>2</sup> = 0.273) and a marginally significant effect of Attention (F<sub>1,19</sub> = 3.05, p = 0.097, η<sub>p</sub><sup>2</sup> = 0.138). Critically, the interaction between FM coherence × Attention was significant (F<sub>1,19</sub> = 4.63, p = 0.044, η<sub>p</sub><sup>2</sup> = 0.196), due to a larger response difference between stimulus types when participants attended to auditory stimuli (sound duration task) compared to when participants attended to visual stimuli (MOT task). In fact, the sustained response was larger (more negative) for stimuli with 79% congruent frequency components compared to stimuli with 0% congruent frequency components only when participants attended to the auditory stimuli (F<sub>1,19</sub> = 9.52, p<sub>FDR</sub> = 0.05, η<sub>p</sub><sup>2</sup> = 0.334), but not when they performed the visual distraction task (F<sub>1,19</sub> = 0.74, p<sub>FDR</sub> &gt; 0.05, η<sub>p</sub><sup>2</sup> = 0.037). An exploratory investigation of the effect of regularity on EEG activity separately for each electrode revealed only one electrode that was sensitive to auditory regularity when listeners performed the visual task (i.e., the sounds were not the focus of attention; Fp2; p = 0.027, uncorrected). In contrast, 11 out of the 16 electrodes were sensitive to regularity when listeners performed the auditory task (i.e., attended to the sounds; all p ≤ 0.05, uncorrected; Fp1, Fp2, Fz, F3, C3, C4, Cz, P3, P4, Pz, Oz).</p><p hwp:id="p-67">Results for the inter-trial phase coherence (ITPC) analysis are shown in <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure 9</xref>. The rmANOVA for the 3.5-Hz frequency revealed a main effect of FM coherence (F<sub>1,19</sub> = 12.40, p = 0.002, η<sub>p</sub><sup>2</sup> = 0.395), due to larger ITPC values for stimuli with 79% congruent frequency components compared to stimuli with 0% congruent frequency components. Neither the main effect of Attention (F<sub>1,19</sub> &lt; 0.01, p = 0.970, η<sub>p</sub><sup>2</sup> &lt; 0.001) nor the FM coherence × Attention interaction were significant (F<sub>1,19</sub> = 1.66, p = 0.213, η<sub>p</sub><sup>2</sup> = 0.080).</p><fig id="fig9" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;261271v2/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9:</label><caption hwp:id="caption-9"><title hwp:id="title-38">Inter-trial phase coherence (ITPC) results for Experiment III.</title><p hwp:id="p-68">Condition labels: A0 – attention to auditory task, 0% congruent stimulus components; A79 – attention to auditory task, 79% congruent stimulus components; V0 – attention to visual task, 0% congruent stimulus components; V79 – attention to visual task, 79% congruent stimulus components. Error bars reflect the standard error of the mean following removal of between-subject variance (<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-6" hwp:rel-id="ref-47">Masson and Loftus, 2003</xref>). *p &lt; 0.05.</p></caption><graphic xlink:href="261271_fig9" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-69">The rmANOVA for the 7-Hz frequency (first harmonic) revealed a main effect of FM coherence (F<sub>1,19</sub> = 9.82, p = 0.005, η<sub>p</sub><sup>2</sup> = 0.341), no effect of Attention (F<sub>1,19</sub> = 0.81, p = 0.381, η<sub>p</sub><sup>2</sup> = 0.041), and a significant FM coherence × Attention interaction (F<sub>1,19</sub> = 7.21, p = 0.015, η<sub>p</sub><sup>2</sup> = 0.275). The interaction was due to a larger ITPC difference between stimulus types when participants attended to the visual stimuli (MOT task) compared to when participants attended to the auditory stimuli (sound duration task). In fact, ITPC was larger for stimuli with 79% congruent frequency components compared to stimuli with 0% congruent frequency components only when participants attended to the visual stimuli (F 1,19 = 16.06, p<sub>FDR</sub> = 0.05, η<sub>p</sub><sup>2</sup> = 0.458), but not when they attended to auditory stimuli (F<sub>1,19</sub> = 1.46, p<sub>FDR</sub> &gt; 0.05, η<sub>p</sub><sup>2</sup> = 0.071).</p></sec><sec id="s6f" hwp:id="sec-29"><title hwp:id="title-39">Summary</title><p hwp:id="p-70">The results of Experiment III show a differential effect of attention on the regularity-related sustained activity compared to neural synchronization. The sustained activity was affected by attention, such that an effect of FM phase coherence (i.e., temporal regularity) was only observed when participants attended to the sounds. In contrast, neural synchronization at the stimulation frequency (3.5 Hz) was not affected by the manipulation of attention, whereas neural synchronization at the first harmonic (7 Hz) differed between stimulus conditions only when participants attended to the visual stimulation. These results show that, for the processing of temporal regularity in sounds, neural synchronization and sustained activity are differently affected by a distracting visual task.</p><p hwp:id="p-71">Rather unexpectedly, we observed a marginally significant increase in sustained activity levels when participants performed the visual compared to the auditory task (<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-4" hwp:rel-id="F8">Figure 8B</xref>). Note that, in conducting this study, we matched the behavioral performance level of the two tasks across participants, and accounted for any remaining within-subject differences between tasks by using the behavioral performance difference as a co-variate in our neural response analysis. This reduces the likelihood that the overall increase in sustained activity reflects a difference in task difficulty. It is also unlikely that the sustained response reached a ceiling level, given that it continued to increase (i.e., became more negative) throughout the epoch. Finally, another possibility is that visual activity increased when participants performed the MOT task (even though visual and auditory stimulation was physically identical in both tasks), leading to an overall increase in sustained activity. Although there have been reports of sustained activity for visual stimuli (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Järvilehto et al., 1978</xref>), topographical distributions of the sustained response were very similar across the three experiments (without clear contributions from occipital electrodes), which suggests no additional generator in visual areas specifically in Experiment III compared to Experiments I and II.</p></sec></sec><sec id="s7" hwp:id="sec-30"><title hwp:id="title-40">Discussion</title><p hwp:id="p-72">The current set of experiments investigated the relation between two types of neural responses – synchronization and sustained activity – that are associated with the processing of temporal regularity in sounds. Experiment I demonstrates that neural activity synchronizes with a sound’s temporal regularity while sustained activity increases. In Experiment II, parametric manipulation of FM phase coherence in sounds systematically modulated neural synchronization and sustained neural activity, albeit neural synchronization was more sensitive to coherent frequency modulation. Experiment III revealed that under visual distraction neural activity synchronized with a sound’s temporal regularity, whereas sustained activity was not sensitive to this regularity. The current data show that, in the context of temporal regularity in sounds, neural synchronization and sustained neural activity co-occur, but that the two neural signatures are differentially affected when a listener is distracted by a demanding visual task.</p><sec id="s7a" hwp:id="sec-31"><title hwp:id="title-41">Sensitivity of neural synchronization and sustained activity to temporal regularity in sounds</title><p hwp:id="p-73">In each of the three EEG experiments, we observed that neural activity synchronized with the temporal regularity (i.e., frequency modulation) in the sounds (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figures 2</xref>, <xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">6</xref>, and <xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-2" hwp:rel-id="F9">9</xref>). Synchronization of neural activity – sometimes referred to as phase-locking or entrainment of neural oscillations – is commonly observed for sounds that contain temporal regularity (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">John et al., 2002</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Stefanics et al., 2010</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-2" hwp:rel-id="ref-43">Lakatos et al., 2013b</xref>; <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Kayser et al., 2015</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Presacco et al., 2016</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Herrmann et al., 2017</xref>), including frequency-modulated sounds similar to the ones utilized here (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">Boettcher et al., 2002</xref>; <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-4" hwp:rel-id="ref-57">Picton et al., 2003</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-3" hwp:rel-id="ref-29">Herrmann et al., 2013</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Henry et al., 2017</xref>).</p><p hwp:id="p-74">The current data also show that, for sounds that consist of brief tones and narrow-band noise sounds, sustained neural activity increases following the onset of a regularity; in this case, coherent frequency modulation (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-8" hwp:rel-id="F5">Figures 5</xref> and <xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-5" hwp:rel-id="F8">8</xref>). Hence, our data are consistent with previous studies that have demonstrated sensitivity of sustained neural activity to other types of regularities in sounds such as repeating tone-sequence patterns and coherent frequency changes in broadband chords (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-13" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-5" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-5" hwp:rel-id="ref-71">Teki et al., 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-9" hwp:rel-id="ref-66">Southwell et al., 2017</xref>). Our results are also in line with studies that report sustained activity in response to sounds with a very simple regular structure such as pure tones (<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Köhler and Wegner, 1955</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">David et al., 1969</xref>; <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Picton et al., 1978</xref>; <xref ref-type="bibr" rid="c74" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">Weise et al.</xref>, in press).</p><p hwp:id="p-75">Parametric manipulation of the degree of coherent frequency modulation (i.e., temporal regularity) in a narrow-band noise led to a systematic increase in both neural synchronization (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6</xref>) and sustained neural activity (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-9" hwp:rel-id="F5">Figure 5</xref>) although neural synchronization was more sensitive to the degree of FM coherence compared to sustained neural activity. Within-participant correlations further indicate that the increase in neural synchronization and the increase in sustained activity are somehow related. Future work will investigate whether the neural populations underlying neural synchronization and sustained activity are directly coupled or whether their apparent relation is instead due to a third process that influences both.</p><p hwp:id="p-76">One important contribution of the current study is that we demonstrate the simultaneous occurrence of increased sustained neural activity and neural synchronization for temporally regular sounds. Previous studies that investigated neural synchronization (e.g., (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Maiste and Picton, 1989</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-3" hwp:rel-id="ref-23">Henry and Obleser, 2012</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-4" hwp:rel-id="ref-29">Herrmann et al., 2013</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">Keitel et al., 2017</xref>) have not reported changes in sustained activity, likely because the sustained activity is a low-frequency (DC – direct current) signal and the application of a high-pass filter is a common pre-processing step for electrophysiological data. The current data thus indicate that multiple, concurrent neural signals index the processing of temporally regular structure in sounds.</p></sec><sec id="s7b" hwp:id="sec-32"><title hwp:id="title-42">The influence of attention on neural synchronization and sustained activity</title><p hwp:id="p-77">In Experiment III, we showed that neural activity thought to originate in auditory cortex (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">Näätänen and Picton, 1987</xref>; <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-5" hwp:rel-id="ref-57">Picton et al., 2003</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-5" hwp:rel-id="ref-29">Herrmann et al., 2013</xref>) synchronizes with a sound’s temporal regularity independent of attention. In contrast, the regularity-based increase in sustained response was only observed when participants attended to auditory stimuli or when they were lightly distracted by watching a movie (Experiments I and II), but not when participants were distracted by a demanding visual task (Experiment III).</p><p hwp:id="p-78">Previous work investigating the neural correlates of attention to a sound’s rhythm (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Elhilali et al., 2009</xref>; <xref ref-type="bibr" rid="c78" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">Xiang et al., 2010</xref>; <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Lakatos et al., 2013a</xref>; <xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">Zion Golumbic et al., 2013</xref>) observed that synchronization depended on attention. However, attention to a sound’s rhythm may enhance the representation of periodicity (and thus synchronization), whereas attention to a sound’s duration (as in this study) may not have the same effect. Consistent with our and previous work, much of the existing research suggests that detection of regularities in sounds can occur automatically (<xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Sussman et al., 2007</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-3" hwp:rel-id="ref-77">Winkler et al., 2009</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">Bendixen, 2014</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-6" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-10" hwp:rel-id="ref-66">Southwell et al., 2017</xref>; <xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">Sussman, 2017</xref>), but that attention may interact with automatic processing depending on the task (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-2" hwp:rel-id="ref-69">Sussman, 2017</xref>).</p><p hwp:id="p-79">Previous work suggested that an increase in sustained activity due to a sound’s regularity is independent of attention (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-7" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>). In this study, an increase was observed when participants performed either a visual image-detection task (while ignoring sounds) or an acoustic change-detection task (i.e., attending to sounds). The difference between this previous work and our results may be due to two reasons. First, in the previous study, attention was manipulated using a between-subject design, in which one participant group performed the visual task and another group the auditory task (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-8" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>). In the current study, the same participants performed both the visual and the auditory task (in different blocks using a within-subject design), which may increase sensitivity. Second, in the previous study, images for the visual distraction task were presented at intervals ranging from 0.5 to 4 s (and only 20% of the images required a response), which may have left time for participant to switch attention to the auditory stimuli (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-9" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>). The MOT task utilized here requires continuous attention (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Cavanagh and Alvarez, 2005</xref>; <xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-2" hwp:rel-id="ref-62">Scholl, 2009</xref>) and is thus suitable for distraction from auditory stimuli over multiple seconds (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-2" hwp:rel-id="ref-48">Masutomi et al., 2016</xref>). The data indicate that a demanding visual task may be suppressing regularity-based sustained activity rather than that attention to sounds is enhancing it.</p></sec><sec id="s7c" hwp:id="sec-33"><title hwp:id="title-43">Neural synchronization and sustained activity may reflect different stages of regularity processing</title><p hwp:id="p-80">The scalp topographies in the current study show a focal spatial distribution for neural synchronization as opposed to the more wide-spread spatial distribution for the sustained response. Source localization studies suggest the main source underlying neural synchronization to low-frequency periodicities (&lt;8 Hz) is located in auditory cortex (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-6" hwp:rel-id="ref-29">Herrmann et al., 2013</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Herrmann et al., 2016</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-3" hwp:rel-id="ref-38">Keitel et al., 2017</xref>), whereas the sustained activity appears to additionally involve the inferior frontal cortex, parietal cortex, and hippocampus (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-14" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-6" hwp:rel-id="ref-71">Teki et al., 2016</xref>). A difference in neural sources between the two responses may also indicate a functional difference. Neural synchronization is directly related to stimulus acoustics as it reflects a neural signal that tracks a sound’s periodicity (temporal regularity). In contrast, an increase in sustained activity appears to be independent of a specific regularity in a sound as it has been observed for repeating tone-sequence patterns, coherent frequency changes in broadband chords, and here for frequency-modulated sounds (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-15" hwp:rel-id="ref-5">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-10" hwp:rel-id="ref-65">Sohoglu and Chait, 2016</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-7" hwp:rel-id="ref-71">Teki et al., 2016</xref>). Hence, it is possible that neural synchronization may reflect a hierarchically lower, sensory process, whereas sustained neural activity reflects a more abstract process related to more general structure in sounds.</p><p hwp:id="p-81">The results from Experiment III are consistent with the neural representation of a sound’s temporal regularity in auditory cortex being unaffected by attention, whereas an attentionally demanding visual task suppresses the neural representation of the regularity in higher-level brain regions. This is also in line with the well-accepted view of auditory processing that assumes that neural activity in hierarchically lower regions in the auditory system is mostly sensitive to acoustic properties of sounds and less receptive to a listener’s attentional state, whereas neural activity in hierarchically higher regions is more sensitive to the attentional state of a listener (whether a listener attends to auditory stimuli or ignores them; (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Davis et al., 2007</xref>; <xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">Wild et al., 2012</xref>; <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Puvvada and Simon, 2017</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Holmes et al.</xref>, in press). We speculate that a listener’s attentional state affects the progression of the neural representation of a sound’s temporal regularity from auditory cortex to higher-level brain regions, and that this progression is suppressed in situations with distracting visual stimulation.</p></sec></sec><sec id="s8" hwp:id="sec-34"><title hwp:id="title-44">Conclusions</title><p hwp:id="p-82">The current study investigated the functional relationship between two neural signatures that index the detection of a regularity in sounds: neural synchronization and sustained activity. In three EEG experiments we show that neural synchronization and sustained activity occur concomitantly, that both neural signatures are parametrically modulated by the degree of temporal regularity in sounds, but that the two signatures are differentially affected when a listener is distracted by a demanding visual task. Our data may indicate that neural synchronization reflects a more sensory-driven response to regularity compared with sustained activity, which may be influenced by attentional, contextual, or other experiential factors.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-45">Acknowledgements</title><p hwp:id="p-83">Research was supported by the Canadian Institutes of Health Research (MOP133450 to I.S. Johnsrude), and the CFREF BrainsCAN postdoctoral fellowship awarded to B. Herrmann. We thank Youngkyung Jung and Suvarna Moharir for their help during data acquisition.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-46">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Alvarez GA"><surname>Alvarez</surname> <given-names>GA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Franconeri SL"><surname>Franconeri</surname> <given-names>SL</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-2">How many objects can you track?: Evidence for a resource-limited attentive tracking mechanism</article-title>. <source hwp:id="source-1">Journal of Vision</source> <volume>7</volume>:<fpage>1</fpage>–<lpage>10</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Baese-Berk MM"><surname>Baese-Berk</surname> <given-names>MM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heffner CC"><surname>Heffner</surname> <given-names>CC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dilley LC"><surname>Dilley</surname> <given-names>LC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pitt MA"><surname>Pitt</surname> <given-names>MA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morrill TH"><surname>Morrill</surname> <given-names>TH</given-names></string-name>, <string-name name-style="western" hwp:sortable="McAuley JD"><surname>McAuley</surname> <given-names>JD</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-3">Long-term temporal tracking of speech rate affects spoken-word recognition</article-title>. <source hwp:id="source-2">Psychological Science</source> <volume>25</volume>:<fpage>1546</fpage>–<lpage>1553</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2 xref-ref-3-3"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bakdash JZ"><surname>Bakdash</surname> <given-names>JZ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marusich LR"><surname>Marusich</surname> <given-names>LR</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-4">Repeated Measures Correlation</article-title>. <source hwp:id="source-3">Frontiers in Psychology</source> <volume>8</volume>:Article <fpage>456</fpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Baldeweg T"><surname>Baldeweg</surname> <given-names>T</given-names></string-name> (<year>2006</year>) <article-title hwp:id="article-title-5">Repetition effects to sounds: evidence for predictive coding in the auditory system</article-title>. <source hwp:id="source-4">Trends in Cognitive Sciences</source> <volume>10</volume>:<fpage>93</fpage>–<lpage>94</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2 xref-ref-5-3 xref-ref-5-4 xref-ref-5-5 xref-ref-5-6 xref-ref-5-7 xref-ref-5-8 xref-ref-5-9 xref-ref-5-10 xref-ref-5-11 xref-ref-5-12 xref-ref-5-13 xref-ref-5-14 xref-ref-5-15"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Barascud N"><surname>Barascud</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pearce MT"><surname>Pearce</surname> <given-names>MT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Griffiths TD"><surname>Griffiths</surname> <given-names>TD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Friston KJ"><surname>Friston</surname> <given-names>KJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-6">Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title>. <source hwp:id="source-5">Proceedings of the National Academy of Sciences</source> <volume>113</volume>:<fpage>E616</fpage>–<lpage>E625</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Barnes R"><surname>Barnes</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones MR"><surname>Jones</surname> <given-names>MR</given-names></string-name> (<year>2000</year>) <article-title hwp:id="article-title-7">Expectancy, Attention, and Time</article-title>. <source hwp:id="source-6">Cognitive Psychology</source> <volume>41</volume>:<fpage>254</fpage>–<lpage>311</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Bell AJ"><surname>Bell</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sejnowski TJ"><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name> (<year>1995</year>) <article-title hwp:id="article-title-8">An information maximization approach to blind separation and blind deconvolution</article-title>. <source hwp:id="source-7">Neural Computation</source> <volume>7</volume>:<fpage>1129</fpage>–<lpage>1159</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Bendixen A"><surname>Bendixen</surname> <given-names>A</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-9">Predictability effects in auditory scene analysis: a review</article-title>. <source hwp:id="source-8">Frontiers in Neuroscience</source> <volume>2014</volume>:Article <fpage>60</fpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Benjamini Y"><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hochberg Y"><surname>Hochberg</surname> <given-names>Y</given-names></string-name> (<year>1995</year>) <article-title hwp:id="article-title-10">Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source hwp:id="source-9">Journal of the Royal Statistical Society Series B</source> <volume>57</volume>:<fpage>289</fpage>–<lpage>300</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Boettcher FA"><surname>Boettcher</surname> <given-names>FA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Madhotra D"><surname>Madhotra</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poth EA"><surname>Poth</surname> <given-names>EA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mills JH"><surname>Mills</surname> <given-names>JH</given-names></string-name> (<year>2002</year>) <article-title hwp:id="article-title-11">The frequency-modulation following response in young and aged human subjects</article-title>. <source hwp:id="source-10">Hearing Research</source> <volume>165</volume>:<fpage>10</fpage>–<lpage>18</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Cavanagh P"><surname>Cavanagh</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Alvarez GA"><surname>Alvarez</surname> <given-names>GA</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-12">Tracking multiple targets with multifocal attention</article-title>. <source hwp:id="source-11">Trends in Cognitive Sciences</source> <volume>9</volume>:<fpage>349</fpage>–<lpage>354</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Costa-Faidella J"><surname>Costa-Faidella</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sussman ES"><surname>Sussman</surname> <given-names>ES</given-names></string-name>, <string-name name-style="western" hwp:sortable="Escera C"><surname>Escera</surname> <given-names>C</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-13">Selective entrainment of brain oscillations drives auditory perceptual organization</article-title>. <source hwp:id="source-12">NeuroImage</source> <volume>159</volume>:<fpage>195</fpage>–<lpage>206</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="David E"><surname>David</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Finkenzeller P"><surname>Finkenzeller</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kallert S"><surname>Kallert</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Keidel WD"><surname>Keidel</surname> <given-names>WD</given-names></string-name> (<year>1969</year>) <article-title hwp:id="article-title-14">Akustischen Reizen zugeordnete Gleichspannungsänderungen am intakten Schädel des Menschen</article-title>. <source hwp:id="source-13">Pflügers Archiv</source> <volume>309</volume>:<fpage>362</fpage>–<lpage>367</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Davis MH"><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Coleman MR"><surname>Coleman</surname> <given-names>MR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Absalom AR"><surname>Absalom</surname> <given-names>AR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rodd JM"><surname>Rodd</surname> <given-names>JM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnsrude IS"><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Matta BF"><surname>Matta</surname> <given-names>BF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Owen AM"><surname>Owen</surname> <given-names>AM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Menon DK"><surname>Menon</surname> <given-names>DK</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-15">Dissociating speech perception and comprehension at reduced levels of awareness</article-title>. <source hwp:id="source-14">Proceedings of the National Academy of Sciences</source> <volume>104</volume>:<fpage>16032</fpage>–<lpage>16037</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-16">Emergence of neural encoding of auditory objects while listening to competing speakers</article-title>. <source hwp:id="source-15">Proceedings of the National Academy of Sciences</source> <volume>109</volume>:<fpage>11854</fpage>–<lpage>11859</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Doelling KB"><surname>Doelling</surname> <given-names>KB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poeppel D"><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-17">Cortical entrainment to music and its modulation by expertise</article-title>. <source hwp:id="source-16">Proceedings of the National Academy of Sciences</source> <volume>112</volume>:<fpage>E6233</fpage>–<lpage>E6242</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Elhilali M"><surname>Elhilali</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xiang J"><surname>Xiang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shamma SA"><surname>Shamma</surname> <given-names>SA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-18">Interaction between Attention and Bottom-Up Saliency Mediates the Representation of Foreground and Background in an Auditory Scene</article-title>. <source hwp:id="source-17">PLoS Biology</source> <volume>7</volume>:<fpage>e1000129</fpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Garrido MI"><surname>Garrido</surname> <given-names>MI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sahani M"><surname>Sahani</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dolan RJ"><surname>Dolan</surname> <given-names>RJ</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-19">Outlier Responses Reflect Sensitivity to Statistical Structure in the Human Brain</article-title>. <source hwp:id="source-18">PLoS Computational Biology</source> <volume>9</volume>:<fpage>e1002999</fpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Genovese CR"><surname>Genovese</surname> <given-names>CR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lazar NA"><surname>Lazar</surname> <given-names>NA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichols T"><surname>Nichols</surname> <given-names>T</given-names></string-name> (<year>2002</year>) <article-title hwp:id="article-title-20">Thresholding of statistical maps in functional neuroimaging using the false discovery rate</article-title>. <source hwp:id="source-19">NeuroImage</source> <volume>15</volume>:<fpage>870</fpage>–<lpage>878</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Giraud A-L"><surname>Giraud</surname> <given-names>A-L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poeppel D"><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-21">Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source hwp:id="source-20">Nature Neuroscience</source> <volume>15</volume>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Greenhouse SW"><surname>Greenhouse</surname> <given-names>SW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Geisser S"><surname>Geisser</surname> <given-names>S</given-names></string-name> (<year>1959</year>) <article-title hwp:id="article-title-22">On Methods in the Analysis of Profile Data</article-title>. <source hwp:id="source-21">Psychometrika</source> <volume>24</volume>:<fpage>95</fpage>–<lpage>112</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Heilbron M"><surname>Heilbron</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name> (in press) <article-title hwp:id="article-title-23">Great expectations: Is there evidence for predictive coding in auditory cortex?</article-title> <source hwp:id="source-22">Neuroscience</source>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2 xref-ref-23-3"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-24">Frequency modulation entrains slow neural oscillations and optimizes human listening behavior</article-title>. <source hwp:id="source-23">Proceedings of the National Academy of Sciences</source> <volume>109</volume>:<fpage>20095</fpage>–<lpage>20100</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-25">Low-Frequency Neural Oscillations Support Dynamic Attending in Temporal Context</article-title>. <source hwp:id="source-24">Timing &amp; Time Perception</source> <volume>2</volume>:<fpage>62</fpage>–<lpage>86</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name> (<year>2014</year>) <article-title hwp:id="article-title-26">Entrained neural oscillations in multiple frequency bands co-modulate behavior</article-title>. <source hwp:id="source-25">Proceedings of the National Academy of Sciences</source> <volume>111</volume>:<fpage>14935</fpage>–<lpage>14940</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kunke D"><surname>Kunke</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-27">Aging affects the balance of neural entrainment and top-down neural modulation in the listening brain</article-title>. <source hwp:id="source-26">Nature Communications</source> <volume>8</volume>:<fpage>15801</fpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnsrude IS"><surname>Johnsrude</surname> <given-names>IS</given-names></string-name> (<year>2018</year>) <article-title hwp:id="article-title-28">Attentional State Modulates the Effect of an Irrelevant Stimulus Dimension on Perception</article-title>. <source hwp:id="source-27">Journal of Experimental Psychology: Human Perception and Performance</source> <volume>44</volume>:<fpage>89</fpage>–<lpage>105</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Parthasarathy A"><surname>Parthasarathy</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bartlett EL"><surname>Bartlett</surname> <given-names>EL</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-29">Aging affects dual encoding of periodicity and envelope shape in rat inferior colliculus neurons</article-title>. <source hwp:id="source-28">European Journal of Neuroscience</source> <volume>45</volume>:<fpage>299</fpage>–<lpage>311</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2 xref-ref-29-3 xref-ref-29-4 xref-ref-29-5 xref-ref-29-6"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grigutsch M"><surname>Grigutsch</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-30">Oscillatory Phase Dynamics in Neural Entrainment Underpin Illusory Percepts of Time</article-title>. <source hwp:id="source-29">The Journal of Neuroscience</source> <volume>33</volume>:<fpage>15799</fpage>–<lpage>15809</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Herrmann B"><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henry MJ"><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haegens S"><surname>Haegens</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Obleser J"><surname>Obleser</surname> <given-names>J</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-31">Temporal expectations and neural amplitude fluctuations in auditory cortex interactively influence perception</article-title>. <source hwp:id="source-30">NeuroImage</source> <volume>124</volume>:<fpage>487</fpage>–<lpage>497</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Holmes E"><surname>Holmes</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Purcell DW"><surname>Purcell</surname> <given-names>DW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carlyon RP"><surname>Carlyon</surname> <given-names>RP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gockel HE"><surname>Gockel</surname> <given-names>HE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnsrude IS"><surname>Johnsrude</surname> <given-names>IS</given-names></string-name> (in press) <article-title hwp:id="article-title-32">Attentional Modulation of Envelope-Following Responses at Lower (93-109 Hz) but Not Higher (217-233 Hz) Modulation Rates</article-title>. <source hwp:id="source-31">Journal of the Association for Research in Otolaryngology</source>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Idemaru K"><surname>Idemaru</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holt LL"><surname>Holt</surname> <given-names>LL</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-33">Word Recognition Reflects Dimension-based Statistical Learning</article-title>. <source hwp:id="source-32">Journal of Experimental Psychology: Human Perception and Performance</source> <volume>37</volume>:<fpage>1939</fpage>–<lpage>1956</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Järvilehto T"><surname>Järvilehto</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hari R"><surname>Hari</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sams M"><surname>Sams</surname> <given-names>M</given-names></string-name> (<year>1978</year>) <article-title hwp:id="article-title-34">Effect of stimulus repetition on negative sustained potentials elicited by auditory and visual stimuli in the human EEG</article-title>. <source hwp:id="source-33">Biological Psychology</source> <volume>7</volume>:<fpage>1</fpage>–<lpage>12</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="John MS"><surname>John</surname> <given-names>MS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dimitrijevic A"><surname>Dimitrijevic</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name> (<year>2002</year>) <article-title hwp:id="article-title-35">Auditory Steady-State Responses to Exponential Modulation Envelopes</article-title>. <source hwp:id="source-34">Ear &amp; Hearing</source> <volume>23</volume>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="John MS"><surname>John</surname> <given-names>MS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dimitrijevic A"><surname>Dimitrijevic</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Roon P"><surname>van Roon</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name> (<year>2001</year>) <article-title hwp:id="article-title-36">Multiple Auditory Steady-State Responses to AM and FM Stimuli</article-title>. <source hwp:id="source-35">Audiology &amp; Neuro-Otology</source> <volume>6</volume>:<fpage>12</fpage>–<lpage>27</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Jones MR"><surname>Jones</surname> <given-names>MR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moynihan H"><surname>Moynihan</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="MacKenzie N"><surname>MacKenzie</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Puente J"><surname>Puente</surname> <given-names>J</given-names></string-name> (<year>2002</year>) <article-title hwp:id="article-title-37">Temporal Aspects of Stimulus-Driven Attending in Dynamic Arrays</article-title>. <source hwp:id="source-36">Psychological Science</source> <volume>13</volume>:<fpage>313</fpage>–<lpage>319</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Kayser SJI"><surname>Kayser</surname> <given-names>SJI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Robin A. A."><surname>Robin</surname> <given-names>A. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross J"><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-38">Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha</article-title>. <source hwp:id="source-37">The Journal of Neuroscience</source> <volume>35</volume>:1469114701.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2 xref-ref-38-3"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Keitel A"><surname>Keitel</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ince RAA"><surname>Ince</surname> <given-names>RAA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross J"><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kayser C"><surname>Kayser</surname> <given-names>C</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-39">Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</article-title>. <source hwp:id="source-38">NeuroImage</source> <volume>147</volume>:<fpage>32</fpage>–<lpage>42</lpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Köhler W"><surname>Köhler</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wegner J"><surname>Wegner</surname> <given-names>J</given-names></string-name> (<year>1955</year>) <article-title hwp:id="article-title-40">Currents of the human auditory cortex</article-title>. <source hwp:id="source-39">Journal of Cellular Physiology</source> <volume>45</volume>:<fpage>25</fpage>–<lpage>54</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Lachaux J-P"><surname>Lachaux</surname> <given-names>J-P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rodriguez E"><surname>Rodriguez</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martinerie J"><surname>Martinerie</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varela FJ"><surname>Varela</surname> <given-names>FJ</given-names></string-name> (<year>1999</year>) <article-title hwp:id="article-title-41">Measuring Phase Synchrony in Brain Signals</article-title>. <source hwp:id="source-40">Human Brain Mapping</source> <volume>8</volume>:<fpage>194</fpage>–<lpage>208</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leitman DI"><surname>Leitman</surname> <given-names>DI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Javitt DC"><surname>Javitt</surname> <given-names>DC</given-names></string-name> (<year>2013a</year>) <article-title hwp:id="article-title-42">Predictive Suppression of Cortical Excitability and Its Deficit in Schizophrenia</article-title>. <source hwp:id="source-41">The Journal of Neuroscience</source> <volume>33</volume>:<fpage>11692</fpage>–<lpage>11702</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Karmos G"><surname>Karmos</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mehta AD"><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ulbert I"><surname>Ulbert</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name> (<year>2008</year>) <article-title hwp:id="article-title-43">Entrainment of Neuronal Oscillations as a Mechanism of Attentional Selection</article-title>. <source hwp:id="source-42">Science</source> <volume>320</volume>:<fpage>110</fpage>–<lpage>113</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1 xref-ref-43-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Musacchia G"><surname>Musacchia</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Connel MN"><surname>O’Connel</surname> <given-names>MN</given-names></string-name>, <string-name name-style="western" hwp:sortable="Falchier AY"><surname>Falchier</surname> <given-names>AY</given-names></string-name>, <string-name name-style="western" hwp:sortable="Javitt DC"><surname>Javitt</surname> <given-names>DC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name> (<year>2013b</year>) <article-title hwp:id="article-title-44">The Spectrotemporal Filter Mechanism of Auditory Selective Attention</article-title>. <source hwp:id="source-43">Neuron</source> <volume>77</volume>:<fpage>750</fpage>–<lpage>761</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2"><citation publication-type="book" citation-type="book" ref:id="261271v2.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Macmillan NA"><surname>Macmillan</surname> <given-names>NA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Creelman CD"><surname>Creelman</surname> <given-names>CD</given-names></string-name> (<year>2004</year>) <chapter-title>Detection Theory: A User’s Guide</chapter-title>. <publisher-loc>New Jersey, USA</publisher-loc>: <publisher-name>Psychology Press</publisher-name>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Maiste A"><surname>Maiste</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name> (<year>1989</year>) <article-title hwp:id="article-title-45">Human auditory evoked potentials to frequency-modulated tones</article-title>. <source hwp:id="source-44">Ear &amp; Hearing</source> <volume>10</volume>:<fpage>153</fpage>–<lpage>160</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="book" citation-type="book" ref:id="261271v2.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Makeig S"><surname>Makeig</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bell AJ"><surname>Bell</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jung T-P"><surname>Jung</surname> <given-names>T-P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sejnowski TJ"><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name> (<year>1996</year>) <chapter-title>Independent component analysis of electroencephalographic data</chapter-title>. <source hwp:id="source-45">In: Advances in Neural Information Processing Systems</source> (<person-group hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Touretzky D"><surname>Touretzky</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mozer M"><surname>Mozer</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hasselmo M"><surname>Hasselmo</surname> <given-names>M</given-names></string-name></person-group>, eds). <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2 xref-ref-47-3 xref-ref-47-4 xref-ref-47-5 xref-ref-47-6"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Masson MEJ"><surname>Masson</surname> <given-names>MEJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Loftus GR"><surname>Loftus</surname> <given-names>GR</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-46">Using confidence intervals for graphically based data interpretation</article-title>. <source hwp:id="source-46">Canadian Journal of Experimental Psychology and Aging</source> <volume>57</volume>:<fpage>203</fpage>–<lpage>220</lpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1 xref-ref-48-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Masutomi K"><surname>Masutomi</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barascud N"><surname>Barascud</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kashino M"><surname>Kashino</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="McDermott JH"><surname>McDermott</surname> <given-names>JH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-47">Sound Segregation via Embedded Repetition Is Robust to Inattention</article-title>. <source hwp:id="source-47">Journal of Experimental Psychology: Human Perception and Performance</source> <volume>42</volume>:<fpage>386</fpage>–<lpage>400</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="McDermott JH"><surname>McDermott</surname> <given-names>JH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schemitsch M"><surname>Schemitsch</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli EP"><surname>Simoncelli</surname> <given-names>EP</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-48">Summary statistics in auditory perception</article-title>. <source hwp:id="source-48">Nature Neuroscience</source> <volume>16</volume>:<fpage>493</fpage>–<lpage>498</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Millman RE"><surname>Millman</surname> <given-names>RE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mattys SL"><surname>Mattys</surname> <given-names>SL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gouws AD"><surname>Gouws</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prendergast G"><surname>Prendergast</surname> <given-names>G</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-49">Magnified Neural Envelope Coding Predicts Deficits in Speech Perception in Noise</article-title>. <source hwp:id="source-49">The Journal of Neuroscience</source> <volume>37</volume>:<fpage>7727</fpage>–<lpage>7736</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Näätänen R"><surname>Näätänen</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name> (<year>1987</year>) <article-title hwp:id="article-title-50">The N1 wave of the human electric and magnetic response to sound: a review and an analysis of the component structure</article-title>. <source hwp:id="source-50">Psychophysiology</source> <volume>24</volume>:<fpage>375</fpage>–<lpage>425</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Nobre AC"><surname>Nobre</surname> <given-names>AC</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Ede F"><surname>van Ede</surname> <given-names>F</given-names></string-name> (<year>2018</year>) <article-title hwp:id="article-title-51">Anticipated moments: temporal structure in attention</article-title>. <source hwp:id="source-51">Nature Reviews Neuroscience</source> <volume>19</volume>:<fpage>34</fpage>–<lpage>48</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Nozaradan S"><surname>Nozaradan</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peretz I"><surname>Peretz</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Missal M"><surname>Missal</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mouraux A"><surname>Mouraux</surname> <given-names>A</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-52">Tagging the Neuronal Entrainment to Beat and Meter</article-title>. <source hwp:id="source-52">The Journal of Neuroscience</source> <volume>31</volume>:<fpage>10234</fpage>–<lpage>10240</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Oostenveld R"><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fries P"><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maris E"><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schoffelen JM"><surname>Schoffelen</surname> <given-names>JM</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-53">FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source hwp:id="source-53">Computational Intelligence and Neuroscience</source> <volume>2011</volume>:Article ID <fpage>156869</fpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Peelle JE"><surname>Peelle</surname> <given-names>JE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davis MH"><surname>Davis</surname> <given-names>MH</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-54">Neural oscillations carry speech rhythm through to comprehension</article-title>. <source hwp:id="source-54">Frontiers in Psychology</source> <volume>3</volume>:Article <fpage>320</fpage>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Woods DL"><surname>Woods</surname> <given-names>DL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Proulx GB"><surname>Proulx</surname> <given-names>GB</given-names></string-name> (<year>1978</year>) <article-title hwp:id="article-title-55">Human auditory sustained potentials. I. The nature of the response</article-title>. <source hwp:id="source-55">Electroencephalography and Clinical Neurophysiology</source> <volume>45</volume>:<fpage>186</fpage>–<lpage>197</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2 xref-ref-57-3 xref-ref-57-4 xref-ref-57-5"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Picton TW"><surname>Picton</surname> <given-names>TW</given-names></string-name>, <string-name name-style="western" hwp:sortable="John SM"><surname>John</surname> <given-names>SM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dimitrijevic A"><surname>Dimitrijevic</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Purcell DW"><surname>Purcell</surname> <given-names>DW</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-56">Human auditory steady-state responses</article-title>. <source hwp:id="source-56">International Journal of Audiology</source> <volume>42</volume>:<fpage>177</fpage>–<lpage>219</lpage>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Presacco A"><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anderson S"><surname>Anderson</surname> <given-names>S</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-57">Evidence of degraded representation of speech in noise, in the aging midbrain and cortex</article-title>. <source hwp:id="source-57">Journal of Neurophysiology</source> <volume>116</volume>:<fpage>2346</fpage>–<lpage>2355</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Puvvada KC"><surname>Puvvada</surname> <given-names>KC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-58">Cortical Representations of Speech in a Multi-talker Auditory Scene</article-title>. <source hwp:id="source-58">The Journal of Neuroscience</source> <volume>37</volume>:<fpage>9189</fpage>–<lpage>9196</lpage>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Pylyshyn ZW"><surname>Pylyshyn</surname> <given-names>ZW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Storm RW"><surname>Storm</surname> <given-names>RW</given-names></string-name> (<year>1988</year>) <article-title hwp:id="article-title-59">Tracking multiple independent targets: Evidence for a parallel tracking mechanism</article-title>. <source hwp:id="source-59">Spatial Vision</source> <volume>3</volume>:<fpage>179</fpage>–<lpage>197</lpage>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Rosenthal R"><surname>Rosenthal</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rubin DB"><surname>Rubin</surname> <given-names>DB</given-names></string-name> (<year>2003</year>) <article-title hwp:id="article-title-60">requivalent: A Simple Effect Size Indicator</article-title>. <source hwp:id="source-60">Psychological Methods</source> <volume>8</volume>:<fpage>492</fpage>–<lpage>496</lpage>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1 xref-ref-62-2"><citation publication-type="book" citation-type="book" ref:id="261271v2.62" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Scholl BJ"><surname>Scholl</surname> <given-names>BJ</given-names></string-name> (<year>2009</year>) <chapter-title>What have we learned about attention from multiple object tracking (and vice versa)?</chapter-title> <source hwp:id="source-61">In: Computation, cognition, and Pylyshyn</source> (<person-group hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Dedrick D"><surname>Dedrick</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Trick L"><surname>Trick</surname> <given-names>L</given-names></string-name></person-group>, eds), pp <fpage>49</fpage>–<lpage>78</lpage>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-61">Low-frequency neuronal oscillations as instruments of sensory selection</article-title>. <source hwp:id="source-62">Trends in Neurosciences</source> <volume>32</volume>:<fpage>9</fpage>–<lpage>18</lpage>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Schröger E"><surname>Schröger</surname> <given-names>E</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-62">Mismatch Negativity: A Microphone into Auditory Memory</article-title>. <source hwp:id="source-63">Journal of Psychophysiology</source> <volume>21</volume>:<fpage>138</fpage>–<lpage>146</lpage>.</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1 xref-ref-65-2 xref-ref-65-3 xref-ref-65-4 xref-ref-65-5 xref-ref-65-6 xref-ref-65-7 xref-ref-65-8 xref-ref-65-9 xref-ref-65-10"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Sohoglu E"><surname>Sohoglu</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-63">Detecting and representing predictable structure during auditory scene analysis</article-title>. <source hwp:id="source-64">eLife</source> <volume>5</volume>:<fpage>e19113</fpage>.</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1 xref-ref-66-2 xref-ref-66-3 xref-ref-66-4 xref-ref-66-5 xref-ref-66-6 xref-ref-66-7 xref-ref-66-8 xref-ref-66-9 xref-ref-66-10"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Southwell R"><surname>Southwell</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baumann A"><surname>Baumann</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gal C"><surname>Gal</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barascud N"><surname>Barascud</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Friston KJ"><surname>Friston</surname> <given-names>KJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-64">Is predictability salient? A study of attentional capture by auditory patterns</article-title>. <source hwp:id="source-65">Philosophical Transactions of the Royal Society B</source> <volume>372</volume>:<fpage>20160105</fpage>.</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Stefanics G"><surname>Stefanics</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hangya B"><surname>Hangya</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hernádi I"><surname>Hernádi</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winkler I"><surname>Winkler</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ulbert I"><surname>Ulbert</surname> <given-names>I</given-names></string-name> (<year>2010</year>) <article-title hwp:id="article-title-65">Phase Entrainment of Human Delta Oscillations Can Mediate the Effects of Expectation on Reaction Speed</article-title>. <source hwp:id="source-66">The Journal of Neuroscience</source> <volume>30</volume>:<fpage>13578</fpage>–<lpage>13585</lpage>.</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Studebaker GA"><surname>Studebaker</surname> <given-names>GA</given-names></string-name> (<year>1985</year>) <article-title hwp:id="article-title-66">A “rationalized” arcsine transform</article-title>. <source hwp:id="source-67">Journal of Speech and Hearing Research</source> <volume>28</volume>:<fpage>455</fpage>–<lpage>462</lpage>.</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1 xref-ref-69-2"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Sussman ES"><surname>Sussman</surname> <given-names>ES</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-67">Auditory Scene Analysis: An Attention Perspective</article-title>. <source hwp:id="source-68">Journal of Speech, Language, and Hearing Research</source> <volume>60</volume>:<fpage>2989</fpage>–<lpage>3000</lpage>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Sussman ES"><surname>Sussman</surname> <given-names>ES</given-names></string-name>, <string-name name-style="western" hwp:sortable="Horváth J"><surname>Horváth</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winkler I"><surname>Winkler</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Orr M"><surname>Orr</surname> <given-names>M</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-68">The role of attention in the formation of auditory streams</article-title>. <source hwp:id="source-69">Perception &amp; Psychophysics</source> <volume>69</volume>:<fpage>136</fpage>–<lpage>152</lpage>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1 xref-ref-71-2 xref-ref-71-3 xref-ref-71-4 xref-ref-71-5 xref-ref-71-6 xref-ref-71-7"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Teki S"><surname>Teki</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barascud N"><surname>Barascud</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Picard S"><surname>Picard</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Payne C"><surname>Payne</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Griffiths TD"><surname>Griffiths</surname> <given-names>TD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chait M"><surname>Chait</surname> <given-names>M</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-69">Neural Correlates of Auditory Figure-Ground Segregation Based on Temporal Coherence</article-title>. <source hwp:id="source-70">Cerebral Cortex</source> <volume>26</volume>:<fpage>3669</fpage>–<lpage>3680</lpage>.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="ten Oever S"><surname>ten Oever</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poeppel D"><surname>Poeppel</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Atteveldt N"><surname>van Atteveldt</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Metha AD"><surname>Metha</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Megevand P"><surname>Megevand</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Groppe DM"><surname>Groppe</surname> <given-names>DM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zion-Golumbic E"><surname>Zion-Golumbic</surname> <given-names>E</given-names></string-name> (<year>2017</year>) <article-title hwp:id="article-title-70">Low-Frequency Cortical Oscillations Entrain to Subthreshold Rhythmic Auditory Stimuli</article-title>. <source hwp:id="source-71">The Journal of Neuroscience</source> <volume>37</volume>:<fpage>4903</fpage>–<lpage>4912</lpage>.</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Tombu M"><surname>Tombu</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seiffert AE"><surname>Seiffert</surname> <given-names>AE</given-names></string-name> (<year>2008</year>) <article-title hwp:id="article-title-71">Attentional costs in multiple-object tracking</article-title>. <source hwp:id="source-72">Cognition</source> <volume>108</volume>:<fpage>1</fpage>–<lpage>25</lpage>.</citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.74" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Weise A"><surname>Weise</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schröger E"><surname>Schröger</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Horváth J"><surname>Horváth</surname> <given-names>J</given-names></string-name> (in press) <article-title hwp:id="article-title-72">The detection of higher-order acoustic transitions is reflected in the N1 ERP</article-title>. <source hwp:id="source-73">Psychophysiology</source>.</citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Whittingstall K"><surname>Whittingstall</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Logothetis NK"><surname>Logothetis</surname> <given-names>NK</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-73">Frequency-Band Coupling in Surface EEG Reflects Spiking Activity in Monkey Visual Cortex</article-title>. <source hwp:id="source-74">Neuron</source> <volume>64</volume>:<fpage>281</fpage>–<lpage>289</lpage>.</citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Wild C"><surname>Wild</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yusuf A"><surname>Yusuf</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wilson DE"><surname>Wilson</surname> <given-names>DE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peelle JE"><surname>Peelle</surname> <given-names>JE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Davis MH"><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnsrude IS"><surname>Johnsrude</surname> <given-names>IS</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-74">Effortful listening: the processing of degraded speech depends critically on attention</article-title>. <source hwp:id="source-75">The Journal of Neuroscience</source> <volume>32</volume>:<fpage>14010</fpage>–<lpage>14021</lpage>.</citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1 xref-ref-77-2 xref-ref-77-3"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Winkler I"><surname>Winkler</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Denham SL"><surname>Denham</surname> <given-names>SL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nelken I"><surname>Nelken</surname> <given-names>I</given-names></string-name> (<year>2009</year>) <article-title hwp:id="article-title-75">Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title>. <source hwp:id="source-76">Trends in Cognitive Sciences</source> <volume>13</volume>:<fpage>532</fpage>–<lpage>540</lpage>.</citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.78" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Xiang J"><surname>Xiang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Elhilali M"><surname>Elhilali</surname> <given-names>M</given-names></string-name> (<year>2010</year>) <article-title hwp:id="article-title-76">Competing Streams at the Cocktail Party: Exploring the Mechanisms of Attention and Temporal Integration</article-title>. <source hwp:id="source-77">The Journal of Neuroscience</source> <volume>30</volume>:<fpage>1208412093</fpage>.</citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><citation publication-type="journal" citation-type="journal" ref:id="261271v2.79" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Zion Golumbic EM"><surname>Zion Golumbic</surname> <given-names>EM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ding N"><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bickel S"><surname>Bickel</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lakatos P"><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schevon CA"><surname>Schevon</surname> <given-names>CA</given-names></string-name>, <string-name name-style="western" hwp:sortable="McKhann GM"><surname>McKhann</surname> <given-names>GM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goodman RR"><surname>Goodman</surname> <given-names>RR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Emerson R"><surname>Emerson</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mehta AD"><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon JZ"><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poeppel D"><surname>Poeppel</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schroeder CE"><surname>Schroeder</surname> <given-names>CE</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-77">Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party”</article-title>. <source hwp:id="source-78">Neuron</source> <volume>77</volume>:<fpage>980</fpage>–<lpage>991</lpage>.</citation></ref></ref-list></back></article>
