<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.06.03.446953</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.06.03.446953</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.06.03.446953</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.06.03.446953</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.06.03.446953</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Reverse-Complement Equivariant Networks for DNA Sequences</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label> Corresponding author; email: <email hwp:id="email-1">vincent.mallet96@gmail.com</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Mallet Vincent"><surname>Mallet</surname><given-names>Vincent</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Vert Jean-Philippe"><surname>Vert</surname><given-names>Jean-Philippe</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Structural Bioinformatics Unit, Department of Structural Biology and Chemistry, Institut Pasteur, CNRS UMR3528, C3BI, USR3756, Mines ParisTech, PSL University, Center for Computational Biology</institution>, <email hwp:id="email-2">vincent.mallet96@gmail.com</email></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Google Research, Brain team</institution>, Paris, <email hwp:id="email-3">jpvert@google.com</email></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-06-03T15:45:23-07:00">
    <day>3</day><month>6</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-10-23T00:51:14-07:00">
    <day>23</day><month>10</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-06-03T15:49:04-07:00">
    <day>3</day><month>6</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-10-23T00:57:18-07:00">
    <day>23</day><month>10</month><year>2021</year>
  </pub-date><elocation-id>2021.06.03.446953</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-06-03"><day>03</day><month>6</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-10-22"><day>22</day><month>10</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-10-22"><day>22</day><month>10</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by/4.0/</ext-link></p></license></permissions><self-uri xlink:href="446953.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.06.03.446953v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="446953.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.06.03.446953v2/2021.06.03.446953v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.06.03.446953v2/2021.06.03.446953v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">As DNA sequencing technologies keep improving in scale and cost, there is a growing need to develop machine learning models to analyze DNA sequences, e.g., to decipher regulatory signals from DNA fragments bound by a particular protein of interest. As a double helix made of two complementary strands, a DNA fragment can be sequenced as two equivalent, so-called <italic toggle="yes">Reverse Complement</italic> (RC) sequences of nucleotides. To take into account this inherent symmetry of the data in machine learning models can facilitate learning. In this sense, several authors have recently proposed particular RC-equivariant convolutional neural networks (CNNs). However, it remains unknown whether other RC-equivariant architectures exist, which could potentially increase the set of basic models adapted to DNA sequences for practitioners. Here, we close this gap by characterizing the set of all linear RC-equivariant layers, and show in particular that new architectures exist beyond the ones already explored. We further discuss RC-equivariant pointwise nonlinearities adapted to different architectures, as well as RC-equivariant embeddings of <italic toggle="yes">k</italic>-mers as an alternative to one-hot encoding of nucleotides. We show experimentally that the new architectures can outperform existing ones.</p></abstract><counts><page-count count="21"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-3">Introduction</title><p hwp:id="p-4">Incorporating prior knowledge about the structure of data in the architecture of neural networks is a promising approach to design expressive models with good generalization properties. In particular, exploiting natural symmetries in the data can lead to models with fewer parameters to estimate than agnostic approaches. This is especially beneficial when the amount of available data is limited. A famous example of such an architecture is the convolutional neural network (CNN) for 1D sequences or 2D images, which is well adapted to problems which are invariant to translations in the data, while exploiting multiscale and local information in the signals. Motivated by the success of CNNs, there has been a fast-growing body of research in recent years to build the theoretical underpinnings and design architectures and efficient algorithms to systematically exploit symmetries and structures in the data [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>].</p><p hwp:id="p-5">A central idea that has emerged is to formalize the symmetries in data by a particular <italic toggle="yes">group action</italic> (e.g., the group of translations or rotations on images), and to create multilayer neural networks which, by design, “behave well” under the action of the group. This is captured formally by the concept of <italic toggle="yes">equivariance</italic>, which states that each equivariant layer should be designed to be subject to the group action (e.g., we should be able to “translate” or “rotate” the signal in each layer), and that when an input data is transformed by a particular group element, then its representation in an equivariant layer should also be transformed according to the same group element. While it is easy to see that convolutional layers in CNNs are equivariant to translations, Cohen and Welling [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>] formalized the concept of group equivariance CNN (G-CNN) for more general groups and showed in particular how to design convolutional layers equivariant not only to translations but also to reflections and to a discrete set of rotations. Following this seminal work, the theoretical foundations of group equivariant neural networks were then expanded, going beyond regular representations [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>], for more groups [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>, <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>, <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>, <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>], in less regular spaces [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>, <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>] or with more general results on their generality and universality [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>, <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>, <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>, <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>]. The main applications were developed with the groups of rotations in 2D and 3D, mostly to computer vision problems, but also in biology with histopathology [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>, <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>], medicine [<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>] and quantum chemistry [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>].</p><p hwp:id="p-6">In this paper, we explore and study the potential benefits of equivariant architectures for an important class of data, namely deoxyribonucleic acid (DNA) sequences. DNA is the major form of genetic material in most organisms, from bacteria to mammals, which encodes in particular all proteins that a cell can produce and which is transmitted from generation to generation. The study of DNA in humans and various organisms has led to tremendous progress in biology and medicine since the 1970s, when the first DNA sequencing technologies were invented, and the collapsing cost of sequencing in the last twenty years has accelerated the production of DNA sequences: there are for example about 2.8 billion sequences for a total length of ~ 10<sup>13</sup> nucleotides publicly available at the European Nucleotide Archive (ENA<xref ref-type="fn" rid="fn1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1"><sup>1</sup></xref>). Unsurprisingly in such a data-rich field, machine learning-based approaches are increasingly used to analyze DNA sequences, e.g., in metagenomics to automatically predict the species present in an environment from randomly sequenced DNA fragments [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>, <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>, <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>, <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>] and to detect the presence of viral DNA in human samples [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">36</xref>], in functional genomics to predict the presence of protein binding sites or other regulatory elements in DNA sequences of interest [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>, <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>, <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>, <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>, <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>, <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>], to predict epigenetic modifications [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>], or to predict the effect of variations in the DNA sequence on a phenotype of interest [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>, <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>].</p><p hwp:id="p-7">Due to the sequential nature of DNA and the translation-equivariant nature of the questions addressed, many of these works are based on 1D CNN architectures, although recently transformer-based language models have also shown promising results on various tasks [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>, <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>, <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>]. However, besides translation, DNA has an additional fundamental symmetry that has been largely ignored so far: the so-called reverse complement (RC) symmetry, due to the fact that DNA is made of two strands oriented in opposite direction and encoding complementary nucleotides. In other words, a given DNA segment can be sequenced as two RC DNA sequences, depending on which strand is sequenced; any predictive model for, e.g., DNA sequence classification should therefore be RC-invariant, which calls for RC-equivariant architectures. While strategies based on data augmentation and prediction averaging has been commonly used to handle the need for RC invariance [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">1</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>], one translation- and RC-equivariant CNN architecture has been proposed and led to promising results [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>, <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>, <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>]. However, it remains unclear whether that architecture is the only one that can encode translation- and RC-equivariance, or if alternative models exist to complement the toolbox of users wishing to develop deep learning models for DNA sequences.</p><p hwp:id="p-8">Using the general theory of equivariant representations, in particular steerable CNNs [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>], we answer that question by characterizing the set of all linear translation- and RC-equivariant layers. We show in particular that new architectures exist beyond the ones already explored by [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">4</xref>, <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">29</xref>, <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">34</xref>], which in the language of equivariant CNNs only make use of the regular representation [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">7</xref>] while more general representations lead to different layers. We further discuss RC-equivariant pointwise nonlinearities adapted to different representations, as well as RC-equivariant embeddings of <italic toggle="yes">k</italic>-mers as an alternative to one-hot encoding of nucleotides. We test the new architecture on several protein binding prediction problems, and show experimentally that the new models can outperform existing ones, confirming the potential benefit of exploring the full set of RC-equivariant layers when manipulating DNA sequences with deep neural networks.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-4">Methods</title><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-5">Group action of translation and reverse complementarity on DNA sequence</title><p hwp:id="p-9">DNA is a long polymer made of two intertwined strands, forming the well-known double-helical structure. Each strand is a non-symmetric polymer that can be described as an oriented chain of four possible monomers called nucleotides and denoted respectively {<monospace>A, C, G, T</monospace>}. The two strands are oriented in opposite directions, and their nucleotides face each other to form hydrogen bonds. They interact at each position in a deterministic way because only two nucleotides pairings can happen: (<monospace>A,T</monospace>) and (<monospace>G,C</monospace>). Thus, given a nucleotide sequence on one strand, we can deduce the so-called RC sequence of its corresponding strand by complementing each nucleotide and reversing the order (<bold><xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>)</bold>. When a double-stranded DNA fragment is sequenced, the two strands are first separated and, typically, only one of them is randomly selected and is decrypted by the machine. This implies that any given DNA fragment can be equivalently described by two RC sequences of nucleotides. Moreover, several genomic learning tasks amount to a sequence annotation that does not depend on the strand. For example, a protein can bind a double-stranded DNA fragment, and both strands of the bound part can get sequenced. This motivates the search for equivariance to this RC-action for the prediction functions. Moreover, the sequencing often results in long sequences where the relevant parts of the sequence do not correlate with their position. The task of prediction over genomic sequences is thus largely translation equivariant, which explains why the community settled on the use of CNNs to train and predict on arbitrary length segments.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-10">Illustration of the reverse-complement symmetry. Both DNA strands get sequenced in opposite directions resulting in redundant information.</p></caption><graphic xlink:href="446953v2_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-11">To formalize mathematically the translation and RC operations on DNA sequences, we first encode the raw genetic sequence as a signal function in <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="446953v2_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>, as the one hot encoding of the nucleotide content for each integer position. Because of the finite length of this polymer, we assume that beyond a compact support this function takes a constant value of zero. The group <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="446953v2_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> of translations acts naturally on this encoding by <italic toggle="yes">T<sub>u</sub></italic>(<italic toggle="yes">f</italic>)(<italic toggle="yes">x</italic>) = <italic toggle="yes">f</italic>(<italic toggle="yes">x − u</italic>), for a translation <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="446953v2_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula>, and the RC operations amounts to the following : <italic toggle="yes">RC</italic>(<italic toggle="yes">f</italic>)(<italic toggle="yes">x</italic>) = <italic toggle="yes">σ</italic>(−1)[<italic toggle="yes">f</italic>(−<italic toggle="yes">x</italic>)], where <italic toggle="yes">σ</italic>(−1) is the 4 × 4 permutation matrix that exchanges complementary bases <monospace>A/T</monospace> and <monospace>C/G</monospace> (while we denote by <italic toggle="yes">σ</italic>(1) the 4 × 4 identity matrix). We notice that <italic toggle="yes">RC</italic> is a linear operation on <italic toggle="yes">F</italic><sub>0</sub> that satisfies <italic toggle="yes">RC</italic><sup>2</sup> = <italic toggle="yes">I</italic>, and thus that the RC operation is a group representation on <italic toggle="yes">F</italic><sub>0</sub> for the group <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="446953v2_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> endowed with multiplication.</p><p hwp:id="p-12">To jointly consider translations and RC actions, we naturally consider the semi-direct product group <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="446953v2_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula>. Elements <italic toggle="yes">g</italic> ∈ <italic toggle="yes">G</italic> can be written as <italic toggle="yes">g</italic> = <italic toggle="yes">ts</italic> with <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="446953v2_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> and the group <italic toggle="yes">G</italic> acts on <italic toggle="yes">F</italic><sub>0</sub> by the action <italic toggle="yes">π</italic><sub>0</sub> defined by:
<disp-formula hwp:id="disp-formula-1"><alternatives hwp:id="alternatives-7"><graphic xlink:href="446953v2_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives></disp-formula></p><p hwp:id="p-13">In other words, <italic toggle="yes">π</italic><sub>0</sub> is the representation of <italic toggle="yes">G</italic> on <italic toggle="yes">F</italic><sub>0</sub> induced by the representation <italic toggle="yes">σ</italic> of RC on <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="446953v2_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>].</p></sec><sec id="s2b" hwp:id="sec-4"><label>2.2</label><title hwp:id="title-6">Features spaces of equivariant layers</title><p hwp:id="p-14">Let us now describe the structure of intermediate layers of a neural network equivariant to translations and RC. Following the theory of steerable CNNs [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">9</xref>], we consider successive representations of the input DNA sequence in the following way:
<statement id="def1" hwp:id="statement-1" hwp:rev-id="xref-statement-1-1 xref-statement-1-2 xref-statement-1-3"><label>Definition 1.</label><p hwp:id="p-15"><italic toggle="yes">Given ρ a representation of</italic> <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="446953v2_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> <italic toggle="yes">on</italic> <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="446953v2_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> <italic toggle="yes">for some</italic> <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="446953v2_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula>, <italic toggle="yes">a ρ-feature space is the set of signals</italic> <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="446953v2_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> <italic toggle="yes">endowed with the G group action π, known as the representation induced by ρ</italic>:
<disp-formula id="eqn1" hwp:id="disp-formula-2" hwp:rev-id="xref-disp-formula-2-1 xref-disp-formula-2-2"><alternatives hwp:id="alternatives-13"><graphic xlink:href="446953v2_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives></disp-formula></p></statement></p><p hwp:id="p-16">With this definition, we see in particular that the one-hot encoding input layer maps the input DNA sequence to a <italic toggle="yes">σ</italic>-feature space, and that the dimension (i.e., number of channels in the language of deep learning) and group action of <italic toggle="yes">ρ</italic>-feature space are fully characterized by the representation <italic toggle="yes">ρ</italic>. Interestingly, the theory of linear group representations allows us to characterize more precisely <italic toggle="yes">all</italic> such representations:
<statement id="thm1" hwp:id="statement-2" hwp:rev-id="xref-statement-2-1 xref-statement-2-2 xref-statement-2-3 xref-statement-2-4 xref-statement-2-5"><label>Theorem 1.</label><p hwp:id="p-17"><italic toggle="yes">For any representation ρ of</italic> <inline-formula hwp:id="inline-formula-12"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="446953v2_inline12.gif" hwp:id="inline-graphic-12"/></alternatives></inline-formula> <italic toggle="yes">on</italic> <inline-formula hwp:id="inline-formula-13"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="446953v2_inline13.gif" hwp:id="inline-graphic-13"/></alternatives></inline-formula>, <italic toggle="yes">there exist</italic> <inline-formula hwp:id="inline-formula-14"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="446953v2_inline14.gif" hwp:id="inline-graphic-14"/></alternatives></inline-formula> <italic toggle="yes">such that a</italic> + <italic toggle="yes">b</italic> = <italic toggle="yes">D and an invertible matrix</italic> <inline-formula hwp:id="inline-formula-15"><alternatives hwp:id="alternatives-17"><inline-graphic xlink:href="446953v2_inline15.gif" hwp:id="inline-graphic-15"/></alternatives></inline-formula> <italic toggle="yes">such that</italic>
<disp-formula hwp:id="disp-formula-3"><alternatives hwp:id="alternatives-18"><graphic xlink:href="446953v2_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives></disp-formula></p></statement></p><p hwp:id="p-18">In other words, combining <xref ref-type="statement" rid="def1" hwp:id="xref-statement-1-1" hwp:rel-id="statement-1">Definition 1</xref> and <xref ref-type="statement" rid="thm1" hwp:id="xref-statement-2-1" hwp:rel-id="statement-2">Theorem 1</xref>, we see that any <italic toggle="yes">ρ</italic>-feature space that we will use to build translation- and RC-equivariant layers is fully characterized by a triplet (<italic toggle="yes">P, a, b</italic>), which we call its <italic toggle="yes">type</italic>, and which characterizes both its dimension <italic toggle="yes">D</italic> = <italic toggle="yes">a</italic> + <italic toggle="yes">b</italic> and the action of the group <italic toggle="yes">G</italic> by (<xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-1" hwp:rel-id="disp-formula-2">1</xref>). By slight abuse of language, we also refer to (<italic toggle="yes">P, a, b</italic>) as the type of <italic toggle="yes">ρ</italic>.</p><p hwp:id="p-19"><xref ref-type="statement" rid="thm1" hwp:id="xref-statement-2-2" hwp:rel-id="statement-2">Theorem 1</xref> is a standard result of group theory, which explicits the decomposition of any representation <italic toggle="yes">ρ</italic> in terms of so-called irreducible representation, or <italic toggle="yes">irreps</italic>. In the case of <inline-formula hwp:id="inline-formula-16"><alternatives hwp:id="alternatives-19"><inline-graphic xlink:href="446953v2_inline16.gif" hwp:id="inline-graphic-16"/></alternatives></inline-formula>, there are exactly two irreps which act on <inline-formula hwp:id="inline-formula-17"><alternatives hwp:id="alternatives-20"><inline-graphic xlink:href="446953v2_inline17.gif" hwp:id="inline-graphic-17"/></alternatives></inline-formula>, namely, <italic toggle="yes">ρ</italic><sub>1</sub>(<italic toggle="yes">s</italic>) = 1 and <italic toggle="yes">ρ</italic><sub>−1</sub>(<italic toggle="yes">s</italic>) = <italic toggle="yes">s</italic>. If <italic toggle="yes">ρ</italic> has type (<italic toggle="yes">P, a, b</italic>), then it means that it can be decomposed as <italic toggle="yes">a</italic> times <italic toggle="yes">ρ</italic><sub>1</sub>(<italic toggle="yes">s</italic>) and <italic toggle="yes">b</italic> times <italic toggle="yes">ρ</italic><sub>−1</sub>(<italic toggle="yes">s</italic>). In the particular case where <italic toggle="yes">P</italic> is the identity matrix, i.e., when we consider a type (<italic toggle="yes">I, a, b</italic>), then <italic toggle="yes">ρ</italic>(<italic toggle="yes">s</italic>) is a diagonal matrix for any <inline-formula hwp:id="inline-formula-18"><alternatives hwp:id="alternatives-21"><inline-graphic xlink:href="446953v2_inline18.gif" hwp:id="inline-graphic-18"/></alternatives></inline-formula>, and each channel of <italic toggle="yes">F</italic> is acted upon by a single irrep. In that case, we will call the channels of type “1” (resp. “-1”) if they are acted upon by <italic toggle="yes">ρ</italic><sub>1</sub>(resp. <italic toggle="yes">ρ</italic><sub>−1</sub>), and we will say that <italic toggle="yes">F</italic> is an “irrep feature space”.</p><p hwp:id="p-20">Now, let us introduce another special case. Since <inline-formula hwp:id="inline-formula-19"><alternatives hwp:id="alternatives-22"><inline-graphic xlink:href="446953v2_inline19.gif" hwp:id="inline-graphic-19"/></alternatives></inline-formula> is finite of cardinality 2, let us consider the <italic toggle="yes">regular representation ρ<sub>reg</sub></italic> of <inline-formula hwp:id="inline-formula-20"><alternatives hwp:id="alternatives-23"><inline-graphic xlink:href="446953v2_inline20.gif" hwp:id="inline-graphic-20"/></alternatives></inline-formula> on <inline-formula hwp:id="inline-formula-21"><alternatives hwp:id="alternatives-24"><inline-graphic xlink:href="446953v2_inline21.gif" hwp:id="inline-graphic-21"/></alternatives></inline-formula> defined by:
<disp-formula hwp:id="disp-formula-4"><alternatives hwp:id="alternatives-25"><graphic xlink:href="446953v2_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives></disp-formula></p><p hwp:id="p-21">One can easily check that <italic toggle="yes">ρ<sub>reg</sub></italic> is of type (<italic toggle="yes">P<sub>reg</sub></italic>, 1, 1), where <inline-formula hwp:id="inline-formula-22"><alternatives hwp:id="alternatives-26"><inline-graphic xlink:href="446953v2_inline22.gif" hwp:id="inline-graphic-22"/></alternatives></inline-formula>. It corresponds to a <italic toggle="yes">ρ</italic>-feature space with two channels, where the RC operations flips the two channels (and of course the sequence coordinates).</p><p hwp:id="p-22">Let us now consider feature spaces of interest. In the input layer, nucleotides are one-hot encoded in a certain order, let us say (<monospace>A, T, G, C</monospace>). As stated above, this input space is acted upon by <italic toggle="yes">σ</italic>, a 2–cycle that swaps bases <monospace>A/T</monospace> and <monospace>C/G</monospace>. We see that we can rewrite <inline-formula hwp:id="inline-formula-23"><alternatives hwp:id="alternatives-27"><inline-graphic xlink:href="446953v2_inline23.gif" hwp:id="inline-graphic-23"/></alternatives></inline-formula>, where ⊕ is the bloc-diagonal operation. Because <italic toggle="yes">ρ<sub>reg</sub></italic> is of type (<italic toggle="yes">P<sub>reg</sub></italic>, 1, 1), we can diagonalize <italic toggle="yes">σ</italic> with <inline-formula hwp:id="inline-formula-24"><alternatives hwp:id="alternatives-28"><inline-graphic xlink:href="446953v2_inline24.gif" hwp:id="inline-graphic-24"/></alternatives></inline-formula> and the diagonal would be alternated +1 and −1 values. Thus, there exists a permutation Π such that <italic toggle="yes">σ</italic> is of type (<italic toggle="yes">P</italic>, 2, 2), with <inline-formula hwp:id="inline-formula-25"><alternatives hwp:id="alternatives-29"><inline-graphic xlink:href="446953v2_inline25.gif" hwp:id="inline-graphic-25"/></alternatives></inline-formula>. These concepts are illustrated in Supplementary Section A.1.</p><p hwp:id="p-23">Interestingly, all RC-equivariant layers proposed so far in [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-3" hwp:rel-id="ref-4">4</xref>, <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-3" hwp:rel-id="ref-29">29</xref>, <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-3" hwp:rel-id="ref-34">34</xref>] follow a similar pattern: the channels go by pair, and the RC action amounts to flipping the channel values within a pair and reversing the sequence coordinates. In our formalism, this corresponds to channels of type (<italic toggle="yes">P, a, a</italic>), where <inline-formula hwp:id="inline-formula-26"><alternatives hwp:id="alternatives-30"><inline-graphic xlink:href="446953v2_inline26.gif" hwp:id="inline-graphic-26"/></alternatives></inline-formula> is the number of pairs of channels, and where up to a permutation of channels the matrix <italic toggle="yes">P</italic> satisfies <inline-formula hwp:id="inline-formula-27"><alternatives hwp:id="alternatives-31"><inline-graphic xlink:href="446953v2_inline27.gif" hwp:id="inline-graphic-27"/></alternatives></inline-formula>. Following [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-4" hwp:rel-id="ref-34">34</xref>], we will refer to these layers as <italic toggle="yes">Reverse Complement Parameter Sharing</italic> (RCPS) layers below.</p><p hwp:id="p-24">This highlights the fact that translation- and RC-equivariant layers explored so far are equivariant according to <xref ref-type="statement" rid="def1" hwp:id="xref-statement-1-2" hwp:rel-id="statement-1">Definition 1</xref>, but that there exists potentially many other equivariant layers, obtained in particular by allowing <italic toggle="yes">ρ</italic>-feature spaces of types (<italic toggle="yes">P, a, b</italic>) where <italic toggle="yes">a</italic> ≠ <italic toggle="yes">b</italic>, on the one hand, and where <italic toggle="yes">P</italic> is not a direct sum of <italic toggle="yes">P<sub>reg</sub></italic>, on the other hand. We investigate such variants below.</p></sec><sec id="s2c" hwp:id="sec-5"><label>2.3</label><title hwp:id="title-7">Equivariant linear layers</title><p hwp:id="p-25">While <xref ref-type="statement" rid="def1" hwp:id="xref-statement-1-3" hwp:rel-id="statement-1">Definition 1</xref> characterizes <italic toggle="yes">ρ</italic>-feature space in terms of structure and group action, an equivariant multilayer neural network is built by stacking <italic toggle="yes">ρ</italic>-feature spaces on top of each other and connecting them with equivariant layers. Cohen et al. [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref>, <xref ref-type="statement" rid="thm2" hwp:id="xref-statement-3-1" hwp:rel-id="statement-3">Theorem 2</xref>] gives us a general result about such equivariant mappings. Here, we apply this result to our specific data and group, and characterize the class of equivariant linear layers, i.e., the linear functions <italic toggle="yes">ϕ</italic> : <italic toggle="yes">F<sub>n</sub></italic> → <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> that satisfy <italic toggle="yes">π</italic><sub><italic toggle="yes">n</italic>+1</sub><italic toggle="yes">ϕ</italic> = <italic toggle="yes">ϕπ<sub>n</sub></italic>, where <italic toggle="yes">π<sub>n</sub></italic> and <italic toggle="yes">π</italic><sub><italic toggle="yes">n</italic>+1</sub> are respectively the group action on <italic toggle="yes">F<sub>n</sub></italic> and <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub>.</p><statement id="thm2" hwp:id="statement-3" hwp:rev-id="xref-statement-3-1 xref-statement-3-2 xref-statement-3-3 xref-statement-3-4 xref-statement-3-5"><label>Theorem 2.</label><p hwp:id="p-26"><italic toggle="yes">Given two representations ρ<sub>n</sub> and ρ</italic><sub><italic toggle="yes">n</italic>+1</sub> <italic toggle="yes">of</italic> <inline-formula hwp:id="inline-formula-28"><alternatives hwp:id="alternatives-32"><inline-graphic xlink:href="446953v2_inline28.gif" hwp:id="inline-graphic-28"/></alternatives></inline-formula>, <italic toggle="yes">of respective types</italic> (<italic toggle="yes">P<sub>n</sub></italic>, <italic toggle="yes">a<sub>n</sub>, b<sub>n</sub></italic>) <italic toggle="yes">and</italic> (<italic toggle="yes">P</italic><sub><italic toggle="yes">n</italic>+1</sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">n</italic>+1</sub>, <italic toggle="yes">b</italic><sub><italic toggle="yes">n</italic>+1</sub>) <italic toggle="yes">with a<sub>n</sub></italic> + <italic toggle="yes">b<sub>n</sub></italic> = <italic toggle="yes">D<sub>n</sub> and a</italic><sub><italic toggle="yes">n</italic>+1</sub> + <italic toggle="yes">b</italic><sub><italic toggle="yes">n</italic>+1</sub> = <italic toggle="yes">D</italic><sub><italic toggle="yes">n</italic>+1</sub>, <italic toggle="yes">and respective ρ<sub>n<sup>-</sup></sub> and ρ</italic><sub><italic toggle="yes">n</italic>+1<sup>-</sup></sub> <italic toggle="yes">feature spaces F<sub>n</sub> and F</italic><sub><italic toggle="yes">n</italic>+1</sub>, <italic toggle="yes">a linear map ϕ</italic> : <italic toggle="yes">F<sub>n</sub></italic> → <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> <italic toggle="yes">is equivariant if and only if it can be written as a convolution</italic>:
<disp-formula id="eqn2" hwp:id="disp-formula-5" hwp:rev-id="xref-disp-formula-5-1 xref-disp-formula-5-2 xref-disp-formula-5-3 xref-disp-formula-5-4"><alternatives hwp:id="alternatives-33"><graphic xlink:href="446953v2_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives></disp-formula>
<italic toggle="yes">where the kernel</italic> <inline-formula hwp:id="inline-formula-29"><alternatives hwp:id="alternatives-34"><inline-graphic xlink:href="446953v2_inline29.gif" hwp:id="inline-graphic-29"/></alternatives></inline-formula> <italic toggle="yes">satisfies</italic>:
<disp-formula id="eqn3" hwp:id="disp-formula-6" hwp:rev-id="xref-disp-formula-6-1 xref-disp-formula-6-2 xref-disp-formula-6-3 xref-disp-formula-6-4 xref-disp-formula-6-5 xref-disp-formula-6-6 xref-disp-formula-6-7 xref-disp-formula-6-8 xref-disp-formula-6-9 xref-disp-formula-6-10 xref-disp-formula-6-11"><alternatives hwp:id="alternatives-35"><graphic xlink:href="446953v2_eqn3.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives></disp-formula>
<italic toggle="yes">or equivalently</italic>:
<disp-formula id="eqn4" hwp:id="disp-formula-7" hwp:rev-id="xref-disp-formula-7-1 xref-disp-formula-7-2 xref-disp-formula-7-3 xref-disp-formula-7-4 xref-disp-formula-7-5 xref-disp-formula-7-6 xref-disp-formula-7-7"><alternatives hwp:id="alternatives-36"><graphic xlink:href="446953v2_eqn4.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives></disp-formula>
<italic toggle="yes">where</italic> <inline-formula hwp:id="inline-formula-30"><alternatives hwp:id="alternatives-37"><inline-graphic xlink:href="446953v2_inline30.gif" hwp:id="inline-graphic-30"/></alternatives></inline-formula> <italic toggle="yes">and</italic> <inline-formula hwp:id="inline-formula-31"><alternatives hwp:id="alternatives-38"><inline-graphic xlink:href="446953v2_inline31.gif" hwp:id="inline-graphic-31"/></alternatives></inline-formula> <italic toggle="yes">are even, while</italic> <inline-formula hwp:id="inline-formula-32"><alternatives hwp:id="alternatives-39"><inline-graphic xlink:href="446953v2_inline32.gif" hwp:id="inline-graphic-32"/></alternatives></inline-formula> <italic toggle="yes">and</italic> <inline-formula hwp:id="inline-formula-33"><alternatives hwp:id="alternatives-40"><inline-graphic xlink:href="446953v2_inline33.gif" hwp:id="inline-graphic-33"/></alternatives></inline-formula> <italic toggle="yes">are odd functions</italic>.</p></statement><p hwp:id="p-27">As stated in Cohen et al. [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-3" hwp:rel-id="ref-11">11</xref>], “Convolution is all you need” to define linear layers which are equivariant to our group. In addition, <xref ref-type="statement" rid="thm2" hwp:id="xref-statement-3-2" hwp:rel-id="statement-3">Theorem 2</xref> characterizes all the convolution kernels that ensure equivariance through the two equivalent constraints (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-1" hwp:rel-id="disp-formula-6">3</xref>) and (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-1" hwp:rel-id="disp-formula-7">4</xref>).</p><p hwp:id="p-28">To illustrate this result, let us consider two RCPS feature spaces <italic toggle="yes">F<sub>n</sub></italic> and <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> of respective types <inline-formula hwp:id="inline-formula-34"><alternatives hwp:id="alternatives-41"><inline-graphic xlink:href="446953v2_inline34.gif" hwp:id="inline-graphic-34"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-35"><alternatives hwp:id="alternatives-42"><inline-graphic xlink:href="446953v2_inline35.gif" hwp:id="inline-graphic-35"/></alternatives></inline-formula>. Then, the channels in <italic toggle="yes">F<sub>n</sub></italic> and <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> go by pair, and if we consider a slice <inline-formula hwp:id="inline-formula-36"><alternatives hwp:id="alternatives-43"><inline-graphic xlink:href="446953v2_inline36.gif" hwp:id="inline-graphic-36"/></alternatives></inline-formula> of the convolution kernel <italic toggle="yes">κ</italic> describing how a pair of channels in <italic toggle="yes">F<sub>n</sub></italic> maps to a pair of channels in <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub>, (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-2" hwp:rel-id="disp-formula-6">3</xref>) gives the constraint:
<disp-formula hwp:id="disp-formula-8"><alternatives hwp:id="alternatives-44"><graphic xlink:href="446953v2_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives></disp-formula></p><p hwp:id="p-29">We recover exactly the constraints of the RCPS filters first proposed by [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-5" hwp:rel-id="ref-34">34</xref>], proving as a consequence of <xref ref-type="statement" rid="thm2" hwp:id="xref-statement-3-3" hwp:rel-id="statement-3">Theorem 2</xref> that RCPS convolution filters describe exactly <italic toggle="yes">all</italic> equivariant linear mappings between RCPS feature spaces.</p><p hwp:id="p-30">Moreover, if we now consider any two feature spaces <italic toggle="yes">F<sub>n</sub></italic> and <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> of respective types (<italic toggle="yes">P<sub>n</sub>, a<sub>n</sub>, b<sub>n</sub></italic>) and (<italic toggle="yes">P</italic><sub><italic toggle="yes">n</italic>+1</sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">n</italic>+1</sub>, <italic toggle="yes">b</italic><sub><italic toggle="yes">n</italic>+1</sub>), then <xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-2" hwp:rel-id="disp-formula-7">Equation (4)</xref> tells us that up to multiplications by matrices <italic toggle="yes">P</italic><sub><italic toggle="yes">n</italic>+1</sub> and <inline-formula hwp:id="inline-formula-37"><alternatives hwp:id="alternatives-45"><inline-graphic xlink:href="446953v2_inline37.gif" hwp:id="inline-graphic-37"/></alternatives></inline-formula>, the kernel is expressed in terms of even and odd functions, which can be trivially implemented with parameter sharing. For example, to represent the even function <italic toggle="yes">α</italic>, one just need to parameterize the values of <italic toggle="yes">α</italic>(<italic toggle="yes">x</italic>) for <italic toggle="yes">x</italic> ≥ 0, and complete the negative values by parameter sharing <italic toggle="yes">α</italic>(−<italic toggle="yes">x</italic>) = <italic toggle="yes">α</italic>(<italic toggle="yes">x</italic>). Hence, the parameter sharing idea used in RCPS [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-6" hwp:rel-id="ref-34">34</xref>] extends to any equivariant linear map.</p><p hwp:id="p-31">Instead of using (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-3" hwp:rel-id="disp-formula-7">4</xref>) to parameterize equivariant convolution kernels, one may also directly write the constraints (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-3" hwp:rel-id="disp-formula-6">3</xref>) for specific representations, and potentially save the need of multiplication by <italic toggle="yes">P</italic><sub><italic toggle="yes">n</italic>+1</sub> and <inline-formula hwp:id="inline-formula-38"><alternatives hwp:id="alternatives-46"><inline-graphic xlink:href="446953v2_inline38.gif" hwp:id="inline-graphic-38"/></alternatives></inline-formula> in (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-4" hwp:rel-id="disp-formula-7">4</xref>). This is for example the case in RCPS layers [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-7" hwp:rel-id="ref-34">34</xref>], and more generally for channels acted upon by the regular representation; for the sake of completeness, we derive in <xref ref-type="app" rid="app1d" hwp:id="xref-sec-24-1" hwp:rel-id="sec-24">Appendix A.4</xref> the constraints to go from and to the regular representation or the irreps, and use them in our implementation.</p></sec><sec id="s2d" hwp:id="sec-6"><label>2.4</label><title hwp:id="title-8">Equivariant nonlinear layers</title><p hwp:id="p-32">Besides equivariant linear layers, a crucial component needed for multilayer neural networks is the possibility to have equivariant nonlinear layers, such as nonlinear pointwise activation functions or batch normalization [<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]. In this section, we discuss particular nonlinearities that are adapted to various equivariant layers.</p><sec id="s2d1" hwp:id="sec-7"><title hwp:id="title-9">Pointwise activations</title><p hwp:id="p-33">Let us begin with pointwise transformations, that encompass most activation functions used in deep learning. Pointwise transformations are formally defined as follows: given a function <inline-formula hwp:id="inline-formula-39"><alternatives hwp:id="alternatives-47"><inline-graphic xlink:href="446953v2_inline39.gif" hwp:id="inline-graphic-39"/></alternatives></inline-formula> and a vector space <inline-formula hwp:id="inline-formula-40"><alternatives hwp:id="alternatives-48"><inline-graphic xlink:href="446953v2_inline40.gif" hwp:id="inline-graphic-40"/></alternatives></inline-formula> for some index set <italic toggle="yes">A</italic>, the pointwise extension of <italic toggle="yes">θ</italic> to <italic toggle="yes">V</italic> is the mapping <inline-formula hwp:id="inline-formula-41"><alternatives hwp:id="alternatives-49"><inline-graphic xlink:href="446953v2_inline41.gif" hwp:id="inline-graphic-41"/></alternatives></inline-formula> defined by <inline-formula hwp:id="inline-formula-42"><alternatives hwp:id="alternatives-50"><inline-graphic xlink:href="446953v2_inline42.gif" hwp:id="inline-graphic-42"/></alternatives></inline-formula>, for any (<italic toggle="yes">f, a</italic>) ∈ <italic toggle="yes">V</italic> × <italic toggle="yes">A</italic>. For a <italic toggle="yes">D</italic>-dimensional representation <italic toggle="yes">ρ</italic> of <inline-formula hwp:id="inline-formula-43"><alternatives hwp:id="alternatives-51"><inline-graphic xlink:href="446953v2_inline43.gif" hwp:id="inline-graphic-43"/></alternatives></inline-formula> and a <italic toggle="yes">ρ</italic>-feature space <italic toggle="yes">F</italic> with <italic toggle="yes">G</italic>-group action <italic toggle="yes">π</italic>, we say that a pointwise extension <inline-formula hwp:id="inline-formula-44"><alternatives hwp:id="alternatives-52"><inline-graphic xlink:href="446953v2_inline44.gif" hwp:id="inline-graphic-44"/></alternatives></inline-formula> is equivariant if it commutes with <italic toggle="yes">π</italic>, i.e., <inline-formula hwp:id="inline-formula-45"><alternatives hwp:id="alternatives-53"><inline-graphic xlink:href="446953v2_inline45.gif" hwp:id="inline-graphic-45"/></alternatives></inline-formula>. By definition of the group action (<xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-2-2" hwp:rel-id="disp-formula-2">1</xref>), this is equivalent to saying that the pointwise extension <inline-formula hwp:id="inline-formula-46"><alternatives hwp:id="alternatives-54"><inline-graphic xlink:href="446953v2_inline46.gif" hwp:id="inline-graphic-46"/></alternatives></inline-formula> of <italic toggle="yes">θ</italic> to <inline-formula hwp:id="inline-formula-47"><alternatives hwp:id="alternatives-55"><inline-graphic xlink:href="446953v2_inline47.gif" hwp:id="inline-graphic-47"/></alternatives></inline-formula> commutes with <italic toggle="yes">ρ</italic>. The following theorem gives an exhaustive characterization of a large class of equivariant pointwise extensions for any <italic toggle="yes">ρ</italic>-feature space:
<statement id="thm3" hwp:id="statement-4" hwp:rev-id="xref-statement-4-1 xref-statement-4-2 xref-statement-4-3 xref-statement-4-4 xref-statement-4-5 xref-statement-4-6 xref-statement-4-7 xref-statement-4-8 xref-statement-4-9 xref-statement-4-10 xref-statement-4-11 xref-statement-4-12 xref-statement-4-13"><label>Theorem 3.</label><p hwp:id="p-34"><italic toggle="yes">Let ρ be a representation of</italic> <inline-formula hwp:id="inline-formula-48"><alternatives hwp:id="alternatives-56"><inline-graphic xlink:href="446953v2_inline48.gif" hwp:id="inline-graphic-48"/></alternatives></inline-formula> <italic toggle="yes">and</italic> <inline-formula hwp:id="inline-formula-49"><alternatives hwp:id="alternatives-57"><inline-graphic xlink:href="446953v2_inline49.gif" hwp:id="inline-graphic-49"/></alternatives></inline-formula> <italic toggle="yes">be a continuous function with left and right derivatives at</italic> 0. <italic toggle="yes">Let F be a ρ-layer and</italic> <inline-formula hwp:id="inline-formula-50"><alternatives hwp:id="alternatives-58"><inline-graphic xlink:href="446953v2_inline50.gif" hwp:id="inline-graphic-50"/></alternatives></inline-formula> <italic toggle="yes">be the point-wise extension of θ on this layer. Then</italic> <inline-formula hwp:id="inline-formula-51"><alternatives hwp:id="alternatives-59"><inline-graphic xlink:href="446953v2_inline51.gif" hwp:id="inline-graphic-51"/></alternatives></inline-formula> <italic toggle="yes">is equivariant if and only if at least one of the following cases holds</italic>:
<list list-type="order" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-35"><italic toggle="yes">θ is a linear function</italic>.</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-36"><italic toggle="yes">θ is an affine function, and ρ</italic>(−1)1 = 1.</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-37"><italic toggle="yes">θ is not an affine function, and there exists a permutation matrix</italic> Π, <italic toggle="yes">integers</italic> <inline-formula hwp:id="inline-formula-52"><alternatives hwp:id="alternatives-60"><inline-graphic xlink:href="446953v2_inline52.gif" hwp:id="inline-graphic-52"/></alternatives></inline-formula>, <italic toggle="yes">and scalars</italic> <inline-formula hwp:id="inline-formula-53"><alternatives hwp:id="alternatives-61"><inline-graphic xlink:href="446953v2_inline53.gif" hwp:id="inline-graphic-53"/></alternatives></inline-formula>, <italic toggle="yes">such that ρ decomposes as</italic>
<disp-formula id="eqn5" hwp:id="disp-formula-9" hwp:rev-id="xref-disp-formula-9-1 xref-disp-formula-9-2 xref-disp-formula-9-3 xref-disp-formula-9-4 xref-disp-formula-9-5 xref-disp-formula-9-6 xref-disp-formula-9-7"><alternatives hwp:id="alternatives-62"><graphic xlink:href="446953v2_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives></disp-formula></p><p hwp:id="p-38"><italic toggle="yes">In that case</italic>,</p><list list-type="bullet" hwp:id="list-2"><list-item hwp:id="list-item-4"><p hwp:id="p-39"><italic toggle="yes">Either b</italic> = <italic toggle="yes">d</italic> = 0 <italic toggle="yes">and</italic> ∀<italic toggle="yes">i</italic>, λ<sub><italic toggle="yes">i</italic></sub> = 1 <italic toggle="yes">and θ is any function</italic>,</p></list-item><list-item hwp:id="list-item-5"><p hwp:id="p-40"><italic toggle="yes">Or b</italic> = <italic toggle="yes">d</italic> = 0 <italic toggle="yes">and</italic> ∃<italic toggle="yes">i</italic>, λ<sub><italic toggle="yes">i</italic></sub> ≠ 1 <italic toggle="yes">and θ is a leaky ReLu function</italic>.<xref ref-type="fn" rid="fn2" hwp:id="xref-fn-2-1" hwp:rel-id="fn-2"><sup>2</sup></xref></p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-41"><italic toggle="yes">Or b</italic> + <italic toggle="yes">d</italic> &gt; 0 <italic toggle="yes">and</italic> ∀<italic toggle="yes">i</italic>, λ<sub><italic toggle="yes">i</italic></sub> = 1 <italic toggle="yes">and θ is an odd function</italic>,</p></list-item></list></list-item></list></p></statement></p><p hwp:id="p-42">The first case in <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-1" hwp:rel-id="statement-4">Theorem 3</xref> is of little interest, since pointwise linear functions are always equivariant to linear group actions. The second case essentially says that adding a constant to a pointwise linear function is only equivariant for representations <italic toggle="yes">ρ</italic> such that the sum of all rows of <italic toggle="yes">ρ</italic>(−1) is equal to 1. This holds for example for the regular representation and the RCPS layers, but not for an irrep feature space of type (<italic toggle="yes">I, a, b</italic>) with <italic toggle="yes">b</italic> &gt; 0, since in that case, some rows have a single “-1” entry. The most interesting case is the third one, since it describes what pointwise nonlinearities one can apply. The condition (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-1" hwp:rel-id="disp-formula-9">5</xref>) on the decomposition of <italic toggle="yes">ρ</italic> essentially excludes all representations that have more than one nonzero value in at least one row of <italic toggle="yes">ρ</italic>(−1). Among valid <italic toggle="yes">ρ</italic>’s that decompose as (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-2" hwp:rel-id="disp-formula-9">5</xref>), we see that the regular representation (corresponding to the first block in (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-3" hwp:rel-id="disp-formula-9">5</xref>) with λ<sub><italic toggle="yes">i</italic></sub> = 1)), used in RCPS, stands out as the only that allows <italic toggle="yes">any</italic> nonlinearity, besides of course invariant channels of type “+1” (third block in (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-4" hwp:rel-id="disp-formula-9">5</xref>)). Replacing a “1” in the regular representation by a scalar λ<sub><italic toggle="yes">i</italic></sub> ≠ 1 (in the first block of (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-5" hwp:rel-id="disp-formula-9">5</xref>), with <italic toggle="yes">b</italic> = <italic toggle="yes">d</italic> = 0) creates a valid representation <italic toggle="yes">ρ</italic>, however only leaky ReLu pointwise nonlinearities can be applied in that case. Another case of practical interest is the irrep feature space of type (<italic toggle="yes">I, c, d</italic>) for some <italic toggle="yes">c</italic> &gt; 0 and <italic toggle="yes">d</italic> &gt; 0. By <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-2" hwp:rel-id="statement-4">Theorem 3</xref>, only odd nonlinearities are allowed in that case, such as the hyperbolic tangent function. Finally, one should keep in mind that other representations, which do not satisfy the conditions listed in <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-3" hwp:rel-id="statement-4">Theorem 3</xref>, do not allow any equivariant nonlinear pointwise transform; this is for example the case of <inline-formula hwp:id="inline-formula-54"><alternatives hwp:id="alternatives-63"><inline-graphic xlink:href="446953v2_inline54.gif" hwp:id="inline-graphic-54"/></alternatives></inline-formula>, which is a valid representation of <inline-formula hwp:id="inline-formula-55"><alternatives hwp:id="alternatives-64"><inline-graphic xlink:href="446953v2_inline55.gif" hwp:id="inline-graphic-55"/></alternatives></inline-formula> but does neither meet the condition to accept affine activations (case 2), nor to accept nonlinear activations (case 3) because <italic toggle="yes">ρ</italic>(−1) does not decompose according to (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-6" hwp:rel-id="disp-formula-9">5</xref>).</p></sec><sec id="s2d2" hwp:id="sec-8"><title hwp:id="title-10">Other activation functions</title><p hwp:id="p-43">Besides pointwise transformations from a <italic toggle="yes">ρ</italic>-feature space to itself characterized in <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-4" hwp:rel-id="statement-4">Theorem 3</xref>, the set of nonlinear equivariant layer is tremendous and the design choices are endless. A first extension is to keep pointwise activation, but to allow different nonlinearities on different channels, e.g., by using any function on the “+1” channels and an odd function on the“-1” channels of an irrep feature space. Another relaxation is to use different input and output representations. While odd functions will not affect the field type, even functions will turn a field of type “-1” into a “+1” type. It is well known that any function decomposes into a sum of an odd and even function. Therefore, given <italic toggle="yes">ρ</italic>, a representation decomposed as in (<xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-9-7" hwp:rel-id="disp-formula-9">5</xref>), any point-wise non-linearity can be used in a <italic toggle="yes">ρ</italic>-feature space by first decomposing it into its odd and even components and applying each component separately for the second and fourth blocks.</p><p hwp:id="p-44">Other possibilities exist and include creating new representations by tensorization, which amounts to taking pointwise products between different channels [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref>, <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>, <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">37</xref>]. or using non point-wise activation layers, that act on several coupled dimensions, such as the ones used in [<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-3" hwp:rel-id="ref-37">37</xref>]. For instance, we could apply the max function to paired channels. These possibilities are discussed in [<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>]</p></sec><sec id="s2d3" hwp:id="sec-9"><title hwp:id="title-11">Batch normalization</title><p hwp:id="p-45">An equivariant batch normalization was introduced by [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-8" hwp:rel-id="ref-34">34</xref>]. It considers a feature map and its reverse complement as two instances, which is easy to do because the reverse complement feature map is already computed when using regular representation. We propose another batch normalization for irrep feature spaces that also gives the result we would have had if the batch contained all the reverse complement of its sequences. For the “+1” dimensions, it amounts to scaling as we would have the same values twice. For the “-1” dimensions, we enforce a zero mean and compute a variance estimate based on this constraint.</p></sec><sec id="s2d4" hwp:id="sec-10"><title hwp:id="title-12">K-mers</title><p hwp:id="p-46">Instead of the standard one-hot encoding of individual nucleotides as input layer, we propose to one-hot encode <italic toggle="yes">k</italic>-mers for <italic toggle="yes">k</italic> ≥ 1, i.e., overlapping blocks of <italic toggle="yes">k</italic> consecutive nucleotides. This technique is known to improve performance in several tasks [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>, <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref>]. In order to implement it into an equivariant network, we need to know how the group acts on the <italic toggle="yes">k</italic>-mers space, made of 4<sup><italic toggle="yes">k</italic></sup> elements. The simplest idea is to pair the index of the channels of two RC <italic toggle="yes">k</italic>-mers. Because some <italic toggle="yes">k</italic>-mers are their own reverse complement, the canonical way to do so is to have a representation that is a blend of “+1” irrep and regular representation. An alternative is to make the regular representation act on the <italic toggle="yes">k</italic>-mers instead by redundantly encoding these <italic toggle="yes">k</italic>-mers into paired dimensions. This is the strategy we follow in our implementation, to be more coherent with the usual input group action.</p></sec></sec></sec><sec id="s3" hwp:id="sec-11"><label>3</label><title hwp:id="title-13">Experiments</title><p hwp:id="p-47">We assess the performance of various equivariant architectures on a set of three binary prediction and four sequence prediction problems used by Zhou et al. [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>] to assess the performance of RCPS networks. The binary classification problems aim to predict if a DNA sequence binds to three transcription factors (TFs), based on genome-wide binarized TF-ChIP-seq data for Max, Ctcf and Spi1 in the GM12878 lymphoblastoid cell-line [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-9" hwp:rel-id="ref-34">34</xref>]. The sequence prediction problems aim to predict TF binding at the base-pair resolution, using genome-wide ChIP-nexus profiles of four TFs-Oct4, Sox2, Nanog and Klf4 in mouse embryonic stem cells. For a more detailed explanation of the experimental setup, please refer to Zhou et al. [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">45</xref>]. We report “significant” differences in performance below when the P-value of a Wilcoxon signed rank test is smaller than 0.05.</p><sec id="s3a" hwp:id="sec-12"><title hwp:id="title-14">Models</title><p hwp:id="p-48">We build over the work of Zhou et al. [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">45</xref>] for both the binary and the sequence prediction problems. They benchmarked an equivariant RCPS architecture and a corresponding non-equivariant model, with the same number of filters and trained with data augmentation, which we respectively refer to as “RCPS” and “Standard” models below. The data augmentation scheme for the “Standard” model consists in adding to the training set the reverse complement sequences of all training sequences, which is a natural procedure to let the model “learn” the equivariance without encoding it explicitly in the architecture of the network. We checked empirically that data augmentation significantly improves the performance of non-equivariant models (<xref ref-type="app" rid="app1f1" hwp:id="xref-sec-27-1" hwp:rel-id="sec-27">Appendix A.6.1</xref>). In addition, we extend the RCPS architecture with one-hot encoding of <italic toggle="yes">k</italic>-mers as input layers, which we refer to as “Regular” below. Finally, we add to the comparison a new equivariant network where each RCPS layer is replaced by an (<italic toggle="yes">I, a, b</italic>) layer with the same number of filters, which we call “Irrep” below. We also use <italic toggle="yes">k</italic>-mers and vary the ratio <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic> + <italic toggle="yes">b</italic>) in this model. We combine the regular and “+1” dimensions with <italic toggle="yes">ReLu</italic> activations and the “-1” dimensions with a <italic toggle="yes">tanh</italic> activation.</p></sec><sec id="s3b" hwp:id="sec-13"><title hwp:id="title-15">Influence of hyperparameters in equivariant models</title><p hwp:id="p-49">To assess the impact of different hyperparameters in the family of equivariant models we propose (<italic toggle="yes">k</italic>-mer length for Irrep and Regular, <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic> + <italic toggle="yes">b</italic>) ratio for Irrep), we train equivariant models with different combinations of hyperparameters on the training set and assess their performance on the validation set, repeating the process ten times with different random seeds. We assess the performance of each run in terms of Area under the Receiver Operator Characteristic (AuROC), and show in <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref> the average performance reached by all runs with a given ratio <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic> + <italic toggle="yes">b</italic>) ∈ {0, 1/4, 1/2, 3/4, 1} (left) and with a given <italic toggle="yes">k</italic> ∈ {1, 2, 3, 4} (right). We see a clear asymmetry in the performance as a function of <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic> + <italic toggle="yes">b</italic>), with poor performance when <italic toggle="yes">a</italic> = 0 and optimal performance for <italic toggle="yes">a</italic> = 0.75, significantly better than all other ratios tested. This confirms that exploring different irreps may be valuable. As for the <italic toggle="yes">k</italic>-mer length, setting <italic toggle="yes">k</italic> = 3 gives the best performance and significantly outperforms all other values of <italic toggle="yes">k</italic> tested. This confirms that going beyond one-hot encoding of nucleotides in equivariant architectures can be beneficial.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-50">Average AuROC performance across four TFs and 10 random seeds for the Irrep model as a function of <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic>, + <italic toggle="yes">b</italic>) (<italic toggle="yes">left</italic>, also averaged over <italic toggle="yes">k</italic> values) and for the Irrep and Regular models as a function of <italic toggle="yes">k</italic> (<italic toggle="yes">right</italic>, also averaged over <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic> + <italic toggle="yes">b</italic>) values for Irrep).</p></caption><graphic xlink:href="446953v2_fig2" position="float" orientation="portrait" hwp:id="graphic-11"/></fig></sec><sec id="s3c" hwp:id="sec-14"><title hwp:id="title-16">Binary task</title><p hwp:id="p-51">We then compare the test set performance of three different models for the binary classification task: 1) Standard, 2) RCPS, and 3) the best Irrep or Regular equivariant model, where hyperparameters are selected based on the AuROC on the validation set, which we denote as “Best Equivariant”. <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> (left) shows the performance of each model on each TF task and overall. As already observed by [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-10" hwp:rel-id="ref-34">34</xref>], the equivariant RCPS architecture has a strong lead over the Standard, non-equivariant model in spite of data augmentation. Interestingly, we see that Best Equivariant is significantly better than RCPS on all tasks, and that the performance gain from RCPS to Best equivariant is of the same order as the performance gain from Standard to RCPS. This demonstrates that the family of equivariant architectures we introduce in this paper can lead to significant improvement over existing architectures.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-52">AuROC performance of the three different models (Standard, RCPS and Best equivariant after hyperparameter selection on the validation set) on the three binary classification problems CTCF, MAX and SPI1, as well as their average. Error bars correspond to an estimate of the standard error on 10 repeats with different random seeds. The left plot is the performance on the full datasets, while the right plot shows the performance where models are trained on a subset of 1,000 sequences only (notice the differences of AuROC values on the vertical axis in both plots).</p></caption><graphic xlink:href="446953v2_fig3" position="float" orientation="portrait" hwp:id="graphic-12"/></fig></sec><sec id="s3d" hwp:id="sec-15"><title hwp:id="title-17">Reduced models</title><p hwp:id="p-53">Since equivariant architectures are meant to be particularly beneficial in the low-data regime [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>], we further assess the performance of the three models on the same binary classification problems but with only 1,000 sequences used to train the models, and show the results on <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3</xref> (right). Overall, the performances are worse than in the full-data regime (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3</xref>, left), which confirms that this is a regime where more data helps. We also see that the relative order of the three different methods remains overall the same, with Best Equivariant outperforming RCPS, which itself outperforms Standard. Interestingly, the gaps between the best and worse models widens in the low-data regime, showing that the prior is more useful in this setting. More precisely, there is a large gap of about 1% between Best Equivariant and Standard in the low data regime, compared to a gap of about 0.3% on the full dataset. We also investigated whether equivariant models converge faster to their solutions, but found not noticeable difference (<xref ref-type="app" rid="app1f2" hwp:id="xref-sec-28-1" hwp:rel-id="sec-28">Appendix A.6.2</xref>).</p></sec><sec id="s3e" hwp:id="sec-16"><title hwp:id="title-18">On post-hoc models</title><p hwp:id="p-54">Zhou et al. [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-4" hwp:rel-id="ref-45">45</xref>] introduced the so-called <italic toggle="yes">post-hoc</italic> model, another equivariant method obtained by averaging the predictions of a Standard model over a sequence and its reversecomplement, and showed that it is competitive with and often outperforms RCPS. The post-hoc model only requires training and storing one network, but aggregates two predictions for each sequence at inference time. Because of that, the good performance of post-hoc may be due in part to the aggregation step common to all ensemble models [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>]. To decipher the respective contributions of the network architecture, on the one hand, and of the aggregation of predictions, on the other hand, we add to the comparison an ensemble of two Standard models trained with different random seeds (<italic toggle="yes">Ensemble Standard</italic>) and an ensemble of two equivariant Irrep models (<italic toggle="yes">Ensemble Irrep</italic>) and present the results in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>. We see that Ensemble Irrep strongly outperforms Best Equivariant, and both post-hoc and Ensemble Standard widely outperform the Standard architecture. This confirms that ensembling equivariant or non-equivariant models through post-hoc of ensemble aggregation is always useful (at the cost of increased computational time). We see that Ensemble Standard is not significantly different from post-hoc Standard on CTCF and SPI1, but that post-hoc Standard is better on MAX, suggesting that most of the benefits of post-hoc Standard indeed comes from the ensembling effect. Regarding the impact of the architecture for a given budget of predictions, we saw earlier than Best equivariant significantly outperforms Standard when a single prediction per test sequence is allowed, and see now that Ensemble Irrep strongly outperforms both post-hoc and Ensemble Standard when two predictions are allowed, thus confirming the benefit of equivariant architectures in all settings. We also see that a single Best equivariant models outperforms post-hoc and Ensemble Standard, indicating that enforcing equivariance throughout the network is not only faster but also more more accurate than averaging a non-equivariant model over group transformed inputs.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-55">AuROC performance on the three binary classification problems, for the Best Equivariant model, the post-hoc Standard model, and an ensemble of two Standard or Irrep models. Error bars correspond to an estimate of the standard error on 10 repeats with different random seeds.</p></caption><graphic xlink:href="446953v2_fig4" position="float" orientation="portrait" hwp:id="graphic-13"/></fig></sec><sec id="s3f" hwp:id="sec-17"><title hwp:id="title-19">Profile task</title><p hwp:id="p-56">We now compare the performance of different models on the profile prediction tasks. To limit the carbon footprint of this study, and based on the influence of hyperparameters on the binary task (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2</xref>), we only test two equivariant models in addition to Standard and RCPS: a Regular model with <italic toggle="yes">k</italic> = 3, and an Irrep model with <italic toggle="yes">k</italic> = 3 and <italic toggle="yes">a</italic>/(<italic toggle="yes">a</italic> + <italic toggle="yes">b</italic>) = 75%. We also assess the performance of post-hoc Standard (the best model in [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-5" hwp:rel-id="ref-45">45</xref>]), and an ensemble of two models of the best performing equivariant model. <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref> shows the performance of all models in terms of Spearman correlation between the target profile and the predicted ones, on the full dataset (left) or a reduced experiment with only 1,000 training sequences (right).</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-57">Spearman Correlation between true and predicted profiles by different methods for four data sets.</p></caption><graphic xlink:href="446953v2_fig5" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><p hwp:id="p-58">First of all, we observe as before that in the low-data regime, the gap between standard and equivariant networks grows in favor of equivariant ones. We also observe, surprisingly, that Irrep, which outperformed RCPS on the binary task, now underperforms it. A possible explanation could be that since this task aims to annotate an individual nucleotide, encoding the nucleotide level information using <italic toggle="yes">k</italic>-mers makes the signal blurry and decreases performance. However, in the reduced setting, Irrep performs better again. These results indicate that for now the best model should be chosen empirically on a validation set. Finally, despite good performance of post-hoc Standard, the ensemble equivariant model once again performs better for the same computational cost at inference.</p></sec><sec id="s3g" hwp:id="sec-18"><title hwp:id="title-20">Experiment settings and computational cost</title><p hwp:id="p-59">All experiments were run on a single GPU (either a GTX1080 or a RTX6000), with 20 CPU cores. The binary classification experiments were shorter to train. To limit our carbon footprint, we chose to run more experiments on this task, e.g., for hyperparameter tuning and to reduce the number of replicates for the profile task. The total runtimes of each of those tasks were approximately of a week.</p></sec></sec><sec id="s4" hwp:id="sec-19"><label>4</label><title hwp:id="title-21">Conclusion</title><p hwp:id="p-60">In this paper, we addressed the problem of including the RC symmetry prior in neural networks. Leveraging the framework of equivariant networks, in particular steerable CNNs, we deepened existing methods by unraveling the whole space of linear layers and pointwise nonlinearities that are translation and RC-equivariant. We also investigated the links between the linear representations and the non-linear layers of neural networks, exposing the special role of the regular representation in equivariant networks. Finally, we implemented new linear and nonlinear equivariant layers and make all these equivariant layers available in Keras [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>] and Pytorch [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>].<xref ref-type="fn" rid="fn3" hwp:id="xref-fn-3-1" hwp:rel-id="fn-3"><sup>3</sup></xref> We then explored empirically how this larger equivariant functional space behaves in terms of learning. Our best results improve the state of the art performance of equivariant networks, showing that new equivariant architectures can have practical benefits. In the future we plan to test more deeply the newly proposed architectures on prediction tasks involving double-stranded DNA, such as DNA-protein binding prediction, epigenetics or metagenomics. On the theoretical side, we characterized equivariant pointwise nonlinearities that preserve the layer type, but more general nonlinear transforms (e.g., not pointwise, or changing the layer type) remain to be fully characterized.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-22">Acknowledgments and Disclosure of Funding</title><p hwp:id="p-61">V.M. is recipient of a doctoral fellowship from the INCEPTION project [PIA/ANR-16-CONV-0005] and benefits from support from the CRI through Ecole Doctorale FIRE - Programme Bettencourt. We thank Marie Dechelle, Jacques Boitreaud, Carlos G. Oliver and Guillaume Bouvier for reviewing the manuscript. We thank Avanti Shrikumar, Hannah Zhou and Anshul Kundaje for helpful discussions and sharing their code.</p></ack><sec hwp:id="sec-20"><title hwp:id="title-23">Conflict of Interest</title><p hwp:id="p-62">None declared.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-24">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Alipanahi B."><given-names>B.</given-names> <surname>Alipanahi</surname></string-name>, <string-name name-style="western" hwp:sortable="Delong A."><given-names>A.</given-names> <surname>Delong</surname></string-name>, <string-name name-style="western" hwp:sortable="Weirauch M. T."><given-names>M. T.</given-names> <surname>Weirauch</surname></string-name>, and <string-name name-style="western" hwp:sortable="Frey B. J."><given-names>B. J.</given-names> <surname>Frey</surname></string-name>. <article-title hwp:id="article-title-2">Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning</article-title>. <source hwp:id="source-1">Nature biotechnology</source>, <volume>33</volume>(<issue>8</issue>):<fpage>831</fpage>–<lpage>838</lpage>, <year>2015</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>[2]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.03.446953v2.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Anderson B."><given-names>B.</given-names> <surname>Anderson</surname></string-name>, <string-name name-style="western" hwp:sortable="Hy T. S."><given-names>T. S.</given-names> <surname>Hy</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kondor R."><given-names>R.</given-names> <surname>Kondor</surname></string-name>. <chapter-title>Cormorant: Covariant molecular neural networks</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Wallach H."><given-names>H.</given-names> <surname>Wallach</surname></string-name>, <string-name name-style="western" hwp:sortable="Larochelle H."><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name name-style="western" hwp:sortable="Beygelzimer A."><given-names>A.</given-names> <surname>Beygelzimer</surname></string-name>, <string-name name-style="western" hwp:sortable="d’Alché-Buc F."><given-names>F.</given-names> <surname>d’Alché-Buc</surname></string-name>, <string-name name-style="western" hwp:sortable="Fox E."><given-names>E.</given-names> <surname>Fox</surname></string-name>, and <string-name name-style="western" hwp:sortable="Garnett R."><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group>, editors, <source hwp:id="source-2">Advances in Neural Information Processing Systems</source>, volume <volume>32</volume>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2019</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bronstein M. M."><given-names>M. M.</given-names> <surname>Bronstein</surname></string-name>, <string-name name-style="western" hwp:sortable="Bruna J."><given-names>J.</given-names> <surname>Bruna</surname></string-name>, <string-name name-style="western" hwp:sortable="Cohen T."><given-names>T.</given-names> <surname>Cohen</surname></string-name>, and <string-name name-style="western" hwp:sortable="Veličković P."><given-names>P.</given-names> <surname>Veličković</surname></string-name>. <article-title hwp:id="article-title-3">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</article-title>. <source hwp:id="source-3">Technical Report 2104.13478, arXiv</source>, <year>2021</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2 xref-ref-4-3"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Brown R. C."><given-names>R. C.</given-names> <surname>Brown</surname></string-name> and <string-name name-style="western" hwp:sortable="Lunter G."><given-names>G.</given-names> <surname>Lunter</surname></string-name>. <article-title hwp:id="article-title-4">An equivariant Bayesian convolutional network predicts recombination hotspots and accurately resolves binding motifs</article-title>. <source hwp:id="source-4">Bioinformatics</source>, <volume>35</volume>(<issue>13</issue>):<fpage>2177</fpage>–<lpage>2184</lpage>, <year>2019</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="website" citation-type="web" ref:id="2021.06.03.446953v2.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Chollet F."><given-names>F.</given-names> <surname>Chollet</surname></string-name>. <article-title hwp:id="article-title-5">Keras</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/fchollet/keras" ext-link-type="uri" xlink:href="https://github.com/fchollet/keras" hwp:id="ext-link-2">https://github.com/fchollet/keras</ext-link>, <year>2015</year>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>[6]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Clauwaert J."><given-names>J.</given-names> <surname>Clauwaert</surname></string-name> and <string-name name-style="western" hwp:sortable="Waegeman W."><given-names>W.</given-names> <surname>Waegeman</surname></string-name>. <article-title hwp:id="article-title-6">Novel transformer networks for improved sequence labeling in genomics</article-title>. <source hwp:id="source-5">IEEE/ACM Trans. Comput. Biol. Bioinform</source>., <year>2021</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><label>[7]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.03.446953v2.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Cohen T."><given-names>T.</given-names> <surname>Cohen</surname></string-name> and <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>. <chapter-title>Group equivariant convolutional networks</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Balcan M. F."><given-names>M. F.</given-names> <surname>Balcan</surname></string-name> and <string-name name-style="western" hwp:sortable="Weinberger K. Q."><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name></person-group>, editors, <source hwp:id="source-6">Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research</source>, pages <fpage>2990</fpage>–<lpage>2999</lpage>, <publisher-loc>New York, New York, USA</publisher-loc>, <day>20–22</day> <month>Jun</month> <year>2016</year>. <publisher-name>PMLR</publisher-name>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>[8]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Cohen T."><given-names>T.</given-names> <surname>Cohen</surname></string-name>, <string-name name-style="western" hwp:sortable="Weiler M."><given-names>M.</given-names> <surname>Weiler</surname></string-name>, <string-name name-style="western" hwp:sortable="Kicanaoglu B."><given-names>B.</given-names> <surname>Kicanaoglu</surname></string-name>, and <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>. <article-title hwp:id="article-title-7">Gauge equivariant convolutional networks and the icosahedral CNN</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, pages <fpage>1321</fpage>–<lpage>1330</lpage>. <source hwp:id="source-7">PMLR</source>, <year>2019</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4"><label>[9]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.9" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Cohen T. S."><given-names>T. S.</given-names> <surname>Cohen</surname></string-name> and <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>. <article-title hwp:id="article-title-8">Steerable CNNs</article-title>. In <conf-name>International Conference on Learning Representations (ICLR)</conf-name>, <year>2017</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Cohen T. S."><given-names>T. S.</given-names> <surname>Cohen</surname></string-name>, <string-name name-style="western" hwp:sortable="Geiger M."><given-names>M.</given-names> <surname>Geiger</surname></string-name>, <string-name name-style="western" hwp:sortable="Köhler J."><given-names>J.</given-names> <surname>Köhler</surname></string-name>, and <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>. <article-title hwp:id="article-title-9">Spherical CNNs</article-title>. In <conf-name>Proceedings of the 6th International Conference on Learning Representations (ICLR)</conf-name>, <year>2018</year>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2 xref-ref-11-3 xref-ref-11-4 xref-ref-11-5 xref-ref-11-6 xref-ref-11-7 xref-ref-11-8"><label>[11]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.03.446953v2.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Cohen T. S."><given-names>T. S.</given-names> <surname>Cohen</surname></string-name>, <string-name name-style="western" hwp:sortable="Geiger M."><given-names>M.</given-names> <surname>Geiger</surname></string-name>, and <string-name name-style="western" hwp:sortable="Weiler M."><given-names>M.</given-names> <surname>Weiler</surname></string-name>. <chapter-title>A General Theory of Equivariant CNNs on Homogeneous Spaces</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-3"><string-name name-style="western" hwp:sortable="Wallach H."><given-names>H.</given-names> <surname>Wallach</surname></string-name>, <string-name name-style="western" hwp:sortable="Larochelle H."><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name name-style="western" hwp:sortable="Beygelzimer A."><given-names>A.</given-names> <surname>Beygelzimer</surname></string-name>, <string-name name-style="western" hwp:sortable="d’Alché-Buc F."><given-names>F.</given-names> <surname>d’Alché-Buc</surname></string-name>, <string-name name-style="western" hwp:sortable="Fox E."><given-names>E.</given-names> <surname>Fox</surname></string-name>, and <string-name name-style="western" hwp:sortable="Garnett R."><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group>, editors, <source hwp:id="source-8">Advances in Neural Information Processing Systems</source>, volume <volume>32</volume>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2019</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.03.446953v2.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Dietterich T. G."><given-names>T. G.</given-names> <surname>Dietterich</surname></string-name>. <chapter-title>Ensemble methods in machine learning</chapter-title>. In <source hwp:id="source-9">Proceedings of the First International Workshop on Multiple Classifier Systems, MCS ’00</source>, page <fpage>1</fpage>–<lpage>15</lpage>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <year>2000</year>. <publisher-name>Springer-Verlag</publisher-name>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>[13]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Dym N."><given-names>N.</given-names> <surname>Dym</surname></string-name> and <string-name name-style="western" hwp:sortable="Maron H."><given-names>H.</given-names> <surname>Maron</surname></string-name>. <article-title hwp:id="article-title-10">On the Universality of Rotation Equivariant Point Cloud Networks</article-title>. <source hwp:id="source-10">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2010.02449</pub-id>, <year>2020</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Esteves C."><given-names>C.</given-names> <surname>Esteves</surname></string-name>. <article-title hwp:id="article-title-11">Theoretical aspects of group equivariant neural networks</article-title>. <source hwp:id="source-11">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2004.05154</pub-id>, <year>2020</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>[15]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.03.446953v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Fuchs F."><given-names>F.</given-names> <surname>Fuchs</surname></string-name>, <string-name name-style="western" hwp:sortable="Worrall D."><given-names>D.</given-names> <surname>Worrall</surname></string-name>, <string-name name-style="western" hwp:sortable="Fischer V."><given-names>V.</given-names> <surname>Fischer</surname></string-name>, and <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>. <chapter-title>SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-4"><string-name name-style="western" hwp:sortable="Larochelle H."><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name name-style="western" hwp:sortable="Ranzato M."><given-names>M.</given-names> <surname>Ranzato</surname></string-name>, <string-name name-style="western" hwp:sortable="Hadsell R."><given-names>R.</given-names> <surname>Hadsell</surname></string-name>, <string-name name-style="western" hwp:sortable="Balcan M. F."><given-names>M. F.</given-names> <surname>Balcan</surname></string-name>, and <string-name name-style="western" hwp:sortable="Lin H."><given-names>H.</given-names> <surname>Lin</surname></string-name></person-group>, editors, <source hwp:id="source-12">Advances in Neural Information Processing Systems</source>, volume <volume>33</volume>, pages <fpage>1970</fpage>–<lpage>1981</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2020</year>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Ghandi M."><given-names>M.</given-names> <surname>Ghandi</surname></string-name>, <string-name name-style="western" hwp:sortable="Lee D."><given-names>D.</given-names> <surname>Lee</surname></string-name>, <string-name name-style="western" hwp:sortable="Mohammad-Noori M."><given-names>M.</given-names> <surname>Mohammad-Noori</surname></string-name>, and <string-name name-style="western" hwp:sortable="Beer M. A."><given-names>M. A.</given-names> <surname>Beer</surname></string-name>. <article-title hwp:id="article-title-12">Enhanced regulatory sequence prediction using gapped k-mer features</article-title>. <source hwp:id="source-13">PLoS Comput Biol</source>, <volume>10</volume>(<issue>7</issue>):<fpage>e1003711</fpage>, <year>2014</year>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Graham S."><given-names>S.</given-names> <surname>Graham</surname></string-name>, <string-name name-style="western" hwp:sortable="Epstein D."><given-names>D.</given-names> <surname>Epstein</surname></string-name>, and <string-name name-style="western" hwp:sortable="Rajpoot N."><given-names>N.</given-names> <surname>Rajpoot</surname></string-name>. <article-title hwp:id="article-title-13">Dense steerable filter cnns for exploiting rotational symmetry in histology images</article-title>. <source hwp:id="source-14">IEEE Transactions on Medical Imaging</source>, <volume>39</volume>(<issue>12</issue>):<fpage>4124</fpage>–<lpage>4136</lpage>, <year>2020</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Hoogeboom E."><given-names>E.</given-names> <surname>Hoogeboom</surname></string-name>, <string-name name-style="western" hwp:sortable="Peters J. W. T."><given-names>J. W. T.</given-names> <surname>Peters</surname></string-name>, <string-name name-style="western" hwp:sortable="Cohen T. S."><given-names>T. S.</given-names> <surname>Cohen</surname></string-name>, and <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>. <article-title hwp:id="article-title-14">Hexaconv</article-title>. In <source hwp:id="source-15">6th International Conference on Learning Representations, ICLR 2018</source>, <conf-loc>Vancouver, BC, Canada</conf-loc>, <conf-name>April 30 - May 3, 2018, Conference Track Proceedings</conf-name>. <ext-link l:rel="related" l:ref-type="uri" l:ref="http://OpenReview.net" ext-link-type="uri" xlink:href="http://OpenReview.net" hwp:id="ext-link-3">OpenReview.net</ext-link>, <year>2018</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Ioffe S."><given-names>S.</given-names> <surname>Ioffe</surname></string-name> and <string-name name-style="western" hwp:sortable="Szegedy C."><given-names>C.</given-names> <surname>Szegedy</surname></string-name>. <article-title hwp:id="article-title-15">Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title>. In <conf-name>International conference on machine learning</conf-name>, pages <fpage>448</fpage>–<lpage>456</lpage>. <source hwp:id="source-16">PMLR</source>, <year>2015</year>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>[20]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Ji Y."><given-names>Y.</given-names> <surname>Ji</surname></string-name>, <string-name name-style="western" hwp:sortable="Zhou Z."><given-names>Z.</given-names> <surname>Zhou</surname></string-name>, <string-name name-style="western" hwp:sortable="Liu H."><given-names>H.</given-names> <surname>Liu</surname></string-name>, and <string-name name-style="western" hwp:sortable="Davuluri R. V."><given-names>R. V.</given-names> <surname>Davuluri</surname></string-name>. <article-title hwp:id="article-title-16">DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</article-title>. <source hwp:id="source-17">Bioinformatics</source>, <month>Feb</month>. <year>2021</year>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Kondor R."><given-names>R.</given-names> <surname>Kondor</surname></string-name>. <article-title hwp:id="article-title-17">N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials</article-title>. <source hwp:id="source-18">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1803.01588</pub-id>, <year>2018</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>[22]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Kondor R."><given-names>R.</given-names> <surname>Kondor</surname></string-name> and <string-name name-style="western" hwp:sortable="Trivedi S."><given-names>S.</given-names> <surname>Trivedi</surname></string-name>. <article-title hwp:id="article-title-18">On the generalization of equivariance and convolution in neural networks to the action of compact groups</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, pages <fpage>2747</fpage>–<lpage>2755</lpage>. <source hwp:id="source-19">PMLR</source>, <year>2018</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Lafarge M. W."><given-names>M. W.</given-names> <surname>Lafarge</surname></string-name>, <string-name name-style="western" hwp:sortable="Bekkers E. J."><given-names>E. J.</given-names> <surname>Bekkers</surname></string-name>, <string-name name-style="western" hwp:sortable="Pluim J. P."><given-names>J. P.</given-names> <surname>Pluim</surname></string-name>, <string-name name-style="western" hwp:sortable="Duits R."><given-names>R.</given-names> <surname>Duits</surname></string-name>, and <string-name name-style="western" hwp:sortable="Veta M."><given-names>M.</given-names> <surname>Veta</surname></string-name>. <article-title hwp:id="article-title-19">Roto-translation equivariant convolutional networks: Application to histopathology image analysis</article-title>. <source hwp:id="source-20">Medical Image Analysis</source>, <volume>68</volume>:<fpage>101849</fpage>, <year>2021</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Lee D."><given-names>D.</given-names> <surname>Lee</surname></string-name>, <string-name name-style="western" hwp:sortable="Gorkin D. U."><given-names>D. U.</given-names> <surname>Gorkin</surname></string-name>, <string-name name-style="western" hwp:sortable="Baker M."><given-names>M.</given-names> <surname>Baker</surname></string-name>, <string-name name-style="western" hwp:sortable="Strober B. J."><given-names>B. J.</given-names> <surname>Strober</surname></string-name>, <string-name name-style="western" hwp:sortable="Asoni A. L."><given-names>A. L.</given-names> <surname>Asoni</surname></string-name>, <string-name name-style="western" hwp:sortable="McCallion A. S."><given-names>A. S.</given-names> <surname>McCallion</surname></string-name>, and <string-name name-style="western" hwp:sortable="Beer M. A."><given-names>M. A.</given-names> <surname>Beer</surname></string-name>. <article-title hwp:id="article-title-20">A method to predict the impact of regulatory variants from DNA sequence</article-title>. <source hwp:id="source-21">Nature genetics</source>, <volume>47</volume>(<issue>8</issue>):<fpage>955</fpage>, <year>2015</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Levy J. J."><given-names>J. J.</given-names> <surname>Levy</surname></string-name>, <string-name name-style="western" hwp:sortable="Titus A. J."><given-names>A. J.</given-names> <surname>Titus</surname></string-name>, <string-name name-style="western" hwp:sortable="Petersen C. L."><given-names>C. L.</given-names> <surname>Petersen</surname></string-name>, <string-name name-style="western" hwp:sortable="Chen Y."><given-names>Y.</given-names> <surname>Chen</surname></string-name>, <string-name name-style="western" hwp:sortable="Salas L. A."><given-names>L. A.</given-names> <surname>Salas</surname></string-name>, and <string-name name-style="western" hwp:sortable="Christensen B. C."><given-names>B. C.</given-names> <surname>Christensen</surname></string-name>. <article-title hwp:id="article-title-21">MethylNet: an automated and modular deep learning approach for DNA methylation analysis</article-title>. <source hwp:id="source-22">BMC bioinformatics</source>, <volume>21</volume>:<fpage>108</fpage>, <year>2020</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Liang Q."><given-names>Q.</given-names> <surname>Liang</surname></string-name>, <string-name name-style="western" hwp:sortable="Bible P. W."><given-names>P. W.</given-names> <surname>Bible</surname></string-name>, <string-name name-style="western" hwp:sortable="Liu Y."><given-names>Y.</given-names> <surname>Liu</surname></string-name>, <string-name name-style="western" hwp:sortable="Zou B."><given-names>B.</given-names> <surname>Zou</surname></string-name>, and <string-name name-style="western" hwp:sortable="Wei L."><given-names>L.</given-names> <surname>Wei</surname></string-name>. <article-title hwp:id="article-title-22">DeepMicrobes: taxonomic classification for metagenomics with deep learning</article-title>. <source hwp:id="source-23">NAR genom. bioinform</source>., <volume>2</volume>:<fpage>lqaa009</fpage>, <month>Mar</month>. <year>2020</year>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Liang W."><given-names>W.</given-names> <surname>Liang</surname></string-name>. <article-title hwp:id="article-title-23">Segmenting DNA sequence into words based on statistical language model</article-title>. <source hwp:id="source-24">Nature Precedings</source>, pages <fpage>1</fpage>–<lpage>1</lpage>, <year>2012</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2"><label>[28]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Menegaux R."><given-names>R.</given-names> <surname>Menegaux</surname></string-name> and <string-name name-style="western" hwp:sortable="Vert J.-P."><given-names>J.-P.</given-names> <surname>Vert</surname></string-name>. <article-title hwp:id="article-title-24">Continuous embeddings of DNA sequencing reads and application to metagenomics</article-title>. <source hwp:id="source-25">Journal of Computational Biology</source>, <volume>26</volume>(<issue>6</issue>):<fpage>509</fpage>–<lpage>518</lpage>, <year>2019</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2 xref-ref-29-3"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Onimaru K."><given-names>K.</given-names> <surname>Onimaru</surname></string-name>, <string-name name-style="western" hwp:sortable="Nishimura O."><given-names>O.</given-names> <surname>Nishimura</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kuraku S."><given-names>S.</given-names> <surname>Kuraku</surname></string-name>. <article-title hwp:id="article-title-25">Predicting gene regulatory regions with a convolutional neural network for processing double-strand genome sequence information</article-title>. <source hwp:id="source-26">PloS one</source>, <volume>15</volume>(<issue>7</issue>):<fpage>e0235748</fpage>, <year>2020</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Oubounyt M."><given-names>M.</given-names> <surname>Oubounyt</surname></string-name>, <string-name name-style="western" hwp:sortable="Louadi Z."><given-names>Z.</given-names> <surname>Louadi</surname></string-name>, <string-name name-style="western" hwp:sortable="Tayara H."><given-names>H.</given-names> <surname>Tayara</surname></string-name>, and <string-name name-style="western" hwp:sortable="Chong K. T."><given-names>K. T.</given-names> <surname>Chong</surname></string-name>. <article-title hwp:id="article-title-26">DeePromoter: Robust promoter predictor using deep learning</article-title>. <source hwp:id="source-27">Frontiers in genetics</source>, <volume>10</volume>:<fpage>286</fpage>, <year>2019</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="book" citation-type="book" ref:id="2021.06.03.446953v2.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Paszke A."><given-names>A.</given-names> <surname>Paszke</surname></string-name>, <string-name name-style="western" hwp:sortable="Gross S."><given-names>S.</given-names> <surname>Gross</surname></string-name>, <string-name name-style="western" hwp:sortable="Massa F."><given-names>F.</given-names> <surname>Massa</surname></string-name>, <string-name name-style="western" hwp:sortable="Lerer A."><given-names>A.</given-names> <surname>Lerer</surname></string-name>, <string-name name-style="western" hwp:sortable="Bradbury J."><given-names>J.</given-names> <surname>Bradbury</surname></string-name>, <string-name name-style="western" hwp:sortable="Chanan G."><given-names>G.</given-names> <surname>Chanan</surname></string-name>, <string-name name-style="western" hwp:sortable="Killeen T."><given-names>T.</given-names> <surname>Killeen</surname></string-name>, <string-name name-style="western" hwp:sortable="Lin Z."><given-names>Z.</given-names> <surname>Lin</surname></string-name>, <string-name name-style="western" hwp:sortable="Gimelshein N."><given-names>N.</given-names> <surname>Gimelshein</surname></string-name>, <string-name name-style="western" hwp:sortable="Antiga L."><given-names>L.</given-names> <surname>Antiga</surname></string-name>, <string-name name-style="western" hwp:sortable="Desmaison A."><given-names>A.</given-names> <surname>Desmaison</surname></string-name>, <string-name name-style="western" hwp:sortable="Kopf A."><given-names>A.</given-names> <surname>Kopf</surname></string-name>, <string-name name-style="western" hwp:sortable="Yang E."><given-names>E.</given-names> <surname>Yang</surname></string-name>, <string-name name-style="western" hwp:sortable="DeVito Z."><given-names>Z.</given-names> <surname>DeVito</surname></string-name>, <string-name name-style="western" hwp:sortable="Raison M."><given-names>M.</given-names> <surname>Raison</surname></string-name>, <string-name name-style="western" hwp:sortable="Tejani A."><given-names>A.</given-names> <surname>Tejani</surname></string-name>, <string-name name-style="western" hwp:sortable="Chilamkurthy S."><given-names>S.</given-names> <surname>Chilamkurthy</surname></string-name>, <string-name name-style="western" hwp:sortable="Steiner B."><given-names>B.</given-names> <surname>Steiner</surname></string-name>, <string-name name-style="western" hwp:sortable="Fang L."><given-names>L.</given-names> <surname>Fang</surname></string-name>, <string-name name-style="western" hwp:sortable="Bai J."><given-names>J.</given-names> <surname>Bai</surname></string-name>, and <string-name name-style="western" hwp:sortable="Chintala S."><given-names>S.</given-names> <surname>Chintala</surname></string-name>. <chapter-title>Pytorch: An imperative style, high-performance deep learning library</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-5"><string-name name-style="western" hwp:sortable="Wallach H."><given-names>H.</given-names> <surname>Wallach</surname></string-name>, <string-name name-style="western" hwp:sortable="Larochelle H."><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name name-style="western" hwp:sortable="Beygelzimer A."><given-names>A.</given-names> <surname>Beygelzimer</surname></string-name>, <string-name name-style="western" hwp:sortable="d’Alché-Buc F."><given-names>F.</given-names> <surname>d’Alché-Buc</surname></string-name>, <string-name name-style="western" hwp:sortable="Fox E."><given-names>E.</given-names> <surname>Fox</surname></string-name>, and <string-name name-style="western" hwp:sortable="Garnett R."><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group>, editors, <source hwp:id="source-28">Advances in Neural Information Processing Systems 32</source>, pages <fpage>8024</fpage>–<lpage>8035</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2019</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Quang D."><given-names>D.</given-names> <surname>Quang</surname></string-name> and <string-name name-style="western" hwp:sortable="Xie X."><given-names>X.</given-names> <surname>Xie</surname></string-name>. <article-title hwp:id="article-title-27">Factornet: a deep learning framework for predicting cell type specific transcription factor binding from nucleotide-resolution sequential data</article-title>. <source hwp:id="source-29">Methods</source>, <volume>166</volume>:<fpage>40</fpage>–<lpage>47</lpage>, <year>2019</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Schütt K. T."><given-names>K. T.</given-names> <surname>Schütt</surname></string-name>, <string-name name-style="western" hwp:sortable="Unke O. T."><given-names>O. T.</given-names> <surname>Unke</surname></string-name>, and <string-name name-style="western" hwp:sortable="Gastegger M."><given-names>M.</given-names> <surname>Gastegger</surname></string-name>. <article-title hwp:id="article-title-28">Equivariant message passing for the prediction of tensorial properties and molecular spectra</article-title>. <source hwp:id="source-30">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2102.03150</pub-id>, <year>2021</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2 xref-ref-34-3 xref-ref-34-4 xref-ref-34-5 xref-ref-34-6 xref-ref-34-7 xref-ref-34-8 xref-ref-34-9 xref-ref-34-10"><label>[34]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Shrikumar A."><given-names>A.</given-names> <surname>Shrikumar</surname></string-name>, <string-name name-style="western" hwp:sortable="Greenside P."><given-names>P.</given-names> <surname>Greenside</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kundaje A."><given-names>A.</given-names> <surname>Kundaje</surname></string-name>. <article-title hwp:id="article-title-29">Reverse-complement parameter sharing improves deep learning models for genomics</article-title>. <source hwp:id="source-31">bioRxiv</source>, page <fpage>103663</fpage>, <year>2017</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Stormo G. D."><given-names>G. D.</given-names> <surname>Stormo</surname></string-name>. <article-title hwp:id="article-title-30">DNA binding sites: representation and discovery</article-title>. <source hwp:id="source-32">Bioinformatics</source>, <volume>16</volume>(<issue>1</issue>):<fpage>16</fpage>–<lpage>23</lpage>, <year>2000</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2"><label>[36]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Tampuu A."><given-names>A.</given-names> <surname>Tampuu</surname></string-name>, <string-name name-style="western" hwp:sortable="Bzhalava Z."><given-names>Z.</given-names> <surname>Bzhalava</surname></string-name>, <string-name name-style="western" hwp:sortable="Dillner J."><given-names>J.</given-names> <surname>Dillner</surname></string-name>, and <string-name name-style="western" hwp:sortable="Vicente R."><given-names>R.</given-names> <surname>Vicente</surname></string-name>. <article-title hwp:id="article-title-31">ViraMiner: Deep learning on raw DNA sequences for identifying viral genomes in human samples</article-title>. <source hwp:id="source-33">PloS one</source>, <volume>14</volume>:<fpage>e0222271</fpage>, <year>2019</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2 xref-ref-37-3"><label>[37]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Thomas N."><given-names>N.</given-names> <surname>Thomas</surname></string-name>, <string-name name-style="western" hwp:sortable="Smidt T."><given-names>T.</given-names> <surname>Smidt</surname></string-name>, <string-name name-style="western" hwp:sortable="Kearnes S."><given-names>S.</given-names> <surname>Kearnes</surname></string-name>, <string-name name-style="western" hwp:sortable="Yang L."><given-names>L.</given-names> <surname>Yang</surname></string-name>, <string-name name-style="western" hwp:sortable="Li L."><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Kohlhoff K."><given-names>K.</given-names> <surname>Kohlhoff</surname></string-name>, and <string-name name-style="western" hwp:sortable="Riley P."><given-names>P.</given-names> <surname>Riley</surname></string-name>. <article-title hwp:id="article-title-32">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</article-title>. <source hwp:id="source-34">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1802.08219</pub-id>, <year>2018</year>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>[38]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Vervier K."><given-names>K.</given-names> <surname>Vervier</surname></string-name>, <string-name name-style="western" hwp:sortable="Mahé P."><given-names>P.</given-names> <surname>Mahé</surname></string-name>, <string-name name-style="western" hwp:sortable="Tournoud M."><given-names>M.</given-names> <surname>Tournoud</surname></string-name>, <string-name name-style="western" hwp:sortable="Veyrieras J.-B."><given-names>J.-B.</given-names> <surname>Veyrieras</surname></string-name>, and <string-name name-style="western" hwp:sortable="Vert J.-P."><given-names>J.-P.</given-names> <surname>Vert</surname></string-name>. <article-title hwp:id="article-title-33">Large-scale machine learning for metagenomics sequence classification</article-title>. <source hwp:id="source-35">Bioinformatics</source>, <volume>32</volume>:<fpage>1023</fpage>–<lpage>1032</lpage>, <year>2016</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>[39]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Weiler M."><given-names>M.</given-names> <surname>Weiler</surname></string-name> and <string-name name-style="western" hwp:sortable="Cesa G."><given-names>G.</given-names> <surname>Cesa</surname></string-name>. <article-title hwp:id="article-title-34">General <italic toggle="yes">e</italic>(2)-equivariant steerable cnns</article-title>. <source hwp:id="source-36">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1911.08251</pub-id>, <year>2019</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2"><label>[40]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Weiler M."><given-names>M.</given-names> <surname>Weiler</surname></string-name>, <string-name name-style="western" hwp:sortable="Geiger M."><given-names>M.</given-names> <surname>Geiger</surname></string-name>, <string-name name-style="western" hwp:sortable="Welling M."><given-names>M.</given-names> <surname>Welling</surname></string-name>, <string-name name-style="western" hwp:sortable="Boomsma W."><given-names>W.</given-names> <surname>Boomsma</surname></string-name>, and <string-name name-style="western" hwp:sortable="Cohen T. S."><given-names>T. S.</given-names> <surname>Cohen</surname></string-name>. <article-title hwp:id="article-title-35">3d steerable cnns: Learning rotationally equivariant features in volumetric data</article-title>. In <source hwp:id="source-37">Advances in Neural Information Processing Systems</source>, pages <fpage>10381</fpage>–<lpage>10392</lpage>, <year>2018</year>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>[41]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Winkels M."><given-names>M.</given-names> <surname>Winkels</surname></string-name> and <string-name name-style="western" hwp:sortable="Cohen T. S."><given-names>T. S.</given-names> <surname>Cohen</surname></string-name>. <article-title hwp:id="article-title-36">Pulmonary nodule detection in ct scans with equivariant cnns</article-title>. <source hwp:id="source-38">Medical image analysis</source>, <volume>55</volume>:<fpage>15</fpage>–<lpage>26</lpage>, <year>2019</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>[42]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2021.06.03.446953v2.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Zaheer M."><given-names>M.</given-names> <surname>Zaheer</surname></string-name>, <string-name name-style="western" hwp:sortable="Guruganesh G."><given-names>G.</given-names> <surname>Guruganesh</surname></string-name>, <string-name name-style="western" hwp:sortable="Dubey K. A."><given-names>K. A.</given-names> <surname>Dubey</surname></string-name>, <string-name name-style="western" hwp:sortable="Ainslie J."><given-names>J.</given-names> <surname>Ainslie</surname></string-name>, <string-name name-style="western" hwp:sortable="Alberti C."><given-names>C.</given-names> <surname>Alberti</surname></string-name>, <string-name name-style="western" hwp:sortable="Ontañón S."><given-names>S.</given-names> <surname>Ontañón</surname></string-name>, <string-name name-style="western" hwp:sortable="Pham P."><given-names>P.</given-names> <surname>Pham</surname></string-name>, <string-name name-style="western" hwp:sortable="Ravula A."><given-names>A.</given-names> <surname>Ravula</surname></string-name>, <string-name name-style="western" hwp:sortable="Wang Q."><given-names>Q.</given-names> <surname>Wang</surname></string-name>, <string-name name-style="western" hwp:sortable="Yang L."><given-names>L.</given-names> <surname>Yang</surname></string-name>, and <string-name name-style="western" hwp:sortable="Ahmed A."><given-names>A.</given-names> <surname>Ahmed</surname></string-name>. <article-title hwp:id="article-title-37">Big Bird: Transformers for Longer Sequences</article-title>. In <person-group person-group-type="editor" hwp:id="person-group-6"><string-name name-style="western" hwp:sortable="Larochelle H."><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name name-style="western" hwp:sortable="Ranzato M."><given-names>M.</given-names> <surname>Ranzato</surname></string-name>, <string-name name-style="western" hwp:sortable="Hadsell R."><given-names>R.</given-names> <surname>Hadsell</surname></string-name>, <string-name name-style="western" hwp:sortable="Balcan M."><given-names>M.</given-names> <surname>Balcan</surname></string-name>, and <string-name name-style="western" hwp:sortable="Lin H."><given-names>H.</given-names> <surname>Lin</surname></string-name></person-group>, editors, <conf-name>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</conf-name>, <year>2020</year>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>[43]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Zeng H."><given-names>H.</given-names> <surname>Zeng</surname></string-name>, <string-name name-style="western" hwp:sortable="Edwards M. D."><given-names>M. D.</given-names> <surname>Edwards</surname></string-name>, <string-name name-style="western" hwp:sortable="Liu G."><given-names>G.</given-names> <surname>Liu</surname></string-name>, and <string-name name-style="western" hwp:sortable="Gifford D. K."><given-names>D. K.</given-names> <surname>Gifford</surname></string-name>. <article-title hwp:id="article-title-38">Convolutional neural network architectures for predicting DNA–protein binding</article-title>. <source hwp:id="source-39">Bioinformatics</source>, <volume>32</volume>(<issue>12</issue>):<fpage>i121</fpage>–<lpage>i127</lpage>, <year>2016</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>[44]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Zhang H."><given-names>H.</given-names> <surname>Zhang</surname></string-name>, <string-name name-style="western" hwp:sortable="Hung C.-L."><given-names>C.-L.</given-names> <surname>Hung</surname></string-name>, <string-name name-style="western" hwp:sortable="Liu M."><given-names>M.</given-names> <surname>Liu</surname></string-name>, <string-name name-style="western" hwp:sortable="Hu X."><given-names>X.</given-names> <surname>Hu</surname></string-name>, and <string-name name-style="western" hwp:sortable="Lin Y.-Y."><given-names>Y.-Y.</given-names> <surname>Lin</surname></string-name>. <article-title hwp:id="article-title-39">NCNet: Deep learning network models for predicting function of non-coding DNA</article-title>. <source hwp:id="source-40">Frontiers in genetics</source>, <volume>10</volume>:<fpage>432</fpage>, <year>2019</year>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3 xref-ref-45-4 xref-ref-45-5"><label>[45]</label><citation publication-type="other" citation-type="journal" ref:id="2021.06.03.446953v2.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Zhou H."><given-names>H.</given-names> <surname>Zhou</surname></string-name>, <string-name name-style="western" hwp:sortable="Shrikumar A."><given-names>A.</given-names> <surname>Shrikumar</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kundaje A."><given-names>A.</given-names> <surname>Kundaje</surname></string-name>. <article-title hwp:id="article-title-40">Towards a better understanding of reverse-complement equivariance for deep learning models in regulatory genomics</article-title>. <source hwp:id="source-41">bioRxiv</source>, <fpage>2020.11.04.368803</fpage>, <year>2020</year>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>[46]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.06.03.446953v2.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Zhou J."><given-names>J.</given-names> <surname>Zhou</surname></string-name> and <string-name name-style="western" hwp:sortable="Troyanskaya O. G."><given-names>O. G.</given-names> <surname>Troyanskaya</surname></string-name>. <article-title hwp:id="article-title-41">Predicting effects of noncoding variants with deep learning–based sequence model</article-title>. <source hwp:id="source-42">Nature methods</source>, <volume>12</volume>(<issue>10</issue>):<fpage>931</fpage>–<lpage>934</lpage>, <year>2015</year>.</citation></ref></ref-list><app-group hwp:id="app-group-1"><app id="app1" hwp:id="app-1"><label>A</label><title hwp:id="title-25">Appendix</title><sec id="app1a" hwp:id="sec-21"><label>A.1</label><title hwp:id="title-26">Illustration of group actions</title><p hwp:id="p-63">This section is intended to provide a visual, more intuitive understanding of the different group actions on the tensors of our network. We begin with a visualization of the group action for the input space. We exemplify it over the sequence <monospace>GGACT</monospace>, whose reverse complement is <monospace>AGTCC</monospace>. The sequence is one hot encoded as explained in the main text and the group action over <inline-formula hwp:id="inline-formula-56"><alternatives hwp:id="alternatives-65"><inline-graphic xlink:href="446953v2_inline57.gif" hwp:id="inline-graphic-56"/></alternatives></inline-formula> consist in flipping the tensor along the spatial axis and swapping the channels pairwise.
<disp-formula hwp:id="disp-formula-10"><alternatives hwp:id="alternatives-66"><graphic xlink:href="446953v2_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-15"/></alternatives></disp-formula></p><p hwp:id="p-64">Now we illustrate the actions of other representations, on an example tensor <inline-formula hwp:id="inline-formula-57"><alternatives hwp:id="alternatives-67"><inline-graphic xlink:href="446953v2_inline58.gif" hwp:id="inline-graphic-57"/></alternatives></inline-formula> with two channels (of type <italic toggle="yes">a</italic> or <italic toggle="yes">b</italic>) and three positions; this could typically be the representation of an input sequence of length 3 in an intermediate layer of dimention 2. Choosing the canonical representations of type (<italic toggle="yes">I</italic>, 2, 0), (<italic toggle="yes">I</italic>, 0, 2) and (<italic toggle="yes">I</italic>, 1, 1) respectively, we get the following group actions (for clarity we add the channel type, <italic toggle="yes">a</italic> or <italic toggle="yes">b</italic>, near each matrix row):
<disp-formula hwp:id="disp-formula-11"><alternatives hwp:id="alternatives-68"><graphic xlink:href="446953v2_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-16"/></alternatives></disp-formula></p><p hwp:id="p-65">Finally, when using different values for P, we can get other group actions. As mentioned in the main text, by choosing (<italic toggle="yes">P<sub>reg</sub></italic>, 1, 1), where <inline-formula hwp:id="inline-formula-58"><alternatives hwp:id="alternatives-69"><inline-graphic xlink:href="446953v2_inline59.gif" hwp:id="inline-graphic-58"/></alternatives></inline-formula>, we get the regular representation that flips the input channel. We also provide an example of the group action for a general P matrix, by choosing (<italic toggle="yes">P<sub>general</sub></italic>, 1, 1), where <inline-formula hwp:id="inline-formula-59"><alternatives hwp:id="alternatives-70"><inline-graphic xlink:href="446953v2_inline60.gif" hwp:id="inline-graphic-59"/></alternatives></inline-formula>, we get a representation on the fibers <inline-formula hwp:id="inline-formula-60"><alternatives hwp:id="alternatives-71"><inline-graphic xlink:href="446953v2_inline61.gif" hwp:id="inline-graphic-60"/></alternatives></inline-formula>
<disp-formula hwp:id="disp-formula-12"><alternatives hwp:id="alternatives-72"><graphic xlink:href="446953v2_ueqn7.gif" position="float" orientation="portrait" hwp:id="graphic-17"/></alternatives></disp-formula></p><p hwp:id="p-66">Over the course of these examples, we have limited ourselves to the case where the input tensor had only three nucleotides and two channels, but this is coincidental. The representation with arbitrary P can mix an arbitrary number of channels together with the group action.</p></sec><sec id="app1b" hwp:id="sec-22"><label>A.2</label><title hwp:id="title-27">Proof of <xref ref-type="statement" rid="thm1" hwp:id="xref-statement-2-3" hwp:rel-id="statement-2">Theorem 1</xref></title><statement hwp:id="statement-5"><label>Proof.</label><p hwp:id="p-67">The irreducible representations (irreps) of the 2-elements group <inline-formula hwp:id="inline-formula-61"><alternatives hwp:id="alternatives-73"><inline-graphic xlink:href="446953v2_inline62.gif" hwp:id="inline-graphic-61"/></alternatives></inline-formula> are the 1-dimensional trivial and sign representations, given respectively by <italic toggle="yes">ρ</italic><sub>1</sub>(<italic toggle="yes">s</italic>) = 1 and <italic toggle="yes">ρ</italic><sub>1</sub>(<italic toggle="yes">s</italic>) = <italic toggle="yes">s</italic>. Any representation <italic toggle="yes">ρ<sub>n</sub></italic> can be decomposed as a direct sum of irreps, and since each irrep is 1-dimensional this means that there exists an invertible matrix <italic toggle="yes">P</italic> such that <italic toggle="yes">P<sub>ρ<sub>n</sub></sub></italic>(<italic toggle="yes">s</italic>)<italic toggle="yes">P</italic><sup>−1</sup> is diagonal, with diagonal terms either equal to 1 or equal to s. If we denote by <italic toggle="yes">a<sub>n</sub></italic> (resp. <italic toggle="yes">b<sub>n</sub></italic>) the number of diagonal terms equal to 1 (resp. s), then <xref ref-type="statement" rid="thm1" hwp:id="xref-statement-2-4" hwp:rel-id="statement-2">Theorem 1</xref> follows.</p></statement></sec><sec id="app1c" hwp:id="sec-23"><label>A.3</label><title hwp:id="title-28">Proof of <xref ref-type="statement" rid="thm2" hwp:id="xref-statement-3-4" hwp:rel-id="statement-3">Theorem 2</xref></title><statement hwp:id="statement-6"><label>Proof.</label><p hwp:id="p-68">Cohen et al. [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-4" hwp:rel-id="ref-11">11</xref>, Theorem 3.3] gives a general result about linear equivariant mapping. We first show that this result can be applied here, to show that these linear mappings are exactly the ones written as (<xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-5-1" hwp:rel-id="disp-formula-5">2</xref>) and (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-4" hwp:rel-id="disp-formula-6">3</xref>). For sake of clarity, we then provide a fully self-contained proof of the same result.</p><p hwp:id="p-69">Let us first show that (<xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-5-2" hwp:rel-id="disp-formula-5">2</xref>) and (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-5" hwp:rel-id="disp-formula-6">3</xref>) correspond to a particular case of Cohen et al. [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-5" hwp:rel-id="ref-11">11</xref>, Theorem 3.3]. Under the notations of [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-6" hwp:rel-id="ref-11">11</xref>], our group is <inline-formula hwp:id="inline-formula-62"><alternatives hwp:id="alternatives-74"><inline-graphic xlink:href="446953v2_inline63.gif" hwp:id="inline-graphic-62"/></alternatives></inline-formula>, a locally compact, semi-direct product group. We choose <inline-formula hwp:id="inline-formula-63"><alternatives hwp:id="alternatives-75"><inline-graphic xlink:href="446953v2_inline64.gif" hwp:id="inline-graphic-63"/></alternatives></inline-formula>, making the coset space <inline-formula hwp:id="inline-formula-64"><alternatives hwp:id="alternatives-76"><inline-graphic xlink:href="446953v2_inline65.gif" hwp:id="inline-graphic-64"/></alternatives></inline-formula>. Since our group is a semi direct product group, we have <italic toggle="yes">h</italic><sub>1</sub>(<italic toggle="yes">x, s</italic>) = <italic toggle="yes">s</italic>. The spaces <italic toggle="yes">F<sub>n</sub></italic> that we have considered are signals in <inline-formula hwp:id="inline-formula-65"><alternatives hwp:id="alternatives-77"><inline-graphic xlink:href="446953v2_inline66.gif" hwp:id="inline-graphic-65"/></alternatives></inline-formula> over the coset space, acted upon by the representation induced by <italic toggle="yes">ρ</italic>. Equivalently, they are sections of the associated vector bundle for the trivial case of a product group. Therefore, these <italic toggle="yes">F<sub><italic toggle="yes">n</italic></sub></italic> exactly coincide with the setting of Cohen et al. [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-7" hwp:rel-id="ref-11">11</xref>, Theorem 3.3] and {<italic toggle="yes">ϕ</italic> : <italic toggle="yes">F<sub>n</sub></italic> → <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub>|<italic toggle="yes">π</italic><sub><italic toggle="yes">n</italic>+1</sub><italic toggle="yes">ϕ</italic> = <italic toggle="yes">ϕπ<sub>n</sub></italic>} is exactly <inline-formula hwp:id="inline-formula-66"><alternatives hwp:id="alternatives-78"><inline-graphic xlink:href="446953v2_inline67.gif" hwp:id="inline-graphic-66"/></alternatives></inline-formula>. Then, by [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-8" hwp:rel-id="ref-11">11</xref>, Theorem 3.3], <italic toggle="yes">ϕ</italic>: <italic toggle="yes">F<sub>n</sub></italic> → <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> is equivariant if and only if it can be written as a convolution:
<disp-formula id="eqn2a" hwp:id="disp-formula-13"><alternatives hwp:id="alternatives-79"><graphic xlink:href="446953v2_eqn2a.gif" position="float" orientation="portrait" hwp:id="graphic-18"/></alternatives></disp-formula>
where the kernel <inline-formula hwp:id="inline-formula-67"><alternatives hwp:id="alternatives-80"><inline-graphic xlink:href="446953v2_inline68.gif" hwp:id="inline-graphic-67"/></alternatives></inline-formula> satisfies:
<disp-formula id="eqn6" hwp:id="disp-formula-14" hwp:rev-id="xref-disp-formula-14-1"><alternatives hwp:id="alternatives-81"><graphic xlink:href="446953v2_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-19"/></alternatives></disp-formula></p><p hwp:id="p-70">Using that for <inline-formula hwp:id="inline-formula-68"><alternatives hwp:id="alternatives-82"><inline-graphic xlink:href="446953v2_inline69.gif" hwp:id="inline-graphic-68"/></alternatives></inline-formula>, <italic toggle="yes">s</italic><sup>−1</sup> = <italic toggle="yes">s</italic>, and the triviality of this equation for <italic toggle="yes">s</italic> = 1, we get that (<xref ref-type="disp-formula" rid="eqn6" hwp:id="xref-disp-formula-14-1" hwp:rel-id="disp-formula-14">6</xref>) is equivalent to (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-6" hwp:rel-id="disp-formula-6">3</xref>)</p><p hwp:id="p-71">For sake of clarity and completeness, we now provide a more explicit and self-contained proof for (<xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-5-3" hwp:rel-id="disp-formula-5">2</xref>) and (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-7" hwp:rel-id="disp-formula-6">3</xref>), that follows the one of [<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">40</xref>, <xref ref-type="statement" rid="thm2" hwp:id="xref-statement-3-5" hwp:rel-id="statement-3">Theorem 2</xref>] in our specific setting. We first notice that any linear mapping <italic toggle="yes">ϕ</italic>; <italic toggle="yes">F<sub>n</sub></italic> → <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> can be written as
<disp-formula hwp:id="disp-formula-15"><alternatives hwp:id="alternatives-83"><graphic xlink:href="446953v2_ueqn8.gif" position="float" orientation="portrait" hwp:id="graphic-20"/></alternatives></disp-formula>
for some function <inline-formula hwp:id="inline-formula-69"><alternatives hwp:id="alternatives-84"><inline-graphic xlink:href="446953v2_inline70.gif" hwp:id="inline-graphic-69"/></alternatives></inline-formula>. For any <italic toggle="yes">g</italic> = <italic toggle="yes">ts</italic> ∈ <italic toggle="yes">G</italic>, the action of <italic toggle="yes">G</italic> on <italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic>+1</sub> gives:
<disp-formula id="eqn7" hwp:id="disp-formula-16" hwp:rev-id="xref-disp-formula-16-1"><alternatives hwp:id="alternatives-85"><graphic xlink:href="446953v2_eqn7.gif" position="float" orientation="portrait" hwp:id="graphic-21"/></alternatives></disp-formula></p><p hwp:id="p-72">Similarly, the action of <italic toggle="yes">G</italic> on <italic toggle="yes">F<sub>n</sub></italic> followed by <italic toggle="yes">ϕ</italic> gives:
<disp-formula id="eqn8" hwp:id="disp-formula-17" hwp:rev-id="xref-disp-formula-17-1"><alternatives hwp:id="alternatives-86"><graphic xlink:href="446953v2_eqn8.gif" position="float" orientation="portrait" hwp:id="graphic-22"/></alternatives></disp-formula>
where we made the change of variable <italic toggle="yes">y</italic> ↦ <italic toggle="yes">sy</italic> + <italic toggle="yes">t</italic> to get the last equality. <italic toggle="yes">ϕ</italic> is equivariant if and only if, for any <italic toggle="yes">g</italic> ∈ <italic toggle="yes">G</italic>, <italic toggle="yes">ϕ</italic> ○ <italic toggle="yes">π<sub>n</sub></italic>(<italic toggle="yes">g</italic>) = <italic toggle="yes">π</italic><sub><italic toggle="yes">n</italic>+1</sub>(<italic toggle="yes">g</italic>) ○ <italic toggle="yes">ϕ</italic>, which from (<xref ref-type="disp-formula" rid="eqn7" hwp:id="xref-disp-formula-16-1" hwp:rel-id="disp-formula-16">7</xref>) and (<xref ref-type="disp-formula" rid="eqn8" hwp:id="xref-disp-formula-17-1" hwp:rel-id="disp-formula-17">8</xref>) is equivalent to:
<disp-formula id="eqn9" hwp:id="disp-formula-18" hwp:rev-id="xref-disp-formula-18-1"><alternatives hwp:id="alternatives-87"><graphic xlink:href="446953v2_eqn9.gif" position="float" orientation="portrait" hwp:id="graphic-23"/></alternatives></disp-formula></p><p hwp:id="p-73">For any <inline-formula hwp:id="inline-formula-70"><alternatives hwp:id="alternatives-88"><inline-graphic xlink:href="446953v2_inline71.gif" hwp:id="inline-graphic-70"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-71"><alternatives hwp:id="alternatives-89"><inline-graphic xlink:href="446953v2_inline72.gif" hwp:id="inline-graphic-71"/></alternatives></inline-formula>, let us apply this equality to the function <italic toggle="yes">f</italic> ∈ <italic toggle="yes">F<sub>n</sub></italic> given by <italic toggle="yes">f</italic>(<italic toggle="yes">y</italic><sub>0</sub>) = <italic toggle="yes">v</italic> and <italic toggle="yes">f</italic>(<italic toggle="yes">y</italic>) = 0 for <italic toggle="yes">y</italic> ≠ <italic toggle="yes">y</italic><sub>0</sub>:
<disp-formula hwp:id="disp-formula-19"><alternatives hwp:id="alternatives-90"><graphic xlink:href="446953v2_ueqn9.gif" position="float" orientation="portrait" hwp:id="graphic-24"/></alternatives></disp-formula></p><p hwp:id="p-74">Since this must hold for any <inline-formula hwp:id="inline-formula-72"><alternatives hwp:id="alternatives-91"><inline-graphic xlink:href="446953v2_inline73.gif" hwp:id="inline-graphic-72"/></alternatives></inline-formula> this necessarily implies:
<disp-formula hwp:id="disp-formula-20"><alternatives hwp:id="alternatives-92"><graphic xlink:href="446953v2_ueqn10.gif" position="float" orientation="portrait" hwp:id="graphic-25"/></alternatives></disp-formula></p><p hwp:id="p-75">With the change of variable <italic toggle="yes">y</italic> = <italic toggle="yes">s</italic>(<italic toggle="yes">y</italic><sub>0</sub> − <italic toggle="yes">t</italic>), this is equivalent to:
<disp-formula hwp:id="disp-formula-21"><alternatives hwp:id="alternatives-93"><graphic xlink:href="446953v2_ueqn11.gif" position="float" orientation="portrait" hwp:id="graphic-26"/></alternatives></disp-formula>
which itself is equivalent to
<disp-formula id="eqn10" hwp:id="disp-formula-22" hwp:rev-id="xref-disp-formula-22-1"><alternatives hwp:id="alternatives-94"><graphic xlink:href="446953v2_eqn10.gif" position="float" orientation="portrait" hwp:id="graphic-27"/></alternatives></disp-formula>
where we used the fact that <italic toggle="yes">ρ</italic><sub><italic toggle="yes">n</italic>+1</sub>(<italic toggle="yes">s</italic>)<sup>2</sup> = <italic toggle="yes">ρ</italic><sub><italic toggle="yes">n</italic>+1</sub>(<italic toggle="yes">s</italic><sup>2</sup>) = <italic toggle="yes">I</italic> for any <inline-formula hwp:id="inline-formula-73"><alternatives hwp:id="alternatives-95"><inline-graphic xlink:href="446953v2_inline74.gif" hwp:id="inline-graphic-73"/></alternatives></inline-formula>. This must hold in particular for <italic toggle="yes">s</italic> = 1 and <italic toggle="yes">t</italic> = <italic toggle="yes">x</italic>, which gives:
<disp-formula hwp:id="disp-formula-23"><alternatives hwp:id="alternatives-96"><graphic xlink:href="446953v2_ueqn12.gif" position="float" orientation="portrait" hwp:id="graphic-28"/></alternatives></disp-formula>
i.e., <italic toggle="yes">k</italic> is necessarily translation invariant in the sense that there must exist a function <inline-formula hwp:id="inline-formula-74"><alternatives hwp:id="alternatives-97"><inline-graphic xlink:href="446953v2_inline75.gif" hwp:id="inline-graphic-74"/></alternatives></inline-formula> such that
<disp-formula hwp:id="disp-formula-24"><alternatives hwp:id="alternatives-98"><graphic xlink:href="446953v2_ueqn13.gif" position="float" orientation="portrait" hwp:id="graphic-29"/></alternatives></disp-formula></p><p hwp:id="p-76">From (<xref ref-type="disp-formula" rid="eqn10" hwp:id="xref-disp-formula-22-1" hwp:rel-id="disp-formula-22">10</xref>) we see that <italic toggle="yes">κ</italic> must satisfy
<disp-formula hwp:id="disp-formula-25"><alternatives hwp:id="alternatives-99"><graphic xlink:href="446953v2_ueqn14.gif" position="float" orientation="portrait" hwp:id="graphic-30"/></alternatives></disp-formula>
which boils down to the following constraint, after observing that the constraint is always true for <italic toggle="yes">s</italic> = 1 and is therefore only nontrivial for <italic toggle="yes">s</italic> = −1:
<disp-formula id="eqn11" hwp:id="disp-formula-26" hwp:rev-id="xref-disp-formula-26-1"><alternatives hwp:id="alternatives-100"><graphic xlink:href="446953v2_eqn11.gif" position="float" orientation="portrait" hwp:id="graphic-31"/></alternatives></disp-formula></p><p hwp:id="p-77">At this point, we have therefore shown that an equivariant linear function must have an expansion of the form
<disp-formula hwp:id="disp-formula-27"><alternatives hwp:id="alternatives-101"><graphic xlink:href="446953v2_ueqn15.gif" position="float" orientation="portrait" hwp:id="graphic-32"/></alternatives></disp-formula>
where <italic toggle="yes">κ</italic> must satisfy (<xref ref-type="disp-formula" rid="eqn11" hwp:id="xref-disp-formula-26-1" hwp:rel-id="disp-formula-26">11</xref>). Conversely, such a linear layer trivially satisfies (<xref ref-type="disp-formula" rid="eqn9" hwp:id="xref-disp-formula-18-1" hwp:rel-id="disp-formula-18">9</xref>), and is therefore equivariant. This proves (<xref ref-type="disp-formula" rid="eqn2" hwp:id="xref-disp-formula-5-4" hwp:rel-id="disp-formula-5">2</xref>) and (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-8" hwp:rel-id="disp-formula-6">3</xref>).</p><p hwp:id="p-78">To prove (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-5" hwp:rel-id="disp-formula-7">4</xref>), we simply rewrite (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-9" hwp:rel-id="disp-formula-6">3</xref>) using <xref ref-type="statement" rid="thm1" hwp:id="xref-statement-2-5" hwp:rel-id="statement-2">Theorem 1</xref>:
<disp-formula id="eqn12" hwp:id="disp-formula-28" hwp:rev-id="xref-disp-formula-28-1"><alternatives hwp:id="alternatives-102"><graphic xlink:href="446953v2_eqn12.gif" position="float" orientation="portrait" hwp:id="graphic-33"/></alternatives></disp-formula></p><p hwp:id="p-79">Thus writing the matrix <inline-formula hwp:id="inline-formula-75"><alternatives hwp:id="alternatives-103"><inline-graphic xlink:href="446953v2_inline76.gif" hwp:id="inline-graphic-75"/></alternatives></inline-formula> by blocs of sizes <italic toggle="yes">a</italic><sub><italic toggle="yes">n</italic>+1</sub> × <italic toggle="yes">a<sub>n</sub></italic>, <italic toggle="yes">a</italic><sub><italic toggle="yes">n</italic>+1</sub> × <italic toggle="yes">b<sub>n</sub></italic>, <italic toggle="yes">b</italic><sub><italic toggle="yes">n</italic>+1</sub> × <italic toggle="yes">a<sub>n</sub></italic> and <italic toggle="yes">b</italic><sub><italic toggle="yes">n</italic>+1</sub> × <italic toggle="yes">b<sub>n</sub></italic>, we have :
<disp-formula hwp:id="disp-formula-29"><alternatives hwp:id="alternatives-104"><graphic xlink:href="446953v2_ueqn16.gif" position="float" orientation="portrait" hwp:id="graphic-34"/></alternatives></disp-formula></p><p hwp:id="p-80">This gives us the equivalence (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-10" hwp:rel-id="disp-formula-6">3</xref>) ⇔ (<xref ref-type="disp-formula" rid="eqn12" hwp:id="xref-disp-formula-28-1" hwp:rel-id="disp-formula-28">12</xref>) ⇔ (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-6" hwp:rel-id="disp-formula-7">4</xref>).</p></statement></sec><sec id="app1d" hwp:id="sec-24" hwp:rev-id="xref-sec-24-1"><label>A.4</label><title hwp:id="title-29">Resolution of the constraint for other basis</title><p hwp:id="p-81">To go from an arbitrary representation (<italic toggle="yes">P, a, b</italic>) to another, we can write an odd/even kernel and change of basis. One may also solve the constraints (<xref ref-type="disp-formula" rid="eqn3" hwp:id="xref-disp-formula-6-11" hwp:rel-id="disp-formula-6">3</xref>) for specific representations, and save the need of multiplication by <italic toggle="yes">P</italic><sub><italic toggle="yes">n</italic>+1</sub> and <inline-formula hwp:id="inline-formula-76"><alternatives hwp:id="alternatives-105"><inline-graphic xlink:href="446953v2_inline77.gif" hwp:id="inline-graphic-76"/></alternatives></inline-formula> in (<xref ref-type="disp-formula" rid="eqn4" hwp:id="xref-disp-formula-7-7" hwp:rel-id="disp-formula-7">4</xref>). In this section, we solve the constraint in other basis, to go from one kind of representation (irrep or regular) to another. We just substitute the correct representation and see what constrained kernel it gives. The irrep and regular representations are in a basis such that they write as :
<disp-formula hwp:id="disp-formula-30"><alternatives hwp:id="alternatives-106"><graphic xlink:href="446953v2_ueqn17.gif" position="float" orientation="portrait" hwp:id="graphic-35"/></alternatives></disp-formula></p><p hwp:id="p-82">We get the following table of constraints :</p><table-wrap orientation="portrait" position="anchor" hwp:id="T1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/table1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><graphic xlink:href="446953v2_utbl1" position="float" orientation="portrait" hwp:id="graphic-36"/></table-wrap></sec><sec id="app1e" hwp:id="sec-25"><label>A.5</label><title hwp:id="title-30">Proof of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-5" hwp:rel-id="statement-4">Theorem 3</xref></title><p hwp:id="p-83">With a slight abuse of notations, in this section we denote the matrix <italic toggle="yes">ρ</italic>(−1) simply by <inline-formula hwp:id="inline-formula-77"><alternatives hwp:id="alternatives-107"><inline-graphic xlink:href="446953v2_inline78.gif" hwp:id="inline-graphic-77"/></alternatives></inline-formula>, and for any <inline-formula hwp:id="inline-formula-78"><alternatives hwp:id="alternatives-108"><inline-graphic xlink:href="446953v2_inline79.gif" hwp:id="inline-graphic-78"/></alternatives></inline-formula> we define <inline-formula hwp:id="inline-formula-79"><alternatives hwp:id="alternatives-109"><inline-graphic xlink:href="446953v2_inline80.gif" hwp:id="inline-graphic-79"/></alternatives></inline-formula>. We start with three technical lemmas, before proving <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-6" hwp:rel-id="statement-4">Theorem 3</xref>.</p><statement id="lem4" hwp:id="statement-7" hwp:rev-id="xref-statement-7-1 xref-statement-7-2"><label>Lemma 4.</label><p hwp:id="p-84"><italic toggle="yes">Let h</italic> : <inline-formula hwp:id="inline-formula-80"><alternatives hwp:id="alternatives-110"><inline-graphic xlink:href="446953v2_inline81.gif" hwp:id="inline-graphic-80"/></alternatives></inline-formula> <italic toggle="yes">be a continuous function with left and right derivatives at 0. If there exists</italic> <inline-formula hwp:id="inline-formula-81"><alternatives hwp:id="alternatives-111"><inline-graphic xlink:href="446953v2_inline82.gif" hwp:id="inline-graphic-81"/></alternatives></inline-formula> <italic toggle="yes">with</italic> |<italic toggle="yes">A</italic>| &gt; 1 <italic toggle="yes">such that</italic>
<disp-formula id="eqn13" hwp:id="disp-formula-31" hwp:rev-id="xref-disp-formula-31-1"><alternatives hwp:id="alternatives-112"><graphic xlink:href="446953v2_eqn13.gif" position="float" orientation="portrait" hwp:id="graphic-37"/></alternatives></disp-formula>
<italic toggle="yes">then h is a leaky ReLu function, i.e., there exists</italic> <inline-formula hwp:id="inline-formula-82"><alternatives hwp:id="alternatives-113"><inline-graphic xlink:href="446953v2_inline83.gif" hwp:id="inline-graphic-82"/></alternatives></inline-formula> <italic toggle="yes">such that</italic>
<disp-formula hwp:id="disp-formula-32"><alternatives hwp:id="alternatives-114"><graphic xlink:href="446953v2_ueqn18.gif" position="float" orientation="portrait" hwp:id="graphic-38"/></alternatives></disp-formula></p><p hwp:id="p-85"><italic toggle="yes">In addition, if A</italic> &lt; −1, <italic toggle="yes">then α</italic><sub>−</sub> = <italic toggle="yes">α</italic><sub>+</sub>, <italic toggle="yes">i.e., h is linear</italic>.</p><p hwp:id="p-86"><italic toggle="yes">Proof</italic>. <xref ref-type="disp-formula" rid="eqn13" hwp:id="xref-disp-formula-31-1" hwp:rel-id="disp-formula-31">Equation (13)</xref> implies <italic toggle="yes">h</italic>(0) = 0 and
<disp-formula hwp:id="disp-formula-33"><alternatives hwp:id="alternatives-115"><graphic xlink:href="446953v2_ueqn19.gif" position="float" orientation="portrait" hwp:id="graphic-39"/></alternatives></disp-formula>
which by simple induction gives more generally:
<disp-formula id="eqn14" hwp:id="disp-formula-34" hwp:rev-id="xref-disp-formula-34-1 xref-disp-formula-34-2"><alternatives hwp:id="alternatives-116"><graphic xlink:href="446953v2_eqn14.gif" position="float" orientation="portrait" hwp:id="graphic-40"/></alternatives></disp-formula></p><p hwp:id="p-87">The right-hand side of (<xref ref-type="disp-formula" rid="eqn14" hwp:id="xref-disp-formula-34-1" hwp:rel-id="disp-formula-34">14</xref>) for <italic toggle="yes">n</italic> = 2<italic toggle="yes">k</italic> converges to <inline-formula hwp:id="inline-formula-83"><alternatives hwp:id="alternatives-117"><inline-graphic xlink:href="446953v2_inline84.gif" hwp:id="inline-graphic-83"/></alternatives></inline-formula> when <italic toggle="yes">k</italic> → +∞, which by unicity of the limit must be equal to the left-hand side. As a result, for any <inline-formula hwp:id="inline-formula-84"><alternatives hwp:id="alternatives-118"><inline-graphic xlink:href="446953v2_inline85.gif" hwp:id="inline-graphic-84"/></alternatives></inline-formula>, i.e., <italic toggle="yes">h</italic> is a leaky ReLu function with <inline-formula hwp:id="inline-formula-85"><alternatives hwp:id="alternatives-119"><inline-graphic xlink:href="446953v2_inline86.gif" hwp:id="inline-graphic-85"/></alternatives></inline-formula> for <italic toggle="yes">s</italic> ∈ {−, +}. If in addition <italic toggle="yes">A</italic> &lt; −1, then (<xref ref-type="disp-formula" rid="eqn14" hwp:id="xref-disp-formula-34-2" hwp:rel-id="disp-formula-34">14</xref>) for <italic toggle="yes">n</italic> = 2<italic toggle="yes">k</italic> + 1 converges to <inline-formula hwp:id="inline-formula-86"><alternatives hwp:id="alternatives-120"><inline-graphic xlink:href="446953v2_inline87.gif" hwp:id="inline-graphic-86"/></alternatives></inline-formula> when <italic toggle="yes">k</italic> → +∞. By unicity of the limit, this implies <inline-formula hwp:id="inline-formula-87"><alternatives hwp:id="alternatives-121"><inline-graphic xlink:href="446953v2_inline88.gif" hwp:id="inline-graphic-87"/></alternatives></inline-formula>, i.e., <italic toggle="yes">α</italic><sub>−</sub> = <italic toggle="yes">α</italic><sub>+</sub>.</p></statement><statement id="lem5" hwp:id="statement-8" hwp:rev-id="xref-statement-8-1 xref-statement-8-2"><label>Lemma 5.</label><p hwp:id="p-88"><italic toggle="yes">Under the assumptions of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-7" hwp:rel-id="statement-4">Theorem 3</xref>, if</italic> <inline-formula hwp:id="inline-formula-88"><alternatives hwp:id="alternatives-122"><inline-graphic xlink:href="446953v2_inline89.gif" hwp:id="inline-graphic-88"/></alternatives></inline-formula> <italic toggle="yes">is equivariant and if there exists</italic> (<italic toggle="yes">i, j</italic>) ∈ [1, <italic toggle="yes">D</italic>]<sup>2</sup> <italic toggle="yes">such that ρ<sub>ij</sub></italic> ∉ {−1, 0, 1}, <italic toggle="yes">then necessarily</italic> <inline-formula hwp:id="inline-formula-89"><alternatives hwp:id="alternatives-123"><inline-graphic xlink:href="446953v2_inline90.gif" hwp:id="inline-graphic-89"/></alternatives></inline-formula> <italic toggle="yes">is a leaky ReLu function</italic>.</p><p hwp:id="p-89"><italic toggle="yes">Proof</italic>. For any (<italic toggle="yes">i, j</italic>), applying the equivariance constraint <italic toggle="yes">θ</italic>(<italic toggle="yes">ρx</italic>)<sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">ρθ</italic>(<italic toggle="yes">x</italic>)<sub><italic toggle="yes">i</italic></sub> to the vector <italic toggle="yes">x</italic> = <italic toggle="yes">ae<sub>j</sub></italic>, for any <inline-formula hwp:id="inline-formula-90"><alternatives hwp:id="alternatives-124"><inline-graphic xlink:href="446953v2_inline91.gif" hwp:id="inline-graphic-90"/></alternatives></inline-formula>, gives the equation:
<disp-formula hwp:id="disp-formula-35"><alternatives hwp:id="alternatives-125"><graphic xlink:href="446953v2_ueqn20.gif" position="float" orientation="portrait" hwp:id="graphic-41"/></alternatives></disp-formula></p><p hwp:id="p-90">If |<italic toggle="yes">ρ<sub>ij</sub></italic>| &gt; 1, we can rewrite it as
<disp-formula hwp:id="disp-formula-36"><alternatives hwp:id="alternatives-126"><graphic xlink:href="446953v2_ueqn21.gif" position="float" orientation="portrait" hwp:id="graphic-42"/></alternatives></disp-formula>
and if 0 &lt; |<italic toggle="yes">ρ<sub>ij</sub></italic>| &lt; 1 we can rewrite it as
<disp-formula hwp:id="disp-formula-37"><alternatives hwp:id="alternatives-127"><graphic xlink:href="446953v2_ueqn22.gif" position="float" orientation="portrait" hwp:id="graphic-43"/></alternatives></disp-formula></p><p hwp:id="p-91">In both cases, this is an equation of the form
<disp-formula hwp:id="disp-formula-38"><alternatives hwp:id="alternatives-128"><graphic xlink:href="446953v2_ueqn23.gif" position="float" orientation="portrait" hwp:id="graphic-44"/></alternatives></disp-formula>
where |<italic toggle="yes">A</italic>| &gt; 1. Subtracting to this equation the same equation written for <italic toggle="yes">a</italic> = 0 gives
<disp-formula id="eqn15" hwp:id="disp-formula-39"><alternatives hwp:id="alternatives-129"><graphic xlink:href="446953v2_eqn15.gif" position="float" orientation="portrait" hwp:id="graphic-45"/></alternatives></disp-formula></p><p hwp:id="p-92">By <xref ref-type="statement" rid="lem4" hwp:id="xref-statement-7-1" hwp:rel-id="statement-7">Lemma 4</xref>, <inline-formula hwp:id="inline-formula-91"><alternatives hwp:id="alternatives-130"><inline-graphic xlink:href="446953v2_inline92.gif" hwp:id="inline-graphic-91"/></alternatives></inline-formula> is a leaky ReLu function.</p></statement><statement id="lem6" hwp:id="statement-9" hwp:rev-id="xref-statement-9-1"><label>Lemma 6.</label><p hwp:id="p-93"><italic toggle="yes">Under the assumptions of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-8" hwp:rel-id="statement-4">Theorem 3</xref>, if</italic> <inline-formula hwp:id="inline-formula-92"><alternatives hwp:id="alternatives-131"><inline-graphic xlink:href="446953v2_inline93.gif" hwp:id="inline-graphic-92"/></alternatives></inline-formula> <italic toggle="yes">is equivariant and if there exists at least one row in ρ with at least two nonzero entry, then necessarily θ is an affine function</italic>.</p><p hwp:id="p-94"><italic toggle="yes">Proof</italic>. Let us suppose that <italic toggle="yes">ρ</italic> contains at least a row <italic toggle="yes">i</italic> with two nonzero entries, say <italic toggle="yes">ρ<sub>ij</sub></italic> ≠ 0 and <italic toggle="yes">ρ<sub>ik</sub></italic> ≠ 0. Then taking <italic toggle="yes">x</italic> = <italic toggle="yes">x<sub>j</sub>e<sub>j</sub></italic> + <italic toggle="yes">x<sub>k</sub>e<sub>k</sub></italic> with <italic toggle="yes">x<sub>j</sub></italic>, <inline-formula hwp:id="inline-formula-93"><alternatives hwp:id="alternatives-132"><inline-graphic xlink:href="446953v2_inline94.gif" hwp:id="inline-graphic-93"/></alternatives></inline-formula>, the equivariance constraint for the <italic toggle="yes">i</italic>-th dimension gives
<disp-formula hwp:id="disp-formula-40"><alternatives hwp:id="alternatives-133"><graphic xlink:href="446953v2_ueqn24.gif" position="float" orientation="portrait" hwp:id="graphic-46"/></alternatives></disp-formula>
with <italic toggle="yes">C</italic> = Σ<sub><italic toggle="yes">p</italic>∉{<italic toggle="yes">j,k</italic>}</sub> <italic toggle="yes">ρ<sub>ip</sub></italic>. Subtracting to this equation the same equation written for <italic toggle="yes">x<sub>j</sub></italic> = <italic toggle="yes">x<sub>k</sub></italic> = 0 allows us to remove the constant term and get
<disp-formula id="eqn16" hwp:id="disp-formula-41" hwp:rev-id="xref-disp-formula-41-1 xref-disp-formula-41-2"><alternatives hwp:id="alternatives-134"><graphic xlink:href="446953v2_eqn16.gif" position="float" orientation="portrait" hwp:id="graphic-47"/></alternatives></disp-formula></p><p hwp:id="p-95">We now prove that <inline-formula hwp:id="inline-formula-94"><alternatives hwp:id="alternatives-135"><inline-graphic xlink:href="446953v2_inline95.gif" hwp:id="inline-graphic-94"/></alternatives></inline-formula> is necessarily a leaky ReLu function, i.e., that there exist <inline-formula hwp:id="inline-formula-95"><alternatives hwp:id="alternatives-136"><inline-graphic xlink:href="446953v2_inline96.gif" hwp:id="inline-graphic-95"/></alternatives></inline-formula> such that <inline-formula hwp:id="inline-formula-96"><alternatives hwp:id="alternatives-137"><inline-graphic xlink:href="446953v2_inline97.gif" hwp:id="inline-graphic-96"/></alternatives></inline-formula>, with potentially <italic toggle="yes">α</italic><sub>+</sub> ≠ <italic toggle="yes">α</italic><sub>−</sub>. By <xref ref-type="statement" rid="lem5" hwp:id="xref-statement-8-1" hwp:rel-id="statement-8">Lemma 5</xref> this is true if |<italic toggle="yes">ρ<sub>ij</sub></italic>| ≠ 1 or |<italic toggle="yes">ρ<sub>ik</sub></italic>| ≠ 1, so we focus on the case |<italic toggle="yes">ρ<sub>ij</sub></italic>| = |<italic toggle="yes">ρ<sub>ik</sub></italic>| = 1, which we decompose in two subcases. First, if <italic toggle="yes">ρ<sub>ij</sub></italic> = <italic toggle="yes">ρ<sub>ik</sub></italic> = <italic toggle="yes">s</italic> with <italic toggle="yes">s</italic> ∈ {−1, 1}, then taking <italic toggle="yes">x<sub>j</sub></italic> = <italic toggle="yes">x<sub>k</sub></italic> = <italic toggle="yes">a</italic> in (<xref ref-type="disp-formula" rid="eqn16" hwp:id="xref-disp-formula-41-1" hwp:rel-id="disp-formula-41">16</xref>) gives <inline-formula hwp:id="inline-formula-97"><alternatives hwp:id="alternatives-138"><inline-graphic xlink:href="446953v2_inline98.gif" hwp:id="inline-graphic-97"/></alternatives></inline-formula>, for any <inline-formula hwp:id="inline-formula-98"><alternatives hwp:id="alternatives-139"><inline-graphic xlink:href="446953v2_inline99.gif" hwp:id="inline-graphic-98"/></alternatives></inline-formula>. Second, if <italic toggle="yes">ρ<sub>ij</sub></italic> = −<italic toggle="yes">ρ<sub>ik</sub></italic> = 1 (resp. <italic toggle="yes">ρ<sub>ij</sub></italic> = −<italic toggle="yes">ρ<sub>ik</sub></italic> = 1), then taking <italic toggle="yes">x<sub>j</sub></italic> = 2<italic toggle="yes">a</italic> and <italic toggle="yes">x<sub>k</sub></italic> = <italic toggle="yes">a</italic> (resp. <italic toggle="yes">x<sub>j</sub></italic> = <italic toggle="yes">a</italic> and <italic toggle="yes">x<sub>k</sub></italic> = 2<italic toggle="yes">a</italic>) gives <inline-formula hwp:id="inline-formula-99"><alternatives hwp:id="alternatives-140"><inline-graphic xlink:href="446953v2_inline100.gif" hwp:id="inline-graphic-99"/></alternatives></inline-formula>. In both subcases, by <xref ref-type="statement" rid="lem4" hwp:id="xref-statement-7-2" hwp:rel-id="statement-7">Lemma 4</xref>, <inline-formula hwp:id="inline-formula-100"><alternatives hwp:id="alternatives-141"><inline-graphic xlink:href="446953v2_inline101.gif" hwp:id="inline-graphic-100"/></alternatives></inline-formula> must be a leaky ReLu function.</p><p hwp:id="p-96">Knowing that <inline-formula hwp:id="inline-formula-101"><alternatives hwp:id="alternatives-142"><inline-graphic xlink:href="446953v2_inline102.gif" hwp:id="inline-graphic-101"/></alternatives></inline-formula> is a leaky ReLu function with coefficients <italic toggle="yes">α</italic><sub>+</sub> and <italic toggle="yes">α</italic><sub>−</sub>, in order to prove that <italic toggle="yes">θ</italic> is necessarily an affine function (i.e., that <inline-formula hwp:id="inline-formula-102"><alternatives hwp:id="alternatives-143"><inline-graphic xlink:href="446953v2_inline103.gif" hwp:id="inline-graphic-102"/></alternatives></inline-formula> is linear), we need to show that <italic toggle="yes">α</italic><sub>+</sub> = <italic toggle="yes">α</italic><sub>−</sub>. For that purpose, let us first suppose that <italic toggle="yes">ρ<sub>ij</sub></italic> and <italic toggle="yes">ρ<sub>ik</sub></italic> are both positive or both negative. Then there exists a pair <inline-formula hwp:id="inline-formula-103"><alternatives hwp:id="alternatives-144"><inline-graphic xlink:href="446953v2_inline104.gif" hwp:id="inline-graphic-103"/></alternatives></inline-formula> such that <italic toggle="yes">x<sub>j</sub></italic> &gt; 0, <italic toggle="yes">x<sub>k</sub></italic> &lt; 0 and <italic toggle="yes">ρ<sub>ij</sub>x<sub>j</sub></italic> + <italic toggle="yes">ρ<sub>ik</sub>x<sub>k</sub></italic> &lt; 0. Similarly, if <italic toggle="yes">ρ<sub>ij</sub></italic> and <italic toggle="yes">ρ<sub>ik</sub></italic> are of different signs, say without loss of generality <italic toggle="yes">ρ<sub>ij</sub></italic> &lt; 0 and <italic toggle="yes">ρ<sub>ik</sub></italic> &gt; 0, then any pair <inline-formula hwp:id="inline-formula-104"><alternatives hwp:id="alternatives-145"><inline-graphic xlink:href="446953v2_inline105.gif" hwp:id="inline-graphic-104"/></alternatives></inline-formula> such that <italic toggle="yes">x<sub>j</sub></italic> &gt; 0, <italic toggle="yes">x<sub>k</sub></italic> &lt; 0 satisfies <italic toggle="yes">ρ<sub>ij</sub>x<sub>j</sub></italic> + <italic toggle="yes">ρ<sub>ik</sub>x<sub>k</sub></italic> &lt; 0. In both cases, using the fact that <inline-formula hwp:id="inline-formula-105"><alternatives hwp:id="alternatives-146"><inline-graphic xlink:href="446953v2_inline106.gif" hwp:id="inline-graphic-105"/></alternatives></inline-formula> is linear on <inline-formula hwp:id="inline-formula-106"><alternatives hwp:id="alternatives-147"><inline-graphic xlink:href="446953v2_inline107.gif" hwp:id="inline-graphic-106"/></alternatives></inline-formula> and on <inline-formula hwp:id="inline-formula-107"><alternatives hwp:id="alternatives-148"><inline-graphic xlink:href="446953v2_inline108.gif" hwp:id="inline-graphic-107"/></alternatives></inline-formula>, (<xref ref-type="disp-formula" rid="eqn16" hwp:id="xref-disp-formula-41-2" hwp:rel-id="disp-formula-41">16</xref>) gives
<disp-formula hwp:id="disp-formula-42"><alternatives hwp:id="alternatives-149"><graphic xlink:href="446953v2_ueqn25.gif" position="float" orientation="portrait" hwp:id="graphic-48"/></alternatives></disp-formula></p></statement><p hwp:id="p-97">We are now ready to prove <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-9" hwp:rel-id="statement-4">Theorem 3</xref>.</p><statement hwp:id="statement-10"><label>Proof of Theorem 3.</label><p hwp:id="p-98">To characterize the functions <italic toggle="yes">θ</italic> and representations <italic toggle="yes">ρ</italic> such that <inline-formula hwp:id="inline-formula-108"><alternatives hwp:id="alternatives-150"><inline-graphic xlink:href="446953v2_inline109.gif" hwp:id="inline-graphic-108"/></alternatives></inline-formula> is equivariant, we proceed by a disjunction of cases on <italic toggle="yes">θ</italic>, depending on whether it is affine.</p><p hwp:id="p-99">If <italic toggle="yes">θ</italic> is affine, say <italic toggle="yes">θ</italic>(<italic toggle="yes">x</italic>) = <italic toggle="yes">αx</italic> + <italic toggle="yes">β</italic>, then <inline-formula hwp:id="inline-formula-109"><alternatives hwp:id="alternatives-151"><inline-graphic xlink:href="446953v2_inline110.gif" hwp:id="inline-graphic-109"/></alternatives></inline-formula> is equivariant if and only if, for any <inline-formula hwp:id="inline-formula-110"><alternatives hwp:id="alternatives-152"><inline-graphic xlink:href="446953v2_inline111.gif" hwp:id="inline-graphic-110"/></alternatives></inline-formula>. This is equivalent to
<disp-formula hwp:id="disp-formula-43"><alternatives hwp:id="alternatives-153"><graphic xlink:href="446953v2_ueqn26.gif" position="float" orientation="portrait" hwp:id="graphic-49"/></alternatives></disp-formula></p><p hwp:id="p-100">This shows that if <italic toggle="yes">θ</italic> is affine, then <inline-formula hwp:id="inline-formula-111"><alternatives hwp:id="alternatives-154"><inline-graphic xlink:href="446953v2_inline112.gif" hwp:id="inline-graphic-111"/></alternatives></inline-formula> is equivariant if and only <italic toggle="yes">β</italic> = 0, i.e., <italic toggle="yes">θ</italic> is linear (case 1 of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-10" hwp:rel-id="statement-4">Theorem 3</xref>), or <italic toggle="yes">ρ</italic><bold>1</bold> = <bold>1</bold> (case 2 of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-11" hwp:rel-id="statement-4">Theorem 3</xref>).</p><p hwp:id="p-101">If <italic toggle="yes">θ</italic> is not affine and <inline-formula hwp:id="inline-formula-112"><alternatives hwp:id="alternatives-155"><inline-graphic xlink:href="446953v2_inline113.gif" hwp:id="inline-graphic-112"/></alternatives></inline-formula> is equivariant, then by <xref ref-type="statement" rid="lem6" hwp:id="xref-statement-9-1" hwp:rel-id="statement-9">Lemma 6</xref> we know that <italic toggle="yes">ρ</italic> can have at most one nonzero entry per row. Since <italic toggle="yes">ρ</italic> is invertible, it must have at least one nonzero entry per row, so we conclude that if contains exactly one nonzero entry per row, hence a total of <italic toggle="yes">D</italic> nonzero entries. Being invertible, it must also contain at least one nonzero entry per column, so we conclude that it contains also exactly one nonzero entry per column. Using the fact that <italic toggle="yes">ρ</italic><sup>2</sup> = <italic toggle="yes">I</italic>, we can further clarify how nonzero entries must be organized:
<list list-type="bullet" hwp:id="list-3"><list-item hwp:id="list-item-7"><p hwp:id="p-102">For a nonzero entry <italic toggle="yes">ρ<sub>ii</sub></italic> ≠ 0 on the diagonal, we must have <inline-formula hwp:id="inline-formula-113"><alternatives hwp:id="alternatives-156"><inline-graphic xlink:href="446953v2_inline114.gif" hwp:id="inline-graphic-113"/></alternatives></inline-formula>, i.e., <italic toggle="yes">ρ<sub>ii</sub></italic> ∈ {−1, +1}.</p></list-item><list-item hwp:id="list-item-8"><p hwp:id="p-103">For an off-diagonal nonzero entry <italic toggle="yes">ρ<sub>ij</sub></italic> ≠ 0 with <italic toggle="yes">i</italic> ≠ <italic toggle="yes">j</italic>, we must have <italic toggle="yes">ρ<sub>ij</sub>ρ<sub>ji</sub></italic> = 1, i.e., <inline-formula hwp:id="inline-formula-114"><alternatives hwp:id="alternatives-157"><inline-graphic xlink:href="446953v2_inline115.gif" hwp:id="inline-graphic-114"/></alternatives></inline-formula>.</p></list-item></list></p><p hwp:id="p-104">Splitting the nonzero entries by sign, this implies that there exists a permutation matrix Π such that
<disp-formula id="eqn17" hwp:id="disp-formula-44" hwp:rev-id="xref-disp-formula-44-1 xref-disp-formula-44-2 xref-disp-formula-44-3"><alternatives hwp:id="alternatives-158"><graphic xlink:href="446953v2_eqn17.gif" position="float" orientation="portrait" hwp:id="graphic-50"/></alternatives></disp-formula>
for some <inline-formula hwp:id="inline-formula-115"><alternatives hwp:id="alternatives-159"><inline-graphic xlink:href="446953v2_inline116.gif" hwp:id="inline-graphic-115"/></alternatives></inline-formula> such that <italic toggle="yes">a</italic> + <italic toggle="yes">b</italic> + <italic toggle="yes">c</italic> + <italic toggle="yes">d</italic> = <italic toggle="yes">D</italic> and <inline-formula hwp:id="inline-formula-116"><alternatives hwp:id="alternatives-160"><inline-graphic xlink:href="446953v2_inline117.gif" hwp:id="inline-graphic-116"/></alternatives></inline-formula>. For any <italic toggle="yes">i</italic> ∈ [1, <italic toggle="yes">D</italic>], let us now denote by <italic toggle="yes">τ</italic>(<italic toggle="yes">i</italic>) the column corresponding to the nonzero entry of the <italic toggle="yes">i</italic>-th row of <inline-formula hwp:id="inline-formula-117"><alternatives hwp:id="alternatives-161"><inline-graphic xlink:href="446953v2_inline118.gif" hwp:id="inline-graphic-117"/></alternatives></inline-formula>, i.e., the only index such that <inline-formula hwp:id="inline-formula-118"><alternatives hwp:id="alternatives-162"><inline-graphic xlink:href="446953v2_inline119.gif" hwp:id="inline-graphic-118"/></alternatives></inline-formula>. Then the action of <inline-formula hwp:id="inline-formula-119"><alternatives hwp:id="alternatives-163"><inline-graphic xlink:href="446953v2_inline120.gif" hwp:id="inline-graphic-119"/></alternatives></inline-formula> on a vector <inline-formula hwp:id="inline-formula-120"><alternatives hwp:id="alternatives-164"><inline-graphic xlink:href="446953v2_inline121.gif" hwp:id="inline-graphic-120"/></alternatives></inline-formula> has the simple form <inline-formula hwp:id="inline-formula-121"><alternatives hwp:id="alternatives-165"><inline-graphic xlink:href="446953v2_inline122.gif" hwp:id="inline-graphic-121"/></alternatives></inline-formula>. By writing the equivariance property <inline-formula hwp:id="inline-formula-122"><alternatives hwp:id="alternatives-166"><inline-graphic xlink:href="446953v2_inline123.gif" hwp:id="inline-graphic-122"/></alternatives></inline-formula> coordinate by coordinate, we can therefore say that <inline-formula hwp:id="inline-formula-123"><alternatives hwp:id="alternatives-167"><inline-graphic xlink:href="446953v2_inline124.gif" hwp:id="inline-graphic-123"/></alternatives></inline-formula> is equivariant if and only if:
<disp-formula id="eqn18" hwp:id="disp-formula-45" hwp:rev-id="xref-disp-formula-45-1 xref-disp-formula-45-2"><alternatives hwp:id="alternatives-168"><graphic xlink:href="446953v2_eqn18.gif" position="float" orientation="portrait" hwp:id="graphic-51"/></alternatives></disp-formula></p><p hwp:id="p-105">Let us now consider two possible cases:
<list list-type="bullet" hwp:id="list-4"><list-item hwp:id="list-item-9"><p hwp:id="p-106">If there exists <italic toggle="yes">i</italic> ∈ [1, <italic toggle="yes">D</italic>] such that <inline-formula hwp:id="inline-formula-124"><alternatives hwp:id="alternatives-169"><inline-graphic xlink:href="446953v2_inline125.gif" hwp:id="inline-graphic-124"/></alternatives></inline-formula>, then by <xref ref-type="statement" rid="lem5" hwp:id="xref-statement-8-2" hwp:rel-id="statement-8">Lemma 5</xref> <inline-formula hwp:id="inline-formula-125"><alternatives hwp:id="alternatives-170"><inline-graphic xlink:href="446953v2_inline126.gif" hwp:id="inline-graphic-125"/></alternatives></inline-formula> is a leaky ReLu function, i.e., there exist <inline-formula hwp:id="inline-formula-126"><alternatives hwp:id="alternatives-171"><inline-graphic xlink:href="446953v2_inline127.gif" hwp:id="inline-graphic-126"/></alternatives></inline-formula> such that <inline-formula hwp:id="inline-formula-127"><alternatives hwp:id="alternatives-172"><inline-graphic xlink:href="446953v2_inline128.gif" hwp:id="inline-graphic-127"/></alternatives></inline-formula>, <italic toggle="yes">θ</italic>(<italic toggle="yes">x</italic>) <italic toggle="yes">α</italic><sub><italic toggle="yes">sign</italic>(<italic toggle="yes">x</italic>)</sub><italic toggle="yes">x</italic> + <italic toggle="yes">β</italic>. In that case, by (<xref ref-type="disp-formula" rid="eqn18" hwp:id="xref-disp-formula-45-1" hwp:rel-id="disp-formula-45">18</xref>), <inline-formula hwp:id="inline-formula-128"><alternatives hwp:id="alternatives-173"><inline-graphic xlink:href="446953v2_inline129.gif" hwp:id="inline-graphic-128"/></alternatives></inline-formula> is equivariant if and only if:
<disp-formula id="eqn19" hwp:id="disp-formula-46" hwp:rev-id="xref-disp-formula-46-1 xref-disp-formula-46-2"><alternatives hwp:id="alternatives-174"><graphic xlink:href="446953v2_eqn19.gif" position="float" orientation="portrait" hwp:id="graphic-52"/></alternatives></disp-formula>
where the first equivalence comes from identifying the coefficients of the linear equation in <italic toggle="yes">x</italic> on <inline-formula hwp:id="inline-formula-129"><alternatives hwp:id="alternatives-175"><inline-graphic xlink:href="446953v2_inline130.gif" hwp:id="inline-graphic-129"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-130"><alternatives hwp:id="alternatives-176"><inline-graphic xlink:href="446953v2_inline131.gif" hwp:id="inline-graphic-130"/></alternatives></inline-formula>, and the second equivalence comes from the observation that the two conditions in <italic toggle="yes">α</italic> in the first equivalence are themselves equivalent to each other, so we can keep only one of them, and that the condition on <italic toggle="yes">β</italic> is equivalent to <italic toggle="yes">β</italic> = 0 since we assume the existence of an <italic toggle="yes">i</italic> ∈ [1, <italic toggle="yes">D</italic>] such that <inline-formula hwp:id="inline-formula-131"><alternatives hwp:id="alternatives-177"><inline-graphic xlink:href="446953v2_inline132.gif" hwp:id="inline-graphic-131"/></alternatives></inline-formula>. Since we assume that <italic toggle="yes">θ</italic> is not affine, we can not have <italic toggle="yes">α</italic><sub>−</sub> = <italic toggle="yes">α</italic><sub>+</sub>, which by (<xref ref-type="disp-formula" rid="eqn19" hwp:id="xref-disp-formula-46-1" hwp:rel-id="disp-formula-46">19</xref>) rules out the possibility of having negative entries in <inline-formula hwp:id="inline-formula-132"><alternatives hwp:id="alternatives-178"><inline-graphic xlink:href="446953v2_inline133.gif" hwp:id="inline-graphic-132"/></alternatives></inline-formula>, i.e., necessarily <italic toggle="yes">b</italic> = <italic toggle="yes">d</italic> = 0 in (<xref ref-type="disp-formula" rid="eqn17" hwp:id="xref-disp-formula-44-1" hwp:rel-id="disp-formula-44">17</xref>). If that is not the case, then the condition on <italic toggle="yes">α</italic> in (<xref ref-type="disp-formula" rid="eqn19" hwp:id="xref-disp-formula-46-2" hwp:rel-id="disp-formula-46">19</xref>) is automatically met for all <italic toggle="yes">i</italic> ∈ [1, <italic toggle="yes">D</italic>], so we have that <inline-formula hwp:id="inline-formula-133"><alternatives hwp:id="alternatives-179"><inline-graphic xlink:href="446953v2_inline134.gif" hwp:id="inline-graphic-133"/></alternatives></inline-formula> is equivariant if and only if <italic toggle="yes">β</italic> = 0, i.e., if and only if <italic toggle="yes">θ</italic> is a leaky ReLu function. This is the second statement in Case 3 of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-12" hwp:rel-id="statement-4">Theorem 3</xref>, when we further notice that when <italic toggle="yes">b</italic> = 0 the only entry in <inline-formula hwp:id="inline-formula-134"><alternatives hwp:id="alternatives-180"><inline-graphic xlink:href="446953v2_inline135.gif" hwp:id="inline-graphic-134"/></alternatives></inline-formula> that can have been different from −1 and 1 is a λ<sub><italic toggle="yes">i</italic></sub> in (<xref ref-type="disp-formula" rid="eqn17" hwp:id="xref-disp-formula-44-2" hwp:rel-id="disp-formula-44">17</xref>).</p></list-item><list-item hwp:id="list-item-10"><p hwp:id="p-107">If for all <italic toggle="yes">i</italic> ∈ [1, <italic toggle="yes">D</italic>], <inline-formula hwp:id="inline-formula-135"><alternatives hwp:id="alternatives-181"><inline-graphic xlink:href="446953v2_inline136.gif" hwp:id="inline-graphic-135"/></alternatives></inline-formula>, then (<xref ref-type="disp-formula" rid="eqn17" hwp:id="xref-disp-formula-44-3" hwp:rel-id="disp-formula-44">17</xref>) simplifies as
<disp-formula hwp:id="disp-formula-47"><alternatives hwp:id="alternatives-182"><graphic xlink:href="446953v2_ueqn27.gif" position="float" orientation="portrait" hwp:id="graphic-53"/></alternatives></disp-formula></p><p hwp:id="p-108">In that case, the equivariance condition (<xref ref-type="disp-formula" rid="eqn18" hwp:id="xref-disp-formula-45-2" hwp:rel-id="disp-formula-45">18</xref>) is particularly simple, and true for any <italic toggle="yes">θ</italic> for positive values. For each <italic toggle="yes">i</italic> such that <inline-formula hwp:id="inline-formula-136"><alternatives hwp:id="alternatives-183"><inline-graphic xlink:href="446953v2_inline137.gif" hwp:id="inline-graphic-136"/></alternatives></inline-formula> it reads <inline-formula hwp:id="inline-formula-137"><alternatives hwp:id="alternatives-184"><inline-graphic xlink:href="446953v2_inline138.gif" hwp:id="inline-graphic-137"/></alternatives></inline-formula>, −<italic toggle="yes">θ</italic>(<italic toggle="yes">x</italic>) = <italic toggle="yes">θ</italic>(−<italic toggle="yes">x</italic>), and is therefore true if and only if <italic toggle="yes">θ</italic> is odd. Noticing that the latter constraint occurs if and only if <italic toggle="yes">b</italic> + <italic toggle="yes">d</italic> &gt; 0 finally leads to the first and third statements in Case 3 of <xref ref-type="statement" rid="thm3" hwp:id="xref-statement-4-13" hwp:rel-id="statement-4">Theorem 3</xref>.</p></list-item></list></p></statement></sec><sec id="app1f" hwp:id="sec-26"><label>A.6</label><title hwp:id="title-31">Additional result</title><sec id="app1f1" hwp:id="sec-27" hwp:rev-id="xref-sec-27-1"><label>A.6.1</label><title hwp:id="title-32">Effect of data augmentation and size for non-equivariant models</title><p hwp:id="p-109">Given a non-equivariant model, a simple way to let it “learn” to be equivariant is to train it with data augmentation, where for each sequence in the training set we add its reverse complement to the training set. This doubles the size of the training set, which increases the training time. If we compare such a non-equivariant model with an equivariant model with the same number of channels in each layers, then it has about twice the same number of free parameters to train, and we therefore call it “big”; as an alternative, one may want to restrict the number of channels in each layer to enforce the same number of parameters as the equivariant model. To assess the benefits of data augmentation and number of channels, we plot in <xref ref-type="fig" rid="figA6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref> the performance of a standard, non-equivariant model with or without data augmentation, and with the same number of channels or half of it, on the binary classification tasks. We see that the number of channels has no significant impact on the performance, but that data augmentation has a significant positive impact. In the main text, we therefore restrict ourselves to the standard model with data augmentation as non-equivariant baseline model.</p><fig id="figA6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIGA6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figA6</object-id><label>Figure 6:</label><caption hwp:id="caption-6"><p hwp:id="p-110">Binary task performance of a standard, non-equivariant model trained with (“Aug”) or without (“NoAug”) data augmentation, and with more (“Big”) or less (“Standard”) channels.</p></caption><graphic xlink:href="446953v2_figA6" position="float" orientation="portrait" hwp:id="graphic-54"/></fig></sec><sec id="app1f2" hwp:id="sec-28" hwp:rev-id="xref-sec-28-1"><label>A.6.2</label><title hwp:id="title-33">Comparison of learning curves</title><p hwp:id="p-111">Because equivariant model are supposed to converge faster, we looked into the learning curves of our models, i.e., how the test performance increases as a function of the number of epochs during training. However, we do not see a major difference in the learning dynamics between the equivariant and non equivariant models.</p><fig id="figA7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.06.03.446953v2/FIGA7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figA7</object-id><label>Figure 7:</label><caption hwp:id="caption-7"><p hwp:id="p-112">AuROC performance of the four different models on the three binary classification problems CTCF, MAX and SPI1, as well as their average over the course of learning.</p></caption><graphic xlink:href="446953v2_figA7" position="float" orientation="portrait" hwp:id="graphic-55"/></fig></sec></sec></app></app-group><fn-group hwp:id="fn-group-1"><fn id="fn1" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1"><label>1</label><p hwp:id="p-113">As of May, 2021: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.ebi.ac.uk/ena" ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/ena" hwp:id="ext-link-4">https://www.ebi.ac.uk/ena</ext-link></p></fn><fn id="fn2" hwp:id="fn-2" hwp:rev-id="xref-fn-2-1"><label>2</label><p hwp:id="p-114">A leaky ReLu function is <italic toggle="yes">θ</italic>(<italic toggle="yes">x</italic>) =<italic toggle="yes">α</italic><sub><italic toggle="yes">sign</italic>(<italic toggle="yes">x</italic>)</sub><italic toggle="yes">x</italic> for some <inline-formula hwp:id="inline-formula-138"><alternatives hwp:id="alternatives-185"><inline-graphic xlink:href="446953v2_inline56.gif" hwp:id="inline-graphic-138"/></alternatives></inline-formula>.</p></fn><fn id="fn3" hwp:id="fn-3" hwp:rev-id="xref-fn-3-1"><label>3</label><p hwp:id="p-115">code available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/Vincentx15/Equi-RC" ext-link-type="uri" xlink:href="https://github.com/Vincentx15/Equi-RC" hwp:id="ext-link-5">https://github.com/Vincentx15/Equi-RC</ext-link></p></fn></fn-group></back></article>
