<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/478347</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;478347</article-id><article-id pub-id-type="other" hwp:sub-type="slug">478347</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">478347</article-id><article-id pub-id-type="other" hwp:sub-type="tag">478347</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Neural Basis of the Sound-Symbolic Crossmodal Correspondence Between Auditory Pseudowords and Visual Shapes</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author: K. Sathian, Department of Neurology, Penn State Health Milton S. Hershey Medical Center 30 Hope Drive, PO Box 859, Mail Code EC037 Hershey, PA 17033-0859, USA, Tel: 717-531-1801, Fax: 717-531-0384, Email: <email hwp:id="email-1">ksathian@pennstatehealth.psu.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="McCormick Kelly"><surname>McCormick</surname><given-names>Kelly</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Lacey Simon"><surname>Lacey</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Stilla Randall"><surname>Stilla</surname><given-names>Randall</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Nygaard Lynne C."><surname>Nygaard</surname><given-names>Lynne C.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Sathian K."><surname>Sathian</surname><given-names>K.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-2" hwp:rel-id="aff-4">4</xref><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">Department of Psychology, Emory University</institution>, Atlanta, GA 30322, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Winship Cancer Institute, Emory University</institution>, Atlanta, GA 30322, <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2"><label>3</label><institution hwp:id="institution-3">Department of Neurology, Penn State College of Medicine</institution>, Hershey, PA 17033-0859, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1 xref-aff-4-2"><label>4</label><institution hwp:id="institution-4">Neural and Behavioral Sciences Penn State College of Medicine</institution>, Hershey, PA 17033-0859, <country>USA</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Psychology, Milton S. Hershey Medical Center, Penn State College of Medicine</institution>, Hershey, PA 17033-0859, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-11-28T07:03:09-08:00">
    <day>28</day><month>11</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-05-17T20:18:13-07:00">
    <day>17</day><month>5</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-11-28T07:09:25-08:00">
    <day>28</day><month>11</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-05-17T20:22:01-07:00">
    <day>17</day><month>5</month><year>2021</year>
  </pub-date><elocation-id>478347</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-11-26"><day>26</day><month>11</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2021-05-17"><day>17</day><month>5</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-05-17"><day>17</day><month>5</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="478347.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/478347v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="478347.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/478347v3/478347v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/478347v3/478347v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">ABSTRACT</title><p hwp:id="p-2">Sound symbolism refers to the association between the sounds of words and their meanings, often studied using the crossmodal correspondence between auditory pseudowords, e.g. ‘takete’ or ‘maluma’, and pointed or rounded visual shapes, respectively. In a functional magnetic resonance imaging study, participants were presented with pseudoword-shape pairs that were sound-symbolically congruent or incongruent. We found no significant congruency effects in the blood oxygenation level-dependent (BOLD) signal when participants were attending to visual shapes. During attention to auditory pseudowords, however, we observed greater BOLD activity for incongruent compared to congruent audiovisual pairs bilaterally in the intraparietal sulcus and supramarginal gyrus, and in the left middle frontal gyrus. We compared this activity to independent functional contrasts designed to test competing explanations of sound symbolism, but found no evidence for mediation via language, and only limited evidence for accounts based on multisensory integration and a general magnitude system. Instead, we suggest that the observed incongruency effects are likely to reflect phonological processing and/or multisensory attention. These findings advance our understanding of sound-to-meaning mapping in the brain.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">multisensory</kwd><kwd hwp:id="kwd-2">magnitude</kwd><kwd hwp:id="kwd-3">semantic</kwd><kwd hwp:id="kwd-4">phonology</kwd><kwd hwp:id="kwd-5">attention</kwd><kwd hwp:id="kwd-6">congruency effect</kwd></kwd-group><counts><page-count count="70"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-4">INTRODUCTION</title><p hwp:id="p-4">Although the relationship between the sound and meaning of a word is often considered primarily arbitrary (de Saussure, 1916/2009; <xref ref-type="bibr" rid="c88" hwp:id="xref-ref-88-1" hwp:rel-id="ref-88">Pinker, 1999</xref>; <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Jackendoff, 2002</xref>; but see also <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Joseph, 2015</xref>), there is evidence that sound-meaning mappings occur reliably in a variety of natural languages (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Blasi et al., 2016</xref>): such mappings are termed ‘sound-symbolic’ (e.g., Svantesson, 2017). A clear example of sound-meaning mapping is onomatopoeia, in which the sound of a word mimics the sound that the word represents (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Catricalà and Guidi, 2015</xref>; <xref ref-type="bibr" rid="c101" hwp:id="xref-ref-101-1" hwp:rel-id="ref-101">Schmidtke et al., 2014</xref>), for example: ‘fizzle’, ‘bang’, or ‘splash’. The sound-symbolic relation in onomatopoeia is within-modal, i.e., word sounds are used to index sound-related meanings, but in other classes of words, sounds can index visual, tactile, or other sensory meanings, i.e., the sound-symbolic relation is crossmodal. For example, Japanese has a distinct class of words called mimetics including ‘<italic toggle="yes">kirakira’</italic> (flickering light) and ‘<italic toggle="yes">nurunuru’</italic> (the tactile sensation of sliminess: <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Akita and Tsujimura, 2016</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Kita, 1997</xref>). Sound symbolism is often studied by examining the crossmodal correspondence between auditory pseudowords and two-dimensional visual shapes: e.g. ‘takete’ or ‘kiki’ are matched with pointed shapes whereas ‘maluma’ or ‘bouba’ are matched with rounded shapes (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Köhler, 1929</xref>, <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">1947</xref>; <xref ref-type="bibr" rid="c93" hwp:id="xref-ref-93-1" hwp:rel-id="ref-93">Ramachandran and Hubbard, 2001</xref>). In this kind of audiovisual pairing, the shapes can be considered as representing potential referents or meanings for the pseudowords<sup>1</sup>. However, the conditions under which sound-symbolic associations arise are not clear. <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-1" hwp:rel-id="ref-110">Spence (2011)</xref> proposed that crossmodal correspondences may be rooted in statistical, structural or semantic origins; these closely approximate explanations for sound-symbolic associations proposed by <xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-1" hwp:rel-id="ref-105">Sidhu and Pexman (2018)</xref>.</p><p hwp:id="p-5">One possible explanation for sound symbolism is that the relationships between sound-symbolic words and their visual or semantic referents reflect audiovisual statistical regularities in the natural environment that are integrated into a single perceptual object, for example, the sight of a stone thrown into water and the sound of it splashing might give rise to the onomatopoeic sound-to-meaning correspondence mentioned above. Such regularities may also underlie other crossmodal correspondences (<xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-2" hwp:rel-id="ref-110">Spence, 2011</xref>), for example that between high/low auditory pitch and high/low auditory and visual elevation (<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Jamal et al., 2017</xref>), since high-pitched sounds tend to emanate from high locations and low-pitched sounds from low locations (<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">Parise et al., 2014</xref>).</p><p hwp:id="p-6">Thus, sound symbolism might be related to a more general multisensory integration process in which auditory and visual features are linked (Kovi et al., 2010) and which may also underlie other crossmodal correspondences (<xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-2" hwp:rel-id="ref-105">Sidhu and Pexman, 2018</xref>). If so, neural activity related to sound-symbolic processing might co-localize with activity related to multisensory integration, e.g. in the superior temporal sulcus (STS) when audiovisual synchrony (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Beauchamp, 2005a</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">b</xref>; <xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-1" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-1" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Marchant et al., 2012</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Erickson et al., 2014</xref>) or audiovisual identity (<xref ref-type="bibr" rid="c102" hwp:id="xref-ref-102-1" hwp:rel-id="ref-102">Sestieri et al., 2006</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Erickson et al., 2014</xref>) are manipulated, or in regions such as the intraparietal sulcus (IPS) when audiovisual spatial congruency is manipulated (<xref ref-type="bibr" rid="c102" hwp:id="xref-ref-102-2" hwp:rel-id="ref-102">Sestieri et al., 2006</xref>).</p><p hwp:id="p-7">Sensory features can often be characterized along polar dimensions of magnitude where one end is ‘more than’ the other (<xref ref-type="bibr" rid="c108" hwp:id="xref-ref-108-1" hwp:rel-id="ref-108">Smith and Sera, 1992</xref>). Both visuospatial attributes of shapes (e.g., size, spatial frequency) and acoustic-phonetic attributes of speech sounds (e.g., sonority, formant frequencies) could be encoded by a domain-general magnitude system (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Dehaene et al., 2003</xref>; <xref ref-type="bibr" rid="c121" hwp:id="xref-ref-121-1" hwp:rel-id="ref-121">Walsh, 2003</xref>; <xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Lourenco and Longo, 2011</xref>) in which different attributes might become associated by virtue of occupying similar positions along magnitude dimensions. Such crossmodal magnitude relations have been proposed to underpin some crossmodal correspondences (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-2" hwp:rel-id="ref-60">Lourenco and Longo, 2011</xref>; <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-3" hwp:rel-id="ref-110">Spence, 2011</xref>) as well as sound-symbolic associations (<xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-3" hwp:rel-id="ref-105">Sidhu and Pexman, 2018</xref>). <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-4" hwp:rel-id="ref-110">Spence (2011)</xref> referred to this possibility as a “structural” account emerging as a consequence of modality-independent neural coding mechanisms, e.g. the neural coding of stimulus magnitude in neuronal firing rates across multiple sensory modalities. Thus, in sound-symbolic language, magnitude representations may serve to link the sound structure of spoken language with object shapes (which can vary along dimensions of pointedness, roundedness, or other shape attributes). That judgments of the sounds of language vary systematically along such dimensions is supported by findings that pseudowords with varying collections of phonetic features are readily placed along continua of pointedness and roundedness (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">McCormick et al., 2015</xref>), and that ratings transition from rounded to pointed with increasing vocal roughness, as indexed by a number of acoustic parameters (<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Lacey et al., 2020</xref>). On this account, activity related to sound symbolism might be expected in the intraparietal sulcus (IPS), a region involved in processing both numerical and non-numerical (e.g., luminance) magnitude (<xref ref-type="bibr" rid="c99" hwp:id="xref-ref-99-1" hwp:rel-id="ref-99">Sathian et al., 1999</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Eger et al., 2003</xref>; <xref ref-type="bibr" rid="c121" hwp:id="xref-ref-121-2" hwp:rel-id="ref-121">Walsh, 2003</xref>; <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-1" hwp:rel-id="ref-87">Pinel et al., 2004</xref>; <xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">Piazza et al., 2004</xref>, <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">2007</xref>; Sokolowski et al., 2017).</p><p hwp:id="p-8">The third possible explanation for crossmodal correspondences proposed by <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-5" hwp:rel-id="ref-110">Spence (2011)</xref> is semantic mediation. By this account, an unfamiliar word or pseudoword could be associated with a particular meaning by virtue of its similarity to other, real, words since crossmodal sound-symbolic associations exist in natural language, for example, ‘balloon’ and ‘spike’ for rounded and pointed shapes (<xref ref-type="bibr" rid="c115" hwp:id="xref-ref-115-1" hwp:rel-id="ref-115">Sučević et al., 2015</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">Blasi et al., 2016</xref>; <xref ref-type="bibr" rid="c106" hwp:id="xref-ref-106-1" hwp:rel-id="ref-106">Sidhu et al., 2021</xref>). Familiar words such as these have established semantic, as well as sound-symbolic, associations. Thus, when assigning ‘maluma’ to a rounded shape (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-2" hwp:rel-id="ref-48">Köhler, 1929</xref>, <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-2" hwp:rel-id="ref-49">1947</xref>), people might invoke real words with similar phonological content (for example, ‘balloon’). This semantic mediation account predicts that pseudoword-shape associations might activate regions of the left hemisphere language network.</p><p hwp:id="p-9">Despite extensive behavioral investigations of sound symbolism, there have been relatively few studies of its neural basis and each has some limitations. Some studies employed a variety of sound-symbolic words and referent types (Kovi et al., 2010; <xref ref-type="bibr" rid="c96" hwp:id="xref-ref-96-1" hwp:rel-id="ref-96">Revill et al., 2014</xref>; <xref ref-type="bibr" rid="c115" hwp:id="xref-ref-115-2" hwp:rel-id="ref-115">Sučević et al., 2015</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Lockwood et al., 2016</xref>), which may obscure processing differences between different sound-symbolic effects (<xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-4" hwp:rel-id="ref-105">Sidhu and Pexman, 2018</xref>). Some earlier functional magnetic resonance imaging (fMRI) studies did not attempt to distinguish between potential explanations (<xref ref-type="bibr" rid="c96" hwp:id="xref-ref-96-2" hwp:rel-id="ref-96">Revill et al., 2014</xref>; <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen, 2019</xref>); whereas EEG studies (Kovi et al., 2010; <xref ref-type="bibr" rid="c115" hwp:id="xref-ref-115-3" hwp:rel-id="ref-115">Sučević et al., 2015</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-2" hwp:rel-id="ref-58">Lockwood et al., 2016</xref>), while providing excellent temporal information, may not have sufficient spatial resolution to identify the anatomical sources of activity in the brain.</p><p hwp:id="p-10">In the present study, we used fMRI to investigate cerebral cortical localization of differential activations for congruent vs. incongruent sound-symbolic crossmodal correspondences between auditory pseudowords and visual shapes. We chose to rely on implicit correspondences by presenting audiovisual pseudoword-shape pairs, asking participants to discriminate either the auditory pseudowords or the visual shapes. In order to investigate the competing explanations outlined above, we conducted three independent task contrasts in the same individuals, reflecting multisensory integration, magnitude estimation, and language processing. We hypothesized that a potential role for one of these accounts would be supported by finding cortical activations common to one of these independent task contrasts and the sound-symbolic (in)congruency between pseudowords and shapes. On the other hand, cortical activations lacking overlap with one of the independent task contrasts would require an alternative explanation. Finally, few studies have examined individual differences in crossmodal correspondences. We addressed this by administering the Object-Spatial Imagery and Verbal Questionnaire (OSIVQ: <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Blazhenkova and Kozhevnikov, 2009</xref>). We hypothesized that individuals with a preference for verbal processing might be more adept at assigning potential meanings to pseudowords; this hypothesis would be supported by significant correlations between task performance and/or neural activity with the verbal sub-scale of the OSIVQ. Additionally, object imagers might be more apt to visualize the shapes associated with pseudowords since such individuals typically integrate multiple sources of information about an object, compared to the more schematic representations of spatial imagers (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">Blazhenkova and Kozhevnikov, 2009</xref>; <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Lacey et al., 2011</xref>). This hypothesis would be supported by significant correlations between task performance and/or neural activity with preferences for object, rather than spatial, imagery.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-5">MATERIALS AND METHODS</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-6">Participants</title><p hwp:id="p-11">Twenty participants took part in this study, but one was excluded for excessive movement in the scanner (&gt; 1.5mm), leaving a final sample of 19 (9 male, 10 female; mean age 25 years, 1 month). All participants were right-handed based on a validated subset of the Edinburgh handedness inventory (<xref ref-type="bibr" rid="c92" hwp:id="xref-ref-92-1" hwp:rel-id="ref-92">Raczkowski et al., 1974</xref>) and reported normal hearing and normal, or corrected-to-normal, vision. All participants gave informed consent and were compensated for their time. All procedures were approved by the Emory University Institutional Review Board.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-7">Procedures</title><sec id="s2b1" hwp:id="sec-5"><title hwp:id="title-8">General</title><p hwp:id="p-12">Five participants took part in the pseudoword-shape scans first, and then underwent three scans to test competing accounts of the processes underlying the pseudoword-shape correspondence. The remaining participants had already undergone these latter scans approximately four months earlier as part of a separate study and thus completed the pseudoword-shape scans after these scans. After completing the required scan sessions, all participants performed a behavioral task to determine the strength of their crossmodal pseudoword-shape correspondence. All experiments were presented via Presentation software (Neurobehavioral Systems Inc., Albany CA), which allowed synchronization of scan acquisition with experiments and also recorded responses and response times (RTs). After the final scan session, participants also completed the OSIVQ (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-3" hwp:rel-id="ref-14">Blazhenkova and Kozhevnikov, 2009</xref>).</p></sec><sec id="s2b2" hwp:id="sec-6"><title hwp:id="title-9">Pseudoword-shape fMRI task</title><p hwp:id="p-13">We created two auditory and two visual stimuli (pseudowords and novel two-dimensional outline shapes, respectively). The auditory pseudowords were ‘lohmoh’ (rounded) and ‘keekay’ (pointed). The pseudowords were digitally recorded in a female voice using Audacity v2.0.1 (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Audacity Team, 2012</xref>), with a SHURE 5115D microphone and an EMU 0202 USB external sound card, at a 44.1 kHz sampling rate. The recordings were then processed in Sound Studio (Felt Tip Inc., NY), using standard tools and default settings, edited into separate files, amplitude-normalized, and down-sampled to a 22.05 kHz sampling rate (standard for analyses of speech). Stimulus duration was 533 ms for ‘keekay’ and 600 ms for ‘lohmoh’, the differences reflecting natural pronunciation (see below). The visual stimuli were gray outline shapes on a black background (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1a</xref>), each subtending approximately 1° of visual angle and presented at the center of the screen for 500ms. The selected stimuli lay near the ends of independent rounded and pointed dimensions in each modality based on empirical ratings for 537 pseudowords (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-2" hwp:rel-id="ref-63">McCormick et al., 2015</xref>) and 90 visual shapes (<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">McCormick, 2015</xref>, unpublished data). These independent rating scales were converted to a single scale on which ‘most rounded’ = 1 and ‘most pointed’ = 7; ratings for the pseudowords ranged from 2.3 – 5.8 (median 4.2) and for the shapes from 1.6 – 6.7 (median 4.4). ‘Lohmoh’ was the 10<sup>th</sup> most rounded pseudoword, rated 2.8, and ‘keekay’ was the 7th most pointed pseudoword, rated 5.6, and were the most similar in duration of possible pairs from the 10 pseudowords at either extreme. We used the most rounded visual shape, rated 1.6, but had to use the 11<sup>th</sup> most pointed visual shape, rated 6.4, in order to match the number of protuberances (four) between the shapes. A more detailed analysis of the pseudowords and shapes can be found in <xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-2" hwp:rel-id="ref-53">Lacey et al., (2020)</xref> and the stimuli themselves are available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/ekpgh/" ext-link-type="uri" xlink:href="https://osf.io/ekpgh/" hwp:id="ext-link-2">https://osf.io/ekpgh/</ext-link>. While the pseudowords differed in duration by 67ms, approximately 12%, sound segments in language naturally differ in duration and digitally altering them to match in duration can cause them to sound artificially rapid or prolonged. Although it would have been possible to match the duration of the visual shapes to whichever pseudowords they accompanied, either 533 ms or 600 ms, this would mean that shape duration would vary as a function of congruency and thus introduce a confound, particularly in the ‘attend visual’ condition (see below).</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1</label><caption hwp:id="caption-1"><p hwp:id="p-14">Example stimuli for (a) the sound-symbolic word-shape correspondence task and the independent localization tasks: (b) magnitude, (c) multisensory integration, (d) language.</p></caption><graphic xlink:href="478347v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-15">Stimuli were presented concurrently in audiovisual pairs (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1a</xref>) that were either congruent (‘keekay’/pointed shape or ‘lohmoh’/rounded shape) or incongruent (‘keekay’/rounded shape or ‘lohmoh’/pointed shape) with respect to the crossmodal pseudoword-shape (sound-symbolic) correspondence. A mirror angled over the head coil enabled participants to see the visual stimuli projected onto a screen placed in the rear magnet aperture. Auditory stimuli were presented via scanner-compatible headphones. There were four runs, each consisting of 8 task blocks (4 congruent and 4 incongruent, each block containing 10 word-shape stimuli with a 3s interval between the onset of successive stimuli), each lasting 30s and alternating with 9 rest blocks, during which a fixation cross was displayed, each lasting 16s; total run duration was 384s.</p><p hwp:id="p-16">Pseudoword-shape pairs were pseudorandomly interleaved (no more than three trials in a row of the same pairing, no more than two blocks in a row of the same condition). In two of the runs, participants attended to auditory stimuli; in the other two runs, they attended to visual stimuli; the order of attended modality was counterbalanced across participants, who performed a 2AFC task in the attended modality. Participants pressed one of two buttons on a hand-held response box when they heard either ‘keekay’ or ‘lohmoh’ (attend auditory condition) or saw either the rounded or pointed shape (attend visual condition). The right index and middle fingers were used to indicate responses, counterbalanced between subjects across modalities and across rounded and pointed stimuli. Note that requiring a response on every trial and monitoring performance accuracy provides assurance that participants were attending to the stimuli. In this respect the task differs from that used by <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-2" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref> in which participants only had to detect rare occurrences of visual crosses and auditory beeps rather than respond to the pseudowords and shapes. Thus, the sound-symbolic stimuli of <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-3" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref> were processed incidentally and attention to these could not be guaranteed. As <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-4" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref> note, the need to focus on the rare target stimuli may have modulated the processing of pseudowords and shapes, whereas in the present study, the in-scanner behavioral task involved distinguishing between stimuli in the attended modality and the effect of the unattended modality was implicit.</p></sec><sec id="s2b3" hwp:id="sec-7"><title hwp:id="title-10">Independent localization tasks to test potential explanations of sound symbolic crossmodal correspondences</title><p hwp:id="p-17">The order of these tasks was fixed, progressing from the one perceived as most difficult to the easiest: participants completed the magnitude estimation task first, then the temporal synchrony task, and finally the language task. Each of these tasks comprised two runs with a fixed stimulus order; the order of runs was counterbalanced across participants. These tasks were completed in a single session for most participants. As outlined in the Introduction, we chose these tasks to broadly reflect the three potential explanations underlying crossmodal correspondences proposed by <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-6" hwp:rel-id="ref-110">Spence (2011)</xref>: “structural”, e.g., driven by intensity or magnitude; statistical, i.e., features that regularly co-occur in the world and thus might lend themselves to multisensory integration; and semantic, i.e. driven by linguistic factors. In order to test the accounts proposed by <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-7" hwp:rel-id="ref-110">Spence (2011)</xref>, we chose localization tasks from the literature on language processing (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>), multisensory integration (e.g., <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Beauchamp, 2005a</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">b</xref>; <xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-2" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-2" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-2" hwp:rel-id="ref-66">Marchant et al., 2012</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-2" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-3" hwp:rel-id="ref-27">Erickson et al., 2014</xref>), and magnitude processing (<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Lourenco et al. 2012</xref>) as described below. A number of different task contrasts could potentially be designed to test each explanation; thus it would be difficult to test all possibilities in a single study. We made choices from the literature that seemed reasonable <italic toggle="yes">a priori</italic>, with the thinking that the results of the present study should help to refine such choices in future work (see Discussion) and with the goal of avoiding potential confounds. For example, in the multisensory task, we avoided audiovisual speech (e.g., <xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-3" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-3" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-3" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-4" hwp:rel-id="ref-27">Erickson et al., 2014</xref>) and familiar environmental sounds and images (e.g., <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Hein et al., 2007</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>) because these would likely also involve semantic processing which we wished to test separately from multisensory integration.</p><p hwp:id="p-18">Additionally, we should note that we used the framework of <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-8" hwp:rel-id="ref-110">Spence (2011)</xref> as a starting point for a tractable set of testable hypotheses rather than an exhaustive list.</p></sec><sec id="s2b4" hwp:id="sec-8"><title hwp:id="title-11">Magnitude estimation</title><p hwp:id="p-19">In order to identify brain regions sensitive to magnitude, we used a modified form of the estimation task from <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-2" hwp:rel-id="ref-59">Lourenco et al. (2012)</xref>. In each trial of this task, participants were asked to indicate whether there were more black or white elements in a visual array of small rectangles by pressing one of two buttons on a response box. For a control task, we modified these arrays so that one item was a triangle and participants indicated whether the triangle was black or white using the same button presses as for the estimation task. There were two runs, each containing 12 active 16s-blocks (6 of each type, each block containing 4 trials lasting 1s with a 3s ISI) and alternating with 13 rest blocks, during which a fixation cross was displayed, each lasting 14s; total run duration was 374s. As can be seen in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1b</xref>, the two tasks involve very similar visual displays and both required some element of visual search and the same 2AFC response, either ‘black’ or ‘white’. Thus, low-level visual, response, and motor processing were approximately equated between experimental and control tasks; the only difference was that the control task did not depend on the number of items in the array. The contrast of magnitude &gt; control should therefore reveal areas more active during magnitude estimation than during the control task, thus identifying regions sensitive to magnitude in a domain-general magnitude system which may be relevant to assessing the magnitude along dimensions such as roundedness or pointedness. We expected that such regions would be primarily in posterior parietal cortex, particularly the IPS (<xref ref-type="bibr" rid="c99" hwp:id="xref-ref-99-2" hwp:rel-id="ref-99">Sathian et al., 1999</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Eger et al., 2003</xref>; <xref ref-type="bibr" rid="c121" hwp:id="xref-ref-121-3" hwp:rel-id="ref-121">Walsh, 2003</xref>; <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-2" hwp:rel-id="ref-87">Pinel et al., 2004</xref>; <xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-2" hwp:rel-id="ref-85">Piazza et al., 2004</xref>, <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-2" hwp:rel-id="ref-86">2007</xref>; Lourenco and Longo, 2012; Sokolowski et al., 2017).</p></sec><sec id="s2b5" hwp:id="sec-9"><title hwp:id="title-12">Multisensory integration</title><p hwp:id="p-20">Among a number of possible tasks that could be used to test multisensory integration, we chose one that is sensitive to the synchrony of auditory and visual stimuli, as used in many studies of audiovisual integration (e.g., <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-3" hwp:rel-id="ref-7">Beauchamp, 2005a</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">b</xref>; <xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-4" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-4" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-3" hwp:rel-id="ref-66">Marchant et al., 2012</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-4" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-5" hwp:rel-id="ref-27">Erickson et al., 2014</xref>). As noted above, we attempted to avoid semantic connotations of the stimuli used in this task, such as speech or environmental sounds, with the goal of investigating non-linguistic audiovisual integration. The auditory stimulus was an 810Hz tone of 800ms duration with a 20ms on/off ramp. The visual stimulus was a gray circle (RGB values 240, 240, 240) subtending approximately 1° of visual angle and presented centrally for 800ms. In synchronous trials, auditory and visual stimuli were presented simultaneously for 800ms followed by a 3200ms blank for a total trial length of 4s, while in asynchronous trials auditory and visual stimuli were presented for 800ms each but separated by an inter-stimulus interval (ISI) of 200ms followed by an 2200ms blank, also totaling 4s (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1c</xref>). During a trial, when no visual stimulus was present, the screen remained blank. Half the asynchronous trials presented the auditory stimulus first and half the visual stimulus first. There were two runs, each consisting of 12 active 16s-blocks (6 of each type, each block containing 4 trials) and alternating with 13 rest blocks each lasting 14s, during which a fixation cross was displayed; total run duration was 374s. Participants had to press a button whenever an oddball stimulus (e.g., <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Crottaz-Herbette and Menon, 2006</xref>), either a square or a burst of white noise, occurred; two oddballs of each type occurred in each run, one in a synchronous block and one in an asynchronous block. The contrast between synchronous and asynchronous trials was used to identify brain regions sensitive to audiovisual synchrony; we anticipated that this contrast would activate superior temporal cortex (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-4" hwp:rel-id="ref-7">Beauchamp, 2005a</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-4" hwp:rel-id="ref-8">b</xref>; <xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-5" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-5" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-4" hwp:rel-id="ref-66">Marchant et al., 2012</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-5" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-6" hwp:rel-id="ref-27">Erickson et al., 2014</xref>).</p></sec><sec id="s2b6" hwp:id="sec-10"><title hwp:id="title-13">Language task</title><p hwp:id="p-21">In this task, adapted from <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Fedorenko et al. (2010)</xref>, we contrasted complete semantically and syntactically intact sentences with sequences of pseudowords (examples are provided in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1d</xref>) to identify brain regions processing word- and sentence-level meaning (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-3" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>, <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">2011</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Bedny et al., 2011</xref>). The aim of this task contrast was to reveal potential overlaps between the processing of meaning in language and the artificial pseudowords we used; for the pseudowords, meaning is implied by the correspondence of their sounds with the visual shapes, as is the case in the extensive literature on sound-symbolic crossmodal correspondences. The complete sentences and pseudoword sequences each contained 12 items, each item being presented visually for 450ms for a total stimulus duration of 5.4s. As in <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-4" hwp:rel-id="ref-30">Fedorenko et al. (2010)</xref>, participants were instructed to read each sentence/sequence and at the end of each they were visually prompted, by a cue displayed for 600ms, to press a button, following which the next trial was presented immediately. There were two runs, each consisting of 16 task blocks (8 of each type, each block containing 3 trials) of 18s duration and alternating with 17 rest blocks of 12s duration, during which a fixation cross was displayed; total run duration was 492s. We expected the contrast of complete sentences &gt; pseudowords to reveal canonical language regions mediating both semantic and syntactic processing (see <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-5" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>, <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">2011</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">Bedny et al., 2011</xref>), i.e., a largely left hemisphere network comprising the inferior frontal gyrus (IFG), angular gyrus (AG), and extensive sectors of the temporal lobe including the superior temporal sulcus (STS).</p></sec></sec><sec id="s2c" hwp:id="sec-11"><title hwp:id="title-14">Post-scan behavioral testing</title><p hwp:id="p-22">As a final step, we tested whether participants reliably demonstrated the crossmodal pseudoword-shape correspondence using the implicit association test (IAT: <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Greenwald et al., 1998</xref>; <xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">Parise and Spence, 2012</xref>; <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Lacey et al., 2016</xref>). Originally devised as a test of social attitudes (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">Greenwald et al., 1998</xref>), the IAT has been successfully used to test the very different associations involved in crossmodal correspondences, including sound-symbolic ones (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-5" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen, 2019</xref>; <xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-2" hwp:rel-id="ref-83">Parise and Spence, 2012</xref>; <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">Lacey et al., 2016</xref>). The underlying principle is the same: response times (RTs) are faster if the stimuli assigned to a particular response button are congruent and slower if they are incongruent (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-3" hwp:rel-id="ref-34">Greenwald et al., 1998</xref>; <xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-3" hwp:rel-id="ref-83">Parise and Spence, 2012</xref>). The advantage of the IAT for testing crossmodal correspondences is that presenting each stimulus in isolation eliminates confounding by selective attention effects potentially causing slower RTs for incongruent pairings (<xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-4" hwp:rel-id="ref-83">Parise and Spence, 2012</xref>).</p><p hwp:id="p-23">For the IAT, participants were presented with one of the two auditory pseudowords, ‘keekay’ or ‘lohmoh’, or one of the two visual shapes, pointed or rounded, as single items. Participants had to press one of two response buttons (the ‘left’ and ‘right’ arrows on a standard US ‘QWERTY’ keyboard) whenever they heard or saw a specific item. Thus, the key to the IAT is that there were four stimuli but only two response buttons, so that each button was used to respond to both pseudoword and shape presentations. These response button assignments could be sound-symbolically congruent (e.g., participants would press ‘left’ if they heard ‘keekay’ or saw the pointed shape, but press ‘right’ if they heard ‘lohmoh’ or saw the rounded shape) or incongruent (e.g., press ‘left’ for ‘keekay’ or the rounded shape, but press ‘right’ for ‘lohmoh’ or the pointed shape). Participants were asked to respond as quickly as possible but were expected to be slower in the incongruent condition where the pseudoword and shape assigned to each response button did not ‘go together’, i.e., were sound-symbolically unrelated. By contrast, responses were expected to be faster in the congruent condition where the pseudoword and shape assigned to each response button were sound-symbolically matched.</p><p hwp:id="p-24">We used the same pseudowords and shapes as in the main fMRI experiment but the absolute size of the shapes was altered so that they subtended approximately 1° of visual angle in both the fMRI and IAT experimental set-ups. A trial consisted of a blank 1000ms followed by one of the pseudowords or one of the shapes. As before, pseudoword duration was either 533ms (‘keekay’) or 600ms (‘lohmoh’) but shapes were presented for 1000ms. A trial was terminated either by the participant pressing a response button or automatically 3500ms after stimulus onset if no response was made. On all trials, response buttons did not become active until 300ms after stimulus onset so that trials could not be terminated by rapid or accidental responses. The length of a block of trials (see next paragraph) thus varied slightly between participants but the maximum was 330s.</p><p hwp:id="p-25">There were four ‘runs’, each consisting of two blocks of trials. Each block began with an instruction screen describing the task and which pseudoword and shape were assigned to each response button. This was followed by 12 practice trials (not included in the analysis) with on-screen feedback as to accuracy. Practice trials were followed by 48 active trials – with no feedback – 12 for each of the two pseudowords and two shapes, occurring in pseudorandom order. The response button assignments for all 48 trials were either congruent or incongruent. At the end of the first of the two blocks, there was a new instruction screen and the response button assignments changed from congruent to incongruent or vice-versa, depending on the first block. This was followed by a new set of 12 practice trials for the new response button assignments, and then a new set of 48 active trials. Across the four runs, two began with a congruent block followed by an incongruent block and two with the reverse, counterbalanced across participants. The pseudowords and shapes were counterbalanced across the left and right response buttons in both congruent and incongruent conditions. Across the four runs, there were 192 trials each for congruent and incongruent conditions with each pseudoword and shape occurring equally often in each condition. The IAT was presented via Presentation software which also recorded RTs, measured from stimulus onset.</p><p hwp:id="p-26">Participants also completed the OSIVQ (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-4" hwp:rel-id="ref-14">Blazhenkova and Kozhevnikov, 2009</xref>), with the goal of examining the relationship between sound symbolism and individual differences in preferences for processing based on verbal, visual object imagery or visuospatial imagery. Such preferences have been argued to reflect fundamental styles of cognitive processing that vary between individuals and can predict abilities in many, seemingly unrelated, domains. In order to test the relationship to object or spatial imagery preference, which tend to oppose each other, we calculated the OSdiff score: the spatial score is subtracted from the object score of the OSIVQ to give a single scale on which positive/negative scores indicate a relative preference for object/spatial imagery respectively (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-2" hwp:rel-id="ref-54">Lacey et al., 2011</xref>, <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">2014</xref>, <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">2017</xref>; <xref ref-type="bibr" rid="c80" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">Occelli et al., 2014</xref>). The relationship to verbal preference was tested using the verbal score from the OSIVQ.</p></sec><sec id="s2d" hwp:id="sec-12"><title hwp:id="title-15">Image acquisition</title><p hwp:id="p-27">MR scans were performed on a 3 Tesla Siemens Trio TIM whole body scanner (Siemens Medical Solutions, Malvern, PA), using a 12-channel head coil. T2*-weighted functional images were acquired using a single-shot, gradient-recalled, echoplanar imaging (EPI) sequence for blood oxygenation level-dependent (BOLD) contrast. For all functional scans, 34 axial slices of 3.1mm thickness were acquired using the following parameters: repetition time (TR) 2000ms, echo time (TE) 30ms, field of view (FOV) 200mm, flip angle (FA) 90°, in-plane resolution 3.125×3.125mm, and in-plane matrix 64×64. High-resolution 3D anatomic images were acquired using an MPRAGE sequence (TR 2300ms, TE 3.02ms, inversion time 1100ms, FA 8°) comprising 176 sagittal slices of 1mm thickness (FOV 256mm, in-plane resolution 1×1mm, in-plane matrix 256×256). Once magnetic stabilization was achieved in each run, the scanner triggered the computer running Presentation software so that the sequence of experimental trials was synchronized with scan acquisition.</p></sec><sec id="s2e" hwp:id="sec-13"><title hwp:id="title-16">Image processing and analysis</title><p hwp:id="p-28">Image processing and analysis was performed using BrainVoyager QX v2.8.4 (Brain Innovation, Maastricht, Netherlands). Each participant’s functional runs were real-time motion-corrected utilizing Siemens 3D-PACE (prospective acquisition motion correction). Functional images were preprocessed employing cubic spline interpolation for slice scan time correction, trilinear-sinc interpolation for intra-session alignment of functional volumes, and high-pass temporal filtering to 2 cycles per run to remove slow drifts in the data without compromising task-related effects. Anatomic 3D images were processed, co-registered with the functional data, and transformed into Talairach space (<xref ref-type="bibr" rid="c117" hwp:id="xref-ref-117-1" hwp:rel-id="ref-117">Talairach and Tournoux, 1988</xref>). Talairach-normalized anatomic data sets from multiple scan sessions were averaged for each individual, to minimize noise and maximize spatial resolution.</p><p hwp:id="p-29">For group analyses, the Talairach-transformed data were spatially smoothed with an isotropic Gaussian kernel (full-width half-maximum 4mm). The 4mm filter is within the 3-6mm range recommended to reduce the possibility of blurring together activations that are in fact anatomically and/or functionally distinct (<xref ref-type="bibr" rid="c123" hwp:id="xref-ref-123-1" hwp:rel-id="ref-123">White et al., 2001</xref>), and the ratio of the smoothing kernel to the spatial resolution of the functional images (1.33) matches that of studies in which larger smoothing kernels were used (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">Mikl et al., 2008</xref>). Runs were percent signal change normalized (i.e., the mean signal value for each voxel’s time course was transformed to a value of 100, so that the individual values fluctuated around that mean as percent signal deviations).</p><p hwp:id="p-30">For display of group activations, we created a group average brain. We first selected a representative (target) Talairach-normalized brain from the 19-participant group. We then individually aligned the 18 remaining participants’ Talairach-normalized brains to this target (co-registration to match the gyral and sulcal pattern, followed by sinc interpolation). These 18 aligned brains were then averaged. This 18-subject average brain was then averaged with the target brain, creating a single Talairach template, with 1mm isotropic resolution, which was used to display the activations for the 19-subject group. This group average brain was displayed using the real-time volume rendering option in BrainVoyager QX. For statistical analysis, the 19-subject Talairach template was manually segmented in order to create a group average cortical ‘mask’ file with 3mm spatial resolution, equivalent to the spatial resolution of the functional data files.</p><p hwp:id="p-31">Statistical analyses of group data used general linear models (GLMs) treating participant as a random factor (so that the degrees of freedom equal n-1, i.e. 18), followed by pairwise contrasts. This analysis allows generalization to untested individuals. Correction for multiple comparisons within a cortical mask (corrected p &lt; .05) was achieved by imposing a threshold for the volume of clusters comprising contiguous voxels that passed a voxel-wise threshold of p &lt; 0.001, using a 3D extension (implemented in BrainVoyager QX) of the 2D Monte Carlo simulation procedure described by <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Forman et al. (1995)</xref>. This stringent voxel-wise threshold is recommended to avoid potential problems of false positives and also permits more accurate spatial localization of activation clusters than is possible with more lenient thresholds (<xref ref-type="bibr" rid="c125" hwp:id="xref-ref-125-1" hwp:rel-id="ref-125">Woo et al., 2014</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Eklund et al., 2016</xref>). Activations were localized with respect to 3D cortical anatomy with the help of an MRI atlas (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Duvernoy, 1999</xref>).</p><p hwp:id="p-32">In reporting activations, we do not provide ‘hotspot’ or ‘t<sub>max</sub>’ coordinates (i.e. the voxel with the largest t-value) because the statistical significance of specific voxels is not tested against other voxels within the activation (<xref ref-type="bibr" rid="c125" hwp:id="xref-ref-125-2" hwp:rel-id="ref-125">Woo et al., 2014</xref>). Instead, we provide the ‘center of gravity’ (CoG) coordinates since these orient the reader to the anatomical location but are statistically neutral.</p><p hwp:id="p-33">Where an activation spans several anatomical locations, we provide an extended description (see <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>). Likewise, in order to compare the present results to previous studies, we have not computed the Euclidean distance between sets of coordinates since, in most cases, this would involve comparing t<sub>max</sub> to CoG and therefore we would not be comparing like to like. Instead, we plotted the coordinates reported in previous studies onto our data in order to assess whether these fell within the activations we found here, such that we could reasonably say that there was some degree of overlap in the activations.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3 xref-table-wrap-1-4 xref-table-wrap-1-5 xref-table-wrap-1-6 xref-table-wrap-1-7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1</label><caption hwp:id="caption-2"><p hwp:id="p-34">Incongruency effects: pseudoword-shape incongruency-related activations in the ‘attend auditory’ condition within the cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05 cluster threshold 5 voxels); x,y,z, Talairach coordinates for centers of gravity.</p></caption><graphic xlink:href="478347v3_tbl1" position="float" orientation="portrait" hwp:id="graphic-2"/></table-wrap></sec></sec><sec id="s3" hwp:id="sec-14"><title hwp:id="title-17">RESULTS</title><sec id="s3a" hwp:id="sec-15"><title hwp:id="title-18"><bold>Behavioral</bold></title><sec id="s3a1" hwp:id="sec-16"><title hwp:id="title-19">In-scanner tasks</title><sec id="s3a1a" hwp:id="sec-17"><title hwp:id="title-20">Pseudoword-shape task</title><p hwp:id="p-35">Two-way (congruency, attended modality) repeated-measures ANOVA (RM-ANOVA) showed that mean (±sem) accuracy was not significantly different between the ‘attend auditory’ (97.4±0.7%) and ‘attend visual’ conditions (97.6±0.6%: F<sub>1, 18</sub> = .05, p = .8), nor between congruent (97.7±0.5%) and incongruent (97.3±0.6%) trials (F<sub>1, 18</sub> = .9, p = .3). There was a significant interaction between the attended modality and congruency (F<sub>1,18</sub> = 9.6, p = 0.006); this was due to greater accuracy for congruent compared to incongruent trials in the ‘attend auditory’ condition (98.2% vs 96.5%; t<sub>18</sub> = 2.6, p = 0.018) but not the ‘attend visual’ condition (97.1% vs 98%; t<sub>18</sub> = −1.9 p = 0.06: note that we are only concerned with these two comparisons, thus Bonferroni-corrected alpha = 0.025). We also performed non-parametric tests on the accuracy data since the ‘ceiling’ effects in all conditions indicate that the data were likely not normally distributed: these confirmed the result of the RM-ANOVA.</p><p hwp:id="p-36">Analysis of RTs excluded trials for which there was no response (.6% of all trials) or an incorrect response (2.5% of responses), and further excluded trials for which the RT was more than ± 2.5 standard deviations from the individual participant mean (2.6% of correct response trials). RM-ANOVA showed that RTs were faster for the ‘attend visual’ (474±21ms) compared to the ‘attend auditory’ condition (527±23ms: F<sub>1,18</sub> = 21.3, p &lt; 0.001) and for congruent (489±21ms) compared to incongruent trials (513±22ms: F<sub>1,18</sub> = 18.7, p &lt; 0.001). There was a significant interaction between the attended modality and congruency (F<sub>1,18</sub> = 9.2, p &lt; 0.007) in which both auditory (545ms) and visual (480ms) incongruent RTs were slower than congruent RTs in the same modality (auditory 509ms, t<sub>18</sub> = −4.0, p = 0.001; visual 469ms, t<sub>18</sub>, p = 0.009) but the absolute difference was greater for auditory than visual RTs (36ms vs 11ms).</p><p hwp:id="p-37">Note that, despite the highly repetitive nature of the pseudoword and shape stimuli, it is unlikely that participants stopped paying attention to them given the high accuracy rates (&gt; 97% in all conditions) and the fact that responses were made on 99.4% of trials. In addition, the mean auditory RTs (527ms overall, and 509ms vs. 545ms for congruent vs incongruent trials) suggest that participants often responded before the auditory stimulus was complete (duration = 533ms vs 600ms for ‘keekay’ vs. ‘lohmoh’). This was likely unavoidable given that participants only had to identify which of the two, highly repeated, pseudowords they heard; but the first syllable of each pseudoword, either ‘kee-’ or ‘loh-’, also respects the class of the entire word, either pointed or rounded respectively. Thus, we do not believe that the results are affected by such rapid responses since our analyses reveal that while being scanned, participants still exhibited significant behavioral congruency effects related to sound symbolism, more prominently in the attend-auditory than the attend-visual condition. On a related note, we do not believe that the longer auditory RTs indicate that the pseudoword detection task was more difficult than the visual shape task. The auditory task was to press one button whenever participants heard ‘keekay’ and the other for ‘lohmoh’, i.e., it was not an inherently difficult task and, as we point out above, the auditory RT results suggest that participants often responded before the end of the pseudoword which, given the high accuracy level (equal to that in the visual shape task), suggests that the task was not difficult. The longer auditory RTs appear to be driven by the incongruent trials rather than inherent task difficulty.</p></sec><sec id="s3a1b" hwp:id="sec-18"><title hwp:id="title-21">Independent localization tasks</title><p hwp:id="p-38">For the magnitude task, there was no significant difference in accuracy between the magnitude estimation (92.2±2.0%) and control (96.4±1.3%) tasks (t<sub>18</sub> = −1.8, p = 0.1) although RTs were significantly faster for the control task (900±45ms) compared to the magnitude task (991±53ms; t<sub>18</sub> = 3.4, p &lt; 0.01). Because of the low number of oddball trials (four for each participant) in the multisensory task, we conducted a non-parametric Wilcoxon test, which showed no significant difference between detection of synchronous (90.1±3.9%) and asynchronous (85.5±5.5%) oddballs (Z = −1.4, p = 0.2). In the language task, participants were equally accurate in responding to the visual cue at the end of each sentence (mean ± sem: 98.4±1.0%) or pseudoword string (97.7±1.1%; t18 = 0.9, p = 0.4). These analyses allow us to infer that participants paid attention to the stimuli used in the independent localization tasks.</p></sec></sec></sec><sec id="s3b" hwp:id="sec-19"><title hwp:id="title-22">Post-scan pseudoword-shape IAT</title><p hwp:id="p-39">An RM-ANOVA with factors of modality (auditory pseudowords, visual shapes) and response key association (congruent, incongruent) showed that accuracy was higher for the auditory pseudowords (95.8±0.8%) compared to the visual shapes (91.9±0.8%: F<sub>1,18</sub> = 31.8, p &lt; 0.001) and when response key associations were congruent (95.3±0.7%) compared to incongruent (92.4±1.2%: F<sub>1,18</sub> = 5.7, p = 0.03). The modality x response key association interaction was not significant (F<sub>1,18</sub> &lt; .01, p = 0.9).</p><p hwp:id="p-40">Analysis of RTs excluded trials for which there was no response or which failed to log (.7% of all trials), or incorrect responses (6.2% of responses), and further excluded trials for which the RT was more than ± 2.5 standard deviations from the individual participant mean (2.7% of correct response trials). RM-ANOVA showed that RTs were faster for the visual (606±19ms) compared to the auditory (702±21ms) stimuli (F<sub>1,18</sub> = 31.15, p &lt; 0.001) and when the response key associations were congruent (580±18ms) compared to incongruent (728±22ms: F<sub>1,18</sub> = 64.5, p &lt; 0.001). The modality x response key association interaction was not significant (F<sub>1,18</sub> = 0.5, p = 0.5).</p><p hwp:id="p-41">These analyses provide converging evidence, using a different approach from that employed during scanning, that participants experienced sound-symbolic congruency effects with the stimuli that were used.</p></sec><sec id="s3c" hwp:id="sec-20"><title hwp:id="title-23">Imaging</title><sec id="s3c1" hwp:id="sec-21"><title hwp:id="title-24">Pseudoword-shape task</title><p hwp:id="p-42">To test for regions involved in processing the sound-symbolic word-shape correspondence, we tested for voxels showing differential activation for congruent and incongruent trials (C &gt; I or I &gt; C), i.e., congruency or incongruency effects, respectively. These analyses were performed independently for the attend-auditory and attend-visual runs. Within the group average cortical mask, at a voxel-wise threshold of p &lt; 0.001, there were no activations that survived correction for multiple comparisons in the ‘attend visual’ condition, either for the C &gt; I contrast or its reverse, the I &gt; C contrast.</p><p hwp:id="p-43">However, in the ‘attend auditory’ condition, while there was no effect favoring the congruent over the incongruent condition, several regions showed greater activation for incongruent, compared to congruent, trials (the I &gt; C contrast) within the cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 5 voxels). These regions included bilateral foci in the anterior IPS (aIPS) extending on the left into the mid-IPS and the SMG, and activations in the right SMG extending into the postcentral sulcus (poCS), in the left SPG and the left MFG (<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>; <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>). Representative time-course curves for the I &gt; C contrast in these regions are displayed in <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6A</xref>. These show greater activation for incongruent compared to congruent trials in all regions except for the left SPG region in which there was differential <italic toggle="yes">de</italic>activation. Since it is not clear how to interpret differential deactivations, we do not consider this SPG focus further<sup>2</sup>. The neural incongruency effects were associated with a behavioral congruency effect in the ‘attend auditory’ condition in which responses to congruent trials were faster and more accurate than those to incongruent trials; this is consistent with the processing of incongruent trials being more effortful. The absence of a neural (in)congruency effect in the ‘attend visual’ condition is consonant with the absence of a congruency effect for accuracy data in this condition; although there was a congruency effect for RTs, this effect was significantly smaller in the ‘attend visual’ condition compared to the ‘attend auditory’ condition (see above).</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2</label><caption hwp:id="caption-3"><p hwp:id="p-44">Sound-symbolic incongruency effects: pseudoword-shape incongruency-related activations in the ‘attend auditory’ condition (<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Table 1</xref>).</p></caption><graphic xlink:href="478347v3_fig2" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec></sec><sec id="s3d" hwp:id="sec-22"><title hwp:id="title-25">Independent localization tasks</title><sec id="s3d1" hwp:id="sec-23"><title hwp:id="title-26">Magnitude estimation</title><p hwp:id="p-45">The contrast of magnitude estimation &gt; control within the cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 7 voxels) showed exclusively right hemisphere activity in the SMG, the SPG extending into the IPS, and the middle occipital gyrus (MOG) extending through the intra-occipital sulcus (IOS) to the superior occipital gyrus (SOG: <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2a</xref>; <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>). These loci are consistent with activations reported in previous studies of magnitude processing (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-3" hwp:rel-id="ref-25">Eger et al., 2003</xref>; <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-3" hwp:rel-id="ref-87">Pinel et al., 2004</xref>; <xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-3" hwp:rel-id="ref-85">Piazza et al., 2004</xref>, <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-3" hwp:rel-id="ref-86">2007</xref>) and a recent meta-analysis (Sokolowski et al., 2017). The right MOG region is close to foci previously implicated in subitizing (<xref ref-type="bibr" rid="c99" hwp:id="xref-ref-99-3" hwp:rel-id="ref-99">Sathian et al., 1999</xref>) or that showed adaptation to magnitude (<xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-4" hwp:rel-id="ref-86">Piazza et al., 2007</xref>) while the right SPG-IPS region is close to a region involved in counting visual objects (<xref ref-type="bibr" rid="c99" hwp:id="xref-ref-99-4" hwp:rel-id="ref-99">Sathian et al., 1999</xref>).</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3</label><caption hwp:id="caption-4"><p hwp:id="p-46">Magnitude task effects within cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 7 voxels). Contrast of magnitude estimation &gt; control reveals magnitude network (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2a</xref>).</p></caption><graphic xlink:href="478347v3_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2 xref-table-wrap-2-3 xref-table-wrap-2-4 xref-table-wrap-2-5 xref-table-wrap-2-6 xref-table-wrap-2-7 xref-table-wrap-2-8 xref-table-wrap-2-9 xref-table-wrap-2-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2</label><caption hwp:id="caption-5"><p hwp:id="p-47">Activations on independent localization tasks: magnitude (a), multisensory integration (b,c), and language (d,e); all within a cortical mask, voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster thresholds = magnitude, 7 voxels; multisensory integration, 8 voxels; language, 9 voxels; x,y,z Talairach coordinates for centers of gravity.</p></caption><graphic xlink:href="478347v3_tbl2" position="float" orientation="portrait" hwp:id="graphic-5"/><graphic xlink:href="478347v3_tbl2a" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap></sec><sec id="s3d2" hwp:id="sec-24"><title hwp:id="title-27">Multisensory synchrony</title><p hwp:id="p-48">The contrast of synchronous &gt; asynchronous within the cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 8 voxels) revealed bilateral activations in the anterior calcarine sulcus extending through the POF to the cuneus; in the left hemisphere, this activation further encompassed foci in the lingual and posterior cingulate gyri (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-3" hwp:rel-id="T2">Table 2b</xref>; <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>). The reverse contrast, asynchronous &gt; synchronous, resulted in two right hemisphere activations, one in inferior parietal cortex extending across the SMG and AG and into the IPS, and one in the IFG (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-4" hwp:rel-id="T2">Table 2c</xref>; <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>). While some previous studies have shown greater activation for synchronous compared to asynchronous audiovisual stimuli, others have shown the reverse, and in some cases preference for synchrony and asynchrony occurred at foci in proximity to each other (e.g. <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-6" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>). The regions most consistently reported as being sensitive to synchrony in previous studies were in the STS and/or STG – we did not observe activations in these regions on the multisensory synchrony task here, perhaps reflecting differences in stimuli and tasks (see Discussion). However, the right IFG area that was more active for asynchronous than synchronous stimuli in the present study was close to a region identified on the meta-analysis of <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-7" hwp:rel-id="ref-27">Erickson et al. (2014)</xref> as exhibiting a preference for audiovisual stimuli characterized by either content incongruency or asynchrony.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4</label><caption hwp:id="caption-6"><p hwp:id="p-49">Multisensory synchrony task effects within cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 8 voxels). Contrast of synchronous &gt; asynchronous (turquoise) reveals putative integration network (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-5" hwp:rel-id="T2">Table 2b</xref>); asynchronous &gt; synchronous (yellow: <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-6" hwp:rel-id="T2">Table 2c</xref>).</p></caption><graphic xlink:href="478347v3_fig4" position="float" orientation="portrait" hwp:id="graphic-7"/></fig></sec><sec id="s3d3" hwp:id="sec-25"><title hwp:id="title-28">Language</title><p hwp:id="p-50">As expected, the contrast of complete sentences &gt; pseudowords within the group average cortical mask described in the Methods section (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 9 voxels) revealed large activations bilaterally along the STS, extending into parts of the superior temporal gyrus (STG); this activation extended more posteriorly on the left than the right. Additional activations were noted on the left precentrally and in the middle occipital gyrus (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-7" hwp:rel-id="T2">Table 2d</xref>; <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>). Thus, the language task broadly replicated the previously reported language network (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-6" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>, <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-3" hwp:rel-id="ref-28">2011</xref>).</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5</label><caption hwp:id="caption-7"><p hwp:id="p-51">Language task effects within cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 9 voxels). Contrast of sentences &gt; pseudowords (orange) reveals semantic network (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-8" hwp:rel-id="T2">Table 2d</xref>); pseudowords &gt; sentences (olive) (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-9" hwp:rel-id="T2">Table 2e</xref>).</p></caption><graphic xlink:href="478347v3_fig5" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-52">Although not part of the original design of the present study, the contrast of pseudowords &gt; sentences may reflect the greater role of phonological processing in reading strings of pseudowords compared to normal intact sentences (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-7" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>). Given that our auditory stimuli were pseudowords, we were interested in also examining this reverse contrast. The contrast of pseudowords &gt; sentences within the cortical mask (voxel-wise threshold p &lt; 0.001, cluster-corrected p &lt; 0.05, cluster threshold 9 voxels) revealed large, bilateral frontoparietal activations: in and around the superior frontal sulcus (SFS) extending all the way into the IFG, and in the AG extending into the supramarginal gyrus (SMG) on the left and the IPS on the right. Smaller activations were also found along the medial surface of the left hemisphere in frontal and posterior cingulate cortex and in the parieto-occipital fissure (POF) (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-10" hwp:rel-id="T2">Table 2e</xref>; <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5</xref>). Consistent with a phonological basis, the left IFG activation on this contrast is close to an IFG focus showing greater activity for visually presented pseudowords relative to concrete words (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Binder et al., 2005</xref>), a contrast similar to that used here. Additionally, the left SMG activation on this contrast is at a site reported to be involved in phonological processing for visually presented words (<xref ref-type="bibr" rid="c91" hwp:id="xref-ref-91-1" hwp:rel-id="ref-91">Price et al., 1997</xref>; <xref ref-type="bibr" rid="c124" hwp:id="xref-ref-124-1" hwp:rel-id="ref-124">Wilson et al., 2011</xref>). However, reading pseudowords is also more effortful than reading complete sentences, perhaps due to mapping unfamiliar orthographic representations to phonological representations. Therefore, another possibility is that some or all of the activations on this contrast might reflect this additional effort, particularly since many of the activations (principally those in the frontal cortex bilaterally) were also found in regions identified as part of the frontoparietal domain-general, ‘multiple demand’ system (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Duncan, 2013</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Fedorenko et al., 2013</xref>). We return to this point in the Discussion.</p></sec></sec><sec id="s3e" hwp:id="sec-26"><title hwp:id="title-29">Overlap of incongruency effect with independent localization task contrasts</title><p hwp:id="p-53">Our approach to distinguishing between the competing potential explanatory accounts examined here was to look for overlap between areas showing pseudoword-shape (in)congruency effects and areas revealed by the independent language, magnitude, and multisensory tasks. Note that we assessed such overlap at voxel-wise thresholds of p &lt; 0.001 for the activations being compared. Overlaps at this strict threshold, avoiding potential false positives and allowing more accurate spatial localization than at more liberal thresholds (<xref ref-type="bibr" rid="c125" hwp:id="xref-ref-125-3" hwp:rel-id="ref-125">Woo et al., 2014</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Eklund et al., 2016</xref>), would provide support for candidate explanations. However, the presence of an overlap does not guarantee that the same process underlies both tasks at that locus nor that the same neuronal populations are activated; moreover, note that absence of overlaps would not allow such accounts to be definitively ruled <italic toggle="yes">out</italic>. The evidence of overlaps should be regarded as indicative and supportive of further research into the functional roles of those loci.</p><p hwp:id="p-54">In the magnitude task, the main magnitude estimation &gt; control contrast overlapped with the right SMG incongruency region and was also contiguous with the right aIPS incongruency region (see notes to <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-4" hwp:rel-id="T1">Table 1</xref>; <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7</xref>). The BOLD signal time-course for the right SMG region showed similar response patterns for the sound-symbolic and magnitude tasks: greater activation for incongruent and magnitude estimation trials, respectively, relative to the corresponding comparison conditions (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6A and B</xref>).</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6 xref-fig-6-7 xref-fig-6-8"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6</label><caption hwp:id="caption-8"><p hwp:id="p-55">Representative time-course curves for regions showing sound-symbolic incongruency effects in (A) the incongruent &gt; congruent contrast and (B-D) contrasts from the independent functional contrasts with which they shared an overlap zone.</p></caption><graphic xlink:href="478347v3_fig6" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-56">In the multisensory task, the contrast of asynchronous &gt; synchronous showed overlaps with the right aIPS and SMG incongruency regions (see notes to <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-5" hwp:rel-id="T1">Table 1</xref>; <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7</xref>). BOLD signal time-courses for these regions showed that both exhibited similar response patterns for the sound-symbolic and multisensory synchrony tasks: greater activation for incongruent and asynchronous trials, respectively, relative to the corresponding comparison conditions (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6A and C</xref>). Since the multisensory task did not reveal the STS activity that was expected on the basis of previous studies (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-5" hwp:rel-id="ref-7">Beauchamp, 2005a</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-5" hwp:rel-id="ref-8">b</xref>; <xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-6" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-7" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-5" hwp:rel-id="ref-66">Marchant et al., 2012</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-6" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-8" hwp:rel-id="ref-27">Erickson et al., 2014</xref>), we also carried out an ROI analysis in bilateral STS ROIs, chosen because they are sensitive to audiovisual integration of non-speech stimuli (tools and musical instruments and their associated sounds) and also behaviorally relevant in that activation profiles predict task performance (<xref ref-type="bibr" rid="c122" hwp:id="xref-ref-122-1" hwp:rel-id="ref-122">Werner and Noppeney, 2010</xref>).</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7</label><caption hwp:id="caption-9"><p hwp:id="p-57">Sound-symbolic incongruency effects (blue) in relation to independent functional contrasts for magnitude processing (green), multisensory integration (turquoise), and language: sentences &gt; pseudowords (orange), pseudowords &gt; sentences (olive). Circles indicate areas of overlap or contiguity between incongruency effects and independent functional contrasts (see notes to <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-6" hwp:rel-id="T1">Table 1</xref>).</p></caption><graphic xlink:href="478347v3_fig7" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-58">These ROIs, which were also used in a prior study from our group on the pitch-elevation crossmodal correspondence (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">McCormick et al., 2018</xref>), comprised cubes of 15mm side, centered on coordinates from <xref ref-type="bibr" rid="c122" hwp:id="xref-ref-122-2" hwp:rel-id="ref-122">Werner and Noppeney (2010</xref>; Talairach coordinates −57, −41, 14; 52, −33, 9; MNI coordinates were transformed into Talairach space using the online tool provided by <xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Lacadie et al., 2008</xref>). However, the contrast of incongruent &gt; congruent was not significant in these ROIs, either in the ‘attend auditory’ condition (left: t<sub>18</sub> = 0.3, p = 0.8; right: t<sub>18</sub> = 0.4, p = 0.7) or the ‘attend visual’ condition (left: t<sub>18</sub> = 1.6, p = 0.1; right: t<sub>18</sub> = 1.2, p = 0.2).</p><p hwp:id="p-59">Finally, none of the regions showing sensitivity to the pseudoword-shape correspondence overlapped or were contiguous with (i.e. shared a common edge or vertex) any area revealed by the language contrasts of sentences &gt; pseudowords or the multisensory contrast of synchronous &gt; asynchronous (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7</xref>). Instead, regions showing pseudoword-shape incongruency effects showed overlaps and contiguities with the control conditions from the language and multisensory tasks, i.e. when the primary contrasts for these conditions were reversed (see notes to <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-7" hwp:rel-id="T1">Table 1</xref>; <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Figure 7</xref>). The contrast of pseudowords &gt; complete sentences from the language task revealed overlap with incongruency effects in the left MFG and SMG. Comparing the BOLD signal time-courses for these regions (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Figure 6A and D</xref>) suggests that the left SMG region may be the more relevant of the two since it showed greater activation on the reverse contrasts. By contrast, the left MFG showed different response patterns in the sound-symbolic and language tasks: while there was greater activation for incongruent than congruent trials in the former task, it barely responded to pseudowords while deactivating to the sentence trials in the language task.</p><p hwp:id="p-60">Although the pseudoword condition likely involves processing the phonological form of the pseudowords rather than the semantic or syntactic aspects of natural language (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-8" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>), the pseudowords &gt; sentences contrast may have limitations as a formal test for phonological processing (see Discussion). Therefore, we also conducted a region-of-interest (ROI) analysis in which we created bilateral SMG ROIs, chosen because they were shown to be functionally involved in phonological processing using transcranial magnetic stimulation (TMS; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Hartwigsen et al., 2010</xref>). The ROIs consisted of cubes of 15mm side, centered on coordinates from <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">Hartwigsen et al. (2010</xref>; Talairach coordinates −45, −37, 42; 45, −36, 42; MNI coordinates were transformed into Talairach space as above). Although the SMG was part of the inferior parietal cluster of sound-symbolic incongruency-related activations in both hemispheres, the ROI approach allows a more specific test of the relationship to phonology. The contrast of incongruent &gt; congruent in the ‘attend auditory’ condition was significant in both left (t<sub>18</sub> = 3.6, p = 0.002) and right (t<sub>18</sub> = 3.9, p = 0.001) SMG ROIs. For completeness, there were no significant effects in either ROI in the ‘attend visual’ condition (left: t<sub>18</sub> = 1.1, p = 0.3; right: t<sub>18</sub> = 0.8, p = 0.5), matching the absence of activations within the cortical mask.</p></sec><sec id="s3f" hwp:id="sec-27"><title hwp:id="title-30">Correlation analyses</title><p hwp:id="p-61">Finally, we tested whether pseudoword-shape activation magnitudes correlated, across participants, with the magnitude of individual congruency effects derived from in-scanner RTs. The congruency effect computation used the formula (RT<sub>i</sub> – RT<sub>c</sub>) / (RT<sub>i</sub> + RT<sub>c</sub>), where RT<sub>i</sub> and RT<sub>c</sub> represent RT for incongruent and congruent trials, respectively. We also tested for correlations with scores on the verbal sub-scale of the OSIVQ (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-5" hwp:rel-id="ref-14">Blazhenkova and Kozhevnikov, 2009</xref>) and with individual preferences for object compared to spatial imagery by reference to the OSdiff score (see Methods). To avoid circularity, we conducted these correlation tests in the whole brain, i.e. independently of the activations, and in a similarly stringent manner, by setting a strict voxel-wise threshold of p &lt; 0.001 within the cortical mask before applying cluster correction (corrected p &lt; 0.05).</p><p hwp:id="p-62">In the ‘attend visual’ condition, activation magnitudes for the C &gt; I contrast were negatively correlated with the magnitude of the in-scanner RT congruency effects at a focus in the left SMG (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3a</xref>; <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref>); this focus overlapped with the left SMG focus in the language control task. Additionally, a single voxel in the right SFS, contiguous with right SFS activation on the language control task (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-2" hwp:rel-id="T3">Table 3b</xref>; <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-2" hwp:rel-id="F8">Figure 8</xref>), had a C &gt; I activation magnitude that was positively correlated with verbal scores from the OSIVQ.</p><fig id="fig8" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3 xref-fig-8-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8</label><caption hwp:id="caption-10"><p hwp:id="p-63">Cortical regions showing correlations between activation magnitudes for the C &gt; I contrast in the ‘attend visual’ condition and the magnitude of the RT congruency effect (brown: <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-3" hwp:rel-id="T3">Table 3a</xref>) and scores on the verbal sub-scale of the OSIVQ (blue-gray: <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-4" hwp:rel-id="T3">Table 3b</xref>); and between activation magnitudes for the I &gt; C contrast in the ‘attend auditory condition and the magnitude of the RT congruency effect (red: <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-5" hwp:rel-id="T3">Table 3c</xref>) and OSdiff scores (magenta: <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-6" hwp:rel-id="T3">Table 3d</xref>) – higher OSdiff scores indicate stronger preference for object, rather than spatial, imagery. See notes to <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-7" hwp:rel-id="T3">Table 3</xref> for relationships to pseudoword-shape incongruency regions and independent functional contrasts.</p></caption><graphic xlink:href="478347v3_fig8" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1 xref-table-wrap-3-2 xref-table-wrap-3-3 xref-table-wrap-3-4 xref-table-wrap-3-5 xref-table-wrap-3-6 xref-table-wrap-3-7 xref-table-wrap-3-8 xref-table-wrap-3-9"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;478347v3/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3</label><caption hwp:id="caption-11"><p hwp:id="p-64">Correlations within cortical mask (r-map threshold p &lt; 0.001, cluster-corrected p &lt; 0.05 (except (b), FDR corrected q &lt; 0.05), cluster thresholds = (a, c) 5 voxels, (d) 4 voxels; x,y,z, Talairach coordinates for centers of gravity.</p></caption><graphic xlink:href="478347v3_tbl3" position="float" orientation="portrait" hwp:id="graphic-12"/><graphic xlink:href="478347v3_tbl3a" position="float" orientation="portrait" hwp:id="graphic-13"/></table-wrap><p hwp:id="p-65">In the ‘attend auditory’ condition, several foci showed strong positive correlations between activation magnitudes for the I &gt; C contrast and the magnitude of the in-scanner RT congruency effects (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-8" hwp:rel-id="T3">Table 3c</xref>; <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Figure 8</xref>), overlapping with the ‘attend auditory’ incongruency region in the left aIPS/SMG and close to the left mid-IPS incongruency region. Among the regions identified on the independent localization tasks, correlation foci were contiguous with pseudoword &gt; sentence activations in the right precuneus and the left AG, IPS, and SMG, and close to the language control activation in the right IFS/IFG. A single correlation focus was close to the right SPG activation from the magnitude task. There were also foci at which the magnitude of the BOLD incongruency effect (I &gt; C contrast) was positively correlated with preference for visual object imagery (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-9" hwp:rel-id="T3">Table 3d</xref>; <xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-4" hwp:rel-id="F8">Figure 8</xref>), i.e., magnitudes increased with increasing preference for object relative to spatial imagery. These correlated regions were primarily in the left precuneus, and visual cortical areas: the IOS, the posterior calcarine sulcus extending into the lingual gyrus, and the right MOG.</p></sec></sec><sec id="s4" hwp:id="sec-28"><title hwp:id="title-31">DISCUSSION</title><sec id="s4a" hwp:id="sec-29"><title hwp:id="title-32"><bold>Comparison of present study with prior studies</bold></title><p hwp:id="p-66">To our knowledge, the present study is the first attempt to investigate, in a principled way, various explanations of the neural basis of <italic toggle="yes">how</italic> sound-symbolic associations are processed, as opposed to merely the neural loci <italic toggle="yes">at</italic> which they are processed. Previous studies have employed sound-symbolic words and pseudowords relating to a range of different domains (<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Ković et al., 2010</xref>; <xref ref-type="bibr" rid="c96" hwp:id="xref-ref-96-3" hwp:rel-id="ref-96">Revill et al., 2014</xref>; <xref ref-type="bibr" rid="c115" hwp:id="xref-ref-115-4" hwp:rel-id="ref-115">Sučević et al., 2015</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-3" hwp:rel-id="ref-58">Lockwood et al., 2016</xref>; <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-6" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen, 2019</xref>), but the neural processes involved may differ depending on the particular sound-symbolic association (<xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-5" hwp:rel-id="ref-105">Sidhu and Pexman, 2018</xref>). To avoid this problem, we concentrated on the specific association between auditory pseudowords and visual shapes, and tested the evidence for competing explanations. In addition, we used a post-scan behavioral task to verify that participants did, in fact, make the intended sound-shape mapping.</p><p hwp:id="p-67">Sound-symbolic associations are a form of crossmodal correspondence. <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-9" hwp:rel-id="ref-110">Spence (2011)</xref> suggests that crossmodal correspondences might reflect processes originating in multisensory integration, magnitude estimation, or semantic mediation, depending on the particular correspondence. These suggestions closely approximate three of five potential explanations for sound-symbolic associations put forward by <xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-6" hwp:rel-id="ref-105">Sidhu and Pexman (2018)</xref> although their report was published after data acquisition for the present study had been completed. The remaining two proposals – shared properties across modalities or species-general mechanisms – were not testable in the current study but cannot be excluded. Likewise, the design of our study is also open to the possibility of alternative explanations not considered by <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-10" hwp:rel-id="ref-110">Spence (2011)</xref> or <xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-7" hwp:rel-id="ref-105">Sidhu and Pexman (2018)</xref>.</p><p hwp:id="p-68">Here, we investigated sound-to-shape mapping between auditory pseudowords and visual shapes by manipulating the congruency of this sound-symbolic correspondence and employing fMRI which offers greater anatomical resolution than the EEG paradigms of some prior studies (e.g., Kovi et al., 2010; <xref ref-type="bibr" rid="c115" hwp:id="xref-ref-115-5" hwp:rel-id="ref-115">Sučević et al., 2015</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-4" hwp:rel-id="ref-58">Lockwood et al., 2016</xref>). Briefly, we compared the resulting neocortical activations to the results of functional contrasts designed to reflect the neural processes underlying potential explanations. There were no significant activations during attention to visual shape, but during attention to auditory pseudowords we found greater activity for incongruent, compared to congruent, pseudoword-shape pairs in bilateral IPS and SMG, and in the left MFG. On comparison to functional contrasts, we suggest that these results provide no evidence for semantic mediation, and limited evidence for processes involved in multisensory integration or magnitude estimation; instead, we propose that they likely reflect phonological processing and/or multisensory attention. We emphasize, however, that the present study is exploratory and intended to serve as a spur to further research.</p></sec><sec id="s4b" hwp:id="sec-30"><title hwp:id="title-33">Sound-symbolic incongruency effects</title><p hwp:id="p-69">During attention to visual shapes, no activations survived correction in either the C&gt;I contrast or the reverse. Behaviorally, there was no difference in accuracy between congruent and incongruent trials during attention to shapes, and accuracy was just as high as during attention to pseudowords. RTs during attention to shapes were overall faster than during attention to pseudowords and, although they were slower for incongruent, compared to congruent, trials, this difference was 70% smaller than that during attention to pseudowords. This difference in congruency effects depending on the attended modality may arise from timing differences in auditory and visual processing. The visual shapes appeared in their entirety at the start of each trial and remained onscreen for 500 ms. By contrast, auditory pseudowords, perforce, unfold over time, which may have made them more subject to influence of the unattended modality. In other words, during attention to shapes, a participant could see the entire shape immediately and prepare a response almost at once, even before the auditory pseudoword had been completely presented: note that mean response times for visual shapes (overall 474ms, congruent 469ms, incongruent 480ms) were all shorter than the duration of either pseudoword (‘keekay’ 533ms, ‘lohmoh’ 600ms). This suggests that the unattended pseudowords may have had little influence on either neural processing or behavior when participants attended to the visual shapes.</p><p hwp:id="p-70">Although several studies indicate that, even when attention is focused on a single modality of a multisensory object, information in an unattended (and task-irrelevant) modality is processed as well (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Busse et al., 2005</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Driver and Spence, 2000</xref>; <xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Miller, 1991</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">Molholm et al., 2007</xref>; <xref ref-type="bibr" rid="c127" hwp:id="xref-ref-127-1" hwp:rel-id="ref-127">Zimmer et al., 2010</xref>), a more recent study suggests that intermodal processing is modulated by attention at a relatively late stage (<xref ref-type="bibr" rid="c104" hwp:id="xref-ref-104-1" hwp:rel-id="ref-104">Shrem and Deouell, 2017</xref>). Thus, the asymmetry we observed here between attention to visual and auditory stimulus components might reflect this relatively late influence of crossmodal attention, as it might have been more likely to influence the temporally-unfolding pseudowords, which required more time to process in full than the visual shapes.</p><p hwp:id="p-71">Additionally, when there is a conflict between visual and auditory sources of information, there may be a processing bias towards the visual modality (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Ben-Artzi and Marks, 1995</xref>; <xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">Molholm et al., 2004</xref>). Such a bias could result in auditory information having less of an influence during attention to vision and/or produce a stronger incongruency response when, supposedly unattended, visual information is incongruent with an attended auditory stream.</p><p hwp:id="p-72">During attention to auditory pseudowords, we observed greater cortical activity for incongruent, compared to congruent, pseudoword-shape pairs, which may seem puzzling at first glance. But it is not clear, <italic toggle="yes">a priori</italic>, whether the congruent or incongruent audiovisual condition would evoke the greater activity. On each trial in the imaging experiment of the present study, participants were only required to identify which pseudoword or shape was presented, depending on the attended modality; thus, the congruency or incongruency of the pseudoword/shape pair was only processed implicitly. Similarly, in <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-7" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref>, participants responded to ‘oddball’ trials and not to pseudowords or shapes at all; thus, here too, sound-symbolic (in)congruency was only processed incidentally and incongruency effects were found. By contrast, a preliminary report of another study from our group, using an <italic toggle="yes">explicit</italic> match/mismatch decision on each trial, found that a focus in the right caudate nucleus exhibited greater activity for congruent than incongruent stimuli, whereas a left precentral focus showed the opposite preference (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Barany et al., 2021</xref>). Furthermore, a recent study of Japanese mimetic words relating to tactile hardness/softness, in which participants explicitly judged congruency between tactile stimuli and auditory mimetic words on every trial, also found activations for congruency relative to incongruency (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Kitada et al., 2021</xref>). Thus, effects favoring congruent over incongruent conditions have so far been found only when participants rendered congruency judgments during scanning, implying that this kind of task may be a prerequisite for finding such effects. Effects in the opposite direction, i.e., favoring incongruent over congruent conditions, on the other hand, have been reported as the only or dominant effect independently by multiple studies of sound-to-shape mapping and across different experimental designs and tasks: how are these to be interpreted?</p><p hwp:id="p-73">One view of congruency manipulations as a test of multisensory integration is that multiple sensory inputs are available to brain regions involved in integration, and that such regions respond more to congruent than incongruent multisensory inputs (<xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">Noppeney, 2012</xref>). However, <xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-2" hwp:rel-id="ref-76">Noppeney (2012)</xref> also argued that the brain might attempt, unsuccessfully, to integrate incongruent multisensory inputs before rejecting them as not belonging together. If so, it would not be unexpected to find greater activity for incongruent than congruent stimuli as a result of the more effortful processing involved. In fact, separate previous studies, of audiovisual integration (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">Hein et al., 2007</xref>) and crossmodal visual priming of auditory targets (<xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-2" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>), have both found incongruency effects – importantly, these were not studies of sound symbolism. Incongruent priming effects appeared to be meaningful in that they were selective for spoken words in L STS and for environmental sounds in L AG, while L IFS and prefrontal cortex showed incongruency effects for both (<xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-3" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>).</p><p hwp:id="p-74">One way in which incongruency effects might arise is because congruent pairings represent a type of intersensory redundancy, deriving from, or referring to the same semantic source or object and thus requiring fewer processing resources (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Bahrick et al., 2004</xref>; <xref ref-type="bibr" rid="c100" hwp:id="xref-ref-100-1" hwp:rel-id="ref-100">Shams and Seitz, 2008</xref>). If sound symbolism is based in a multisensory system that responds to intersensory redundancy, this could account for the incongruency effects observed in our study and others (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-8" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen, 2019</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">Barany et al., 2021</xref>), possibly due to the added burden of attempting to integrate incongruent or otherwise mismatched sensory inputs (<xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-4" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>). In the present study, it is therefore interesting to note that the R aIPS and R SMG-poCS foci showing greater activity for mismatched than matched pseudoword-shape pairs (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Figure 6A</xref>) both overlapped with regions more sensitive to audiovisual asynchrony than synchrony (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Figure 6C</xref>); i.e., both foci were processing items that did not ‘go together’ in some way.</p></sec><sec id="s4c" hwp:id="sec-31"><title hwp:id="title-34">Relationship of incongruency effects to independent functional contrasts</title><sec id="s4c1" hwp:id="sec-32"><title hwp:id="title-35">Semantic and phonological processes</title><p hwp:id="p-75">The regions identified on the language task showed no overlap at all with the observed activations due to incongruent, compared to congruent, pairings of auditory pseudowords and visual shapes. Thus, of the <italic toggle="yes">a priori</italic> explanations considered in the Introduction, the possibility of semantic mediation, i.e. correspondence between meanings that might be implicit in the pseudowords (e.g., by reference to relevant words with similar phonological content) and the shapes that are associated, appears least likely. However, the incongruency-related region in the left SMG was also more active for pseudowords than complete sentences in the language task. In addition, foci demonstrating correlations of neural incongruency effects with in-scanner RT incongruency effects during the ‘attend auditory’ condition, in the right precuneus and right inferior frontal cortex, were adjacent to or near areas with a preference for pseudowords over sentences.</p><p hwp:id="p-76">The premise of sound symbolism is that the sound structure of words mimics or relates to some aspect of what they represent – either explicitly, and within-modally, in the auditory domain as in onomatopoeia (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">Catricalà and Guidi, 2015</xref>; <xref ref-type="bibr" rid="c101" hwp:id="xref-ref-101-2" hwp:rel-id="ref-101">Schmidtke et al., 2014</xref>); or by analogy, and crossmodally, as for ideophones or mimetics which can also refer to non-auditory meanings (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">Akita and Tsujimura, 2016</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Kita, 1997</xref>). For example, the repetitive sound of the Japanese mimetic term ‘kirakira’ may be considered analogous to the flickering quality of light it describes. It is thus interesting that we found a left-lateralized region in the SMG that was common to both the pseudoword-shape incongruency effect and the pseudoword condition of the language task, even though pseudowords were presented auditorily in the sound-symbolic task and visually in the language control task. Given that reading pseudowords likely depends on phonological processing (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-9" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>) whereas reading sentences also involves syntactic and semantic processing, this suggests that phonological processes may underlie this commonality. Both the left and right SMG are involved in phonological processing (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-3" hwp:rel-id="ref-35">Hartwigsen et al., 2010</xref>; <xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">Oberhuber et al., 2016</xref>), and the left SMG focus has previously been implicated in the phonological processing of visually presented words (<xref ref-type="bibr" rid="c91" hwp:id="xref-ref-91-2" hwp:rel-id="ref-91">Price et al., 1997</xref>; <xref ref-type="bibr" rid="c124" hwp:id="xref-ref-124-2" hwp:rel-id="ref-124">Wilson et al., 2011</xref>). In order to test the phonology hypothesis further, we conducted an ROI analysis of bilateral SMG foci derived from an earlier study (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-4" hwp:rel-id="ref-35">Hartwigsen et al., 2010</xref>), i.e., independently of our findings. This ROI analysis showed incongruency effects in bilateral SMG in the ‘attend auditory’, but not visual, condition. Based on these findings, we suggest that phonological processing could contribute to the pseudoword-shape correspondence. Since the phonological account was not originally hypothesized, and hence the language task was not designed to test this, we cannot currently rule out alternative explanations, so this idea requires further investigation.</p><p hwp:id="p-77">There are many aspects to phonology, e.g., syllable structure, stress, and prosody, and while the pseudowords &gt; sentences contrast may reflect phonological processing, it likely involves multiple aspects that need to be disentangled. Although several studies suggest that consonants contribute more than vowels to associations with pointed and rounded shapes (e.g., <xref ref-type="bibr" rid="c74" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">Nielsen and Rendall, 2011</xref>; <xref ref-type="bibr" rid="c81" hwp:id="xref-ref-81-1" hwp:rel-id="ref-81">Ozturk et al., 2013</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Fort et al., 2015</xref>; but see <xref ref-type="bibr" rid="c114" hwp:id="xref-ref-114-1" hwp:rel-id="ref-114">Styles and Gawne, 2017</xref>) and that pointed/rounded shapes are associated with plosive/sonorant consonants (<xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Monaghan et al., 2012</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">Fort et al., 2015</xref>), further work is required to investigate the relative contributions of different phonological and/or phonetic features to particular sound-symbolic associations (e.g. <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Knoeferle et al., 2017</xref>; <xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-3" hwp:rel-id="ref-63">McCormick et al., 2015</xref>). Finally, it should be noted that reading pseudowords (as in the control condition of the language task) involves mapping unfamiliar orthographic patterns<sup>3</sup> to phonological representations and may thus require more effort than reading complete sentences (<xref ref-type="bibr" rid="c103" hwp:id="xref-ref-103-1" hwp:rel-id="ref-103">Shechter and Share, 2020</xref>). Therefore, the overlaps of activations in the language control task with those related to sound symbolism may reflect involvement of the domain-general multiple demand system (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Duncan, 2013</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">Fedorenko et al., 2013</xref>), rather than, or in addition to, phonological processes (see below).</p><p hwp:id="p-78">The finding that brain regions active during processing of sound-symbolic pseudowords are distinct from the canonical language system (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-10" hwp:rel-id="ref-30">Fedorenko et al., 2010</xref>, <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-4" hwp:rel-id="ref-28">2011</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">Bedny et al., 2011</xref>) may offer a potential route for rehabilitation after language impairment, for example in stroke patients. To the extent that sound-symbolic and classical language systems involved in structure and meaning are dissociated, either channel could be exploited to tap into meaningful representations if the other channel showed a deficit. In the same way that hearing a tool sound can remind patients with apraxia how to use that tool (<xref ref-type="bibr" rid="c126" hwp:id="xref-ref-126-1" hwp:rel-id="ref-126">Worthington, 2016</xref>), intensive exposure to sound-symbolic words might provide a means of rehabilitating language abilities. In fact, there is preliminary evidence to suggest that patients with aphasia process sound-symbolic words better than more arbitrary word-meaning mappings (<xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Meteyard et al., 2015</xref>). This possibility is worth exploring in further research.</p></sec><sec id="s4c2" hwp:id="sec-33"><title hwp:id="title-36">Multisensory integration processes</title><p hwp:id="p-79">As described in the Results, the multisensory task did not show activity in superior temporal cortex, the region most commonly activated in prior studies employing audiovisual (a)synchrony to demonstrate multisensory integration. Most previous studies employed audiovisual speech (<xref ref-type="bibr" rid="c120" hwp:id="xref-ref-120-7" hwp:rel-id="ref-120">van Atteveldt et al., 2007</xref>; <xref ref-type="bibr" rid="c113" hwp:id="xref-ref-113-8" hwp:rel-id="ref-113">Stevenson et al., 2010</xref>; <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-7" hwp:rel-id="ref-75">Noesselt et al., 2012</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-9" hwp:rel-id="ref-27">Erickson et al., 2014</xref>) or combinations of familiar environmental sounds and images (e.g., <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-3" hwp:rel-id="ref-37">Hein et al., 2007</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-5" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>). Since both these stimulus types would be expected to trigger some form of semantic processing, we avoided them given that semantic mediation might have been covered separately by the language task as a potential explanation for the correspondence between spoken pseudowords and visual shapes (although an EEG study suggests similar multisensory integration effects for both speech and non-speech stimuli [<xref ref-type="bibr" rid="c112" hwp:id="xref-ref-112-1" hwp:rel-id="ref-112">Stekelenburg and Vroomen, 2007</xref>]).</p><p hwp:id="p-80">Additionally, asynchronous audiovisual stimuli are more likely to be perceived as synchronous if the visual element precedes the auditory element (V-A) compared to the reverse (A-V) (e.g., <xref ref-type="bibr" rid="c90" hwp:id="xref-ref-90-1" hwp:rel-id="ref-90">Powers et al., 2009</xref>, using simple perceptual stimuli as in the present study; and see <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Bhat et al., 2015</xref>, for the same effect using audiovisual speech stimuli). Since there were equal numbers of V-A and A-V trials in the asynchronous condition, its effectiveness may have been reduced, if some of the V-A trials were actually perceived as synchronous. In order to compensate for this and to test the multisensory integration hypothesis independently of the multisensory synchrony task, we conducted a further ROI analysis of foci in the STS, a classical multisensory region, derived from a previous study involving non-verbal, but semantically related, stimuli (<xref ref-type="bibr" rid="c122" hwp:id="xref-ref-122-3" hwp:rel-id="ref-122">Werner and Noppeney, 2010</xref>), but this showed no significant sound-symbolic effects in either of the attended modalities.</p><p hwp:id="p-81">Nonetheless, pseudoword-shape incongruency-related activations in the right aIPS and SMG overlapped with areas active during the asynchronous condition of the multisensory task. As noted above, whether mismatched pseudoword-shape pairs (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-7" hwp:rel-id="F6">Figure 6A</xref>) or auditory and visual stimuli that were presented asynchronously (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-8" hwp:rel-id="F6">Figure 6C</xref>), both foci appear to be involved in processing items that did not ‘go together’ in some way. The right aIPS incongruency site is close to several IPS foci activated during processing of audiovisual spatial congruency (<xref ref-type="bibr" rid="c102" hwp:id="xref-ref-102-3" hwp:rel-id="ref-102">Sestieri et al., 2006</xref>). The IPS portions of our parietal incongruency regions also overlap with regions implicated in multisensory attention during difficult tasks (<xref ref-type="bibr" rid="c95" hwp:id="xref-ref-95-1" hwp:rel-id="ref-95">Regenbogen et al., 2018</xref>). This may be relevant to the contention that the brain is trying harder to integrate these inputs when they are incongruent (<xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-3" hwp:rel-id="ref-76">Noppeney, 2012</xref>; see also ‘Incongruency Effects and Attention’ below). The left MFG incongruency region likely overlaps with a region in inferior frontal cortex responding to audiovisual incongruency for familiar environmental stimuli (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-4" hwp:rel-id="ref-37">Hein et al., 2007</xref>) and incongruency between visual primes and auditory targets, whether environmental sounds or spoken words (<xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-6" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>). Since the relevant effects were evoked by multiple types of stimulus incongruency, either of content or of timing, the most plausible explanation for these overlaps is the attention-driven effects cited above, perhaps due to unsuccessful attempts to integrate the audiovisual stimuli.</p><p hwp:id="p-82">Although foci that responded to incongruent pseudoword/shape pairs also responded to temporal asynchrony, a limitation of the multisensory functional contrast that was intended to broadly reflect multisensory integration is that it may be overly specific to integration arising from temporal co-occurrence, which may not be important in assigning pseudowords to spatial entities like shapes. Certainly, integration can also arise from other types of congruency, such as spatial co-location (<xref ref-type="bibr" rid="c111" hwp:id="xref-ref-111-1" hwp:rel-id="ref-111">Spence, 2013</xref>). However, whether spatial co-location is important for integration may be modality- and task-dependent, being less important for some audiovisual (e.g., <xref ref-type="bibr" rid="c94" hwp:id="xref-ref-94-1" hwp:rel-id="ref-94">Regan and Spekreisje, 1977</xref>) than visuotactile tasks (e.g., <xref ref-type="bibr" rid="c98" hwp:id="xref-ref-98-1" hwp:rel-id="ref-98">Sambo and Forster, 2009</xref>), perhaps especially if, as here, the task is simply to identify the stimulus (<xref ref-type="bibr" rid="c111" hwp:id="xref-ref-111-2" hwp:rel-id="ref-111">Spence, 2013</xref>). As noted by <xref ref-type="bibr" rid="c111" hwp:id="xref-ref-111-3" hwp:rel-id="ref-111">Spence (2013</xref>, footnote b), auditory stimuli presented via headphones and visual stimuli presented on a screen are different spatial locations and it may be difficult to determine the role of spatial matching in such a set-up. However, participants were both faster and more accurate for congruent than incongruent trials in the ‘attend auditory’ condition, indicating that the spatially different sources of auditory pseudowords and visual shapes had little effect (in the ‘attend visual’ condition, participants were faster, but not significantly differentially accurate, for congruent trials).</p><p hwp:id="p-83">Finally, we avoided audiovisual speech or environmental sound and images in the multisensory task on the grounds that these might trigger semantic processes that might also have been reflected in the language task to some extent. However, since sound symbolism implicitly assumes that ‘keekay’/‘lohmoh’ and the pointed/rounded shapes are related, an argument could be made that a multisensory task targeting semantic (in)congruency, either audiovisual speech or environmental stimuli, might have been employed as an independent test of congruency itself. In this respect, it is interesting that we find an incongruency region in the left MFG (and outside the classic language network) that potentially overlaps with that found by <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-5" hwp:rel-id="ref-37">Hein et al. (2007)</xref> as responsive to semantically incongruent environmental audiovisual pairs. Ultimately, further research is needed to more fully explore the relationship between multisensory integration, semantic processes, and sound symbolism.</p></sec><sec id="s4c3" hwp:id="sec-34"><title hwp:id="title-37">Magnitude estimation processes</title><p hwp:id="p-84">In the present study, the right SMG incongruency region overlapped with activation during magnitude estimation and was contiguous with the right aIPS magnitude-sensitive region. The IPS is a classical locus for magnitude processing (<xref ref-type="bibr" rid="c99" hwp:id="xref-ref-99-5" hwp:rel-id="ref-99">Sathian et al., 1999</xref>; <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-4" hwp:rel-id="ref-25">Eger et al., 2003</xref>; <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-4" hwp:rel-id="ref-87">Pinel et al., 2004</xref>; <xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-4" hwp:rel-id="ref-85">Piazza et al., 2004</xref>, <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-5" hwp:rel-id="ref-86">2007</xref>; Sokolowski et al., 2017) while the SMG is among regions showing adaptation to magnitude (<xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-6" hwp:rel-id="ref-86">Piazza et al., 2007</xref>) or involved in detecting changes in numerosity (<xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-5" hwp:rel-id="ref-85">Piazza et al., 2004</xref>). Additionally, part of the right POF-precuneus region, whose activation magnitude correlated with RTs during attention to auditory stimuli in the scanner, was close to the right SPG activation evoked by magnitude estimation in the independent localization contrast – this region has been implicated in counting visual objects (<xref ref-type="bibr" rid="c99" hwp:id="xref-ref-99-6" hwp:rel-id="ref-99">Sathian et al., 1999</xref>). The two pseudowords used in this experiment were drawn, like the shapes, from a much larger set which were empirically rated as sounding more or less rounded or pointed (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-4" hwp:rel-id="ref-63">McCormick et al., 2015</xref>). It follows that participants had no problem in applying magnitude estimation (of roundedness or pointedness) to pseudowords. As noted earlier, sensory features often relate to polar dimensions of magnitude where one end is ‘more than’ the other (<xref ref-type="bibr" rid="c108" hwp:id="xref-ref-108-2" hwp:rel-id="ref-108">Smith and Sera, 1992</xref>), for example brighter vs. darker or louder vs. quieter, and a contrast targeting such sensory dimensions may have been relevant to the dimensions of pointedness and roundedness. We expected that magnitude processing would be reflected in congruent and incongruent trials because congruent trials involve stimuli occupying similar positions on a continuum whilst incongruent trials involve stimuli occupying different positions. It remains possible that the incongruency effects do indeed reflect magnitude processing because there are different magnitudes involved. However, the role of magnitude processing might become clearer in a parametric setting, using a range of pseudowords and shapes and thus sampling at multiple intervals along the rounded/pointed continuum (whilst accepting that the continuum might not be consistently mapped across individuals). Despite extensive overlap between brain regions involved in processing number- and sensory-based magnitude (<xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-5" hwp:rel-id="ref-87">Pinel et al., 2004</xref>), caution is indicated by recent challenges to the idea of a domain-general magnitude system (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Anobile et al., 2018</xref>; <xref ref-type="bibr" rid="c89" hwp:id="xref-ref-89-1" hwp:rel-id="ref-89">Pitt and Casasanto, 2018</xref>; but see <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Holmes et al., 2019</xref>) which has been posited as an explanation for crossmodal correspondences (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-3" hwp:rel-id="ref-60">Lourenco and Longo, 2011</xref>; <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-11" hwp:rel-id="ref-110">Spence, 2011</xref>; <xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-8" hwp:rel-id="ref-105">Sidhu and Pexman, 2018</xref>). Additionally, while we cannot rule out magnitude estimation as a possible explanation for the observed sound-symbolic incongruency effects, the regions of overlap between the magnitude estimation task and the incongruency effects are also implicated in multisensory attentional processes, which may ultimately turn out to be more important (see below).</p></sec></sec><sec id="s4d" hwp:id="sec-35"><title hwp:id="title-38">Incongruency effects and attention</title><p hwp:id="p-85">No brain regions showed congruency effects in either attended modality in the present study, which was also the case in the study of <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-9" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref>. However, when participants were attending to the auditory pseudowords, several neocortical regions showed greater activity for incongruent, compared to congruent, pairings with visual shapes. These areas were more widespread than the dorsolateral prefrontal regions found by <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-10" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref>, who used a task requiring only occasional responses (see below). We did not find incongruency effects when participants attended to the visual shapes, consistent with a behavioral congruency effect being absent in the ‘attend visual’ runs when accuracy was the dependent measure, and being significantly smaller in the ‘attend visual’ than the ‘attend auditory’ runs when RT was the dependent measure. These differences as a function of the attended modality may stem from greater unfamiliarity of auditory pseudowords relative to two-dimensional visual shapes. Alternatively, they might reflect timing differences in processing pseudowords compared to visual shapes, since the pseudowords unfold over time, whereas the visual shapes are in evidence from the start of each trial. Consequently, there may be more of an incongruency effect in the ‘attend auditory’ condition because the concurrent visual stimulus could trigger particular representations immediately, whereas in the ‘attend visual’ condition, the concurrent pseudoword may still be playing as the participant prepares a response to the visual stimulus.</p><p hwp:id="p-86">The regions demonstrating activations attributable to incongruency were in various parts of parietal cortex bilaterally, and in the left MFG. The left MFG activation likely overlapped with an extensive zone of frontal cortical sensitivity to pseudoword-shape incongruency in the study of <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-11" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen (2019)</xref>, although this was associated with greater deactivation for congruent than incongruent stimuli in that study and, as noted earlier, deactivations are hard to interpret (see Footnote 2). Furthermore, since the task in that study was to detect rare occurrences of visual crosses and auditory beeps, the sound-symbolic pseudoword and shape stimuli were actually irrelevant to the main detection task; thus the frontal deactivation to mismatched pseudoword-shape pairs may well have arisen because these were a distraction from the explicit detection task (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-12" hwp:rel-id="ref-84">Peiffer-Smadja and Cohen, 2019</xref>).</p><p hwp:id="p-87">An incongruency effect of the type reported here might be enhanced when, as in the present study, participants selectively attend to one modality while ignoring the other (<xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-4" hwp:rel-id="ref-76">Noppeney, 2012</xref>). For example, when participants attended auditory targets after passively viewing visual primes, greater activity was elicited in several cortical areas (including one that likely overlaps with our left MFG incongruency region) when primes and targets were incongruent compared to congruent (<xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-7" hwp:rel-id="ref-77">Noppeney et al., 2008</xref>). Our left MFG incongruency region likely overlapped with a focus displaying greater activity for audiovisual stimuli that were familiar but semantically incongruent, compared to unfamiliar arbitrary pairings of abstract images and sounds (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-6" hwp:rel-id="ref-37">Hein et al., 2007</xref>). This region also corresponds to part of the domain-general ‘multiple demand’ attentional system described by <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-3" hwp:rel-id="ref-23">Duncan (2013)</xref>. In the ‘attend auditory’ incongruent condition, then, what may be happening is that the brain attempts to integrate a stimulus pair but instead detects a mismatch between a rounded pseudoword and a pointed shape or vice versa. On this reasoning, the basis of the observed incongruency-related activations might be greater attentional demand, rather than multisensory integration, during discrimination of (unfamiliar) auditory pseudowords in the presence of incongruent, compared to congruent, visual shapes. Consistent with this, the incongruency region in the right SMG is close to a focus exhibiting a preference for novel over familiar stimuli across auditory, visual, and tactile modalities (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Downar et al., 2002</xref>), and to one in which activation was stronger for unfamiliar abstract audiovisual stimuli compared to familiar, but semantically incongruent, audiovisual stimuli (<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-7" hwp:rel-id="ref-37">Hein et al., 2007</xref>). Furthermore, the incongruency regions found here in bilateral aIPS and left SPG correspond to regions active during top-down attentional processing when visual targets were cued by semantically incongruent audiovisual stimuli (<xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Mastroberardino et al., 2015</xref>), and to bilateral IPS foci (both of which overlap with our incongruency regions) shown to mediate audiovisual interaction during difficult processing of degraded stimuli (<xref ref-type="bibr" rid="c95" hwp:id="xref-ref-95-2" hwp:rel-id="ref-95">Regenbogen et al., 2018</xref>). This attentional explanation is especially likely for the subset of incongruency regions, comprising foci in the left IPS/SMG that also exhibited correlations between the magnitudes of these neural incongruency effects and the corresponding RT congruency effects during scanning. We acknowledge that these considerations are subject to the limitations of reverse inference, although the limitations tend to be mitigated by our use of task-related reverse inference (Hutzler et al., 2014; <xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-2" hwp:rel-id="ref-64">McCormick et al., 2018</xref>). We suggest that the role of multisensory attention in relation to sound-symbolic crossmodal correspondences merits further research.</p></sec><sec id="s4e" hwp:id="sec-36"><title hwp:id="title-39">Relationship to visual imagery</title><p hwp:id="p-88">Across participants, increasing preference for object as opposed to spatial imagery was strongly positively correlated with activation magnitudes for the incongruent &gt; congruent contrast in the ‘attend auditory’ condition. Many of these correlated regions were in visual cortex (in the left IOS, posterior calcarine sulcus/lingual gyrus, and the right MOG), and one was in the left precuneus. This may reflect a greater tendency for object imagers to visualize shapes when associating these with pseudowords. The precuneus (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Cavanna and Trimble, 2006</xref>), calcarine sulcus, lingual gyrus and MOG (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Ganis et al., 2004</xref>) are all involved in visual imagery. Another possibility is that, since object imagers tend to integrate structural properties like shape with surface properties like texture (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-3" hwp:rel-id="ref-54">Lacey et al., 2011</xref>), the phonetic properties of the pseudowords, for example whether or not consonants are voiced or vowels are rounded, are more strongly bound to the visualized pointed and rounded shapes in these individuals. Interestingly, in contrast to the relatively widespread correlations with imagery preferences, there was little evidence that preference for verbal processing was connected to sound-symbolic associations since correlation of activation magnitudes with OSIVQ verbal scores was limited to a single voxel in the right SFS. Further work is required to investigate the potential for individual differences since these might underlie the ability to either guess the meaning of sound-symbolic words in unknown foreign languages (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Kunihira, 1971</xref>), or to learn such associations (<xref ref-type="bibr" rid="c78" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">Nygaard et al., 2009</xref>; <xref ref-type="bibr" rid="c97" hwp:id="xref-ref-97-1" hwp:rel-id="ref-97">Revill et al., 2018</xref>).</p></sec><sec id="s4f" hwp:id="sec-37"><title hwp:id="title-40">Limitations</title><p hwp:id="p-89">We note some limitations of the current study. Firstly, as noted above, alternative functional contrasts might have been adopted for both the magnitude and multisensory integration tasks. The magnitude task essentially involved a numerosity judgment whereas the pseudowords and shapes were derived from a rounded-pointed continuum. The domain-general magnitude system suggested as a potential basis for crossmodal correspondences (<xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-12" hwp:rel-id="ref-110">Spence, 2011</xref>) and some instances of sound symbolism (<xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-9" hwp:rel-id="ref-105">Sidhu and Pexman, 2018</xref>) would be expected to be involved in both numerosity and attribute magnitude or intensity (see <xref ref-type="bibr" rid="c121" hwp:id="xref-ref-121-4" hwp:rel-id="ref-121">Walsh, 2003</xref>). But it is possible that a more sensory-based magnitude continuum, e.g., lighter-darker (see <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-6" hwp:rel-id="ref-87">Pinel et al., 2004</xref>), would have been more akin to the sensory pointed/rounded continuum of the pseudowords and shape stimuli. For the multisensory integration task, we avoided speech and environmental stimuli because semantic processing was covered by the language task. But the language task did not address semantic (in)congruency directly, which is arguably part of what the pseudoword-shape pairs reflected. Thus, a functional contrast addressing semantic (in)congruency might have captured a more specific aspect of the sound-symbolic pairs. We should also note that, in the sound-symbolic task, all conditions were multisensory; thus, we were unable to compare activity in these conditions to that in unisensory auditory and visual conditions.</p><p hwp:id="p-90">The pseudowords were spoken by an adult female and we did not vary the speaker (e.g. to include male or children’s voices). While including multiple speakers might be desirable in studies of speech perception, it is common for studies of sound symbolism to restrict stimuli to a single speaker voice or to use a synthetic voice. One reason this is important is that different voices could be associated with particular stimuli independently of the words they are uttering. For example, a child’s voice is higher pitched than an adult’s, and female voices are typically higher pitched than male voices; higher pitch in either case could be associated with more pointed shapes. Using a single speaker as we did allows us to avoid these confounds and examine the role of phonological structure of the pseudowords of interest. Nonetheless, we are typically exposed to a variety of voices in normal life and it would thus be desirable to incorporate multiple speakers, balancing the number of utterances across male and female child and adult speakers. Additionally, one could manipulate the factor of speaker variability and investigate whether responses to pointed shape are greater for higher-pitched children’s voices than lower-pitched adult voices and whether responses to rounded shapes are greater for female compared to male voices.</p><p hwp:id="p-91">Finally, we only used a single rounded and pointed exemplar pseudoword and shape, which might restrict generalizability of the present findings. However, these stimuli were chosen because they represent extreme values along the rounded-pointed continuum and thus could offer the strongest possible response for congruent/incongruent pseudoword-shape pairs. Critically, sound-symbolic mappings appear to be both graded and relative (<xref ref-type="bibr" rid="c118" hwp:id="xref-ref-118-1" hwp:rel-id="ref-118">Thompson and Estes, 2011</xref>); thus, if we had used multiple exemplars, some would likely have been perceived as less rounded or pointed than others, which could raise issues with interpreting congruency. Future work could address this via multivariate analyses in a parametric setting, using a range of pseudowords and shapes and thus sampling at multiple intervals along the rounded/pointed continuum (whilst accepting that the continuum might not be consistently mapped across individuals).</p></sec></sec><sec id="s5" hwp:id="sec-38"><title hwp:id="title-41">Conclusions</title><p hwp:id="p-92">This study is the first to systematically examine competing explanations underlying the sound-symbolic crossmodal correspondence between auditory pseudowords and visual shapes. We used independent contrasts and ROI analyses to test a number of <italic toggle="yes">a priori</italic> hypotheses for the neural basis of these correspondences. Overall, evidence for semantic mediation was lacking while that for multisensory integration and magnitude estimation was weak, at best. Albeit via task-based reverse inference, multisensory attention emerged as an important potential candidate account, as we also proposed previously for the crossmodal pitch-elevation correspondence (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-3" hwp:rel-id="ref-64">McCormick et al., 2018</xref>); we also found evidence consistent with phonological processing as an underlying explanation though this requires more rigorous testing. While this study addressed a tractable set of candidate explanations drawn from <xref ref-type="bibr" rid="c110" hwp:id="xref-ref-110-13" hwp:rel-id="ref-110">Spence (2011)</xref> and relevant to <xref ref-type="bibr" rid="c105" hwp:id="xref-ref-105-10" hwp:rel-id="ref-105">Sidhu and Pexman (2018)</xref>, other possibilities remain to be explored, for example the neural basis of the sensorimotor hypothesis proposed by <xref ref-type="bibr" rid="c93" hwp:id="xref-ref-93-2" hwp:rel-id="ref-93">Ramachandran and Hubbard (2001)</xref>. Nonetheless, our findings provide a basis for further research, which should seek converging evidence using other methods (for example, multivariate fMRI analyses, repetition suppression fMRI studies, or transcranial magnetic stimulation) and should also address the relative weights of these different processes.</p><p hwp:id="p-93">Further, we observed several regions in visual cortex in which activation magnitudes scaled with individual preference for object imagery, thus suggesting a basis for individual differences in processing sound-symbolic associations to be followed up in future research. An important goal for research into crossmodal correspondences in general, as well as sound symbolism in particular, will be to distinguish the roles of high-level cognitive processes, such as phonology and attention, from those of lower-level sensory and perceptual processes.</p></sec></body><back><sec id="s6" hwp:id="sec-39"><glossary hwp:id="glossary-1"><title hwp:id="title-42">Abbreviations: Directional</title><def-list hwp:id="def-list-1"><def-item hwp:id="def-item-1"><term hwp:id="term-1">a</term><def hwp:id="def-1"><p hwp:id="p-94">anterior</p></def></def-item></def-list><def-list hwp:id="def-list-2"><def-item hwp:id="def-item-2"><term hwp:id="term-2">front</term><def hwp:id="def-2"><p hwp:id="p-95">frontal</p></def></def-item></def-list><def-list hwp:id="def-list-3"><def-item hwp:id="def-item-3"><term hwp:id="term-3">i</term><def hwp:id="def-3"><p hwp:id="p-96">inferior</p></def></def-item></def-list><def-list hwp:id="def-list-4"><def-item hwp:id="def-item-4"><term hwp:id="term-4">lat</term><def hwp:id="def-4"><p hwp:id="p-97">lateral</p></def></def-item></def-list><def-list hwp:id="def-list-5"><def-item hwp:id="def-item-5"><term hwp:id="term-5">m</term><def hwp:id="def-5"><p hwp:id="p-98">mid</p></def></def-item></def-list><def-list hwp:id="def-list-6"><def-item hwp:id="def-item-6"><term hwp:id="term-6">med</term><def hwp:id="def-6"><p hwp:id="p-99">medial</p></def></def-item></def-list><def-list hwp:id="def-list-7"><def-item hwp:id="def-item-7"><term hwp:id="term-7">p</term><def hwp:id="def-7"><p hwp:id="p-100">posterior</p></def></def-item></def-list><def-list hwp:id="def-list-8"><def-item hwp:id="def-item-8"><term hwp:id="term-8">s</term><def hwp:id="def-8"><p hwp:id="p-101">superior</p></def></def-item></def-list><def-list hwp:id="def-list-9"><def-item hwp:id="def-item-9"><term hwp:id="term-9">v</term><def hwp:id="def-9"><p hwp:id="p-102">ventral</p></def></def-item></def-list></glossary><glossary hwp:id="glossary-2"><title hwp:id="title-43">Abbreviations: Anatomical</title><def-list hwp:id="def-list-10"><def-item hwp:id="def-item-10"><term hwp:id="term-10">AG</term><def hwp:id="def-10"><p hwp:id="p-103">angular gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-11"><def-item hwp:id="def-item-11"><term hwp:id="term-11">AOS</term><def hwp:id="def-11"><p hwp:id="p-104">anterior occipital sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-12"><def-item hwp:id="def-item-12"><term hwp:id="term-12">calcS</term><def hwp:id="def-12"><p hwp:id="p-105">calcarine sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-13"><def-item hwp:id="def-item-13"><term hwp:id="term-13">CeS</term><def hwp:id="def-13"><p hwp:id="p-106">central sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-14"><def-item hwp:id="def-item-14"><term hwp:id="term-14">cingG</term><def hwp:id="def-14"><p hwp:id="p-107">cingulate gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-15"><def-item hwp:id="def-item-15"><term hwp:id="term-15">cingS</term><def hwp:id="def-15"><p hwp:id="p-108">cingulate sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-16"><def-item hwp:id="def-item-16"><term hwp:id="term-16">collatS</term><def hwp:id="def-16"><p hwp:id="p-109">collateral sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-17"><def-item hwp:id="def-item-17"><term hwp:id="term-17">FG</term><def hwp:id="def-17"><p hwp:id="p-110">fusiform gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-18"><def-item hwp:id="def-item-18"><term hwp:id="term-18">fo</term><def hwp:id="def-18"><p hwp:id="p-111">frontal operculum</p></def></def-item></def-list><def-list hwp:id="def-list-19"><def-item hwp:id="def-item-19"><term hwp:id="term-19">InfOS</term><def hwp:id="def-19"><p hwp:id="p-112">inferior occipital sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-20"><def-item hwp:id="def-item-20"><term hwp:id="term-20">Ins</term><def hwp:id="def-20"><p hwp:id="p-113">insula</p></def></def-item></def-list><def-list hwp:id="def-list-21"><def-item hwp:id="def-item-21"><term hwp:id="term-21">IOG</term><def hwp:id="def-21"><p hwp:id="p-114">inferior occipital gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-22"><def-item hwp:id="def-item-22"><term hwp:id="term-22">IOS</term><def hwp:id="def-22"><p hwp:id="p-115">intra-occipital sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-23"><def-item hwp:id="def-item-23"><term hwp:id="term-23">IPS</term><def hwp:id="def-23"><p hwp:id="p-116">intraparietal sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-24"><def-item hwp:id="def-item-24"><term hwp:id="term-24">ITG</term><def hwp:id="def-24"><p hwp:id="p-117">inferior temporal gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-25"><def-item hwp:id="def-item-25"><term hwp:id="term-25">ITS</term><def hwp:id="def-25"><p hwp:id="p-118">inferior temporal sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-26"><def-item hwp:id="def-item-26"><term hwp:id="term-26">LG</term><def hwp:id="def-26"><p hwp:id="p-119">lingual gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-27"><def-item hwp:id="def-item-27"><term hwp:id="term-27">MFG</term><def hwp:id="def-27"><p hwp:id="p-120">middle frontal gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-28"><def-item hwp:id="def-item-28"><term hwp:id="term-28">MOG</term><def hwp:id="def-28"><p hwp:id="p-121">middle occipital gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-29"><def-item hwp:id="def-item-29"><term hwp:id="term-29">OrbG</term><def hwp:id="def-29"><p hwp:id="p-122">orbital gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-30"><def-item hwp:id="def-item-30"><term hwp:id="term-30">poCG</term><def hwp:id="def-30"><p hwp:id="p-123">post central gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-31"><def-item hwp:id="def-item-31"><term hwp:id="term-31">po</term><def hwp:id="def-31"><p hwp:id="p-124">pars opercularis</p></def></def-item></def-list><def-list hwp:id="def-list-32"><def-item hwp:id="def-item-32"><term hwp:id="term-32">poCS</term><def hwp:id="def-32"><p hwp:id="p-125">post central sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-33"><def-item hwp:id="def-item-33"><term hwp:id="term-33">POF</term><def hwp:id="def-33"><p hwp:id="p-126">parieto-occipital fissure</p></def></def-item></def-list><def-list hwp:id="def-list-34"><def-item hwp:id="def-item-34"><term hwp:id="term-34">preCS</term><def hwp:id="def-34"><p hwp:id="p-127">precentral sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-35"><def-item hwp:id="def-item-35"><term hwp:id="term-35">preCG</term><def hwp:id="def-35"><p hwp:id="p-128">precentral gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-36"><def-item hwp:id="def-item-36"><term hwp:id="term-36">precun</term><def hwp:id="def-36"><p hwp:id="p-129">precuneus</p></def></def-item></def-list><def-list hwp:id="def-list-37"><def-item hwp:id="def-item-37"><term hwp:id="term-37">preSMA</term><def hwp:id="def-37"><p hwp:id="p-130">presupplementary motor area</p></def></def-item></def-list><def-list hwp:id="def-list-38"><def-item hwp:id="def-item-38"><term hwp:id="term-38">pt</term><def hwp:id="def-38"><p hwp:id="p-131">pars triangularis</p></def></def-item></def-list><def-list hwp:id="def-list-39"><def-item hwp:id="def-item-39"><term hwp:id="term-39">SFG</term><def hwp:id="def-39"><p hwp:id="p-132">superior frontal gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-40"><def-item hwp:id="def-item-40"><term hwp:id="term-40">SFS</term><def hwp:id="def-40"><p hwp:id="p-133">superior frontal sulcus</p></def></def-item></def-list><def-list hwp:id="def-list-41"><def-item hwp:id="def-item-41"><term hwp:id="term-41">SMG</term><def hwp:id="def-41"><p hwp:id="p-134">supramarginal gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-42"><def-item hwp:id="def-item-42"><term hwp:id="term-42">SOG</term><def hwp:id="def-42"><p hwp:id="p-135">superior occipital gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-43"><def-item hwp:id="def-item-43"><term hwp:id="term-43">SPG</term><def hwp:id="def-43"><p hwp:id="p-136">superior parietal gyrus</p></def></def-item></def-list><def-list hwp:id="def-list-44"><def-item hwp:id="def-item-44"><term hwp:id="term-44">STS</term><def hwp:id="def-44"><p hwp:id="p-137">superior temporal sulcus. All other abbreviations are as in the main text.</p></def></def-item></def-list></glossary></sec><sec id="s7" hwp:id="sec-40"><title hwp:id="title-44">FUNDING</title><p hwp:id="p-138">This work was supported by grants to KS and LN from the National Eye Institute at the NIH (R01EY025978) and the Emory University Research Council; to KM from the Emory University Facility for Education and Research in Neuroscience and the Laney Graduate School. Support to KS from the Veterans Administration is also acknowledged.</p></sec><ack hwp:id="ack-1"><title hwp:id="title-45">ACKNOWLEDGMENTS</title><p hwp:id="p-139">We thank Lawrence Barsalou, Justin Bonny, Daniel Dilks, Evelina Fedorenko, Tami Feng, Harold Gouzoules, Sara List, Stella Lourenco and Kate Pirog Revill for their advice and assistance. An earlier version of this manuscript has been released as a pre-print at bioRxiv.org, doi:10.1101/478347 (<xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">McCormick et al., 2018a</xref>).</p></ack><sec id="s8" hwp:id="sec-41"><title hwp:id="title-46">AUTHOR CONTRIBUTIONS</title><p hwp:id="p-140">KM, KS, LCN and SL designed the experiment; KM created stimuli; KM and RS performed the research and analyzed data; KM, SL, KS and LCN wrote the paper.</p></sec><sec id="s9" sec-type="COI-statement" hwp:id="sec-42"><title hwp:id="title-47">CONFLICT OF INTEREST</title><p hwp:id="p-141">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-48">REFERENCES</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2"><label>1.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Akita K."><surname>Akita</surname>, <given-names>K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tsujimura N"><surname>Tsujimura</surname>, <given-names>N</given-names></string-name>. (<year>2016</year>). <chapter-title>Mimetics</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Kageyama T."><given-names>T.</given-names> <surname>Kageyama</surname></string-name> and <string-name name-style="western" hwp:sortable="Kishimoto H."><given-names>H.</given-names> <surname>Kishimoto</surname></string-name></person-group> (Eds.) <source hwp:id="source-1">Handbook of Japanese Lexicon and Word Formation</source>, pp<fpage>133</fpage>–<lpage>160</lpage>. <publisher-name>Walter de Gruyter Inc</publisher-name>., <publisher-loc>Boston, USA</publisher-loc>.</citation></ref><ref id="c2" hwp:id="ref-2"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Amedi A."><surname>Amedi</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Malach R."><surname>Malach</surname>, <given-names>R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Pascual-Leone A"><surname>Pascual-Leone</surname>, <given-names>A</given-names></string-name>. (<year>2005</year>). <article-title hwp:id="article-title-2">Negative BOLD differentiates visual imagery and perception</article-title>. <source hwp:id="source-2">Neuron</source> <volume>48</volume>, <fpage>859</fpage>–<lpage>872</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Anobile G."><surname>Anobile</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burr D.C."><surname>Burr</surname>, <given-names>D.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iaia M."><surname>Iaia</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marinelli C.V."><surname>Marinelli</surname>, <given-names>C.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Angelelli P."><surname>Angelelli</surname>, <given-names>P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Turi M"><surname>Turi</surname>, <given-names>M</given-names></string-name>. (<year>2018</year>). <article-title hwp:id="article-title-3">Independent adaptation mechanisms for numerosity and size perception provide evidence against a common sense of magnitude</article-title>. <source hwp:id="source-3">Sci. Rep. UK</source> <volume>8</volume>, <fpage>13571</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41598-018-31893-6</pub-id></citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><collab hwp:id="collab-1">Audacity Team</collab> (<year>2012</year>) <source hwp:id="source-4">Audacity v2.0.1 [Computer program]</source>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://audacity.sourceforge.net/" ext-link-type="uri" xlink:href="http://audacity.sourceforge.net/" hwp:id="ext-link-3">http://audacity.sourceforge.net/</ext-link> Audacity ® software is copyright © 1999-2014 Audacity Team.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Bahrick L. E."><surname>Bahrick</surname>, <given-names>L. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lickliter R."><surname>Lickliter</surname>, <given-names>R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Flom R"><surname>Flom</surname>, <given-names>R</given-names></string-name>. (<year>2004</year>). <article-title hwp:id="article-title-4">Intersensory redundancy guides the development of selective attention, perception, and cognition in infancy</article-title>. <source hwp:id="source-5">Child Dev</source>. <volume>13</volume>, <fpage>99</fpage>–<lpage>102</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Barany D."><surname>Barany</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nygaard L.N."><surname>Nygaard</surname>, <given-names>L.N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2021</year>). <article-title hwp:id="article-title-5">Neural basis of sound-symbolic pseudoword-shape correspondences. Abstract, Society for Neuroscience Global Connectome, online conference</article-title>, <source hwp:id="source-6">January</source> <fpage>11</fpage>–<lpage>13</lpage>, 2021.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2 xref-ref-7-3 xref-ref-7-4 xref-ref-7-5"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Beauchamp M.S"><surname>Beauchamp</surname>, <given-names>M.S</given-names></string-name>. (<year>2005a</year>). <article-title hwp:id="article-title-6">Statistical criteria in fMRI studies of multisensory integration</article-title>. <source hwp:id="source-7">Neuroinformatics</source> <volume>5</volume>, <fpage>93</fpage>–<lpage>114</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3 xref-ref-8-4 xref-ref-8-5"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Beauchamp M.S"><surname>Beauchamp</surname>, <given-names>M.S</given-names></string-name>. (<year>2005b</year>). <article-title hwp:id="article-title-7">See me, hear me, touch me: multisensory integration in lateral occipital-temporal cortex</article-title>. <source hwp:id="source-8">Curr. Opin. Neurobiol</source>. <volume>15</volume>, <fpage>145</fpage>–<lpage>153</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Bedny M."><surname>Bedny</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pascual-Leone A."><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dodell-Feder D."><surname>Dodell-Feder</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fedorenko E."><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Saxe R"><surname>Saxe</surname>, <given-names>R</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-8">Language processing in the occipital cortex of congenitally blind adults</article-title>. <source hwp:id="source-9">Proc. Natl. Acad. Sci. USA</source> <volume>108</volume>, <fpage>4429</fpage>–<lpage>4434</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Ben-Artzi E."><surname>Ben-Artzi</surname>, <given-names>E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Marks L.E"><surname>Marks</surname>, <given-names>L.E</given-names></string-name>. (<year>1995</year>). <article-title hwp:id="article-title-9">Visual-auditory interaction in speeded classification: Role of stimulus difference</article-title>. <source hwp:id="source-10">Percept. Psychophys</source>. <volume>57</volume>, <fpage>1151</fpage>–<lpage>1162</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Bhat J."><surname>Bhat</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Miller L.M."><surname>Miller</surname>, <given-names>L.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pitt M.A."><surname>Pitt</surname>, <given-names>M.A.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Shahin A.J"><surname>Shahin</surname>, <given-names>A.J</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-10">Putative mechanisms mediating tolerance for audiovisual stimulus onset asynchrony</article-title>. <source hwp:id="source-11">J. Neurophysiol</source>. <volume>113</volume>, <fpage>1437</fpage>–<lpage>1450</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Binder J.R."><surname>Binder</surname>, <given-names>J.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Westbury C.F."><surname>Westbury</surname>, <given-names>C.F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McKiernan K.A."><surname>McKiernan</surname>, <given-names>K.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Possing E.T."><surname>Possing</surname>, <given-names>E.T.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Medler D.A"><surname>Medler</surname>, <given-names>D.A</given-names></string-name>. (<year>2005</year>). <article-title hwp:id="article-title-11">Distinct brain systems for processing concrete and abstract concepts</article-title>. <source hwp:id="source-12">J. Cognitive Neurosci</source>. <volume>17</volume>, <fpage>905</fpage>–<lpage>917</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Blasi D.E."><surname>Blasi</surname>, <given-names>D.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wichmann S."><surname>Wichmann</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hammarström H."><surname>Hammarström</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stadler P.F."><surname>Stadler</surname>, <given-names>P.F.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Christiansen M.H"><surname>Christiansen</surname>, <given-names>M.H</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-12">Sound-meaning association biases evidenced across thousands of languages</article-title>. <source hwp:id="source-13">Proc. Natl. Acad. Sci. USA</source> <volume>113</volume>, <fpage>10818</fpage>–<lpage>10823</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2 xref-ref-14-3 xref-ref-14-4 xref-ref-14-5"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Blazhenkova O."><surname>Blazhenkova</surname>, <given-names>O.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kozhevnikov M"><surname>Kozhevnikov</surname>, <given-names>M</given-names></string-name>. (<year>2009</year>). <article-title hwp:id="article-title-13">The new object-spatial-verbal cognitive style model: Theory and measurement</article-title>. <source hwp:id="source-14">Appl. Cognitive Psych</source>. <volume>23</volume>, <fpage>638</fpage>–<lpage>663</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Busse L."><surname>Busse</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Roberts K. C."><surname>Roberts</surname>, <given-names>K. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crist R. E."><surname>Crist</surname>, <given-names>R. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weissman D. H."><surname>Weissman</surname>, <given-names>D. H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Woldorff M. G"><surname>Woldorff</surname>, <given-names>M. G</given-names></string-name>. (<year>2005</year>). <article-title hwp:id="article-title-14">The spread of attention across modalities and space in a multisensory object</article-title>. <source hwp:id="source-15">Proc. Natl. Acad. Sci. USA</source> <volume>102</volume>, <fpage>18751</fpage>–<lpage>18756</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Catricalà M."><surname>Catricalà</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Guidi A"><surname>Guidi</surname>, <given-names>A</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-15">Onomatopoeias: a new perspective around space, image schemas, and phoneme clusters</article-title>. <source hwp:id="source-16">Cogn. Process</source>. <volume>16</volume>(<issue>Suppl 1</issue>), <fpage>S175</fpage>–<lpage>S178</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Cavanna A.E."><surname>Cavanna</surname>, <given-names>A.E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Trimble M.R"><surname>Trimble</surname>, <given-names>M.R</given-names></string-name>. (<year>2006</year>). <article-title hwp:id="article-title-16">The precuneus: a review of its functional anatomy and behavioural correlates</article-title>. <source hwp:id="source-17">Brain</source> <volume>129</volume>, <fpage>564</fpage>–<lpage>583</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Crottaz-Herbette S."><surname>Crottaz-Herbette</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Menon V"><surname>Menon</surname>, <given-names>V</given-names></string-name>. (<year>2006</year>). <article-title hwp:id="article-title-17">Where and when the anterior cingulate cortex modulates attentional response: combined fMRI and ERP evidence</article-title>. <source hwp:id="source-18">J. Cognitive Neurosci</source>. <volume>18</volume>, <fpage>766</fpage>–<lpage>80</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Dehaene S."><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Piazza M."><surname>Piazza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pinel P."><surname>Pinel</surname>, <given-names>P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Cohen L"><surname>Cohen</surname>, <given-names>L</given-names></string-name>. (<year>2003</year>). <article-title hwp:id="article-title-18">Three parietal circuits for number processing</article-title>. <source hwp:id="source-19">Cognitive Neuropsych</source>. <volume>20</volume>, <fpage>487</fpage>–<lpage>506</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20"><label>20.</label><citation publication-type="other" citation-type="journal" ref:id="478347v3.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="de Saussure F."><surname>de Saussure</surname>, <given-names>F.</given-names></string-name> <year>(1916</year>/2009). <article-title hwp:id="article-title-19">Course in General Linguistics</article-title>. <source hwp:id="source-20">Open Court Classics: Peru, IL</source>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Downar J."><surname>Downar</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crawley A.P."><surname>Crawley</surname>, <given-names>A.P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mikulis D.J."><surname>Mikulis</surname>, <given-names>D.J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Davis K.D"><surname>Davis</surname>, <given-names>K.D</given-names></string-name>. (<year>2002</year>). <article-title hwp:id="article-title-20">A cortical network sensitive to stimulus salience in a neural behavioral context across multiple sensory modalities</article-title>. <source hwp:id="source-21">J. Neurophysiol</source>. <volume>87</volume>, <fpage>615</fpage>–<lpage>620</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Driver J."><surname>Driver</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Spence C"><surname>Spence</surname>, <given-names>C</given-names></string-name>. (<year>2000</year>). <article-title hwp:id="article-title-21">Multisensory perception: Beyond modularity and convergence</article-title>. <source hwp:id="source-22">Curr. Biol</source>. <volume>10</volume>, <fpage>R731</fpage>–<lpage>R735</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2 xref-ref-23-3"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Duncan J"><surname>Duncan</surname>, <given-names>J</given-names></string-name>. (<year>2013</year>). <article-title hwp:id="article-title-22">The structure of cognition: attentional episodes in mind and brain</article-title>. <source hwp:id="source-23">Neuron</source> <volume>80</volume>, <fpage>35</fpage>–<lpage>50</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>24.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Duvernoy H.M"><surname>Duvernoy</surname>, <given-names>H.M</given-names></string-name>. (<year>1999</year>). <source hwp:id="source-24">The Human Brain. Surface, Blood Supply and Three-dimensional Sectional Anatomy</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2 xref-ref-25-3 xref-ref-25-4"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Eger E."><surname>Eger</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sterzer P."><surname>Sterzer</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Russ M.O."><surname>Russ</surname>, <given-names>M.O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Giraud A.-L."><surname>Giraud</surname>, <given-names>A.-L.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kleinschmidt A."><surname>Kleinschmidt</surname>, <given-names>A.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-23">A supramodal number representation in human intraparietal cortex</article-title>. <source hwp:id="source-25">Neuron</source> <volume>37</volume>, <fpage>719</fpage>–<lpage>725</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Eklund A."><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichols T.E."><surname>Nichols</surname>, <given-names>T.E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Knutsson H"><surname>Knutsson</surname>, <given-names>H</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-24">Cluster failure: why fMRI inferences for spatial extent have inflated false-positive rates</article-title>. <source hwp:id="source-26">Proc. Natl. Acad. Sci. USA</source> <volume>113</volume>, <fpage>7900</fpage>–<lpage>7905</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2 xref-ref-27-3 xref-ref-27-4 xref-ref-27-5 xref-ref-27-6 xref-ref-27-7 xref-ref-27-8 xref-ref-27-9"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Erickson L.C."><surname>Erickson</surname>, <given-names>L.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heeg E."><surname>Heeg</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rauschecker J.P."><surname>Rauschecker</surname>, <given-names>J.P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Turkeltaub P.E"><surname>Turkeltaub</surname>, <given-names>P.E</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-25">An ALE meta-analysis on the audiovisual integration of speech signals</article-title>. <source hwp:id="source-27">Hum. Brain Mapp</source>. <volume>35</volume>, <fpage>5587</fpage>–<lpage>5605</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2 xref-ref-28-3 xref-ref-28-4"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Fedorenko E."><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Behr M. K."><surname>Behr</surname>, <given-names>M. K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kanwisher N. G"><surname>Kanwisher</surname>, <given-names>N. G</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-26">Functional specificity for high-level linguistic processing in the human brain</article-title>. <source hwp:id="source-28">Proc. Natl. Acad. Sci. USA</source> <volume>108</volume>, <fpage>16428</fpage>–<lpage>16433</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Fedorenko E."><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duncan J."><surname>Duncan</surname>, <given-names>J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kanwisher N"><surname>Kanwisher</surname>, <given-names>N</given-names></string-name>. (<year>2013</year>). <article-title hwp:id="article-title-27">Broad domain generality in focal regions of frontal and parietal cortex</article-title>. <source hwp:id="source-29">Proc. Natl. Acad. Sci. USA</source> <volume>110</volume>, <fpage>16616</fpage>–<lpage>16621</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2 xref-ref-30-3 xref-ref-30-4 xref-ref-30-5 xref-ref-30-6 xref-ref-30-7 xref-ref-30-8 xref-ref-30-9 xref-ref-30-10"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Fedorenko E."><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hsieh P.J."><surname>Hsieh</surname>, <given-names>P.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nieo-Castañón A."><surname>Nieo-Castañón</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Whitfield-Gabrieli S."><surname>Whitfield-Gabrieli</surname>, <given-names>S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kanwisher N"><surname>Kanwisher</surname>, <given-names>N</given-names></string-name>. (<year>2010</year>). <article-title hwp:id="article-title-28">New method for fMRI investigations of language: defining ROIs functionally in individual subjects</article-title>. <source hwp:id="source-30">J. Neurophysiol</source>. <volume>104</volume>, <fpage>1177</fpage>–<lpage>1194</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Forman S.D."><surname>Forman</surname>, <given-names>S.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cohen J.D."><surname>Cohen</surname>, <given-names>J.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fitzgerald M."><surname>Fitzgerald</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eddy W.F."><surname>Eddy</surname>, <given-names>W.F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mintun M.A."><surname>Mintun</surname>, <given-names>M.A.</given-names></string-name> <etal>et al.</etal> (<year>1995</year>). <article-title hwp:id="article-title-29">Improved assessment of significant activation in functional magnetic resonance imaging (fMRI): use of a cluster-size threshold</article-title>. <source hwp:id="source-31">Magn. Reson. Med</source>. <volume>33</volume>, <fpage>636</fpage>–<lpage>647</lpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Fort M."><surname>Fort</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martin A."><surname>Martin</surname>, <given-names>A.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Peperkamp S"><surname>Peperkamp</surname>, <given-names>S</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-30">Consonants are more important than vowels in the Bouba-Kiki effect</article-title>. <source hwp:id="source-32">Lang. Speech</source> <volume>58</volume>, <fpage>247</fpage>–<lpage>266</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Ganis G."><surname>Ganis</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thompson W.L."><surname>Thompson</surname>, <given-names>W.L.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kosslyn S.M"><surname>Kosslyn</surname>, <given-names>S.M</given-names></string-name>. (<year>2004</year>). <article-title hwp:id="article-title-31">Brain areas underlying visual mental imagery and visual perception: an fMRI study</article-title>. <source hwp:id="source-33">Cognitive Brain Res</source>. <volume>20</volume>, <fpage>226</fpage>–<lpage>241</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2 xref-ref-34-3"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Greenwald A.G."><surname>Greenwald</surname>, <given-names>A.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGhee D.E."><surname>McGhee</surname>, <given-names>D.E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Schwarz J.L.K"><surname>Schwarz</surname>, <given-names>J.L.K</given-names></string-name>. (<year>1998</year>). <article-title hwp:id="article-title-32">Measuring individual differences in implicit cognition: the implicit association test</article-title>. <source hwp:id="source-34">J. Pers. Soc. Psychol</source>. <volume>74</volume>, <fpage>1464</fpage>–<lpage>1480</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2 xref-ref-35-3 xref-ref-35-4"><label>35.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Hartwigsen G."><surname>Hartwigsen</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baumgaertner A."><surname>Baumgaertner</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Price C.J."><surname>Price</surname>, <given-names>C.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koehnke M."><surname>Koehnke</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ulmer S."><surname>Ulmer</surname>, <given-names>S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Siebner H.R"><surname>Siebner</surname>, <given-names>H.R</given-names></string-name>. (<year>2010</year>). <article-title hwp:id="article-title-33">Phonological decisions require both left and right supramarginal gyri</article-title>. <source hwp:id="source-35">Proc. Natl. Acad. Sci. USA</source> <volume>107</volume>, <fpage>16494</fpage>–<lpage>16499</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Hayes D.J."><surname>Hayes</surname>, <given-names>D.J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Huxtable A.G"><surname>Huxtable</surname>, <given-names>A.G</given-names></string-name>. (<year>2012</year>). <article-title hwp:id="article-title-34">Interpreting deactivations in neuroimaging</article-title>. <source hwp:id="source-36">Front. Psychol</source>. <volume>3</volume>, <fpage>27</fpage> doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2012.00027</pub-id></citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2 xref-ref-37-3 xref-ref-37-4 xref-ref-37-5 xref-ref-37-6 xref-ref-37-7"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Hein G."><surname>Hein</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doehrmann O."><surname>Doehrmann</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Müller N.G."><surname>Müller</surname>, <given-names>N.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kaiser J."><surname>Kaiser</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Muckli L."><surname>Muckli</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naumer M.J."><surname>Naumer</surname>, <given-names>M.J.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-35">Object familiarity and semantic congruency modulate responses in cortical audiovisual integration areas</article-title>. <source hwp:id="source-37">J. Neurosci</source>. <volume>27</volume>, <fpage>7881</fpage>–<lpage>7887</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Holmes K.J."><surname>Holmes</surname>, <given-names>K.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Alcat C."><surname>Alcat</surname>, <given-names>C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lourenco S.F"><surname>Lourenco</surname>, <given-names>S.F</given-names></string-name>. (<year>2019</year>). <article-title hwp:id="article-title-36">Is emotional magnitude spatialized?</article-title> <source hwp:id="source-38">A further investigation. Cognitive Sci</source>. <volume>43</volume>, <fpage>e12727</fpage>, doi:<pub-id pub-id-type="doi">10.1111/cogs.12727</pub-id>.</citation></ref><ref id="c39" hwp:id="ref-39"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Hu D.W."><surname>Hu</surname>, <given-names>D.W.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Huang L.M"><surname>Huang</surname>, <given-names>L.M</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-37">Negative hemodynamic response in the cortex: evidence opposing neuronal deactivation revealed via optical imaging and electrophysiological recording</article-title>. <source hwp:id="source-39">J. Neurophysiol</source>. <volume>114</volume>, <fpage>2152</fpage>–<lpage>2161</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Hutzler F"><surname>Hutzler</surname>, <given-names>F</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-38">Reverse inference is not a fallacy per se: cognitive processes can be inferred from functional imaging data</article-title>. <source hwp:id="source-40">NeuroImage</source> <volume>84</volume>, <fpage>1061</fpage>–<lpage>1069</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Jackendoff R"><surname>Jackendoff</surname>, <given-names>R</given-names></string-name>. (<year>2002</year>). <source hwp:id="source-41">Foundations of Language: Brain, Meaning, Grammar, Evolution</source>. <publisher-name>Oxford University Press</publisher-name>: <publisher-loc>Oxford, UK</publisher-loc>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Jamal Y."><surname>Jamal</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nygaard L."><surname>Nygaard</surname>, <given-names>L.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-39">Interactions between auditory elevation, auditory pitch, and visual elevation during multisensory perception</article-title>. <source hwp:id="source-42">Multisens. Res</source>. <volume>30</volume>, <fpage>287</fpage>–<lpage>306</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Joseph J. E"><surname>Joseph</surname>, <given-names>J. E</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-40">Iconicity in Saussure’s Linguistic Work, and why it does not contradict the arbitrariness of the sign</article-title>. <source hwp:id="source-43">Hist. Linguist</source>. <volume>42</volume>, <fpage>85</fpage>–<lpage>105</lpage>. doi:<pub-id pub-id-type="doi">10.1075/hl.42.1.05jos</pub-id>.</citation></ref><ref id="c44" hwp:id="ref-44"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Kilintari M."><surname>Kilintari</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Narayana S."><surname>Narayana</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Babajani-Feremi A."><surname>Babajani-Feremi</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rezaie R."><surname>Rezaie</surname>, <given-names>R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Papanicolaou A.C"><surname>Papanicolaou</surname>, <given-names>A.C</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-41">Brain activation profiles during kinesthetic and visual imagery: an fMRI study</article-title>. <source hwp:id="source-44">Brain Res</source>. <volume>1646</volume>, <fpage>249</fpage>–<lpage>261</lpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Kita S"><surname>Kita</surname>, <given-names>S</given-names></string-name>. (<year>1997</year>). <article-title hwp:id="article-title-42">Two-dimensional semantic analysis of Japanese mimetics</article-title>. <source hwp:id="source-45">Linguistics</source> <volume>35</volume>, <fpage>379</fpage>–<lpage>415</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Kitada R."><surname>Kitada</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kwon J."><surname>Kwon</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Doizaki R."><surname>Doizaki</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nakagawa E."><surname>Nakagawa</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tanigawa T."><surname>Tanigawa</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> (<year>2021</year>). <article-title hwp:id="article-title-43">Brain networks underlying the processing of sound symbolism related to softness perception</article-title>. <source hwp:id="source-46">Sci Rep UK</source> <volume>11</volume>, <fpage>7399</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41598-021-86328-6</pub-id></citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Knoeferle K."><surname>Knoeferle</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li J."><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maggioni E."><surname>Maggioni</surname>, <given-names>E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Spence C"><surname>Spence</surname>, <given-names>C</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-44">What drives sound symbolism?</article-title> <source hwp:id="source-47">Different acoustic cues underlie sound-size and sound-shape mappings. Sci. Rep. UK</source> <volume>7</volume>, <fpage>5562</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41598-017-05965-y</pub-id></citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1 xref-ref-48-2"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.48" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Köhler W"><surname>Köhler</surname>, <given-names>W</given-names></string-name>. (<year>1929</year>). <article-title hwp:id="article-title-45"><italic toggle="yes">Gestalt Psychology</italic>. Liveright: New York</article-title>, <source hwp:id="source-48">NY</source>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1 xref-ref-49-2"><label>49.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Köhler W"><surname>Köhler</surname>, <given-names>W</given-names></string-name>. (<year>1947</year>). <article-title hwp:id="article-title-46"><italic toggle="yes">Gestalt Psychology: An Introduction to New Concepts in Modern Psychology.</italic> Liveright: New York</article-title>, <source hwp:id="source-49">NY</source>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Ković V."><surname>Ković</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Plunkett K."><surname>Plunkett</surname>, <given-names>K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Westermann G"><surname>Westermann</surname>, <given-names>G</given-names></string-name>. (<year>2010</year>). <article-title hwp:id="article-title-47">The shape of words in the brain</article-title>. <source hwp:id="source-50">Cognition</source> <volume>114</volume>, <fpage>19</fpage>–<lpage>28</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>51.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Kunihira S"><surname>Kunihira</surname>, <given-names>S</given-names></string-name>. (<year>1971</year>). <article-title hwp:id="article-title-48">Effects of the expressive voice on phonetic symbolism</article-title>. <source hwp:id="source-51">J. Verb. Learn. Verb. Be</source>. <volume>10</volume>, <fpage>427</fpage>–<lpage>429</lpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Lacadie C.M."><surname>Lacadie</surname>, <given-names>C.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fulbright R.K."><surname>Fulbright</surname>, <given-names>R.K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Constable R.T."><surname>Constable</surname>, <given-names>R.T.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Papademetris X"><surname>Papademetris</surname>, <given-names>X</given-names></string-name>. (<year>2008</year>). <article-title hwp:id="article-title-49">More accurate Talairach coordinates for neuroimaging using nonlinear registration</article-title>. <source hwp:id="source-52">NeuroImage</source> <volume>42</volume>, <fpage>717</fpage>–<lpage>725</lpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1 xref-ref-53-2"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jamal Y."><surname>Jamal</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="List S.M."><surname>List</surname>, <given-names>S.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sathian K."><surname>Sathian</surname>, <given-names>K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nygaard L.C"><surname>Nygaard</surname>, <given-names>L.C</given-names></string-name>. (<year>2020</year>). <article-title hwp:id="article-title-50">Stimulus parameters underlying sound-symbolic mapping of auditory pseudowords to visual shapes</article-title>. <source hwp:id="source-53">Cognitive Sci</source>. <volume>44</volume>, <fpage>e12883</fpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1 xref-ref-54-2 xref-ref-54-3"><label>54.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin J.B."><surname>Lin</surname>, <given-names>J.B.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-51">Object and spatial imagery dimensions in visuo-haptic representations</article-title>. <source hwp:id="source-54">Exp. Brain Res</source>. <volume>213</volume>, <fpage>267</fpage>–<lpage>273</lpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2"><label>55.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martinez M.O."><surname>Martinez</surname>, <given-names>M.O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-52">Synesthesia strengthens sound-symbolic cross-modal correspondences</article-title>. <source hwp:id="source-55">Eur. J. Neurosci</source>. <volume>44</volume>, <fpage>2716</fpage>–<lpage>2721</lpage>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>56.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stilla R."><surname>Stilla</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Deshpande G."><surname>Deshpande</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhao S."><surname>Zhao</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stephens C."><surname>Stephens</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kemmerer D."><surname>Kemmerer</surname>, <given-names>D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-53">Engagement of the left extra-striate body area during body-part metaphor comprehension</article-title>. <source hwp:id="source-56">Brain Lang</source>. <volume>166</volume>, <fpage>1</fpage>–<lpage>18</lpage>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><label>57.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stilla R."><surname>Stilla</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sreenivasan K."><surname>Sreenivasan</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Deshpande G."><surname>Deshpande</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-54">Spatial imagery in haptic shape perception</article-title>. <source hwp:id="source-57">Neuropsychologia</source> <volume>60</volume>, <fpage>144</fpage>–<lpage>158</lpage>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1 xref-ref-58-2 xref-ref-58-3 xref-ref-58-4"><label>58.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Lockwood G."><surname>Lockwood</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hagoort P."><surname>Hagoort</surname>, <given-names>P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dingemanse M"><surname>Dingemanse</surname>, <given-names>M</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-55">How iconicity helps people learn new words: neural correlates and individual differences in sound-symbolic bootstrapping</article-title>. <source hwp:id="source-58">Collabra</source> <volume>2</volume>, <fpage>7</fpage>, doi: <pub-id pub-id-type="doi">10.1525/Collabra.42</pub-id></citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1 xref-ref-59-2"><label>59.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Lourenco S.F."><surname>Lourenco</surname>, <given-names>S.F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bonny J.W."><surname>Bonny</surname>, <given-names>J.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fernandez E. P."><surname>Fernandez</surname>, <given-names>E. P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rao S"><surname>Rao</surname>, <given-names>S</given-names></string-name>. (<year>2012</year>). <article-title hwp:id="article-title-56">Nonsymbolic number and cumulative area representations contribute shared and unique variance to symbolic math competence</article-title>. <source hwp:id="source-59">Proc. Natl. Acad. Sci. USA</source> <volume>109</volume>, <fpage>18737</fpage>–<lpage>18742</lpage>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1 xref-ref-60-2 xref-ref-60-3"><label>60.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.60" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Lourenco S. F."><surname>Lourenco</surname>, <given-names>S. F.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Longo M. R"><surname>Longo</surname>, <given-names>M. R</given-names></string-name>. (<year>2011</year>). <chapter-title>Origins and development of generalized magnitude representation</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Dehaene S."><given-names>S.</given-names> <surname>Dehaene</surname></string-name> and <string-name name-style="western" hwp:sortable="Brannon E."><given-names>E.</given-names> <surname>Brannon</surname></string-name></person-group> (Eds.), <source hwp:id="source-60">Space, Time, and Number in the Brain: Searching for the Foundations of Mathematical Thought</source>. (pp. <fpage>225</fpage>–<lpage>244</lpage>). <publisher-name>Elsevier</publisher-name>.</citation></ref><ref id="c61" hwp:id="ref-61"><label>61.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Ma Z.G."><surname>Ma</surname>, <given-names>Z.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cao P.J."><surname>Cao</surname>, <given-names>P.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sun P.C."><surname>Sun</surname>, <given-names>P.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhao L.N."><surname>Zhao</surname>, <given-names>L.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li L.M."><surname>Li</surname>, <given-names>L.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tong S.B."><surname>Tong</surname>, <given-names>S.B.</given-names></string-name> <etal>et al.</etal>, (<year>2016</year>). <article-title hwp:id="article-title-57">Inverted optical intrinsic response accompanied by decreased cerebral blood flow are related to both neuronal inhibition and excitation</article-title>. <source hwp:id="source-61">Sci. Rep. UK</source> <volume>6</volume>, <fpage>21627</fpage> doi: <pub-id pub-id-type="doi">10.1038/srep21627</pub-id></citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><label>62.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.62" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name> (<year>2015</year>). <source hwp:id="source-62">[Sound to visual shape mappings]. Unpublished raw data</source>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1 xref-ref-63-2 xref-ref-63-3 xref-ref-63-4"><label>63.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.63" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kim J.Y."><surname>Kim</surname>, <given-names>J.Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="List S."><surname>List</surname>, <given-names>S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nygaard L.C"><surname>Nygaard</surname>, <given-names>L.C</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-58">Sound to meaning mappings in the bouba-kiki effect</article-title>. <source hwp:id="source-63">Proceedings 37<sup>th</sup> Annual Meeting Cognitive Science Society</source>, <fpage>1565</fpage>-<lpage>1570</lpage>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1 xref-ref-64-2 xref-ref-64-3"><label>64.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stilla R."><surname>Stilla</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nygaard L.C."><surname>Nygaard</surname>, <given-names>L.C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2018</year>). <article-title hwp:id="article-title-59">Neural basis of the crossmodal correspondence between auditory pitch and visuospatial elevation</article-title>. <source hwp:id="source-64">Neuropsychologia</source> <volume>112</volume>, <fpage>19</fpage>–<lpage>30</lpage>.</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1"><label>65.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="McCormick K."><surname>McCormick</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stilla R."><surname>Stilla</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nygaard L.C."><surname>Nygaard</surname>, <given-names>L.C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2018a</year>). <article-title hwp:id="article-title-60">Neural basis of the sound-symbolic crossmodal correspondence between auditory pseudowords and visual shapes. Preprint</article-title>, <source hwp:id="source-65">bioRxiv.org</source>, doi:<pub-id pub-id-type="doi">10.1101/478347</pub-id>.</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1 xref-ref-66-2 xref-ref-66-3 xref-ref-66-4 xref-ref-66-5"><label>66.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Marchant J.L."><surname>Marchant</surname>, <given-names>J.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ruff C.C."><surname>Ruff</surname>, <given-names>C.C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Driver J"><surname>Driver</surname>, <given-names>J</given-names></string-name>. (<year>2012</year>). <article-title hwp:id="article-title-61">Audiovisual asynchrony enhances BOLD responses in a brain network including multisensory STS while also enhancing target-detection performance for both modalities</article-title>. <source hwp:id="source-66">Hum. Brain Mapp</source>. <volume>33</volume>, <fpage>1212</fpage>–<lpage>1224</lpage>.</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><label>67.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Mastroberardino S."><surname>Mastroberardino</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Santangelo V."><surname>Santangelo</surname>, <given-names>V.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Macaluso E"><surname>Macaluso</surname>, <given-names>E</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-62">Crossmodal semantic congruence can affect visuo-spatial processing and activity of the fronto-parietal attention networks</article-title>. <source hwp:id="source-67">Front. Integr. Neurosci</source>. <volume>9</volume>, <fpage>45</fpage>, doi:<pub-id pub-id-type="doi">10.3389/fnint.2015.00045</pub-id></citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><label>68.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Meteyard L."><surname>Meteyard</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stoppard E."><surname>Stoppard</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Snudden D."><surname>Snudden</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cappa S.F."><surname>Cappa</surname>, <given-names>S.F.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Vigliocco G"><surname>Vigliocco</surname>, <given-names>G</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-63">When semantics aids phonology: a processing advantage for iconic word forms in aphasia</article-title>. <source hwp:id="source-68">Neuropsychologia</source> <volume>76</volume>, <fpage>264</fpage>–<lpage>275</lpage>.</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1"><label>69.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Mikl M."><surname>Mikl</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mareček R."><surname>Mareček</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hluštik P."><surname>Hluštik</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pavlicová M."><surname>Pavlicová</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Drastich A."><surname>Drastich</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chlebus P."><surname>Chlebus</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brázdil M."><surname>Brázdil</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Krupa P"><surname>Krupa</surname>, <given-names>P</given-names></string-name>. (<year>2008</year>). <article-title hwp:id="article-title-64">Effects of spatial smoothing on fMRI group inferences</article-title>. <source hwp:id="source-69">Magn. Reson. Imaging</source> <volume>26</volume>, <fpage>490</fpage>–<lpage>503</lpage>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><label>70.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Miller J"><surname>Miller</surname>, <given-names>J</given-names></string-name>. (<year>1991</year>). <article-title hwp:id="article-title-65">Channel interaction and the redundant targets effect in bimodal divided attention</article-title>. <source hwp:id="source-70">J. Exp. Psychol. Human</source> <volume>17</volume>, <fpage>160</fpage>–<lpage>169</lpage>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><label>71.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Molholm S."><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martinez A."><surname>Martinez</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shpaner M."><surname>Shpaner</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Foxe J. J"><surname>Foxe</surname>, <given-names>J. J</given-names></string-name>. (<year>2007</year>). <article-title hwp:id="article-title-66">Object-based attention is multisensory: Coactivation of an object’s representations in ignored sensory modalities</article-title>. <source hwp:id="source-71">Eur. J. Neurosci</source>. <volume>26</volume>, <fpage>499</fpage>–<lpage>509</lpage>.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><label>72.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Molholm S."><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ritter W."><surname>Ritter</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Javitt D. C."><surname>Javitt</surname>, <given-names>D. C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Foxe J. J"><surname>Foxe</surname>, <given-names>J. J</given-names></string-name>. (<year>2004</year>). <article-title hwp:id="article-title-67">Multisensory visual-auditory object recognition in humans: A high-density electrical mapping study</article-title>. <source hwp:id="source-72">Cereb. Cortex</source> <volume>14</volume>, <fpage>452</fpage>–<lpage>465</lpage>.</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><label>73.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Monaghan P."><surname>Monaghan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mattock K."><surname>Mattock</surname>, <given-names>K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Walker P"><surname>Walker</surname>, <given-names>P</given-names></string-name>. (<year>2012</year>). <article-title hwp:id="article-title-68">The role of sound symbolism in language learning</article-title>. <source hwp:id="source-73">J. Exp. Psychol. Learn</source>. <volume>38</volume>, <fpage>1152</fpage>–<lpage>1164</lpage>.</citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><label>74.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Nielsen A."><surname>Nielsen</surname>, <given-names>A.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rendall D"><surname>Rendall</surname>, <given-names>D</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-69">The sound of round: evaluating the sound-symbolic role of consonants in the classic Takete-Maluma phenomenon</article-title>. <source hwp:id="source-74">Can. J. Exp. Psychol</source>. <volume>65</volume>, <fpage>115</fpage>–<lpage>124</lpage>.</citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1 xref-ref-75-2 xref-ref-75-3 xref-ref-75-4 xref-ref-75-5 xref-ref-75-6 xref-ref-75-7"><label>75.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Noesselt T."><surname>Noesselt</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bergmann D."><surname>Bergmann</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heinze H.-J."><surname>Heinze</surname>, <given-names>H.-J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Münte T."><surname>Münte</surname>, <given-names>T.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Spence C"><surname>Spence</surname>, <given-names>C</given-names></string-name>. (<year>2012</year>). <article-title hwp:id="article-title-70">Coding of multisensory temporal patterns in human superior temporal sulcus</article-title>. <source hwp:id="source-75">Front. Integr. Neurosci</source>. <volume>6</volume>, <fpage>64</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fnint.2012.00064</pub-id></citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1 xref-ref-76-2 xref-ref-76-3 xref-ref-76-4"><label>76.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.76" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Noppeney U"><surname>Noppeney</surname>, <given-names>U</given-names></string-name>. (<year>2012</year>). <chapter-title>Characterization of multisensory integration with fMRI</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-3"><string-name name-style="western" hwp:sortable="Murray M.M."><given-names>M.M.</given-names> <surname>Murray</surname></string-name> and <string-name name-style="western" hwp:sortable="Wallace M.T."><given-names>M.T.</given-names> <surname>Wallace</surname></string-name></person-group> (Eds.), <source hwp:id="source-76">The Neural Bases of Multisensory Processes</source>. (pp<fpage>233</fpage>–<lpage>252</lpage>). <publisher-name>CRC Press</publisher-name>.</citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1 xref-ref-77-2 xref-ref-77-3 xref-ref-77-4 xref-ref-77-5 xref-ref-77-6 xref-ref-77-7"><label>77.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Noppeney U."><surname>Noppeney</surname>, <given-names>U.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Josephs O."><surname>Josephs</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hocking J."><surname>Hocking</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Price C.J."><surname>Price</surname>, <given-names>C.J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Friston K.J"><surname>Friston</surname>, <given-names>K.J</given-names></string-name>. (<year>2008</year>). <article-title hwp:id="article-title-71">The effect of prior visual information on recognition of speech and sounds. <italic toggle="yes">Cerebr</italic></article-title>. <source hwp:id="source-77">Cortex</source> <volume>18</volume>, <fpage>598</fpage>–<lpage>609</lpage>.</citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><label>78.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.78" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Nygaard L. C."><surname>Nygaard</surname>, <given-names>L. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cook A. E."><surname>Cook</surname>, <given-names>A. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Namy L. L"><surname>Namy</surname>, <given-names>L. L</given-names></string-name>. (<year>2009</year>). <article-title hwp:id="article-title-72">Sound to meaning correspondences facilitate word learning</article-title>. <source hwp:id="source-78">Cognition</source> <volume>112</volume>, <fpage>181</fpage>–<lpage>186</lpage>.</citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><label>79.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.79" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Oberhuber M."><surname>Oberhuber</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hope T.M.H."><surname>Hope</surname>, <given-names>T.M.H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seghier M.L."><surname>Seghier</surname>, <given-names>M.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones O.P."><surname>Jones</surname>, <given-names>O.P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prejawa S."><surname>Prejawa</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Green D.W."><surname>Green</surname>, <given-names>D.W.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Price C.J"><surname>Price</surname>, <given-names>C.J</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-73">Four functionally distinct regions in the left supramarginal gyrus support word processing. <italic toggle="yes">Cerebr</italic></article-title>. <source hwp:id="source-79">Cortex</source> <volume>26</volume>, <fpage>4212</fpage>–<lpage>4226</lpage>.</citation></ref><ref id="c80" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1"><label>80.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.80" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="Occelli V."><surname>Occelli</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin J.B."><surname>Lin</surname>, <given-names>J.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lacey S."><surname>Lacey</surname>, <given-names>S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sathian K"><surname>Sathian</surname>, <given-names>K</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-74">Loss of form vision impairs spatial imagery</article-title>. <source hwp:id="source-80">Front. Hum. Neurosci</source>. <volume>8</volume>, <fpage>159</fpage>, doi:<pub-id pub-id-type="doi">10.3389/fnhum.2014.00159</pub-id></citation></ref><ref id="c81" hwp:id="ref-81" hwp:rev-id="xref-ref-81-1"><label>81.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.81" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Ozturk O."><surname>Ozturk</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krehm M."><surname>Krehm</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Vouloumanos A"><surname>Vouloumanos</surname>, <given-names>A</given-names></string-name>. (<year>2013</year>). <article-title hwp:id="article-title-75">Sound symbolism in infancy: evidence for sound-shape cross-modal correspondences in 4-month-olds</article-title>. <source hwp:id="source-81">J. Exp. Child Psychol</source>. <volume>114</volume>, <fpage>173</fpage>–<lpage>186</lpage>.</citation></ref><ref id="c82" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1"><label>82.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.82" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Parise C.V."><surname>Parise</surname>, <given-names>C.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knorre K."><surname>Knorre</surname>, <given-names>K.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ernst M.O"><surname>Ernst</surname>, <given-names>M.O</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-76">Natural auditory scene statistics shapes human spatial hearing</article-title>. <source hwp:id="source-82">Proc. Natl. Acad. Sci. USA</source> <volume>111</volume>, <fpage>6104</fpage>–<lpage>6108</lpage>.</citation></ref><ref id="c83" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1 xref-ref-83-2 xref-ref-83-3 xref-ref-83-4"><label>83.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.83" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Parise C.V."><surname>Parise</surname>, <given-names>C.V.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Spence C"><surname>Spence</surname>, <given-names>C</given-names></string-name>. (<year>2012</year>). <article-title hwp:id="article-title-77">Audiovisual cross-modal correspondences and sound symbolism: a study using the implicit association test</article-title>. <source hwp:id="source-83">Exp. Brain Res</source>. <volume>220</volume>, <fpage>319</fpage>–<lpage>333</lpage>.</citation></ref><ref id="c84" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1 xref-ref-84-2 xref-ref-84-3 xref-ref-84-4 xref-ref-84-5 xref-ref-84-6 xref-ref-84-7 xref-ref-84-8 xref-ref-84-9 xref-ref-84-10 xref-ref-84-11 xref-ref-84-12"><label>84.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Peiffer-Smadja N."><surname>Peiffer-Smadja</surname>, <given-names>N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Cohen L"><surname>Cohen</surname>, <given-names>L</given-names></string-name>. (<year>2019</year>). <article-title hwp:id="article-title-78">The cerebral bases of the bouba-kiki effect</article-title>. <source hwp:id="source-84">NeuroImage</source> <volume>186</volume>, <fpage>679</fpage>–<lpage>689</lpage>.</citation></ref><ref id="c85" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1 xref-ref-85-2 xref-ref-85-3 xref-ref-85-4 xref-ref-85-5"><label>85.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.85" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Piazza M."><surname>Piazza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Izard V."><surname>Izard</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pinel P."><surname>Pinel</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Le Bihan D."><surname>Le Bihan</surname>, <given-names>D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dehaene S."><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-79">Tuning curves for approximate numerosity in the human intraparietal sulcus</article-title>. <source hwp:id="source-85">Neuron</source> <volume>44</volume>, <fpage>547</fpage>–<lpage>555</lpage>.</citation></ref><ref id="c86" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1 xref-ref-86-2 xref-ref-86-3 xref-ref-86-4 xref-ref-86-5 xref-ref-86-6"><label>86.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.86" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Piazza M."><surname>Piazza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pinel P."><surname>Pinel</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Le Bihan D."><surname>Le Bihan</surname>, <given-names>D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dehaene S."><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-80">A magnitude code common to numerosities and number symbols in human intraparietal cortex</article-title>. <source hwp:id="source-86">Neuron</source> <volume>53</volume>, <fpage>293</fpage>–<lpage>305</lpage>.</citation></ref><ref id="c87" hwp:id="ref-87" hwp:rev-id="xref-ref-87-1 xref-ref-87-2 xref-ref-87-3 xref-ref-87-4 xref-ref-87-5 xref-ref-87-6"><label>87.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.87" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-87"><string-name name-style="western" hwp:sortable="Pinel P."><surname>Pinel</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Piazza M."><surname>Piazza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Le Bihan D."><surname>Le Bihan</surname>, <given-names>D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dehaene S."><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-81">Distributed and overlapping cerebral representations of number, size, and luminance during comparative judgments</article-title>. <source hwp:id="source-87">Neuron</source> <volume>41</volume>, <fpage>983</fpage>–<lpage>993</lpage>.</citation></ref><ref id="c88" hwp:id="ref-88" hwp:rev-id="xref-ref-88-1"><label>88.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.88" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-88"><string-name name-style="western" hwp:sortable="Pinker S"><surname>Pinker</surname>, <given-names>S</given-names></string-name>. (<year>1999</year>). <source hwp:id="source-88">Words and Rules: The Ingredients of Language</source>. <publisher-name>Harper Collins</publisher-name>.</citation></ref><ref id="c89" hwp:id="ref-89" hwp:rev-id="xref-ref-89-1"><label>89.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.89" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-89"><string-name name-style="western" hwp:sortable="Pitt B."><surname>Pitt</surname>, <given-names>B.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Casasanto D"><surname>Casasanto</surname>, <given-names>D</given-names></string-name>. (<year>2018</year>). <article-title hwp:id="article-title-82">Spatializing emotion: no evidence for a domain-general magnitude system</article-title>. <source hwp:id="source-89">Cognitive Sci</source>. <volume>42</volume>, <fpage>2150</fpage>–<lpage>2180</lpage>.</citation></ref><ref id="c90" hwp:id="ref-90" hwp:rev-id="xref-ref-90-1"><label>90.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.90" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-90"><string-name name-style="western" hwp:sortable="Powers A.R. III"><surname>Powers</surname> <suffix>III</suffix>, <given-names>A.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hillock A.R."><surname>Hillock</surname>, <given-names>A.R.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wallace M.T"><surname>Wallace</surname>, <given-names>M.T</given-names></string-name>. (<year>2009</year>). <article-title hwp:id="article-title-83">Perceptual training narrows the temporal window of multisensory binding</article-title>. <source hwp:id="source-90">J. Neurosci</source>. <volume>29</volume>, <fpage>12265</fpage>–<lpage>12274</lpage>.</citation></ref><ref id="c91" hwp:id="ref-91" hwp:rev-id="xref-ref-91-1 xref-ref-91-2"><label>91.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.91" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-91"><string-name name-style="western" hwp:sortable="Price C.J."><surname>Price</surname>, <given-names>C.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moore C.J."><surname>Moore</surname>, <given-names>C.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Humphreys G.W."><surname>Humphreys</surname>, <given-names>G.W.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wise R.J.S"><surname>Wise</surname>, <given-names>R.J.S</given-names></string-name>. (<year>1997</year>). <article-title hwp:id="article-title-84">Segregating semantic from phonological processes during reading</article-title>. <source hwp:id="source-91">J. Cognitive Neurosci</source>. <volume>9</volume>, <fpage>727</fpage>–<lpage>733</lpage>.</citation></ref><ref id="c92" hwp:id="ref-92" hwp:rev-id="xref-ref-92-1"><label>92.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.92" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-92"><string-name name-style="western" hwp:sortable="Raczkowski D."><surname>Raczkowski</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kalat J.W."><surname>Kalat</surname>, <given-names>J.W.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nebes R"><surname>Nebes</surname>, <given-names>R</given-names></string-name>. (<year>1974</year>). <article-title hwp:id="article-title-85">Reliability and validity of some handedness questionnaire items</article-title>. <source hwp:id="source-92">Neuropsychologia</source> <volume>12</volume>, <fpage>43</fpage>–<lpage>47</lpage>.</citation></ref><ref id="c93" hwp:id="ref-93" hwp:rev-id="xref-ref-93-1 xref-ref-93-2"><label>93.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.93" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-93"><string-name name-style="western" hwp:sortable="Ramachandran V.S."><surname>Ramachandran</surname>, <given-names>V.S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hubbard E.M"><surname>Hubbard</surname>, <given-names>E.M</given-names></string-name>. (<year>2001</year>). <article-title hwp:id="article-title-86">Synaesthesia – a window into perception, thought and language</article-title>. <source hwp:id="source-93">J. Consciousness Stud</source>. <volume>8</volume>, <fpage>3</fpage>–<lpage>34</lpage>.</citation></ref><ref id="c94" hwp:id="ref-94" hwp:rev-id="xref-ref-94-1"><label>94.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.94" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-94"><string-name name-style="western" hwp:sortable="Regan D."><surname>Regan</surname>, <given-names>D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Spekreisje H"><surname>Spekreisje</surname>, <given-names>H</given-names></string-name>. (<year>1977</year>). <article-title hwp:id="article-title-87">Auditory-visual interactions and the correspondence between perceived auditory space and perceived visual space</article-title>. <source hwp:id="source-94">Perception</source> <volume>6</volume>, <fpage>133</fpage>–<lpage>138</lpage>.</citation></ref><ref id="c95" hwp:id="ref-95" hwp:rev-id="xref-ref-95-1 xref-ref-95-2"><label>95.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.95" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-95"><string-name name-style="western" hwp:sortable="Regenbogen C."><surname>Regenbogen</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seubert J."><surname>Seubert</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johannson E."><surname>Johannson</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Finkelmeyer A."><surname>Finkelmeyer</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Andersson P."><surname>Andersson</surname>, <given-names>P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lundström J.N."><surname>Lundström</surname>, <given-names>J.N.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-88">The intraparietal sulcus governs multisensory integration of audiovisual information based on task difficulty</article-title>. <source hwp:id="source-95">Hum. Brain Mapp</source>. <volume>39</volume>, <fpage>1313</fpage>–<lpage>1326</lpage>.</citation></ref><ref id="c96" hwp:id="ref-96" hwp:rev-id="xref-ref-96-1 xref-ref-96-2 xref-ref-96-3"><label>96.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.96" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-96"><string-name name-style="western" hwp:sortable="Revill K.P."><surname>Revill</surname>, <given-names>K.P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Namy L.L."><surname>Namy</surname>, <given-names>L.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="DeFife L.C."><surname>DeFife</surname>, <given-names>L.C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nygaard L.C"><surname>Nygaard</surname>, <given-names>L.C</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-89">Cross-linguistic sound symbolism and crossmodal correspondence: evidence from fMRI and DTI</article-title>. <source hwp:id="source-96">Brain Lang</source>. <volume>128</volume>, <fpage>18</fpage>–<lpage>24</lpage>.</citation></ref><ref id="c97" hwp:id="ref-97" hwp:rev-id="xref-ref-97-1"><label>97.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.97" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-97"><string-name name-style="western" hwp:sortable="Revill K. P."><surname>Revill</surname>, <given-names>K. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Namy L. L."><surname>Namy</surname>, <given-names>L. L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Nygaard L. C"><surname>Nygaard</surname>, <given-names>L. C</given-names></string-name>. (<year>2018</year>). <article-title hwp:id="article-title-90">Eye movements reveal persistent sensitivity to sound symbolism during word learning</article-title>. <source hwp:id="source-97">J. Exp. Psychol. Learn</source>. <volume>44</volume>, <fpage>680</fpage>–<lpage>698</lpage>.</citation></ref><ref id="c98" hwp:id="ref-98" hwp:rev-id="xref-ref-98-1"><label>98.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.98" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-98"><string-name name-style="western" hwp:sortable="Sambo C.F."><surname>Sambo</surname>, <given-names>C.F.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Forster B"><surname>Forster</surname>, <given-names>B</given-names></string-name>. (<year>2009</year>). <article-title hwp:id="article-title-91">An ERP investigation on visuotactile interactions inn peripersonal and extrapersonal space: evidence for the spatial rule</article-title>. <source hwp:id="source-98">J. Cognitive Neurosci</source>. <volume>21</volume>, <fpage>1550</fpage>–<lpage>1559</lpage>.</citation></ref><ref id="c99" hwp:id="ref-99" hwp:rev-id="xref-ref-99-1 xref-ref-99-2 xref-ref-99-3 xref-ref-99-4 xref-ref-99-5 xref-ref-99-6"><label>99.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.99" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-99"><string-name name-style="western" hwp:sortable="Sathian K."><surname>Sathian</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon T.J."><surname>Simon</surname>, <given-names>T.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peterson S."><surname>Peterson</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Patel G.A."><surname>Patel</surname>, <given-names>G.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoffman J.M."><surname>Hoffman</surname>, <given-names>J.M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Grafton S.T"><surname>Grafton</surname>, <given-names>S.T</given-names></string-name>. (<year>1999</year>). <article-title hwp:id="article-title-92">Neural evidence linking visual object enumeration and attention</article-title>. <source hwp:id="source-99">J. Cognitive Neurosci</source>. <volume>11</volume>, <fpage>36</fpage>–<lpage>51</lpage>.</citation></ref><ref id="c100" hwp:id="ref-100" hwp:rev-id="xref-ref-100-1"><label>100.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.100" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-100"><string-name name-style="western" hwp:sortable="Shams L."><surname>Shams</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Seitz A. R"><surname>Seitz</surname>, <given-names>A. R</given-names></string-name>. (<year>2008</year>). <article-title hwp:id="article-title-93">Benefits of multisensory learning</article-title>. <source hwp:id="source-100">Trends Cogn. Sci</source>. <volume>12</volume>, <fpage>411</fpage>–<lpage>7</lpage>.</citation></ref><ref id="c101" hwp:id="ref-101" hwp:rev-id="xref-ref-101-1 xref-ref-101-2"><label>101.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.101" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-101"><string-name name-style="western" hwp:sortable="Schmidtke D.S."><surname>Schmidtke</surname>, <given-names>D.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Conrad M."><surname>Conrad</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jacobs A.M"><surname>Jacobs</surname>, <given-names>A.M</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-94">Phonological iconicity</article-title>. <source hwp:id="source-101">Front. Psychol</source>. <volume>5</volume>, <fpage>80</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00080</pub-id></citation></ref><ref id="c102" hwp:id="ref-102" hwp:rev-id="xref-ref-102-1 xref-ref-102-2 xref-ref-102-3"><label>102.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.102" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-102"><string-name name-style="western" hwp:sortable="Sestieri C."><surname>Sestieri</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Di Matteo R."><surname>Di Matteo</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ferretti A."><surname>Ferretti</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Del Gratta C."><surname>Del Gratta</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Caulo M."><surname>Caulo</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tartaro A."><surname>Tartaro</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belardinelli M.O."><surname>Belardinelli</surname>, <given-names>M.O.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Romani G.L."><surname>Romani</surname>, <given-names>G.L.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-95">“What” versus “where” in the audiovisual domain: an fMRI study</article-title>. <source hwp:id="source-102">NeuroImage</source> <volume>33</volume>, <fpage>672</fpage>–<lpage>680</lpage>.</citation></ref><ref id="c103" hwp:id="ref-103" hwp:rev-id="xref-ref-103-1"><label>103.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.103" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-103"><string-name name-style="western" hwp:sortable="Shechter A."><surname>Shechter</surname>, <given-names>A.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Share D.L"><surname>Share</surname>, <given-names>D.L</given-names></string-name>. (<year>2020</year>). <article-title hwp:id="article-title-96">Keeping an eye on effort: A pupillometric investigation of effort and effortlessness in visual word recognition</article-title>. <source hwp:id="source-103">Psychol. Sci</source>. <volume>32</volume>, <fpage>80</fpage>–<lpage>95</lpage>.</citation></ref><ref id="c104" hwp:id="ref-104" hwp:rev-id="xref-ref-104-1"><label>104.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.104" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-104"><string-name name-style="western" hwp:sortable="Shrem T."><surname>Shrem</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Deouell L. Y"><surname>Deouell</surname>, <given-names>L. Y</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-97">Hierarchies of attention and experimental designs: Effects of spatial and intermodal attention revisited</article-title>. <source hwp:id="source-104">J. Cognitive Neurosci</source>. <volume>29</volume>, <fpage>203</fpage>–<lpage>219</lpage>.</citation></ref><ref id="c105" hwp:id="ref-105" hwp:rev-id="xref-ref-105-1 xref-ref-105-2 xref-ref-105-3 xref-ref-105-4 xref-ref-105-5 xref-ref-105-6 xref-ref-105-7 xref-ref-105-8 xref-ref-105-9 xref-ref-105-10"><label>105.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.105" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-105"><string-name name-style="western" hwp:sortable="Sidhu D.M."><surname>Sidhu</surname>, <given-names>D.M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Pexman P.M"><surname>Pexman</surname>, <given-names>P.M</given-names></string-name>. (<year>2018</year>). <article-title hwp:id="article-title-98">Five mechanisms of sound symbolic association</article-title>. <source hwp:id="source-105">Psychon. B. Rev</source>. <volume>25</volume>, <fpage>1619</fpage>–<lpage>1643</lpage>.</citation></ref><ref id="c106" hwp:id="ref-106" hwp:rev-id="xref-ref-106-1"><label>106.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.106" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-106"><string-name name-style="western" hwp:sortable="Sidhu D.M."><surname>Sidhu</surname>, <given-names>D.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Westbury C."><surname>Westbury</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hollis G."><surname>Hollis</surname>, <given-names>G.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Pexman P.M"><surname>Pexman</surname>, <given-names>P.M</given-names></string-name>. (<year>2021</year>). <article-title hwp:id="article-title-99">Sound symbolism shapes the English language: The maluma/takete effect in English nouns</article-title>. <source hwp:id="source-106">Psychon. B. Rev</source>. in press, doi: <pub-id pub-id-type="doi">10:3758/s13423-021-01883-3</pub-id></citation></ref><ref id="c107" hwp:id="ref-107"><label>107.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.107" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-107"><string-name name-style="western" hwp:sortable="Singh K.D."><surname>Singh</surname>, <given-names>K.D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Fawcett I.P"><surname>Fawcett</surname>, <given-names>I.P</given-names></string-name>. (<year>2008</year>). <article-title hwp:id="article-title-100">Transient and linearly graded deactivation of the human default-mode network by a visual detection task</article-title>. <source hwp:id="source-107">NeuroImage</source> <volume>41</volume>, <fpage>100</fpage>–<lpage>112</lpage>.</citation></ref><ref id="c108" hwp:id="ref-108" hwp:rev-id="xref-ref-108-1 xref-ref-108-2"><label>108.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.108" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-108"><string-name name-style="western" hwp:sortable="Smith L.B."><surname>Smith</surname>, <given-names>L.B.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sera M.D"><surname>Sera</surname> <given-names>M.D</given-names></string-name>. (<year>1992</year>). <article-title hwp:id="article-title-101">A developmental analysis of the polar structure of dimensions</article-title>. <source hwp:id="source-108">Cognitive Psychol</source>. <volume>24</volume>, <fpage>99</fpage>–<lpage>142</lpage>.</citation></ref><ref id="c109" hwp:id="ref-109"><label>109.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.109" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-109"><string-name name-style="western" hwp:sortable="Sokolowksi H.M."><surname>Sokolowksi</surname>, <given-names>H.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fias W."><surname>Fias</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ononye C.B."><surname>Ononye</surname>, <given-names>C.B.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ansari D"><surname>Ansari</surname>, <given-names>D</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-102">Are numbers grounded in a general magnitude processing system? A functional neuroimaging meta-analysis</article-title>. <source hwp:id="source-109">Neuropsychologia</source> <volume>105</volume>, <fpage>50</fpage>–<lpage>69</lpage>.</citation></ref><ref id="c110" hwp:id="ref-110" hwp:rev-id="xref-ref-110-1 xref-ref-110-2 xref-ref-110-3 xref-ref-110-4 xref-ref-110-5 xref-ref-110-6 xref-ref-110-7 xref-ref-110-8 xref-ref-110-9 xref-ref-110-10 xref-ref-110-11 xref-ref-110-12 xref-ref-110-13"><label>110.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.110" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-110"><string-name name-style="western" hwp:sortable="Spence C"><surname>Spence</surname>, <given-names>C</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-103">Crossmodal correspondences: A tutorial review</article-title>. <source hwp:id="source-110">Atten. Percept. Psycho</source>. <volume>73</volume>, <fpage>971</fpage>–<lpage>95</lpage>.</citation></ref><ref id="c111" hwp:id="ref-111" hwp:rev-id="xref-ref-111-1 xref-ref-111-2 xref-ref-111-3"><label>111.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.111" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-111"><string-name name-style="western" hwp:sortable="Spence C"><surname>Spence</surname>, <given-names>C</given-names></string-name>. (<year>2013</year>). <article-title hwp:id="article-title-104">Just how important is spatial coincidence to multisensory integration?</article-title> <source hwp:id="source-111">Evaluating the spatial rule. Ann. N.Y. Acad. Sci</source>. <volume>1296</volume>, <fpage>31</fpage>–<lpage>49</lpage>.</citation></ref><ref id="c112" hwp:id="ref-112" hwp:rev-id="xref-ref-112-1"><label>112.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.112" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-112"><string-name name-style="western" hwp:sortable="Stekelenburg J.J."><surname>Stekelenburg</surname>, <given-names>J.J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Vroomen J"><surname>Vroomen</surname>, <given-names>J</given-names></string-name>. (<year>2007</year>). <article-title hwp:id="article-title-105">Neural correlates of multisensory integration of ecologically valid audiovisual events</article-title>. <source hwp:id="source-112">J. Cognitive Neurosci</source>. <volume>19</volume>, <fpage>1964</fpage>–<lpage>1973</lpage>.</citation></ref><ref id="c113" hwp:id="ref-113" hwp:rev-id="xref-ref-113-1 xref-ref-113-2 xref-ref-113-3 xref-ref-113-4 xref-ref-113-5 xref-ref-113-6 xref-ref-113-7 xref-ref-113-8"><label>113.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.113" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-113"><string-name name-style="western" hwp:sortable="Stevenson R.A."><surname>Stevenson</surname>, <given-names>R.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Altieri N.A."><surname>Altieri</surname>, <given-names>N.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kim S."><surname>Kim</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pisoni D.B."><surname>Pisoni</surname>, <given-names>D.B.</given-names></string-name> and <string-name name-style="western" hwp:sortable="James T.W"><surname>James</surname>, <given-names>T.W</given-names></string-name>. (<year>2010</year>). <article-title hwp:id="article-title-106">Neural processing of asynchronous audiovisual speech perception</article-title>. <source hwp:id="source-113">NeuroImage</source> <volume>49</volume>, <fpage>3308</fpage>–<lpage>3318</lpage>.</citation></ref><ref id="c114" hwp:id="ref-114" hwp:rev-id="xref-ref-114-1"><label>114.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.114" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-114"><string-name name-style="western" hwp:sortable="Styles S.J."><surname>Styles</surname>, <given-names>S.J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gawne L"><surname>Gawne</surname>, <given-names>L</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-107">When does maluma/takete fail? Two key failures and a meta-analysis suggest that phonology and phonotactics matter</article-title>. <source hwp:id="source-114">i-Perception</source>, doi:<pub-id pub-id-type="doi">10.1177/2041669517724807</pub-id></citation></ref><ref id="c115" hwp:id="ref-115" hwp:rev-id="xref-ref-115-1 xref-ref-115-2 xref-ref-115-3 xref-ref-115-4 xref-ref-115-5"><label>115.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.115" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-115"><string-name name-style="western" hwp:sortable="Sučević J."><surname>Sučević</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Savić A.M."><surname>Savić</surname>, <given-names>A.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Popović M.B."><surname>Popović</surname>, <given-names>M.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Styles S.J."><surname>Styles</surname>, <given-names>S.J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ković V"><surname>Ković</surname>, <given-names>V</given-names></string-name>. (<year>2015</year>). <article-title hwp:id="article-title-108">Balloons and bavoons versus spikes and shikes: ERPs reveal shared neural processes for shape-sound-meaning congruence in words, and shape-sound congruence in pseudowords</article-title>. <source hwp:id="source-115">Brain Lang</source>. <volume>145/146</volume>, <fpage>11</fpage>–<lpage>22</lpage>.</citation></ref><ref id="c116" hwp:id="ref-116"><label>116.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.116" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-116"><string-name name-style="western" hwp:sortable="Svantesson J.-O"><surname>Svantesson</surname>, <given-names>J.-O</given-names></string-name>. (<year>2018</year>). <article-title hwp:id="article-title-109">Sound symbolism: the role of word sound in meaning</article-title>. <source hwp:id="source-116">WIREs Cognitive Sci</source>. <volume>8</volume>, <fpage>e1441</fpage>, doi:<pub-id pub-id-type="doi">10.1002/wcs.1441</pub-id></citation></ref><ref id="c117" hwp:id="ref-117" hwp:rev-id="xref-ref-117-1"><label>117.</label><citation publication-type="book" citation-type="book" ref:id="478347v3.117" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-117"><string-name name-style="western" hwp:sortable="Talairach J."><surname>Talairach</surname>, <given-names>J.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tournoux P"><surname>Tournoux</surname>, <given-names>P</given-names></string-name>. (<year>1988</year>). <source hwp:id="source-117">Co-planar Stereotaxic Atlas of the Human Brain</source>. <publisher-name>Thieme Medical Publishers</publisher-name>; <publisher-loc>New York</publisher-loc>.</citation></ref><ref id="c118" hwp:id="ref-118" hwp:rev-id="xref-ref-118-1"><label>118.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.118" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-118"><string-name name-style="western" hwp:sortable="Thompson P.D."><surname>Thompson</surname>, <given-names>P.D.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Estes Z"><surname>Estes</surname>, <given-names>Z</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-110">Sound symbolic naming of novel objects is a graded function</article-title>. <source hwp:id="source-118">Q. J. Exp. Psychol</source>. <volume>64</volume>, <fpage>2392</fpage>–<lpage>2404</lpage>.</citation></ref><ref id="c119" hwp:id="ref-119"><label>119.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.119" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-119"><string-name name-style="western" hwp:sortable="Tzeng C."><surname>Tzeng</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nygaard L.C."><surname>Nygaard</surname>, <given-names>L.C.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Namy L.L"><surname>Namy</surname>, <given-names>L.L</given-names></string-name>. (<year>2017</year>). <article-title hwp:id="article-title-111">The specificity of sound symbolic correspondences in spoken language</article-title>. <source hwp:id="source-119">Cognitive Sci</source>. <volume>41</volume>, <fpage>2191</fpage>–<lpage>2220</lpage>.</citation></ref><ref id="c120" hwp:id="ref-120" hwp:rev-id="xref-ref-120-1 xref-ref-120-2 xref-ref-120-3 xref-ref-120-4 xref-ref-120-5 xref-ref-120-6 xref-ref-120-7"><label>120.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.120" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-120"><string-name name-style="western" hwp:sortable="van Atteveldt N.M."><surname>van Atteveldt</surname>, <given-names>N.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Formisano E."><surname>Formisano</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blomert L."><surname>Blomert</surname>, <given-names>L.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Goebel R."><surname>Goebel</surname>, <given-names>R.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-112">The effect of temporal asynchrony on the multisensory integration of letters and speech sounds. <italic toggle="yes">Cerebr</italic></article-title>. <source hwp:id="source-120">Cortex</source> <volume>17</volume>, <fpage>962</fpage>–<lpage>974</lpage>.</citation></ref><ref id="c121" hwp:id="ref-121" hwp:rev-id="xref-ref-121-1 xref-ref-121-2 xref-ref-121-3 xref-ref-121-4"><label>121.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.121" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-121"><string-name name-style="western" hwp:sortable="Walsh V"><surname>Walsh</surname>, <given-names>V</given-names></string-name>. (<year>2003</year>). <article-title hwp:id="article-title-113">A theory of magnitude: common cortical metrics of time, space and quantity</article-title>. <source hwp:id="source-121">Trends Cogn Sci</source>. <volume>7</volume>, <fpage>483</fpage>–<lpage>488</lpage>.</citation></ref><ref id="c122" hwp:id="ref-122" hwp:rev-id="xref-ref-122-1 xref-ref-122-2 xref-ref-122-3"><label>122.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.122" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-122"><string-name name-style="western" hwp:sortable="Werner S."><surname>Werner</surname>, <given-names>S.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Noppeney U"><surname>Noppeney</surname>, <given-names>U</given-names></string-name>. (<year>2010</year>). <article-title hwp:id="article-title-114">Superadditive responses in superior temporal sulcus predict audiovisual benefits in object categorization. <italic toggle="yes">Cerebr</italic></article-title>. <source hwp:id="source-122">Cortex</source> <volume>20</volume>, <fpage>1829</fpage>–<lpage>1842</lpage>.</citation></ref><ref id="c123" hwp:id="ref-123" hwp:rev-id="xref-ref-123-1"><label>123.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.123" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-123"><string-name name-style="western" hwp:sortable="White T."><surname>White</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Leary D."><surname>O’Leary</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Magnotta V."><surname>Magnotta</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arndt S."><surname>Arndt</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Flaum M."><surname>Flaum</surname>, <given-names>M.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Andreasen N.C"><surname>Andreasen</surname>, <given-names>N.C</given-names></string-name>. (<year>2001</year>). <article-title hwp:id="article-title-115">Anatomic and functional variability: The effects of filter size in group fMRI data analysis</article-title>. <source hwp:id="source-123">NeuroImage</source> <volume>13</volume>, <fpage>577</fpage>–<lpage>588</lpage>.</citation></ref><ref id="c124" hwp:id="ref-124" hwp:rev-id="xref-ref-124-1 xref-ref-124-2"><label>124.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.124" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-124"><string-name name-style="western" hwp:sortable="Wilson L.B."><surname>Wilson</surname>, <given-names>L.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tregellas J.R."><surname>Tregellas</surname>, <given-names>J.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Slason E."><surname>Slason</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pasko B.E."><surname>Pasko</surname>, <given-names>B.E.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rojas D.C"><surname>Rojas</surname>, <given-names>D.C</given-names></string-name>. (<year>2011</year>). <article-title hwp:id="article-title-116">Implicit phonological priming during visual word recognition</article-title>. <source hwp:id="source-124">NeuroImage</source> <volume>55</volume>, <fpage>724</fpage>–<lpage>731</lpage>.</citation></ref><ref id="c125" hwp:id="ref-125" hwp:rev-id="xref-ref-125-1 xref-ref-125-2 xref-ref-125-3"><label>125.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.125" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-125"><string-name name-style="western" hwp:sortable="Woo C.-W."><surname>Woo</surname>, <given-names>C.-W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krishnan A."><surname>Krishnan</surname>, <given-names>A.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wager T.D"><surname>Wager</surname>, <given-names>T.D</given-names></string-name>. (<year>2014</year>). <article-title hwp:id="article-title-117">Cluster-extent based thresholding in fMRI analyses: pitfalls and recommendations</article-title>. <source hwp:id="source-125">NeuroImage</source> <volume>91</volume>, <fpage>412</fpage>–<lpage>419</lpage>.</citation></ref><ref id="c126" hwp:id="ref-126" hwp:rev-id="xref-ref-126-1"><label>126.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.126" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-126"><string-name name-style="western" hwp:sortable="Worthington A"><surname>Worthington</surname>, <given-names>A</given-names></string-name>. (<year>2016</year>). <article-title hwp:id="article-title-118">Treatments and technologies in the rehabilitation of apraxia and action disorganization syndrome: a review</article-title>. <source hwp:id="source-126">Neurorehabilitation</source> <volume>39</volume>, <fpage>163</fpage>–<lpage>174</lpage>.</citation></ref><ref id="c127" hwp:id="ref-127" hwp:rev-id="xref-ref-127-1"><label>127.</label><citation publication-type="journal" citation-type="journal" ref:id="478347v3.127" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-127"><string-name name-style="western" hwp:sortable="Zimmer U."><surname>Zimmer</surname>, <given-names>U.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Roberts K. C."><surname>Roberts</surname>, <given-names>K. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harshbarger T. B."><surname>Harshbarger</surname>, <given-names>T. B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Woldorff M. G"><surname>Woldorff</surname>, <given-names>M. G</given-names></string-name>. (<year>2010</year>). <article-title hwp:id="article-title-119">Multisensory conflict modulates the spread of visual attention across a multisensory object</article-title>. <source hwp:id="source-127">NeuroImage</source> <volume>52</volume>, <fpage>606</fpage>–<lpage>616</lpage>.</citation></ref></ref-list></back></article>
