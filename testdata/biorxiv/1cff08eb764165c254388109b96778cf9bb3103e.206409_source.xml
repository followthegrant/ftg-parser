<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/206409</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;206409</article-id><article-id pub-id-type="other" hwp:sub-type="slug">206409</article-id><article-id pub-id-type="other" hwp:sub-type="tag">206409</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Simulation of visual perception and learning with a retinal prosthesis</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1">Correspondence: E.J. Chichilnisky, <email hwp:id="email-1">ej@stanford.edu</email>.</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Golden James R."><surname>Golden</surname><given-names>James R.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Erickson-Davis Cordelia"><surname>Erickson-Davis</surname><given-names>Cordelia</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-1" hwp:rel-id="aff-6">6</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Cottaris Nicolas P."><surname>Cottaris</surname><given-names>Nicolas P.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Parthasarathy Nikhil"><surname>Parthasarathy</surname><given-names>Nikhil</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a7" hwp:id="xref-aff-7-1" hwp:rel-id="aff-7">7</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Rieke Fred"><surname>Rieke</surname><given-names>Fred</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Brainard David H."><surname>Brainard</surname><given-names>David H.</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2974-1836</contrib-id><name name-style="western" hwp:sortable="Wandell Brian A."><surname>Wandell</surname><given-names>Brian A.</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-2974-1836"/></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-8"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5613-0248</contrib-id><name name-style="western" hwp:sortable="Chichilnisky E. J."><surname>Chichilnisky</surname><given-names>E. J.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-5613-0248"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">Neurosurgery, Ophthalmology &amp; Hansen Experimental Physics Laboratory, Stanford University</institution>, Stanford, CA 94305 <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">School of Medicine, Stanford University, Stanford</institution>, CA 94305 <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2"><label>3</label><institution hwp:id="institution-3">Psychology, University of Pennsylvania</institution>, Philadelphia, PA 19104 <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Physiology &amp; Biophysics, University of Washington</institution>, Seattle, WA 98105 <country>USA</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Center for Image Systems Engineering, Stanford University</institution>, Stanford, CA 94305 <country>USA</country></aff><aff id="a6" hwp:id="aff-6" hwp:rev-id="xref-aff-6-1"><label>6</label><institution hwp:id="institution-6">Department of Anthropology, Stanford University</institution>, Stanford, CA 94305 <country>USA</country></aff><aff id="a7" hwp:id="aff-7" hwp:rev-id="xref-aff-7-1"><label>7</label><institution hwp:id="institution-7">Institute for Computational &amp; Mathematical Engineering, Stanford University</institution>, Stanford, CA 94305 <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2017"><year>2017</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2017-10-20T09:45:26-07:00">
    <day>20</day><month>10</month><year>2017</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2017-10-20T09:45:26-07:00">
    <day>20</day><month>10</month><year>2017</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2017-10-20T09:51:02-07:00">
    <day>20</day><month>10</month><year>2017</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2017-10-20T09:51:02-07:00">
    <day>20</day><month>10</month><year>2017</year>
  </pub-date><elocation-id>206409</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2017-10-19"><day>19</day><month>10</month><year>2017</year></date>
<date date-type="rev-recd" hwp:start="2017-10-19"><day>19</day><month>10</month><year>2017</year></date>
<date date-type="accepted" hwp:start="2017-10-20"><day>20</day><month>10</month><year>2017</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2017</copyright-year><license hwp:id="license-1"><p hwp:id="p-1">The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.</p></license></permissions><self-uri xlink:href="206409.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/206409v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="206409.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/206409v1/206409v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/206409v1/206409v1.htslp"/><abstract hwp:id="abstract-1"><p hwp:id="p-2">The nature of artificial vision with a retinal prosthesis, and the degree to which the brain can adapt to the unnatural input from such a device, are poorly understood. Therefore, the development of current and future devices may be aided by theory and simulations that help to infer and understand what patients see. A novel computational framework was developed to predict visual perception and the effect of learning with a subretinal prosthesis. The framework is based on the idea that the central visual system efficiently reconstructs the incident image from the retinal output. To implement this idea, a simulation of the normal responses of the major retinal ganglion cell types was used to deduce the optimal linear reconstruction of the visual stimulus from retinal activity. The result was then used to make inferences about visual experience with simulated retinal activation by a subretinal prosthesis. The inferred visual perception obtained with prosthesis activation was substantially degraded compared to the inferred perception obtained with normal retinal responses, as expected given the limited resolution and lack of cell type specificity of the prosthesis. Consistent with the importance of cell type specificity, reconstruction using only ON cells, and not OFF cells, was substantially more accurate. Finally, when reconstruction was re-optimized for electrical stimulation, simulating learning by the patient, the accuracy of inferred perception with prosthesis stimulation was closer to that of natural vision. The reconstruction approach provides a framework for interpreting patient data in clinical trials, and may be useful for improving prosthesis design.</p><sec hwp:id="sec-1"><title hwp:id="title-1">Funding</title><p hwp:id="p-3">Simons Foundation Collaboration on the Global Brain (EJC,BW,DB,FR), Stanford Neurosciences Institute (EJC,SM), DARPA Contract FA8650-16-1-7657 (EJC).</p></sec><sec hwp:id="sec-2"><title hwp:id="title-2">Acknowledgements</title><p hwp:id="p-4">We thank Vincent Bismuth and Daniel Palanker for valuable discussions and comments on the manuscript, Nishal Shah for technical assistance and useful input, and Georges Goetz for helpful discussions.</p></sec></abstract><counts><page-count count="23"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-3"><title hwp:id="title-3">Introduction</title><p hwp:id="p-5">Retinal prostheses can restore visual perception to blind patients through electrical stimulation of surviving neurons in the retina, based on images captured by a camera (<xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Humayun et al. 2012</xref>; <xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Zrenner et al. 2011</xref>). The perceptual experience of patients with existing devices, however, is far from that of a person with normal vision. Several efforts are underway to build next generation devices (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Mathieson et al. 2012</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Lorach et al. 2015a</xref>), and clinical experience with other types of neural implants suggests that learning by the patient may help to overcome some of the technical limitations of current devices (<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Hallberg and Ringdahl 2004</xref>; <xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Shannon 2012</xref>; <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Eisen 2003</xref>; <xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Waltzman et al. 1993</xref>). Nonetheless, the degree to which technical improvements and learning can improve patient outcomes remains poorly understood, and current understanding of the experience of artificial vision itself is limited. Thus, it would be valuable to have a framework to predict the visual experience that retinal prostheses will provide to patients, as well as how that visual experience may improve with learning. Such a framework could help researchers and clinicians to interpret what patients see, and could guide future technical development based on understanding the factors that limit performance.</p><p hwp:id="p-6">The goal of this work is to provide such a framework. The challenge is to develop a simulation of prosthesis function that goes beyond empirical observations and features of the device to produce quantitative predictions of visual experience, grounded in our understanding of the human visual system. To approach this problem, a key conceptual step is proposed: the assumption that perceptual experience is an attempt by the visual system to reconstruct the incident visual image from the retinal signal. With this simple but powerful assumption, a simulation of visual perception and learning with a prosthesis can be accomplished as follows.</p><p hwp:id="p-7">First, the optimal linear reconstruction of the image from retinal activity, deduced from simulations of retinal responses to natural scenes, is computed to produce a prediction of perception for any given evoked pattern of retinal activity (<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Nirenberg and Pandarinath 2012</xref>). Linear reconstruction can be understood as placing a particular spatio-temporal filter in the reconstructed image for each spike produced by a given retinal neuron, and summing the contributions of all neurons (<xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Warland et al. 1997</xref>, Rieke 1997). A unique, optimized reconstruction filter is used for each neuron, reflecting the information that neuron conveys about the image in normal vision. This approach, coupled with a simulation of how the device activates retinal neurons, produces a specific prediction of the perceived image with a retinal prosthesis. Second, the potential impact of a patient learning on artificial vision is then inferred by recomputing the filters for each neuron on the basis of prosthesis stimulation, rather than on the basis of normal visual responses. Thus, the framework permits the evaluation of prosthetic vision compared to normal vision, as well as the effect of learning.</p><p hwp:id="p-8">Here we develop this approach using simulations of healthy retinal function (<underline>ISETBio</underline>; <xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Brainard et al. 2015</xref>; <xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Jiang et al. 2017</xref>) and simulations of retinal activity evoked by a photovoltaic subretinal prosthesis (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Mathieson et al. 2012</xref>; <xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Wang et al. 2012</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Lorach et al. 2015a</xref>) on the basis of data on isolated retina (<xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Goetz et al. 2015</xref>; <xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Mandel et al. 2013</xref>; <xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Boinagrov et al. 2014</xref>; <xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Lorach et al. 2015b</xref>). The approach yields simple quantitative predictions of two aspects of prosthetic vision. First, the predicted image obtained with a retinal prosthesis is degraded compared to the image obtained in normal vision over the same field of view. This finding is consistent with the limited resolution and cell type specificity of prostheses. Second, re-optimizing filters for prosthesis stimulation substantially improves the reconstructed image. This indicates that the potential for learning by patients to improve prosthesis function is high. We comment on potential improvements in the approach, on the use of this framework for design of future prosthesis technology, and on the potential for image processing to optimize the function of existing devices.</p></sec><sec id="s2" hwp:id="sec-4"><title hwp:id="title-4">Methods</title><sec id="s2a" hwp:id="sec-5"><title hwp:id="title-5">Overview</title><p hwp:id="p-9">A computational framework was developed to infer a visual percept based on RGC activity, both for healthy light activation, and for electrical activation using a subretinal prosthesis. The <underline>ISETBio</underline> (Image Systems Engineering Toolbox – Biology) software was used to model transformation of a visual stimulus into RGC spikes in the healthy retina (<xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1</xref>). Additions to software were used to simulate electrical activation of bipolar cells via a retinal prosthesis, in turn producing RGC spikes. A linear transformation of simulated RGC responses was then used to infer the perceived image, over samples of a large image data set. The linear transformation was selected to optimize reconstruction for either the healthy or prosthesis condition.</p></sec><sec id="s2b" hwp:id="sec-6"><title hwp:id="title-6">Healthy Retina Simulation</title><p hwp:id="p-10">The ISETBio simulation begins with a representation of image radiance. This is transformed into retinal irradiance via the simulated cornea, lens, and inert pigments. Irradiance is then transformed into a spatial array of photon absorptions and photocurrent for each cone in the photoreceptor mosaic. Cone photocurrents are then linearly transformed into bipolar responses, with appropriate cone type connectivity. Finally, bipolar responses are transformed into RGC responses via a generalized linear model that produces simulated spikes. This package simulates many aspects of healthy retinal responses to arbitrary visual stimuli, although it does not yet explicitly include the role of horizontal and amacrine cells or diverse nonlinear aspects of retinal processing. Below, the full spatio-temporal model is described. Later, the more limited use of the model in simulating static image presentations, RGC responses, and reconstruction is presented.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-11">The ISETBio computational pipeline. Stimuli represented by spectral radiance over space were transformed to retinal irradiance, cone photopigment isomerizations, photocurrent, bipolar responses, and finally RGC spikes.</p></caption><graphic xlink:href="206409_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-12">The stimulus image was modeled as an array of MxN pixels on a RGB computer display, and summarized by an MxNx3 matrix. The spectral power density of the display primaries was used to calculate a larger array (MxNxW) representing the display radiance, where each value of W indexes the radiance at a particular wavelength. The full spectral image was then passed through a model of the human optics based on mean wavefront aberrations measured on-axis (near fovea) as well as lens and macular pigment densities. The retinal spectral irradiances were used to compute the Poisson distributed photopigment isomerization rates for the L, M and S cones in the simulated cone mosaic. Cone isomerizations were computed using photopigment parameters for human foveal vision, which reproduce the CIE cone fundamentals (<xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">CIE 2006</xref>). A linear temporal filter was applied to the cone isomerizations to generate simulated cone photocurrents over time (<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Angueyra and Rieke 2013</xref>).</p><p hwp:id="p-13">Four types of bipolar cells were modeled, ON diffuse, OFF diffuse, ON midget, and OFF midget. Each type pooled inputs linearly over a local collection of cones. The bipolar temporal response was assumed to be equal to that of the RGC type to which it provided input (see below). RGC temporal responses were obtained from a standard data set (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Pillow et al. 2008</xref>). Thus, the bipolar temporal response was obtained by convolving the cone response in time with a filter obtained by deconvolving the RGC temporal impulse response by the cone temporal impulse response, for the appropriate RGC type. The connectivity of the three cones types to the four types of bipolars simulated was modeled as follows (<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Dacey 2000</xref>): the first three types of bipolar cells received input from only L and M cones, the fourth received weighted input from S cones (25%) in addition to input from L and M cones (<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Field et al. 2010</xref>).</p><p hwp:id="p-14">The RGC response was simulated with a generalized linear model based on bipolar cell input (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">Pillow et al. 2008</xref>). The four bipolar types above provided sole input to ON parasol, OFF parasol, ON midget and OFF midget RGCs, respectively. The input to the RGC was computed as a local sum of bipolar cell responses, chosen so that the stimulus-referred spatial properties of the RGC fitted a published data set (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-3" hwp:rel-id="ref-37">Pillow et al. 2008</xref>). Because the bipolar temporal response was already chosen to equal that of the RGC, the transformation from bipolar to RGC did not involve further temporal filtering. The input to the RGC from bipolars was then followed by exponentiation and Poisson spiking with a post-spike filter, obtained from fits to experimental data (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-4" hwp:rel-id="ref-37">Pillow et al. 2008</xref>).</p><p hwp:id="p-15">The simulated patch of retina was a square region 1.7° (512 μm) on a side, centered at 1.8° eccentricity. The simulated patch had a uniform density of cones, bipolars, and RGCs appropriate for 1.8° eccentricity, and did not account for the changing density of these cells over the field of view. The cone mosaic over the region was a 256x256 square lattice with a pitch of 2 μm. The mosaics of each bipolar cell type had the same dimensions as the cone lattice. Each bipolar cell integrated over cone inputs using a difference of Gaussians spatial weighting function with center peak amplitude of 1, surround peak amplitude of 0.01, center SD of 4 μm, 4 μm, 2 μm and 2 μm for the four types of bipolar cells respectively, and surround SD equal to 9 times the center SD (<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Dacey 2000</xref>). The RGCs in the simulation consisted of approximately hexagonal lattices of 28x32, 31x35, 55x63, and 61x70 cells for the four RGC types, respectively, yielding a total of 9,716 RGCs. The four RGC types received inputs from nearby bipolars, weighted according to a difference of Gaussians function with center peak amplitude of 1, surround peak amplitude of 0.375, 0.375, 0.5 and 0.5 for the four RGC types, center SD of 10 μm, 8 μm, 4 μm, and 3.5 μm, and surround SD equal to 1.15 times the center SD. These parameters were selected to approximately match experimental data (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-5" hwp:rel-id="ref-37">Pillow et al. 2008</xref>; <xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Croner and Kaplan 1995</xref>). Poisson noise in photoisomerization and Poisson spiking by RGCs were the only sources of noise in the simulation.</p></sec><sec id="s2c" hwp:id="sec-7"><title hwp:id="title-7">Subretinal Prosthesis Simulation</title><p hwp:id="p-16">To model the encoding of the visual signal by a subretinal prosthesis, a simulation of the electrical stimulation of bipolar cells was introduced into the ISETBio pipeline. The simulation was modeled after a subretinal prosthesis, in which a visual stimulus captured by a camera mounted on goggles is transformed into pulses of infrared light that irradiate a grid of photovoltaic pixels implanted in the sub-retinal space near the bipolar cells (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-3" hwp:rel-id="ref-33">Mathieson et al. 2012</xref>). In the simulation, an 8x8 rectangular array of photodiodes with 70 μm pitch covered the field of view. The voltage on each photodiode at each time step was assumed to be proportional to the irradiance of the visual stimulus integrated over the photodiode area during that time step. The electrodes produced responses in nearby bipolar cells in proportion to this voltage. The spatial profile of bipolar activation was the profile of the grid of electrode center points, each scaled according to stimulation voltage, convolved with a Gaussian function with SD=35 μm. Each bipolar cell was activated in proportion to the value of this activation profile at its location. The constant of proportionality was set to approximately mimic the distribution of RGC responses that occurs with natural scene stimulation (data not shown). With this constant, the stimulus contrast required for reliable detection of a full-field step of light (see below) was 3.1%. The temporal response of the bipolar was given by the activation value convolved with the bipolar temporal impulse response. This implementation did not capture nonlinear effects such as saturation of the photovoltaic pixels at high input irradiances (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-4" hwp:rel-id="ref-33">Mathieson et al. 2012</xref>).</p></sec><sec id="s2d" hwp:id="sec-8"><title hwp:id="title-8">Static Simulation</title><p hwp:id="p-17">Although the calculations above yield simulations of RGC activity over time, most of the analysis was performed for static images and static RGC responses, for simplicity. In both the healthy retina and prosthesis simulation, images were presented for 0.4 s, and the responses of RGCs were accumulated over the time window 0.320-0.328 s after the stimulus presentation. This produced a vector of spike counts across the RGC population, in response to each static image.</p></sec><sec id="s2e" hwp:id="sec-9"><title hwp:id="title-9">Optimal Linear Reconstruction</title><p hwp:id="p-18">Linear reconstruction was used to infer the image perceived by a patient based on RGC activation. To perform linear reconstruction, a spatial filter was first identified for each RGC. Each spike emitted by that RGC during the response caused a copy of the spatial filter to be placed in the reconstructed image. The final reconstructed image was the sum of the filters contributed by all spikes from all RGCs. The spatial filters for the collection of RGCs were selected so that, when applied to RGC responses in either healthy or prosthesis simulation, produced reconstructed images with least squared deviation of pixel intensity from the original stimulus (see below). The identification of optimal filters was performed on 80% of the images in the data set. The performance of reconstruction was then assessed on the remaining 20% of images. In total, 288,000 image patches were used, each 100x100 pixels, sampled from the Imagenet database (<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Deng et al. 2009</xref>).</p><p hwp:id="p-19">Reconstruction filters were calculated using linear regression (<xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Warland and Meister 1997</xref>). Let s denote the stimulus image as a row vector, let r denote a row vector indicating the number of spikes from each RGC with an offset term of 1 as its first entry, and let W denote the reconstruction matrix, in which each row contains the reconstruction filter for a particular RGC. The reconstruction s<sc>rec</sc> of the stimulus s is then given by:
<disp-formula id="ueqn1" hwp:id="disp-formula-1"><alternatives hwp:id="alternatives-1"><graphic xlink:href="206409_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives></disp-formula></p><p hwp:id="p-20">To obtain a matrix W that reconstructs as accurately as possible for a collection of images, let S be a matrix containing an image in each row, let R be the matrix containing the corresponding responses from all cells in each row, and let S<sub>est</sub> be a matrix containing the reconstructed images in each row. The objective is to minimize the squared difference between reconstructed and original images,
<disp-formula id="ueqn2" hwp:id="disp-formula-2"><alternatives hwp:id="alternatives-2"><graphic xlink:href="206409_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives></disp-formula></p><p hwp:id="p-21">Where ‖⋅‖ represents the sum of squared entries. This is a standard regression problem with the solution (<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Menke 2012</xref>):
<disp-formula id="ueqn3" hwp:id="disp-formula-3"><alternatives hwp:id="alternatives-3"><graphic xlink:href="206409_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives></disp-formula></p><p hwp:id="p-22">The performance of reconstruction using the matrix W was then assessed on a set of held out stimuli and responses. As expected, reconstruction performance was limited by the amount of data used to identify the entries of W. To obtain robust estimates of W, regularization was performed by limiting the rank of W (<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Chambers 1977</xref>). Specifically, if R is given by its singular value decomposition, <disp-formula id="ueqn4" hwp:id="disp-formula-4"><alternatives hwp:id="alternatives-4"><graphic xlink:href="206409_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives></disp-formula></p><p hwp:id="p-23">Then, from the expression for W above, we have: <disp-formula id="ueqn5" hwp:id="disp-formula-5"><alternatives hwp:id="alternatives-5"><graphic xlink:href="206409_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives></disp-formula></p><p hwp:id="p-24">Thus, an approximation to W with rank K is given by: <disp-formula id="ueqn6" hwp:id="disp-formula-6"><alternatives hwp:id="alternatives-6"><graphic xlink:href="206409_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives></disp-formula></p><p hwp:id="p-25">Here D<sc>K</sc> denotes zeroing all but the first K diagonal entries of D (singular values). For the healthy retina reconstruction, retaining the top 50% of singular values (K=4904) produced optimal performance. For the prosthesis reconstruction, retaining the top 5% of singular values (K=491) produced optimal performance. These values were used in all analyses above.</p></sec><sec id="s2f" hwp:id="sec-10"><title hwp:id="title-10">Linear Classification for Comparison to Clinical Tests</title><p hwp:id="p-26">In order to simulate measurements of perceptual thresholds, such as those performed on patients with previous retinal implants (<xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Stingl et al. 2013</xref>), a linear classification approach was used. Two sets of stimuli were processed by the prosthesis simulation, producing reconstructed images, and a linear classifier was trained to discriminate pairs of reconstructed images, one from each set.</p><p hwp:id="p-27">Classification performance was then assessed on held out data as a function of a particular stimulus variable, e.g. contrast or spatial frequency. Discrimination thresholds were defined as the value at which the classifier achieved 82% accuracy.</p></sec><sec id="s2g" hwp:id="sec-11"><title hwp:id="title-11">Results</title><p hwp:id="p-28">To quantify normal visual system function, as a benchmark for artificial vision with a prosthesis, a simulation of the normal (healthy) light activation of retinal ganglion cells (RGCs) by visual images was produced. This simulation was performed using the publicly-available <underline>ISETBio</underline> software (see Methods). Briefly, each RGC was modeled as a linear-nonlinear cascade, with center-surround receptive field organization (Kuffler 1953). Simple cascade models have been used to capture the responses of populations of RGCs in previous studies (<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Chichilnisky 2001</xref>, <xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Chichilnisky and Kalmar 2002</xref>, <xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Field and Chichilnisky 2007</xref>, <xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-6" hwp:rel-id="ref-37">Pillow et al. 2008</xref>). The simulation was built on modeled inputs from photoreceptors transmitted through bipolar cells in the retinal network. The simulation was focused on the four numerically dominant RGC types in macaque and human retinas: ON parasol, OFF parasol, ON midget, and OFF midget (<xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Dacey 1999</xref>). Each of the four cell types forms a mosaic uniformly covering the visual scene (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2</xref>, top). The spatial, temporal, nonlinearity, and noise properties of each RGC type (<xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2</xref>) were taken from experimental findings and incorporated into the software. Presentation of a static natural visual image (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3</xref>, left) to the simulated retina causes a spatial pattern of activation in the four RGC types (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3</xref>, second and third columns). Although the simulation produced RGC spikes over time, for simplicity most of what follows focuses on flashed static images reconstructed from RGC spike counts (see Methods).</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-29">Simulation of RGC responses. Each RGC is modeled as a linear-nonlinear cascade (see <xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">Chichilnisky 2001</xref>, <xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">Chichilnisky and Kalmar 2002</xref>), with an intermediate representation of photoreceptor and bipolar cells (see Methods). Top row: cross-sectional spatial profiles of the spatial receptive fields of each RGC type, with midget cell RF extents smaller than parasol, and OFF slightly smaller than ON. Second row: the temporal impulse response of each RGC type; a typical biphasic response, with different details in midget and parasol RGCs. Third row: the response nonlinearity of each RGC type is an accelerating half-wave rectifying function. Bottom row: the collection of RFs of each RGC type (blue circles) uniformly tile the region of retina, with different densities, and the implant electrodes (red circles) form a much coarser sampling of space.</p></caption><graphic xlink:href="206409_fig2" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-30">To simulate the retinal signal that would be produced by a prosthesis, a model was developed of the activation of the retinal network by the electrodes of a wireless subretinal photovoltaic prosthesis (<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Loudin et al. 2007</xref>). The prosthesis technology has been tested extensively in isolated retina as well as <italic toggle="yes">in vivo</italic> animal studies (<xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">Goetz et al. 2015</xref>; <xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">Mandel et al. 2013</xref>; <xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Boinagrov et al. 2014</xref>; <xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Lorach et al. 2015b</xref>). In the simulation, the presentation of a static visual image to the prosthesis produced spatial patterns of current from the electrodes. This current then caused depolarization of the various bipolar cell types that provide input to the ON and OFF parasol and midget RGCs, resulting in spiking activity in the corresponding RGC populations (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4</xref>, second and third columns). The pattern of activation induced by the prosthesis was quite different from the activation of RGCs in the healthy retina simulation (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3</xref>, second and third columns). A salient difference is that the normal opposite activation of ON and OFF populations by light and dark portions of the visual scene was not reproduced, because light-evoked current in the prosthesis activates ON and OFF bipolar cells indiscriminately and with the same polarity. This lack of cell type specificity is a known limitation of existing prosthesis technology (<xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-3" hwp:rel-id="ref-20">Goetz et al. 2015</xref>; <xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Goetz and Palanker 2016</xref>; <xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Jepson et al. 2013</xref>, <xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">2014</xref>).</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-31">Simulation of visual stimulus, response, and linear reconstruction in healthy retina. Left: a sample visual stimulus. Second and third columns: simulated responses of the four major RGC types over space to the flashed stimulus, with light colors indicating higher activity. Fourth column: linear reconstruction of the visual input by ON cells and OFF cells, using the responses shown in the second and third columns, and reconstruction filters trained on a large collection of responses to natural images. Right: linear reconstruction of the original stimulus from all four RGC types combined.</p></caption><graphic xlink:href="206409_fig3" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><p hwp:id="p-32">To assess how the different patterns of RGC activity with normal and prosthesis stimulation may influence visual perception, a simulation of the perceived image based on activation of RGCs was produced. The key assumption of the simulation is that the visual system reconstructs the visual image from the normal pattern of retinal activity in a way that is optimal for healthy vision, producing a reconstructed image that resembles the real visual scene as closely as possible. Specifically, in the simulation, patterns of RGC activation are linearly transformed into simulated “perceived” images that deviate as little as possible from the original images in terms of mean squared error, with the mean taken across an ensemble of natural images (<xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">Warland 1997</xref>, Rieke 1997, <xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">Nirenberg and Pandarinath 2012</xref>). These key technical assumptions (linear reconstruction, squared error, natural images) are considered further in Discussion. Based on these assumptions, image reconstruction is a regression problem, in which an optimal linear transformation is calculated that transforms RGC activation to the incident image, over 230,400 natural images (see Methods), and the transformation is then evaluated on 57,600 held-out images. Conceptually, this linear transform amounts to assigning a fixed spatial filter to each RGC (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 5</xref>, left), such that each spike from the RGC contributes that filter to the reconstructed image, and contributions from all RGCs are summed. The filter for each cell can be understood as an interpretation of the visual information conveyed by a spike in that cell.</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5 xref-fig-4-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-33">Simulation of stimulus, response, and linear reconstruction with retinal prosthesis. Left: visual stimulus. Second and third columns: simulated responses of four major RGC types to stimulation with the prosthesis. Note the similar patterns of activation of ON and OFF cells by the prosthesis. Fourth column: linear reconstruction of the stimulus, using the reconstruction filters obtained from the healthy retina simulation, separately using ON and OFF cells. Note the reverse polarity of the reconstruction for OFF cells. Right, top: reconstruction with all four RGC types combined normally, causing cancellation of ON and OFF signals. Right, bottom: reconstruction with all four RGC types combined, using reconstruction filters obtained with prosthesis stimulation, mimicking optimal learning by the patient.</p></caption><graphic xlink:href="206409_fig4" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-34">In the case of natural activation of RGCs, linear reconstruction produces an image that resembles the original image (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3</xref>, right), with a resolution that is limited by the density of RGCs. Because the reconstruction is linear, the contributions of the different RGC types can be examined separately. For example, ON midget and ON parasol cells provide similarly detailed contributions the reconstruction as OFF midget and OFF parasol cells (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3</xref>, fourth column).</p><p hwp:id="p-35">In the case of prosthetic activation of RGCs, the naive reconstruction procedure that was optimized for normal vision produces a degraded image, in which few features of the original image are easily recognized (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4</xref>, top right). This degradation is not surprising given that the ON and OFF bipolar cells, and correspondingly ON and OFF RGCs, are activated similarly by the prosthesis model (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4</xref>, second and third columns), while spikes from ON and OFF RGCs are treated oppositely in normal reconstruction (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Fig. 6</xref>, left) – thus, their contributions would often be expected to cancel. This interpretation was confirmed by examination of the ON and OFF reconstructions separately: the two groups of cells each produced more accurate reconstructions (ignoring the sign of the reconstructed contrast) individually (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4</xref>, fourth ′column) than when combined. Averaged over test images, the root mean square (RMS) error for reconstruction with all cells was 0.09 in the normal retina simulation, and 0.17 for the prosthesis simulation. As a comparison, the RMS error obtained with temporally shuffled spikes bearing no relationship to the visual stimulus was 0.24. Thus, a computational framework that assumes ON and OFF RGC signals are equally stimulated and used to reconstruct the visual scene, in a way that is optimal for normal vision, predicts that the perceived visual image with a prosthesis is highly degraded. Under these conditions, factors that might otherwise be expected to improve prosthesis function, namely the density of electrodes and magnitude of current spread, had little impact. Specifically, reducing electrode spacing and current spread by a factor of two produced a decrease in RMS error of only 7% (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig. 5</xref>, left).</p><p hwp:id="p-36">The naive prediction of severely degraded artificial vision above appears inconsistent with patient reports, which indicate that some degree of spatial vision is in fact possible with a prosthesis (see <xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Goetz and Palanker 2016</xref>). One possibility, suggested by the dominance of “bright” (rather than “dark”) phosphenes reported by patients (<xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">Stingl et al. 2013</xref>), as well as by models showing asymmetries in electrical activation of ON vs. OFF cells (<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Werginz et al. 2015</xref>; <xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Cottaris and Elfar 2005</xref>), is that the contribution of ON cells to artificial vision is stronger than that of OFF cells, reducing the cancellation of visual signals. To simulate this hypothesis in an extreme form, reconstruction using only ON midget and parasol cells was examined (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Fig. 4</xref>, top row, fourth column) with no contribution from OFF midget and parasol cells. In this simulation, the average RMS reconstruction error across images was 0.14, a value higher than in the healthy condition, but lower than the error obtained with prosthesis stimulation and reconstruction with all four RGC types. In this simulation, the additional improvement in performance with higher-resolution stimulation was still 7% (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig. 5</xref>, middle). In sum, the artificial vision provided by retinal prostheses in the clinic could benefit substantially from a dominance of specific cell types in artificial vision.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><p hwp:id="p-37">Reconstruction performance with denser electrodes and lower current spread. Lower panels shows the root mean square error (RMSE) for reconstruction of 57,600 tested images, using the original electrode spacing and current spread (70 μm, 35 μm) vs. values obtained with both reduced two-fold (35 μm, 17.5 μm). Left: reconstruction error using all four RGC types. Red line shows linear regression to the data; regression slope 0.93. Middle: reconstruction error using only ON cells; regression slope 0.93. Right: reconstruction error using all four RGC types, with learning; regression slope 0.78. Upper panels show example reconstructed images from each set of parameters.</p></caption><graphic xlink:href="206409_fig5" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><p hwp:id="p-38">Could artificial vision improve if the brain learns to interpret the unnatural patterns of activation of RGCs that occur with a prosthesis? To answer this question, optimal reconstruction filters were recalculated using RGC activity elicited by the prosthesis, instead of natural RGC activity. This simulation of learning mimics a central visual system that learns optimally. As expected, the reconstruction filters associated with RGCs were very different in the learning simulation (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Fig. 6</xref>), because the RGC activity used for learning was very different from the healthy RGC response. Notably, reconstruction filters exhibited asymmetric shapes that varied between cells, and OFF cells often exhibited filters with ON polarity, consistent with the polarity of prosthesis stimulation. With learned filters, reconstruction of images using all RGC types was substantially improved (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-6" hwp:rel-id="F4">Fig. 4</xref>, bottom right). The RMS error of reconstruction across test images was 0.12, closer to the value achieved in natural vision. In this condition, improvements in electrode resolution produced a greater improvement than in the previous conditions: a two-fold reduction in electrode spacing and current spread reduced RMS error by 22% (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig. 5</xref>, right). Thus, the framework predicts that the potential for patients to learn the unnatural neural code and improve prosthesis function is significant, and if such learning does occur, it enhances the beneficial effects of technical improvements in the device.</p><fig id="fig6" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-6"><p hwp:id="p-39">Reconstruction filters in healthy and prosthesis simulations. Left: sample reconstruction filters are shown for ON and OFF parasol cells (four each) obtained from the healthy retina simulation. The filters roughly resemble the spatial receptive fields of the cells (not shown), and their center polarity corresponds to the polarity of light response. Right: sample spatial reconstruction filters for the same cells obtained from the prosthesis simulation. In this case, reconstructions filters are asymmetric and variable, and largely reflect the ON polarity of prosthesis stimulation for both cell types. Images are centered on cell location.</p></caption><graphic xlink:href="206409_fig6" position="float" orientation="portrait" hwp:id="graphic-12"/></fig><p hwp:id="p-40">A practical application of this framework to human patients is the prediction of visual performance on standardized tasks, including discrimination of Landolt C and grating orientation. To predict performance on this type of task, visual images were presented to the simulation, and reconstruction was performed as above. Then, the reconstructed images were used to classify the stimulus, using a linear classifier trained on reconstructions obtained from many stimulus presentations (see Methods). For example, in the Landolt C task, stimuli containing a gap were discriminated from stimuli not containing a gap. As expected, the predicted discrimination performance progressively increased with stimulus contrast (<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. 7</xref>).</p><p hwp:id="p-41">Discrimination thresholds were roughly ten-fold higher in the prosthesis simulation (with learning) than in the healthy retina simulation. Note that the healthy grating orientation discrimination thresholds were unrealistically low compared to normal human performance, reflecting a classifier that has prior “knowledge” of the stimulus pattern and location, as well as the fact that our simulations capture only some of the noise introduced in the retina, and none of the noise introduced downstream of the retina. Thus, a meaningful quantitative prediction of patient performance would require a more realistic simulation of behavioral discrimination, including consideration of the size of the stimulus, the task, the nature of the classifier applied to the simulated output, and assumptions about noise in the retina and the central visual system. However, using this simplified classification approach, the framework can generate predictions of patient performance relative to normal performance in clinical tasks.</p><fig id="fig7" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;206409v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-7"><p hwp:id="p-42">Landolt C and grating orientation discrimination, using healthy and prosthesis simulations. Left: Landolt C stimuli, and similar stimuli lacking a gap, were presented to the simulation, producing reconstructions for healthy retina and prosthesis-stimulated retina with learning. Accuracy of linear classification of stimuli with and without gaps increased with stimulus contrast; classification was nearly an order of magnitude more sensitive with the healthy retina simulation. Right: Grating stimuli, horizontally or vertically oriented, were presented to the simulation and discriminated. A large discrepancy between healthy and prosthesis performance was again observed. Note the unrealistically low normal contrast thresholds, reflecting an highly-trained linear classifier well aligned with the visual stimulus (see Methods).</p></caption><graphic xlink:href="206409_fig7" position="float" orientation="portrait" hwp:id="graphic-13"/></fig><p hwp:id="p-43">The above simulations were all performed using only static spatial images and integrated RGC responses, ignoring time. However, the linear reconstruction framework extends naturally to time-varying stimuli and RGC responses, and reconstructed image sequences. In a dynamic reconstruction, a given spike from a given RGC adds a specific spatio-temporal filter to the reconstruction, aligned in time with the spike. These contributions are summed over cells and over time. Although a quantitative assessment of performance and learning was not performed, dynamic reconstruction produced recognizable estimates of visual perception over time in both the natural and prosthesis simulation (see <underline>supplemental movies</underline>). The dynamic reconstructions revealed persistent temporal structure in natural scenes which could improve performance – these temporal correlations are not captured by the static simulations above.</p></sec></sec><sec id="s3" hwp:id="sec-12"><title hwp:id="title-12">Discussion</title><p hwp:id="p-44">A simulation framework was developed to explore visual perception and learning with a retinal prosthesis. The framework produced three primary predictions. First, as expected, artificial vision mediated by a prosthesis may be limited by the indiscriminate activation of ON and OFF cells, and simulated dominance of ON cells improves reconstruction. Second, learning by the patient could produce substantial improvements in the perceived image, in part by compensating for the lack of cell type specificity. Finally, learning could enable further benefits from technology enhancement. Below, we discuss the assumptions underlying the approach, its utility for future technology development, and major caveats.</p><sec id="s3a" hwp:id="sec-13"><title hwp:id="title-13">Impact of indiscriminate cellular activation</title><p hwp:id="p-45">In healthy vision, roughly 20 different RGC types communicate different visual information to distinct targets in the brain, via their unique temporal patterns of activity (see <xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Gollisch et al. 2010</xref>; <xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">Field and Chichilnisky 2007</xref>). Therefore, indiscriminate activation of different RGC types by a prosthesis produces a highly artificial retinal code, potentially limiting the quality of artificial vision. The most obvious example is simultaneous activation of ON and OFF cells, which may produce conflicting visual signals in the brain. Although this issue is well known, its impact on visual perception has been unclear. The reconstruction approach provides a framework for predicting and interpreting the effect. For instance, a naive simulation in which ON and OFF cells respond equally to prosthesis activation, and their contributions to reconstruction are optimized for natural vision, predicts that artificial vision will be highly degraded. However, previous work suggests that either ON or OFF cells may respond more strongly to electrical stimulation (<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">Werginz et al. 2015</xref>; <xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Cottaris and Elfar 2005</xref>), and patient reports indicate that “bright” (rather than “dark”) phosphenes dominate increasingly over time (<xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-3" hwp:rel-id="ref-40">Stingl et al. 2013</xref>). A simple simulation of ON dominance – a linear reconstruction in which OFF cells are not used – revealed that it could substantially improve artificial vision. In principle, to obtain a quantitative simulation of ON dominance, patient performance in visual tasks (e.g. detection of increments or decrements) could be used to estimate a weighting factor between 0 and 1 applied to the OFF cell contribution. The reconstruction framework may also be useful for assessing the value of developing retinal prostheses that can selectively stimulate distinct RGC types (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Jepson et al. 2013</xref>, <xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">2014</xref>, <xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-3" hwp:rel-id="ref-35">Nirenberg and Pandarinath 2012</xref>).</p></sec><sec id="s3b" hwp:id="sec-14"><title hwp:id="title-14">Impact of learning</title><p hwp:id="p-46">A closely related issue is the impact of learning by the patient, that is, changes in how the brain processes the retinal signal over time in the presence of retinal prosthesis stimulation. Although it is clear that learning occurs and improves artificial vision to some degree (<xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Stingl et al. 2015</xref>; da Cruz et al. 2016), its precise impact and potential to improve artificial vision is unclear. The reconstruction approach provides a framework for predicting and interpreting the effects of learning. In a simulation of ideal learning, linear reconstruction optimized for prosthesis stimulation of the retina produced much more accurate estimates of the visual image, compensating to a substantial degree for the indiscriminate activation of ON and OFF cells. In principle, to obtain a more quantitative account of learning, patient performance on visual tasks could be used to estimate a value between 0 and 1 indicating the degree of learning, that is, the relative contribution of the reconstruction filters optimized for prosthesis vs. healthy conditions. This approach could make it possible to objectively assess the improvements in performance over time in each patient. Note that this approach assumes that the learning process involves enhanced perception, rather than behavioral strategies to deal with degraded perception in specific tasks. The choice of measurement of patient performance would be essential for distinguishing these forms of learning.</p></sec><sec id="s3c" hwp:id="sec-15"><title hwp:id="title-15">Assumptions: linearity, error measure, natural images</title><p hwp:id="p-47">Three primary assumptions were used to produce the simulation. First, the brain reconstructs the visual image linearly from RGC responses. Second, this linear reconstruction minimizes the squared error between the perceived and true pixel intensities. Third, reconstruction is optimized for natural images, rather than other image ensembles. All of these assumptions are likely to be incorrect in detail and can be improved upon. Accepting the assumption that the brain reconstructs the visual image (see below), linear reconstruction is the simplest to implement, but may not perform as accurately as more complex approaches. Nonlinear reconstruction is a large domain of inquiry, and improvements are already being made using artificial neural networks (<xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Parthasarathy et al. 2017</xref>). Similarly, the definition of optimality in terms of squared error between pixel intensities was chosen for technical convenience. One might expect the visual system to instead emphasize accurate reconstruction of certain spatial scales, or specific image features, or even more generally to produce perceptual representations optimized not for physical accuracy but rather to support effective perceptual decisions in the natural environment (Hoffman and Singh, 2012). The image reconstruction assumption, however, is a pragmatic choice that allows us to make progress. In this spirit, a natural extension of the current approach would be to optimize reconstruction using perceptual image metrics, such as SSIM (<xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Wang et al. 2004</xref>). Finally, the use of natural images is predicated on the idea that the visual system evolved to encode the structure in the visual environment. This assumption is important because the reconstruction filters are shaped by the statistics of the visual inputs. However, the statistics of natural images are incompletely understood, and the particular statistical features which are most important to human evolution are even less clear. For example, human faces may be of high importance, but they represent only a tiny subset of the collection of images used. This problem could potentially be explored by using more specific image databases (<xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">Parthasarathy et al. 2017</xref>). In general, it is worth noting that the ability to predict patient experience is based on strong linking assumptions that may not hold. Experimental tests of these assumptions represent an interesting future direction. Even if ultimately the assumptions turns out to be too strong, the reconstruction analysis presented here can be modified (e.g. by allowing for variable weighting of ON and OFF inputs) to provide insight about the information carried through a prosthesis to the brain.</p></sec><sec id="s3d" hwp:id="sec-16"><title hwp:id="title-16">Related approaches</title><p hwp:id="p-48">Most previous attempts to simulate the perceptual experience of patients with retinal prostheses have used a simple “scoreboard model” of phosphenes, in which pixels of an image are translated into perceived dots of light according to the layout of electrodes (<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Chen et al. 2009</xref>). A notable exception is a recent simulation framework that takes into account different electrical sensitivities of bipolar cells, RGCs somas, and RGC axons (<xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Beyeler et al. 2017</xref>). This work reproduces clinical data and accounts for the perceptual effects of axon activation, temporal desensitization, and the distinct effects of pulse frequency and amplitude on phosphene attributes. However, the emphasis is quite different from the current work. Specifically, it is based on an empirical framework rather than a theoretical foundation such as stimulus reconstruction, it does not incorporate the distinct activation of different retinal cell types, and it does not provide a prediction of the effects of learning. A comparison or combination of the two approaches in future patient data may be valuable.</p></sec><sec id="s3e" hwp:id="sec-17"><title hwp:id="title-17">Image processing to optimize artificial vision</title><p hwp:id="p-49">A potential enhancement of existing and future prosthesis technology is image processing prior to electrical stimulation, which could emphasize features of the visual scene of greatest significance to the patient, or more generally could produce a pattern of activation that is most likely to lead to accurate reconstruction. The rapid advance of cheap, low-power digital signal processing makes this kind of image processing much more accessible. Although simple ideas such as edge enhancement have obvious potential, there is currently no framework within which to systematically test their impact. In principle, the framework proposed here could be used to optimize pre-processing of the image. By evaluating patient performance on specific tasks (<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Fig. 6</xref>), and comparing to the predictions, the impact of pre-processing could then be assessed and fine-tuned.</p></sec><sec id="s3f" hwp:id="sec-18"><title hwp:id="title-18">Temporal responses and reconstruction</title><p hwp:id="p-50">The present study tackled only the case of static images, for simplicity. A more complete understanding would require analyzing dynamic vision, with a changing visual environment and head and eye movements. The methods presented translate naturally to the dynamic case. The simulation of RGC responses in the ISETBio software encompasses their dynamics, to the degree that current models can (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-7" hwp:rel-id="ref-37">Pillow et al 2008</xref>; <xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Heitman et al. 2016</xref>). Based on dynamic responses, linear reconstruction of the input is performed by associating with each spike a spatiotemporal reconstruction filter, and summing those filters over space and time. As in the static case, estimation of the filters is obtained by least squares linear regression (<xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-4" hwp:rel-id="ref-45">Warland et al. 1997</xref>). In combination, these properties can be used to produce reconstructed movies that can then be compared to the original dynamic visual stimulus (see Results). For future work, some features of the dynamic simulation would need further investigation. First, although nonlinear reconstruction (see above) is readily accomplished with static images, this has not yet been performed with dynamic scenes, and the dimensionality of the problem is challenging. Second, perceptual measures of reconstruction accuracy, to improve upon the squared error metric, are less well developed for dynamic stimuli. Finally, appropriate stimuli are difficult to obtain, because the dynamic properties of vision due to eye movements are usually not present in the movie data sets that are readily available.</p></sec><sec id="s3g" hwp:id="sec-19"><title hwp:id="title-19">Interpreting the visual experience of patients</title><p hwp:id="p-51">A challenge associated with assessing prosthesis function is attempting to understand what patients see. Verbal reports are helpful, but it is extremely difficult for the report provided by an untrained blind patient to give a complete sense of their visual experience. Because the reconstruction framework produces images as predictions of visual experience, it could be used by clinicians to compare with and interpret patient verbal reports. Such feedback may help to fine-tune the assumptions and parameters of the simulation (see <xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">Beyeler et al. 2017</xref>), such as the degree of ON dominance or learning (see above). Note that variations in performance that are relevant to the clinic, such as surgical details and missing ganglion cells due to degeneration, were not considered here.</p></sec><sec id="s3h" hwp:id="sec-20"><title hwp:id="title-20">Assessing the potential of technological developments</title><p hwp:id="p-52">The development of retinal prostheses is ongoing, and represents a major international effort (<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">Goetz and Palanker 2016</xref>). Given the enormous resources required to develop new technologies and obtain regulatory approval, as well as the risks to human patients receiving the implants, simulations that can predict reasonably accurately the performance of envisioned future devices could be immensely valuable (Beyleyer et al. 2017). The potential for such predictions is illustrated by examining the dependence of reconstruction on features such as electrode spacing and current spread (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig. 5</xref>), which suggests that the biological limitations of the prosthesis need to be considered alongside the technical constraints.</p></sec><sec id="s3i" hwp:id="sec-21"><title hwp:id="title-21">Caveats and future</title><p hwp:id="p-53">Several other uncertain features of the approach deserve mention. First, the concept underlying the approach – that the visual system reconstructs the visual image from the retinal signal – is difficult to test explicitly. Second, the model of retinal activation by the photovoltaic implant and the model of healthy retinal processing are both rudimentary, and will need to be improved to better capture temporal phenomena, nonlinearities, noise, and cell type specificity. Furthermore, it will be challenging to produce an accurate simulation of the retinal activity produced by a specific prosthesis technology implanted in a specific patient with a particular disease process and treatment history. Finally, technical features of the reconstruction approach can surely be improved, as discussed above, and such improvements should be guided by patient reports over time. Thus, the framework presented here is not intended to be used unmodified in a clinical application. Instead, it may be considered a proof of concept and starting point for future efforts.</p></sec></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-22">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Angueyra JM"><surname>Angueyra</surname> <given-names>JM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rieke F"><surname>Rieke</surname> <given-names>F</given-names></string-name>. <article-title hwp:id="article-title-2">Origin and effect of phototransduction noise in primate cone photoreceptors</article-title>. <source hwp:id="source-1">Nat neurosci</source> <volume>16</volume>:<fpage>1692</fpage>–<lpage>700</lpage>, <year>2013</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Beyeler M"><surname>Beyeler</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boynton GM"><surname>Boynton</surname> <given-names>GM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fine I"><surname>Fine</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rokem A"><surname>Rokem</surname> <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-3">pulse2percept: A Python-based simulation framework for bionic vision</article-title>. <source hwp:id="source-2">bioRxiv</source> <volume>1</volume>:<fpage>148015</fpage>, <year>2017</year>.</citation></ref><ref id="c3" hwp:id="ref-3"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Boinagrov D"><surname>Boinagrov</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lei X"><surname>Lei</surname> <given-names>X</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins TI"><surname>Kamins</surname> <given-names>TI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Galambos L"><surname>Galambos</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris JS"><surname>Harris</surname> <given-names>JS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-4">Photovoltaic pixels for neural stimulation: circuit models and performance</article-title>. <source hwp:id="source-3">IEEE Trans. Biomed. Circuits Syst</source> <volume>10</volume>:<fpage>85</fpage>–<lpage>97</lpage>, <year>2016</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Boinagrov D"><surname>Boinagrov</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pangratz-Fuehrer S"><surname>Pangratz-Fuehrer</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-5">Selectivity of direct and network-mediated stimulation of the retinal ganglion cells with epi-, sub-and intraretinal electrodes</article-title>. <source hwp:id="source-4">J Neural Eng</source> <volume>11</volume>(<issue>2</issue>:<fpage>026008</fpage>, <year>2014</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Brainard DH"><surname>Brainard</surname> <given-names>DH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jiang H"><surname>Jiang</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cottaris NP"><surname>Cottaris</surname> <given-names>NP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rieke F"><surname>Rieke</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Farrell JE"><surname>Farrell</surname> <given-names>JE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wandell BA"><surname>Wandell</surname> <given-names>BA</given-names></string-name>. <article-title hwp:id="article-title-6">Isetbio: Computational tools for modeling early human vision. [Abstract] Imaging Systems and Applications. Optical Society of America</article-title>. <source hwp:id="source-5">IT4A-4</source>, <year>2015</year>. [The software is available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/isetbio/isetbio" ext-link-type="uri" xlink:href="https://github.com/isetbio/isetbio" hwp:id="ext-link-1">https://github.com/isetbio/isetbio</ext-link>]</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="book" citation-type="book" ref:id="206409v1.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Chambers JM"><surname>Chambers</surname> <given-names>JM</given-names></string-name>. <chapter-title>Computational methods for data analysis</chapter-title>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>, <year>1977</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Chen SC"><surname>Chen</surname> <given-names>SC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Suaning GJ"><surname>Suaning</surname> <given-names>GJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morley JW"><surname>Morley</surname> <given-names>JW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lovell NH"><surname>Lovell</surname> <given-names>NH</given-names></string-name>. <article-title hwp:id="article-title-7">Simulating prosthetic vision: I. Visual models of phosphenes</article-title>. <source hwp:id="source-6">Vision Res</source> <volume>49</volume>:<fpage>1493</fpage>–<lpage>506</lpage>, <year>2009</year>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>. <article-title hwp:id="article-title-8">A simple white noise analysis of neuronal light responses</article-title>. <source hwp:id="source-7">Network Comp Neural</source> <volume>12</volume>:<fpage>199</fpage>–<lpage>213</lpage>, <year>2001</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kalmar RS"><surname>Kalmar</surname> <given-names>RS</given-names></string-name>. <article-title hwp:id="article-title-9">Functional asymmetries in ON and OFF ganglion cells of primate retina</article-title>. <source hwp:id="source-8">J Neuro</source> <volume>22</volume>:<fpage>2737</fpage>–<lpage>47</lpage>, <year>2002</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="report" citation-type="journal" ref:id="206409v1.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><collab hwp:id="collab-1">CIE. International Commission on Illumination</collab>. <article-title hwp:id="article-title-10">Fundamental chromaticity diagram with physiological axes – Part 1</article-title>. <source hwp:id="source-9">Technical Report</source> <fpage>170</fpage>–<lpage>1</lpage>. <collab hwp:id="collab-2">Vienna: Central Bureau of the Commission Internationale de l’ Éclairage</collab>, <year>2006</year>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Cottaris NP"><surname>Cottaris</surname> <given-names>NP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Elfar SD"><surname>Elfar</surname> <given-names>SD</given-names></string-name>. <article-title hwp:id="article-title-11">How the retinal network reacts to epiretinal stimulation to form the prosthetic visual input to the cortex</article-title>. <source hwp:id="source-10">J Neural Eng</source> <volume>2</volume>:<fpage>S74</fpage>, <year>2005</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Croner LJ"><surname>Croner</surname> <given-names>LJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kaplan E"><surname>Kaplan</surname> <given-names>E</given-names></string-name>. <article-title hwp:id="article-title-12">Receptive fields of P and M ganglion cells across the primate retina</article-title>. <source hwp:id="source-11">Vision Res</source> <volume>35</volume>:<fpage>7</fpage>–<lpage>24</lpage>, <year>1995</year>.</citation></ref><ref id="c13" hwp:id="ref-13"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Da Cruz L"><surname>Da Cruz</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Coley BF"><surname>Coley</surname> <given-names>BF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dorn J"><surname>Dorn</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Merlini F"><surname>Merlini</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Filley E"><surname>Filley</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Christopher P"><surname>Christopher</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen FK"><surname>Chen</surname> <given-names>FK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wuyyuru V"><surname>Wuyyuru</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sahel J"><surname>Sahel</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stanga P"><surname>Stanga</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Humayun M"><surname>Humayun</surname> <given-names>M</given-names></string-name>. <article-title hwp:id="article-title-13">The Argus II epiretinal prosthesis system allows letter and word reading and long-term function in patients with profound vision loss</article-title>. <source hwp:id="source-12">Br J Ophthalmol</source> <volume>97</volume>:<fpage>632</fpage>–<lpage>6</lpage>, <year>2013</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Dacey DM"><surname>Dacey</surname> <given-names>DM</given-names></string-name>. <article-title hwp:id="article-title-14">Primate retina: cell types, circuits and color opponency</article-title>. <source hwp:id="source-13">Prog Retin Eye Res</source> <volume>18</volume>:<fpage>737</fpage>–<lpage>63</lpage>, <year>1999</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Dacey DM"><surname>Dacey</surname> <given-names>DM</given-names></string-name>. <article-title hwp:id="article-title-15">Parallel pathways for spectral coding in primate retina</article-title>. <source hwp:id="source-14">Annu Rev Neurosci</source> <volume>23</volume>:<fpage>743</fpage>–<lpage>75</lpage>, <year>2000</year>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="confproc" citation-type="confproc" ref:id="206409v1.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Deng J"><surname>Deng</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dong W"><surname>Dong</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Socher R"><surname>Socher</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li LJ"><surname>Li</surname> <given-names>LJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li K"><surname>Li</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fei-Fei L"><surname>Fei-Fei</surname> <given-names>L</given-names></string-name>. <article-title hwp:id="article-title-16">Imagenet: A large-scale hierarchical image database</article-title>. <conf-name>[Abstract] In Computer Vision and Pattern Recognition</conf-name>, <conf-sponsor>IEEE Conference</conf-sponsor>, <conf-date>2009</conf-date> <fpage>P248</fpage>–<lpage>255</lpage>, <collab hwp:id="collab-3">P 2009</collab>. [Software available online at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://image-net.org/download.php" ext-link-type="uri" xlink:href="http://image-net.org/download.php" hwp:id="ext-link-2">http://image-net.org/download.php</ext-link>]</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Eisen MD"><surname>Eisen</surname> <given-names>MD</given-names></string-name>. <article-title hwp:id="article-title-17">Djourno, Eyries, and the first implanted electrical neural stimulator to restore hearing</article-title>. <source hwp:id="source-15">Otol Neurotol</source> <volume>24</volume>:<fpage>500</fpage>–<lpage>6</lpage>, <year>2003</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Field GD"><surname>Field</surname> <given-names>GD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>. <article-title hwp:id="article-title-18">Information processing in the primate retina: circuitry and coding</article-title>. <source hwp:id="source-16">Annu Rev Neurosci</source> <volume>30</volume>:<fpage>1</fpage>–<lpage>30</lpage>, <year>2007</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Field GD"><surname>Field</surname> <given-names>GD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gauthier JL"><surname>Gauthier</surname> <given-names>JL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greschner M"><surname>Greschner</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Machado T"><surname>Machado</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jepson LH"><surname>Jepson</surname> <given-names>LH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shlens J"><surname>Shlens</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gunning DE"><surname>Gunning</surname> <given-names>DE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dabrowski W"><surname>Dabrowski</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paninski L"><surname>Paninski</surname> <given-names>L</given-names></string-name>. <article-title hwp:id="article-title-19">Functional connectivity in the retina at the resolution of photoreceptors</article-title>. <source hwp:id="source-17">Nature</source> <volume>467</volume>:<fpage>673</fpage>, <year>2010</year>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2 xref-ref-20-3"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith R"><surname>Smith</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lei X"><surname>Lei</surname> <given-names>X</given-names></string-name>, <string-name name-style="western" hwp:sortable="Galambos L"><surname>Galambos</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins T"><surname>Kamins</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-20">Contrast Sensitivity With a Subretinal Prosthesis and Implications for Efficient Delivery of Visual Information With a Subretinal Prosthesis</article-title>. <source hwp:id="source-18">Invest Ophthalmol Vis Sci</source> <volume>56</volume>:<fpage>7186</fpage>–<lpage>94</lpage>, <year>2015</year>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Goetz GA"><surname>Goetz</surname> <given-names>GA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker DV"><surname>Palanker</surname> <given-names>DV</given-names></string-name>. <article-title hwp:id="article-title-21">Electronic approaches to restoration of sight</article-title>. <source hwp:id="source-19">Rep Prog Phys</source> <volume>79</volume>:<fpage>096701</fpage>, <year>2016</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Gollisch T"><surname>Gollisch</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meister M"><surname>Meister</surname> <given-names>M</given-names></string-name>. <article-title hwp:id="article-title-22">Eye smarter than scientists believed: neural computations in circuits of the retina</article-title>. <source hwp:id="source-20">Neuron</source> <volume>65</volume>:<fpage>150</fpage>–<lpage>64</lpage>, <year>2010</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Hallberg LR"><surname>Hallberg</surname> <given-names>LR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ringdahl A"><surname>Ringdahl</surname> <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-23">Living with cochlear implants: experiences of 17 adult patients in Sweden</article-title>. <source hwp:id="source-21">Int J Audiol</source> <volume>43</volume>:<fpage>115</fpage>–<lpage>21</lpage>, <year>2004</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="other" citation-type="journal" ref:id="206409v1.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Heitman A"><surname>Heitman</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brackbill N"><surname>Brackbill</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greschner M"><surname>Greschner</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Litke AM"><surname>Litke</surname> <given-names>AM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>. <article-title hwp:id="article-title-24">Testing pseudo-linear models of responses to natural scenes in primate retina</article-title>. <source hwp:id="source-22">bioRxiv</source> <fpage>045336</fpage>, <year>2016</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Humayun MS"><surname>Humayun</surname> <given-names>MS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dorn JD"><surname>Dorn</surname> <given-names>JD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Da Cruz L"><surname>Da Cruz</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dagnelie G"><surname>Dagnelie</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sahel JA"><surname>Sahel</surname> <given-names>JA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stanga PE"><surname>Stanga</surname> <given-names>PE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cideciyan AV"><surname>Cideciyan</surname> <given-names>AV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duncan JL"><surname>Duncan</surname> <given-names>JL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eliott D"><surname>Eliott</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Filley E"><surname>Filley</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ho AC"><surname>Ho</surname> <given-names>AC</given-names></string-name>. <article-title hwp:id="article-title-25">Interim results from the international trial of Second Sight's visual prosthesis</article-title>. <source hwp:id="source-23">Ophthalmology</source> <volume>119</volume>:<fpage>779</fpage>–<lpage>88</lpage>, <year>2012</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Jepson LH"><surname>Jepson</surname> <given-names>LH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hottowy P"><surname>Hottowy</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gunning DE"><surname>Gunning</surname> <given-names>DE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dąbrowski W"><surname>Dąbrowski</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Litke AM"><surname>Litke</surname> <given-names>AM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>. <article-title hwp:id="article-title-26">Focal electrical stimulation of major ganglion cell types in the primate retina for the design of visual prostheses</article-title>. <source hwp:id="source-24">J Neurosci</source> <volume>33</volume>:<fpage>7194</fpage>–<lpage>205</lpage>, <year>2013</year>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Jepson LH"><surname>Jepson</surname> <given-names>LH</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hottowy P"><surname>Hottowy</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gunning DE"><surname>Gunning</surname> <given-names>DE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dąbrowski W"><surname>Dąbrowski</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Litke AM"><surname>Litke</surname> <given-names>AM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>. <article-title hwp:id="article-title-27">Spatially patterned electrical stimulation to enhance resolution of retinal prostheses</article-title>. <source hwp:id="source-25">J Neurosci</source> <volume>34</volume>:<fpage>4871</fpage>–<lpage>81</lpage>, <year>2014</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="other" citation-type="journal" ref:id="206409v1.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Jiang H."><surname>Jiang</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cottaris N."><surname>Cottaris</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Golden J."><surname>Golden</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brainard D."><surname>Brainard</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Farrell J."><surname>Farrell</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wandell B."><surname>Wandell</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-28">Simulating retinal encoding: factors influencing Vernier acuity</article-title>. <source hwp:id="source-26">bioRxiv</source>, <fpage>109405</fpage>, <year>2017</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Loudin JD"><surname>Loudin</surname> <given-names>JD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simanovskii DM"><surname>Simanovskii</surname> <given-names>DM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vijayraghavan K"><surname>Vijayraghavan</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sramek CK"><surname>Sramek</surname> <given-names>CK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Butterwick AF"><surname>Butterwick</surname> <given-names>AF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huie P"><surname>Huie</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="McLean GY"><surname>McLean</surname> <given-names>GY</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker DV"><surname>Palanker</surname> <given-names>DV</given-names></string-name>. <article-title hwp:id="article-title-29">Optoelectronic retinal prosthesis: system design and performance</article-title>. <source hwp:id="source-27">J Neural Eng</source> <volume>4</volume>:<fpage>S72</fpage>, <year>2007</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Lorach H"><surname>Lorach</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith R"><surname>Smith</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lei X"><surname>Lei</surname> <given-names>X</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mandel Y"><surname>Mandel</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins T"><surname>Kamins</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huie P"><surname>Huie</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris J"><surname>Harris</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-30">Photovoltaic restoration of sight with high visual acuity</article-title>. <source hwp:id="source-28">Nat Med</source> <volume>21</volume>:<fpage>476</fpage>–<lpage>82</lpage>, <year>2015a</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Lorach H"><surname>Lorach</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mandel Y"><surname>Mandel</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lei X"><surname>Lei</surname> <given-names>X</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins TI"><surname>Kamins</surname> <given-names>TI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huie P"><surname>Huie</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dalal R"><surname>Dalal</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris JS"><surname>Harris</surname> <given-names>JS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-31">Performance of photovoltaic arrays in-vivo and characteristics of prosthetic vision in animals with retinal degeneration</article-title>. <source hwp:id="source-29">Vision Res</source> <volume>111</volume>:<fpage>142</fpage>–<lpage>8</lpage>, <year>2015b</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Mandel Y"><surname>Mandel</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lavinsky D"><surname>Lavinsky</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huie P"><surname>Huie</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang L"><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins T"><surname>Kamins</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Manivanh R"><surname>Manivanh</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris J"><surname>Harris</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-32">Cortical responses elicited by photovoltaic subretinal prostheses exhibit similarities to visually evoked potentials</article-title>. <source hwp:id="source-30">Nat Comm</source> <volume>4</volume>:<fpage>1980</fpage>, <year>2013</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2 xref-ref-33-3 xref-ref-33-4"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Loudin J"><surname>Loudin</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huie P"><surname>Huie</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang L"><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins TI"><surname>Kamins</surname> <given-names>TI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Galambos L"><surname>Galambos</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Smith R"><surname>Smith</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris JS"><surname>Harris</surname> <given-names>JS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Palanker D"><surname>Palanker</surname> <given-names>D</given-names></string-name>. <article-title hwp:id="article-title-33">Photovoltaic retinal prosthesis with high pixel density</article-title>. <source hwp:id="source-31">Nat Photon</source> <volume>6</volume>:<fpage>391</fpage>–<lpage>7</lpage>, <year>2012</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="book" citation-type="book" ref:id="206409v1.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Menke W"><surname>Menke</surname> <given-names>W</given-names></string-name>. <chapter-title>Geophysical data analysis: discrete inverse theory: MATLAB edition</chapter-title>. <publisher-loc>Waltham, MA</publisher-loc>: <publisher-name>Academic press</publisher-name>, <year>2012</year></citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2 xref-ref-35-3"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Nirenberg S"><surname>Nirenberg</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pandarinath C"><surname>Pandarinath</surname> <given-names>C</given-names></string-name>. <article-title hwp:id="article-title-34">Retinal prosthetic strategy with the capacity to restore normal vision</article-title>. <source hwp:id="source-32">PNAS</source> <volume>109</volume>:<fpage>15012</fpage>–<lpage>7</lpage>, <year>2012</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2"><citation publication-type="other" citation-type="journal" ref:id="206409v1.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Parthasarathy N"><surname>Parthasarathy</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Batty E"><surname>Batty</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Falcon W"><surname>Falcon</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rutten T"><surname>Rutten</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rajpal M"><surname>Rajpal</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paninski L"><surname>Paninski</surname> <given-names>L</given-names></string-name>. <article-title hwp:id="article-title-35">Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons</article-title>. <source hwp:id="source-33">bioRxiv</source> <fpage>153759</fpage>, <year>2017</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2 xref-ref-37-3 xref-ref-37-4 xref-ref-37-5 xref-ref-37-6 xref-ref-37-7"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Pillow JW"><surname>Pillow</surname> <given-names>JW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shlens J"><surname>Shlens</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paninski L"><surname>Paninski</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Litke AM"><surname>Litke</surname> <given-names>AM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chichilnisky EJ"><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli EP"><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title hwp:id="article-title-36">Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source hwp:id="source-34">Nature</source> <volume>454</volume>:<fpage>995</fpage>, <year>2008</year>.</citation></ref><ref id="c38" hwp:id="ref-38"><citation publication-type="book" citation-type="book" ref:id="206409v1.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Rieke F"><surname>Rieke</surname> <given-names>F</given-names></string-name>. <chapter-title>Spikes: exploring the neural code</chapter-title>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT press</publisher-name>, <year>1999</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Shannon RV"><surname>Shannon</surname> <given-names>RV</given-names></string-name>. <article-title hwp:id="article-title-37">Advances in auditory prostheses</article-title>. <source hwp:id="source-35">Curr Opin Neurol</source> <volume>25</volume>:<fpage>61</fpage>, <year>2012</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2 xref-ref-40-3"><citation publication-type="confproc" citation-type="confproc" ref:id="206409v1.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Stingl K"><surname>Stingl</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bartz-Schmidt KU"><surname>Bartz-Schmidt</surname> <given-names>KU</given-names></string-name>, <string-name name-style="western" hwp:sortable="Besch D"><surname>Besch</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Braun A"><surname>Braun</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bruckmann A"><surname>Bruckmann</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gekeler F"><surname>Gekeler</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greppmaier U"><surname>Greppmaier</surname> <given-names>U</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hipp S"><surname>Hipp</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hörtdörfer G"><surname>Hörtdörfer</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kernstock C"><surname>Kernstock</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koitschev A"><surname>Koitschev</surname> <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-38">Artificial vision with wirelessly powered subretinal electronic implant alpha-IMS</article-title>. <conf-name>Proc R Soc Lond B Biol Sci</conf-name> <volume>280</volume>: <fpage>20130077</fpage>, <conf-date>2013</conf-date>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Stingl K"><surname>Stingl</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bartz-Schmidt KU"><surname>Bartz-Schmidt</surname> <given-names>KU</given-names></string-name>, <string-name name-style="western" hwp:sortable="Besch D"><surname>Besch</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chee CK"><surname>Chee</surname> <given-names>CK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cottriall CL"><surname>Cottriall</surname> <given-names>CL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gekeler F"><surname>Gekeler</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Groppe M"><surname>Groppe</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jackson TL"><surname>Jackson</surname> <given-names>TL</given-names></string-name>, <string-name name-style="western" hwp:sortable="MacLaren RE"><surname>MacLaren</surname> <given-names>RE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koitschev A"><surname>Koitschev</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kusnyerik A"><surname>Kusnyerik</surname> <given-names>A</given-names></string-name>. <article-title hwp:id="article-title-39">Subretinal visual implant alpha IMS–clinical trial interim report</article-title>. <source hwp:id="source-36">Vision Res</source> <volume>111</volume>:<fpage>149</fpage>–<lpage>60</lpage>, <year>2015</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Waltzman SB"><surname>Waltzman</surname> <given-names>SB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cohen NL"><surname>Cohen</surname> <given-names>NL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shapiro WH"><surname>Shapiro</surname> <given-names>WH</given-names></string-name>. <article-title hwp:id="article-title-40">The benefits of cochlear implantation in the geriatric population</article-title>. <source hwp:id="source-37">Otolaryngol Head Neck Surg</source> <volume>108</volume>:<fpage>329</fpage>–<lpage>33</lpage>, <year>1993</year>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Wang Z"><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bovik AC"><surname>Bovik</surname> <given-names>AC</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sheikh HR"><surname>Sheikh</surname> <given-names>HR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli EP"><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title hwp:id="article-title-41">Image quality assessment: from error visibility to structural similarity</article-title>. <source hwp:id="source-38">IEEE Trans Sig Process</source> <volume>13</volume>:<fpage>600</fpage>–<lpage>12</lpage>, <year>2004</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Wang L"><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathieson K"><surname>Mathieson</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kamins TI"><surname>Kamins</surname> <given-names>TI</given-names></string-name>, <string-name name-style="western" hwp:sortable="Loudin JD"><surname>Loudin</surname> <given-names>JD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Galambos L"><surname>Galambos</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goetz G"><surname>Goetz</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sher A"><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mandel Y"><surname>Mandel</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huie P"><surname>Huie</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lavinsky D"><surname>Lavinsky</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harris JS"><surname>Harris</surname> <given-names>JS</given-names></string-name>. <article-title hwp:id="article-title-42">Photovoltaic retinal prosthesis: implant fabrication and performance</article-title>. <source hwp:id="source-39">J Neural Eng</source> <volume>9</volume>:<fpage>046014</fpage>, <year>2012</year>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3 xref-ref-45-4"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Warland DK"><surname>Warland</surname> <given-names>DK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reinagel P"><surname>Reinagel</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meister M"><surname>Meister</surname> <given-names>M</given-names></string-name>. <article-title hwp:id="article-title-43">Decoding visual information from a population of retinal ganglion cells</article-title>. <source hwp:id="source-40">J Neurophys</source> <volume>78</volume>:<fpage>2336</fpage>–<lpage>50</lpage>, <year>1997</year>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2"><citation publication-type="journal" citation-type="journal" ref:id="206409v1.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Werginz P"><surname>Werginz</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benav H"><surname>Benav</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zrenner E"><surname>Zrenner</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rattay F"><surname>Rattay</surname> <given-names>F</given-names></string-name>. <article-title hwp:id="article-title-44">Modeling the response of ON and OFF retinal bipolar cells during electric stimulation</article-title>. <source hwp:id="source-41">Vision Res</source> <volume>111</volume>:<fpage>170</fpage>–<lpage>81</lpage>, <year>2015</year>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="confproc" citation-type="confproc" ref:id="206409v1.47" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Zrenner E"><surname>Zrenner</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bartz-Schmidt KU"><surname>Bartz-Schmidt</surname> <given-names>KU</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benav H"><surname>Benav</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Besch D"><surname>Besch</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bruckmann A"><surname>Bruckmann</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gabel VP"><surname>Gabel</surname> <given-names>VP</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gekeler F"><surname>Gekeler</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greppmaier U"><surname>Greppmaier</surname> <given-names>U</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harscher A"><surname>Harscher</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kibbel S"><surname>Kibbel</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koch J"><surname>Koch</surname> <given-names>J</given-names></string-name>. <article-title hwp:id="article-title-45">Subretinal electronic chips allow blind patients to read letters and combine them to words</article-title>. <conf-name>Proc R Soc Lond B Biol Sci</conf-name> <volume>278</volume>:<fpage>1489</fpage>–<lpage>97</lpage>, <conf-date>2011</conf-date>.</citation></ref></ref-list></back></article>
