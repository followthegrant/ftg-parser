<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.11.06.467486</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.11.06.467486</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.11.06.467486</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.11.06.467486</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.11.06.467486</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Normalization by orientation anisotropy in human V1-V3</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author: Zeming Fang, <email hwp:id="email-1">fangz5@rpi.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8091-4413</contrib-id><name name-style="western" hwp:sortable="Fang Zeming"><surname>Fang</surname><given-names>Zeming</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-8091-4413"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Olsson Catherine"><surname>Olsson</surname><given-names>Catherine</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Ma Wei Ji"><surname>Ma</surname><given-names>Wei Ji</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7475-5586</contrib-id><name name-style="western" hwp:sortable="Winawer Jonathan"><surname>Winawer</surname><given-names>Jonathan</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-7475-5586"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">Department of Psychology and Center for Neural Science, New York University</institution></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Cognitive Science, Rensselaer Polytechnic Institute</institution></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Anthropic</institution>, San Francisco, CA</aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-11-08T10:15:11-08:00">
    <day>8</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-11-14T11:27:23-08:00">
    <day>14</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-11-08T10:20:24-08:00">
    <day>8</day><month>11</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-11-14T11:32:41-08:00">
    <day>14</day><month>11</month><year>2021</year>
  </pub-date><elocation-id>2021.11.06.467486</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-11-06"><day>06</day><month>11</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-11-14"><day>14</day><month>11</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-11-14"><day>14</day><month>11</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="467486.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.11.06.467486v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="467486.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.11.06.467486v2/2021.11.06.467486v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.11.06.467486v2/2021.11.06.467486v2.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">An influential account of neuronal responses in primary visual cortex is the normalized energy model. This model is often implemented as a two-stage computation. The first stage is the extraction of contrast energy, whereby a complex cell computes the squared and summed outputs of a pair of linear filters in quadrature phase. The second stage is normalization, in which a local population of complex cells mutually inhibit one another. Because the population includes cells tuned to a range of orientations and spatial frequencies, the result is that the responses are effectively normalized by the local stimulus contrast. Here, using evidence from human functional MRI, we show that the classical model fails to account for the relative responses to two classes of stimuli: straight, parallel, band-passed contours (<italic toggle="yes">gratings</italic>), and curved, band-passed contours (<italic toggle="yes">snakes</italic>). The snakes elicit fMRI responses that are about twice as large as the gratings, yet traditional energy models, including normalized energy models, predict responses that are about the same. Here, we propose a computational model, in which responses are normalized not by the sum of the contrast energy, but by the orientation anisotropy, computed as the variance in contrast energy across orientation channels. We first show that this model accounts for differential responses to these two classes of stimuli. We then show that the model successfully generalizes to other band-pass textures, both in V1 and in extrastriate cortex (V2 and V3). We speculate that high anisotropy in the orientation responses leads to larger outputs in downstream areas, which in turn normalizes responses in these later visual areas, as well as in V1 via feedback.</p></abstract><counts><page-count count="55"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-3">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-1"><p hwp:id="p-4">Revise figure 8, add error bar</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1.</label><title hwp:id="title-4">Introduction</title><p hwp:id="p-5">Primary visual cortex (“V1”) has served as a testing ground for studying physiology, anatomy, brain development, neuroimaging, and computational modeling. There has been considerable success in developing general model forms that capture many of the encoding properties of V1 neurons reasonably well over a range of stimulus conditions, including the normalized energy model of V1 complex cells (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Heeger, 1993</xref>). This type of model, like many others (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Hubel and Wiesel, 1962</xref>; reviewed in chapter 6 of <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Wandell, 1995</xref>), includes a linear filter as the first stage, i.e., a weighted sum of the stimulus intensity over space and time. In a second stage, the outputs of the filter are squared and summed across nearby spatial locations or across phase (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Heeger, 1992a</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Pollen and Ronner, 1983</xref>). If the outputs are summed across a pair of linear filters tuned to the same frequency, orientation, and location, but differing in phase by 90 deg, it is called an energy model. In the third stage, the response of each neuron is normalized (divisively suppressed) by the (un-normalized) outputs of the nearby neural population (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Heeger, 1992b</xref>, <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">1993</xref>). This effectively adjusts the gain based on the contrast energy in the image or image patch. There is substantial evidence that each of these three operations–linear filtering, energy, and normalization–contribute to the responses of V1 neurons (reviewed by <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Carandini et al., 2005</xref>).</p><p hwp:id="p-6">The normalized contrast energy model, though initially developed to explain the outputs of single neurons, has also been successfully applied to functional MRI data in human visual cortex. First, a contrast energy model without normalization, applied to voxels in V1, V2, and V3, was used for successful encoding and decoding of natural images (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Kay et al., 2008</xref>). Subsequent work showed that incorporating a normalization-like non-linearity improved model accuracy when testing stimuli that varied substantially in size (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Kay et al., 2013b</xref>) or pattern (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Kay et al., 2013c</xref>), and that normalization could account for the BOLD contrast response function for gratings with and without masking stimuli at other orientations (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Brouwer and Heeger, 2011</xref>). These models have also shown good prediction accuracy for similar stimulus sets used in human intracranial electrode recordings of visual cortex (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Hermes et al., 2019</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Winawer et al., 2013</xref>).</p><p hwp:id="p-7">The normalized contrast energy model, although successful at accounting for responses to a range of stimuli, nonetheless fails to explain some phenomena. There is some evidence that the standard model declines in accuracy for natural images (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">David et al., 2004</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Olshausen and Field, 2005</xref>; <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Vinje and Gallant, 2000</xref>) (but see also (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Rust et al., 2005</xref>)). Recent V1 two-photon calcium recordings included a large stimulus set and found that many cells in the superficial layer were sensitive to curvature, corners, and junctions (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Tang et al., 2018</xref>), which the standard energy model is unlikely to predict, although a model was not fit to the data. Other studies with single unit electrophysiology found that a normalization model could be successful but only if the normalization was flexible, such that its strength depended on statistical dependencies in the image (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Coen-Cagli et al., 2015</xref>). In human fMRI studies, the responses to natural/complex images in V1 also appear to be influenced by statistical dependencies and image context (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Mannion et al., 2015</xref>; <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Qiu et al., 2016</xref>), factors unlikely to influence the predictions of the normalized energy model.</p><p hwp:id="p-8">In visual areas beyond V1, the normalized energy model is expected to be incomplete, as circuits in these areas contribute new computations. There are no widely adopted standard encoding models for these areas analogous to the normalized energy model for V1, but there has been some success in modeling patterns in the extrastriate responses by incorporating higher-order statistical dependencies of the modeled V1 outputs (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Movshon and Simoncelli, 2015</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Okazawa et al., 2017</xref>) or sensitivity to second-order contrast (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Kay et al., 2013c</xref>).</p><p hwp:id="p-9">Here, using evidence from human functional MRI, we show that the classical model fails to account for the relative responses to two classes of stimuli: straight, parallel, band-passed contours (<italic toggle="yes">gratings</italic>), and curved, band-passed contours (<italic toggle="yes">snakes</italic>) (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>). The snakes elicit fMRI responses that are about twice as large as the gratings, yet traditional energy models, including normalized energy models, predict responses that are about the same. This is a large model failure which motivated us to develop a new model. We propose a computational model, in which responses are normalized not by the sum of the contrast energy, but by the variance in contrast energy computed across orientation channels.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><title hwp:id="title-5">Example straight-line and curved-line stimuli from our experiments.</title><p hwp:id="p-10">We observe that in human V1, V2, and V3, stimuli with long straight lines (gratings) reliably evoke a smaller fMRI response than similar stimuli with curved lines.</p></caption><graphic xlink:href="467486v2_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><title hwp:id="title-6">The orientation-tuned surround normalization filter.</title><p hwp:id="p-11">When <italic toggle="yes">θ</italic>′ = <italic toggle="yes">θ</italic>(diagonal), the filter is elongated in the direction of <italic toggle="yes">θ</italic>. For other orientations (off diagonals) the filter is near zero except at the center pixel (cross-orientation suppression).</p></caption><graphic xlink:href="467486v2_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2" hwp:id="sec-2"><label>2.</label><title hwp:id="title-7">Methods</title><p hwp:id="p-12">We analyzed and modeled four fMRI data sets for this paper. Data sets 1 and 2 were collected at NYU for this project. Data sets 3 and 4 are re-analyzed from a previous paper <bold>(</bold><xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-3" hwp:rel-id="ref-26">Kay et al., 2013c</xref><bold>)</bold>, for which the fMRI data and stimuli are freely available online (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://kendrickkay.net/socmodel/" ext-link-type="uri" xlink:href="http://kendrickkay.net/socmodel/" hwp:id="ext-link-2">http://kendrickkay.net/socmodel/</ext-link>).</p><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-8">Participants</title><p hwp:id="p-13">The two NYU participants were both experienced MRI subjects (female; 24, 29 yo). Data were collected at NYU’s Center for Brain Imaging. The experimental protocol was approved by the University Committee on Activities Involving Human Subjects, and informed written consent was obtained from the participants before the study. Both participants had corrected-to-normal vision. The subjects participated in two separate scanning sessions, one for retinotopic mapping and one for the main study on encoding of textures.</p></sec><sec id="s2b" hwp:id="sec-4"><label>2.2</label><title hwp:id="title-9">Stimuli</title><p hwp:id="p-14">Publicly accessible links to the stimuli, the names of the stimulus classes, and the correspondence between our naming convention and those in the <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-4" hwp:rel-id="ref-26">Kay <italic toggle="yes">et al</italic>. (2013c)</xref> paper are described in <xref rid="tblS1" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Supplementary Tables 1</xref> and <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">2</xref>.</p><sec id="s2b1" hwp:id="sec-5"><label>2.2.1</label><title hwp:id="title-10">Stimuli for data sets 3 and 4</title><p hwp:id="p-15">Data sets 3 and 4 correspond to subject 1 and subject 2, respectively, in (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-5" hwp:rel-id="ref-26">Kay et al., 2013c</xref>). The stimuli for the two subjects were the same and are referred to as “Stimulus set 2” on the website (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://kendrickkay.net/socmodel/" ext-link-type="uri" xlink:href="http://kendrickkay.net/socmodel/" hwp:id="ext-link-3">http://kendrickkay.net/socmodel/</ext-link>). The publicly available stimulus set includes 156 stimuli, 39 of which were used for this paper (<xref rid="tblS2" ref-type="table" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Supplementary Tables 2</xref> and <xref ref-type="table" rid="tblS3" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">3</xref>). The reason we use only a subset of the stimuli is that we modeled how visual areas respond to textures ignoring retinotopic preference, and many of the stimuli used in the original paper varied systematically over space in order to map spatial receptive fields. We used only the large-field textures, i.e., the subset of stimuli whose patterns were similar across the whole circular aperture.</p><p hwp:id="p-16">The 39 stimuli are organized into 7 groups, each of which we describe with two terms, one term for the type of texture and one term for the way in which the stimuli within the group vary. For example, GRATINGS (contrast) are stimuli which come from the grating family and vary from low to high contrast. GRATINGS (density) stimuli come from the same family but have uniform contrast and vary in the spacing between the contours. The correspondence between how we refer to the stimuli and how Kay et al referred to them is in <xref rid="tblS2" ref-type="table" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">Supplementary Table 2</xref>. Below we describe the general stimulus characteristics and the 7 specific classes used for this paper. Most of the text is duplicated from (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-6" hwp:rel-id="ref-26">Kay et al., 2013c</xref>)(p. 11), indicated by italics.</p><p hwp:id="p-17"><italic toggle="yes">General stimulus characteristics. Stimuli were constructed at a resolution of 256 pixels x 256 pixels and were upsampled to 800 pixels x 800 pixels for display purposes. All stimuli were presented within a circular aperture filling the height of the display; the rest of the display was filled with neutral gray. The outer 0</italic>.<italic toggle="yes">5 deg of the circular aperture was smoothly blended into the background using a half-cosine function</italic>.</p><p hwp:id="p-18"><italic toggle="yes">Stimuli consisted of grayscale images restricted to a band-pass range of spatial frequencies centered at 3 cycles per degree. To enforce this restriction, a custom band-pass filter was used in the generation of some of the stimuli. The filter was a zero-mean isotropic 2D Difference-of-Gaussians filter whose amplitude spectrum peaks at 3 cycles per degree and drops to half-maximum at 1</italic>.<italic toggle="yes">4 and 4</italic>.<italic toggle="yes">7 cycles per degree. Restricting the spatial frequency content of the stimuli avoids the complications of building multiscale models and helps constrain the scope of the modeling endeavor. Even with the spatial frequency restriction, it is possible to construct a rich diversity of stimuli including objects and other naturalistic stimuli…. Each stimulus consisted of nine distinct images that were presented in quick succession. The purpose of this design was to take advantage of the slow dynamics of the BOLD response and average over stimulus dimensions of no interest (e</italic>.<italic toggle="yes">g</italic>., <italic toggle="yes">using sinusoidal gratings differing in phase to average over phase)</italic>.</p><p hwp:id="p-19">A key motivating observation for this paper is that the response to gratings was lower than to curved stimuli. Four groups of stimuli, two groups of gratings and two groups of snakes, were studied first and are referred to throughout the paper as <italic toggle="yes">target stimuli</italic>. These are described first.</p><sec id="s2b1a" hwp:id="sec-6"><title hwp:id="title-11">Target stimuli</title><p hwp:id="p-20">SNAKES (contrast, 10 stimuli). Kay et al. refers to these stimuli as noise patterns: <italic toggle="yes">Noise patterns were created by low-pass filtering white noise at a cutoff frequency of 0</italic>.<italic toggle="yes">5 cycles per degree, thresholding the result, performing edge detection using derivative filters, inverting image polarity such that edges are black, and applying the custom band-pass filter (described previously). We generated nine distinct noise patterns and scaled the contrast of the patterns to fill the full luminance range</italic>. [The contrast stimuli were then] <italic toggle="yes">constructed by varying the contrast of the noise patterns. Ten different contrast levels were used: 1%, 2%, 3%, 4%, 6%, 9%, 14%, 21%, 32%, and 50%. These contrast levels are relative to the contrast of the patterns used in SPACE</italic> [not used in this study], <italic toggle="yes">which is taken to be 100%</italic>.</p><p hwp:id="p-21">SNAKES (density, 5 stimuli). <italic toggle="yes">These stimuli used the same type of noise patterns as SPACE</italic> [not used here] <italic toggle="yes">but varied the amount of separation between contours. We generated noise patterns using cutoff frequencies of 2</italic>.<italic toggle="yes">8, 1</italic>.<italic toggle="yes">6, 0</italic>.<italic toggle="yes">9, 0</italic>.<italic toggle="yes">5, and 0</italic>.<italic toggle="yes">3 cycles per degree, and numbered these from 1 (smallest separation) to 5 (largest separation). The noise patterns used in SPACE correspond to separation 4; thus, we only constructed stimuli for the remaining separations 1, 2, 3, and 5. The noise patterns occupied the full stimulus extent (no aperture masking)</italic></p><p hwp:id="p-22">GRATINGS (contrast, 4 stimuli). <italic toggle="yes">These stimuli consisted of horizontal sinusoidal gratings at 2%, 4%, 9%, and 20% Michelson contrast. The spatial frequency of the gratings was fixed at 3 cycles per degree</italic>.</p><p hwp:id="p-23">GRATINGS (density, 5 stimuli). The highest density stimulus in this group is similar to the horizontally oriented stimulus in GRATINGS (orientation), i.e., similar to a horizontal high-contrast grating, but it is not precisely a sinusoidal grating. It is made by convolving equally spaced horizontal lines with the custom band-pass filter (described previously). When the gratings are spaced appropriately (⅓ deg spacing) and filtered by a band-pass filter centered at 3 cycles per deg, the result is close to a sinusoidal grating at 3 cycles per deg. When the spacing is larger, there are several parallel band-pass contours with uniform gray between them. The spacing between parallel lines for the 5 stimuli varied in powers of 2, as 1/3 deg x 1,2,4,8, or 16, from densest to sparsest. Because the grating and snakes stimuli were both constructed by convolving lines with the same band-pass filter, they have some similar properties. They differ in that the lines here were straight whereas the lines used for constructing the snakes stimuli were curved.</p></sec><sec id="s2b1b" hwp:id="sec-7"><title hwp:id="title-12">Additional stimuli</title><p hwp:id="p-24">GRATINGS (orientation, 8 stimuli). <italic toggle="yes">These stimuli consisted of full-contrast sinusoidal gratings at eight different orientations. The spatial frequency of the gratings was fixed at 3 cycles per degree. Each [of the 9 exemplars per stimulus] consisted of gratings with the same orientation but nine different phases (equally spaced from 0 to 2π)</italic>.</p><p hwp:id="p-25">PLAID (contrast, 4 stimuli). <italic toggle="yes">These stimuli consisted of plaids at 2%, 4%, 9%, and 20% contrast (defined below). Each condition comprised nine plaids, and each plaid was constructed as the sum of a horizontal and a vertical sinusoidal grating (spatial frequency 3 cycles per degree, random phase). The plaids were scaled in contrast to match the root-mean-square (RMS) contrast of the GRATING stimuli. For example, the plaids in the 9% condition were scaled such that the average RMS contrast of the plaids is identical to the average RMS contrast of the gratings in the 9% GRATING stimulus</italic>.</p><p hwp:id="p-26">CIRCULAR (contrast, 4 stimuli). <italic toggle="yes">These stimuli were identical to the PLAID stimuli except that sixteen different orientations were used instead of two</italic>.</p></sec></sec><sec id="s2b2" hwp:id="sec-8"><label>2.2.2</label><title hwp:id="title-13">Stimuli for data set 1</title><p hwp:id="p-27">Data set 1 was collected at NYU. The data set was designed to replicate some of the effects observed from data sets 3 and 4 (the greater response to snakes than gratings), but also to extend the measurements to new stimulus classes. The general stimulus characteristics were the same as those used in data sets 3 and 4. However, because the display size differed, the image resolution in pixels also differed (400 × 400 here, vs 800 × 800 above), and there were slight differences in the bandpass filter. The stimulus size in degrees of visual angle was the same (12.5 deg diameter). A total of 50 stimuli were tested. (The numbers below total more than 50 because some stimuli belong to more than one group, as indicated in <xref rid="tblS2" ref-type="table" hwp:id="xref-table-wrap-4-3" hwp:rel-id="T4">Supplementary Table 2</xref>).</p><sec id="s2b2a" hwp:id="sec-9"><title hwp:id="title-14">Target stimuli</title><p hwp:id="p-28">GRATINGS (contrast, 5 stimuli). These stimuli are horizontal gratings (but not quite sinusoids), with a similar spatial pattern to the middle stimulus in the GRATINGS (density) stimuli from data sets 3 and 4.</p><p hwp:id="p-29">They were made by convolving horizontal lines spaced every 1.75 deg with a custom band-pass filter. The images were scaled to yield 5 different contrasts of 3%, 10%, 25%, 50% and 100%.</p><p hwp:id="p-30">GRATINGS (density; 5 stimuli). These stimuli are similar to horizontal gratings, made by convolving equally spaced horizontal lines with the custom band-pass filter. The 5 stimuli differed in the spacing of the horizontal lines, spaced every 3, 2.5, 1.75, 1, 0.33 deg. The contrast of all stimuli was 25%. The middle stimulus in this sequence was the same as the middle stimulus in the contrast sequence (spacing 1.75 deg, 25% contrast). The highest density (0.33 deg spacing) is close to a sinusoidal grating, as the spacing is the inverse of the peak spatial frequency of the band-pass filter (3 cycles per degree).</p><p hwp:id="p-31">snakes (contrast, 5 stimuli). The spatial pattern is similar to the snakes stimuli in DS3 and DS4. Contrasts matched the grating contrasts (3%, 10%, 25%, 50% and 100%).</p><p hwp:id="p-32">snakes (density, 5 stimuli). The spatial pattern is similar to the snakes stimuli in DS3 and DS4, but with 5 different densities of the contours. The contrast for all stimuli was 25% (lower than the contrast of the corresponding stimuli in DS3 and DS4). The range of densities used here was also lower than the range used in DS3 and DS4, with the densest pattern here similar to the middle stimuli in DS3 and DS4.</p></sec><sec id="s2b2b" hwp:id="sec-10"><title hwp:id="title-15">Additional stimuli</title><p hwp:id="p-33">GRATINGS (orientation, 4 stimuli). These stimuli are the same as the third stimulus in the GRATINGS (density) group (25% contrast, 1.75 deg spacing between contours), except that they are rotated by 0, 45, 90, or 135 deg. Because the 0 deg rotation does not change the image, a new stimulus was not created; for visualization of results, the BOLD measurements and model predictions are plotted for both groups.</p><p hwp:id="p-34">GRATINGS (cross, 4 stimuli). These stimuli contain horizontal contours similar to two of the stimuli in the GRATINGS (density) sequence, except that they have periodic vertical blank regions which interrupt the contours. For two of the stimuli, the spacing of the horizontal contours matches the densest stimuli in the density sequence (spacing of 0.33 deg) and for two of the stimuli, the spacing matched the middle stimulus in the density sequence (1.75 deg spacing). In all 4 images, the horizontal contours are interrupted by vertical blanks spaced every 1.75 deg. The vertical blanks are either thick (50% duty cycle; 1st and 3rd stimulus) or thin (25% duty cycle; 2nd and 4th stimuli).</p><p hwp:id="p-35">NOISE BARS (density, 5 stimuli). These stimuli have the same contrast apertures as the GRATINGS (density) stimuli. Specifically, there are horizontal bands containing contrast patterns, spaced the same as the grating stimuli (bands every 3, 2.5, 1.75, 1, or 0.33 deg). These stimuli differ from the gratings in that each band contains band-pass filtered noise, equal in power across orientations, rather than horizontal contours.</p><p hwp:id="p-36">NOISE BARS (contrast, 5 stimuli). These stimuli are matched in spatial pattern to the middle density of the NOISE BARS (density) stimuli (horizontal lines, spacing 1.75 deg), but scaled in contrast similar to the grating stimuli (3%, 10%, 25%, 50%, 100%).</p><p hwp:id="p-37">NOISE BARS (orientation, 4 stimuli). The orientation sequence rotated the middle stimulus of the NOISE BARS (density) group (spacing 1.75 deg, contrast 25%) by 0, 45, 90, or 135 deg.</p><p hwp:id="p-38">WAVES (density, 6 stimuli). These are identical to the snakes (density) stimuli, except that they have been filtered by orientation, such that they only contain power at or near the horizontal.</p><p hwp:id="p-39">WAVES (contrast, 5 stimuli). These are identical to the snakes (contrast) stimuli, except that they have been filtered by orientation, such that they only contain power at or near the horizontal.</p><p hwp:id="p-40">WAVES (orientation, 4 stimuli). These are identical to the densest stimulus in the snakes (density) group, except that they have been filtered by orientation, with filter centered at either 0, 45, 90, or 135 deg.</p></sec></sec><sec id="s2b3" hwp:id="sec-11"><label>2.2.3</label><title hwp:id="title-16">Stimuli for data set 2</title><p hwp:id="p-41">Data set 2 was collected at NYU. The stimuli were nearly identical to those in data set 1, differing only in the following ways. First, the stimuli were 50% larger (18.75 × 18.75 deg and 600 × 600 pixels, rather than 12.5 × 12.5 deg and 400 × 400 pixels). The difference in size did not entail a difference in spatial frequency: The spatial frequency was matched between the two data sets (meaning that the stimuli were re-made with a larger aperture rather than by re-scaling). Second, those stimuli which were oriented were oriented vertically rather than horizontally. This applies to all GRATING stimuli, as well as NOISE BARS and WAVES. Third, the WAVES (density) stimuli had only 4 densities rather than 6. We reduced the number of stimuli to slightly shorten the MRI scans.</p></sec></sec><sec id="s2c" hwp:id="sec-12"><label>2.3</label><title hwp:id="title-17">MRI</title><p hwp:id="p-42">The methods for MRI acquisition and preprocessing for data sets 3 and 4 are described in <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-7" hwp:rel-id="ref-26">Kay et al (2013c)</xref>. In brief, each data set comes from one subject, who viewed a variety of stimuli in an event-related fMRI design. Data set 3 was collected over two scan sessions and each stimulus was presented 6 times. Data set 4 was collected over one scan session and each stimulus was presented 3 times. (Note that both in this paper and the Kay website (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://kendrickkay.net/socmodel/" ext-link-type="uri" xlink:href="http://kendrickkay.net/socmodel/" hwp:id="ext-link-4">http://kendrickkay.net/socmodel/</ext-link>), these two datasets are referred to as data sets 3 and 4. However, the Kay website refers to the stimuli for these two datasets as stimulus set 2 and the subjects themselves as “subject B” and “subject C”. We do not adopt these latter two conventions.)</p><p hwp:id="p-43">After preprocessing the data (slice-time correction, co-registration, spatial unwarping), a general linear model was applied using the GLMdenoise toolbox (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Kay et al., 2013a</xref>). The output of this algorithm includes a coefficient (beta weight) for each stimulus for each voxel solved from the whole fMRI session, as well as 30 bootstrapped estimates of each beta weight (bootstrapping across fMRI runs). The publicly available data (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://kendrickkay.net/socmodel/" ext-link-type="uri" xlink:href="http://kendrickkay.net/socmodel/" hwp:id="ext-link-5">http://kendrickkay.net/socmodel/</ext-link>) are already pre-processed, denoised, and organized by ROI. Specifically, the data we used are in the files called “data set03.mat” and “data set04.mat” (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://kendrickkay.net/socmodel/dataset03.mat" ext-link-type="uri" xlink:href="http://kendrickkay.net/socmodel/dataset03.mat" hwp:id="ext-link-6">http://kendrickkay.net/socmodel/dataset03.mat</ext-link>, <ext-link l:rel="related" l:ref-type="uri" l:ref="http://kendrickkay.net/socmodel/dataset04.mat" ext-link-type="uri" xlink:href="http://kendrickkay.net/socmodel/dataset04.mat" hwp:id="ext-link-7">http://kendrickkay.net/socmodel/dataset04.mat</ext-link>). The datasets are described on the website as “Dataset 3 (subject B)” and “Dataset 4 (subject C),” respectively. Within the MATLAB files, we used the stored 3D array called “betas” (voxels × stimuli × bootstraps), limited to V1, V2, V3 as indicated in the grouping variables “roi” and “roilabels”, and limited to the 39 stimuli indicated in <xref rid="tblS2" ref-type="table" hwp:id="xref-table-wrap-4-4" hwp:rel-id="T4">Supplementary Table 2</xref>. Visual areas were identified by retinotopic mapping in a separate session.</p><sec id="s2c1" hwp:id="sec-13"><label>2.3.1</label><title hwp:id="title-18">Acquisition of data sets 1 and 2</title><p hwp:id="p-44">Datasets 1 and 2 were acquired in one scanning session each. Each scanning session had 12 fMRI runs of 249 s each (data set 1) or 241.5 s each (data set 2). For each data set, half of the stimuli were assigned to odd fMRI runs and half to even runs, so that each stimulus was shown 6 times in the session. The stimulus events were 3 s long, consisting of 9 alternations between stimulus exemplar and blank, ⅙ s each. Trial onsets were every 7.5 s (so 4.5 s blank between trials). To help estimate the hemodynamic response function, there were 12 s of blank at the beginning and end of each run, as well as 5 additional trials randomly interspersed with no stimulus (meaning that 5 times during the scan, trials were separated by 15 s instead of 7.5 s). Thus, each complete run consisted of either (25 stimuli + 5 blanks) * 7.5 s + 24 s = 249 s (data set 1) or (24 stimuli + 5 blanks) * 7.5 s + 24 s = 241.5 s (data set 2).</p><p hwp:id="p-45">All MRI data were acquired at New York University Center for Brain Imaging using a Siemens Allegra 3T head-only scanner with a Nova Medical phased array, 8-channel receive surface coil (NMSC072). For each participant, we collected functional images (single shot echo planar images, 1500 ms TR, 30 ms TE, and 72° flip angle). Voxels were 2.0 mm<sup>3</sup> isotropic, with 24 slices, with an inplane sampling of 104 × 80 voxels (208 mm A/P × 160 mm L/R). The slice prescription covered most of the occipital lobe, and the posterior part of both the temporal and parietal lobes. Images were corrected for B0 field inhomogeneity using a calibration scan and Center for Brain Imaging algorithms during offline image reconstruction.</p><p hwp:id="p-46">We also acquired 1 or 2 T1-weighted whole-brain anatomical scans (MPRAGE sequence; 1mm<sup>3</sup>), as well as a T1-weighted “inplane” image with the same slice prescription as the functional scans. This scan had an inplane resolution of 1.25 × 1.25 mm and a slice thickness of 2.5 mm, and was collected to aid alignment of the functional images to the high-resolution T1 weighted anatomical images.</p><p hwp:id="p-47">In a separate session, retinotopy scans were collected and analyzed using a pRF model as implemented in the Vistasoft software tool (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/vistalab/vistasoft" ext-link-type="uri" xlink:href="https://github.com/vistalab/vistasoft" hwp:id="ext-link-8">https://github.com/vistalab/vistasoft</ext-link>). The methods for acquisition and analysis of the retinotopy data are identical to that described by <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Zhou et al. (2018)</xref>.</p></sec><sec id="s2c2" hwp:id="sec-14"><label>2.3.2</label><title hwp:id="title-19">Data Preprocessing and Analysis</title><p hwp:id="p-48"><italic toggle="yes">Data preprocessing</italic>. Processing of the fMRI data was identical to that described by <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">Zhou et al. (2018)</xref>:</p><disp-quote hwp:id="disp-quote-1"><p hwp:id="p-49"><italic toggle="yes">We coregistered and segmented the T1 weighted whole-brain anatomical images into gray and white matter voxels using FreeSurfer’s autosegmentation algorithm (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://surfer.nmr.mgh.harvard.edu" ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu" hwp:id="ext-link-9">http://surfer.nmr.mgh.harvard.edu</ext-link>). Using custom software Vistasoft (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/vistalab/vistasoft" ext-link-type="uri" xlink:href="https://github.com/vistalab/vistasoft" hwp:id="ext-link-10">https://github.com/vistalab/vistasoft</ext-link>), the functional data were slice-time corrected by resampling the time series in each slice to the center of each 1</italic>.<italic toggle="yes">5 s volume. Data were then motion-corrected by coregistering all volumes of all scans to the first volume of the first scan. The first 8 volumes (12 s) of each scan were discarded for analysis to allow longitudinal magnetization and stabilized hemodynamic response</italic>.</p></disp-quote><p hwp:id="p-50"><italic toggle="yes">GLM</italic>. The preprocessed fMRI data were then fit by a general linear model, GLMDenoise (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">Kay et al., 2013a</xref>). This algorithm denoises the data by projecting out nuisance regressors derived in a data-driven manner, and estimates coefficients for each of the 48 or 50 stimuli for each voxel in the functional images. The algorithm bootstraps the data over fMRI runs. For data sets 1 and 2, we generated 50 bootstraps for data set 1 and 100 bootstraps for data set 2. The publicly available data from <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-8" hwp:rel-id="ref-26">Kay et al. (2013c)</xref>, included 30 bootstraps per subject. The algorithm also estimated a hemodynamic impulse response function as a finite impulse response function, with 35 time points (52.5 s) per subject.</p><p hwp:id="p-51"><italic toggle="yes">ROIs</italic>. Regions of interest for V1, V2, V3 were delineated manually using the Vistasoft (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/vistalab/vistasoft" ext-link-type="uri" xlink:href="https://github.com/vistalab/vistasoft" hwp:id="ext-link-11">https://github.com/vistalab/vistasoft</ext-link>) graphical user interface to visualize the results of the pRF models. These methods for identifying these boundaries are well established, as described in many publications (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Sereno et al., 1995</xref>; summarized by <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Wandell and Winawer, 2011</xref>). The ROIs for V1, V2 and V3 were identified on the cortical surface and then projected to the functional images. For purposes of data summary and model fitting, we took the average signal from each ROI. We did this by averaging the beta weight across voxels within an ROI separately for each stimulus, after voxel selection (<bold><xref rid="tblS1" ref-type="table" hwp:id="xref-table-wrap-3-2" hwp:rel-id="T3">Supplementary Table 1</xref></bold>). Because noise can be correlated across voxels, but should not be correlated across scans, when we bootstrapped the data, we average across voxels within an ROI for each bootstrap. For the purposes of model fitting, each of the 4 datasets comprised two matrices, one for the means and one for the standard deviation across bootstraps, each of which had a size equal to the number of stimuli by number of ROIs.</p></sec></sec><sec id="s2d" hwp:id="sec-15"><label>2.4</label><title hwp:id="title-20">Model Equations</title><p hwp:id="p-52">In the Results, we compared the accuracy of 4 models fit to the data, three of which are based on existing models or empirical findings–a contrast energy model, a second order contrast model, and an orientation tuned surround model–and one new model, which computes normalization ny orientation anisotropy. In this section we describe the computation that comprises each model.</p><p hwp:id="p-53">All four models consist of three primary steps: (1) computation of oriented contrast energy, (2) pooling across orientation and space, and (3) a power-law nonlinearity. Steps 1 and 3 are identical for all models. Step 2, spatial pooling, varies between models. To reduce the computational demand, all stimuli were downsampled to 150 × 150 (datasets 1, 3, 4) or 225 × 225 (dataset 2) and remapped to −0.5 to 0.5 intensity values (<xref ref-type="table" rid="tblS2" hwp:id="xref-table-wrap-4-5" hwp:rel-id="T4">Supplementary table S2</xref>). The images were then padded to 166 × 166 (datasets 1, 3, 4) or 241 × 241 (dataset 2).</p><sec id="s2d1" hwp:id="sec-16"><label>1.</label><title hwp:id="title-21">Contrast energy</title><p hwp:id="p-54">We denote by <italic toggle="yes">I</italic>(<italic toggle="yes">x, y</italic>) the value of the pre-processed input image at coordinates (<italic toggle="yes">x,y</italic>). The pre-processing causes the image values to have mean 0 and range −0.5 to 0.5. The image is projected onto a set of 16 Gabor filters, which comprise 8 orientations(<italic toggle="yes">θ</italic>) and 2 phases (<italic toggle="yes">ϕ</italic>), separated by 90° (i.e., “quadrature”). <italic toggle="yes">F</italic>(<italic toggle="yes">x, y, θ, ϕ</italic>) indicates the Gabor filter at a spatial location (<italic toggle="yes">x,y</italic>), orientation <italic toggle="yes">θ</italic>, and phase <italic toggle="yes">ϕ</italic>. Because the stimuli are all band-passed, we only filter at one spatial scale, matched to the stimulus power (3 cpd). The filters had two cycles each. The outputs over the two phases are squared, summed, and square-rooted. Finally, the contrast energy as a function of spatial position (<italic toggle="yes">x, y</italic>) and orientation (<italic toggle="yes">θ</italic>) becomes
<disp-formula id="ueqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="467486v2_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
We convolve the image <italic toggle="yes">I</italic> and the filter <italic toggle="yes">F</italic>. The computation of contrast energy has no free parameters. Typically, the calculation of contrast energy does not have a square root, but we included it for consistency with some prior work (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-9" hwp:rel-id="ref-26">Kay et al., 2013c</xref>).</p></sec><sec id="s2d2" hwp:id="sec-17"><label>2.</label><title hwp:id="title-22">Spatial pooling</title><p hwp:id="p-55">Each model differs in how contrast energy is pooled to yield a scalar value, s:
<disp-formula id="ueqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="467486v2_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
where we use square brackets to indicate a function of a function (also called a <italic toggle="yes">functional</italic>). We describe the pooling functional, Φ, for each model below.</p></sec><sec id="s2d3" hwp:id="sec-18"><label>3.</label><title hwp:id="title-23">Power-law nonlinearity</title><p hwp:id="p-56">Finally, the scalar is passed through a power-law nonlinearity to predict the BOLD amplitude <italic toggle="yes">r</italic> in units of percent signal change:
<disp-formula id="ueqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="467486v2_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
Where <italic toggle="yes">g</italic> is the gain and <italic toggle="yes">α</italic> is the exponent parameter. These are free parameters fit to the human data. The power-law nonlinearity is similar to divisive normalization in the case where each unit in a population is normalized by the same pool (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Kay et al., 2013b</xref>).</p></sec><sec id="s2d4" hwp:id="sec-19"><label>2.4.1</label><title hwp:id="title-24">Contrast energy model</title><p hwp:id="p-57">In the contrast energy model, the contrast energy is summed over orientations and space to yield a scalar output, <italic toggle="yes">s</italic>.
<disp-formula id="ueqn4" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="467486v2_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-6"/></alternatives>
</disp-formula>
<italic toggle="yes">N</italic><sub>ori</sub>is the number of orientation channels (always 8) and <italic toggle="yes">N</italic><sub>pixels</sub>is the number of pixels per stimulus in the downsampled and padded images (166<sup>2</sup> or 241<sup>2</sup>). There are no free parameters in this step, so the contrast energy model has only two free parameters, <italic toggle="yes">g</italic> and <italic toggle="yes">α</italic>, both from the power-law nonlinearity step.</p></sec><sec id="s2d5" hwp:id="sec-20"><label>2.4.2</label><title hwp:id="title-25">Second-order contrast model</title><p hwp:id="p-58">Rather than summing the energy over all channels (space and orientation), the SOC model first sums over orientations at each location to generate a single contrast image, <italic toggle="yes">E</italic><sub>spatial</sub>:
<disp-formula id="ueqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="467486v2_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
The model then computes the second-order contrast (contrast energy within the contrast image), to yield a scalar, <italic toggle="yes">s</italic>:
<disp-formula id="ueqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="467486v2_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
Where <italic toggle="yes">Ē</italic><sub>spatial</sub>is the mean of the contrast image, <italic toggle="yes">Ē</italic><sub>spatial</sub>(<italic toggle="yes">x, y</italic>), computed over space. <italic toggle="yes">w</italic> is a free parameter bounded by 0 and 1. When <italic toggle="yes">w</italic> is 1, <italic toggle="yes">s</italic> is the sum of squared deviations from the mean, analogous to the variance in the contrast image. When <italic toggle="yes">w</italic> is 0, <italic toggle="yes">s</italic> is the sum of squares (the total power in the contrast image), and hence is similar to the contrast energy model.</p><p hwp:id="p-59">This step adds one free parameter, <italic toggle="yes">w</italic>. Thus, the SOC model has 3 free parameters, <italic toggle="yes">w, g</italic>, and <italic toggle="yes">α</italic>.</p></sec><sec id="s2d6" hwp:id="sec-21"><label>2.4.3</label><title hwp:id="title-26">Orientation-tuned surround model</title><p hwp:id="p-60">The contrast energy in the OTS model is normalized before it is summed: Each (<italic toggle="yes">x, y, θ</italic>) element in the energy image is divided by a weighted sum of the other elements. Those weights are relatively high for elements at the same (<italic toggle="yes">x, y</italic>) location but other orientations, and for elements within an ellipse in the (<italic toggle="yes">x, y</italic>) slice at the same theta value. The ellipse is elongated in the orientation defined by theta. This is akin to summing two forms of normalization, cross-orientation suppression (same location, other orientations), and an orientation-tuned surround (same orientation, other locations), with the shape of the surround elongated.
<disp-formula id="ueqn7" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="467486v2_ueqn7.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
<italic toggle="yes">Z</italic>(<italic toggle="yes">x, y, θ</italic>) is the normalizer of the contrast energy image <italic toggle="yes">E</italic>(<italic toggle="yes">x, y, θ</italic>). The filter <italic toggle="yes">F</italic>(<italic toggle="yes">x, y, θ</italic>′; <italic toggle="yes">θ</italic>) is <italic toggle="yes">n</italic> pixels × <italic toggle="yes">n</italic> pixels × 8 orientations, where <italic toggle="yes">n</italic> is 25% of the image width. The filter assigns relative weights for normalization. When the orientational channel of the image and filter matches <italic toggle="yes">θ</italic>′ = <italic toggle="yes">θ</italic>, the 2-D filter of the channel is an elongated Gaussian with standard deviation of 0.85 × <italic toggle="yes">n</italic> pixels at the matched orientation and 0.1 × <italic toggle="yes">n</italic> pixels at the orthogonal orientation. At all other orientations (i.e., <italic toggle="yes">θ</italic>′ ≠ <italic toggle="yes">θ</italic>), the filter is a symmetric 2D Gaussian distribution with a standard deviation with 0.01 × <italic toggle="yes">n</italic> pixels.</p><p hwp:id="p-61">The normalized contrast energy is
<disp-formula id="ueqn8" hwp:id="disp-formula-8">
<alternatives hwp:id="alternatives-8"><graphic xlink:href="467486v2_ueqn8.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula>
where <italic toggle="yes">w</italic> is a parameter to control the strength of normalization. When <italic toggle="yes">w</italic> approaches 0, the normalization is low, and the overall expression approximates the contrast energy model. When <italic toggle="yes">w</italic> is large, there is strong normalization. We then sum <italic toggle="yes">d</italic> across space and orientation to result in the scalar, <italic toggle="yes">s</italic>.
<disp-formula id="ueqn9" hwp:id="disp-formula-9">
<alternatives hwp:id="alternatives-9"><graphic xlink:href="467486v2_ueqn9.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula>
Other than the overall weight of normalization, <italic toggle="yes">w</italic>, the parameters of the surround filter were chosen to maximize the suppression in grating-like stimuli, which contain elongated contours in a single direction. Prior analyses showed that a model with an orientation-tuned surround that was spatially symmetric (circular) predicted little difference between the responses to gratings and to noise patterns (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Olsson, Kay, &amp; Winawer, 2015</xref>).</p><p hwp:id="p-62">We note that this model has two normalization-like calculations, one that is orientation tuned and one on the final output. This is consistent with prior work showing that two stage of normalization (a cascade model) improved model accuracy (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-10" hwp:rel-id="ref-26">Kay et al., 2013c</xref>).</p></sec><sec id="s2d7" hwp:id="sec-22"><label>2.4.4</label><title hwp:id="title-27">Normalization by orientation anisotropy</title><p hwp:id="p-63">In a new model developed here, the normalization by orientation anisotropy (NOA) model, the pooling step first sums the contrast energy across space within an orientation band, resulting on one value per orientation band, <italic toggle="yes">E</italic><sub>ori</sub>(<italic toggle="yes">θ</italic>):
<disp-formula id="ueqn10" hwp:id="disp-formula-10">
<alternatives hwp:id="alternatives-10"><graphic xlink:href="467486v2_ueqn10.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula>
<italic toggle="yes">E</italic><sub>ori</sub>indicates the oriented energy. This energy at each orientation is then normalized by the variance across the 8 orientations, and then summed to produce a scalar.
<disp-formula id="ueqn11" hwp:id="disp-formula-11">
<alternatives hwp:id="alternatives-11"><graphic xlink:href="467486v2_ueqn11.gif" position="float" orientation="portrait" hwp:id="graphic-13"/></alternatives>
</disp-formula>
where <italic toggle="yes">Var</italic>(<italic toggle="yes">E</italic><sub>ori</sub>) = ∑<sub><italic toggle="yes">θ</italic></sub>(<italic toggle="yes">E</italic>(<italic toggle="yes">θ</italic>) − <italic toggle="yes">Ē</italic><sub>ori</sub>)<sup>2</sup> calculates the variance of the oriented energy and a non-negative parameter <italic toggle="yes">w</italic> controls the strength of the normalization. When <italic toggle="yes">w</italic>approaches 0, the normalization is low, and the overall expression approximates the contrast energy model. When <italic toggle="yes">w</italic> is large, there is strong normalization by the variance across orientation channel outputs. Calculating the variance of oriented energy involves a squaring operation. To keep the parameters comparable across different models, we also square the numerator and the parameter <italic toggle="yes">w</italic>.</p><p hwp:id="p-64">The pooling step adds one free parameter, <italic toggle="yes">w</italic>. Thus, the NOA model has 3 free parameters, <italic toggle="yes">w, g</italic>and <italic toggle="yes">α</italic>.</p></sec></sec><sec id="s2e" hwp:id="sec-23"><label>2.5</label><title hwp:id="title-28">Optimization</title><p hwp:id="p-65">In each model, we fitted the model free parameters using the MATLAB optimization tool <italic toggle="yes">fmincon</italic> by minimizing the squared error between the model prediction and the corresponding BOLD amplitude. Because each stimulus consisted of 9 exemplars shown to the subject in rapid succession, the model prediction for each stimulus was obtained by averaging the model predictions across the exemplars. To avoid getting stuck in the local minima of the nonconvex landscape, we ran the optimization algorithm with 40 different parameter initializations. Each initialized value was picked randomly. Except for the parameter <italic toggle="yes">w</italic> of the SOC model, which was constrained to lie between 0 and 1, all parameters were unbounded in the search, minimizing human interference in the fitting.</p></sec><sec id="s2f" hwp:id="sec-24"><label>2.6</label><title hwp:id="title-29">Cross-validation scheme</title><p hwp:id="p-66">All models were fit using an <italic toggle="yes">n</italic>-fold leave-one-out cross-validation scheme, where <italic toggle="yes">n</italic> is the number of stimuli. Thus, the BOLD signal prediction for each stimulus was generated by a model fit to all stimuli except that one. Under this scheme, the models are less likely to overfit data sets.</p></sec><sec id="s2g" hwp:id="sec-25"><label>2.7</label><title hwp:id="title-30">Accuracy metric</title><p hwp:id="p-67">The model accuracy was quantified as the percentage of the explained variance (<italic toggle="yes">R</italic><sup>2</sup>) in the human BOLD data by the cross-validated model predictions,
<disp-formula id="ueqn12" hwp:id="disp-formula-12">
<alternatives hwp:id="alternatives-12"><graphic xlink:href="467486v2_ueqn12.gif" position="float" orientation="portrait" hwp:id="graphic-14"/></alternatives>
</disp-formula>
where <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic></sub>represents the BOLD amplitude to the <italic toggle="yes">i</italic><sup><italic toggle="yes">th</italic></sup> stimulus, <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="467486v2_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> represents the corresponding model prediction, and <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="467486v2_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> is the mean response across stimuli. We can understand this metric as the extra uncertainty reduction brought by the model beyond describing the human data by its mean.</p></sec></sec><sec id="s3" hwp:id="sec-26"><label>3.</label><title hwp:id="title-31">Results</title><sec id="s3a" hwp:id="sec-27"><label>3.1.</label><title hwp:id="title-32">The fMRI BOLD responses to curved patterns are larger than to straight patterns</title><p hwp:id="p-68">We first consider an observation about fMRI responses to two classes of simple, gray-scale, band-passed, static images. For one class, the stimuli contain several curved contours, which we refer to throughout as snakes. For the other class, the stimuli contain several straight, parallel contours, which we refer to as gratings. The surprising observation is that for V1, V2 and V3, the fMRI responses are substantially larger for the snakes than for gratings (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref></bold>). The responses to the gratings, irrespective of density, are only about as high as the response to the sparsest snakes. We confirm this pattern with three additional fMRI data sets, which also show larger responses to snakes than gratings in V1, V2 and V3 (<bold><xref rid="figS2a" ref-type="fig" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Supplementary Figure S2</xref></bold>).</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><title hwp:id="title-33">FMRI responses are larger for curved patterns than for straight patterns.</title><p hwp:id="p-69">The mean fMRI responses within a visual area are plotted for V1, V2, and V3 in subject S1. For both curved stimuli (<italic toggle="yes">snakes</italic>, dark bars) and straight stimuli (<italic toggle="yes">gratings</italic>, light bars), data are plotted in order of increasing texture density from left to right. Examples of the 5 densities are shown above. Error bars are the standard deviation of the mean, bootstrapped across fMRI runs. Responses are larger for the <italic toggle="yes">snakes</italic> than <italic toggle="yes">gratings</italic>. The same effect is also observed for subjects S2-S4 (<xref ref-type="fig" rid="figS2a" hwp:id="xref-fig-12-2" hwp:rel-id="F12">Supplementary Figure 2</xref>). Bottom panel: For comparison, we replot data from intracranial recordings (ECoG) from <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Hermes et al 2019</xref>, which also show larger responses for <italic toggle="yes">snakes</italic> than for <italic toggle="yes">gratings</italic>. The full set of stimuli is shown in <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Supplementary Figure 1</xref>. Data plotted from the function <italic toggle="yes">s4_visualize(‘<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">figure3</xref>’)</italic> in the GitHub code repository.</p></caption><graphic xlink:href="467486v2_fig3" position="float" orientation="portrait" hwp:id="graphic-15"/></fig><p hwp:id="p-70">To check whether this pattern of results is specific to fMRI, we replot published intracranial data from <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">Hermes et al (2019)</xref>, in which human subjects with ECoG recordings viewed a similar set of stimuli (<bold><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3</xref></bold>, bottom panel). The ECoG data, plotted as the percent increase in broadband power over baseline, show the same general effect as the fMRI data: The responses to snakes are much larger than to gratings, irrespective of texture density, indicating that the effect is not limited to the fMRI BOLD response. See <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-4" hwp:rel-id="ref-21">Hermes et al. (2019)</xref> for methodological details.</p><p hwp:id="p-71">In addition to the difference in the mean responses to snakes and gratings, there are also differences in the slope, meaning the increase in response amplitude as a function of texture density. From the sparsest to densest patterns, the responses rise from about 0.25% (V1) or 0.5% (V2 and V3) BOLD signal change to over 1%. The responses to the grating stimuli vary much less with density. For the ECoG data, the responses also rise much more as a function of density for the snakes than for the gratings.</p><p hwp:id="p-72">In the next four sections, we describe the four models that are fit to the data. For simplicity, for each model we consider, the parameters are fit to the aggregate (average) data within a visual area, rather than to each voxel individually. All of the stimuli are texture-like, meaning that they have similar properties across the whole image aperture, and for each stimulus class, nine different exemplars were shown per 3-s trial. The different exemplars have the same higher-order statistics but vary in the precise spatial distributions. Hence model computations, such as contrast energy, would result in similar values for spatially localized portions of the image (as one would compute for an individual voxel) and for the whole image (as we compute to model the aggregate response of a visual area). For this reason, we did not include model parameters for the spatial location or spatial extent of the receptive fields for individual voxels. We first describe models fit to the two classes of stimuli described above (gratings and snakes) that vary in either density or contrast, which we refer to as <italic toggle="yes">target stimuli</italic>. We then summarize fits to the larger set of stimuli viewed by the subjects.</p></sec><sec id="s3b" hwp:id="sec-28"><label>3.2.</label><title hwp:id="title-34">The larger response to snake stimuli is not captured in a simple energy model</title><p hwp:id="p-73">The larger response to snakes than to gratings is not predicted by a standard contrast energy model. The model computes the oriented contrast energy at the stimulus spatial frequency. The contrast energy is then summed across space and orientations to give a total contrast energy. Finally, this value is passed through a power-law nonlinearity to predict the percent BOLD responses. The power-law nonlinearity acts similarly to a divisive normalization computation in which the normalization pool is shared across units (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-3" hwp:rel-id="ref-25">Kay et al., 2013b</xref>, section “Relationship to Divisive Normalization”).</p><p hwp:id="p-74">The contrast energy model predicts that responses should increase with both stimulus contrast and with density of the pattern. It does not predict systematic differences between the two classes of stimuli (<bold><xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref></bold>). Specifically, it does not predict larger responses to snakes than to gratings, contrary to the data, nor does it predict a higher slope as a function of density or contrast. In fact, the cross-validated variance explained is negative for all 3 visual areas in data set 1, meaning that the model prediction is less accurate than it would have been if it simply predicted the mean response for all stimuli. In short, the contrast energy model provides a poor fit to the fMRI data in V1-V3 for these classes of stimuli. It is also a poor fit to the target stimuli in the other three data sets (<bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref></bold>, first row). This does not mean contrast energy models are always poor fits to fMRI responses in V1-V3. For example, when stimuli vary in how contrast energy is distributed across space, a contrast energy model can capture a lot of the variance in the responses across images, as shown by <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Kay et al (2008)</xref>.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4 xref-fig-4-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><title hwp:id="title-35">Contrast energy model does not account for V1-V3 responses to these stimuli.</title><p hwp:id="p-75">The mean fMRI responses from V1, V2, and V3 are shown for snakes and grating stimuli that vary in density and contrast for subject S1. The bar plots show the mean and standard error of the responses. The dots are the cross-validated predictions from the contrast energy model. Dark bars represent snake stimuli and light bars represent grating stimuli. Each group of stimuli is arranged in increasing order of either density or contrast. Data and model fits plotted from the function <italic toggle="yes">s4_visualize(‘<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">figure4</xref>’)</italic> in the code repository. The upper panel is a schematic representation of the contrast energy model. See <italic toggle="yes"><xref ref-type="fig" rid="figS2b" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Figure S2b</xref></italic> for fits to all 4 datasets.</p></caption><graphic xlink:href="467486v2_fig4" position="float" orientation="portrait" hwp:id="graphic-16"/></fig><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3 xref-table-wrap-1-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-5"><title hwp:id="title-36">Cross-validated variance explained for 4 models on <italic toggle="yes">target stimuli</italic>.</title><p hwp:id="p-76">The table shows the leave-one-out cross validated coefficient of determination (<italic toggle="yes">R</italic><sup>2</sup>) for four data sets in three visual areas. The number of fitted model parameters (degrees of freedom) is indicated in parentheses for each model type (column 1). The number of stimuli comprising the target set are 18 for data set 1 (DS1) and data set 2 (DS2) and 17 for data set 3 (DS3), data set (DS4). The bold-faced font indicates the model with the highest variance explained in that column.</p></caption><graphic xlink:href="467486v2_tbl1" position="float" orientation="portrait" hwp:id="graphic-17"/></table-wrap><p hwp:id="p-77">Because the contrast energy model failed to capture the effect of interest, we considered two alternative models, a Second-order contrast model (SOC) that has been previously fit to both fMRI data (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-11" hwp:rel-id="ref-26">Kay et al., 2013c</xref>) and ECoG data (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-5" hwp:rel-id="ref-21">Hermes et al., 2019</xref>), and a contrast energy model with orientation tuned surround suppression. A preliminary implementation of the Orientation-tuned surround model was presented as a conference abstract (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">Olsson et al., 2015</xref>).</p></sec><sec id="s3c" hwp:id="sec-29"><label>3.3.</label><title hwp:id="title-37">The larger response to snake stimuli is not captured by a second-order contrast model</title><p hwp:id="p-78">The SOC model is a two-stage cascade model developed by <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-12" hwp:rel-id="ref-26">Kay et al (2013c)</xref>, which computes the contrast energy in the image (first stage), and then the contrast within the contrast image (second stage). The contrast in the contrast image is called second-order contrast, which gives the model its name. The motivation for incorporating a second-order contrast term is that the observed BOLD response is sometimes greater for a stimulus where the contrast energy is distributed heterogeneously compared to a stimulus with a similar or greater total contrast energy distributed more homogeneously across the image. Second-order contrast computations are also a well-established component of many texture models (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Landy and Graham, 2004</xref>; <xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Larsson et al., 2006</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Shooner et al., 2015</xref>). As with all the models in this paper, we did not fit spatially local receptive fields to each voxel, so we compute both first-order contrast energy and second-order contrast over the whole stimulus. This required a modification from the published SOC model.</p><p hwp:id="p-79">The SOC model predicts a similar BOLD amplitude for snakes and gratings, thereby failing to account for the observation about the two stimulus classes (<bold><xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref></bold>). While the SOC model is sensitive to heterogeneity in contrast energy across space, it is not sensitive to heterogeneity across orientations. This is because the non-linearities computed in the second stage come after energy is pooled across orientation channels in the first stage. Therefore, the output does not depend on whether the contrast energy is concentrated in one orientation channel as in the gratings, or spread across many channels as in the snakes.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-6"><title hwp:id="title-38">The Second-order contrast model does not account for V1-V3 responses to these stimuli.</title><p hwp:id="p-80">The mean fMRI responses from V1, V2, and V3 are replotted from <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4</xref>. The blue dots are the cross-validated predictions from the Second-order contrast model. Data and model fits plotted from the function <italic toggle="yes">s4_visualize(‘<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">figure5</xref>’)</italic> in the code repository. See <italic toggle="yes"><xref ref-type="fig" rid="figS2c" hwp:id="xref-fig-14-1" hwp:rel-id="F14">Figure S2c</xref></italic> for fits to all 4 datasets.</p></caption><graphic xlink:href="467486v2_fig5" position="float" orientation="portrait" hwp:id="graphic-18"/></fig><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-7"><title hwp:id="title-39">An Orientation-tuned surround model does not account for V1-V3 responses to these stimuli.</title><p hwp:id="p-81">The mean fMRI responses from V1, V2, and V3 are replotted from <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4</xref>. Z = FE. The yellow dots are the cross-validated predictions from the Orientation-tuned surround model. Data and model fits plotted from the function <italic toggle="yes">s4_visualize(‘<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-1" hwp:rel-id="F6">figure6</xref>’)</italic> in the code repository. See <italic toggle="yes"><xref ref-type="fig" rid="figS2d" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Figure S2d</xref></italic> for fits to all 4 datasets.</p></caption><graphic xlink:href="467486v2_fig6" position="float" orientation="portrait" hwp:id="graphic-19"/></fig><p hwp:id="p-82">The SOC model responds differently to density variations than the contrast energy model. For the contrast energy model, greater density of gratings or snakes results in more contrast energy and hence a larger predicted output. For the SOC model, there is a tradeoff: as density increases, the first-order contrast increases while the second-order contrast can decrease. In the limit, a pure harmonic has no second-order contrast. Because of the tradeoff, for some stimuli and model parameters, the two effects approximately cancel, which is why the predictions are close to flat in V1-V3 for the SOC model predictions in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">figure 5</xref>. When density is held fixed, the SOC model predictions increase as the stimulus first-order contrast increases, similar to the contrast energy model.</p><p hwp:id="p-83">The SOC model’s failure to account for the greater response to snakes is confirmed by low variance explained in each of the four data sets with these target stimuli (<bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref></bold>, second row).</p></sec><sec id="s3d" hwp:id="sec-30"><label>3.4.</label><title hwp:id="title-40">The larger response to snake stimuli is not captured by an Orientation-tuned surround model</title><p hwp:id="p-84">The Second-order contrast model can be thought of as implementing surround suppression: Second-order contrast is low when both the center and surround of a receptive field are stimulated, and high when only the center is stimulated. The SOC model does not take into account the specific orientations stimulating the center or surround. Work from macaque electrophysiology, however, indicates that surround suppression is orientation-tuned. Specifically, <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Cavanaugh, Bair, and Movshon (2002)</xref> reported that the response of a neuron to a stimulus at its preferred orientation in its receptive field is suppressed more when the surrounding region contains contrast at the same orientation compared to contrast at different orientations. Because our grating stimuli have contrast energy concentrated at a single orientation, and the snake stimuli do not, one might surmise that an Orientation-tuned surround model would show greater suppression for the gratings, where the RF centers and surrounds will have matched orientations, than for the snakes, where the orientations are more likely to differ between center and surround. If so, this could then account for our observed effect.</p><p hwp:id="p-85">To implement an Orientation-tuned surround model, we use the same first-stage contrast energy calculation as used in both the contrast energy model and the SOC model. We then normalize the contrast energy at each location and orientation by the outputs across orientations at the same location (also called cross-orientation suppression), and across nearby locations at the same orientation (orientation-tuned surround). The normalized energy images are then combined (summed over space and orientations) and passed through a nonlinearity power law, as in the contrast energy model.</p><p hwp:id="p-86">The OTS model is slightly improved compared to the SOC model for these data, but nonetheless it fails to capture the large and systematic differences in response amplitude between gratings and snakes. In V2 and V3, the model underpredicts the responses to snakes and overpredicts the responses to gratings, similar to the contrast energy model and the SOC model. Across the 4 stimulus sets, its performance on these target stimuli is better than the first two models tested, but it still has low accuracy, especially for V2 and V3 (R<sup>2</sup> ranging from 0.033 to 0.414; <bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Table 1</xref></bold>, third row).</p></sec><sec id="s3e" hwp:id="sec-31" hwp:rev-id="xref-sec-31-1"><label>3.5.</label><title hwp:id="title-41">Normalization By Orientation Anisotropy Model accounts for the larger response to snake stimuli</title><p hwp:id="p-87">The failures of the three models described above to account for the responses to the target stimuli motivated us to develop a new model. The reduced response to the gratings suggests the possibility that when the contrast energy is concentrated in a single orientation channel, there is greater normalization. We implemented this possibility in a model, in which the contrast energy is normalized by the variance across the outputs of the orientation channels, rather than the sum of the outputs. We refer to this as the NOA model (<bold><xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7</xref></bold>).</p><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-8"><title hwp:id="title-42">The NOA model better accounts for the responses in V1-V3.</title><p hwp:id="p-88">The mean fMRI responses from V1, V2, and V3 are replotted from <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-5" hwp:rel-id="F4">Figure 4</xref>. The red dots are the cross-validated predictions from the normalization by orientation anisotropy (NOA) model. Data and model fits plotted from the function <italic toggle="yes">s4_visualize(‘<xref ref-type="fig" rid="fig7" hwp:id="xref-fig-7-2" hwp:rel-id="F7">figure7</xref>’)</italic> in the code repository. See <italic toggle="yes"><xref ref-type="fig" rid="figS2a" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Figure S2a</xref></italic> for fits to all 4 datasets.</p></caption><graphic xlink:href="467486v2_fig7" position="float" orientation="portrait" hwp:id="graphic-20"/></fig><p hwp:id="p-89">The NOA model does an excellent job accounting for the differences in the responses to gratings and snakes. In data set 1, the BOLD amplitude and the NOA model predictions are about one-third for the gratings compared to the snakes, both for the density and contrast stimulus manipulations, and for all three visual areas. In addition to capturing this difference in the means between the two stimulus classes, the model also captures the difference in slope. As the density or contrast increases, the model predicts steeper slopes for snakes than gratings. These patterns, both in the data and in the model fits, were found across the four data sets (<bold><xref rid="figS2a" ref-type="fig" hwp:id="xref-fig-12-4" hwp:rel-id="F12">Supplementary Figure 2</xref></bold>). The NOA model is more accurate than the other three models in all 12 cases (four datasets and three ROIs). The improvement over other models is modest for V1 and large for V2 and V3 (<bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-4" hwp:rel-id="T1">Table 1</xref></bold>).</p><p hwp:id="p-90">To summarize the greater response to snakes than data, we computed the ratio of snakes to gratings (<bold><xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref></bold>). For the contrast energy and SOC model predictions, this value is close to 1. For the data and the NOA model, it is about 2. For the OTS model, it is about half of the values in the data and in the NOA model.</p><fig id="fig8" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1 xref-fig-8-2 xref-fig-8-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-9"><title hwp:id="title-43">The NOA model accounts for the higher responses to snakes than gratings.</title><p hwp:id="p-91">For each visual area, V1-V3, we averaged the response to snakes and to gratings across stimuli in the target set and across the 4 data sets to compute the ratio of snakes to gratings: mean(snakes) / mean(gratings). We computed this value for each subject and plotted the average and standard error across the four subjects. In the data, the response to snakes is about double to gratings. This is matched in the NOA model predictions but not the predictions from other models. Data and model fits plotted from the function <italic toggle="yes">s4_visualize</italic>(‘<xref ref-type="fig" rid="fig8" hwp:id="xref-fig-8-2" hwp:rel-id="F8">figure8</xref>’) in the code repository.</p></caption><graphic xlink:href="467486v2_fig8" position="float" orientation="portrait" hwp:id="graphic-21"/></fig></sec><sec id="s3f" hwp:id="sec-32"><label>3.6.</label><title hwp:id="title-44">The NOA Model accurately predicts responses to a wide range of stimuli</title><p hwp:id="p-92">The greater accuracy for the NOA model was not due to having more free parameters than the other models. The SOC, OTS, and NOA all have three free parameters, and the contrast energy model has two. Moreover, the prediction accuracy was summarized by <italic toggle="yes">n</italic>-fold cross validation, so that extra parameters do not necessarily lead to more accurate predictions. Nonetheless, because the NOA model was motivated to explain these data, it is important to validate it on other stimuli. We refit to model the full data sets shown to our four subjects, which consisted of 50 (data set 1), 48 (data set 2) and 39 (data set 3, data set 4) stimuli, spanning a variety of texture types. In addition to the snakes and gratings, there are textures we refer to as noise bars, waves, dust, and chess (<bold><xref rid="figS1" ref-type="fig" hwp:id="xref-fig-11-2" hwp:rel-id="F11">Supplementary Figure 1</xref></bold>).</p><p hwp:id="p-93">Across the larger stimulus sets, the NOA model generalizes well. For data set 1, it explains between 48% and 51% of the variance in V1-V3 (<bold><xref rid="fig9" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure 9</xref></bold>). The fits to the larger stimulus set, like the fits to the target stimuli alone, capture the observation about the two stimulus classes, meaning a larger predicted response for snakes than gratings. It also accurately predicts lower responses for waves than noise bars and snakes, which is a large effect in V1. One failure of the NOA model is that it predicts increasing response amplitude with texture density for the noise bars, whereas the observed responses do not increase with density. Across ROIs and data sets, the NOA model performs well, with R<sup>2</sup> ranging from 0.402 to 0.691 (<bold><xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2</xref></bold>, bottom row). Across the 4 models, it accounts for the highest variance in 4 of 4 V2 data sets and in 3 of 4 V3 data sets. It does not perform quite as well as the OTS model in V1, though it outperforms the contrast energy model and SOC.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-10"><title hwp:id="title-45">Cross-validated variance explained for 4 models on the larger stimulus set.</title><p hwp:id="p-94">The table is organized in the same way as Table 1, differing only in the number of stimuli used to fit the models (50 for data set 1; 48 for data set 2; 39 for data set 3, data set 4).</p></caption><graphic xlink:href="467486v2_tbl2" position="float" orientation="portrait" hwp:id="graphic-22"/></table-wrap><fig id="fig9" position="float" fig-type="figure" orientation="portrait" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">fig9</object-id><label>Figure 9.</label><caption hwp:id="caption-11"><title hwp:id="title-46">The NOA model fits a wide variety of stimuli.</title><p hwp:id="p-95">The mean fMRI responses from V1, V2, and V3 are shown for the full set of stimuli from data set 1 (See <xref ref-type="fig" rid="figS1" hwp:id="xref-fig-11-3" hwp:rel-id="F11">Supplementary Figure 1</xref> for images). The data for snakes and gratings are replotted from <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">figures 3</xref>-<xref ref-type="fig" rid="fig6" hwp:id="xref-fig-6-2" hwp:rel-id="F6">6</xref>, and again shown in dark and light gray. An even lighter gray is used for all other stimulus classes. The red dots are the cross-validated predictions from the NOA model. See <xref ref-type="fig" rid="figS3a" hwp:id="xref-fig-16-1" hwp:rel-id="F16">Supplementary Figures S3</xref>-<xref ref-type="fig" rid="figS6a" hwp:id="xref-fig-27-1" hwp:rel-id="F27">S6</xref> for similar plots for data sets 2-4. Data and model fits plotted from the function <italic toggle="yes">s4_visualize</italic>(‘<xref ref-type="fig" rid="fig9" hwp:id="xref-fig-9-2" hwp:rel-id="F9">figure9</xref>’) in the code repository.</p></caption><graphic xlink:href="467486v2_fig9" position="float" orientation="portrait" hwp:id="graphic-23"/></fig></sec></sec><sec id="s4" hwp:id="sec-33"><label>4.</label><title hwp:id="title-47">Discussion</title><sec id="s4a" hwp:id="sec-34"><label>4.1</label><title hwp:id="title-48">Why do existing models fail to account for the larger response to snake stimuli?</title><p hwp:id="p-96">We began with the observation that the BOLD response is about twice as large for patterns with curved contours (snakes) than for similar stimuli with straight, parallel contours (gratings). The difference in the responses is not a peculiarity of the BOLD signal, as a similar pattern was also observed in human intracranial measures of the field potential (broadband power from ECoG electrodes). Three different models—a contrast energy model, a Second-order contrast model, and an Orientation-tuned surround model—all failed to predict the large differences between the responses to the two stimulus classes. Because the difference in response amplitude is large and systematic, it is unlikely to be explained by subtle confounds such as possible differences in contour density or contrast.</p><p hwp:id="p-97">The reason why the contrast energy and SOC models do not predict systematic differences for snakes and gratings is relatively straightforward. Both models pool contrast energy over space and orientation without an orientation-specific normalization or other orientation specific non-linearity. If two images have the same total contrast energy, the contrast energy model output will be about the same. If they also have the same amount of second-order contrast (defined as spatial variation in contrast energy pooled over orientation), then the SOC output will also be about the same. Neither model is very sensitive to how the contrast energy is distributed across orientation channels.</p><p hwp:id="p-98">We were more surprised by the failure of the OTS model to predict a large difference between the two stimulus types. The implementation of this model was motivated in part from electrophysiology results showing that in V1, surround suppressive fields tend to be elongated along the direction of the RF center’s preferred orientation, and tend to be tuned to orientations close to the RF center’s preferred orientation (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Cavanaugh et al., 2002</xref>). Psychophysical experiments also show interactions between a target and surround that depend on matched orientation (e.g., <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Solomon et al., 1993</xref>). But the model was also motivated by the intuition that if the surround has orientation tuning matched to the center, then stimuli which are elongated in one direction (like gratings) will elicit strong surround suppression and hence small responses. In other words, we implemented the model specifically to account for the larger response to snakes than gratings. Nonetheless, the model generally does not do this: The results show that the OTS model predicts only a small difference in response amplitude between the snakes and gratings. This appears to be due, in part, to the fact that in this model, the driven responses (numerator) and suppressive responses (denominator) tend to be correlated, such that normalization reduces the large responses at the preferred orientation while effectively enhancing response at other orientations. The result is that the total response, pooled across orientations and space, is not reduced much for gratings compared to snakes. And in fact, even this modest difference, evident when fit to the target stimuli (<bold><xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6</xref></bold>), went away when fit to the larger stimulus sets (<bold><xref rid="figS6b" ref-type="fig" hwp:id="xref-fig-28-1" hwp:rel-id="F28">Supplementary figure 6</xref></bold>). As part of our code repository, we include scripts that sweep out model parameters and visualize the intermediate outputs (responses for each orientation channel), showing that no combination of parameters leads to a very large difference between the stimulus classes (<italic toggle="yes">s4_visualize(‘otsexplore’))</italic>.</p></sec><sec id="s4b" hwp:id="sec-35" hwp:rev-id="xref-sec-35-1"><label>4.2</label><title hwp:id="title-49">Why does the NOA model work? Model behavior</title><p hwp:id="p-99">The NOA model predicts larger outputs for snakes than gratings, even when the contrast energy is approximately matched. This is due to how the normalization is computed. The grating stimuli elicit large outputs in the orientation channels that are matched to the stimulus, moderate outputs in adjacent orientation channels, and little to no response in other channels. As a result, there is high anisotropy (variance across channel outputs). The outputs for the snake stimuli are similar across orientation channels, meaning less anisotropy. Because the NOA model normalizes (divides) the contrast energy by variance across orientation channels, the stimulus with higher variance receives more suppression. We illustrate this idea by showing the channel outputs, pooled across space, for several stimuli (<bold><xref rid="fig10" ref-type="fig" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Figure 10</xref>)</bold>. The contrast energy in the grating has relatively high orientation variance across channel outputs. Consequently, the normalized outputs are greatly diminished (first row, right vs left). A snake stimulus has similar contrast energy summed across different orientation channels (0.80 for the snake compared to 0.73 for the grating), but the variance is about 50 times lower, hence there is little normalization. This means that for the snake stimulus, the normalized and unnormalized outputs are nearly the same. After normalization, the summed output is about 3 times higher for the curve than the grating.</p><fig id="fig10" position="float" fig-type="figure" orientation="portrait" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIG10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">fig10</object-id><label>Figure 10.</label><caption hwp:id="caption-12"><title hwp:id="title-50">Model behavior.</title><p hwp:id="p-100">We illustrate the behavior of a simplified version of the NOA model for 4 stimuli. For a high contrast grating (top row), the contrast energy is high in channels where the channel orientation matches (or is close to) the stimulus orientation, and low for other channels. As a result, there is high anisotropy, and the variance in the channel outputs is high. This results in strong normalization, reducing the outputs substantially in all channels (right column). For all other stimuli, the anisotropy is lower, either because the stimulus has power at many orientations (snakes) or because the overall contrast is low. As a result, there is little effect of normalization in the other three rows (summed output across orientations is similar before and after normalization). For simplicity, we omitted the squaring on <italic toggle="yes">E</italic> in the simulations, as well as the exponent on the overall output. Data and model fits plotted from the function <italic toggle="yes">s4_visualize</italic>(‘<xref ref-type="fig" rid="fig10" hwp:id="xref-fig-10-2" hwp:rel-id="F10">figure10</xref>’) in the code repository.</p></caption><graphic xlink:href="467486v2_fig10" position="float" orientation="portrait" hwp:id="graphic-24"/></fig><p hwp:id="p-101">The normalization in the model also makes the surprising prediction that the response should increase more with contrast for snakes than for gratings. Without normalization or an output non-linearity, the contrast response function would be linear for any stimulus. But with normalization, the shape of the contrast response function is stimulus dependent. For the grating stimulus, at high contrast the variance is high and the normalization is high, whereas at low contrast, the variance is low and there is little suppression. Because there is strong normalization for only one of the two stimuli, the difference in the outputs is compressed. Specifically, before normalization, the response difference between a high and low contrast stimulus is about 10x (row 1 vs row 3). After normalization, the difference is only about 4x. For the snakes, the variance is low irrespective of contrast. Hence there is little normalization, and the contrast response function has a greater slope. This general pattern is borne out in the model fits and in the data. For example, in <xref rid="figS2a" ref-type="fig" hwp:id="xref-fig-12-5" hwp:rel-id="F12">figure S2a</xref>, for the contrast stimuli, the data and the predicted snakes are steeper for the curved stimuli and more compressive for the gratings. The same logic also applies to the variation in density: for the grating stimuli, the predicted responses are compressed, meaning there is relatively small difference in the predictions for dense and sparse gratings, but large differences in the predictions for dense and sparse snake stimuli. We see this in the data as well as the model predictions (<bold><xref rid="figS2a" ref-type="fig" hwp:id="xref-fig-12-6" hwp:rel-id="F12">Figure S2a</xref></bold>). The other models do not make this prediction.</p><p hwp:id="p-102">Interestingly, the denominator in the NOA model (the normalizer) is almost the same as the numerator of an orientation variance model used to predict the amplitude of gamma oscillations measured with ECoG electrodes in human subjects (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-6" hwp:rel-id="ref-21">Hermes et al., 2019</xref>). The fact that the same term is used in the numerator to predict gamma oscillations and the denominator to predict BOLD is consistent with the empirical observation that many of the stimuli that are most effective for driving large gamma oscillations (high contrast luminance gratings) are relatively ineffective for eliciting BOLD signals (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Butler et al., 2017</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Hermes et al., 2017b</xref>), and multiunit action potentials (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Hermes et al., 2017a</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Ray and Maunsell, 2011</xref>). The link may be that gamma oscillations, rather than being the fundamental mechanism for perception and long-range cortical communication (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Engel and Singer, 2001</xref>; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Fries, 2005</xref>), are rather a result of the normalization process (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-7" hwp:rel-id="ref-21">Hermes et al., 2019</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Ray et al., 2013</xref>).</p></sec><sec id="s4c" hwp:id="sec-36"><label>4.3</label><title hwp:id="title-51">The NOA model has the largest advantage in extrastriate maps</title><p hwp:id="p-103">The normalization by orientation anisotropy model showed the largest advantage over other models for V2 and V3. There seem to be two reasons for this. First, the difference in BOLD response between snakes and gratings is larger in V2 and V3 than in V1, and only the NOA model predicts this difference (<bold><xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-3" hwp:rel-id="F8">Figure 8</xref></bold>). However, this is only a small effect, since all three areas show larger responses to snakes than gratings. The second reason is due to an interaction between the contrast response function and stimulus class. In V2 and V3, the contrast response function is relatively flat for gratings, but steep for other stimulus classes (snakes as well as circular patterns and plaids). As described in <xref ref-type="sec" rid="s3e" hwp:id="xref-sec-31-1" hwp:rel-id="sec-31">sections 3.5</xref> and <xref ref-type="sec" rid="s4b" hwp:id="xref-sec-35-1" hwp:rel-id="sec-35">4.2</xref>, the NOA model predicts a flatter contrast response function for gratings than for other stimulus classes, matching the data in V2 and V3. The other three models, however, do not show a strong interaction between stimulus class and the steepness of the contrast response function and hence cannot capture the pattern of data in V2 and V3.</p><p hwp:id="p-104">The fact that the NOA model shows a large advantage for extrastriate areas, whereas the contrast energy model and the other two models do not, is consistent with findings from electrophysiology and computational modeling of behavior. In particular, a series of studies suggests that extrastriate maps, especially V2, are more sensitive to higher-order image statistics than to contrast energy per se (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Movshon and Simoncelli, 2015</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">Okazawa et al., 2017</xref>). The NOA model is also sensitive to higher-order image statistics, namely orientation anisotropy (variance across orientation channel outputs). The NOA model differs from these physiology models in that it predicts population data (MRI voxels containing many thousands of neurons), and it is image-computable; the physiology models predict which image pairs will produce similar responses, but not what the response will be for a single image. More recent V2 models for single units have begun to make forward predictions for response of V2 neurons, and it will be interesting to see whether they bear any relation to our NOA model (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Oleskiw et al., 2021</xref>).</p><p hwp:id="p-105">We consider one way that our NOA model might be linked to the V2 models proposed by Simoncelli and colleagues. Suppose the V1 population computes contrast energy localized in orientation and space. (For simplicity, we ignore spatial frequency, as we did here experimentally by band-pass filtering our stimuli.) We then suppose that the V2 cells compute various weighted sums of the V1 outputs. Specifically, we assume that, for each spatial location, the weights among V2 cells form a Fourier basis set on the V1 outputs (across orientation). If these weights are arranged in pairs (similar to the odd and even V1 filters), and the outputs are squared and summed across phase (similar to the V1 energy model), then then the summed V2 population output will be proportional to the variance in the V1 response across orientation (David Heeger, personal communication). If this population output is the normalization pool for V2, then we get a model like NOA, which normalizes by the variance across orientation channels. Feedback to V1, or a related calculation in V1 might result in similar behavior even for V1.</p></sec><sec id="s4d" hwp:id="sec-37"><label>4.4</label><title hwp:id="title-52">Are standard V1 models missing something important?</title><p hwp:id="p-106">No model is complete. The standard normalized contrast energy model of V1, when fit with appropriate parameters, can capture substantial variance in V1 responses (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Carandini et al., 1997</xref>), but not all. Our results indicate that the model fails for at least some relatively simple stimulus classes, and that the failure can be large. But whether a more complex model is needed, such as a gated normalization model (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">Coen-Cagli et al., 2015</xref>) or the NOA model here, will depend on the stimulus set tested and the purpose of modeling. It is reasonably likely that the computations in the NOA model are more connected to computations in extrastriate visual maps than in V1, but may also influence the response in V1 via feedback.</p><p hwp:id="p-107">More generally, since the development of divisive normalization models in the early 1990s, there has not yet been a generally agreed up description of exactly what cell populations (and with what weights) contribute to normalization. Some attempts have been made from assuming efficient coding of natural images {Schwartz, 2001 #2320} or from fitting model parameters to neural data {Burg, 2021 #2980}. In this sense, a standard model of V1, with parameters set, that is downloadable and executable on arbitrary input images, does not yet really exist.</p><p hwp:id="p-108">The NOA model we presented makes some advance but also has some important limits, most notably the lack of spatial receptive fields, as well as a lack of sensitivity to second-order contrast. Hence it does not supersede other models, but rather provides a compact computational summary of response patterns that are not well captured by other models. Integrating better computational tools for validation (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Lerma-Usabiaga et al., 2020</xref>), model-based stimulus development (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Golan et al., 2020</xref>; <xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Wang and Simoncelli, 2008</xref>), high-quality standardized datasets (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Allen et al., 2021</xref>), and theory (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Simoncelli and Olshausen, 2001</xref>), may offer the best path toward more complete understanding of neural circuits in visual cortex.</p></sec></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-53">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Adelson E.H."><surname>Adelson</surname>, <given-names>E.H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bergen J.R."><surname>Bergen</surname>, <given-names>J.R.</given-names></string-name> (<year>1985</year>). <article-title hwp:id="article-title-2">Spatiotemporal energy models for the perception of motion</article-title>. <source hwp:id="source-1">J Opt Soc Am A</source> <volume>2</volume>, <fpage>284</fpage>–<lpage>299</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="other" citation-type="journal" ref:id="2021.11.06.467486v2.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Allen E.J."><surname>Allen</surname>, <given-names>E.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="St-Yves G."><surname>St-Yves</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu Y."><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Breedlove J.L."><surname>Breedlove</surname>, <given-names>J.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dowdle L.T."><surname>Dowdle</surname>, <given-names>L.T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Caron B."><surname>Caron</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pestilli F."><surname>Pestilli</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Charest I."><surname>Charest</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hutchinson J.B."><surname>Hutchinson</surname>, <given-names>J.B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-3">A massive 7T fMRI dataset to bridge cognitive and computational neuroscience</article-title>. <source hwp:id="source-2">bioRxiv</source>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Brouwer G.J."><surname>Brouwer</surname>, <given-names>G.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-4">Cross-orientation suppression in human visual cortex</article-title>. <source hwp:id="source-3">J Neurophysiol</source> <volume>106</volume>, <fpage>2108</fpage>–<lpage>2119</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Butler R."><surname>Butler</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bernier P.M."><surname>Bernier</surname>, <given-names>P.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lefebvre J."><surname>Lefebvre</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gilbert G."><surname>Gilbert</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Whittingstall K."><surname>Whittingstall</surname>, <given-names>K.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-5">Decorrelated Input Dissociates Narrow Band gamma Power and BOLD in Human Visual Cortex</article-title>. <source hwp:id="source-4">J Neurosci</source> <volume>37</volume>, <fpage>5408</fpage>–<lpage>5418</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Carandini M."><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Demb J.B."><surname>Demb</surname>, <given-names>J.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mante V."><surname>Mante</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tolhurst D.J."><surname>Tolhurst</surname>, <given-names>D.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dan Y."><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Olshausen B.A."><surname>Olshausen</surname>, <given-names>B.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Rust N.C."><surname>Rust</surname>, <given-names>N.C.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-6">Do we know what the early visual system does?</article-title> <source hwp:id="source-5">J Neurosci</source> <volume>25</volume>, <fpage>10577</fpage>–<lpage>10597</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Carandini M."><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name> (<year>1997</year>). <article-title hwp:id="article-title-7">Linearity and normalization in simple cells of the macaque primary visual cortex</article-title>. <source hwp:id="source-6">J Neurosci</source> <volume>17</volume>, <fpage>8621</fpage>–<lpage>8644</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Cavanaugh J.R."><surname>Cavanaugh</surname>, <given-names>J.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bair W."><surname>Bair</surname>, <given-names>W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-8">Selectivity and spatial distribution of signals from the receptive field surround in macaque V1 neurons</article-title>. <source hwp:id="source-7">J Neurophysiol</source> <volume>88</volume>, <fpage>2547</fpage>–<lpage>2556</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Coen-Cagli R."><surname>Coen-Cagli</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kohn A."><surname>Kohn</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Schwartz O."><surname>Schwartz</surname>, <given-names>O.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-9">Flexible gating of contextual influences in natural vision</article-title>. <source hwp:id="source-8">Nat Neurosci</source> <volume>18</volume>, <fpage>1648</fpage>–<lpage>1655</lpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="David S.V."><surname>David</surname>, <given-names>S.V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vinje W.E."><surname>Vinje</surname>, <given-names>W.E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-10">Natural stimulus statistics alter the receptive field structure of v1 neurons</article-title>. <source hwp:id="source-9">J Neurosci</source> <volume>24</volume>, <fpage>6991</fpage>–<lpage>7006</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Engel A.K."><surname>Engel</surname>, <given-names>A.K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Singer W."><surname>Singer</surname>, <given-names>W.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-11">Temporal binding and the neural correlates of sensory awareness</article-title>. <source hwp:id="source-10">Trends Cogn Sci</source> <volume>5</volume>, <fpage>16</fpage>–<lpage>25</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Engel S.A."><surname>Engel</surname>, <given-names>S.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glover G.H."><surname>Glover</surname>, <given-names>G.H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>1997</year>). <article-title hwp:id="article-title-12">Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title>. <source hwp:id="source-11">Cerebral Cortex</source> <volume>7</volume>, <fpage>181</fpage>–<lpage>192</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Freeman J."><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-13">Metamers of the ventral stream</article-title>. <source hwp:id="source-12">Nat Neurosci</source> <volume>14</volume>, <fpage>1195</fpage>–<lpage>1201</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Freeman J."><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ziemba C.M."><surname>Ziemba</surname>, <given-names>C.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-14">A functional and perceptual signature of the second visual area in primates</article-title>. <source hwp:id="source-13">Nat Neurosci</source> <volume>16</volume>, <fpage>974</fpage>–<lpage>981</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Fries P."><surname>Fries</surname>, <given-names>P.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-15">A mechanism for cognitive dynamics: neuronal communication through neuronal coherence</article-title>. <source hwp:id="source-14">Trends Cogn Sci</source> <volume>9</volume>, <fpage>474</fpage>–<lpage>480</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Golan T."><surname>Golan</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Raju P.C."><surname>Raju</surname>, <given-names>P.C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-16">Controversial stimuli: Pitting neural networks against each other as models of human cognition</article-title>. <source hwp:id="source-15">Proc Natl Acad Sci U S A</source> <volume>117</volume>, <fpage>29330</fpage>–<lpage>29337</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name> (<year>1992a</year>). <article-title hwp:id="article-title-17">Half-squaring in responses of cat striate cells</article-title>. <source hwp:id="source-16">Vis Neurosci</source> <volume>9</volume>, <fpage>427</fpage>–<lpage>443</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name> (<year>1992b</year>). <article-title hwp:id="article-title-18">Normalization of cell responses in cat striate cortex</article-title>. <source hwp:id="source-17">Vis Neurosci</source> <volume>9</volume>, <fpage>181</fpage>–<lpage>197</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name> (<year>1993</year>). <article-title hwp:id="article-title-19">Modeling simple-cell direction selectivity with normalized, half-squared, linear operators</article-title>. <source hwp:id="source-18">J Neurophysiol</source> <volume>70</volume>, <fpage>1885</fpage>–<lpage>1898</lpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Hermes D."><surname>Hermes</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kasteleijn-Nolst Trenite D.G.A."><surname>Kasteleijn-Nolst Trenite</surname>, <given-names>D.G.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2017a</year>). <article-title hwp:id="article-title-20">Gamma oscillations and photosensitive epilepsy</article-title>. <source hwp:id="source-19">Curr Biol</source> <volume>27</volume>, <fpage>R336</fpage>–<lpage>R338</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Hermes D."><surname>Hermes</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nguyen M."><surname>Nguyen</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2017b</year>). <article-title hwp:id="article-title-21">Neuronal synchrony and the relation between the blood-oxygen-level dependent response and the local field potential</article-title>. <source hwp:id="source-20">PLoS Biol</source> <volume>15</volume>, <fpage>e2001461</fpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3 xref-ref-21-4 xref-ref-21-5 xref-ref-21-6 xref-ref-21-7"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Hermes D."><surname>Hermes</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Petridou N."><surname>Petridou</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-22">An image-computable model for the stimulus selectivity of gamma oscillations</article-title>. <source hwp:id="source-21">Elife</source> <volume>8</volume>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Hubel D.H."><surname>Hubel</surname>, <given-names>D.H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wiesel T.N."><surname>Wiesel</surname>, <given-names>T.N.</given-names></string-name> (<year>1962</year>). <article-title hwp:id="article-title-23">Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source hwp:id="source-22">J Physiol</source> <volume>160</volume>, <fpage>106</fpage>–<lpage>154</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prenger R.J."><surname>Prenger</surname>, <given-names>R.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-24">Identifying natural images from human brain activity</article-title>. <source hwp:id="source-23">Nature</source> <volume>452</volume>, <fpage>352</fpage>–<lpage>355</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rokem A."><surname>Rokem</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dougherty R.F."><surname>Dougherty</surname>, <given-names>R.F.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>2013a</year>). <article-title hwp:id="article-title-25">GLMdenoise: a fast, automated technique for denoising task-based fMRI data</article-title>. <source hwp:id="source-24">Front Neurosci</source> <volume>7</volume>, <fpage>247</fpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2 xref-ref-25-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mezer A."><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>2013b</year>). <article-title hwp:id="article-title-26">Compressive spatial summation in human visual cortex</article-title>. <source hwp:id="source-25">J Neurophysiol</source> <volume>110</volume>, <fpage>481</fpage>–<lpage>494</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2 xref-ref-26-3 xref-ref-26-4 xref-ref-26-5 xref-ref-26-6 xref-ref-26-7 xref-ref-26-8 xref-ref-26-9 xref-ref-26-10 xref-ref-26-11 xref-ref-26-12"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rokem A."><surname>Rokem</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mezer A."><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>2013c</year>). <article-title hwp:id="article-title-27">A two-stage cascade model of BOLD responses in human visual cortex</article-title>. <source hwp:id="source-26">PLoS Comput Biol</source> <volume>9</volume>, <fpage>e1003079</fpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Landy M.S."><surname>Landy</surname>, <given-names>M.S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Graham N."><surname>Graham</surname>, <given-names>N.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-28">Visual Perception of Texture</article-title>. <source hwp:id="source-27">The visual neurosciences</source>, <volume>1106</volume>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Larsson J."><surname>Larsson</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Landy M.S."><surname>Landy</surname>, <given-names>M.S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Heeger D.J."><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-29">Orientation-selective adaptation to first-and second-order patterns in human visual cortex</article-title>. <source hwp:id="source-28">J Neurophysiol</source> <volume>95</volume>, <fpage>862</fpage>–<lpage>881</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Lerma-Usabiaga G."><surname>Lerma-Usabiaga</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benson N."><surname>Benson</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-30">A validation framework for neuroimaging software: The case of population receptive fields</article-title>. <source hwp:id="source-29">PLoS Comput Biol</source> <volume>16</volume>, <fpage>e1007924</fpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Mannion D.J."><surname>Mannion</surname>, <given-names>D.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kersten D.J."><surname>Kersten</surname>, <given-names>D.J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Olman C.A."><surname>Olman</surname>, <given-names>C.A.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-31">Scene coherence can affect the local response to natural images in human V1</article-title>. <source hwp:id="source-30">Eur J Neurosci</source> <volume>42</volume>, <fpage>2895</fpage>–<lpage>2903</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><citation publication-type="other" citation-type="journal" ref:id="2021.11.06.467486v2.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-32">Representation of Naturalistic Image Structure in the Primate Visual Cortex. Cold Spring Harb</article-title> <source hwp:id="source-31">Symp Quant Biol</source>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Okazawa G."><surname>Okazawa</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tajima S."><surname>Tajima</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Komatsu H."><surname>Komatsu</surname>, <given-names>H.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-33">Gradual Development of Visual Texture-Selective Properties Between Macaque Areas V2 and V4</article-title>. <source hwp:id="source-32">Cereb Cortex</source> <volume>27</volume>, <fpage>4867</fpage>–<lpage>4880</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Oleskiw T.D."><surname>Oleskiw</surname>, <given-names>T.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Diaz-Pacheco R.R."><surname>Diaz-Pacheco</surname>, <given-names>R.R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-34">A two-stage model of V2 demonstrates efficient higher-order feature representation</article-title>. <source hwp:id="source-33">Journal of Vision</source> <volume>21</volume>, <fpage>2654</fpage>–<lpage>2654</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Olshausen B.A."><surname>Olshausen</surname>, <given-names>B.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Field D.J."><surname>Field</surname>, <given-names>D.J.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-35">How close are we to understanding v1?</article-title> <source hwp:id="source-34">Neural Comput</source> <volume>17</volume>, <fpage>1665</fpage>–<lpage>1699</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Olsson C."><surname>Olsson</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kay K."><surname>Kay</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-36">Orientation-tuned surround suppression improves computational models of human visual cortex</article-title>. <source hwp:id="source-35">Journal of Vision</source> <volume>15</volume>, <fpage>1001</fpage>–<lpage>1001</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="other" citation-type="journal" ref:id="2021.11.06.467486v2.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Pollen D.A."><surname>Pollen</surname>, <given-names>D.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Ronner S.F."><surname>Ronner</surname>, <given-names>S.F.</given-names></string-name> (<year>1983</year>). <article-title hwp:id="article-title-37">Visual cortical neurons as localized spatial frequency filters</article-title>. <source hwp:id="source-36">IEEE Transactions on Systems, Man, and Cybernetics</source>, <fpage>907</fpage>–<lpage>916</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Qiu C."><surname>Qiu</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burton P.C."><surname>Burton</surname>, <given-names>P.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kersten D."><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Olman C.A."><surname>Olman</surname>, <given-names>C.A.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-38">Responses in early visual areas to contour integration are context dependent</article-title>. <source hwp:id="source-37">J Vis</source> <volume>16</volume>, <fpage>19</fpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Ray S."><surname>Ray</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Maunsell J.H."><surname>Maunsell</surname>, <given-names>J.H.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-39">Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title>. <source hwp:id="source-38">PLoS Biol</source> <volume>9</volume>, <fpage>e1000610</fpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Ray S."><surname>Ray</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ni A.M."><surname>Ni</surname>, <given-names>A.M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Maunsell J.H."><surname>Maunsell</surname>, <given-names>J.H.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-40">Strength of gamma rhythm depends on normalization</article-title>. <source hwp:id="source-39">PLoS Biol</source> <volume>11</volume>, <fpage>e1001477</fpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Rust N.C."><surname>Rust</surname>, <given-names>N.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schwartz O."><surname>Schwartz</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-41">Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source hwp:id="source-40">Neuron</source> <volume>46</volume>, <fpage>945</fpage>–<lpage>956</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Sereno M.I."><surname>Sereno</surname>, <given-names>M.I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dale A.M."><surname>Dale</surname>, <given-names>A.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reppas J.B."><surname>Reppas</surname>, <given-names>J.B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kwong K.K."><surname>Kwong</surname>, <given-names>K.K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belliveau J.W."><surname>Belliveau</surname>, <given-names>J.W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brady T.J."><surname>Brady</surname>, <given-names>T.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rosen B.R."><surname>Rosen</surname>, <given-names>B.R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Tootell R.B."><surname>Tootell</surname>, <given-names>R.B.</given-names></string-name> (<year>1995</year>). <article-title hwp:id="article-title-42">Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source hwp:id="source-41">Science</source> <volume>268</volume>, <fpage>889</fpage>–<lpage>893</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="other" citation-type="journal" ref:id="2021.11.06.467486v2.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Shooner C."><surname>Shooner</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hallum L.E."><surname>Hallum</surname>, <given-names>L.E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kumbhani R.D."><surname>Kumbhani</surname>, <given-names>R.D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ziemba C.M."><surname>Ziemba</surname>, <given-names>C.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garcia-Marin V."><surname>Garcia-Marin</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kelly J.G."><surname>Kelly</surname>, <given-names>J.G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Majaj N.J."><surname>Majaj</surname>, <given-names>N.J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Movshon J.A."><surname>Movshon</surname>, <given-names>J.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kiorpes L."><surname>Kiorpes</surname>, <given-names>L.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-43">Population representation of visual information in areas V1 and V2 of amblyopic macaques</article-title>. <source hwp:id="source-42">Vision Res</source>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Olshausen B.A."><surname>Olshausen</surname>, <given-names>B.A.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-44">Natural image statistics and neural representation</article-title>. <source hwp:id="source-43">Annu Rev Neurosci</source> <volume>24</volume>, <fpage>1193</fpage>–<lpage>1216</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Solomon J.A."><surname>Solomon</surname>, <given-names>J.A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sperling G."><surname>Sperling</surname>, <given-names>G.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Chubb C."><surname>Chubb</surname>, <given-names>C.</given-names></string-name> (<year>1993</year>). <article-title hwp:id="article-title-45">The lateral inhibition of perceived contrast is indifferent to on-center/off-center segregation, but specific to orientation</article-title>. <source hwp:id="source-44">Vision Res</source> <volume>33</volume>, <fpage>2671</fpage>–<lpage>2683</lpage>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Tang S."><surname>Tang</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee T.S."><surname>Lee</surname>, <given-names>T.S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li M."><surname>Li</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang Y."><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xu Y."><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu F."><surname>Liu</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Teo B."><surname>Teo</surname>, <given-names>B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Jiang H."><surname>Jiang</surname>, <given-names>H.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-46">Complex Pattern Selectivity in Macaque Primary Visual Cortex Revealed by Large-Scale Two-Photon Imaging</article-title>. <source hwp:id="source-45">Curr Biol</source> <volume>28</volume>, <fpage>38</fpage>–<lpage>48</lpage> e33.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Vinje W.E."><surname>Vinje</surname>, <given-names>W.E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Gallant J.L."><surname>Gallant</surname>, <given-names>J.L.</given-names></string-name> (<year>2000</year>). <article-title hwp:id="article-title-47">Sparse coding and decorrelation in primary visual cortex during natural vision</article-title>. <source hwp:id="source-46">Science</source> <volume>287</volume>, <fpage>1273</fpage>–<lpage>1276</lpage>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><citation publication-type="book" citation-type="book" ref:id="2021.11.06.467486v2.47" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>1995</year>). <source hwp:id="source-47">Foundations of vision</source> (<publisher-loc>Sunderland, Mass</publisher-loc>.: <publisher-name>Sinauer Associates</publisher-name>).</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-48">Imaging retinotopic maps in the human brain</article-title>. <source hwp:id="source-48">Vision Res</source> <volume>51</volume>, <fpage>718</fpage>–<lpage>737</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Wang Z."><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli E.P."><surname>Simoncelli</surname>, <given-names>E.P.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-49">Maximum differentiation (MAD) competition: a methodology for comparing computational models of perceptual quantities</article-title>. <source hwp:id="source-49">J Vis</source> <volume>8</volume>, <fpage>8 1</fpage>–<lpage>13</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foster B.L."><surname>Foster</surname>, <given-names>B.L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rauschecker A.M."><surname>Rauschecker</surname>, <given-names>A.M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Parvizi J."><surname>Parvizi</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Wandell B.A."><surname>Wandell</surname>, <given-names>B.A.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-50">Asynchronous broadband signals are the principal source of the BOLD response in human visual cortex</article-title>. <source hwp:id="source-50">Curr Biol</source> <volume>23</volume>, <fpage>1145</fpage>–<lpage>1153</lpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.11.06.467486v2.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Zhou J."><surname>Zhou</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benson N.C."><surname>Benson</surname>, <given-names>N.C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kay K.N."><surname>Kay</surname>, <given-names>K.N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Winawer J."><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-51">Compressive temporal summation in human visual cortex</article-title>. <source hwp:id="source-51">J Neurosci</source> <volume>38</volume>, <fpage>691</fpage>–<lpage>709</lpage>.</citation></ref></ref-list><sec id="s5" hwp:id="sec-38"><title hwp:id="title-54">Supplementary figures</title><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1 xref-table-wrap-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Table S1:</label><caption hwp:id="caption-13"><title hwp:id="title-55">Data set properties for all experiments</title></caption><graphic xlink:href="467486v2_tblS1" position="float" orientation="portrait" hwp:id="graphic-25"/></table-wrap><table-wrap id="tblS2" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2 xref-table-wrap-4-3 xref-table-wrap-4-4 xref-table-wrap-4-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBLS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tblS2</object-id><label>Table S2:</label><caption hwp:id="caption-14"><title hwp:id="title-56">Stimulus set properties for all experiments</title></caption><graphic xlink:href="467486v2_tblS2" position="float" orientation="portrait" hwp:id="graphic-26"/></table-wrap><table-wrap id="tblS3" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBLS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tblS3</object-id><label>Table S3:</label><caption hwp:id="caption-15"><title hwp:id="title-57">Stimulus set group for all experiments</title></caption><graphic xlink:href="467486v2_tblS3" position="float" orientation="portrait" hwp:id="graphic-27"/><graphic xlink:href="467486v2_tblS3a" position="float" orientation="portrait" hwp:id="graphic-28"/></table-wrap><table-wrap id="tblS4a" orientation="portrait" position="float" hwp:id="T6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBLS4A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tblS4a</object-id><label>TableS4a:</label><caption hwp:id="caption-16"><title hwp:id="title-58">Parameters for cross-validated fits for all stimuli</title></caption><graphic xlink:href="467486v2_tblS4a" position="float" orientation="portrait" hwp:id="graphic-29"/></table-wrap><table-wrap id="tblS4b" orientation="portrait" position="float" hwp:id="T7"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/TBLS4B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tblS4b</object-id><label>TableS4b:</label><caption hwp:id="caption-17"><title hwp:id="title-59">Parameters for cross-validated fits for target stimuli</title></caption><graphic xlink:href="467486v2_tblS4b" position="float" orientation="portrait" hwp:id="graphic-30"/></table-wrap><fig id="figS1" position="float" fig-type="figure" orientation="portrait" hwp:id="F11" hwp:rev-id="xref-fig-11-1 xref-fig-11-2 xref-fig-11-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1:</label><caption hwp:id="caption-18"><title hwp:id="title-60">Stimuli for data set 1</title></caption><graphic xlink:href="467486v2_figS1" position="float" orientation="portrait" hwp:id="graphic-31"/><graphic xlink:href="467486v2_figS1a" position="float" orientation="portrait" hwp:id="graphic-32"/></fig><fig id="figS2a" position="float" fig-type="figure" orientation="portrait" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3 xref-fig-12-4 xref-fig-12-5 xref-fig-12-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS2A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figS2a</object-id><label>Figure S2a:</label><caption hwp:id="caption-19"><title hwp:id="title-61">Responses and NOA model fits for target stimuli, all datasets</title></caption><graphic xlink:href="467486v2_figS2a" position="float" orientation="portrait" hwp:id="graphic-33"/></fig><fig id="figS2b" position="float" fig-type="figure" orientation="portrait" hwp:id="F13" hwp:rev-id="xref-fig-13-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS2B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figS2b</object-id><label>Figure S2b:</label><caption hwp:id="caption-20"><title hwp:id="title-62">Responses and contrast energy model fits for target stimuli, all datasets</title></caption><graphic xlink:href="467486v2_figS2b" position="float" orientation="portrait" hwp:id="graphic-34"/></fig><fig id="figS2c" position="float" fig-type="figure" orientation="portrait" hwp:id="F14" hwp:rev-id="xref-fig-14-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS2C</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figS2c</object-id><label>Figure S2c:</label><caption hwp:id="caption-21"><title hwp:id="title-63">Responses and Second-order contrast model fits for target stimuli, all datasets</title></caption><graphic xlink:href="467486v2_figS2c" position="float" orientation="portrait" hwp:id="graphic-35"/></fig><fig id="figS2d" position="float" fig-type="figure" orientation="portrait" hwp:id="F15" hwp:rev-id="xref-fig-15-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS2D</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figS2d</object-id><label>Figure S2d:</label><caption hwp:id="caption-22"><title hwp:id="title-64">Responses and Orientation-tuned surround model fits for target stimuli, all datasets</title></caption><graphic xlink:href="467486v2_figS2d" position="float" orientation="portrait" hwp:id="graphic-36"/></fig><fig id="figS3a" position="float" fig-type="figure" orientation="portrait" hwp:id="F16" hwp:rev-id="xref-fig-16-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS3A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F16</object-id><object-id pub-id-type="publisher-id">figS3a</object-id><label>Figure S3a:</label><caption hwp:id="caption-23"><title hwp:id="title-65">Responses and normalization by orientation anisotropy model fits for all stimuli, data set 2</title></caption><graphic xlink:href="467486v2_figS3a" position="float" orientation="portrait" hwp:id="graphic-37"/></fig><fig id="figS3b" position="float" fig-type="figure" orientation="portrait" hwp:id="F17"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS3B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F17</object-id><object-id pub-id-type="publisher-id">figS3b</object-id><label>Figure S3b:</label><caption hwp:id="caption-24"><title hwp:id="title-66">Responses and NOA model fits for all stimuli, data set 3</title></caption><graphic xlink:href="467486v2_figS3b" position="float" orientation="portrait" hwp:id="graphic-38"/></fig><fig id="figS3c" position="float" fig-type="figure" orientation="portrait" hwp:id="F18"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS3C</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F18</object-id><object-id pub-id-type="publisher-id">figS3c</object-id><label>Figure S3c:</label><caption hwp:id="caption-25"><title hwp:id="title-67">Responses and NOA model fits for all stimuli, data set 4</title></caption><graphic xlink:href="467486v2_figS3c" position="float" orientation="portrait" hwp:id="graphic-39"/></fig><fig id="figS4a" position="float" fig-type="figure" orientation="portrait" hwp:id="F19"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS4A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F19</object-id><object-id pub-id-type="publisher-id">figS4a</object-id><label>Figure S4a:</label><caption hwp:id="caption-26"><title hwp:id="title-68">Responses and CE model fits for all stimuli, data set 1</title></caption><graphic xlink:href="467486v2_figS4a" position="float" orientation="portrait" hwp:id="graphic-40"/></fig><fig id="figS4b" position="float" fig-type="figure" orientation="portrait" hwp:id="F20"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS4B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F20</object-id><object-id pub-id-type="publisher-id">figS4b</object-id><label>Figure S4b:</label><caption hwp:id="caption-27"><title hwp:id="title-69">Responses and CE model fits for all stimuli, data set 2</title></caption><graphic xlink:href="467486v2_figS4b" position="float" orientation="portrait" hwp:id="graphic-41"/></fig><fig id="figS4c" position="float" fig-type="figure" orientation="portrait" hwp:id="F21"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS4C</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F21</object-id><object-id pub-id-type="publisher-id">figS4c</object-id><label>Figure S4c:</label><caption hwp:id="caption-28"><title hwp:id="title-70">Responses and CE model fits for all stimuli, data set 3</title></caption><graphic xlink:href="467486v2_figS4c" position="float" orientation="portrait" hwp:id="graphic-42"/></fig><fig id="figS4d" position="float" fig-type="figure" orientation="portrait" hwp:id="F22"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS4D</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F22</object-id><object-id pub-id-type="publisher-id">figS4d</object-id><label>Figure S4d:</label><caption hwp:id="caption-29"><title hwp:id="title-71">Responses and CE model fits for all stimuli, data set 4</title></caption><graphic xlink:href="467486v2_figS4d" position="float" orientation="portrait" hwp:id="graphic-43"/></fig><fig id="figS5a" position="float" fig-type="figure" orientation="portrait" hwp:id="F23"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS5A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F23</object-id><object-id pub-id-type="publisher-id">figS5a</object-id><label>Figure S5a:</label><caption hwp:id="caption-30"><title hwp:id="title-72">Responses and SOC model fits for all stimuli, data set 1</title></caption><graphic xlink:href="467486v2_figS5a" position="float" orientation="portrait" hwp:id="graphic-44"/></fig><fig id="figS5b" position="float" fig-type="figure" orientation="portrait" hwp:id="F24"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS5B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F24</object-id><object-id pub-id-type="publisher-id">figS5b</object-id><label>Figure S5b:</label><caption hwp:id="caption-31"><title hwp:id="title-73">Responses and SOC model fits for all stimuli, data set 2</title></caption><graphic xlink:href="467486v2_figS5b" position="float" orientation="portrait" hwp:id="graphic-45"/></fig><fig id="figS5c" position="float" fig-type="figure" orientation="portrait" hwp:id="F25"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS5C</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F25</object-id><object-id pub-id-type="publisher-id">figS5c</object-id><label>Figure S5c:</label><caption hwp:id="caption-32"><title hwp:id="title-74">Responses and SOC model fits for all stimuli, data set 3</title></caption><graphic xlink:href="467486v2_figS5c" position="float" orientation="portrait" hwp:id="graphic-46"/></fig><fig id="figS5d" position="float" fig-type="figure" orientation="portrait" hwp:id="F26"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS5D</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F26</object-id><object-id pub-id-type="publisher-id">figS5d</object-id><label>Figure S5d:</label><caption hwp:id="caption-33"><title hwp:id="title-75">Responses and SOC model fits for all stimuli, data set 4</title></caption><graphic xlink:href="467486v2_figS5d" position="float" orientation="portrait" hwp:id="graphic-47"/></fig><fig id="figS6a" position="float" fig-type="figure" orientation="portrait" hwp:id="F27" hwp:rev-id="xref-fig-27-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS6A</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F27</object-id><object-id pub-id-type="publisher-id">figS6a</object-id><label>Figure S6a:</label><caption hwp:id="caption-34"><title hwp:id="title-76">Responses and OTS model fits for all stimuli, data set 1</title></caption><graphic xlink:href="467486v2_figS6a" position="float" orientation="portrait" hwp:id="graphic-48"/></fig><fig id="figS6b" position="float" fig-type="figure" orientation="portrait" hwp:id="F28" hwp:rev-id="xref-fig-28-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS6B</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F28</object-id><object-id pub-id-type="publisher-id">figS6b</object-id><label>Figure S6b:</label><caption hwp:id="caption-35"><title hwp:id="title-77">Responses and OTS model fits for all stimuli, data set 2</title></caption><graphic xlink:href="467486v2_figS6b" position="float" orientation="portrait" hwp:id="graphic-49"/></fig><fig id="figS6c" position="float" fig-type="figure" orientation="portrait" hwp:id="F29"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS6C</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F29</object-id><object-id pub-id-type="publisher-id">figS6c</object-id><label>Figure S6c:</label><caption hwp:id="caption-36"><title hwp:id="title-78">Responses and OTS model fits for all stimuli, data set 3</title></caption><graphic xlink:href="467486v2_figS6c" position="float" orientation="portrait" hwp:id="graphic-50"/></fig><fig id="figS6d" position="float" fig-type="figure" orientation="portrait" hwp:id="F30"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.11.06.467486v2/FIGS6D</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F30</object-id><object-id pub-id-type="publisher-id">figS6d</object-id><label>Figure S6d:</label><caption hwp:id="caption-37"><title hwp:id="title-79">Responses and OTS model fits for all stimuli, data set 4</title></caption><graphic xlink:href="467486v2_figS6d" position="float" orientation="portrait" hwp:id="graphic-51"/></fig></sec></back></article>
