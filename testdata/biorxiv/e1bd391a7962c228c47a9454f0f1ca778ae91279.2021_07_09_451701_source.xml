<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.07.09.451701</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.07.09.451701</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.07.09.451701</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.07.09.451701</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.07.09.451701</article-id><article-version>1.3</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Your head is there to move you around: Goal-driven models of the primate dorsal pathway</article-title></title-group><author-notes hwp:id="author-notes-1"><fn fn-type="others" hwp:id="fn-1"><p hwp:id="p-1"><email hwp:id="email-1">patrick.mineault@gmail.com</email></p></fn><fn fn-type="others" hwp:id="fn-2"><p hwp:id="p-2"><email hwp:id="email-2">bakhtias@mila.quebec</email></p></fn><fn fn-type="others" hwp:id="fn-3"><p hwp:id="p-3"><email hwp:id="email-3">blake.richards@mila.quebec</email></p></fn><fn fn-type="others" hwp:id="fn-4"><p hwp:id="p-4"><email hwp:id="email-4">christopher.pack@mcgill.ca</email></p></fn><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label> Corresponding author; email: <email hwp:id="email-5">christopher.pack@mcgill.ca</email></corresp><fn fn-type="others" hwp:id="fn-5"><p hwp:id="p-5">35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5519-842X</contrib-id><name name-style="western" hwp:sortable="Mineault Patrick J"><surname>Mineault</surname><given-names>Patrick J</given-names></name><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-5519-842X"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2439-7252</contrib-id><name name-style="western" hwp:sortable="Bakhtiari Shahab"><surname>Bakhtiari</surname><given-names>Shahab</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-2439-7252"/></contrib><contrib contrib-type="author" hwp:id="contrib-3"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9662-2151</contrib-id><name name-style="western" hwp:sortable="Richards Blake A"><surname>Richards</surname><given-names>Blake A</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-9662-2151"/></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Pack Christopher C"><surname>Pack</surname><given-names>Christopher C</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Mila, McGill University</institution></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Mila, Montreal Neurological Institute, McGill University</institution></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Montreal Neurological Institute, McGill University</institution></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-07-09T18:36:10-07:00">
    <day>9</day><month>7</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-10-26T18:39:17-07:00">
    <day>26</day><month>10</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-07-09T18:40:21-07:00">
    <day>9</day><month>7</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-10-26T18:45:35-07:00">
    <day>26</day><month>10</month><year>2021</year>
  </pub-date><elocation-id>2021.07.09.451701</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-07-09"><day>09</day><month>7</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-10-26"><day>26</day><month>10</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-10-26"><day>26</day><month>10</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-6">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="451701.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.07.09.451701v3.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="451701.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.07.09.451701v3/2021.07.09.451701v3.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.07.09.451701v3/2021.07.09.451701v3.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-7">Neurons in the dorsal visual pathway of the mammalian brain are selective for motion stimuli, with the complexity of stimulus representations increasing along the hierarchy. This progression is similar to that of the ventral visual pathway, which is well characterized by artificial neural networks (ANNs) optimized for object recognition. In contrast, there are no image-computable models of the dorsal stream with comparable explanatory power. We hypothesized that the properties of dorsal stream neurons could be explained by a simple learning objective: the need for an organism to orient itself during self-motion. To test this hypothesis, we trained a 3D ResNet to predict an agent’s self-motion parameters from visual stimuli in a simulated environment. We found that the responses in this network accounted well for the selectivity of neurons in a large database of single-neuron recordings from the dorsal visual stream of non-human primates. In contrast, ANNs trained on an action recognition dataset through supervised or self-supervised learning could not explain responses in the dorsal stream, despite also being trained on naturalistic videos with moving objects. These results demonstrate that an ecologically relevant cost function can account for dorsal stream properties in the primate brain.</p></abstract><counts><page-count count="19"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-8">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-3">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-6"><p hwp:id="p-9">Camera-ready version accepted to NeurIPS 2021</p></fn></fn-group><fn-group content-type="external-links" hwp:id="fn-group-2"><fn fn-type="dataset" hwp:id="fn-7"><p hwp:id="p-10">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://your-head-is-there-to-move-you-around.netlify.app/" ext-link-type="uri" xlink:href="https://your-head-is-there-to-move-you-around.netlify.app/" hwp:id="ext-link-2">https://your-head-is-there-to-move-you-around.netlify.app/</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1" hwp:rev-id="xref-sec-1-1"><label>1</label><title hwp:id="title-4">Introduction</title><p hwp:id="p-11">The mammalian visual cortex is organized into two processing streams [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>]: the ventral stream, where neurons are selective for object class and identity; and the dorsal stream, where neurons are selective for motion. Neurons in the ventral stream exhibit selectivity for increasingly complex stimulus features at successive stages, from oriented lines in V1, to textures in V2, curved lines in V4, and culminating in representations of natural objects in the inferotemporal (IT) cortex [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>, <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>, <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>, <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>]. The myriad response properties within and across stages have been difficult to understand computationally [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>].</p><p hwp:id="p-12">However, in recent years, a large body of work [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>, <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>, <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>, <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>, <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>, <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>, <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>] has found that modern convolutional neural networks trained on image classification develop representations that match those found in the ventral stream. Early CNN layers match primary visual cortex (V1), while higher-level layers better match higher-level ventral stream areas, both in terms of qualitative preferred features [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref>] and quantitative predictions of responses to arbitrary stimuli [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref>, <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]. Moreover, models which perform well on ImageNet image classification tend to explain a larger proportion of the variance in ventral stream area IT [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>]. It has recently been found that high-performing networks can emerge through more biologically plausible self-supervised training [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>, <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>]. These results make it possible to interpret the sometimes baffling data about neural responses in the ventral stream in terms of a biologically plausible distributed learning algorithm whose goal is to develop invariant representations that can support object recognition behavior [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>, <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>].</p><p hwp:id="p-13">Although this approach has been similarly fruitful in other domains (e.g., audition [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]), it has not yet been applied to the dorsal visual pathway. From physiological recordings in dorsal stream areas like MT and MST [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>, <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>, <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>], we know that neurons in this pathway are exquisitely selective for motion and increase in receptive field size and complexity along their hierarchy. These properties have inspired different conceptions of dorsal pathway function, including action recognition [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>, <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>], prediction of image sequences [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>], and tracking of object motion [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>], to name just a few. At present, there is no way to know which, if any, of these proposals is correct.</p><p hwp:id="p-14">We hypothesized that dorsal pathway representations emerge from a simple objective: the need for the organism to orient itself during self-motion. As animals move through the world, they must estimate the parameters of their own motion, in order to avoid collisions, to plan trajectories, and to stabilize their gaze on objects of interest; the latter is critical for maintaining visual acuity. We suggest that this can be accomplished by learning, in a self-supervised way, the relationship between retinal images and self-motion parameters inferred from oculomotor and vestibular signals that exist in the brain [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>] [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>]. To test this hypothesis, we trained a 3D ResNet to predict the parameters of simulated self-motion - walking speed and head rotation – in short sequences of motion through simulated environments. We found that this network, dubbed <italic toggle="yes">DorsalNet</italic>, learned motion representations that were qualitatively similar to those found in the dorsal visual stream. Specifically, units were tuned for local motion direction in the earliest layers, object motion in intermediate layers, and complex optic flow in the highest layers [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>].</p><p hwp:id="p-15">To test our hypothesis quantitatively, we built a database of neural recordings from different regions of the dorsal visual pathway in non-human primates [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">14</xref>]. We then compared the ability of different networks to explain responses in areas V1, MT, and MST. We found that DorsalNet consistently outperformed 3D ResNets trained on action recognition in a supervised manner. Both the self-motion estimation objective and the training stimulus seemed to be critical, since 3D ResNets trained with a predictive objective [CPC; 30] or supervised on action sequences showed weaker performance. Thus, we demonstrate that the diverse neural response properties in the dorsal stream can be captured by a network that has the goal of estimating self-motion parameters from natural image sequences, both elucidating the functional role of the dorsal stream and creating a best-in-class, in-silico model of the dorsal stream.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-5">Background and related work</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-6">Dorsal stream processing</title><p hwp:id="p-16">The dorsal stream - also known as the <italic toggle="yes">where</italic> pathway - is a network of cortical areas that are selective for visual motion (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1A</xref>). It originates in primary visual cortex (area V1) with a subpopulation of neurons that respond selectively to oriented edges moving in a particular direction. These cells project to areas MT/V5 [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">27</xref>, <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>], where most neurons respond selectively to motion direction, even for relatively complex stimuli comprised of multiple edges or features [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. These neurons in turn project to area MST, where many neurons are selective for the kinds of complex motion patterns that arise during locomotion [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>]. MST is considered the terminal stage of the dorsal stream, with subsequent areas integrating information from other senses to support diverse roles in action recognition [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">23</xref>], decision-making [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>], and spatial memory [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>].</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-17">A 3D ResNet trained for self-motion estimation learns dorsal-like representations. A. The goal of this study is to model the dorsal visual stream, including V1, MT and MST. B. The 3D Resnet model is trained to estimate self-motion parameters from image sequences. C. Weights of the first layer. First layer filters are selective in space-time. D. Sample tuning of layer 1 features. Layer 1 contains many direction-selective cells reminiscent of V1. E. Sample tuning of layer 2 features. Many layer 2 features exhibit tuning for rigid motion, similar to MT. F. Sample tuning curve of layer 3 features. Many cells in layer 3 are tuned for complex optic flow, like MST</p></caption><graphic xlink:href="451701v3_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-7">Models of the dorsal stream</title><p hwp:id="p-18">Previous models of the dorsal stream have emphasized different possible functions. Giese and Poggio [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-3" hwp:rel-id="ref-23">23</xref>] have argued that the progression of selectivity along the pathway is well-suited to the recognition of biological movements, and this is consistent with studies showing that the ability to identify shapes from motion patterns is disrupted by lesions to area MT [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>]. Other models have posited a role for the dorsal pathway in segmenting moving objects [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>, <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>], predicting future image frames [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>], or supporting reaching movements [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>]. Finally, a body of computational [<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>, <xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>] and experimental [<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>] work has analyzed the potential role of dorsal stream neurons in the perception of heading or path [<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>]. None of these models has been quantitatively compared to the detailed properties of neural responses in the dorsal stream.</p><p hwp:id="p-19">Other models have made this kind of comparison, but they have been based on shallow architectures and fit directly to the data from dorsal stream areas, including V1 [<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>], MT [<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>, <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>, <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>] and MST [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>]. Although these models shed light on the mechanisms by which neurons attain their stimulus selectivity, they do not relate in any clear way to the functional hypotheses mentioned above.</p><p hwp:id="p-20">We have therefore attempted to link the properties of dorsal stream neurons, obtained from a database of recordings in non-human primate cortex, to specific functional objectives hypothesized in previous work. In this sense our work is in line with the goals of BrainScore [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-3" hwp:rel-id="ref-14">14</xref>], which seeks to benchmark ANNs by their ability to explain ventral stream neurons, and to recent work examining self-supervised networks’ fits to ventral areas [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref>, <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>].</p></sec></sec><sec id="s3" hwp:id="sec-5" hwp:rev-id="xref-sec-5-1 xref-sec-5-2 xref-sec-5-3 xref-sec-5-4"><label>3</label><title hwp:id="title-8">Methods</title><sec id="s3a" hwp:id="sec-6"><title hwp:id="title-9">Training network for self-motion</title><p hwp:id="p-21">We generated a dataset consisting of short videos (10 frames) of self-motion in AirSim, a package for drone and land vehicle simulations in Unreal Engine [<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>]. These videos simulated walking along linear trajectories with constant head rotations in two environments (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1B</xref>), starting at random positions, varying environmental conditions, hour of day, starting head pose speed and walking speed (<xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Table S1</xref> in the Appendix). Sequences that led to collisions with the environment were removed.</p><p hwp:id="p-22">We trained a 6-layer 3D ResNet (layer definitions in <xref ref-type="table" rid="tblS1" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">table S1</xref>) to predict 2 of the components of head rotation (yaw and pitch rotation speed; roll was not simulated) and the 3 components of linear velocity (parametrized as yaw and pitch heading and speed). We chose a 3D ResNet architecture over alternatives for its stable training, wide use in video tasks [<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>, <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>], and the high performance of 2D resnets in modeling the ventral stream [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-4" hwp:rel-id="ref-14">14</xref>]. We discretized each component into 72 bins and trained the network with a cross-entropy objective for each of the 5 components. We used the Adam optimizer with a step size of 0.003, batch norm, and trained for 100 epochs.</p></sec><sec id="s3b" hwp:id="sec-7"><title hwp:id="title-10">Neural datasets</title><p hwp:id="p-23">Datasets are listed in <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>. All experiments were conducted in non-human primates (macaca fascicularis and macaca mulatta) and were approved by the governing IRB; detailed experimental procedures are available in the corresponding publications. Data are used under the license terms listed on <ext-link l:rel="related" l:ref-type="uri" l:ref="http://crcns.org" ext-link-type="uri" xlink:href="http://crcns.org" hwp:id="ext-link-3">crcns.org</ext-link> or by permission from the authors [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">47</xref>]. Methods varied from dataset to dataset, but generally, non-human primates were instructed to fixate on a small target while a contiguous image sequence was presented for several minutes. In some cases, parts of the image sequence were repeated when fixation was lost. Image sequences consisted of color movies, black-and-white movies, static pictures with simulated motions, and random dot kinematograms. Data was collected using single electrodes or multi-electrode arrays. Where available, we used previously published sorted spikes; when spikes were unsorted, we used multi-unit activity.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-2"><p hwp:id="p-24">Datasets</p></caption><graphic xlink:href="451701v3_tbl1" position="float" orientation="portrait" hwp:id="graphic-2"/></table-wrap><p hwp:id="p-25">We split each dataset into a train and test set [<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>]; when only a subset of these stimuli were repeated several times, or a dataset had a designated test subset, we used this subset as the test set; in other cases, we split the data into 6-second blocks, and concatenated every 10th block to form a test set. We kept the sampling rate of the image sequence at its natural rate and resampled neural activity at the same rate, indicated in <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>. We resampled all stimuli spatially to 112×112. In control analyses, we resampled the input to 74×74 or 168×168 to measure the sensitivity of the results to scale. In the case of [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-3" hwp:rel-id="ref-47">47</xref>, <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">44</xref>], the seeds originally used to determine the exact location of dots in the random dot kinematograms were lost, hence we regenerated stimuli with dots in different locations; it should be noted that this could limit the maximal performance of networks [<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]. All data used in this paper has been previously published; we release preprocessing scripts and PyTorch loaders to facilitate replication.</p></sec><sec id="s3c" hwp:id="sec-8"><title hwp:id="title-11">Aligning ANNs and neural activity</title><p hwp:id="p-26">We computed latent representations at different layers of the target ANNs, listed in <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>, in windows of 10 image frames preceding neural activity. We cropped the first and last latent activity frame and downsampled the activity 2-fold temporally to obtain 4 frames of latent representations preceding the neural activity, and spatially averaged and downsampled each layer output to 8 by 8. Following [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-5" hwp:rel-id="ref-14">14</xref>], we kept the first 500 PCs of the intermediate representation and used ridge regression to find mappings from latent space to experimental neural activity. We selected the ridge parameter using 5-fold cross-validation within the train set. Where test sets with 5 or more disaggregated repeats were available, we report an R score normalized against the maximum attainable R score [<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>]; otherwise, we report the raw R score.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2 xref-table-wrap-2-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-3"><p hwp:id="p-27">Models tested</p></caption><graphic xlink:href="451701v3_tbl2" position="float" orientation="portrait" hwp:id="graphic-3"/></table-wrap><p hwp:id="p-28">In control analyses, we replaced ridge regression with a sparse regression estimated through boosting. To fit all the intermediate representations in memory and fit a boosted regression model, we used two different strategies to reduce the memory footprint: downsampling layer outputs (with spatial averaging as in the linear regression; <xref ref-type="table" rid="tblS4" hwp:id="xref-table-wrap-7-1" hwp:rel-id="T7">table S4</xref> in the appendix) or subsampling (without spatial averaging; <xref ref-type="table" rid="tblS4" hwp:id="xref-table-wrap-7-2" hwp:rel-id="T7">Table S4</xref>). We selected the number of boosting iterations using 5-fold cross-validation within the train set. We fit these models on a commodity GPUs including P5000 and 1080Ti locally, in Paperspace and in AWS for a total of ∼ 1000 single-GPU-hours. Model weights and code are available <sup>1</sup> under an MIT license.</p></sec><sec id="s3d" hwp:id="sec-9"><title hwp:id="title-12">Contrastive Predictive Coding (CPC)</title><p hwp:id="p-29">CPC is a self-supervised learning algorithm that learns to predict the next latent state of a sequence (e.g. a video sequence) given its present and past states. The details of the CPC algorithm can be found in [<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>] and [<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref>], but we summarize it briefly here. A sequence of video frames (<italic toggle="yes">x</italic><sub><italic toggle="yes">t</italic></sub>) are passed as input to a 3D CNN. The CNN output (<italic toggle="yes">z</italic><sub><italic toggle="yes">t</italic></sub>), which is a latent representation of the video sequence, is fed to a recurrent neural net (RNN). The RNN aggregates past and present latent states (i.e. CNN output) and generates a context variable as its output (<italic toggle="yes">c</italic><sub><italic toggle="yes">t</italic></sub>). The context variable is then passed to a single layer MLP which predicts the future latent state of the video. The predicted latent state and the true latent state (positive pairs), along with some incorrect examples of the next state (negative pairs) are given to a contrastive loss function. Minimizing the contrastive loss maximizes the similarity of the predicted and the true next states, and minimizes the similarity of the predicted and the false next states.</p></sec></sec><sec id="s4" hwp:id="sec-10"><title hwp:id="title-13">Results</title><sec id="s4a" hwp:id="sec-11"><label>4.1</label><title hwp:id="title-14">3D resnets trained for self-motion learn dorsal-like representations</title><p hwp:id="p-30">We hypothesized that learning to estimate self-motion from visual inputs would lead to dorsal stream-like representations. As in the ventral stream, these representations begin in V1 with receptive fields that encode simple, local features of stimuli. Through subsequent recombinations at different layers, more complex and ecologically relevant encoding emerges. To test this hypothesis, we generated self-motion videos in a simulation environment, and trained a 3D ResNet to predict its self-motion parameters, namely head rotation and linear locomotion (see Methods for details).</p><sec id="s4a1" hwp:id="sec-12"><title hwp:id="title-15">Qualitative matches to the dorsal stream</title><p hwp:id="p-31">The 6-layer 3D ResNet trained in this way learned representations similar to single units in the primate dorsal stream. We focus our attention here on layers 1, 2 and 3 of the network. Preferred features of layer 1 contained many spatiotemporally slanted filters (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Figure 1C</xref>), which are the building blocks of motion selectivity in primate V1 [<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref>]. We quantified this slant with the separability index <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="451701v3_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> from the singular values of the grayscale filters <italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub>; this matched values reported in the literature for V1 [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref>] [.72 +/-.16 for trained network,.71 +/- .15 in real V1 neurons; <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">figure S1</xref> in the appendix].</p><p hwp:id="p-32">To gain insight into the stimulus selectivity of these representations, we generated optimal stimuli for individual units in intermediate layers of the network by optimization [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-3" hwp:rel-id="ref-12">12</xref>]; we present static images of the intermediate preferred frame here, while animations can be visualized on the companion website<sup>2</sup>. Probed in this fashion, many intermediate features in layer 1 preferred what looked like drifting gratings (examples in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Figure 1D</xref>), consistent with the selectivity of V1 cells [<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">55</xref>, <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref>]. Hence, to further probe the selectivity of these units, we used full contrast, drifting gratings of different spatial and temporal frequencies, placed in the center of the visual field. Tuning curves in layer 1 (samples in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Figure 1</xref>) tended to have a bias towards direction selectivity, with a mean circular variance at the preferred spatial and temporal frequency of 0.75 and a median direction selectivity index - defined as 1 − <italic toggle="yes">r</italic><sub><italic toggle="yes">pref</italic></sub> <italic toggle="yes">/r</italic><sub><italic toggle="yes">antipref</italic></sub> on the centered tuning curves - of 0.98. This is somewhat higher than is typically found in V1 [<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref>], likely due to lack of noise, but it is close to the selectivity of the V1 neurons that actually project to higher levels of the dorsal visual pathway [<xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>].</p><p hwp:id="p-33">Layer 2 units tended to prefer more spatially broadband moving stimuli, not unlike the plaids conventionally used in probing MT cells [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">21</xref>] (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Figure 1E</xref>, left column). Indeed, probing the representations with sums of gratings revealed similar selectivity to a single grating in a subset of cells (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Figure 1E</xref>, middle column; pattern selectivity plots in <xref rid="figS1" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure S1</xref> in the appendix). These cells likely encode stimulus velocity in a manner that is invariant of the composition of the stimulus [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-4" hwp:rel-id="ref-21">21</xref>]. Like MT cells, subunits in this layer tended to be highly direction selective, with the average circular variance of the direction tuning curves being .41.</p><p hwp:id="p-34">MT cells are also known to be selective for stimulus speed, which is the ratio of temporal to spatial frequencies [<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref>]. A similar kind of selectivity emerged in layer 2 of the model, where many units preferred higher temporal frequencies when the spatial frequencies were higher (example tuning curve in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Figure 1E</xref>). To quantify this selectivity, we probed the model units with a range of spatiotemporal frequencies and fit the data with slanted Gaussian functions [<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref>], which revealed a mean speed selectivity index of -.14 in layer 1, compared to 0.58 in layer 2, the latter being similar to the value of 0.52 reported in MT [<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-2" hwp:rel-id="ref-60">60</xref>]. Probing layer 2 units with moving dots, we found a majority of neurons with simple receptive fields that prefer linear motion, with a smaller number of complex receptive fields (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Figure 1E</xref>, bottom right).</p><p hwp:id="p-35">Finally, we found many cells in layer 3 that combined the outputs of lower-level units to generate selectivity for more complex motion patterns (example cells in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Figure 1F</xref>). Dot pattern probes revealed selectivity for rotations, spirals or single axis expansion. As in primate area MST, these units tended to emphasize expansion motion rather than contraction, similar to the bias experienced during forward navigation (<xref rid="figS1" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure S1</xref>) [<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref>].</p></sec><sec id="s4a2" hwp:id="sec-13"><title hwp:id="title-16">Regression analysis of representations</title><p hwp:id="p-36">Given that the trained network recapitulated many qualitative properties of the dorsal stream, we next investigated whether they quantitatively matched dorsal stream areas, for which single-neuron data was available. We used ridge regression to learn a mapping from latent representations at each layer of the network to single neural responses to complex stimuli, including black and white and color movies, along with random dot kinematograms (See Methods and <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Table 1</xref> for details). We learned a separate mapping for each layer of the network, allowing us to match the depth of the network to each brain area. As seen in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>, this showed a hierarchical progression, with higher-level cortex matching higher-level layers in the ResNet. The average best matching layer across cells with report correlation greater than .01, illustrated by the horizontal lines, was 1.1 for V1 cells [(0.9, 1.5) 95% CI, bootstrap across cells], 2.0 for MT cells [(1.8, 2.3)] and 2.9 for MST cells [(2.3, 3.4)].</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-4"><p hwp:id="p-37">Layers 1, 2, and 3 of DorsalNet best match areas V1, MT and MST. Lines show correlation (R) relative to maximum for area. Horizontal lines: 95% CI of layer with maximal alignment to area.</p></caption><graphic xlink:href="451701v3_fig2" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-38">We noticed that the mapping was less distinct for layer 3. We investigated this further by measuring the response of the network to a gauntlet of stimuli from the Airsim dataset. Centered kernel alignment (CKA) [<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">69</xref>] revealed that while layers 0, 1, and 2 had highly distinct representations, subsequent layers were less distinct S3. We also investigated the robustness of the mapping to a change of scale of the input sequences [<xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">70</xref>]. We saw some minor shifts: the median mean layer assignment for V1 was 1.0 at 0.66X scale, 1.1 at the standard 1X scale, and 1.3 at 1.5X scale (<xref rid="figS2" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure S2</xref>). Overall, however, mappings were robust to a change of scale. Thus, broadly speaking, layers 1, 2 and 3 of the network recapitulated V1, MT and MST, respectively.</p></sec></sec><sec id="s4b" hwp:id="sec-14"><label>4.2</label><title hwp:id="title-17">Networks with alternative objectives do not account for responses in the dorsal stream</title><sec id="s4b1" hwp:id="sec-15"><title hwp:id="title-18">Action recognition networks</title><p hwp:id="p-39">To examine the specificity of these results, we tested other networks trained with different objective functions. Action recognition is a popular computer vision task, and so we tested 3D ResNets trained on Kinetics400 [<xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">71</xref>]. These networks performed admirably in explaining V1 responses, reaching an average R &gt; .4 on the pvc1 dataset. However, across our MT and MST datasets, performance was poor, failing to exceed that of a null model [<xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">72</xref>] consisting of a 3D Gabor pyramid (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref>). We note that only a small fraction of V1 neurons project to the dorsal stream, with the majority projecting to ventral stream areas; we interpret the relative performance in V1 vs. MT and MST as a sign that these networks learned representations more aligned with the <italic toggle="yes">ventral</italic> stream, supporting object recognition and by extension action recognition. Consistent with this interpretation, we found that the first layer of 3D ResNets trained for action recognition did not learn motion in the traditional sense (<xref rid="figS5" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure S5</xref>). Instead, their filters were mostly separable in space and time, meaning they were not selective for motion energy <italic toggle="yes">per se</italic>.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1 xref-table-wrap-3-2 xref-table-wrap-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3:</label><caption hwp:id="caption-5"><p hwp:id="p-40">DorsalNet quantitatively performs best across the dorsal stream. Table shows normalized pearson correlation (R; see Methods for definition) of different models on different datasets. In parenthesis: standard error of the mean over cells.</p></caption><graphic xlink:href="451701v3_tbl3" position="float" orientation="portrait" hwp:id="graphic-5"/></table-wrap><p hwp:id="p-41"><bold>CPC</bold> Our results indicate that learning to estimate self-motion in a simulated environment creates representations similar to those in the primate dorsal stream. The neural network architecture (3D ResNets) was similar for the self-motion estimation objective and action recognition tasks. However, both the task - prediction of self-motion parameters - and the stimulus ensemble - self-motion sequences in the Airsim environment - differed. To tease apart the relative importance of these two factors, we tested the ability of contrastive predictive coding (CPC) networks [<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">30</xref>] to account for responses in the dorsal stream when trained over different stimulus ensembles. CPC is a self-supervised training method that finds predictive latent representations that can distinguish between image sequences. Importantly, it is possible to apply the CPC objective to different stimulus ensembles, thereby differentiating between task and stimulus ensemble effects. We trained an 11-layer network with a CPC objective on the UCF101 dataset and our Airsim dataset. The Airsim-trained network performed significantly better than the UCF101-trained network, approaching the performance of DorsalNet in MT but not in MST. Examining first layer filters revealed direction-selective receptive fields after training on the Airsim dataset but not with UCF101 (<xref rid="figS5" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure S5</xref>). This is consistent with the training set being necessary, though not sufficient, to match primate dorsal stream neurons.</p></sec></sec><sec id="s4c" hwp:id="sec-16"><title hwp:id="title-19">MotionNet</title><p hwp:id="p-42">We next tested a much simpler 2-layer network from the neuroscience literature, which was trained to estimate the linear motion of black and white image patches [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">26</xref>]. The original model was a fully connected architecture working on small image patches, and we made it convolutional by tiling. We used the checkpoints shared by the authors as the model weights. This model had not previously been directly benchmarked against neural data, and given the small size of its stimulus ensemble, we did not expect it to perform well. Surprisingly, it scored far better in predicting MT and MST responses than action recognition networks (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-2" hwp:rel-id="T3">Table 3</xref>). We found in a control analysis (<xref ref-type="table" rid="tblS3" hwp:id="xref-table-wrap-6-1" hwp:rel-id="T6">Table S3</xref> in the appendix) that the relative performance of MotionNet could be improved still by spatially scaling up the stimulus, matching the performance of DorsalNet on 2 out of 3 MT and MST datasets. These results are consistent with solving 2D motion being an important sub-goal of the dorsal stream.</p><p hwp:id="p-43">We next asked whether there existed a one-to-one or few-to-one relationship between model subunits and single neurons. Using sparse regression, we found that DorsalNet better matched individual neurons across all MT and MST datasets than MotionNet, regardless of scaling (<xref ref-type="table" rid="tblS4" hwp:id="xref-table-wrap-7-3" hwp:rel-id="T7">Tables S4</xref> and <xref ref-type="table" rid="tblS5" hwp:id="xref-table-wrap-8-1" hwp:rel-id="T8">S5</xref> in the appendix). Thus, DorsalNet subunits were more directly aligned to single neurons across the dorsal stream.</p></sec></sec><sec id="s5" hwp:id="sec-17"><label>5</label><title hwp:id="title-20">Self-motion estimation performance correlates with dorsal stream match</title><p hwp:id="p-44">Across our baselines, there was a large range in the ability of different models to reproduce dorsal stream data. We asked whether this heterogeneity could be linked to performance on a self-motion estimation task. We froze the weights of our baseline networks and trained linear decoders to estimate self-motion parameters on the AirSim dataset from hidden layer representations. We excluded DorsalNet and Airsim-trained CPC from these comparisons. Across our baselines, there was a highly significant correlation between self-motion estimation performance and match to MT and MST neurons (<xref rid="figS4" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure S4</xref>; <xref ref-type="table" rid="tblS2" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Table S2</xref> in the appendix). Interestingly, when looking at individual self-motion parameters, head rotation estimation accuracy was most correlated with performance on MT and MST datasets. Thus, those networks which happen to be best at self-motion estimation, especially head rotation, can best explain responses in the dorsal stream, consistent with a formative role of self-motion estimation in dorsal stream representations.</p></sec><sec id="s6" hwp:id="sec-18"><label>6</label><title hwp:id="title-21">Limitations</title><sec id="s6a" hwp:id="sec-19"><title hwp:id="title-22">Multiple interpretations</title><p hwp:id="p-45">We show that learning to estimate one’s self-motion from visual cues leads to representations which are similar to those of the dorsal stream. We benchmark against several other candidate models, including localized frequency detectors, which form a sparse basis for images [<xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">73</xref>], predictive coding models, models trained for action recognition, and models trained trained to estimate the motion of small image patches. While DorsalNet performed best overall across the dorsal stream, we found that MotionNet and a CPC-based network trained on our AirSim dataset were close contenders. With the available data, we cannot conclusively rule out that these alternative objectives, with the right tweaks, could not account for the data. One interesting possibility is that, as MotionNet hints, solving rigid 2D motion is a sub-goal of the dorsal stream; and, as DorsalNet shows, the supervisory signal needed to learn to solve that sub-goal could come from head movements, especially head rotations, via efference copy. An open benchmark in the style of [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-6" hwp:rel-id="ref-14">14</xref>] could reveal other objectives compatible with the data and refine these results.</p></sec><sec id="s6b" hwp:id="sec-20"><title hwp:id="title-23">Data limitations</title><p hwp:id="p-46">To the best of our knowledge, we used all of the relevant publicly available non-human primate data for this study. Most of this data was collected more than a decade ago in time-consuming single-electrode experiments, with electrode drift, loss of fixation, short recording times and small numbers of recordings per experiment being significant limitations. The MST dataset in particular is not very discriminative across models. Differences in stimuli and number of repetitions make absolute comparisons across areas difficult. Improvements in recording technology as well as better-designed hypothesis-driven studies will allow the collection of more discriminative data in the future. Our study paves the way for closed-loop experiments to verify that the estimated stimuli indeed maximally drive dorsal stream neurons [<xref ref-type="bibr" rid="c74" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">74</xref>, <xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">75</xref>].</p></sec></sec><sec id="s7" hwp:id="sec-21"><label>7</label><title hwp:id="title-24">Discussion</title><p hwp:id="p-47">Systems neuroscience aims to explain how the brain solves behavioral tasks at the algorithmic level [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref>]. While a rich literature has linked the ventral visual stream to the task of object recognition, little work has focused on understanding how and why dorsal streams representations emerge. Noting the critical role of self-motion estimation across the animal kingdom [<xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">76</xref>], we hypothesized that training an artificial neural net on self-motion estimation from image sequences would lead to representations similar to the dorsal stream. We verified this qualitatively by probing networks with artificial stimuli and by finding maximizing stimuli. We confirmed these findings quantitatively by benchmarking existing computer vision networks on a gauntlet of neural data [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-7" hwp:rel-id="ref-14">14</xref>].</p><p hwp:id="p-48">In the framework of [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">17</xref>], the objective, learning rule and architecture specify how a task is to be solved by an artificial or biological neural network. Implicit in the framework is a fourth critical ingredient: the dataset, or distribution of training examples. Our work focuses on how an objective, learning rule and dataset interact to form representations similar to the dorsal stream. In contradistinction with previous work, we focus on a single architecture of 3D ResNets, a coarse approximation to early and intermediate visual processing stages, highlighting the formative role of objective, learning rule and dataset in the creation of useful representations for action.</p><sec id="s7a" hwp:id="sec-22"><title hwp:id="title-25">Maximizing stimuli reveal selectivity</title><p hwp:id="p-49">Systems identification has long been used in systems neu-roscience to estimate preferred stimuli in different brain areas [<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-3" hwp:rel-id="ref-55">55</xref>, <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-2" hwp:rel-id="ref-56">56</xref>, <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-2" hwp:rel-id="ref-43">43</xref>, <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">46</xref>, <xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">51</xref>, <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-3" hwp:rel-id="ref-20">20</xref>, <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-4" hwp:rel-id="ref-47">47</xref>, <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">77</xref>, <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-3" hwp:rel-id="ref-44">44</xref>]. More recently, systems identification has been used to better understand mechanisms of selectivity in deep neural nets [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-4" hwp:rel-id="ref-12">12</xref>]. Given the breadth of available systems identification results in brains, we suggest that systems identification is a particularly powerful tool to relate brains and artificial neural nets, especially when combined with benchmarking: it can offer clues as to why certain networks perform better than others. In this article, we identified direction selectivity in the first layer as a strong clue that networks develop good motion representations.</p></sec><sec id="s7b" hwp:id="sec-23"><title hwp:id="title-26">Action recognition is poorly aligned to the dorsal stream</title><p hwp:id="p-50">Our work shows that ANNs trained on the standard computer vision task of action recognition fail to learn motion representations that correlate with single neurons in MT and MST. [<xref ref-type="bibr" rid="c78" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">78</xref>] reported that on Kinetics400 and UCF101, a single image is sufficient to get within 6% of the action recognition accuracy of a full image sequence, indicating that motion has a limited role in action recognition in these datasets. Motion selectivity can be reintroduced via a parallel optic flow pathway [<xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">79</xref>] or by enforcing that the network reproduce dense optic flow following early layers [<xref ref-type="bibr" rid="c80" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">80</xref>, <xref ref-type="bibr" rid="c81" hwp:id="xref-ref-81-1" hwp:rel-id="ref-81">81</xref>], with modest improvements in classification accuracy. Our benchmarks strongly suggest that current action recognition datasets can be solved without motion and that good motion representations don’t emerge from supervised learning on them alone.</p></sec><sec id="s7c" hwp:id="sec-24"><title hwp:id="title-27">Self-supervision through cross-modal prediction</title><p hwp:id="p-51">We train DorsalNet in a supervised way. From the agent’s perspective, however, corollary discharges of the motor plan are available, as well as vestibular inputs. Thus, the objective can be viewed as a self-supervised objective which aims to predict one modality or channel of the input from the other, in line with other proxy tasks including colorization and audiovisual alignment [<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">82</xref>, <xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">83</xref>, <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">84</xref>]. Because multisensory integration and corollary discharges are ubiquitous across mobile animals [<xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">85</xref>], self-supervision through cross-modal prediction could be potentially widely used across species to learn useful representations.</p></sec><sec id="s7d" hwp:id="sec-25"><title hwp:id="title-28">Evolution and learning in sensory systems</title><p hwp:id="p-52">Thompson [<xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">86</xref>] identifies four set of constraints against which in silico models of sensory systems can be evaluated:</p><list list-type="bullet" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-53">Whether it can perform a relevant task</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-54">Whether it accounts for neural activity</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-55">Whether it is biologically plausible</p></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-56">Whether it could have evolved</p></list-item></list><p hwp:id="p-57">We presented a model of the dorsal stream that is trained to estimate self-motion. It accounts for neural responses in 3 different areas, taken from 5 different datasets. The model weights can be learned by the agent through biologically plausible self-supervision, since the approximate parameters of self-motion are known to the agent, via corollary discharges and vestibular and proprioceptive inputs [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref>]. Self-motion estimation is particularly important for gaze stabilization, which evolved in tandem with the earliest visual functions [<xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-1" hwp:rel-id="ref-87">87</xref>, <xref ref-type="bibr" rid="c88" hwp:id="xref-ref-88-1" hwp:rel-id="ref-88">88</xref>], and continues to be necessary for visual processing, including that performed in the ventral pathway [<xref ref-type="bibr" rid="c89" hwp:id="xref-ref-89-1" hwp:rel-id="ref-89">89</xref>]. Given this evolutionary pressure, some aspects of the dorsal pathway are likely hard-coded in the genome, while others are learned through development [<xref ref-type="bibr" rid="c90" hwp:id="xref-ref-90-1" hwp:rel-id="ref-90">90</xref>]; further work will focus on better understanding the relative role of evolution vs. learning in dorsal stream processing. This work and its follow-ups thus have the potential to elucidate long-standing questions about how sensory systems evolved.</p></sec></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-29">Acknowledgments and Disclosure of Funding</title><p hwp:id="p-58">This work was supported by a NSERC (Discovery Grant: RGPIN-2020-05105; Discovery Accelerator Supplement: RGPAS-2020-00031), Healthy Brains, Healthy Lives (New Investigator Award: 2b-NISU-8; Innovative Ideas Grant: 1c-II-15), and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship). CCP was funded by a CIHR grant (MOP-115178). This work was also funded by the Canada First Research Excellence Fund (CFREF Competition 2, 2015-2016) awarded to the Healthy Brains, Healthy Lives initiative at McGill University, through the Helmholtz International BigBrain Analytics and Learning Laboratory (HIBALL).</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-30">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Ungerleider Leslie G."><given-names>Leslie G.</given-names> <surname>Ungerleider</surname></string-name> and <string-name name-style="western" hwp:sortable="Mishkin Mortimer"><given-names>Mortimer</given-names> <surname>Mishkin</surname></string-name>. <article-title hwp:id="article-title-2">Two cortical visual systems</article-title>. <source hwp:id="source-1">Analysis of visual behavior</source>, pages <fpage>549</fpage>–<lpage>586</lpage> <year>1982</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Pasupathy A."><given-names>A.</given-names> <surname>Pasupathy</surname></string-name> and <string-name name-style="western" hwp:sortable="Connor C. E"><given-names>C. E</given-names> <surname>Connor</surname></string-name>. <article-title hwp:id="article-title-3">Population coding of shape in area V4</article-title>. <source hwp:id="source-2">Nature Neuroscience</source>, <volume>5</volume>(<issue>12</issue>):<fpage>1332</fpage>–<lpage>1338</lpage> <year>2002</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Rust Nicole C."><given-names>Nicole C.</given-names> <surname>Rust</surname></string-name> and <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <article-title hwp:id="article-title-4">Selectivity and Tolerance (“Invariance”) Both Increase as Visual Information Propagates from Cortical Area V4 to IT</article-title>. <source hwp:id="source-3">The Journal of Neuroscience</source>, <volume>30</volume>(<issue>39</issue>):<fpage>12978</fpage> –<lpage>12995</lpage> <year>2010</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Brincat S. L"><given-names>S. L</given-names> <surname>Brincat</surname></string-name> and <string-name name-style="western" hwp:sortable="Connor C. E"><given-names>C. E</given-names> <surname>Connor</surname></string-name>. <article-title hwp:id="article-title-5">Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title>. <source hwp:id="source-4">Nature Neuroscience</source>, <volume>7</volume>(<issue>8</issue>):<fpage>880</fpage>–<lpage>886</lpage> <year>2004</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Freeman Jeremy"><given-names>Jeremy</given-names> <surname>Freeman</surname></string-name>, <string-name name-style="western" hwp:sortable="Ziemba Corey M."><given-names>Corey M.</given-names> <surname>Ziemba</surname></string-name>, <string-name name-style="western" hwp:sortable="Heeger David J."><given-names>David J.</given-names> <surname>Heeger</surname></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli Eero P."><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name>, and <string-name name-style="western" hwp:sortable="Movshon J. Anthony"><given-names>J. Anthony</given-names> <surname>Movshon</surname></string-name>. <article-title hwp:id="article-title-6">A functional and perceptual signature of the second visual area in primates</article-title>. <source hwp:id="source-5">Nature Neuroscience</source>, <volume>16</volume>(<issue>7</issue>):<fpage>974</fpage>–<lpage>981</lpage> <month>July</month> <year>2013</year>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Parker Andrew J"><given-names>Andrew J</given-names> <surname>Parker</surname></string-name>. <article-title hwp:id="article-title-7">Intermediate level cortical areas and the multiple roles of area V4</article-title>. <source hwp:id="source-6">Current Opinion in Physiology</source>, <volume>16</volume>:<fpage>61</fpage>–<lpage>67</lpage> <month>August</month> <year>2020</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>[7]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Yamins Daniel"><given-names>Daniel</given-names> <surname>Yamins</surname></string-name>, <string-name name-style="western" hwp:sortable="Hong Ha"><given-names>Ha</given-names> <surname>Hong</surname></string-name>, <string-name name-style="western" hwp:sortable="Cadieu Charles"><given-names>Charles</given-names> <surname>Cadieu</surname></string-name>, and <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <source hwp:id="source-7">Hierarchical modular optimization of convolutional networks achieves representations similar to macaque IT and human ventral stream</source>. <year>2013</year>. Publisher: <publisher-name>Neural Information Processing Systems Foundation</publisher-name>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><label>[8]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Yamins Daniel LK"><given-names>Daniel LK</given-names> <surname>Yamins</surname></string-name>, <string-name name-style="western" hwp:sortable="Hong Ha"><given-names>Ha</given-names> <surname>Hong</surname></string-name>, <string-name name-style="western" hwp:sortable="Cadieu Charles F."><given-names>Charles F.</given-names> <surname>Cadieu</surname></string-name>, <string-name name-style="western" hwp:sortable="Solomon Ethan A."><given-names>Ethan A.</given-names> <surname>Solomon</surname></string-name>, <string-name name-style="western" hwp:sortable="Seibert Darren"><given-names>Darren</given-names> <surname>Seibert</surname></string-name>, and <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <chapter-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</chapter-title>. <source hwp:id="source-8">Proceedings of the national academy of sciences</source>, <volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage> <year>2014</year>. <collab hwp:id="collab-1">Publisher</collab>: <publisher-name>National Acad Sciences</publisher-name>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2"><label>[9]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Yamins Daniel LK"><given-names>Daniel LK</given-names> <surname>Yamins</surname></string-name> and <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <chapter-title>Using goal-driven deep learning models to understand sensory cortex</chapter-title>. <source hwp:id="source-9">Nature neuroscience</source>, <volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>365</lpage> <year>2016</year>. <collab hwp:id="collab-2">Publisher</collab>: <publisher-name>Nature Publishing Group</publisher-name>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>[10]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Khaligh-Razavi Seyed-Mahdi"><given-names>Seyed-Mahdi</given-names> <surname>Khaligh-Razavi</surname></string-name> and <string-name name-style="western" hwp:sortable="Kriegeskorte Nikolaus"><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <chapter-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</chapter-title>. <source hwp:id="source-10">PLoS computational biology</source>, <volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage> <year>2014</year>. <collab hwp:id="collab-3">Publisher</collab>: <publisher-name>Public Library of Science San Francisco, USA</publisher-name>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Lindsay Grace W."><given-names>Grace W.</given-names> <surname>Lindsay</surname></string-name>. <chapter-title>Convolutional neural networks as a model of the visual system: past, present, and future</chapter-title>. <source hwp:id="source-11">Journal of cognitive neuroscience, pages 1–15</source> <year>2020</year>. <collab hwp:id="collab-4">Publisher</collab>: <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2 xref-ref-12-3 xref-ref-12-4"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Cammarata Nick"><given-names>Nick</given-names> <surname>Cammarata</surname></string-name>, <string-name name-style="western" hwp:sortable="Carter Shan"><given-names>Shan</given-names> <surname>Carter</surname></string-name>, <string-name name-style="western" hwp:sortable="Goh Gabriel"><given-names>Gabriel</given-names> <surname>Goh</surname></string-name>, <string-name name-style="western" hwp:sortable="Olah Chris"><given-names>Chris</given-names> <surname>Olah</surname></string-name>, <string-name name-style="western" hwp:sortable="Petrov Michael"><given-names>Michael</given-names> <surname>Petrov</surname></string-name>, and <string-name name-style="western" hwp:sortable="Schubert Ludwig"><given-names>Ludwig</given-names> <surname>Schubert</surname></string-name>. <article-title hwp:id="article-title-8">Thread: Circuits</article-title>. <source hwp:id="source-12">Distill</source>, <volume>5</volume>(<issue>3</issue>):<fpage>e24</fpage> <month>March</month> <year>2020</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>[13]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Güçlü Umut"><given-names>Umut</given-names> <surname>Güçlü</surname></string-name> and <string-name name-style="western" hwp:sortable="van Gerven Marcel AJ"><given-names>Marcel AJ</given-names> <surname>van Gerven</surname></string-name>. <chapter-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</chapter-title>. <source hwp:id="source-13">Journal of Neuroscience</source>, <volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>10014</lpage> <year>2015</year>. <collab hwp:id="collab-5">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2 xref-ref-14-3 xref-ref-14-4 xref-ref-14-5 xref-ref-14-6 xref-ref-14-7"><label>[14]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Schrimpf Martin"><given-names>Martin</given-names> <surname>Schrimpf</surname></string-name>, <string-name name-style="western" hwp:sortable="Kubilius Jonas"><given-names>Jonas</given-names> <surname>Kubilius</surname></string-name>, <string-name name-style="western" hwp:sortable="Hong Ha"><given-names>Ha</given-names> <surname>Hong</surname></string-name>, <string-name name-style="western" hwp:sortable="Majaj Najib J."><given-names>Najib J.</given-names> <surname>Majaj</surname></string-name>, <string-name name-style="western" hwp:sortable="Rajalingham Rishi"><given-names>Rishi</given-names> <surname>Rajalingham</surname></string-name>, <string-name name-style="western" hwp:sortable="Issa Elias B."><given-names>Elias B.</given-names> <surname>Issa</surname></string-name>, <string-name name-style="western" hwp:sortable="Kar Kohitij"><given-names>Kohitij</given-names> <surname>Kar</surname></string-name>, <string-name name-style="western" hwp:sortable="Bashivan Pouya"><given-names>Pouya</given-names> <surname>Bashivan</surname></string-name>, <string-name name-style="western" hwp:sortable="Prescott-Roy Jonathan"><given-names>Jonathan</given-names> <surname>Prescott-Roy</surname></string-name>, and <string-name name-style="western" hwp:sortable="Schmidt Kailyn"><given-names>Kailyn</given-names> <surname>Schmidt</surname></string-name>. <chapter-title>Brain-score: Which artificial neural network for object recognition is most brain-like?</chapter-title> <source hwp:id="source-14">BioRxiv</source>, page <fpage>407007</fpage> <year>2018</year>. <collab hwp:id="collab-6">Publisher</collab>: <publisher-name>Cold Spring Harbor Laboratory</publisher-name>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><label>[15]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Konkle Talia"><given-names>Talia</given-names> <surname>Konkle</surname></string-name> and <string-name name-style="western" hwp:sortable="Alvarez George"><given-names>George</given-names> <surname>Alvarez</surname></string-name>. <chapter-title>Deepnets do not need category supervision to predict visual system responses to objects</chapter-title>. <source hwp:id="source-15">Journal of Vision</source>, <volume>20</volume>(<issue>11</issue>):<fpage>498</fpage>–<lpage>498</lpage> <month>October</month> <year>2020</year>. <collab hwp:id="collab-7">Publisher</collab>: <publisher-name>The Association for Research in Vision and Ophthalmology</publisher-name>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2"><label>[16]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Zhuang Chengxu"><given-names>Chengxu</given-names> <surname>Zhuang</surname></string-name>, <string-name name-style="western" hwp:sortable="Yan Siming"><given-names>Siming</given-names> <surname>Yan</surname></string-name>, <string-name name-style="western" hwp:sortable="Nayebi Aran"><given-names>Aran</given-names> <surname>Nayebi</surname></string-name>, <string-name name-style="western" hwp:sortable="Schrimpf Martin"><given-names>Martin</given-names> <surname>Schrimpf</surname></string-name>, <string-name name-style="western" hwp:sortable="Frank Michael C."><given-names>Michael C.</given-names> <surname>Frank</surname></string-name>, <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>, and <string-name name-style="western" hwp:sortable="Yamins Daniel L. K."><given-names>Daniel L. K.</given-names> <surname>Yamins</surname></string-name>. <chapter-title>Unsupervised neural network models of the ventral visual stream</chapter-title>. <source hwp:id="source-16">Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>3</issue>), <month>January</month> <year>2021</year>. <collab hwp:id="collab-8">Publisher</collab>: <publisher-name>National Academy of Sciences Section: Biological Sciences</publisher-name>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3"><label>[17]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Richards Blake A."><given-names>Blake A.</given-names> <surname>Richards</surname></string-name>, <string-name name-style="western" hwp:sortable="Lillicrap Timothy P."><given-names>Timothy P.</given-names> <surname>Lillicrap</surname></string-name>, <string-name name-style="western" hwp:sortable="Beaudoin Philippe"><given-names>Philippe</given-names> <surname>Beaudoin</surname></string-name>, <string-name name-style="western" hwp:sortable="Bengio Yoshua"><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <string-name name-style="western" hwp:sortable="Bogacz Rafal"><given-names>Rafal</given-names> <surname>Bogacz</surname></string-name>, <string-name name-style="western" hwp:sortable="Christensen Amelia"><given-names>Amelia</given-names> <surname>Christensen</surname></string-name>, <string-name name-style="western" hwp:sortable="Clopath Claudia"><given-names>Claudia</given-names> <surname>Clopath</surname></string-name>, <string-name name-style="western" hwp:sortable="Costa Rui Ponte"><given-names>Rui Ponte</given-names> <surname>Costa</surname></string-name>, <string-name name-style="western" hwp:sortable="de Berker Archy"><given-names>Archy</given-names> <surname>de Berker</surname></string-name>, and <string-name name-style="western" hwp:sortable="Ganguli Surya"><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>. <chapter-title>A deep learning framework for neuroscience</chapter-title>. <source hwp:id="source-17">Nature neuroscience</source>, <volume>22</volume>(<issue>11</issue>):<fpage>1761</fpage>–<lpage>1770</lpage> <year>2019</year>. <collab hwp:id="collab-9">Publisher</collab>: <publisher-name>Nature Publishing Group</publisher-name>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Kell Alexander J. E."><given-names>Alexander J. E.</given-names> <surname>Kell</surname></string-name>, <string-name name-style="western" hwp:sortable="Yamins Daniel L. K."><given-names>Daniel L. K.</given-names> <surname>Yamins</surname></string-name>, <string-name name-style="western" hwp:sortable="Shook Erica N."><given-names>Erica N.</given-names> <surname>Shook</surname></string-name>, <string-name name-style="western" hwp:sortable="Norman-Haignere Sam V."><given-names>Sam V.</given-names> <surname>Norman-Haignere</surname></string-name>, and <string-name name-style="western" hwp:sortable="McDermott Josh H."><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name>. <article-title hwp:id="article-title-9">A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title>. <source hwp:id="source-18">Neuron</source>, <volume>98</volume>(<issue>3</issue>):<fpage>630</fpage>–<lpage>644</lpage>.e16 <month>May</month> <year>2018</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>[19]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Kell Alexander JE"><given-names>Alexander JE</given-names> <surname>Kell</surname></string-name> and <string-name name-style="western" hwp:sortable="McDermott Josh H."><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name>. <chapter-title>Deep neural network models of sensory systems: windows onto the role of task constraints</chapter-title>. <source hwp:id="source-19">Current opinion in neurobiology</source>, <volume>55</volume>:<fpage>121</fpage>–<lpage>132</lpage> <year>2019</year>. <collab hwp:id="collab-10">Publisher</collab>: <publisher-name>Elsevier</publisher-name>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2 xref-ref-20-3"><label>[20]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Pack Christopher C."><given-names>Christopher C.</given-names> <surname>Pack</surname></string-name>, <string-name name-style="western" hwp:sortable="Conway Bevil R."><given-names>Bevil R.</given-names> <surname>Conway</surname></string-name>, <string-name name-style="western" hwp:sortable="Born Richard T."><given-names>Richard T.</given-names> <surname>Born</surname></string-name>, and <string-name name-style="western" hwp:sortable="Livingstone Margaret S."><given-names>Margaret S.</given-names> <surname>Livingstone</surname></string-name>. <chapter-title>Spatiotemporal structure of nonlinear subunits in macaque visual cortex</chapter-title>. <source hwp:id="source-20">Journal of Neuroscience</source>, <volume>26</volume>(<issue>3</issue>):<fpage>893</fpage>–<lpage>907</lpage> <year>2006</year>. <collab hwp:id="collab-11">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3 xref-ref-21-4"><label>[21]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Movshon J."><given-names>J.</given-names> <surname>Movshon</surname></string-name>, <string-name name-style="western" hwp:sortable="Adelson E. H."><given-names>E. H.</given-names> <surname>Adelson</surname></string-name>, <string-name name-style="western" hwp:sortable="Gizzi M. S."><given-names>M. S.</given-names> <surname>Gizzi</surname></string-name>, and <string-name name-style="western" hwp:sortable="Newsome William T."><given-names>William T.</given-names> <surname>Newsome</surname></string-name>. <chapter-title>The analysis of moving visual patterns</chapter-title>. <source hwp:id="source-21">Pattern recognition mechanisms</source>, pages <fpage>117</fpage>–<lpage>151</lpage> <year>1985</year>. <collab hwp:id="collab-12">Publisher</collab>: <publisher-name>Vatican Press</publisher-name>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>[22]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Duffy Charles J."><given-names>Charles J.</given-names> <surname>Duffy</surname></string-name> and <string-name name-style="western" hwp:sortable="Wurtz Robert H."><given-names>Robert H.</given-names> <surname>Wurtz</surname></string-name>. <chapter-title>Sensitivity of MST neurons to optic flow stimuli. I. A continuum of response selectivity to large-field stimuli</chapter-title>. <source hwp:id="source-22">Journal of neurophysiology</source>, <volume>65</volume>(<issue>6</issue>):<fpage>1329</fpage>–<lpage>1345</lpage> <year>1991</year>. <collab hwp:id="collab-13">Publisher</collab>: <publisher-name>American Physiological Society Bethesda, MD</publisher-name>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2 xref-ref-23-3"><label>[23]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Giese Martin A."><given-names>Martin A.</given-names> <surname>Giese</surname></string-name> and <string-name name-style="western" hwp:sortable="Poggio Tomaso"><given-names>Tomaso</given-names> <surname>Poggio</surname></string-name>. <article-title hwp:id="article-title-10">Neural mechanisms for the recognition of biological movements</article-title>. <source hwp:id="source-23">Nature Reviews. Neuroscience</source>, <volume>4</volume>(<issue>3</issue>):<fpage>179</fpage>–<lpage>192</lpage> <month>March</month> <year>2003</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.24" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Jhuang Hueihan"><given-names>Hueihan</given-names> <surname>Jhuang</surname></string-name>. <source hwp:id="source-24">Dorsal stream : from algorithm to neuroscience. Thesis, Massachusetts Institute of Technology</source>, <year>2011</year>. Accepted: 2011-09-27T18:31:39Z.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Jehee Janneke F. M."><given-names>Janneke F. M.</given-names> <surname>Jehee</surname></string-name>, <string-name name-style="western" hwp:sortable="Rothkopf Constantin"><given-names>Constantin</given-names> <surname>Rothkopf</surname></string-name>, <string-name name-style="western" hwp:sortable="Beck Jeffrey M."><given-names>Jeffrey M.</given-names> <surname>Beck</surname></string-name>, and <string-name name-style="western" hwp:sortable="Ballard Dana H."><given-names>Dana H.</given-names> <surname>Ballard</surname></string-name>. <article-title hwp:id="article-title-11">Learning receptive fields using predictive feedback</article-title>. <source hwp:id="source-25">Journal of Physiology-Paris</source>, <volume>100</volume>(<issue>1</issue>):<fpage>125</fpage>–<lpage>132</lpage> <month>July</month> <year>2006</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><label>[26]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Rideaux Reuben"><given-names>Reuben</given-names> <surname>Rideaux</surname></string-name> and <string-name name-style="western" hwp:sortable="Welchman Andrew E."><given-names>Andrew E.</given-names> <surname>Welchman</surname></string-name>. <chapter-title>But still it moves: static image statistics underlie how we see motion</chapter-title>. <source hwp:id="source-26">Journal of Neuroscience</source>, <volume>40</volume>(<issue>12</issue>):<fpage>2538</fpage>–<lpage>2552</lpage> <year>2020</year>. <collab hwp:id="collab-14">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><label>[27]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Newsome William T."><given-names>William T.</given-names> <surname>Newsome</surname></string-name>, <string-name name-style="western" hwp:sortable="Wurtz Robert H."><given-names>Robert H.</given-names> <surname>Wurtz</surname></string-name>, and <string-name name-style="western" hwp:sortable="Komatsu Hidehiko"><given-names>Hidehiko</given-names> <surname>Komatsu</surname></string-name>. <chapter-title>Relation of cortical areas MT and MST to pursuit eye movements. II. Differentiation of retinal from extraretinal inputs</chapter-title>. <source hwp:id="source-27">Journal of neurophysiology</source>, <volume>60</volume>(<issue>2</issue>):<fpage>604</fpage>–<lpage>620</lpage> <year>1988</year>. <collab hwp:id="collab-15">Publisher</collab>: <publisher-name>American Physiological Society Bethesda, MD</publisher-name>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2"><label>[28]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Gu Yong"><given-names>Yong</given-names> <surname>Gu</surname></string-name>, <string-name name-style="western" hwp:sortable="Watkins Paul V."><given-names>Paul V.</given-names> <surname>Watkins</surname></string-name>, <string-name name-style="western" hwp:sortable="Angelaki Dora E."><given-names>Dora E.</given-names> <surname>Angelaki</surname></string-name>, and <string-name name-style="western" hwp:sortable="DeAngelis Gregory C."><given-names>Gregory C.</given-names> <surname>DeAngelis</surname></string-name>. <chapter-title>Visual and nonvisual contributions to three-dimensional heading selectivity in the medial superior temporal area</chapter-title>. <source hwp:id="source-28">Journal of Neuroscience</source>, <volume>26</volume>(<issue>1</issue>):<fpage>73</fpage>–<lpage>85</lpage> <year>2006</year>. <collab hwp:id="collab-16">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Andersen R. A."><given-names>R. A.</given-names> <surname>Andersen</surname></string-name>, <string-name name-style="western" hwp:sortable="Snyder L. H."><given-names>L. H.</given-names> <surname>Snyder</surname></string-name>, <string-name name-style="western" hwp:sortable="Bradley D. C."><given-names>D. C.</given-names> <surname>Bradley</surname></string-name>, and <string-name name-style="western" hwp:sortable="Xing J."><given-names>J.</given-names> <surname>Xing</surname></string-name>. <article-title hwp:id="article-title-12">Multimodal representation of space in the posterior parietal cortex and its use in planning movements</article-title>. <source hwp:id="source-29">Annual Review of Neuroscience</source>, <volume>20</volume>:<fpage>303</fpage>–<lpage>330</lpage> <year>1997</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><label>[30]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="van den Oord Aaron"><given-names>Aaron</given-names> <surname>van den Oord</surname></string-name>, <string-name name-style="western" hwp:sortable="Li Yazhe"><given-names>Yazhe</given-names> <surname>Li</surname></string-name>, and <string-name name-style="western" hwp:sortable="Vinyals Oriol"><given-names>Oriol</given-names> <surname>Vinyals</surname></string-name>. <source hwp:id="source-30">Representation Learning with Contrastive Predictive Coding</source>. <pub-id pub-id-type="arxiv">1807.03748</pub-id> [cs, stat], <month>January</month> <year>2019</year>. arXiv: 1807.03748.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Zeki Semir M."><given-names>Semir M.</given-names> <surname>Zeki</surname></string-name>. <chapter-title>Functional organization of a visual area in the posterior bank of the superior temporal sulcus of the rhesus monkey</chapter-title>. <source hwp:id="source-31">The Journal of physiology</source>, <volume>236</volume>(<issue>3</issue>):<fpage>549</fpage>–<lpage>573</lpage> <year>1974</year>. <collab hwp:id="collab-17">Publisher</collab>: <publisher-name>Wiley Online Library</publisher-name>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Khawaja Farhan A."><given-names>Farhan A.</given-names> <surname>Khawaja</surname></string-name>, <string-name name-style="western" hwp:sortable="Tsui James MG"><given-names>James MG</given-names> <surname>Tsui</surname></string-name>, and <string-name name-style="western" hwp:sortable="Pack Christopher C."><given-names>Christopher C.</given-names> <surname>Pack</surname></string-name>. <chapter-title>Pattern motion selectivity of spiking outputs and local field potentials in macaque visual cortex</chapter-title>. <source hwp:id="source-32">Journal of Neuroscience</source>, <volume>29</volume>(<issue>43</issue>):<fpage>13702</fpage>–<lpage>13709</lpage> <year>2009</year>. <collab hwp:id="collab-18">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Huk Alexander C."><given-names>Alexander C.</given-names> <surname>Huk</surname></string-name> and <string-name name-style="western" hwp:sortable="Shadlen Michael N."><given-names>Michael N.</given-names> <surname>Shadlen</surname></string-name>. <chapter-title>Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making</chapter-title>. <source hwp:id="source-33">Journal of Neuroscience</source>, <volume>25</volume>(<issue>45</issue>):<fpage>10420</fpage>–<lpage>10436</lpage> <year>2005</year>. <collab hwp:id="collab-19">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>[34]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Pesaran Bijan"><given-names>Bijan</given-names> <surname>Pesaran</surname></string-name>, <string-name name-style="western" hwp:sortable="Pezaris John S."><given-names>John S.</given-names> <surname>Pezaris</surname></string-name>, <string-name name-style="western" hwp:sortable="Sahani Maneesh"><given-names>Maneesh</given-names> <surname>Sahani</surname></string-name>, <string-name name-style="western" hwp:sortable="Mitra Partha P."><given-names>Partha P.</given-names> <surname>Mitra</surname></string-name>, and <string-name name-style="western" hwp:sortable="Andersen Richard A."><given-names>Richard A.</given-names> <surname>Andersen</surname></string-name>. <chapter-title>Temporal structure in neuronal activity during working memory in macaque parietal cortex</chapter-title>. <source hwp:id="source-34">Nature neuroscience</source>, <volume>5</volume>(<issue>8</issue>):<fpage>805</fpage>–<lpage>811</lpage> <year>2002</year>. <collab hwp:id="collab-20">Publisher</collab>: <publisher-name>Nature Publishing Group</publisher-name>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Schiller P. H."><given-names>P. H.</given-names> <surname>Schiller</surname></string-name>. <article-title hwp:id="article-title-13">The effects of V4 and middle temporal (MT) area lesions on visual performance in the rhesus monkey</article-title>. <source hwp:id="source-35">Visual Neuroscience</source>, <volume>10</volume>(<issue>4</issue>):<fpage>717</fpage>–<lpage>746</lpage> <month>August</month> <year>1993</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>[36]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Nowlan Steven J."><given-names>Steven J.</given-names> <surname>Nowlan</surname></string-name> and <string-name name-style="western" hwp:sortable="Sejnowski Terrence J."><given-names>Terrence J.</given-names> <surname>Sejnowski</surname></string-name>. <chapter-title>A selection model for motion processing in area MT of primates</chapter-title>. <source hwp:id="source-36">Journal of Neuroscience</source>, <volume>15</volume>(<issue>2</issue>):<fpage>1195</fpage>–<lpage>1214</lpage> <year>1995</year>. <collab hwp:id="collab-21">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>[37]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Zemel Richard S."><given-names>Richard S.</given-names> <surname>Zemel</surname></string-name> and <string-name name-style="western" hwp:sortable="Sejnowski Terrence J."><given-names>Terrence J.</given-names> <surname>Sejnowski</surname></string-name>. <chapter-title>A model for encoding multiple object motions and self-motion in area MST of primate visual cortex</chapter-title>. <source hwp:id="source-37">Journal of Neuroscience</source>, <volume>18</volume>(<issue>1</issue>):<fpage>531</fpage>–<lpage>547</lpage> <year>1998</year>. <collab hwp:id="collab-22">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>[38]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Galletti Claudio"><given-names>Claudio</given-names> <surname>Galletti</surname></string-name> and <string-name name-style="western" hwp:sortable="Fattori Patrizia"><given-names>Patrizia</given-names> <surname>Fattori</surname></string-name>. <article-title hwp:id="article-title-14">Neuronal mechanisms for detection of motion in the field of view</article-title>. <source hwp:id="source-38">Neuropsychologia</source>, <volume>41</volume>(<issue>13</issue>):<fpage>1717</fpage>–<lpage>1727</lpage> <month>January</month> <year>2003</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>[39]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Perrone J. A."><given-names>J. A.</given-names> <surname>Perrone</surname></string-name>. <article-title hwp:id="article-title-15">Model for the computation of self-motion in biological systems</article-title>. <source hwp:id="source-39">Journal of the Optical Society of America A, Optics and Image Science</source>, <volume>9</volume>(<issue>2</issue>):<fpage>177</fpage>–<lpage>194</lpage> <year>1992</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>[40]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Lappe M."><given-names>M.</given-names> <surname>Lappe</surname></string-name>. <article-title hwp:id="article-title-16">Computational mechanisms for optic flow analysis in primate cortex</article-title>. <source hwp:id="source-40">International Review of Neurobiology</source>, pages <fpage>235</fpage>–<lpage>268</lpage> <year>2000</year>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>[41]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Gu Yong"><given-names>Yong</given-names> <surname>Gu</surname></string-name>, <string-name name-style="western" hwp:sortable="Fetsch Christopher R."><given-names>Christopher R.</given-names> <surname>Fetsch</surname></string-name>, <string-name name-style="western" hwp:sortable="Adeyemo Babatunde"><given-names>Babatunde</given-names> <surname>Adeyemo</surname></string-name>, <string-name name-style="western" hwp:sortable="DeAngelis Gregory C."><given-names>Gregory C.</given-names> <surname>DeAngelis</surname></string-name>, and <string-name name-style="western" hwp:sortable="Angelaki Dora E."><given-names>Dora E.</given-names> <surname>Angelaki</surname></string-name>. <article-title hwp:id="article-title-17">Decoding of MSTd population activity accounts for variations in the precision of heading perception</article-title>. <source hwp:id="source-41">Neuron</source>, <volume>66</volume>(<issue>4</issue>):<fpage>596</fpage>–<lpage>609</lpage> <year>2010</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>[42]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Layton Oliver W."><given-names>Oliver W.</given-names> <surname>Layton</surname></string-name> and <string-name name-style="western" hwp:sortable="Browning N. Andrew"><given-names>N. Andrew</given-names> <surname>Browning</surname></string-name>. <chapter-title>A unified model of heading and path perception in primate MSTd</chapter-title>. <source hwp:id="source-42">PLoS Comput Biol</source>, <volume>10</volume>(<issue>2</issue>):<fpage>e1003476</fpage> <year>2014</year>. <collab hwp:id="collab-23">Publisher</collab>: <publisher-name>Public Library of Science</publisher-name>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1 xref-ref-43-2"><label>[43]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Rust Nicole C."><given-names>Nicole C.</given-names> <surname>Rust</surname></string-name>, <string-name name-style="western" hwp:sortable="Schwartz Odelia"><given-names>Odelia</given-names> <surname>Schwartz</surname></string-name>, <string-name name-style="western" hwp:sortable="Movshon J. Anthony"><given-names>J. Anthony</given-names> <surname>Movshon</surname></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli Eero P."><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name>. <article-title hwp:id="article-title-18">Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source hwp:id="source-43">Neuron</source>, <volume>46</volume>(<issue>6</issue>):<fpage>945</fpage>–<lpage>956</lpage> <year>2005</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2 xref-ref-44-3"><label>[44]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Cui Yuwei"><given-names>Yuwei</given-names> <surname>Cui</surname></string-name>, <string-name name-style="western" hwp:sortable="Liu Liu D."><given-names>Liu D.</given-names> <surname>Liu</surname></string-name>, <string-name name-style="western" hwp:sortable="Khawaja Farhan A."><given-names>Farhan A.</given-names> <surname>Khawaja</surname></string-name>, <string-name name-style="western" hwp:sortable="Pack Christopher C."><given-names>Christopher C.</given-names> <surname>Pack</surname></string-name>, and <string-name name-style="western" hwp:sortable="Butts Daniel A."><given-names>Daniel A.</given-names> <surname>Butts</surname></string-name>. <chapter-title>Diverse suppressive influences in area MT and selectivity to complex motion features</chapter-title>. <source hwp:id="source-44">Journal of Neuroscience</source>, <volume>33</volume>(<issue>42</issue>):<fpage>16715</fpage>–<lpage>16728</lpage> <year>2013</year>. <collab hwp:id="collab-24">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>[45]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Rust Nicole C."><given-names>Nicole C.</given-names> <surname>Rust</surname></string-name>, <string-name name-style="western" hwp:sortable="Mante Valerio"><given-names>Valerio</given-names> <surname>Mante</surname></string-name>, <string-name name-style="western" hwp:sortable="Simoncelli Eero P."><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name>, and <string-name name-style="western" hwp:sortable="Movshon J. Anthony"><given-names>J. Anthony</given-names> <surname>Movshon</surname></string-name>. <chapter-title>How MT cells analyze the motion of visual patterns</chapter-title>. <source hwp:id="source-45">Nature neuroscience</source>, <volume>9</volume>(<issue>11</issue>):<fpage>1421</fpage>–<lpage>1431</lpage> <year>2006</year>. <collab hwp:id="collab-25">Publisher</collab>: <publisher-name>Nature Publishing Group</publisher-name>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2"><label>[46]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Nishimoto Shinji"><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name> and <string-name name-style="western" hwp:sortable="Gallant Jack L."><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name>. <chapter-title>A three-dimensional spatiotemporal receptive field model explains responses of area MT neurons to naturalistic movies</chapter-title>. <source hwp:id="source-46">Journal of Neuroscience</source>, <volume>31</volume>(<issue>41</issue>):<fpage>14551</fpage>–<lpage>14564</lpage> <year>2011</year>. <collab hwp:id="collab-26">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2 xref-ref-47-3 xref-ref-47-4"><label>[47]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Mineault Patrick J."><given-names>Patrick J.</given-names> <surname>Mineault</surname></string-name>, <string-name name-style="western" hwp:sortable="Khawaja Farhan A."><given-names>Farhan A.</given-names> <surname>Khawaja</surname></string-name>, <string-name name-style="western" hwp:sortable="Butts Daniel A."><given-names>Daniel A.</given-names> <surname>Butts</surname></string-name>, and <string-name name-style="western" hwp:sortable="Pack Christopher C."><given-names>Christopher C.</given-names> <surname>Pack</surname></string-name>. <chapter-title>Hierarchical processing of complex motion along the primate dorsal visual pathway</chapter-title>. <source hwp:id="source-47">Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>16</issue>):<fpage>E972</fpage>–<lpage>E980</lpage> <year>2012</year>. <collab hwp:id="collab-27">Publisher</collab>: <publisher-name>National Acad Sciences</publisher-name>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>[48]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Shah Shital"><given-names>Shital</given-names> <surname>Shah</surname></string-name>, <string-name name-style="western" hwp:sortable="Dey Debadeepta"><given-names>Debadeepta</given-names> <surname>Dey</surname></string-name>, <string-name name-style="western" hwp:sortable="Lovett Chris"><given-names>Chris</given-names> <surname>Lovett</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kapoor Ashish"><given-names>Ashish</given-names> <surname>Kapoor</surname></string-name>. <article-title hwp:id="article-title-19">AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles</article-title>. <source hwp:id="source-48">In Field and Service Robotics</source>, <year>2017</year>. _eprint: <pub-id pub-id-type="arxiv">1705.05065</pub-id>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>[49]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Feichtenhofer Christoph"><given-names>Christoph</given-names> <surname>Feichtenhofer</surname></string-name>, <string-name name-style="western" hwp:sortable="Fan Haoqi"><given-names>Haoqi</given-names> <surname>Fan</surname></string-name>, <string-name name-style="western" hwp:sortable="Malik Jitendra"><given-names>Jitendra</given-names> <surname>Malik</surname></string-name>, and <string-name name-style="western" hwp:sortable="He Kaiming"><given-names>Kaiming</given-names> <surname>He</surname></string-name>. <article-title hwp:id="article-title-20">Slowfast networks for video recognition</article-title>. <source hwp:id="source-49">In Proceedings of the IEEE/CVF International Conference on Computer Vision</source>, pages <fpage>6202</fpage>–<lpage>6211</lpage> <year>2019</year>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>[50]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.50" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Du Tran Heng Wang"><surname>Du Tran</surname>, <given-names>Heng Wang</given-names></string-name>, <string-name name-style="western" hwp:sortable="Torresani Lorenzo"><given-names>Lorenzo</given-names> <surname>Torresani</surname></string-name>, <string-name name-style="western" hwp:sortable="Ray Jamie"><given-names>Jamie</given-names> <surname>Ray</surname></string-name>, <string-name name-style="western" hwp:sortable="LeCun Yann"><given-names>Yann</given-names> <surname>LeCun</surname></string-name>, and <string-name name-style="western" hwp:sortable="Paluri Manohar"><given-names>Manohar</given-names> <surname>Paluri</surname></string-name>. <article-title hwp:id="article-title-21">A closer look at spatiotemporal convolutions for action recognition</article-title>. <source hwp:id="source-50">In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</source>, pages <fpage>6450</fpage>–<lpage>6459</lpage> <year>2018</year>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><label>[51]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Wu Michael C.-K."><given-names>Michael C.-K.</given-names> <surname>Wu</surname></string-name>, <string-name name-style="western" hwp:sortable="David Stephen V."><given-names>Stephen V.</given-names> <surname>David</surname></string-name>, and <string-name name-style="western" hwp:sortable="Gallant Jack L."><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name>. <chapter-title>Complete functional characterization of sensory neurons by system identification</chapter-title>. <source hwp:id="source-51">Annu. Rev. Neurosci</source>., <volume>29</volume>:<fpage>477</fpage>–<lpage>505</lpage> <year>2006</year>. <collab hwp:id="collab-28">Publisher</collab>: <publisher-name>Annual Reviews</publisher-name>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>[52]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Bair Wyeth"><given-names>Wyeth</given-names> <surname>Bair</surname></string-name> and <string-name name-style="western" hwp:sortable="Koch Christof"><given-names>Christof</given-names> <surname>Koch</surname></string-name>. <chapter-title>Temporal precision of spike trains in extrastriate cortex of the behaving macaque monkey</chapter-title>. <source hwp:id="source-52">Neural computation</source>, <volume>8</volume>(<issue>6</issue>):<fpage>1185</fpage>–<lpage>1202</lpage> <year>1996</year>. <collab hwp:id="collab-29">Publisher</collab>: <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>[53]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.53" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Sahani Maneesh"><given-names>Maneesh</given-names> <surname>Sahani</surname></string-name> and <string-name name-style="western" hwp:sortable="Linden Jennifer F."><given-names>Jennifer F.</given-names> <surname>Linden</surname></string-name>. <chapter-title>How linear are auditory cortical responses?</chapter-title> <source hwp:id="source-53">Advances in neural information processing systems</source>, pages <fpage>125</fpage>–<lpage>132</lpage> <year>2003</year>. <collab hwp:id="collab-30">Publisher</collab>: <publisher-name>MIT</publisher-name>; 1998.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>[54]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Han Tengda"><given-names>Tengda</given-names> <surname>Han</surname></string-name>, <string-name name-style="western" hwp:sortable="Xie Weidi"><given-names>Weidi</given-names> <surname>Xie</surname></string-name>, and <string-name name-style="western" hwp:sortable="Zisserman Andrew"><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <source hwp:id="source-54">Video Representation Learning by Dense Predictive Coding</source>. <pub-id pub-id-type="arxiv">1909.04656</pub-id> [cs], <month>September</month> <year>2019</year>. arXiv: 1909.04656.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2 xref-ref-55-3"><label>[55]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Livingstone M. S."><given-names>M. S.</given-names> <surname>Livingstone</surname></string-name>. <article-title hwp:id="article-title-22">Mechanisms of direction selectivity in macaque V1</article-title>. <source hwp:id="source-55">Neuron</source>, <volume>20</volume>(<issue>3</issue>):<fpage>509</fpage>–<lpage>526</lpage> <month>March</month> <year>1998</year>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1 xref-ref-56-2"><label>[56]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Rust Nicole C."><given-names>Nicole C.</given-names> <surname>Rust</surname></string-name>, <string-name name-style="western" hwp:sortable="Schwartz Odelia"><given-names>Odelia</given-names> <surname>Schwartz</surname></string-name>, <string-name name-style="western" hwp:sortable="Movshon J. Anthony"><given-names>J. Anthony</given-names> <surname>Movshon</surname></string-name>, and <string-name name-style="western" hwp:sortable="Simoncelli Eero"><given-names>Eero</given-names> <surname>Simoncelli</surname></string-name>. <chapter-title>Spike-triggered characterization of excitatory and suppressive stimulus dimensions in monkey V1</chapter-title>. <source hwp:id="source-56">Neurocomputing</source>, <volume>58</volume>:<fpage>793</fpage>–<lpage>799</lpage> <year>2004</year>. <collab hwp:id="collab-31">Publisher</collab>: <publisher-name>Elsevier</publisher-name>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><label>[57]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="De Valois Russell L."><given-names>Russell L.</given-names> <surname>De Valois</surname></string-name>, <string-name name-style="western" hwp:sortable="Yund E. William"><given-names>E. William</given-names> <surname>Yund</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hepler Norva"><given-names>Norva</given-names> <surname>Hepler</surname></string-name>. <article-title hwp:id="article-title-23">The orientation and direction selectivity of cells in macaque visual cortex</article-title>. <source hwp:id="source-57">Vision Research</source>, <volume>22</volume>(<issue>5</issue>):<fpage>531</fpage>–<lpage>544</lpage> <month>January</month> <year>1982</year>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>[58]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Movshon J. Anthony"><given-names>J. Anthony</given-names> <surname>Movshon</surname></string-name> and <string-name name-style="western" hwp:sortable="Newsome William T."><given-names>William T.</given-names> <surname>Newsome</surname></string-name>. <chapter-title>Visual response properties of striate cortical neurons projecting to area MT in macaque monkeys</chapter-title>. <source hwp:id="source-58">Journal of Neuroscience</source>, <volume>16</volume>(<issue>23</issue>):<fpage>7733</fpage>–<lpage>7741</lpage> <year>1996</year>. <collab hwp:id="collab-32">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>[59]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Perrone J. A."><given-names>J. A.</given-names> <surname>Perrone</surname></string-name> and <string-name name-style="western" hwp:sortable="Thiele A."><given-names>A.</given-names> <surname>Thiele</surname></string-name>. <article-title hwp:id="article-title-24">Speed skills: measuring the visual speed analyzing properties of primate MT neurons</article-title>. <source hwp:id="source-59">Nat Neurosci</source>, <volume>4</volume>(<issue>5</issue>):<fpage>526</fpage>–<lpage>32</lpage> <month>May</month> <year>2001</year>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1 xref-ref-60-2"><label>[60]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Churchland Mark M."><given-names>Mark M.</given-names> <surname>Churchland</surname></string-name>, <string-name name-style="western" hwp:sortable="Priebe Nicholas J."><given-names>Nicholas J.</given-names> <surname>Priebe</surname></string-name>, and <string-name name-style="western" hwp:sortable="Lisberger Stephen G."><given-names>Stephen G.</given-names> <surname>Lisberger</surname></string-name>. <chapter-title>Comparison of the Spatial Limits on Direction Selectivity in Visual Areas MT and V1</chapter-title>. <source hwp:id="source-60">Journal of Neurophysiology</source>, <volume>93</volume>(<issue>3</issue>):<fpage>1235</fpage>–<lpage>1245</lpage> <month>March</month> <year>2005</year>. <collab hwp:id="collab-33">Publisher</collab>: <publisher-name>American Physiological Society</publisher-name>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><label>[61]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.61" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Graziano Michael SA"><given-names>Michael SA</given-names> <surname>Graziano</surname></string-name> and <string-name name-style="western" hwp:sortable="Gross Charles G."><given-names>Charles G.</given-names> <surname>Gross</surname></string-name>. <article-title hwp:id="article-title-25">The representation of extrapersonal space: A possible role for bimodal, visual-tactile neurons</article-title>. <source hwp:id="source-61">The cognitive neurosciences</source>, pages <fpage>1021</fpage>–<lpage>1034</lpage> <year>1995</year>.</citation></ref><ref id="c62" hwp:id="ref-62"><label>[62]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Nauhaus Ian"><given-names>Ian</given-names> <surname>Nauhaus</surname></string-name> and <string-name name-style="western" hwp:sortable="Ringach Dario L."><given-names>Dario L.</given-names> <surname>Ringach</surname></string-name>. <chapter-title>Precise alignment of micromachined electrode arrays with V1 functional maps</chapter-title>. <source hwp:id="source-62">Journal of neurophysiology</source>, <volume>97</volume>(<issue>5</issue>):<fpage>3781</fpage>–<lpage>3789</lpage> <year>2007</year>. <collab hwp:id="collab-34">Publisher</collab>: <publisher-name>American Physiological Society</publisher-name>.</citation></ref><ref id="c63" hwp:id="ref-63"><label>[63]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.63" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Ringach Dario"><given-names>Dario</given-names> <surname>Ringach</surname></string-name> and <string-name name-style="western" hwp:sortable="Nauhaus Ian"><given-names>Ian</given-names> <surname>Nauhaus</surname></string-name>. <source hwp:id="source-63">Single- and multi-unit recordings from monkey primary visual cortex</source>.</citation></ref><ref id="c64" hwp:id="ref-64"><label>[64]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Vinje William E."><given-names>William E.</given-names> <surname>Vinje</surname></string-name> and <string-name name-style="western" hwp:sortable="Gallant Jack L."><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name>. <chapter-title>Sparse coding and decorrelation in primary visual cortex during natural vision</chapter-title>. <source hwp:id="source-64">Science</source>, <volume>287</volume>(<issue>5456</issue>):<fpage>1273</fpage>–<lpage>1276</lpage> <year>2000</year>. <collab hwp:id="collab-35">Publisher</collab>: <publisher-name>American Association for the Advancement of Science</publisher-name>.</citation></ref><ref id="c65" hwp:id="ref-65"><label>[65]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="David Stephen V."><given-names>Stephen V.</given-names> <surname>David</surname></string-name>, <string-name name-style="western" hwp:sortable="Vinje William E."><given-names>William E.</given-names> <surname>Vinje</surname></string-name>, and <string-name name-style="western" hwp:sortable="Gallant Jack L."><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name>. <chapter-title>Natural stimulus statistics alter the receptive field structure of v1 neurons</chapter-title>. <source hwp:id="source-65">Journal of Neuroscience</source>, <volume>24</volume>(<issue>31</issue>):<fpage>6991</fpage>–<lpage>7006</lpage> <year>2004</year>. <collab hwp:id="collab-36">Publisher</collab>: <publisher-name>Soc Neuroscience</publisher-name>.</citation></ref><ref id="c66" hwp:id="ref-66"><label>[66]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.66" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="David Stephen V"><given-names>Stephen V</given-names> <surname>David</surname></string-name>, <string-name name-style="western" hwp:sortable="Vinje William E"><given-names>William E</given-names> <surname>Vinje</surname></string-name>, and <string-name name-style="western" hwp:sortable="Gallant Jack L"><given-names>Jack L</given-names> <surname>Gallant</surname></string-name>. <source hwp:id="source-66">Single electrode recordings from primary visual cortex</source>.</citation></ref><ref id="c67" hwp:id="ref-67"><label>[67]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.67" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Cui Y"><given-names>Y</given-names> <surname>Cui</surname></string-name>, <string-name name-style="western" hwp:sortable="Liu DL"><given-names>DL</given-names> <surname>Liu</surname></string-name>, <string-name name-style="western" hwp:sortable="Khawaja FA"><given-names>FA</given-names> <surname>Khawaja</surname></string-name>, <string-name name-style="western" hwp:sortable="Pack CC"><given-names>CC</given-names> <surname>Pack</surname></string-name>, and <string-name name-style="western" hwp:sortable="Butts DA"><given-names>DA</given-names> <surname>Butts</surname></string-name>. <source hwp:id="source-67">Spiking activity in area MT of awake adult macaques in response to complex motion features</source>.</citation></ref><ref id="c68" hwp:id="ref-68"><label>[68]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.68" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Nishimoto S"><given-names>S</given-names> <surname>Nishimoto</surname></string-name> and <string-name name-style="western" hwp:sortable="Gallant JL"><given-names>JL</given-names> <surname>Gallant</surname></string-name>. <source hwp:id="source-68">Extracellular recordings from area MT of awake macaques in response to naturalistic movies</source>.</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1 xref-ref-69-2"><label>[69]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.69" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Kornblith Simon"><given-names>Simon</given-names> <surname>Kornblith</surname></string-name>, <string-name name-style="western" hwp:sortable="Norouzi Mohammad"><given-names>Mohammad</given-names> <surname>Norouzi</surname></string-name>, <string-name name-style="western" hwp:sortable="Lee Honglak"><given-names>Honglak</given-names> <surname>Lee</surname></string-name>, and <string-name name-style="western" hwp:sortable="Hinton Geoffrey"><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>. <chapter-title>Similarity of neural network representations revisited</chapter-title>. <source hwp:id="source-69">In International Conference on Machine Learning</source>, pages <fpage>3519</fpage>–<lpage>3529</lpage>. <publisher-name>PMLR</publisher-name>, <year>2019</year>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><label>[70]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Cadena Santiago A."><given-names>Santiago A.</given-names> <surname>Cadena</surname></string-name>, <string-name name-style="western" hwp:sortable="Denfield George H."><given-names>George H.</given-names> <surname>Denfield</surname></string-name>, <string-name name-style="western" hwp:sortable="Walker Edgar Y."><given-names>Edgar Y.</given-names> <surname>Walker</surname></string-name>, <string-name name-style="western" hwp:sortable="Gatys Leon A."><given-names>Leon A.</given-names> <surname>Gatys</surname></string-name>, <string-name name-style="western" hwp:sortable="Tolias Andreas S."><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name>, <string-name name-style="western" hwp:sortable="Bethge Matthias"><given-names>Matthias</given-names> <surname>Bethge</surname></string-name>, and <string-name name-style="western" hwp:sortable="Ecker Alexander S."><given-names>Alexander S.</given-names> <surname>Ecker</surname></string-name>. <chapter-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</chapter-title>. <source hwp:id="source-70">PLoS computational biology</source>, <volume>15</volume>(<issue>4</issue>):<fpage>e1006897</fpage> <year>2019</year>. <collab hwp:id="collab-37">Publisher</collab>: <publisher-name>Public Library of Science San Francisco, CA USA</publisher-name>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><label>[71]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Kay Will"><given-names>Will</given-names> <surname>Kay</surname></string-name>, <string-name name-style="western" hwp:sortable="Carreira Joao"><given-names>Joao</given-names> <surname>Carreira</surname></string-name>, <string-name name-style="western" hwp:sortable="Simonyan Karen"><given-names>Karen</given-names> <surname>Simonyan</surname></string-name>, <string-name name-style="western" hwp:sortable="Zhang Brian"><given-names>Brian</given-names> <surname>Zhang</surname></string-name>, <string-name name-style="western" hwp:sortable="Hillier Chloe"><given-names>Chloe</given-names> <surname>Hillier</surname></string-name>, <string-name name-style="western" hwp:sortable="Vijayanarasimhan Sudheendra"><given-names>Sudheendra</given-names> <surname>Vijayanarasimhan</surname></string-name>, <string-name name-style="western" hwp:sortable="Viola Fabio"><given-names>Fabio</given-names> <surname>Viola</surname></string-name>, <string-name name-style="western" hwp:sortable="Green Tim"><given-names>Tim</given-names> <surname>Green</surname></string-name>, <string-name name-style="western" hwp:sortable="Back Trevor"><given-names>Trevor</given-names> <surname>Back</surname></string-name>, <string-name name-style="western" hwp:sortable="Natsev Paul"><given-names>Paul</given-names> <surname>Natsev</surname></string-name>, <string-name name-style="western" hwp:sortable="Suleyman Mustafa"><given-names>Mustafa</given-names> <surname>Suleyman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Zisserman Andrew"><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <source hwp:id="source-71">The Kinetics Human Action Video Dataset</source>. <pub-id pub-id-type="arxiv">1705.06950</pub-id> [cs], <month>May</month> <year>2017</year>. arXiv: 1705.06950.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><label>[72]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Pinto Nicolas"><given-names>Nicolas</given-names> <surname>Pinto</surname></string-name>, <string-name name-style="western" hwp:sortable="Cox David D."><given-names>David D.</given-names> <surname>Cox</surname></string-name>, and <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <chapter-title>Why is Real-World Visual Object Recognition Hard?</chapter-title> <source hwp:id="source-72">PLOS Computational Biology</source>, <volume>4</volume>(<issue>1</issue>):<fpage>e27</fpage> <month>January</month> <year>2008</year>. <collab hwp:id="collab-38">Publisher</collab>: <publisher-name>Public Library of Science</publisher-name>.</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><label>[73]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Olshausen B. A."><given-names>B. A.</given-names> <surname>Olshausen</surname></string-name> and <string-name name-style="western" hwp:sortable="Field D. J."><given-names>D. J.</given-names> <surname>Field</surname></string-name>. <article-title hwp:id="article-title-26">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source hwp:id="source-73">Nature</source>, <volume>381</volume>:<fpage>607</fpage>–<lpage>609</lpage> <year>1996</year>.</citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><label>[74]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Ponce Carlos R."><given-names>Carlos R.</given-names> <surname>Ponce</surname></string-name>, <string-name name-style="western" hwp:sortable="Xiao Will"><given-names>Will</given-names> <surname>Xiao</surname></string-name>, <string-name name-style="western" hwp:sortable="Schade Peter F."><given-names>Peter F.</given-names> <surname>Schade</surname></string-name>, <string-name name-style="western" hwp:sortable="Hartmann Till S."><given-names>Till S.</given-names> <surname>Hartmann</surname></string-name>, <string-name name-style="western" hwp:sortable="Kreiman Gabriel"><given-names>Gabriel</given-names> <surname>Kreiman</surname></string-name>, and <string-name name-style="western" hwp:sortable="Margaret S."><surname>Margaret</surname> <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-27">Living-stone. Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences</article-title>. <source hwp:id="source-74">Cell</source>, <volume>177</volume>(<issue>4</issue>):<fpage>999</fpage>–<lpage>1009</lpage>.e10 <month>May</month> <year>2019</year>.</citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><label>[75]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.75" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Bashivan Pouya"><given-names>Pouya</given-names> <surname>Bashivan</surname></string-name>, <string-name name-style="western" hwp:sortable="Kar Kohitij"><given-names>Kohitij</given-names> <surname>Kar</surname></string-name>, and <string-name name-style="western" hwp:sortable="DiCarlo James J."><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <chapter-title>Neural population control via deep image synthesis</chapter-title>. <source hwp:id="source-75">Science</source>, <volume>364</volume>(<issue>6439</issue>), <month>May</month> <year>2019</year>. <collab hwp:id="collab-39">Publisher</collab>: <publisher-name>American Association for the Advancement of Science Section: Research Article</publisher-name>.</citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><label>[76]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Hardcastle Ben J."><given-names>Ben J.</given-names> <surname>Hardcastle</surname></string-name> and <string-name name-style="western" hwp:sortable="Krapp Holger G."><given-names>Holger G.</given-names> <surname>Krapp</surname></string-name>. <article-title hwp:id="article-title-28">Evolution of Biological Image Stabilization</article-title>. <source hwp:id="source-76">Current biology: CB</source>, <volume>26</volume>(<issue>20</issue>):<fpage>R1010</fpage>–<lpage>R1021</lpage> <month>October</month> <year>2016</year>.</citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1"><label>[77]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Ringach Dario L."><given-names>Dario L.</given-names> <surname>Ringach</surname></string-name>, <string-name name-style="western" hwp:sortable="Sapiro Guillermo"><given-names>Guillermo</given-names> <surname>Sapiro</surname></string-name>, and <string-name name-style="western" hwp:sortable="Shapley Robert"><given-names>Robert</given-names> <surname>Shapley</surname></string-name>. <article-title hwp:id="article-title-29">A subspace reverse-correlation technique for the study of visual neurons</article-title>. <source hwp:id="source-77">Vision research</source>, <volume>37</volume>(<issue>17</issue>):<fpage>2455</fpage>–<lpage>2464</lpage> <year>1997</year>.</citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><label>[78]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.78" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Huang De-An"><given-names>De-An</given-names> <surname>Huang</surname></string-name>, <string-name name-style="western" hwp:sortable="Ramanathan Vignesh"><given-names>Vignesh</given-names> <surname>Ramanathan</surname></string-name>, <string-name name-style="western" hwp:sortable="Mahajan Dhruv"><given-names>Dhruv</given-names> <surname>Mahajan</surname></string-name>, <string-name name-style="western" hwp:sortable="Torresani Lorenzo"><given-names>Lorenzo</given-names> <surname>Torresani</surname></string-name>, <string-name name-style="western" hwp:sortable="Paluri Manohar"><given-names>Manohar</given-names> <surname>Paluri</surname></string-name>, <string-name name-style="western" hwp:sortable="Fei-Fei Li"><given-names>Li</given-names> <surname>Fei-Fei</surname></string-name>, and <string-name name-style="western" hwp:sortable="Carlos Niebles Juan"><given-names>Juan</given-names> <surname>Carlos Niebles</surname></string-name>. <article-title hwp:id="article-title-30">What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets</article-title>. <source hwp:id="source-78">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, pages <fpage>7366</fpage>–<lpage>7375</lpage> <month>June</month> <year>2018</year>.</citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><label>[79]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.79" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Simonyan Karen"><given-names>Karen</given-names> <surname>Simonyan</surname></string-name> and <string-name name-style="western" hwp:sortable="Zisserman Andrew"><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <article-title hwp:id="article-title-31">Two-stream convolutional networks for action recognition in videos</article-title>. <source hwp:id="source-79">In Proceedings of the Neural Information Processing Systems (NIPS)</source>, <year>2014</year>.</citation></ref><ref id="c80" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1"><label>[80]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.80" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="Crasto Nieves"><given-names>Nieves</given-names> <surname>Crasto</surname></string-name>, <string-name name-style="western" hwp:sortable="Weinzaepfel Philippe"><given-names>Philippe</given-names> <surname>Weinzaepfel</surname></string-name>, <string-name name-style="western" hwp:sortable="Alahari Karteek"><given-names>Karteek</given-names> <surname>Alahari</surname></string-name>, and <string-name name-style="western" hwp:sortable="Schmid Cordelia"><given-names>Cordelia</given-names> <surname>Schmid</surname></string-name>. <article-title hwp:id="article-title-32">Mars: Motion-augmented rgb stream for action recognition</article-title>. <source hwp:id="source-80">In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>, pages <fpage>7882</fpage>–<lpage>7891</lpage> <year>2019</year>.</citation></ref><ref id="c81" hwp:id="ref-81" hwp:rev-id="xref-ref-81-1"><label>[81]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.81" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Stroud Jonathan"><given-names>Jonathan</given-names> <surname>Stroud</surname></string-name>, <string-name name-style="western" hwp:sortable="Ross David"><given-names>David</given-names> <surname>Ross</surname></string-name>, <string-name name-style="western" hwp:sortable="Sun Chen"><given-names>Chen</given-names> <surname>Sun</surname></string-name>, <string-name name-style="western" hwp:sortable="Deng Jia"><given-names>Jia</given-names> <surname>Deng</surname></string-name>, and <string-name name-style="western" hwp:sortable="Sukthankar Rahul"><given-names>Rahul</given-names> <surname>Sukthankar</surname></string-name>. <article-title hwp:id="article-title-33">D3d: Distilled 3d networks for video action recognition</article-title>. <source hwp:id="source-81">In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source>, pages <fpage>625</fpage>–<lpage>634</lpage> <year>2020</year>.</citation></ref><ref id="c82" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1"><label>[82]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.82" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Zhang Richard"><given-names>Richard</given-names> <surname>Zhang</surname></string-name>, <string-name name-style="western" hwp:sortable="Isola Phillip"><given-names>Phillip</given-names> <surname>Isola</surname></string-name>, and <string-name name-style="western" hwp:sortable="Alexei A."><surname>Alexei</surname> <given-names>A.</given-names></string-name> <source hwp:id="source-82">Efros. Colorful Image Colorization. In</source> <pub-id pub-id-type="arxiv">1603.08511</pub-id> [cs], <month>October</month> <year>2016</year>. arXiv: 1603.08511.</citation></ref><ref id="c83" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1"><label>[83]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.83" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Owens Andrew"><given-names>Andrew</given-names> <surname>Owens</surname></string-name> and <string-name name-style="western" hwp:sortable="Efros Alexei A."><given-names>Alexei A.</given-names> <surname>Efros</surname></string-name>. <source hwp:id="source-83">Audio-Visual Scene Analysis with Self-Supervised Multisensory Features</source>. <pub-id pub-id-type="arxiv">1804.03641</pub-id> [cs, eess], <month>October</month> <year>2018</year>. arXiv: 1804.03641.</citation></ref><ref id="c84" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1"><label>[84]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Alwassel Humam"><given-names>Humam</given-names> <surname>Alwassel</surname></string-name>, <string-name name-style="western" hwp:sortable="Mahajan Dhruv"><given-names>Dhruv</given-names> <surname>Mahajan</surname></string-name>, <string-name name-style="western" hwp:sortable="Korbar Bruno"><given-names>Bruno</given-names> <surname>Korbar</surname></string-name>, <string-name name-style="western" hwp:sortable="Torresani Lorenzo"><given-names>Lorenzo</given-names> <surname>Torresani</surname></string-name>, <string-name name-style="western" hwp:sortable="Ghanem Bernard"><given-names>Bernard</given-names> <surname>Ghanem</surname></string-name>, and <string-name name-style="western" hwp:sortable="Tran Du"><given-names>Du</given-names> <surname>Tran</surname></string-name>. <source hwp:id="source-84">Self-Supervised Learning by Cross-Modal Audio-Video Clustering</source>. <pub-id pub-id-type="arxiv">1911.12667</pub-id> [cs], <month>October</month> <year>2020</year>. arXiv: 1911.12667.</citation></ref><ref id="c85" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1"><label>[85]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.85" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Crapse Trinity B."><given-names>Trinity B.</given-names> <surname>Crapse</surname></string-name> and <string-name name-style="western" hwp:sortable="Marc A."><surname>Marc</surname> <given-names>A.</given-names></string-name> <chapter-title>Sommer. Corollary discharge across the animal kingdom</chapter-title>. <source hwp:id="source-85">Nature Reviews Neuroscience</source>, <volume>9</volume>(<issue>8</issue>):<fpage>587</fpage>–<lpage>600</lpage> <month>August</month> <year>2008</year>. Number: 8 Publisher: <publisher-name>Nature Publishing Group</publisher-name>.</citation></ref><ref id="c86" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1"><label>[86]</label><citation publication-type="other" citation-type="journal" ref:id="2021.07.09.451701v3.86" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Thompson Jessica AF"><given-names>Jessica AF</given-names> <surname>Thompson</surname></string-name>. <source hwp:id="source-86">Characterizing and comparing acoustic representations in convolutional neural networks and the human auditory system. PhD thesis, Universite de Montreal</source>, <year>2020</year>.</citation></ref><ref id="c87" hwp:id="ref-87" hwp:rev-id="xref-ref-87-1"><label>[87]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.87" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-87"><string-name name-style="western" hwp:sortable="Striedter Georg F."><given-names>Georg F.</given-names> <surname>Striedter</surname></string-name> and <string-name name-style="western" hwp:sortable="Northcutt R. Glenn"><given-names>R. Glenn</given-names> <surname>Northcutt</surname></string-name>. <source hwp:id="source-87">Brains Through Time: A Natural History of Vertebrates</source>. <publisher-name>Oxford University Press</publisher-name>, <year>2019</year>.</citation></ref><ref id="c88" hwp:id="ref-88" hwp:rev-id="xref-ref-88-1"><label>[88]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.88" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-88"><string-name name-style="western" hwp:sortable="Albert Miles Frederick"><given-names>Frederick</given-names> <surname>Albert Miles</surname></string-name> and <string-name name-style="western" hwp:sortable="Wallman Joshua"><given-names>Joshua</given-names> <surname>Wallman</surname></string-name>. <source hwp:id="source-88">Visual Motion and its Role in the Stabilization of Gaze</source>, volume <volume>5</volume>. <publisher-name>Elsevier Science Limited</publisher-name>, <year>1993</year>.</citation></ref><ref id="c89" hwp:id="ref-89" hwp:rev-id="xref-ref-89-1"><label>[89]</label><citation publication-type="book" citation-type="book" ref:id="2021.07.09.451701v3.89" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-89"><string-name name-style="western" hwp:sortable="Le Grand Richard"><given-names>Richard</given-names> <surname>Le Grand</surname></string-name>, <string-name name-style="western" hwp:sortable="Mondloch Catherine J."><given-names>Catherine J.</given-names> <surname>Mondloch</surname></string-name>, <string-name name-style="western" hwp:sortable="Maurer Daphne"><given-names>Daphne</given-names> <surname>Maurer</surname></string-name>, and <string-name name-style="western" hwp:sortable="Brent Henry P."><given-names>Henry P.</given-names> <surname>Brent</surname></string-name>. <chapter-title>Early visual experience and face processing</chapter-title>. <source hwp:id="source-89">Nature</source>, <volume>410</volume>(<issue>6831</issue>):<fpage>890</fpage>–<lpage>890</lpage> <year>2001</year>. <collab hwp:id="collab-40">Publisher</collab>: <publisher-name>Nature Publishing Group</publisher-name>.</citation></ref><ref id="c90" hwp:id="ref-90" hwp:rev-id="xref-ref-90-1"><label>[90]</label><citation publication-type="journal" citation-type="journal" ref:id="2021.07.09.451701v3.90" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-90"><string-name name-style="western" hwp:sortable="Hasson Uri"><given-names>Uri</given-names> <surname>Hasson</surname></string-name>, <string-name name-style="western" hwp:sortable="Nastase Samuel A."><given-names>Samuel A.</given-names> <surname>Nastase</surname></string-name>, and <string-name name-style="western" hwp:sortable="Goldstein Ariel"><given-names>Ariel</given-names> <surname>Goldstein</surname></string-name>. <article-title hwp:id="article-title-34">Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks</article-title>. <source hwp:id="source-90">Neuron</source>, <volume>105</volume>(<issue>3</issue>):<fpage>416</fpage>–<lpage>434</lpage> <month>February</month> <year>2020</year>.</citation></ref></ref-list><sec id="s8" hwp:id="sec-26"><title hwp:id="title-31">Checklist</title><p hwp:id="p-59">The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default <bold>[TODO]</bold> to [Yes], [No], or [N/A]. You are strongly encouraged to include a <bold>justification to your answer</bold>, either by referencing the appropriate section of your paper or providing a brief inline description. For example:</p><list list-type="bullet" hwp:id="list-2"><list-item hwp:id="list-item-5"><p hwp:id="p-60">Did you include the license to the code and datasets? [Yes] See <xref ref-type="sec" rid="s1" hwp:id="xref-sec-1-1" hwp:rel-id="sec-1">Section 1</xref>.</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-61">Did you include the license to the code and datasets? [No] The code and the data are proprietary.</p></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-62">Did you include the license to the code and datasets? [N/A]</p></list-item></list><p hwp:id="p-63">Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.</p><list list-type="order" hwp:id="list-3"><list-item hwp:id="list-item-8"><p hwp:id="p-64">For all authors…</p><list list-type="alpha-lower" hwp:id="list-4"><list-item hwp:id="list-item-9"><p hwp:id="p-65">Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] We have been careful not to overclaim (for example, not claiming that this is the only possible explanation of dorsal stream processing), and we describe limitations of our work in the Limitations section.</p></list-item><list-item hwp:id="list-item-10"><p hwp:id="p-66">Did you describe the limitations of your work? [Yes] Yes, we describe limitations of our work in the Limitations section.</p></list-item><list-item hwp:id="list-item-11"><p hwp:id="p-67">Did you discuss any potential negative societal impacts of your work? [N/A] We don’t anticipate negative social impacts from this work.</p></list-item><list-item hwp:id="list-item-12"><p hwp:id="p-68">Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] Yes, we have read the guidelines and ensured our paper conforms to them.</p></list-item></list></list-item><list-item hwp:id="list-item-13"><p hwp:id="p-69">If you are including theoretical results…</p><list list-type="alpha-lower" hwp:id="list-5"><list-item hwp:id="list-item-14"><p hwp:id="p-70">Did you state the full set of assumptions of all theoretical results? [N/A] We do not have theoretical results.</p></list-item><list-item hwp:id="list-item-15"><p hwp:id="p-71">Did you include complete proofs of all theoretical results? [N/A] We do not have theoretical results.</p></list-item></list></list-item><list-item hwp:id="list-item-16"><p hwp:id="p-72">If you ran experiments…</p><list list-type="alpha-lower" hwp:id="list-6"><list-item hwp:id="list-item-17"><p hwp:id="p-73">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Yes, data, code and instructions are online</p></list-item><list-item hwp:id="list-item-18"><p hwp:id="p-74">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Yes, discussed in the Methods section</p></list-item><list-item hwp:id="list-item-19"><p hwp:id="p-75">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We reported standard error of the mean calculated over cells in <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-3" hwp:rel-id="T3">Table 3</xref>.</p></list-item><list-item hwp:id="list-item-20"><p hwp:id="p-76">Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We reported our compute in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-5-1" hwp:rel-id="sec-5">section 3</xref>.</p></list-item></list></list-item><list-item hwp:id="list-item-21"><p hwp:id="p-77">If you are using existing assets (e.g., code, data, models) or curating/releasing new assets…</p><list list-type="alpha-lower" hwp:id="list-7"><list-item hwp:id="list-item-22"><p hwp:id="p-78">If your work uses existing assets, did you cite the creators? [Yes] Yes, cited in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-5-2" hwp:rel-id="sec-5">section 3</xref> and <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">table 2</xref>.</p></list-item><list-item hwp:id="list-item-23"><p hwp:id="p-79">Did you mention the license of the assets? [Yes] Yes, listed in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-5-3" hwp:rel-id="sec-5">section 3</xref> and <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-3" hwp:rel-id="T2">table 2</xref>.</p></list-item><list-item hwp:id="list-item-24"><p hwp:id="p-80">Did you include any new assets either in the supplemental material or as a URL? [Yes] Yes, the data generated in airsim is available online</p></list-item><list-item hwp:id="list-item-25"><p hwp:id="p-81">Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes] We mention that we received permission from the authors where applicable in <xref ref-type="sec" rid="s3" hwp:id="xref-sec-5-4" hwp:rel-id="sec-5">section 3</xref>. In other cases, reuse was allowed by the license under which data was released.</p></list-item><list-item hwp:id="list-item-26"><p hwp:id="p-82">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] We did not use human data.</p></list-item></list></list-item><list-item hwp:id="list-item-27"><p hwp:id="p-83">If you used crowdsourcing or conducted research with human subjects…</p><list list-type="alpha-lower" hwp:id="list-8"><list-item hwp:id="list-item-28"><p hwp:id="p-84">Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] We did not collect human data.</p></list-item><list-item hwp:id="list-item-29"><p hwp:id="p-85">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] We did not collect human data.</p></list-item><list-item hwp:id="list-item-30"><p hwp:id="p-86">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] We did not collect human data.</p></list-item></list></list-item></list></sec><app-group hwp:id="app-group-1"><app id="app1" hwp:id="app-1"><label>A Appendix</label><table-wrap id="tblS1" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBLS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tblS1</object-id><label>Table S1:</label><caption hwp:id="caption-6"><p hwp:id="p-87">Airsim dataset and training parameters</p></caption><graphic xlink:href="451701v3_tblS1" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap><fig id="figS1" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIGS1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">figS1</object-id><label>Figure S1:</label><caption hwp:id="caption-7"><p hwp:id="p-88">A: Separability index of layer 1. B: pattern index for layers 1 and 2. C: population curves for optic flow in layer 3</p></caption><graphic xlink:href="451701v3_figS1" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><fig id="figS2" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIGS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">figS2</object-id><label>Figure S2:</label><caption hwp:id="caption-8"><p hwp:id="p-89">Alignment between layers of DorsalNet and datasets when resizing stimuli. V1 alignment shifts slightly higher as scale is increased, as expected. Alignment is nevertheless broadly similar across different scales.</p></caption><graphic xlink:href="451701v3_figS2" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><fig id="figS3" position="float" fig-type="figure" orientation="portrait" hwp:id="F5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIGS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">figS3</object-id><label>Figure S3:</label><caption hwp:id="caption-9"><p hwp:id="p-90">CKA across layers. We used a battery of all stimuli from the airsim dataset to compare representations across layers. We extracted the response of the central pixel of a representation in a given layer and computed alignment between internal representations using centered kernel alignment (CKA) [<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-2" hwp:rel-id="ref-69">69</xref>]. 0 indicates no alignment, while 1 indicates perfect alignment between layers.</p></caption><graphic xlink:href="451701v3_figS3" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><table-wrap id="tblS2" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBLS2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tblS2</object-id><label>Table S2:</label><caption hwp:id="caption-10"><p hwp:id="p-91">Correlation between loss on heading task and performance on data from different areas across models and layers. Performance on predicting head rotation parameters (rotation pitch and rotation yaw) is most correlated with match to different brain areas.</p></caption><graphic xlink:href="451701v3_tblS2" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap><table-wrap id="tblS3" orientation="portrait" position="float" hwp:id="T6" hwp:rev-id="xref-table-wrap-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBLS3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tblS3</object-id><label>Table S3:</label><caption hwp:id="caption-11"><p hwp:id="p-92">Relative performance of DorsalNet and MotionNet across different scalings of the input, measured with ridge regression. MotionNet generally benefits from scaling up the videos (1.5X), presumably because of its large second layer receptive fields (27×27). DorsalNet performance is relatively constant across scalings. Table shows normalized pearson correlation (R; see Methods for definition) of different models with different input scaling on different datasets.</p></caption><graphic xlink:href="451701v3_tblS3" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap><table-wrap id="tblS4" orientation="portrait" position="float" hwp:id="T7" hwp:rev-id="xref-table-wrap-7-1 xref-table-wrap-7-2 xref-table-wrap-7-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBLS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tblS4</object-id><label>Table S4:</label><caption hwp:id="caption-12"><p hwp:id="p-93">DorsalNet quantitatively performs best across the dorsal stream across different scalings, as measured with boosting after downsampling. Table shows normalized pearson correlation (R; see Methods for definition) of different models with different input scaling on different datasets.</p></caption><graphic xlink:href="451701v3_tblS4" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap><table-wrap id="tblS5" orientation="portrait" position="float" hwp:id="T8" hwp:rev-id="xref-table-wrap-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/TBLS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T8</object-id><object-id pub-id-type="publisher-id">tblS5</object-id><label>Table S5:</label><caption hwp:id="caption-13"><p hwp:id="p-94">DorsalNet quantitatively performs best across the dorsal stream across different scalings, as measured with boosting after subsampling. Table shows normalized pearson correlation (R; see Methods for definition) of different models with different input scaling on different datasets.</p></caption><graphic xlink:href="451701v3_tblS5" position="float" orientation="portrait" hwp:id="graphic-13"/></table-wrap><fig id="figS4" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIGS4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figS4</object-id><label>Figure S4:</label><caption hwp:id="caption-14"><p hwp:id="p-95">Correlation between heading loss and performance on dorsal stream datasets across networks and layers. Networks and layers which perform better at heading discrimination tend to better match the dorsal stream.</p></caption><graphic xlink:href="451701v3_figS4" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><fig id="figS5" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.07.09.451701v3/FIGS5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figS5</object-id><label>Figure S5:</label><caption hwp:id="caption-15"><p hwp:id="p-96">First layer filters for alternative networks. i3d and CPC on UCF learn orientation selectivity but not direction selectivity. CPC on Airsim learns both.</p></caption><graphic xlink:href="451701v3_figS5" position="float" orientation="portrait" hwp:id="graphic-15"/></fig></app></app-group><fn-group hwp:id="fn-group-3"><fn id="fn1" hwp:id="fn-8"><label><sup>1</sup></label><p hwp:id="p-97"><ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/patrickmineault/your-head-is-there-to-move-you-around" ext-link-type="uri" xlink:href="https://github.com/patrickmineault/your-head-is-there-to-move-you-around" hwp:id="ext-link-4">https://github.com/patrickmineault/your-head-is-there-to-move-you-around</ext-link></p></fn><fn id="fn2" hwp:id="fn-9"><label><sup>2</sup></label><p hwp:id="p-98"><ext-link l:rel="related" l:ref-type="uri" l:ref="https://your-head-is-there-to-move-you-around.netlify.app" ext-link-type="uri" xlink:href="https://your-head-is-there-to-move-you-around.netlify.app" hwp:id="ext-link-5">https://your-head-is-there-to-move-you-around.netlify.app</ext-link></p></fn></fn-group></back></article>
