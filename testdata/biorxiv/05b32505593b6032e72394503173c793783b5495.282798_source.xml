<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/282798</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;282798</article-id><article-id pub-id-type="other" hwp:sub-type="slug">282798</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">282798</article-id><article-id pub-id-type="other" hwp:sub-type="tag">282798</article-id><article-version>1.2</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Efficient inverse graphics in biological face processing</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp hwp:id="corresp-1">Correspondence should be addressed to Ilker Yildirim (email: <email hwp:id="email-1">ilkery@mit.edu</email>), Winrich Freiwald (<email hwp:id="email-2">wfreiwald@rockefeller.edu</email>), or Joshua Tenenbaum (<email hwp:id="email-3">jbt@mit.edu</email>).</corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Yildirim Ilker"><surname>Yildirim</surname><given-names>Ilker</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Belledonne Mario"><surname>Belledonne</surname><given-names>Mario</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-2" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Freiwald Winrich"><surname>Freiwald</surname><given-names>Winrich</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-3" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Tenenbaum Joshua"><surname>Tenenbaum</surname><given-names>Joshua</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-4" hwp:rel-id="aff-3">3</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">Department of Brain &amp; Cognitive Sciences</institution>, MIT, Cambridge, MA</aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Laboratory of Neural Systems</institution>, The Rockefeller University, New York, NY</aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1 xref-aff-3-2 xref-aff-3-3 xref-aff-3-4"><label>3</label><institution hwp:id="institution-3">The Center for Brains</institution>, Minds, and Machines, MIT, Cambridge, MA</aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2019"><year>2019</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-04-02T12:00:37-07:00">
    <day>2</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2019-02-13T17:30:12-08:00">
    <day>13</day><month>2</month><year>2019</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-04-02T12:05:47-07:00">
    <day>2</day><month>4</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2019-02-13T17:36:00-08:00">
    <day>13</day><month>2</month><year>2019</year>
  </pub-date><elocation-id>282798</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-04-02"><day>02</day><month>4</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2019-02-13"><day>13</day><month>2</month><year>2019</year></date>
<date date-type="accepted" hwp:start="2019-02-13"><day>13</day><month>2</month><year>2019</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2019, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2019</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="282798.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/282798v2.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="282798.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/282798v2/282798v2.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/282798v2/282798v2.htslp"/><abstract hwp:id="abstract-1"><p hwp:id="p-2">Vision must not only recognize and localize objects, but perform richer inferences about the underlying causes in the world that give rise to sensory data. How the brain performs these inferences remains unknown: Theoretical proposals based on inverting generative models (or “analysis-by-synthesis”) have a long history but their mechanistic implementations have typically been too slow to support online perception, and their mapping to neural circuits is unclear. Here we present a neurally plausible model for efficiently inverting generative models of images and test it as an account of one high-level visual capacity, the perception of faces. The model is based on a deep neural network that learns to invert a three-dimensional (3D) face graphics program in a single fast feedforward pass. It explains both human behavioral data and multiple levels of neural processing in non-human primates, as well as a classic illusion, the “hollow face” effect. The model fits qualitatively better than state-of-the-art computer vision models, and suggests an interpretable reverse-engineering account of how images are transformed into percepts in the ventral stream.</p></abstract><counts><page-count count="97"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-1">Introduction</title><p hwp:id="p-3">Perception confronts us with a basic puzzle: how can our experiences be so rich in content, so robust to environmental variation, and yet so fast to compute, all at the same time? Vision theorists have long argued that the brain must not only recognize and localize objects, but make inferences about the underlying causal structure of scenes<sup><xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>–<xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref></sup>. When we see a chair or a tree, we perceive it not only as a member of one of those classes, but also as an individual instance with many fine-grained three-dimensional (3D) shape and surface details. These details can persist in long-term memory<sup><xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref></sup> and are crucial for planning our actions – sitting in that chair or climbing that tree. Similarly, when seeing a face, we can not only identify a person if they are familiar, but also perceive so many details of shape, texture, and subtleties of expression even in people we have never met before.</p><p hwp:id="p-4">To explain these inferences, early vision scientists proposed that scene analysis proceeds by inverting causal generative models, also known as “analysis-by-synthesis” or “inverse graphics”. Computational approaches to inverse graphics have been considered for decades in computational vision<sup><xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">3</xref>, <xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>–<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref></sup>, and these models have some behavioral support<sup><xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>, <xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref></sup>. However, inference in these models has traditionally been based on top-down stochastic search algorithms, such as Markov Chain Monte Carlo (MCMC), which are highly iterative and implausibly slow. A single scene percept may take many iterations to compute via MCMC (which could be seconds or minutes on conventional hardware), in contrast to processing in the visual system which is nearly instantaneous. While top-down processing likely plays a role in some of the brain’s visual computations, such as surface segregation in complex scenes<sup><xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>, <xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref></sup>, both humans and non-human primates can extract rich high-level information about objects, faces and scene gists in a time window (150 milliseconds or less) that requires much (if not all) processing to be driven by a single feedforward pass<sup><xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>–<xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref></sup>.</p><p hwp:id="p-5">In part for these reasons, modern work in computational vision and neuroscience has focused on a different class of architectures, deep convolutional neural networks (DCNNs), which are more consistent with the fast, mostly feedforward dynamics of vision in the brain, and which benefit from simple, direct hypotheses about how their computations map onto neural circuits<sup><xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref>, <xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-2" hwp:rel-id="ref-16">16</xref></sup>. DCNNs consist of many layers of features arranged in a feedforward hierarchy, typically trained discriminatively to optimize recognition of objects or object classes from labeled data. They have been instrumental both in leading engineering applications<sup><xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>–<xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref></sup> and in predicting neural responses in the primate visual system, both at the level of single units in macaque cortex as well as fMRI in humans<sup><xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>–<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref></sup>. Despite their impressive successes, however, conventional DCNNs do not attempt to address the question of how vision infers the causal structure underlying images. How we see so much so quickly, how our brains compute rich descriptions of scenes with detailed 3D shapes and surface appearances, in a few hundred milliseconds or less, remains a challenge.</p><p hwp:id="p-6">Most recently, a new class of computational architectures has been developed that can potentially answer this challenge, by combining the best features of DCNNs and analysis-by-synthesis approaches. Several artificial intelligence (AI) research groups, including ours, have shown how neural network “inference models” can be built from a feedforward or recurrent network architecture trained to infer the underlying scene structure, rather than to recognize objects or classify object categories as in conventional DCNNs. In contrast to early analysis-by-synthesis algorithms, inference is fast, following a single bottom-up passes from the image or a small number of bottom-up-top-down cycles, without the need for extensive iterative processing<sup><xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>–<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref></sup>. These models have been developed in an engineering setting, and are just beginning to be tested in machine vision problems; their correspondence with human perception or neural mechanisms is unexplored. Here we introduce a specific model in this class, which we call the Efficient Inverse Graphics (EIG) network, and evaluate it as an account of face perception, arguably the best studied domain of high-level vision. The EIG model makes a number of fine-grained, quantitatively testable predictions, but also lets us evaluate the more general hypothesis that face perception in the brain is best understood in terms of an inference network that inverts a causal generative model (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1a</xref>), as opposed to the more conventional view in both AI and neuroscience that perception is best approached using neural networks optimized for classification, trained to recognize or distinguish object or face identities<sup><xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-3" hwp:rel-id="ref-16">16</xref>, <xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref>, <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>, <xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref></sup>. We find that the EIG model is uniquely compatible with known data on human and non-human primate face processing, and provides the first quantitatively accurate and functionally explanatory account of both neural population responses in macaques and a range of challenging perceptual judgments in humans.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11 xref-fig-1-12 xref-fig-1-13 xref-fig-1-14 xref-fig-1-15 xref-fig-1-16 xref-fig-1-17 xref-fig-1-18 xref-fig-1-19 xref-fig-1-20"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-7">Overview of the modeling framework. <bold>a</bold>, Schematic illustration of two alternative hypotheses about the function of ventral stream processing: the recognition or classification hypothesis (top) and the inverse-graphics or inference network hypothesis (bottom). <bold>b-d</bold>, Schematic of the EIG model. Rounded rectangles indicate representations, arrows or trapezoids indicate causal transformations or inferential mappings between representations. <bold>b</bold>, The probabilistic generative model (right to left) draws an identity from a distribution over familiar and unfamiliar individuals, then through a series of graphics stages generates 3D shape, texture and viewing parameters, renders a 2D image via 2.5D image-based surface representations, and places the face image on an arbitrary background. <bold>c</bold>, The EIG inference network efficiently inverts this generative model using a cascade of DNNs with intermediate steps corresponding to intermediate stages in the graphics pipeline, including face segmentation and normalization (<italic toggle="yes">f</italic><sub>1</sub>), inference of 3D scene properties via increasingly abstract image-based representations (convolution and pooling, <italic toggle="yes">f</italic><sub>2</sub> to <italic toggle="yes">f</italic><sub>3</sub>) followed by two fully connected layers (<italic toggle="yes">f</italic><sub>4</sub> to <italic toggle="yes">f</italic><sub>5</sub>), and finally a person identification network (<italic toggle="yes">f</italic><sub>6</sub>). <bold>d</bold>, Schematic of ventral-stream face perception in the macaque brain, from V1 up through Inferotemporal cortex (IT), including three major IT face-selective sites (ML/MF, AL, AM), and on to downstream medial temporal lobe (MTL) areas where identity information is likely computed. Pins indicate empirically established or suggested functional explanations for different neural stages, based on the generative and inference models of EIG. Pins attached to horizontal dashed lines indicate untested but possible correspondences. <bold>e</bold>, Image-based log-likelihood scores for a random sample of observations using the EIG network’s inferred scene parameters (layer <italic toggle="yes">f</italic><sub>5</sub>), compared to a conventional MCMC-based analysis-by-synthesis method. EIG estimates are computed with no iterations (red line; pink shows min-max interval), yet achieve a higher score and lower variance than MCMC, which requires hundreds of iterations to achieve a similar mean level of inference quality (thick line; thin lines show individual runs, see Methods). <bold>f</bold>, Example inference results from EIG, on held-out real face scans rendered against cluttered backgrounds. Inferred scene parameters are rendered, re-posed, and re-lit using the generative model.</p></caption><graphic xlink:href="282798v2_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-8">The EIG model consists of two parts: a probabilistic generative model based on a multistage 3D graphics program for image synthesis (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1b</xref>), and an approximate inverse function of this generative model based on a DCNN that inverts (explicitly or implicitly) each successive stage of the graphics program (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1c</xref>), layer by layer. The inverse model, also known as an inference network or inference model, is the heart of EIG and the component we can most easily test with available neural and behavioral data. But the generative model is essential as well: it produces the training targets and training data for building the inference network, which is trained to infer the latent inputs or causes in the generative model conditioned on its outputs, rather than to predict class labels such as object categories or face identities as in conventional machine vision systems. The generative model, as we will see, also provides the basis for a functional interpretation of the representations the inference network learns. In this way, the EIG network embodies principles similar to the Helmholtz machine originally proposed by Hinton and colleagues in the 1990’s<sup><xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>, <xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref></sup> and its modern cousins based on variational autoencoders (VAEs)<sup><xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">24</xref>, <xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref></sup>. However, EIG differs from these approaches in that the generative model is based on an explicit graphics program (rather than a second deep neural network learned generically from data) and the EIG inference network is designed to parallel, in reverse, the graphics program’s structure. This allows EIG to more faithfully capture the causal processes of how real-world scenes give rise to images, and to exploit this structure for efficient learning and inference.</p><p hwp:id="p-9">As a test case, we apply EIG in the domain of face perception where, in a rare co-occurrence, data from brain imaging, single-cell recordings, quantitative psychophysics and classic visual illusions all come together to strongly constrain possible models. EIG implements the hypothesis that the downstream targets of the ventral visual pathway, a series of interconnected cortical areas<sup><xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref></sup> in inferotemporal (IT) cortex, are 3D scene properties analogous to the latent variables in a causal generative model of image formation (referred to as the “latent variables” or “inverse graphics” hypothesis; <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1a</xref>); moreover, EIG specifies a precise circuit mechanism by which these properties are plausibly computed in the ventral stream (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1d</xref>). We compare EIG against a broad range of alternatives, including both lesions of EIG (leaving out components of the model) and multiple variants of state-of-the-art networks for face recognition in computer vision, implementing versions of the alternative hypothesis that the targets of ventral stream processing are points in an embedding space optimized for discriminating across facial identities (referred to as the “classification” or “recognition” hypothesis; <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1a</xref>). We also consider alternative instantiations of the latent variables hypothesis, based on VAEs, which replace the structured generative graphics program in EIG with an unstructured generic deep neural network trained to reconstruct images. Only the EIG model, and therefore its more structured version of the latent variables hypothesis, accounts for the full set of neural and behavioral data, at the same time as it matches one of the most challenging perceptual functions of the ventral pathway: computing a rich, accurate percept of the intrinsic 3D shape and texture of a novel face from an observed image in a mostly feedforward pass.</p><sec id="s1a" hwp:id="sec-2"><title hwp:id="title-2">Efficient Inverse Graphics (EIG) Network</title><p hwp:id="p-10">The core of EIG is the DCNN-based inference network, but we begin by describing the probabilistic generative model component, which determines the training objectives and produces the training data for the inference network. The generative model takes the form of a hierarchy of latent variables and causal relations between them representing multiple stages in a probabilistic graphics program for sampling face images (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1b</xref>). The top level random variable specifies an abstract person identity, <italic toggle="yes">F</italic>, drawn from a prior <italic toggle="yes">Pr</italic>(<italic toggle="yes">F</italic>) over a finite set of familiar individuals but allowing for the possibility of encountering a new, unfamiliar individual. The second level random variables specify scene properties: an intrinsic space of 3D face shape <italic toggle="yes">S</italic> and texture <italic toggle="yes">T</italic> descriptors drawn from the distribution Pr(<italic toggle="yes">S, T |F</italic>), as well as extrinsic scene attributes controlling the lighting direction, <italic toggle="yes">L</italic>, and viewing direction (or equivalently, the head pose), <italic toggle="yes">P</italic>, from the distribution Pr(<italic toggle="yes">L, P</italic>). We implement this stage using the Basel Face Model (a probabilistic 3D Morphable Model)<sup><xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>, <xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref></sup>, although other implementations are possible. These 3D scene parameters provide inputs to a z-buffer algorithm Ψ(·) that outputs the third level of random variables, corresponding to intermediate-stage graphics representations (or 2.5D components) for viewpoint-specific surface geometry (normal map, <italic toggle="yes">N</italic>) and color (albedo or reflectance map, <italic toggle="yes">R</italic>), {<italic toggle="yes">N, R</italic>} = Ψ(<italic toggle="yes">S, T, P</italic>). These view-based representations and the lighting direction then provide inputs to a renderer, Φ(·), that outputs an idealized face image, <italic toggle="yes">I</italic> = Φ(<italic toggle="yes">N, R, L</italic>). Finally, the idealized face image is subject to a set of image-level operations including translation, scaling, and background addition, Θ;(·), that outputs an observable raw image, <italic toggle="yes">O</italic> = Θ(<italic toggle="yes">I</italic>) (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig. 1b</xref>; see Methods).</p><p hwp:id="p-11">In principle, perception in this generative model can be formulated as MAP (Maximum A Posteriori) Bayesian inference as follows. We seek to infer the individual face <italic toggle="yes">F</italic>, as well as intrinsic and extrinsic scene properties <italic toggle="yes">S, T, L, P</italic> that maximize the posterior probability
<disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1 xref-disp-formula-1-2 xref-disp-formula-1-3 xref-disp-formula-1-4">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="282798v2_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
where Pr(<italic toggle="yes">N, R|S, T, P</italic>), Pr(<italic toggle="yes">I|N, R, L</italic>) and Pr(<italic toggle="yes">O|I</italic>) express likelihood terms induced by the mappings Ψ, Φ, and Θ respectively, and we have integrated out the intermediate representations of surface geometry and reflectance <italic toggle="yes">N</italic> and <italic toggle="yes">R</italic>, which perceivers do not normally have conscious access to, as well as the ideal face image <italic toggle="yes">I</italic>. Traditional analysis-by-synthesis methods seek to maximize <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">Eq. 1</xref> by stochastic local search, or to sample from the posterior by top-down MCMC inference methods; all of these computations can be very slow. Instead we consider a bottom-up feedforward inference model that is trained to directly estimate MAP values for the latent variables, <italic toggle="yes">F *, S*, T *, L*, P *</italic>.</p><p hwp:id="p-12">This inference network (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1c</xref>) comprises a bottom-up hierarchy of functional mappings that parallels (in reverse) the top-down hierarchy of the generative model, and exploits the conditional independence structure inherent in the generative model for efficient modular inference. In general, if a random variable (or set of variables) <italic toggle="yes">Z</italic> renders two (sets of) variables <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic> conditionally independent in the generative model, and if our goal is to infer <italic toggle="yes">A</italic> from observations of <italic toggle="yes">B</italic>, then an optimal (maximally accurate and efficient) feedforward inference network can be constructed in two stages that map <italic toggle="yes">B</italic> to <italic toggle="yes">Z</italic> and <italic toggle="yes">Z</italic> to <italic toggle="yes">A</italic> respectively<sup><xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>, <xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref></sup>. Here our inference model exploits two such crucial independence relations: (i) The observable raw image is conditionally independent of the 2.5D face components, given the ideal face image and (ii) The 2.5D components are conditionally independent of person identity, given the 3D scene parameters that describe the individual’s face. This conditional independence structure suggests an inference network with three main stages, which can be implemented in a sequence of deep neural networks where the output of each stage’s network is the input to the next stage’s network.</p><p hwp:id="p-13">The first stage segments and normalizes the input image to compute the attended face image, the most probable value for the ideal image <italic toggle="yes">I*</italic> given the observed image <italic toggle="yes">O</italic>, by maximizing Pr(<italic toggle="yes">I|O</italic>) using a DCNN module trained for face volume segmentation<sup><xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref></sup> and adapted to compute the face region given images of faces with background clutter (<italic toggle="yes">f</italic><sub>1</sub> in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Fig. 1c</xref>).</p><p hwp:id="p-14">The second stage is the core of our EIG model, and consists of a DCNN module trained to estimate intrinsic and extrinsic scene properties {<italic toggle="yes">S∗, T ∗, L∗, P ∗</italic>} maximizing Pr(<italic toggle="yes">S, T, L, P |I∗</italic>) from the attended face image. This network is adapted from the architecture of a standard “AlexNet” DCNN<sup><xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref></sup> for object recognition, which consists of four convolutional layers (<italic toggle="yes">f</italic><sub>2</sub> in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Fig. 1c</xref>) ending in a fifth, top convolutional feature space (TCL, <italic toggle="yes">f</italic><sub>3</sub> in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-12" hwp:rel-id="F1">Fig. 1c</xref>), followed by two fully connected layers (FCLs, <italic toggle="yes">f</italic><sub>4</sub> and <italic toggle="yes">f</italic><sub>5</sub> respectively). The training target for the second and final FCL <italic toggle="yes">f</italic><sub>5</sub> is the key difference from the conventional object recognition or face recognition pipeline: instead of being trained to predict class labels or identities, <italic toggle="yes">f</italic><sub>5</sub> is trained to predict scene properties, {<italic toggle="yes">S, T, L, P</italic> }. Training begins from a pretrained version of the basic architecture, fixing or finetuning weights up to layer <italic toggle="yes">f</italic><sub>4</sub>, with only weights in the new scene property layer <italic toggle="yes">f</italic><sub>5</sub> being learned from random initial values. Training images for stage two are generated by forward-simulating images drawn from the generative model (in the spirit of the Helmholtz machine<sup><xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">32</xref></sup>), each with a different randomly drawn value for the scene parameters {<italic toggle="yes">S, T, L, P</italic> }, and using the generative model to produce the corresponding ideal face image <italic toggle="yes">I</italic> conditioned on those scene parameters.</p><p hwp:id="p-15">Finally, a third inference stage estimates the most likely face identity label <italic toggle="yes">F ∗</italic> given the scene properties, maximizing Pr(<italic toggle="yes">F |S∗, T ∗, L∗, P ∗</italic>). Such identity labels are only introduced for familiar faces, with sufficient experience associating an individual’s identity to that face. This module comprises a single new FCL <italic toggle="yes">f</italic><sub>6</sub> for person identity classification, and is trained on labeled image-identity pairs. We generate these pairs from real-world experience if available, or by simulating real-world experience drawing faces randomly from the generative model and its prior over individuals <italic toggle="yes">P</italic> (<italic toggle="yes">F</italic>). In modeling particular experimental data, we tune training to the distribution of faces presented and introduce classification nodes for specific individuals if participants have sufficient opportunity to become familiar with them. See Methods section “EIG model” for further details of each of the three stages of the EIG network.</p><p hwp:id="p-16">Together these three modules form a complete inference pipeline (approximately) inverting the generative model of face images, which satisfies the crucial characteristics of face perception and perceptual systems more generally: The inverse model (i) infers both rich 3D scene structure and the identities or class labels of individuals present in the scene, in a way that is robust to many dimensions of image variation and clutter, and (ii) computes these inferences in a fast, almost instantaneous manner given observed images.</p><p hwp:id="p-17">We have tested the EIG inference network on both synthetic and held-out real face images, both isolated and superimposed on random backgrounds, and compared its performance with classic top-down analysis-by-synthesis algorithms based on MCMC<sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref></sup>. EIG inferences are at least as accurate, assessed both quantitatively (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-13" hwp:rel-id="F1">Fig. 1e</xref>; see also Methods) and qualitatively (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-14" hwp:rel-id="F1">Fig. 1f</xref>), while being far faster (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-15" hwp:rel-id="F1">Fig. 1e</xref>). Thus EIG is at least a viable functional solution to the problem of face perception (but see Methods for a discussion of potential weaknesses as well). In the remainder of the paper, we ask how well the model captures the mechanisms of face perception in the mind and brain, by comparing its internal representations (especially <italic toggle="yes">f</italic><sub>3</sub>, <italic toggle="yes">f</italic><sub>4</sub>, <italic toggle="yes">f</italic><sub>5</sub>) to neural representations of faces in the primate ventral stream, and its estimates of intrinsic and extrinsic face properties with the judgments of human observers in several hard perceptual tasks.</p></sec><sec id="s1b" hwp:id="sec-3"><title hwp:id="title-3">Efficient inverse graphics stages explain macaque face-processing hierarchy</title><p hwp:id="p-18">The best-understood neural architecture on which we can evaluate EIG as an account of perception in the brain is the macaque face-processing network<sup><xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref></sup> [<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2a</xref>; see Supplementary Information (SI) Section 4 for experimental procedure and neural recording details]. Freiwald and Tsao<sup><xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-2" hwp:rel-id="ref-39">39</xref></sup> presented macaques with images of different individuals in different viewing poses (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2b</xref>), and found that this three-level hierarchy exhibits a systematic progression of tuning properties (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig. 2c</xref>). Neurons in the bottom-level face patches ML/MF have responses driven largely by the pose of a face, independent of the face’s identity. Those in the mid-level patch AL also exhibit posespecific tuning, but with a strong mirror-symmetry effect: faces in poses mirror-reflected about the frontal view axes exhibit similar responses. Neurons in the top-level patch AM exhibit view-robust identity coding. It has also been argued that these neural populations encode a multidimensional space for face, based on controlled sets of synthetically generated images.<sup><xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>–<xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref></sup>. However, it remains unclear how the full range of three-dimensional shapes and appearances for natural faces viewed under widely varying natural viewing conditions might be encoded, and how high-level face space representations are computed from observed images through the multiple stages of the face-processing hierarchy.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12 xref-fig-2-13 xref-fig-2-14 xref-fig-2-15 xref-fig-2-16 xref-fig-2-17 xref-fig-2-18 xref-fig-2-19 xref-fig-2-20 xref-fig-2-21 xref-fig-2-22 xref-fig-2-23 xref-fig-2-24 xref-fig-2-25 xref-fig-2-26 xref-fig-2-27 xref-fig-2-28 xref-fig-2-29 xref-fig-2-30 xref-fig-2-31 xref-fig-2-32 xref-fig-2-33 xref-fig-2-34 xref-fig-2-35 xref-fig-2-36"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-19">Inverse graphics in the brain. <bold>a</bold>, Inflated macaque right hemisphere showing six temporal pole face patches, including middle lateral and middle fundus areas ML/MF, anterior lateral area, AL, and anteriormedial area, AM. <bold>b</bold>, Sample FIV images. <bold>c</bold>, Population-level similarity matrices for each face patch. Each matrix shows correlation coefficients of population-level responses for each image pair from the FIV image set (25 individuals each shown in seven poses, total of 175 images)<sup><xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-3" hwp:rel-id="ref-39">39</xref></sup>. <bold>d</bold>, Coefficients resulting from a linear decomposition of the population similarity matrices in terms of idealized similarity matrices for view-specificity, mirror-symmetry, and view-invariance shown in (<bold>e</bold>), in addition to a constant background factor to account for overall mean similarity. <bold>f</bold>, Similarity matrices for each key layer of the EIG network, <italic toggle="yes">f</italic><sub>3</sub>, <italic toggle="yes">f</italic><sub>4</sub>, and <italic toggle="yes">f</italic><sub>5</sub>, tested with FIV image set. Each image is represented as a vector of activations in the corresponding layer. <bold>g</bold>, Linear regression coefficients showing contribution of each idealized similarity matrix for each layer. <bold>h</bold>, Comparing full set of neural transformations to model transformations using these coefficients. <bold>i</bold>, Pearson’s r between similarity matrices of each neural population and model layer. <bold>j-m</bold>, VGG network tested using FIV image set. Panels follow the same convention as the EIG results. Error bars show 95% bootstrap confidence intervals (CIs; see Methods).</p></caption><graphic xlink:href="282798v2_fig2" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-20">We address these questions by first quantifying the population-level tuning properties for the three successive levels of face patches, ML/MF, AL and AM, using linear combinations of three idealized similarity templates representing the abstract properties of view specificity, mirror symmetry, and view-invariant identity selectivity<sup><xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>, <xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref></sup> (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2e</xref>) to fit the empirical similarity matrices for neural populations in each of these patches (see Methods). The coefficients of these different matrices (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig. 2d</xref>) measure, in objective terms, how view-specificity decreases from ML/MF to AM (yellow bars), how mirror-symmetry peaks in AL (light blue bars), and how view-invariant identity coding increases from ML/MF to AL and further to AM (dark blue bars), complementing the qualitative features shown in the population-level similarity matrices (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig. 2c</xref>).</p><p hwp:id="p-21">We then evaluated the ability of the EIG network and other models to explain these qualitative and quantitative tuning properties of ML/MF, AL and AM. In particular we contrast EIG with several variants of the VGG network, based on a state-of-the-art DCNN for machine face recognition built via supervised training with millions of labeled face images from thousands of individual identities<sup><xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">31</xref></sup> (see Methods). These comparisons allow us to tell apart the inverse graphics hypothesis and the classification hypothesis at the level of neural representation.</p><p hwp:id="p-22">We first test models using the FIV set of natural face images, with 175 images of 25 individuals in 7 poses, shown to monkeys during neural recording of the face patches (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2b</xref>). The EIG network faithfully reproduces all patterns in the neural data, both qualitatively (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Fig. 2f</xref>) and quantitatively in terms of the idealized similarity matrix analysis (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2g</xref>). The coefficients of all three idealized similarity templates (view specificity, mirror symmetry, view invariant coding) across all three levels of representation (<italic toggle="yes">f</italic><sub>3</sub>/ML/MF, <italic toggle="yes">f</italic><sub>4</sub>/AL, <italic toggle="yes">f</italic><sub>5</sub>/AM) correlate almost perfectly between EIG and cortical face circuitry (<italic toggle="yes">r</italic> = 0.96, <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2h</xref>). EIG also tracks the functional compartmentalization observed in the cortical hierarchy, as measured by raw correlations between similarities in corresponding layers: similarity in layer <italic toggle="yes">f</italic><sub>3</sub> best correlates with ML/MF, layer <italic toggle="yes">f</italic><sub>4</sub> best correlates with AL, and layer <italic toggle="yes">f</italic><sub>5</sub> best correlates with AM (<italic toggle="yes">p &lt;</italic> 0.05, <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-11" hwp:rel-id="F2">Fig. 2i</xref>). By all these measures, EIG appears to capture the full progression of three functionally distinct stages in face processing, from ML/MF through AL up to AM.</p><p hwp:id="p-23">We evaluate VGG based on the three layers most analogous to EIG: the top convolutional layer (TCL), analogous to <italic toggle="yes">f</italic><sub>3</sub> of EIG, the first fully connected layer (FFCL) analogous to <italic toggle="yes">f</italic><sub>4</sub>, and the second fully connected layer (SFCL) analogous to <italic toggle="yes">f</italic><sub>5</sub>; these are also the layers of VGG most similar in response to the three levels of the face patch system. In contrast to EIG, VGG showed patterns of selectivity with some qualitative similarity to those of the neural data (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-12" hwp:rel-id="F2">Fig. 2j</xref>), but with pronounced qualitative and quantitative differences. Representations in VGG were substantially more view-invariant than either cortex or EIG across all three layers (<italic toggle="yes">p &lt;</italic> 0.05, compare all yellow bars and dark blue bars in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-13" hwp:rel-id="F2">Fig. 2k</xref> versus 2d, g), with the biggest disparities occurring at the intermediate level (compare FFCL to <italic toggle="yes">f</italic><sub>4</sub> and AL in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-14" hwp:rel-id="F2">Fig. 2k, g, d</xref>). Indeed, there is no layer of VGG that shows the characteristic tuning of the intermediate patch AL, as <italic toggle="yes">f</italic><sub>4</sub> of EIG does, nor does any layer of VGG correlate maximally with AL relative to other neural sites; each layer is either a better fit to ML/MF or AM (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-15" hwp:rel-id="F2">Fig. 2m</xref>). Across all three levels in VGG, coefficients of the three idealized similarity matrices correlated much less strongly with analogous coefficients for neural data (<italic toggle="yes">r</italic> = 0.36, <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-16" hwp:rel-id="F2">Fig. 2l</xref>), suggesting a failure to capture how face processing progresses through the cortical hierarchy. Most dramatically, the two highest layers of VGG (FFCL and SFCL) were almost indistinguishable from each other (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-17" hwp:rel-id="F2">Fig. 2j</xref>), which fails to reflect the clear progression in function from mirror-symmetric tuning to view invariant coding that is seen in both the corresponding layers of EIG (<italic toggle="yes">f</italic><sub>3</sub> and <italic toggle="yes">f</italic><sub>4</sub>) and the corresponding neural sites (AL and AM).</p><p hwp:id="p-24">Other analyses show that VGG performance does not depend on whether it is fine-tuned to these specific face identities (as in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-18" hwp:rel-id="F2">Fig. 2b</xref>; see <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Extended Data Fig. 2</xref> for VGG in its raw pretrained state), and that the initial face segmentation and normalization stage of EIG, which has not been a component of previous ventral stream models<sup><xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-3" hwp:rel-id="ref-13">13</xref>, <xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-4" hwp:rel-id="ref-16">16</xref>, <xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-3" hwp:rel-id="ref-20">20</xref></sup>, is necessary for its strong performance (but has little effect on VGG; see Methods and <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Extended Data Fig. 2</xref>). Taken together, these results strongly support the hypothesis that ventral stream face processing begins with an initial segmenting operation and culminates in targets that encode the latent variables of a face generative model, rather than mapping raw images to features optimized for face identity recognition or discrimination, as in conventional machine vision approaches.</p><p hwp:id="p-25">To better understand the reasons why a fully brain-like pattern of responses arises in EIG, and the conditions under which it might arise in other neural network models, we studied a large number of model alternatives, varying in network architecture, training set and objective, and standard aspects of training procedure (see Methods and SI Sections 1,2; <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Extended Data Figs. 4</xref>-<xref rid="figE7" ref-type="fig" hwp:id="xref-fig-12-1" hwp:rel-id="F12">7</xref>). We used a controlled synthetic analog of the FIV image set, in which only faces were rendered (without clothing or backgrounds; FIV-S; see Methods). In particular, we tested several VAE variants that shared EIG’s feedforward inference-network architecture but used a different training objective (image reconstruction loss; <xref rid="figE5" ref-type="fig" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Extended Data Fig. 5</xref>) and used deep neural networks to parametrize a learned generative model (as opposed to EIG’s structured graphics engine). We also tested several variants of the VGG architecture (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-2" hwp:rel-id="F9">Extended Data Fig. 4</xref>) to unconfound effects of the VGG architecture, training set and training objective. We say that a model produces a “fully brain-like pattern of responses” to the extent that it has three progressive layers with idealized similarity coefficients matching those in ML/MF, AL, AM (i.e., the bar plots shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-19" hwp:rel-id="F2">Fig. 2d</xref>), correlating highly across layers (as in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-20" hwp:rel-id="F2">Fig. 2h</xref>), and with raw similarities in each of these model layers correlating maximally and distinctively with raw similarities in the corresponding neural sites (as in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-21" hwp:rel-id="F2">Fig. 2i</xref>). Two aspects of the EIG network, its training targets and architecture, proved necessary to obtain fully-brain like representations: (1) The targets of inference should be the latent variables of the causal generative model (3D face shape and face texture descriptors), and (2) There should be a stack of convolutional layers processing the attended face image followed by at least one fully connected hidden layer between the top convolutional layer and the final layer trained to estimate the latent variables. Other aspects of the EIG training procedure, such as the magnitude of dropout and initialization with pretrained network weights, were not essential for producing fully brain-like responses but do make training much more efficient (Extended Data Figs. 6,7).</p><p hwp:id="p-26">Finally we ask whether intermediate stages of the face-processing hierarchy, ML/MF and AL in the primate brain or <italic toggle="yes">f</italic><sub>3</sub> and <italic toggle="yes">f</italic><sub>4</sub> in the EIG network, can be given an interpretable functional account as we did for AM and <italic toggle="yes">f</italic><sub>5</sub>, or whether instead these patches are best understood simply as a hierarchy of “black box” function approximators. <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-16" hwp:rel-id="F1">Fig. 1b</xref> and <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-17" hwp:rel-id="F1">c</xref> suggest one possible functional interpretation based on correspondences between the graphics and inverse-graphics pathways: ML/MF could be understood as computing a reconstruction of an intermediate stage of the generative model, the 2.5D components of a face (e.g., albedos and surface normals, or surface depths) analogous to the “intrinsic images” or “2.5D sketch” of classic computer vision systems<sup><xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-3" hwp:rel-id="ref-3">3</xref>, <xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref></sup>. It is also possible that these patches compute a reconstruction of an earlier stage in the generative model such as the attended face image (corresponding to the output of <italic toggle="yes">f</italic><sub>1</sub>), or that they are just stepping stones to higher-level representations without distinct functional interpretations in terms of the generative graphics model. We computed similarity matrices for each of these candidate interpretations (each generative model stage), as well as for the raw pixel images as a control (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3a</xref>; see SI Section 3 for how 2.5D components of the FIV images are approximated). We then correlated these similarity matrices with those for ML/MF and AL. We find that the 2.5D components best explain ML/MF (<italic toggle="yes">p &lt;</italic> 0.001), and closely resemble their overall similarity structure (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3b</xref>). Attended images also provide a better account of ML/MF than the raw pixel images (<italic toggle="yes">p &lt;</italic> 0.001) but significantly worse than the 2.5D components (<italic toggle="yes">p &lt;</italic> 0.001 for each component; <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3b</xref>). We also find that the 2.5D components explain <italic toggle="yes">f</italic><sub>3</sub> layer responses in the EIG model better than the raw pixel images, and better than the attended face image when these can be discriminated (see SI section 3; <xref rid="figE8" ref-type="fig" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Extended Data Fig. 8</xref>).</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4 xref-fig-3-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-27">Understanding ML/MF computations using the generative model and the 2.5D (or intrinsic image) components. <bold>a</bold>, Similarity matrices based on raw input images, attended images, albedos, and normals. Colors indicate the direction of the normal of the underlying 3D surface at each pixel location. <bold>b</bold>, Correlation coefficients between ML/MF and the similarity matrices of each image representation in (<bold>a</bold>) and <italic toggle="yes">f</italic><sub>3</sub>. Error bars indicate 95% bootstrap CIs.</p></caption><graphic xlink:href="282798v2_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-28">AL has no such straightforward representational account, but it may be understood as implementing a densely connected hidden-layer mapping the estimated 2.5D face components (in ML/MF and <italic toggle="yes">f</italic><sub>3</sub>) to estimated 3D face properties (in AM and <italic toggle="yes">f</italic><sub>5</sub>). This highly nonlinear transformation can be facilitated using some kind of hidden layer, and this could be the role of AL in the primate brain and the corresponding layer <italic toggle="yes">f</italic><sub>4</sub> in EIG. Note that such an intermediate layer appears to be functionally missing from VGG and its variants trained to discriminatively predict identity rather than 3D object properties. These models always show very similar responses in all their fully connected layers (compare <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-22" hwp:rel-id="F2">Fig. 2c</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-23" hwp:rel-id="F2">2j</xref> and also see <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-3" hwp:rel-id="F9">Extended Data Fig. 4</xref>). We conjecture that this AL-like intermediate stage nonlinearity is not necessary because the fully connected layers of VGG are solving a different task than EIG or the brain: VGG appears to be mapping high-level image features (computed at the top of the convolutional layers) to person identities which are almost linearly decodable from these features, without ever having to explicitly represent the 3D properties of a face (see SI Section 3; <xref rid="figE9" ref-type="fig" hwp:id="xref-fig-14-1" hwp:rel-id="F14">Extended Data Fig. 9</xref>). The VGG network design may be a reasonable, perhaps even superior, way to build a system for face perception if the goal is merely to classify or recognize individuals through their facial appearance, as in most of today’s computer vision system. But the brain needs to compute much richer information about the 3D shape and texture of faces, in order to analyze expressions, emotions, mood and health, or to use face perception as a cue in spoken language understanding. The inverse graphics design of the EIG network offers a possible route to those richer percepts, and our analyses suggest that the ML/MF-AL-AM circuit may be the locus of these computations in the brain.</p></sec><sec id="s1c" hwp:id="sec-4"><title hwp:id="title-4">Efficient inverse graphics scene parameters predict human behavior</title><p hwp:id="p-29">We also tested EIG and alternative models’ ability to explain the behavioral aspects of face perception, by comparing their responses to people’s judgments in a suite of challenging unfamiliar face recognition tasks<sup><xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref></sup>. In three experiments (inspired by the passport photo verification task), subjects were asked to judge whether two sequentially presented face images showed the same or different identity (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4a</xref>). In Experiment 1 (“Regular”), both study and test images were presented with pose and lighting directions chosen randomly over the full range covered by the generative model. Experiments 2 and 3 probed generalization abilities, using the same study items from Experiment 1 but test items that extended qualitatively the range of training stimuli. In Experiment 2 (“Sculpture”), the test items were images of face sculptures (i.e., texture-less face shapes rendered with a stone-like uniform grey albedo in frontal pose) eliminating all cues from skin coloration or texture normally present in face inputs. In Experiment 3, the test items were flat frontal facial textures, produced by distorting normal images using a fish-eye lens effect to reduce shape information in the input (see Methods).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-30">Across three behavioral experiments, EIG consistently predicts human face recognition performance. <bold>a</bold>, Example stimuli testing same-different judgments (“same” trials rows 1-2, “different” trials rows 3-4) with normal test faces (Exp. 1), “sculpture” (texture-less) test faces (Exp. 2), and fish-eye lens distorted shade-less facial textures as test faces (Exp. 3). <bold>b</bold>, Correlations between model similarity judgments and human judges’ probability of responding “same”. <bold>c</bold>, Inferred weights (a value between 0 and 1 that maximized model’s recognition accuracy) of the shape properties (relative to texture properties) in the EIG model predictions for Exps. 1-3. Error bars indicate 95% bootstrap CIs (see SI Section 5).</p></caption><graphic xlink:href="282798v2_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-31">We hypothesized that if face perception is based on inverting a generative model with independent 3D shape and texture latents, as in EIG but not VGG, VGG-Raw or other classification/recognition alternatives, then participants might be able to selectively attend to shape or texture estimates in their internal representations, in order to optimize performance on these different challenge tasks. Crucially, EIG and VGG models are both trained using an equal number of images synthesized from the same graphics program used to generate the stimuli (although VGG is finetuned on top of the VGG network which itself is trained with millions of other face images); only their training targets are different: latent variables of the generative face model for EIG, versus an embedding space for discriminating person identities for VGG. This allows our behavioral analyses, like our neural analyses, to test between the two different hypotheses about the functional goal of face perception, inference in a generative model versus classification or recognition of individuals’ identities.</p><p hwp:id="p-32">For each experiment, we compared average human responses – i.e., Pr(“Same”), frequency of the “same” response – to the models’ predicted similarity across trials. A model’s predicted similarity for a given trial was computed as the similarity between the model’s outputs (i.e., its top layer) for the study and test items (see Methods). The VGG and VGG-Raw networks’ outputs for an image is their identity-embedding spaces, the layer SFCL. (No other layer in the VGG network provided a better account of the human behavior than its SSFL layer.) EIG’s output is its shape and texture parameters, which unlike other models supports selective attention to these different aspects of a face. For each experiment we fit a single weight for the shape parameters in EIG’s computation of face similarity (constant across all trials and participants); the weight of the texture component is 1 minus that value.</p><p hwp:id="p-33">Overall, participants performed significantly better than chance (50% correct): Average performance was 66% correct in Experiment 1, 64% in Experiment 2, and 61% in Experiment 3. (See SI Section 5 for model-free behavioral analysis.) In trial-by-trial comparisons to behavior, EIG consistently predicted human error patterns across all three experiments, with <italic toggle="yes">r</italic> values 0.70[0.65, 0.76], 0.64[0.58, 0.69], and 0.54[0.47, 0.61] (where [<italic toggle="yes">l, u</italic>] indicates lower/upper 95% confidence intervals; <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4b</xref>). VGG (though not VGG-Raw) performed comparably on Experiment 1, but EIG fit human judgments significantly better than both alternative models in Exps. 2 and 3 (<italic toggle="yes">p &lt;</italic> 0.001 for all comparisons based on direct bootstrap hypothesis tests; see SI Section 5). EIG’s inferred attention weight showed a bias towards shape properties in Experiment 1 and a bias towards texture properties in Experiment 3, but it attended to shape parameters almost exclusively for Experiment 2 (inferred weight 0.99; <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4c</xref>). These results suggest that EIG captures human face perception abilities more accurately than other models, especially under less familiar stimulus conditions and tasks requiring extreme generalization between study and test faces. They also lend further support to the latent variables hypothesis over the classification hypothesis for ventral stream face processing.</p><p hwp:id="p-34">Human face perception is susceptible to illusions, and our model naturally captures one of the most famous. In the hollow face illusion, a face mask reversed in depth (so the nose points away from the viewer) appears to be a normally shaped face with two distinctions: (i) hollow faces lit from the top or side appear to be lit from the bottom or alternate side<sup><xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref></sup>, and (ii) hollow faces appear flatter than normal faces<sup><xref rid="c48" ref-type="bibr" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref></sup>. It has been suggested that this illusion could be a result of Bayesian inference, arising from the integration of top-down priors for natural face geometry, appearance and lighting with ambiguous bottom-up cues to depth such as shading patterns<sup><xref rid="c48" ref-type="bibr" hwp:id="xref-ref-48-2" hwp:rel-id="ref-48">48</xref>, <xref rid="c49" ref-type="bibr" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref></sup>. To our knowledge, this proposal has not previously been tested quantitatively or implemented in a working computational model. Here, we psychophysically study the hollow-face effect in greater detail using graded levels of depth reversals, and test EIG quantitatively as a computational account of human illusory percepts at a trial-level granularity.</p><p hwp:id="p-35">We compared our model’s inferences about lighting direction and face depth with people’s judgments, in both graded versions of the hollow face illusion and normal lighting direction variation, as a control (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig. 5a, b</xref>). We found that the EIG network, like humans, perceived the light source direction to covary illusorily with graded reversal of the face depth map, in a highly nonlinear pattern inflecting just when depth values turned negative; in contrast, varying lighting direction in a normal way while keeping face shape constant (the control condition) was perceived linearly and largely veridically by both people and the model (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Fig. 5c,d</xref>). We also found that the EIG network, like humans, perceived depth-inverted faces as more flat when compared to their control counterparts with the lighting source elevation matched to its illusorily perceived location in the depth-inverted condition; indeed, the EIG network closely matched the magnitude of flattening in depth judgments as a function of the level of depth reversal for hollow faces, as well as a subtle effect of lighting elevation on judged depth in the control condition (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Fig. 5e,f</xref>). We also attempted to decode these same lighting and profile depth parameters from the VGG network, and found significantly worse fits to human judgments in all cases, but especially in depth judgments where VGG fits were barely better than chance (see SI Section 5; <xref rid="figE13" ref-type="fig" hwp:id="xref-fig-18-1" hwp:rel-id="F18">Extended Data Fig. 13</xref>). The fact that the EIG network captures the nonlinear interaction of depth and lighting percepts in the hollow face illusion does not uniquely support EIG as an account of the ventral face pathway; a “vanilla” network could be trained to estimate either lighting or profile depth from face images and might predict the same judgments. Rather, EIG’s success here relative to VGG, without EIG having to be trained specially on these atypical images or ever being trained explicitly to estimate profile depth, provides further evidence that ventral stream face perception as modeled by EIG is implementing some form of fast approximate analysis-by-synthesis or inverse graphics computation, as opposed to being optimized for recognition of face identity.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6 xref-fig-5-7 xref-fig-5-8 xref-fig-5-9 xref-fig-5-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-36">Psychophysics of the “hollow face” effect. On a given trial, participants saw an image of a face lit by a single light source and judged either the elevation of the light source (<bold>c</bold>, <bold>d</bold>) or the profile depth of the presented face (<bold>e</bold>, <bold>f</bold>) using a scale between 1 and 7 (also see Methods). <bold>a</bold>, One group of participants (depth suppression group) were presented with images of faces that were always lit from the top, but where the shape of the face was gradually reversed from a normally shaped face (convexity=1) to a flat surface (convexity=0) to an inverted hollow face (convexity=-1). <bold>b</bold>, Another group of participants (control group) were presented with images of normally shaped faces (convexity=1) lit from one of the 9 possible elevations ranging from the top of the face to the bottom. <bold>c</bold>, Normalized average light source elevation judgments of the depth-suppression group (left), the control group (right), EIG’s lighting elevation inferences, and the ground truth light source location. <bold>d</bold>, Average human judgments vs. EIG’s lighting source elevation inferences across all 90 trials without pooling to 9 bins. Pearson’s r values are shown for all trials (grey), control trials (red), and depth suppression trials (blue). <bold>e</bold>, Normalized average profile depth judgments of the depth-suppression group (left), control group (right) and EIG’s inferred profile depth. <bold>f</bold>, Average human judgments vs. EIG’s inferred profile depths across all 108 trials without pooling to 9 bins. Pearson’s r values are shown as in (<bold>d</bold>).</p></caption><graphic xlink:href="282798v2_fig5" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec></sec><sec id="s2" hwp:id="sec-5"><title hwp:id="title-5">Discussion</title><p hwp:id="p-37">Our results suggest that the primate ventral stream approaches at least the first feedforward pass in face perception – and perhaps object perception more generally – with an “inverse graphics” strategy implemented via an efficient hierarchical inference network: Observed images are mapped via a segmentation and normalization mechanism to a 2.5D-like map of intrinsic surface properties (view-centered geometry and albedo) represented in ML/MF, which is then mapped via a nonlinear transform through AL to a largely viewpoint-independent representation of 3D object properties (shape and texture) in AM. The EIG network simulates this process and captures the key qualitative and quantitative features of neural responses across the face-patch system, as well as human perception for both typical and atypical face stimuli. The EIG model thus suggests how the structure of the visual system might be optimized for its function: computing a rich representation of behaviorally relevant causal properties underlying the appearance of a novel object or scene, as quickly and as accurately as possible.</p><p hwp:id="p-38">Our results are consistent with strong evidence that neurons in areas ML/MF and AM code faces in terms of a continuous “shape-appearance” space<sup><xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">42</xref></sup>, not simply discrete identities. However, the EIG model goes beyond this finding to address core, long-standing questions of neural computation: How is the ultimate percept of an object (or face) derived from an image via a hierarchy of intermediate processing stages, and why does this hierarchy have the structure it does? EIG is an image-computable model that faithfully reproduces representations in all three face patches of ML/MF, AL and AM, and explains mechanistically how each stage is computed. It also suggests why these representations would be computed in the sequence observed, in terms of a network for moving from 2D images to 2.5D surface components to 3D object properties, which exploits the conditional independence properties of a generative model for how face scenes produce images to efficiently invert that process. The model thus gives a systems-level functional understanding of perhaps the best characterized circuitry in the higher ventral stream.</p><p hwp:id="p-39">Anatomical connectivity and temporal dynamics of responses in the face patches suggest the existence of feedback and other non-hierarchical connectivity that our current model does not capture<sup><xref rid="c50" ref-type="bibr" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref></sup>. Following earlier models of primate face and object processing<sup><xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-5" hwp:rel-id="ref-16">16</xref>, <xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-4" hwp:rel-id="ref-20">20</xref>, <xref rid="c51" ref-type="bibr" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref></sup>, we see a feed-forward hierarchical network such as EIG as only a first approximation of the system’s functional architecture – a natural starting point, as so much rich information about faces (and objects and scenes) is already computed in the first 150 msecs of feedforward inference, but clearly just a first step that future work should go beyond. More generally, there are important functions of vision that can be understood in terms of inverting generative models, such as segregating multiple objects or surfaces in complex or cluttered scenes, which appear to depend on feedback or recurrent connections, especially to early visual areas (V1/V2)<sup><xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">11</xref>, <xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">12</xref></sup>. Explaining these neural computations could benefit greatly from the study of efficient inverse graphics architectures that integrate bottom-up and top-down processing<sup><xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>, <xref rid="c52" ref-type="bibr" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref></sup>. It is also possible that such feedback architectures could provide a fuller account of the mechanisms by which the computations in our EIG network are implemented in the brain.</p><p hwp:id="p-40">That our model simultaneously explains the full macaque face patch system and the outputs of human psychophysical judgment provides further support that human and non-human primate face systems share similar organization<sup><xref rid="c53" ref-type="bibr" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>, <xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref></sup>. Recent work comparing VGG face network representations with neural representations in humans using intracranial EEG (iEEG) data<sup><xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">30</xref></sup>, taken together with our results here, also suggests a consistent picture. Grossman et al. find evidence that VGG only matches human face representations up to the model network’s TCL, in areas of human IT thought to best correspond to the middle face patches we study here (ML/MF). They take this as evidence that human face circuitry is performing a more “pictorial” form of processing than VGG’s recognition computations, but they do not specify an alternative network architecture or concrete computational hypothesis for what that pictorial processing might be. Our model suggests one such hypothesis, in the form of 2.5D or intrinsic image components, which capture facial appearance and shape in a view-based, image-centric frame, and correspond well to middle face patch representations in macaques. Our model also suggests how those pictorial 2.5D representations can lead downstream to a full 3D description of face shape and appearance, which would correspond to more anterior face regions that (as Grossman et al. note) have yet to be studied intracranially in humans, and then further downstream to representations of familiar individuals’ identies (e.g, MTL, perirhinal cortex), which have been well-characterized in both humans<sup><xref rid="c55" ref-type="bibr" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref></sup> and macaques<sup><xref rid="c56" ref-type="bibr" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref></sup>.</p><p hwp:id="p-41">Our approach also has broader implications for neuroscience, perception, and cognition. The finding that IT supports decoding of category-orthogonal shape information for a wide range of objects, in addition to object category identity<sup><xref rid="c57" ref-type="bibr" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref></sup>, suggests that an extension of EIG could account for how the brain perceives the three-dimensional structure of objects beyond the domain of faces. With other collaborators, we have recently shown in an AI context that EIG-like networks for efficient inference of 3D shapes from 2D images via 2.5D sketches can work for arbitrary object classes (e.g., chairs, cars)<sup><xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">29</xref>, <xref rid="c58" ref-type="bibr" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref></sup>, and can even generalize to a range of novel, unseen classes<sup><xref rid="c59" ref-type="bibr" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref></sup>. In future work, we hope to explore these models of how the ventral visual pathway processes other object classes with functionally specific, localized representations (bodies, hands, word forms), as well as objects more generally.</p><p hwp:id="p-42">If this larger program is successful, it may offer a resolution to the problem of interpretability in visual neuroscience<sup><xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref></sup>: Today’s best performing models are remarkable for their ability to fit stimulus-dependent variance in neural firing rates, but often without an interpretable explanation of what those neurons are computing. Our work suggests that in addition to maximizing variance explained, computational neuroscientists could aim for “semi-interpretable” models of perception, in which some neural populations (such as ML/MF and AM) can be understood as representing stages in the inverse of a generative model (such as 2.5D components and 3D shape and texture properties), while other populations (such as AL) might be better explained as implementing necessary hidden-layer (nonlinear) transforms between interpretable stages.</p><p hwp:id="p-43">The efficient inverse graphics approach can also be extended to richer perceptual inferences where there is currently no consensus on how these computations are implemented in the brain. EIG networks can be augmented with multiple scene layers in order to parse faces or other objects under occlusion<sup><xref rid="c61" ref-type="bibr" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">61</xref>, <xref rid="c62" ref-type="bibr" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">62</xref></sup>. They be deployed in parallel or in series (using attention) to parse out multiple objects in a scene<sup><xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-3" hwp:rel-id="ref-24">24</xref>, <xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">63</xref>–<xref rid="c66" ref-type="bibr" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">66</xref></sup>. They can even be extended to other modalities through which we perceive physical objects, such as touch, and can support flexible crossmodal transfer, allowing objects that have only been experienced in one modality (e.g., by sight) to be recognized in another (touch)<sup><xref rid="c61" ref-type="bibr" hwp:id="xref-ref-61-2" hwp:rel-id="ref-61">61</xref></sup>. All of these extensions suggest testable hypothesis for neural computations and representations, in ways that could also point to crucial functional roles for feedback or recurrent processing, which our work here does not address.</p><p hwp:id="p-44">Finally, while our work suggests a functional role for causal generative models in the visual system, it leaves open many questions about their nature, use, and origins. Interpreted most literally, EIG implies that the brain uses feedforward inference networks as the workhorse of object perception, but uses generative models to provide the targets for training those networks, and as a source of internally generated training data (possibly at multiple stages, in a recognition pipeline that inverts a multi-stage generative process). Generative models in the brain could also support other functional roles, however: They could be used during online perception to refine a percept – particularly in hard cases such as under dim light or under heavy occlusion– by enforcing reprojection consistency with intrinsic image based surface representations<sup><xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">8</xref>, <xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>, <xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-3" hwp:rel-id="ref-29">29</xref>, <xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-2" hwp:rel-id="ref-63">63</xref></sup>. They could also support higher functions in cognition such as mental imagery, planning, and problem solving<sup><xref rid="c67" ref-type="bibr" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">67</xref>, <xref rid="c68" ref-type="bibr" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">68</xref></sup>. It remains to be determined which of these functions are actually operative in the brain, as well as where and how generative models might be implemented in neural circuits, and how they might be built over development, from some combination of genetically programmed mechanisms and early perceptual experience. VAEs, and their close cousins GANs<sup><xref rid="c69" ref-type="bibr" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">69</xref></sup>, capsules<sup><xref rid="c52" ref-type="bibr" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">52</xref></sup>, and GQNs<sup><xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-4" hwp:rel-id="ref-24">24</xref></sup>, as well as RCNs<sup><xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref></sup>, are recent developments in artificial network architectures that suggest at least partial hypotheses for how graphics models might be implemented neurally, or constructed through learning, but none of these suggestions are yet well grounded in experimental work. We hope that the success of the EIG approach here will inspire future work to explore potential neural correlates of these architectures, as well as the other roles that generative models could play in perception, cognition, and learning.</p></sec></body><back><ref-list hwp:id="ref-list-1"><title hwp:id="title-6">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="book" citation-type="book" ref:id="282798v2.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Olshausen B. A."><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> <chapter-title>Perception as an inference problem</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Gazzaniga M."><surname>Gazzaniga</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mangun R."><surname>Mangun</surname>, <given-names>R.</given-names></string-name></person-group> (eds.) <source hwp:id="source-1">The Cognitive Neurosciences</source> (<publisher-name>MIT Press</publisher-name>, <year>2013</year>).</citation></ref><ref id="c2" hwp:id="ref-2"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Yuille A."><surname>Yuille</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kersten D."><surname>Kersten</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-2">Vision as Bayesian inference: analysis by synthesis?</article-title> <source hwp:id="source-2">Trends in Cognitive Sciences</source> <volume>10</volume>, <fpage>301</fpage>–<lpage>308</lpage> (<year>2006</year>).</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2 xref-ref-3-3"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Barrow H."><surname>Barrow</surname>, <given-names>H.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J."><surname>Tenenbaum</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-3">Recovering intrinsic scene characteristics from images</article-title>. <source hwp:id="source-3">Computer Vision Systems</source> <volume>2</volume> (<year>1978</year>).</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Brady T. F."><surname>Brady</surname>, <given-names>T. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Konkle T."><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Alvarez G. A."><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oliva A."><surname>Oliva</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-4">Visual long-term memory has a massive storage capacity for object details</article-title>. <source hwp:id="source-4">Proceedings of the National Academy of Sciences</source> <volume>105</volume>, <fpage>14325</fpage>–<lpage>14329</lpage> (<year>2008</year>).</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Lee T. S."><surname>Lee</surname>, <given-names>T. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mumford D."><surname>Mumford</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-5">Hierarchical bayesian inference in the visual cortex</article-title>. <source hwp:id="source-5">Journal of the Optical Society of America A</source> <volume>20</volume>, <fpage>1434</fpage>–<lpage>1448</lpage> (<year>2003</year>).</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>6.</label><citation publication-type="book" citation-type="book" ref:id="282798v2.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Blanz V."><surname>Blanz</surname>, <given-names>V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Vetter T."><surname>Vetter</surname>, <given-names>T.</given-names></string-name> <chapter-title>A morphable model for the synthesis of 3D faces</chapter-title>. In <source hwp:id="source-6">Annual Conference on Computer Graphics and Interactive Techniques</source>, <fpage>187</fpage>–<lpage>194</lpage> (<publisher-name>ACM Press/Addison-Wesley Publishing Co.</publisher-name>, <year>1999</year>).</citation></ref><ref id="c7" hwp:id="ref-7"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Barron J."><surname>Barron</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Malik J."><surname>Malik</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-6">Shape, illumination, and reflectance from shading</article-title>. <source hwp:id="source-7">IEEE Transactions on Pattern Analysis and Machine Intelligence</source> (<year>2013</year>).</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Kulkarni T. D."><surname>Kulkarni</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kohli P."><surname>Kohli</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Mansinghka V."><surname>Mansinghka</surname>, <given-names>V.</given-names></string-name> <article-title hwp:id="article-title-7">Picture: A Probabilistic Programming Language for Scene Perception</article-title>. In <source hwp:id="source-8">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, <fpage>4390</fpage>–<lpage>4399</lpage> (<year>2015</year>).</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Jacobs R. A."><surname>Jacobs</surname>, <given-names>R. A.</given-names></string-name> <article-title hwp:id="article-title-8">Transfer of Object Category Knowledge Across Visual and Haptic Modalities: Experimental and Computational Studies</article-title>. <source hwp:id="source-9">Cognition</source> <volume>126</volume>, <fpage>135</fpage>–<lpage>148</lpage> (<year>2013</year>).</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Erdogan G."><surname>Erdogan</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Jacobs R. A."><surname>Jacobs</surname>, <given-names>R. A.</given-names></string-name> <article-title hwp:id="article-title-9">Visual shape perception as bayesian inference of 3d object-centered shape representations</article-title>. <source hwp:id="source-10">Psychological Review</source> (<year>2017</year>).</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Lamme V. A."><surname>Lamme</surname>, <given-names>V. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Roelfsema P. R."><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name> <article-title hwp:id="article-title-10">The distinct modes of vision offered by feedforward and recurrent processing</article-title>. <source hwp:id="source-11">Trends in neurosciences</source> <volume>23</volume>, <fpage>571</fpage>–<lpage>579</lpage> (<year>2000</year>).</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Heinen K."><surname>Heinen</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jolij J."><surname>Jolij</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Lamme V. A."><surname>Lamme</surname>, <given-names>V. A.</given-names></string-name> <article-title hwp:id="article-title-11">Figure–ground segregation requires two distinct periods of activity in v1: a transcranial magnetic stimulation study</article-title>. <source hwp:id="source-12">Neuroreport</source> <volume>16</volume>, <fpage>1483</fpage>–<lpage>1487</lpage> (<year>2005</year>).</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2 xref-ref-13-3"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="DiCarlo J. J."><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zoccolan D."><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rust N. C."><surname>Rust</surname>, <given-names>N. C.</given-names></string-name> <article-title hwp:id="article-title-12">How does the brain solve visual object recognition?</article-title> <source hwp:id="source-13">Neuron</source> <volume>73</volume>, <fpage>415</fpage>–<lpage>434</lpage> (<year>2012</year>).</citation></ref><ref id="c14" hwp:id="ref-14"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Vanrullen R."><surname>Vanrullen</surname>, <given-names>R.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Thorpe S. J."><surname>Thorpe</surname>, <given-names>S. J.</given-names></string-name> <article-title hwp:id="article-title-13">The time course of visual processing: from early perception to decision-making</article-title>. <source hwp:id="source-14">Journal of cognitive neuroscience</source> <volume>13</volume>, <fpage>454</fpage>–<lpage>461</lpage> (<year>2001</year>).</citation></ref><ref id="c15" hwp:id="ref-15"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Potter M. C."><surname>Potter</surname>, <given-names>M. C.</given-names></string-name> <article-title hwp:id="article-title-14">Short-term conceptual memory for pictures</article-title>. <source hwp:id="source-15">Journal of experimental psychology: human learning and memory</source> <volume>2</volume>, <fpage>509</fpage> (<year>1976</year>).</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1 xref-ref-16-2 xref-ref-16-3 xref-ref-16-4 xref-ref-16-5"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Serre T."><surname>Serre</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oliva A."><surname>Oliva</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poggio T."><surname>Poggio</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-15">A feedforward architecture accounts for rapid categorization</article-title>. <source hwp:id="source-16">Proceedings of the National Academy of Sciences</source> <volume>104</volume>, <fpage>6424</fpage>–<lpage>6429</lpage> (<year>2007</year>).</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Krizhevsky A."><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G. E."><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> <article-title hwp:id="article-title-16">Imagenet classification with deep convolutional neural networks</article-title>. In <source hwp:id="source-17">Advances in Neural Information Processing Systems</source>, <fpage>1097</fpage>–<lpage>1105</lpage> (<year>2012</year>).</citation></ref><ref id="c18" hwp:id="ref-18"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Simonyan K."><surname>Simonyan</surname>, <given-names>K.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zisserman A."><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> <source hwp:id="source-18">Very deep convolutional networks for large-scale image recognition</source>. arXiv preprint arXiv:1409.1556 (<year>2014</year>).</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Szegedy C."><surname>Szegedy</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-17">Going deeper with convolutions</article-title>. In <source hwp:id="source-19">IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>1</fpage>–<lpage>9</lpage> (<year>2015</year>).</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2 xref-ref-20-3 xref-ref-20-4"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Yamins D. L."><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-18">Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source hwp:id="source-20">Proceedings of the National Academy of Sciences</source> <volume>111</volume>, <fpage>8619</fpage>–<lpage>8624</lpage> (<year>2014</year>).</citation></ref><ref id="c21" hwp:id="ref-21"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Eickenberg M."><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gramfort A."><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varoquaux G."><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Thirion B."><surname>Thirion</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-19">Seeing it all: Convolutional network layers map the function of the human visual system</article-title>. <source hwp:id="source-21">NeuroImage</source> <volume>152</volume>, <fpage>184</fpage>–<lpage>194</lpage> (<year>2017</year>).</citation></ref><ref id="c22" hwp:id="ref-22"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Khaligh-Razavi S.-M."><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-20">Deep supervised, but not unsupervised, models may explain it cortical representation</article-title>. <source hwp:id="source-22">PLoS Computational Biology</source> <volume>10</volume>, <fpage>e1003915</fpage> (<year>2014</year>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Güçlü U."><surname>Güçlü</surname>, <given-names>U.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="van Gerven M. A."><surname>van Gerven</surname>, <given-names>M. A.</given-names></string-name> <article-title hwp:id="article-title-21">Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source hwp:id="source-23">Journal of Neuroscience</source> <volume>35</volume>, <fpage>10005</fpage>–<lpage>10014</lpage> (<year>2015</year>).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2 xref-ref-24-3 xref-ref-24-4"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Eslami S. A."><surname>Eslami</surname>, <given-names>S. A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-22">Neural scene representation and rendering</article-title>. <source hwp:id="source-24">Science</source> <volume>360</volume>, <fpage>1204</fpage>–<lpage>1210</lpage> (<year>2018</year>).</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Kulkarni T. D."><surname>Kulkarni</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Whitney W. F."><surname>Whitney</surname>, <given-names>W. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kohli P."><surname>Kohli</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J."><surname>Tenenbaum</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-23">Deep convolutional inverse graphics network</article-title>. In <source hwp:id="source-25">Advances in Neural Information Processing Systems</source>, <fpage>2539</fpage>–<lpage>2547</lpage> (<year>2015</year>).</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kulkarni T. D."><surname>Kulkarni</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-24">Efficient and robust analysis-by-synthesis in vision: A computational framework, behavioral tests, and modeling neuronal representations</article-title>. In <source hwp:id="source-26">Annual Conference of the Cognitive Science Society</source> (<year>2015</year>).</citation></ref><ref id="c27" hwp:id="ref-27"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Kundu A."><surname>Kundu</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Y."><surname>Li</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rehg J. M."><surname>Rehg</surname>, <given-names>J. M.</given-names></string-name> <article-title hwp:id="article-title-25">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</article-title>. In <source hwp:id="source-27">CVPR</source> (<year>2018</year>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="George D."><surname>George</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-26">A generative vision model that trains with high data efficiency and breaks text-based captchas</article-title>. <source hwp:id="source-28">Science</source> (<year>2017</year>). URL <ext-link l:rel="related" l:ref-type="uri" l:ref="http://science.science.sciencemag.org/content/early/2017/10/26/science.aag2612" ext-link-type="uri" xlink:href="http://science.science.sciencemag.org/content/early/2017/10/26/science.aag2612" hwp:id="ext-link-2">http://science.science.sciencemag.org/content/early/2017/10/26/science.aag2612</ext-link> <ext-link l:rel="related" l:ref-type="uri" l:ref="http://science.sciencemag.org/content/early/2017/10/26/science.aag2612.full.pdf" ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/early/2017/10/26/science.aag2612.full.pdf" hwp:id="ext-link-3">http://science.sciencemag.org/content/early/2017/10/26/science.aag2612.full.pdf</ext-link></citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2 xref-ref-29-3"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Wu J."><surname>Wu</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-27">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</article-title>. In <source hwp:id="source-29">Advances In Neural Information Processing Systems</source> (<year>2017</year>).</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.30" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Grossman S."><surname>Grossman</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-28">Deep convolutional modeling of human face selective columns reveals their role in pictorial face representation</article-title>. <source hwp:id="source-30">bioRxiv</source> <fpage>444323</fpage> (<year>2018</year>).</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Parkhi O. M."><surname>Parkhi</surname>, <given-names>O. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vedaldi A."><surname>Vedaldi</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zisserman A."><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-29">Deep Face Recognition</article-title>. In <source hwp:id="source-31">British Machine Vision Conference (BMVC)</source> (<year>2015</year>).</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Hinton G. E."><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frey B. J."><surname>Frey</surname>, <given-names>B. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Neal R. M."><surname>Neal</surname>, <given-names>R. M.</given-names></string-name> <article-title hwp:id="article-title-30">The “wake-sleep” algorithm for unsuper-vised neural networks</article-title>. <source hwp:id="source-32">Science</source> <volume>268</volume>, <fpage>1158</fpage> (<year>1995</year>).</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hinton G. E."><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Neal R. M."><surname>Neal</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zemel R. S."><surname>Zemel</surname>, <given-names>R. S.</given-names></string-name> <article-title hwp:id="article-title-31">The Helmholtz machine</article-title>. <source hwp:id="source-33">Neural Computation</source> <volume>7</volume>, <fpage>889</fpage>–<lpage>904</lpage> (<year>1995</year>).</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Conway B. R."><surname>Conway</surname>, <given-names>B. R.</given-names></string-name> <article-title hwp:id="article-title-32">The organization and operation of inferior temporal cortex</article-title>. <source hwp:id="source-34">Annual review of vision science</source> (<year>2018</year>).</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>35.</label><citation publication-type="other" citation-type="journal" ref:id="282798v2.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><collab hwp:id="collab-1">IEEE</collab>. <source hwp:id="source-35">A 3D Face Model for Pose and Illumination Invariant Face Recognition</source>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Stuhlmüller A."><surname>Stuhlmüller</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Taylor J."><surname>Taylor</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Goodman N."><surname>Goodman</surname>, <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-33">Learning stochastic inverses</article-title>. In <source hwp:id="source-36">Advances in neural information processing systems</source>, <fpage>3048</fpage>–<lpage>3056</lpage> (<year>2013</year>).</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Lin H. W."><surname>Lin</surname>, <given-names>H. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tegmark M."><surname>Tegmark</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Rolnick D."><surname>Rolnick</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-34">Why does deep and cheap learning work so well?</article-title> <source hwp:id="source-37">Journal of Statistical Physics</source> <volume>168</volume>, <fpage>1223</fpage>–<lpage>1247</lpage> (<year>2017</year>).</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Jackson A. S."><surname>Jackson</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bulat A."><surname>Bulat</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Argyriou V."><surname>Argyriou</surname>, <given-names>V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tzimiropoulos G."><surname>Tzimiropoulos</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-35">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</article-title>. <source hwp:id="source-38">Proceedings of the International Conference on Computer Vision</source> (<year>2017</year>).</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1 xref-ref-39-2 xref-ref-39-3"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> <article-title hwp:id="article-title-36">Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title>. <source hwp:id="source-39">Science</source> <volume>330</volume>, <fpage>845</fpage>–<lpage>851</lpage> (<year>2010</year>).</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>40.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Leopold D. A."><surname>Leopold</surname>, <given-names>D. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bondar I. V."><surname>Bondar</surname>, <given-names>I. V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Giese M. A."><surname>Giese</surname>, <given-names>M. A.</given-names></string-name> <article-title hwp:id="article-title-37">Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title>. <source hwp:id="source-40">Nature</source> <volume>442</volume>, <fpage>572</fpage> (<year>2006</year>).</citation></ref><ref id="c41" hwp:id="ref-41"><label>41.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Livingstone M. S."><surname>Livingstone</surname>, <given-names>M. S.</given-names></string-name> <article-title hwp:id="article-title-38">A face feature space in the macaque temporal lobe</article-title>. <source hwp:id="source-41">Nature neuroscience</source> <volume>12</volume>, <fpage>1187</fpage> (<year>2009</year>).</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Chang L."><surname>Chang</surname>, <given-names>L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> <article-title hwp:id="article-title-39">The Code for Facial Identity in the Primate Brain</article-title>. <source hwp:id="source-42">Cell</source> <volume>169</volume>, <fpage>1013</fpage>–<lpage>1028</lpage> (<year>2017</year>).</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>43.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Kietzmann T. C."><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Swisher J. D."><surname>Swisher</surname>, <given-names>J. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="König P."><surname>König</surname>, <given-names>P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tong F."><surname>Tong</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-40">Prevalence of selectivity for mirror-symmetric views of faces in the ventral and dorsal visual pathways</article-title>. <source hwp:id="source-43">The Journal of Neuroscience</source> <volume>32</volume>, <fpage>11763</fpage>–<lpage>11772</lpage> (<year>2012</year>).</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>44.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Guntupalli J. S."><surname>Guntupalli</surname>, <given-names>J. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wheeler K. G."><surname>Wheeler</surname>, <given-names>K. G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Gobbini M. I."><surname>Gobbini</surname>, <given-names>M. I.</given-names></string-name> <article-title hwp:id="article-title-41">Disentangling the representation of identity from head view along the human face processing pathway</article-title>. <source hwp:id="source-44">Cerebral Cortex</source> <volume>27</volume>, <fpage>46</fpage>–<lpage>53</lpage> (<year>2016</year>).</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Marr D."><surname>Marr</surname>, <given-names>D.</given-names></string-name> <source hwp:id="source-45">Vision: A computational investigation into the human representation and processing of visual information</source>, vol. <volume>2</volume> (<year>1982</year>).</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Hancock P. J."><surname>Hancock</surname>, <given-names>P. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bruce V."><surname>Bruce</surname>, <given-names>V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Burton A. M."><surname>Burton</surname>, <given-names>A. M.</given-names></string-name> <article-title hwp:id="article-title-42">Recognition of unfamiliar faces</article-title>. <source hwp:id="source-46">Trends in cognitive sciences</source> <volume>4</volume>, <fpage>330</fpage>–<lpage>337</lpage> (<year>2000</year>).</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.47" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Gregory R. L."><surname>Gregory</surname>, <given-names>R. L.</given-names></string-name> <source hwp:id="source-47">The intelligent eye</source>. (<year>1970</year>).</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1 xref-ref-48-2"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Hartung B."><surname>Hartung</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schrater P. R."><surname>Schrater</surname>, <given-names>P. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blthoff H. H."><surname>Blthoff</surname>, <given-names>H. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kersten D."><surname>Kersten</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Franz V.H."><surname>Franz</surname>, <given-names>V.H.</given-names></string-name> <article-title hwp:id="article-title-43">Is prior knowledge of object geometry used in visually guided reaching?</article-title> <source hwp:id="source-48">Journal of Vision</source> <volume>5</volume>, <fpage>2</fpage> (<year>2005</year>). URL +<ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1167/5.6.2./data/journals/jov/933512/jov-5-6-2.pdf" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/5.6.2./data/journals/jov/933512/jov-5-6-2.pdf" hwp:id="ext-link-4">http://dx.doi.org/10.1167/5.6.2./data/journals/jov/933512/jov-5-6-2.pdf</ext-link>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>49.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Gregory R. L."><surname>Gregory</surname>, <given-names>R. L.</given-names></string-name> <article-title hwp:id="article-title-44">Knowledge in perception and illusion</article-title>. <source hwp:id="source-49">Philosophical Transactions of the Royal Society of London B: Biological Sciences</source> <volume>352</volume>, <fpage>1121</fpage>–<lpage>1127</lpage> (<year>1997</year>).</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Grimaldi P."><surname>Grimaldi</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Saleem K. S."><surname>Saleem</surname>, <given-names>K. S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tsao D."><surname>Tsao</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-45">Anatomical connections of the functionally defined face patches in the macaque monkey</article-title>. <source hwp:id="source-50">Neuron</source> <volume>90</volume>, <fpage>1325</fpage>–<lpage>1342</lpage> (<year>2016</year>).</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>51.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Leibo J. Z."><surname>Leibo</surname>, <given-names>J. Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liao Q."><surname>Liao</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anselmi F."><surname>Anselmi</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poggio T."><surname>Poggio</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-46">View-tolerant face recognition and hebbian learning imply mirror-symmetric neural tuning to head orientation</article-title>. <source hwp:id="source-51">Current Biology</source> <volume>27</volume>, <fpage>62</fpage>–<lpage>67</lpage> (<year>2017</year>).</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.52" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Sabour S."><surname>Sabour</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frosst N."><surname>Frosst</surname>, <given-names>N.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G. E."><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> <article-title hwp:id="article-title-47">Dynamic routing between capsules</article-title>. In <source hwp:id="source-52">Advances in Neural Information Processing Systems</source> (<year>2017</year>).</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.53" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Yovel G."><surname>Yovel</surname>, <given-names>G.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> <article-title hwp:id="article-title-48">Face recognition systems in monkey and human: are they the same thing?</article-title> <source hwp:id="source-53">F1000prime reports</source> <volume>5</volume> (<year>2013</year>).</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moeller S."><surname>Moeller</surname>, <given-names>S.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> <article-title hwp:id="article-title-49">Comparing face patch systems in macaques and humans</article-title>. <source hwp:id="source-54">Proceedings of the National Academy of Sciences</source> <volume>105</volume>, <fpage>19514</fpage>–<lpage>19519</lpage> (<year>2008</year>).</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>55.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Kreiman G."><surname>Kreiman</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koch C."><surname>Koch</surname>, <given-names>C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Fried I."><surname>Fried</surname>, <given-names>I.</given-names></string-name> <article-title hwp:id="article-title-50">Category-specific visual responses of single neurons in the human medial temporal lobe</article-title>. <source hwp:id="source-55">Nature neuroscience</source> <volume>3</volume>, <fpage>946</fpage> (<year>2000</year>).</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>56.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Landi S. M."><surname>Landi</surname>, <given-names>S. M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> <article-title hwp:id="article-title-51">Two areas for familiar face recognition in the primate brain</article-title>. <source hwp:id="source-56">Science</source> <volume>357</volume>, <fpage>591</fpage>–<lpage>595</lpage> (<year>2017</year>).</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><label>57.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Hong H."><surname>Hong</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamins D. L."><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Majaj N. J."><surname>Majaj</surname>, <given-names>N. J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="DiCarlo J. J."><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> <article-title hwp:id="article-title-52">Explicit information for category-orthogonal object properties increases along the ventral stream</article-title>. <source hwp:id="source-57">Nature neuroscience</source> <volume>19</volume>, <fpage>613</fpage> (<year>2016</year>).</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>58.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.58" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Wu J."><surname>Wu</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-53">Learning 3D Shape Priors for Shape Completion and Reconstruction</article-title>. In <source hwp:id="source-58">European Conference on Computer Vision (ECCV)</source> (<year>2018</year>).</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>59.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.59" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Zhang X."><surname>Zhang</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-54">Learning to Reconstruct Shapes from Unseen Classes</article-title>. In <source hwp:id="source-59">Advances in Neural Information Processing Systems (NIPS)</source> (<year>2018</year>).</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>60.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Yamins D. L."><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="DiCarlo J. J."><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> <article-title hwp:id="article-title-55">Using goal-driven deep learning models to understand sensory cortex</article-title>. <source hwp:id="source-60">Nature Neuroscience</source> <volume>19</volume>, <fpage>356</fpage> (<year>2016</year>).</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1 xref-ref-61-2"><label>61.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.61" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-56">Causal and compositional generative models in online perception</article-title>. In <source hwp:id="source-61">Annual Conference of the Cognitive Science Society</source> (<year>2017</year>).</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><label>62.</label><citation publication-type="book" citation-type="book" ref:id="282798v2.62" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Moreno P."><surname>Moreno</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Williams C. K."><surname>Williams</surname>, <given-names>C. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nash C."><surname>Nash</surname>, <given-names>C.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kohli P."><surname>Kohli</surname>, <given-names>P.</given-names></string-name> <chapter-title>Overcoming occlusion with inverse graphics</chapter-title>. In <source hwp:id="source-62">European Conference on Computer Vision (ECCV) Workshops</source>, <fpage>170</fpage>–<lpage>185</lpage> (<publisher-name>Springer</publisher-name>, <year>2016</year>).</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1 xref-ref-63-2"><label>63.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.63" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Wu J."><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kohli P."><surname>Kohli</surname>, <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-57">Neural scene de-rendering</article-title>. In <source hwp:id="source-63">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<year>2017</year>).</citation></ref><ref id="c64" hwp:id="ref-64"><label>64.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.64" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Du Y."><surname>Du</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-58">Learning to Exploit Stability for 3D Scene Parsing</article-title>. In <source hwp:id="source-64">Advances in Neural Information Processing Systems (NIPS)</source> (<year>2018</year>).</citation></ref><ref id="c65" hwp:id="ref-65"><label>65.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.65" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Romaszko L."><surname>Romaszko</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-59">Vision-as-inverse-graphics: Obtaining a rich 3d explanation of a scene from a single image</article-title>. In <source hwp:id="source-65">IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>851</fpage>–<lpage>859</lpage> (<year>2017</year>).</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><label>66.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.66" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Eslami S."><surname>Eslami</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-60">Attend, Infer, Repeat: Fast Scene Understanding with Generative Models</article-title>. In <source hwp:id="source-66">Advances in Neural Information Processing Systems</source> (<year>2017</year>).</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><label>67.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Battaglia P. W."><surname>Battaglia</surname>, <given-names>P. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hamrick J. B."><surname>Hamrick</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-61">Simulation as an engine of physical scene understanding</article-title>. <source hwp:id="source-67">Proceedings of the National Academy of Sciences</source> <volume>110</volume>, <fpage>18327</fpage>–<lpage>18332</lpage> (<year>2013</year>).</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><label>68.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.68" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Wu J."><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lim J. J."><surname>Lim</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freeman W. T."><surname>Freeman</surname>, <given-names>W. T.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-62">Galileo: Perceiving physical object properties by integrating a physics engine with deep learning</article-title>. In <source hwp:id="source-68">Advances in Neural Information Processing Systems</source>, <fpage>127</fpage>–<lpage>135</lpage> (<year>2015</year>).</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1"><label>69.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.69" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Goodfellow I."><surname>Goodfellow</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-63">Generative adversarial nets</article-title>. In <source hwp:id="source-69">Advances in Neural Information Processing Systems</source>, <fpage>2672</fpage>–<lpage>2680</lpage> (<year>2014</year>).</citation></ref></ref-list><sec id="s3" hwp:id="sec-6"><title hwp:id="title-7">Methods</title><sec id="s3a" hwp:id="sec-7"><title hwp:id="title-8">Generative model</title><p hwp:id="p-45">Our generative model builds on and extends the Basel Face Model (BFM)<sup><xref rid="sc1" ref-type="bibr" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">1</xref></sup>, a statistical shape and texture model obtained by applying probabilistic principal component analysis<sup><xref rid="sc2" ref-type="bibr" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">2</xref></sup> on a dataset of 200 laser-scanned human heads (100 female, 100 male). BFM is publicly available and consists of a mean (or norm) face shape, a mean texture, two sets of principal components of variance, one for shape and the other for texture, and their corresponding eigenvectors that projects these principal components to 3D meshes.</p><p hwp:id="p-46">The principal components of shape <italic toggle="yes">S</italic> and texture <italic toggle="yes">T</italic> accept a standard normal distribution such that Pr(<italic toggle="yes">S</italic>) and Pr(<italic toggle="yes">T</italic>) are each multivariate standard normal distributions with <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="282798v2_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>. Each sample from Pr(<italic toggle="yes">S</italic>) (or Pr(<italic toggle="yes">T</italic>)) is a vector in a <italic toggle="yes">D</italic> = <italic toggle="yes">D</italic><sub><italic toggle="yes">S</italic></sub> (or <italic toggle="yes">D</italic> = <italic toggle="yes">D</italic><sub><italic toggle="yes">T</italic></sub>) dimensional space specifying a direction and a magnitude to perturb the mean face shape (or the mean texture) to obtain a new unique shape (or texture). Mean shape and texture correspond to <italic toggle="yes">s</italic> = {0, 0,<italic toggle="yes">…</italic>, 0} and <italic toggle="yes">t</italic> = {0, 0,<italic toggle="yes">…</italic>, 0}. (Uppercase letters are used for random variables and lowercase letters are used for assignments of these random variables to a sample from their respective distributions. Nonrandom model parameters, such as <italic toggle="yes">D</italic> are also uppercase.) We set <italic toggle="yes">D</italic><sub><italic toggle="yes">S</italic></sub>, <italic toggle="yes">D</italic><sub><italic toggle="yes">T</italic></sub> = 200 in our analysis. We found that the exact values of <italic toggle="yes">D</italic><sub><italic toggle="yes">S</italic></sub> and <italic toggle="yes">D</italic><sub><italic toggle="yes">T</italic></sub> did not matter as long as they were not too small, which leads to very little variation across the samples.</p><p hwp:id="p-47">We used the part-based version of BFM where the principal components of shape and texture are partitioned across four canonical face parts: (i) outline of the face, (ii) eyes area, (iii) nose area, and (iv) mouth area. Each face-part (e.g., shape of the nose area or texture of the eyes area, etc.) was represented using 200<italic toggle="yes">/</italic>4 = 50 principal components. There are four advantages of using BFM: it (i) allows a separable representation of shape and texture, (ii) provides a probability distribution over both of these properties, (iii) allows us to work with lower dimensional continuous vectors (400 dimensions in this case) as opposed to very high dimensional meshes (e.g., meshes consisting of about 1 million vertices), and (iv) consists of dimensions that are often (but not always) perceptually interpretable (e.g., a dimension controlling the inter-eye distance).</p><p hwp:id="p-48">The full scene description in the model also requires choosing extrinsic scene parameters including the lighting direction and viewing direction (or equivalently, head pose). In our simulations, we used Lambertian lighting where the lighting direction <italic toggle="yes">L</italic> can vary along azimuth <italic toggle="yes">L</italic><sub><italic toggle="yes">a</italic></sub> and elevation <italic toggle="yes">L</italic><sub><italic toggle="yes">e</italic></sub>. Pr(<italic toggle="yes">L</italic><sub><italic toggle="yes">a</italic></sub>) and Pr(<italic toggle="yes">L</italic><sub><italic toggle="yes">e</italic></sub>) are uniform distributions in the range {-1.4<sup><italic toggle="yes">rad</italic></sup>} to {1.4<sup><italic toggle="yes">rad</italic></sup>}. The head pose <italic toggle="yes">P</italic> can vary along the z-axis <italic toggle="yes">P</italic><sub><italic toggle="yes">z</italic></sub> with Pr(<italic toggle="yes">P</italic><sub><italic toggle="yes">z</italic></sub>) a uniform distribution in the range −1.5<sup><italic toggle="yes">rad</italic></sup> to 1.5<sup><italic toggle="yes">rad</italic></sup>, and the x-axis <italic toggle="yes">P</italic><sub><italic toggle="yes">x</italic></sub> with Pr(<italic toggle="yes">P</italic><sub><italic toggle="yes">x</italic></sub>) a uniform distribution in the range −0.75<sup><italic toggle="yes">rad</italic></sup> to 0.75<sup><italic toggle="yes">rad</italic></sup>. Finally, we rendered each scene to a 227 × 227 pixels color image, unless otherwise mentioned, with back-face culling.</p><sec id="s3a1" hwp:id="sec-8"><title hwp:id="title-9">Synthetic FIV image sets</title><p hwp:id="p-49">The FIV-S stimuli underlying <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-4" hwp:rel-id="F9">Extended Data Fig. 4</xref>-<xref rid="figE8" ref-type="fig" hwp:id="xref-fig-13-2" hwp:rel-id="F13">8</xref> used the pose distributions in <xref rid="tblE1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Extended Data Table 1</xref>. Each of the 25 identities (i.e., unique pairs of shape and texture properties) were rendered at 7 different poses and with frontal lighting.</p><p hwp:id="p-50">The image set underlying <xref rid="figE8" ref-type="fig" hwp:id="xref-fig-13-3" hwp:rel-id="F13">Extended Data Fig. 8B</xref> (referred to as FIV-S-2) used the same prior over lighting and pose as the generative model, Pr(<italic toggle="yes">L</italic>) and Pr(<italic toggle="yes">P</italic>), described above. It used the same 25 identities as FIV-S image set each rendered 7 times (each with its own randomly drawn pose and lighting parameters) making 175 images in total. Additionally, to increase the variability at the level of raw and attended images, we converted half of these images to gray-scale.</p></sec><sec id="s3a2" hwp:id="sec-9"><title hwp:id="title-10">Conventional top-down inference with MCMC</title><p hwp:id="p-51">Given a single image of a face as observation, <italic toggle="yes">I</italic>, and an approximate rendering engine, <italic toggle="yes">G</italic>(·) – a combination of the z-buffer Ψ(·) and image rendering Φ(·) stages introduced in the main text – face processing in this probabilistic graphics program can be defined as inverting the graphics pipeline using Bayes’s rule:
<disp-formula id="ueqn1" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="282798v2_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-7"/></alternatives>
</disp-formula>
where I<sub><italic toggle="yes">S</italic></sub> is a top-down sample generated using the probabilistic graphics program, and <italic toggle="yes">δ</italic>(·) is a Dirac delta function. (We dropped the corresponding Dirac delta functions in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-2" hwp:rel-id="disp-formula-1">Equation 1</xref> in the main text in order to avoid cluttered notation.) We assume that the image likelihood is an isotropic standard Gaussian distribution, <italic toggle="yes">P</italic> (<italic toggle="yes">I|I</italic><sub><italic toggle="yes">S</italic></sub>) = <italic toggle="yes">N</italic> (<italic toggle="yes">I</italic>; <italic toggle="yes">I</italic><sub><italic toggle="yes">S</italic></sub>, Σ). Note that the posterior space is of high-dimensionality consisting of more than 400 (404, to be exact) highly coupled shape, texture, lighting direction, and head pose variables, making inference a significant challenge.</p><p hwp:id="p-52">Markov chain Monte Carlo (MCMC) methods provide a general framework for inference in generative models and have a long history of application to inverse graphics problems<sup><xref rid="sc3" ref-type="bibr" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">3</xref></sup>. For this specific face model, we explored both traditional single-site MCMC and a more advanced and efficient multi-site elliptical slice sampler<sup><xref rid="sc4" ref-type="bibr" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">4</xref></sup> to infer the shape and texture properties given an image, <italic toggle="yes">I</italic><sub><italic toggle="yes">D</italic></sub><sup>5</sup>. Proposals in elliptical slice sampling are based on defining an ellipse using an auxiliary random variable <italic toggle="yes">X ∼ N</italic> (0, Σ) around the current state of the latent variables (shape and texture properties), and sampling from an adaptive bracket on this ellipse based on the log-likelihood function. For the lighting direction and pose parameters, single-site Metropolis-Hastings steps are used. At each MCMC sweep, the algorithm iterates a proposal-and-acceptance loop over twelve groups of random variables: four shape vectors (each of length 50), four texture vectors (each of length 50), and four scalars for lighting direction and pose parameters. The detailed form of the proposal and acceptance functions can be found in<sup><xref rid="sc4" ref-type="bibr" hwp:id="xref-ref-73-2" hwp:rel-id="ref-73">4</xref></sup>. This method often converges to reasonable inferences within a few hundred iterations, although with substantial variance across multiple runs of the algorithm as shown in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-18" hwp:rel-id="F1">Fig. 1c</xref> in the main text. The y-axis values in that figure are the log-likelihood scores <italic toggle="yes">P</italic> (<italic toggle="yes">I|S, T, L, P</italic>) of 100 individual chains each given as input a different face image (with clean background). The log-likelihood score for each iteration of each chain is calculated by rendering and comparing the current MCMC estimate with the input image. The log-likelihood scores for the EIG network on <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-19" hwp:rel-id="F1">Fig. 1c</xref> are computed in the same way except its estimates are outputs at its layer <italic toggle="yes">f</italic><sub>5</sub>.</p><p hwp:id="p-53">The EIG estimates are computed almost instantaneously, with no iterations, yet achieve a higher score and lower variance (mean score, red line, <italic toggle="yes">∼</italic> 2.5 <italic toggle="yes">×</italic> 10<sup>5</sup>; standard deviation <italic toggle="yes">∼</italic> 1 <italic toggle="yes">×</italic> 10<sup>5</sup>; pink region shows worst to best scores) than the MCMC algorithm. The MCMC algorithm requires a great deal more time because it must perform hundreds of iterations to achieve a similar level of inference quality (mean score <italic toggle="yes">∼ -</italic>5 <italic toggle="yes">×</italic> 10<sup>5</sup>; standard deviation <italic toggle="yes">∼</italic> 8 <italic toggle="yes">×</italic> 10<sup>5</sup>; thick black line shows the mean, thinner black curves show 100 individual runs of the algorithm).</p><p hwp:id="p-54">In summary, the EIG network which we describe below and in the main text reliably produces inferences that are as accurate as the best of these MCMC runs but far more quickly. EIG avoids the need for iterative computation by estimating 3D shape and texture properties via a single feedforward pass through a deep inference network. Further comparisons between MCMC and efficient inference networks for inverse graphics (using an earlier version of EIG, without the initial face detection stage and using a more limited training regime and loss function) can be found in<sup><xref rid="sc6" ref-type="bibr" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">6</xref></sup>.</p></sec></sec><sec id="s3b" hwp:id="sec-10"><title hwp:id="title-11">EIG model</title><p hwp:id="p-55">The EIG model is a multistage neural network that attempts to estimate the MAP (Maximum A Posteriori) 3D scene properties and identity of an observed face image (approximately maximizing the posterior in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-3" hwp:rel-id="disp-formula-1">Equation 1</xref> of the main text). EIG comprises three inference modules arranged in sequence to take advantage of the conditional independence structure in the generative (graphics) model. These three modules compute (1) a segmentation and normalization of the face image; (2) an estimate of the 3D face shape and texture; and (3) a classification of the individual whose face is observed.</p><p hwp:id="p-56">Below we describe how each of these modules is constructed. The EIG network can also be seen as a multitask network that is designed to solve several tasks at once, including segmentation, 3D scene reconstruction, and identification, where the generative model determines which tasks should be solved and the conditional independence structure of the generative model determines the order in which they should be solved.</p><sec id="s3b1" hwp:id="sec-11"><title hwp:id="title-12">Estimating face image given a transformed image, Pr(<italic toggle="yes">I|O</italic>)</title><p hwp:id="p-57">Given an observation consisting of a face image with cluttered background, <italic toggle="yes">O</italic>, MAP inference involves estimating <italic toggle="yes">I∗</italic> that maximizes Pr(<italic toggle="yes">I|O</italic>). This can be achieved by a segmentation of the observed image that only consists of the face-proper region and excludes the rest.</p><p hwp:id="p-58">We implemented this inference problem using a convolutional neural network<sup><xref rid="sc7" ref-type="bibr" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">7</xref>, <xref rid="sc8" ref-type="bibr" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">8</xref></sup>, referred to as <italic toggle="yes">f</italic><sub>1</sub> in the main text. We took a recent convolutional neural network with an hour-glass architecture that is trained for volumetric 3D segmentation of faces from images<sup><xref rid="sc9" ref-type="bibr" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">9</xref></sup>. This model takes as input an image and outputs a 3D voxel map where a value of 1 indicates inside the face region and a value of 0 indicates outside the face region. The output of this network is a rough and noisy estimation of the face shape in the form of a voxel grid, <italic toggle="yes">V</italic><sub><italic toggle="yes">xyz</italic></sub>, of dimensions 192 (width) <italic toggle="yes">×</italic>192 (height) <italic toggle="yes">×</italic>200 (depth), which we found in practice often includes filled but disconnected regions that are outside the face-proper region.</p><p hwp:id="p-59">We adapted this output for accurate 2D segmentation of the face-proper region in the following way. We first sum over the depth dimension of <italic toggle="yes">V</italic><sub><italic toggle="yes">xyz</italic></sub> to obtain a 2D map, <italic toggle="yes">V</italic><sub><italic toggle="yes">xy</italic></sub>, of dimensions 192 <italic toggle="yes">×</italic> 192. We then binarize <italic toggle="yes">V</italic><sub><italic toggle="yes">xy</italic></sub> (i.e., replace all non-zero entries with 1) and compute its connected components. We produce a segmentation of <italic toggle="yes">O</italic> using the largest connected region of <italic toggle="yes">V</italic><sub><italic toggle="yes">xy</italic></sub> as the mask. Finally, we normalize this region by zooming in on the segmented image using bicubic interpolation such that the resulting image’s longer dimension is 227. In practice, this procedure yields good estimates for <italic toggle="yes">I∗</italic>. We also applied a small amount of translation (25 pixels) away from left or right border for the normalized FIV images, which better aligned them with the samples from the generative model.</p></sec><sec id="s3b2" hwp:id="sec-12"><title hwp:id="title-13">Scene parameters given face image, Pr(<italic toggle="yes">S, T, L, P |I</italic>)</title><p hwp:id="p-60">Given a face image as input, MAP inference involves estimating the scene properties (latent variables in the graphics program), {<italic toggle="yes">S∗, T ∗, L∗, P ∗</italic>} maximizing Pr(<italic toggle="yes">S, T, L, P |I</italic>). We accomplish this using an inference model by learning to map inputs to their underlying latent variables in the graphics program.</p><p hwp:id="p-61">Our inference model is a convolutional neural network with each layer implementing a cascade of functions including convolution, rectified linear activation, pooling, and normalization. We obtained this model by modifying Alexnet network architecture in the following way<sup><xref rid="sc10" ref-type="bibr" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">10</xref></sup>: We removed its top two fully-connected layers and replaced them with a single new fully-connected layer. The details of the resulting network architecture is given in <xref rid="tblE2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Extended Data Table 2</xref>.</p><p hwp:id="p-62">We initialized the parameters of <italic toggle="yes">f</italic><sub>2</sub>, <italic toggle="yes">f</italic><sub>3</sub>, and <italic toggle="yes">f</italic><sub>4</sub> in the inference model using the corresponding weights of Alexnet that was pretrained on a large corpus of images, namely the Places dataset<sup><xref rid="sc11" ref-type="bibr" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">11</xref></sup>. The pretrained network weights are provided by its authors and can be downloaded at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://places2.csail.mit.edu/models_places365/alexnet_places365.caffemodel" ext-link-type="uri" xlink:href="http://places2.csail.mit.edu/models_places365/alexnet_places365.caffemodel" hwp:id="ext-link-5">http://places2.csail.mit.edu/models_places365/alexnet_places365.caffemodel</ext-link>. This dataset consists of about 2.5 million images and their corresponding place labels such as “beach,” “classroom,” “landscape,” etc. (365-way categorization). The parameters of the new fully-connected layer (also referred to as scene properties layer or latents layer) were initialized randomly. Using these pretrained weights ensured that the earlier layers of the inference model provided a good generic visual feature extractor not specifically related to faces. We also avoided using a face corpus pretrained weights as this would require access to a large labeled dataset of weights, which EIG doesn’t require.</p><p hwp:id="p-63">To learn the mapping from images to their latent variable representations, we drew 200, 000 random samples from the generative model. Each resulting image was a 227 <italic toggle="yes">×</italic> 227 color image and each target was a concatenation of all the latent variables making a vector of length 404 (200 shape properties, 200 texture properties, and 4 extrinsic scene parameters). Half of the images were added background and were first segmented and normalized using <italic toggle="yes">f</italic><sub>1</sub>, whereas the other half of the images were not added background and were directly used during training. We finetuned the parameters of <italic toggle="yes">f</italic><sub>3</sub>, <italic toggle="yes">f</italic><sub>4</sub> starting from their pretrained weights and trained the parameters of <italic toggle="yes">f</italic><sub>5</sub> starting from random initialization. The network learns a mapping from these images to their latent variable representations, which we accomplish minimizing a mean squared error (MSE) loss function using stochastic gradient descent with minibatches of 20 examples. In our simulations we used a learning rate of 10<sup>−4</sup>. In order to ensure that gradients were large enough throughout training, we multiplied the target latent variable vectors by 10. We accounted for this preprocessing step by dividing the outputs of the network by 10 at test time. We trained the model for 75 epochs.</p></sec><sec id="s3b3" hwp:id="sec-13"><title hwp:id="title-14">Person identity given scene parameters, Pr(<italic toggle="yes">F |S, T, L, P</italic>)</title><p hwp:id="p-64">We provide the details of Pr(<italic toggle="yes">F</italic>) before describing this final component of the inference model. In principle, this distribution is over a finite set of familiar individuals but allowing for possibility of encountering a new, unfamiliar individual<sup><xref rid="sc12" ref-type="bibr" hwp:id="xref-ref-81-1" hwp:rel-id="ref-81">12</xref></sup>. Here, we approximated Pr(<italic toggle="yes">F</italic>) as a uniform distribution over a set of familiar individuals. Specifically, we treated Pr(<italic toggle="yes">F</italic>) as a multinomial categorical distribution with <italic toggle="yes">K</italic> outcomes (i.e., <italic toggle="yes">K</italic> unique person identities) with each outcome equally probable. Each person identity is chosen as a pair of shape and texture properties and denoted as Pr(<italic toggle="yes">S, T |F</italic>).</p><p hwp:id="p-65">Given scene properties, MAP inference involves estimating the person identity, <italic toggle="yes">F ∗</italic>, maximizing Pr(<italic toggle="yes">F |S, T, L, P</italic>). To estimate <italic toggle="yes">F ∗</italic> given scene properties, we extended the inference model with a new fully-connected layer, <italic toggle="yes">f</italic><sub>6</sub>, of length <italic toggle="yes">K</italic>. To learn this mapping from scene properties to identities, we generated a new dataset of <italic toggle="yes">K ∗ M</italic> images where <italic toggle="yes">M</italic> is the number of times the shape and texture properties associated with each of the <italic toggle="yes">K</italic> identities were rendered. For each image, we randomly draw the lighting direction and pose properties from their respective prior distributions, Pr(<italic toggle="yes">L</italic>) and Pr(<italic toggle="yes">P</italic>). In our simulations we set <italic toggle="yes">K</italic> to 25 and <italic toggle="yes">M</italic> to 400.</p><p hwp:id="p-66">For our FIV experiments, we do not have access to the ground truth shapes or textures of the 25 person identities and therefore we cannot use the graphics program for generating a training image set. Instead, for a given identity, we obtained <italic toggle="yes">M</italic> = 400 images by a bootstrapping procedure applied to the whole set of 7 attended face images for that identity. Given the image bounding box of the face proper region, we randomly and independently stretched or shrank each side of the bounding box by 15%. We resized the resulting bounding boxes by a randomly chosen scale between 75 - 99%. Finally, we translated the resulting bounding boxes in the image randomly but ensuring that the entire face-proper region remained in the image. We refer to the resulting image set as the bootstrapped FIV image set.</p><p hwp:id="p-67">The training procedure was identical for the FIV and FIV-S experiments. We train the new identity classification layer <italic toggle="yes">f</italic><sub>6</sub> and finetune the scene properties layer <italic toggle="yes">f</italic><sub>5</sub> using <italic toggle="yes">M ∗ K</italic> = 10, 000 images and their underlying person identity labels minimizing cross-entropy loss<sup><xref rid="sc13" ref-type="bibr" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">13</xref></sup>. We used a learning rate of 0.0005. We performed stochastic gradient descent with minibatches of 20 examples until the training performance was high (e.g., <italic toggle="yes">&gt;</italic> 95%). In practice, it took two additional epochs of training for the FIV-S image set, and 20 additional epochs of training for the FIV image set.</p><p hwp:id="p-68">All of our models are implemented using the pytorch machine learning library and will be made publicly available at the time of publication at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/iyildirim/efficient_inverse_graphics" ext-link-type="uri" xlink:href="https://github.com/iyildirim/efficient_inverse_graphics" hwp:id="ext-link-6">https://github.com/iyildirim/efficient_inverse_graphics</ext-link>.</p></sec><sec id="s3b4" hwp:id="sec-14"><title hwp:id="title-15">Weaknesses of EIG</title><p hwp:id="p-69">We note two potential weaknesses of the inference model. First, it may not perform as well when the segmentation step <italic toggle="yes">f</italic><sub>1</sub> fails (e.g., too much of the background is left in the attended face image). We observed that this is an issue only if the face doesn’t cover a spatially significant portion of the input image. Second, the model’s reconstruction accuracy may degrade when the observed faces have shapes and textures far from the regions of high prior probability in the generative model, Pr(<italic toggle="yes">S, T</italic>). We see these weaknesses mostly as challenges for the model as currently implemented, with a rather limited set of face experiences for training compared to what an individual encounters over the course of their lifetime – let alone what is effectively a much broader base of experience over evolutionary time that also shapes the brain’s representations.</p><p hwp:id="p-70">The training procedure underlying the third component of our inference model, Pr(<italic toggle="yes">F |S, T, L, P</italic>), helps alleviate the second issue by allowing finetuning of <italic toggle="yes">f</italic><sub>5</sub>, thereby adjusting Pr(<italic toggle="yes">S, T, L, P |I</italic>) to the given training set (e.g., the bootstrapped FIV image set).</p></sec></sec><sec id="s3c" hwp:id="sec-15"><title hwp:id="title-16">VGG network</title><p hwp:id="p-71">The VGG network is based on the raw pretrained VGG face network (referred to as VGG-Raw) that is publicly available, <ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" ext-link-type="uri" xlink:href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" hwp:id="ext-link-7">http://www.robots.ox.ac.uk/∼vgg/software/vgg_face/</ext-link>. This network consists of 13 convolutional layers (8 more layers than Alexnet) and 3 fully-connected layers (same as Alexnet). The dataset used for training this network consisted of more than 2.5 million face images where each image is labeled with one of 2622 person identities. The details of the network architecture, its training dataset and training procedure can be found in<sup><xref rid="sc14" ref-type="bibr" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">14</xref></sup>.</p><p hwp:id="p-72">Similar to the EIG network, the VGG network is obtained by finetuning this pretrained VGG-Raw network on the relevant image sets. For our FIV experiments, we used the same bootstrapped training dataset of FIV images as described above. We replaced VGG-Raw’s top 2622-way fully connected classification layer (i.e., its third fully-connected layer; TFCL) with a 25-way classification layer for the FIV identities. Training of VGG started from their pretrained values in VGG-Raw except this final layer which was initialized with random weights. We trained that new classification layer (TFCL) and finetuned the weights in TCL, FFCL, and SFCL using stochastic gradient descent to minimize a cross-entropy loss.</p><p hwp:id="p-73">For our FIV-S experiments, we replaced the final classification layer in the pretrained VGG-Raw network with a 500-way classification layer. To train this network, we obtained a new dataset with the person identities and training images coming from the generative model. We first randomly sampled 500 identities as pairs of shapes and textures from <italic toggle="yes">Pr</italic>(<italic toggle="yes">S, T |F</italic>). We then rendered each identity using 400 viewing conditions randomly drawn from <italic toggle="yes">Pr</italic>(<italic toggle="yes">L, P</italic>), identical to EIG’s training dataset. This procedure gave us a total of 200, 000 images and their corresponding identity labels (from 1 to 500). In line with the training of the VGG-Raw network, the VGG network as well as the EIG network utilized two standard data augmentation methods including making an image grayscale with a low probability (0.1) and mirror reflecting an image with probability 0.5. As for our FIV experiments, we initialized the weights of the VGG network using the weights of the pretrained VGG-Raw network except for its classification layer, which was initialized using random weights. We then finetuned the weights associated with its TCL, FFCL, and SFCL and trained its classification layer using stochastic gradient descent to minimize a cross-entropy loss. We used a learning rate of 0.0001 with minibatches of size 20 images.</p><sec id="s3c1" hwp:id="sec-16"><title hwp:id="title-17">Evaluation of VGG-Raw, VGG<sup>+</sup>, and EIG<sup>−</sup> networks on the FIV image set</title><p hwp:id="p-74">The pretrained VGG-Raw network readily generalizes well to the FIV image set and gives rise to qualitative and quantitative patterns that are similar to the VGG network (compare <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Extended Data Fig. 2b-e</xref> to main text <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-24" hwp:rel-id="F2">Fig. 2j-m</xref>).</p><p hwp:id="p-75">In order to isolate contribution of EIG’s initial segmentation and normalization step (<italic toggle="yes">f</italic><sub>1</sub>) to its better fit to neural data, we also tested VGG<sup>+</sup> and EIG<sup>−</sup> networks. The VGG<sup>+</sup> network extends VGG with the initial segmentation and normalization step from the EIG network (<italic toggle="yes">f</italic><sub>1</sub>; <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Extended Data Fig. 2f</xref>). Overall, the transformations across the layers of the VGG<sup>+</sup> network (<xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-5" hwp:rel-id="F7">Extended Data Fig. 2g</xref>) were very similar to that of the VGG or VGG-Raw networks (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-25" hwp:rel-id="F2">Fig. 2j</xref> main text, <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-6" hwp:rel-id="F7">Extended Data Fig. 2b</xref>). Even though VGG<sup>+</sup> was still more view-invariant when compared to neural data (<xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-7" hwp:rel-id="F7">Extended Data Fig. 2h,i</xref>; <italic toggle="yes">p &lt;</italic> 0.05 comparison of view-invariance coefficients for each layer), it was somewhat less view-invariant when compared to the VGG network (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-26" hwp:rel-id="F2">Fig. 2k,l</xref> main text) across all three of its layer indicating that the VGG network can pick up on spurious cues to discriminate identities (e.g., shirt color or hair) to some extent. Correlations between VGG<sup>+</sup> and data similarity matrices were similar to the correlations based on the VGG network (compare <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-8" hwp:rel-id="F7">Extended Data Fig. 2j</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-27" hwp:rel-id="F2">Fig. 2m</xref> main text) with the exception that the initial segmentation and normalization stage helped with VGG<sup>+</sup>’s fit to ML/MF: its TCL was more like ML/MF and less like AM in comparison to the VGG network’s TCL. These results suggest that even though the initial segmentation and normalization stage can help align the view-based TCL stage in the VGG network with the view-based ML/MF representations in the neural data, it is not sufficient to influence the later, more view-invariant stages of the network closer to the final classification step.</p><p hwp:id="p-76">The EIG<sup>−</sup> network is a lesion of the EIG network without the initial segmentation and normalization step. Even though this network replicates some of the qualitative structure seen in the EIG network, it does not capture the data nearly as well (compare <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-9" hwp:rel-id="F7">Extended Data Figs. 2c</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-28" hwp:rel-id="F2">2d</xref>), providing quantitative support for an initial attention-like step in the EIG network.</p><p hwp:id="p-77">In <xref rid="figE3" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Extended Data Fig. 3</xref>, we also present scatter plots showing the fine-grained relation between all pairwise similarities in each network layer and each neural site, for the EIG network (panel A) and the VGG network (panel B) in addition to showing how their earlier layers relate to neural data (panels C, D). See figure caption for discussion of these results.</p></sec></sec><sec id="s3d" hwp:id="sec-17"><title hwp:id="title-18">Neural data analysis</title><p hwp:id="p-78">The neural experiments and the data presented in the main text were originally reported in<sup><xref rid="sc15" ref-type="bibr" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">15</xref></sup>. Various relevant details can be found in SI Section 2.</p><sec id="s3d1" hwp:id="sec-18"><title hwp:id="title-19">Representational similarity matrices: Neurons</title><p hwp:id="p-79">To compute the neural similarity matrices for a given neural site, each image was represented as a vector of the average spiking rates of all neurons recorded at that site. Following<sup><xref rid="sc16" ref-type="bibr" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">16</xref></sup>, we obtained the average number of spikes for each neuron across the repetitions of a given image using the time-binned spike counts centered at 200 msec after stimulus onset with a time window of 50 msec in each direction. Following<sup><xref rid="sc15" ref-type="bibr" hwp:id="xref-ref-84-2" hwp:rel-id="ref-84">15</xref></sup>, for each site, we min-max (range [0, 1]) normalized the average spiking rate of each neuron. For a given neural site, similarity of a pair of images was computed as the Pearson’s correlation coefficient of the corresponding pair of the average spiking vectors. All spiking data was processed using the Neural Decoding Toolbox<sup><xref rid="sc17" ref-type="bibr" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">17</xref></sup>.</p></sec><sec id="s3d2" hwp:id="sec-19"><title hwp:id="title-20">Representational similarity matrices: Models</title><p hwp:id="p-80">For a given image set, model, and the model’s layer, images were represented as a vector of activations of all units in that layer. The model similarity of a pair of images (e.g., each entry in the similarity matrix shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-29" hwp:rel-id="F2">Fig. 2f</xref>, main text) is the Pearson’s correlation coefficient of their corresponding activations vectors.</p></sec><sec id="s3d3" hwp:id="sec-20"><title hwp:id="title-21">Linear regression analysis using the idealized similarity templates</title><p hwp:id="p-81">For a given representational similarity matrix <italic toggle="yes">M</italic>, we solved the following linear equation.
<disp-formula id="eqnS1" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="282798v2_eqnS1.gif" position="float" orientation="portrait" hwp:id="graphic-8"/></alternatives>
</disp-formula>
where {<italic toggle="yes">c</italic><sub>1</sub>, <italic toggle="yes">c</italic><sub>2</sub>, <italic toggle="yes">c</italic><sub>3</sub>, <italic toggle="yes">c</italic><sub>4</sub>} are coefficients, <italic toggle="yes">I</italic><sub>1</sub> is the idealized view-specificity matrix, <italic toggle="yes">I</italic><sub>2</sub> is the idealized mirror-symmetry matrix, <italic toggle="yes">I</italic><sub>3</sub> is the idealized view-invariant identity coding matrix, and <italic toggle="yes">B</italic> is the background matrix. These matrices are shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-30" hwp:rel-id="F2">Fig. 2e</xref> in the main text. All black entries have a value of 1, all gray entries have a value of 0.5 and all white entries have a value of 0. We solve this equation using a non-negative least squares solver as implemented in the Python package <monospace>scipy</monospace>’s <monospace>nnls</monospace> method.</p></sec><sec id="s3d4" hwp:id="sec-21"><title hwp:id="title-22">Bootstrap procedure</title><p hwp:id="p-82">Due to the small number of subjects (N=3), we performed bootstrap analysis at the image-level. Following the procedure in<sup><xref rid="sc18" ref-type="bibr" hwp:id="xref-ref-87-1" hwp:rel-id="ref-87">18</xref></sup>, a bootstrap sample was obtained by sampling the 175 images in the FIV image set with replacement. Based on this sample, we computed the neural and the model similarity matrices. To avoid spurious positive correlations, we excluded all non-diagonal identity-pairs that could arise due to sampling-with-replacement. Based on the discussion in<sup><xref rid="sc19" ref-type="bibr" hwp:id="xref-ref-88-1" hwp:rel-id="ref-88">19</xref></sup> and following<sup><xref rid="sc20" ref-type="bibr" hwp:id="xref-ref-89-1" hwp:rel-id="ref-89">20</xref></sup>, we computed the Pearson correlation coefficient between pairs of representational similarity matrices. We repeated this procedure for 10, 000 bootstrap samples. Significance was measured using a direct bootstrap hypothesis testing procedure with a significance level of 0.05.</p><p hwp:id="p-83">For the linear regression analysis with idealized similarity matrices, we again bootstrap sampled the 175 images with replacement and performed the linear regression using the resulting similarity matrix. We repeated this procedure for 10000 times.</p></sec></sec><sec id="s3e" hwp:id="sec-22"><title hwp:id="title-23">Psychophysics methods</title><sec id="s3e1" hwp:id="sec-23"><title hwp:id="title-24">Experiment 1</title><p hwp:id="p-84">The experimental procedure consisted of a simple “same”/”different” judgment task as the following. A study item was presented for 150 msecs, which was followed by a masking stimuli in the form of a scrambled image of a face for 500 msecs. Finally a test item appeared and stayed on until a response was entered (the participants were instructed to press “f” for “same” and press “j” for “different”). They performed 10 practice trials before performing 96 experimental trials. Participants did not receive any feedback at all during the practice trial, which aimed to have participants get used to the experiment parameters (e.g., its interface). During the experimental trials, participants were shown their current average performance at every fifth trial.</p><p hwp:id="p-85">The stimuli were 200 <italic toggle="yes">×</italic> 200 color images of faces photo-realistically rendered using the generative model. None of the stimuli across the experiments were used during training of the models. The viewing conditions for both the study and test items were drawn randomly from their respective prior distribution, <italic toggle="yes">Pr</italic>(<italic toggle="yes">L, P</italic>). All participants saw the same image set (i.e., the viewing conditions were sampled once for all participants before the experiment began). There were 48 “same” trials and 48 “different” trials.</p><p hwp:id="p-86">No study identity (i.e., a pair of shape and texture properties) was presented twice across trials. For the “different” trials, we chose the distractor face (the test item) by running a Metropolis-Hasting based search until 50 accepted steps. The search started from a random face but with matching lighting and pose parameters as that of the study item and increasingly moved closer to the study face w.r.t. likelihood <italic toggle="yes">P</italic> (<italic toggle="yes">I|S, T, L, P</italic>) by generating proposals from the prior distribution over shape and texture properties, <italic toggle="yes">Pr</italic>(<italic toggle="yes">S, T</italic>). This procedure aimed to ensure that the test facial identities in “different” trials were not arbitrarily different from the study item in obvious ways. Our data suggested that this procedure was effective: across the “different” trials average Pr(“Same”) was 0.35 with a standard deviation of 0.15, min value of 0.10 and max value of 0.71. All stimuli were rendered using Matlab’s OpenGL-based rendering pipeline.</p></sec><sec id="s3e2" hwp:id="sec-24"><title hwp:id="title-25">Experiment 2</title><p hwp:id="p-87">The stimuli and procedure were identical to Experiment 1 with the following exceptions. The test item was always presented frontal (i.e., frontal lighting and frontal pose) and without texture. This was achieved by assuming a uniform gray color for all vertices of the face mesh before rendering.</p></sec><sec id="s3e3" hwp:id="sec-25"><title hwp:id="title-26">Experiment 3</title><p hwp:id="p-88">The stimuli and procedure were identical to Experiment 1 with the following exception. The test item was always presented frontal (i.e., frontal lighting and frontal pose), however, the texture was rendered on a flat surface in order to eliminate shape information from shading. In an attempt to further eliminate the shape information, we post-processed the resulting images by applying a fish-eye lens effect. All of the code used to generate stimuli as well as the actual images used in the experiments will be released at the time of publication.</p></sec><sec id="s3e4" hwp:id="sec-26"><title hwp:id="title-27">Calculating Similarity(study,test)</title><p hwp:id="p-89">For a given pair of study and test images, their predicted similarity by a model was computed as the similarity of their respective representations under the model. For the EIG network, this representation was its <italic toggle="yes">f</italic><sub>5</sub> consisting of the shape and texture properties (a 400 dimensional vector), excluding the lighting and pose parameters. The model’s similarity prediction was the Pearson’s correlation coefficient of these two vectors.</p><p hwp:id="p-90">For the other networks, the images were represented by their resulting SFCL activations. The model’s prediction is the correlation coefficient of these two vectors. We found that no other layer in the VGG network resulted in a better account of the human behavior than the layer we used. We also considered using other similarity metrics in addition to Pearson’s correlation coefficient such as the cosine of the angle between two vectors and Euclidean distance. We found no significant difference in fits for any of the models.</p></sec><sec id="s3e5" hwp:id="sec-27"><title hwp:id="title-28">Lighting elevation judgment task</title><p hwp:id="p-91">Both groups of participants had to complete 5 training trials before they moved onto 45 test trials. We only used the test trials in our analysis. Each of the 45 trials featured a different facial identity. In the depth-suppression group, each of the 9 levels of depth suppression (from 1, regular faces, to 0, flat face, to −1, fully inverted faces with nose pointing away from the observer; see also the main text) appeared 5 times throughout the experiment. In the lighting source elevation experiment, each of the 9 levels of elevation appeared 5 times (from the top of the face, 1.31 radians of elevation, to the front of the face, 0 radians of elevation, to the bottom of the face, −1.31 radians of elevation; see also the main text).</p><p hwp:id="p-92">For each condition, we z-scored each participant’s responses (a total of 45 ratings each in the range of 1 to 7) before averaging all responses across participants and across the 9 levels. The error bars were obtained for each of the 9 levels as the standard deviation of the average values of the 5 stimuli items corresponding to that level.</p><p hwp:id="p-93">Obtaining the EIG network’s predictions was straightforward. For each condition, we ran the EIG model on the same set of 45 images as the human subjects, recording its outputs for the lighting elevation, <italic toggle="yes">L</italic><sub><italic toggle="yes">e</italic></sub>. We averaged the values for the 5 images of each of the 9 levels. The error bars in the main text (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Fig. 5c</xref>) show the standard deviation across these five images. The main text also reports trial-level correspondence between the model and the behavior as the correlation of model’s predicted lighting elevations and the average human response per each of the 90 test trials (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Fig. 5d</xref>).</p></sec><sec id="s3e6" hwp:id="sec-28"><title hwp:id="title-29">Face depth judgment task</title><p hwp:id="p-94">Before the beginning of the experimental trials, participants were instructed that they would see frontal images of faces and some faces would be more flatter than others. They were shown several examples of fairly flat and fairly deep faces, which were samples chosen from either tail of the flatness distribution of 3000 randomly generated heads. On a given trial, participants were presented frontal image of a face (excluding neck and the ear) and were asked to judge the profile depth of it using a scale of 1 to 7. Next to the flat-end (wide-end) of the continuum participants were presented with the profile view of an altered mean-MFM-face with its depth scaled to −3 (+3) standard deviations away from the mean depth of the above mentioned 3000 faces. An example trial in this experiment is shown in <xref rid="figE12" ref-type="fig" hwp:id="xref-fig-17-1" hwp:rel-id="F17">Extended Data Fig. 12</xref>.</p><p hwp:id="p-95">Participants had to complete 10 training trials before they moved onto 108 test trials. We only used the test trials in our analysis. The 108 trials featured 54 different facial identities with each identity rendered once as a regular face and once with depth suppression. These identities were uniformly assigned to the 9 depth suppression levels (6 identities per level). When rendering an identity as a regular face, we set the lighting elevation location to match where it would be perceived given its depth-suppression level according to the results in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Fig. 5c</xref> in main text. The actual values used are indicated in the x-axis of <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-7" hwp:rel-id="F5">Fig. 5e</xref> right panel. When rendering an identity with depth suppression we always place the lighting elevation at top, at 1.31 radians. Following previous work<sup><xref rid="sc21" ref-type="bibr" hwp:id="xref-ref-90-1" hwp:rel-id="ref-90">21</xref></sup>, we rendered only the face proper region excluding ears and neck. This procedure resulted in 6 images per each of the 9 depth-suppression levels and 6 images per each of the 9 control levels.</p><p hwp:id="p-96">For each condition, we z-scored each participant’s responses (a total of 108 ratings each in the range of 1 to 7) before averaging all responses across participants. The error bars were obtained for each of the 9 levels as the standard deviation of the average values of the 5 stimuli items corresponding to that level.</p><p hwp:id="p-97">The EIG network can be readily used to estimate depth of a given face image. We ran the EIG model on the same set of 108 images as the human subjects, recording its outputs for the shape parameters, <italic toggle="yes">S</italic>. We then assigned a depth for each input image as the average displacement of the three key points (nose, left cheek, and right cheek) of the face shape with respect to the underlying aligned coordinate system of MFM. This coordinate system is in arbitrary units so we z-scored model’s predictions to bring it to the same scale as the behavioral data. The error bars in the main text (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-8" hwp:rel-id="F5">Fig. 5e</xref>, main text) show the standard deviation across six images falling under the same pair of depth-suppressed or control and one of the 9 levels. The main text also reports trial-level correspondence between the model and the behavior as the correlation of model’s predicted depth and the average human response per each of the 108 test trials (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-9" hwp:rel-id="F5">Fig. 5f</xref>, main text).</p></sec></sec></sec><ref-list hwp:id="ref-list-2"><title hwp:id="title-30">References</title><ref id="sc1" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><label>1.</label><citation publication-type="other" citation-type="journal" ref:id="282798v2.70" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-70"><collab hwp:id="collab-2">IEEE</collab>. <source hwp:id="source-70">A 3D Face Model for Pose and Illumination Invariant Face Recognition</source>.</citation></ref><ref id="sc2" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Tipping M. E."><surname>Tipping</surname>, <given-names>M. E.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bishop C. M."><surname>Bishop</surname>, <given-names>C. M.</given-names></string-name> <article-title hwp:id="article-title-64">Probabilistic principal component analysis</article-title>. <source hwp:id="source-71">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source> <volume>61</volume>, <fpage>611</fpage>–<lpage>622</lpage> (<year>1999</year>).</citation></ref><ref id="sc3" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Yuille A."><surname>Yuille</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kersten D."><surname>Kersten</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-65">Vision as Bayesian inference: analysis by synthesis?</article-title> <source hwp:id="source-72">Trends in Cognitive Sciences</source> <volume>10</volume>, <fpage>301</fpage>–<lpage>308</lpage> (<year>2006</year>).</citation></ref><ref id="sc4" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1 xref-ref-73-2"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.73" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Murray I."><surname>Murray</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adams R. P."><surname>Adams</surname>, <given-names>R. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="MacKay D. J."><surname>MacKay</surname>, <given-names>D. J.</given-names></string-name> <source hwp:id="source-73">Elliptical slice sampling</source>. arXiv preprint arXiv:1001.0175 (<year>2009</year>).</citation></ref><ref id="sc5" hwp:id="ref-74"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.74" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Kulkarni T. D."><surname>Kulkarni</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kohli P."><surname>Kohli</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-66">Deep Generative Vision as Approximate Bayesian Computation</article-title>. In <source hwp:id="source-74">Neural Information Processing Systems Workshop on Approximate Bayesian Computation</source> (<year>2014</year>).</citation></ref><ref id="sc6" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.75" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kulkarni T. D."><surname>Kulkarni</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-67">Efficient and robust analysis-by-synthesis in vision: A computational framework, behavioral tests, and modeling neuronal representations</article-title>. In <source hwp:id="source-75">Annual Conference of the Cognitive Science Society</source> (<year>2015</year>).</citation></ref><ref id="sc7" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Fukushima K."><surname>Fukushima</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-68">Neocognitron: A hierarchical neural network capable of visual pattern recognition</article-title>. <source hwp:id="source-76">Neural Networks</source> <volume>1</volume>, <fpage>119</fpage>–<lpage>130</lpage> (<year>1988</year>).</citation></ref><ref id="sc8" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.77" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="LeCun Y."><surname>LeCun</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-69">Convolutional networks for images, speech, and time series</article-title>. <source hwp:id="source-77">The Handbook of Brain Theory and Neural Betworks</source> <volume>3361</volume> (<year>1995</year>).</citation></ref><ref id="sc9" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.78" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Jackson A. S."><surname>Jackson</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bulat A."><surname>Bulat</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Argyriou V."><surname>Argyriou</surname>, <given-names>V.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tzimiropoulos G."><surname>Tzimiropoulos</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-70">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</article-title>. <source hwp:id="source-78">Proceedings of the International Conference on Computer Vision</source> (<year>2017</year>).</citation></ref><ref id="sc10" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.79" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Krizhevsky A."><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sutskever I."><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Hinton G. E."><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> <article-title hwp:id="article-title-71">Imagenet classification with deep convolutional neural networks</article-title>. In <source hwp:id="source-79">Advances in Neural Information Processing Systems</source>, <fpage>1097</fpage>–<lpage>1105</lpage> (<year>2012</year>).</citation></ref><ref id="sc11" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.80" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="Zhou B."><surname>Zhou</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khosla A."><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lapedriza A."><surname>Lapedriza</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Torralba A."><surname>Torralba</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Oliva A."><surname>Oliva</surname>, <given-names>A.</given-names></string-name> <source hwp:id="source-80">Places: An image database for deep scene understanding</source>. arXiv preprint arXiv:1610.02055 (<year>2016</year>).</citation></ref><ref id="sc12" hwp:id="ref-81" hwp:rev-id="xref-ref-81-1"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.81" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Allen K. R."><surname>Allen</surname>, <given-names>K. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yildirim I."><surname>Yildirim</surname>, <given-names>I.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tenenbaum J. B."><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> <article-title hwp:id="article-title-72">Integrating identification and perception: A case study of familiar and unfamiliar face processing</article-title>. In <source hwp:id="source-81">Annual Conference of the Cognitive Science Society</source> (<year>2016</year>).</citation></ref><ref id="sc13" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1"><label>13.</label><citation publication-type="book" citation-type="book" ref:id="282798v2.82" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Kevin M."><surname>Kevin</surname>, <given-names>M.</given-names></string-name> <source hwp:id="source-82">Machine Learning: a Probabilistic Perspective</source> (<publisher-name>The MIT press</publisher-name>, <year>2012</year>).</citation></ref><ref id="sc14" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.83" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Parkhi O. M."><surname>Parkhi</surname>, <given-names>O. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vedaldi A."><surname>Vedaldi</surname>, <given-names>A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Zisserman A."><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-73">Deep Face Recognition</article-title>. In <source hwp:id="source-83">British Machine Vision Conference (BMVC)</source> (<year>2015</year>).</citation></ref><ref id="sc15" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1 xref-ref-84-2"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> <article-title hwp:id="article-title-74">Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title>. <source hwp:id="source-84">Science</source> <volume>330</volume>, <fpage>845</fpage>–<lpage>851</lpage> (<year>2010</year>).</citation></ref><ref id="sc16" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.85" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Meyers E. M."><surname>Meyers</surname>, <given-names>E. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Borzello M."><surname>Borzello</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tsao D."><surname>Tsao</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-75">Intelligent Information Loss: The Coding of Facial Identity, Head Pose, and Non-Face Information in the Macaque Face Patch System</article-title>. <source hwp:id="source-85">The Journal of Neuroscience</source> <volume>35</volume>, <fpage>7069</fpage>–<lpage>7081</lpage> (<year>2015</year>).</citation></ref><ref id="sc17" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.86" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Meyers E."><surname>Meyers</surname>, <given-names>E.</given-names></string-name> <article-title hwp:id="article-title-76">The neural decoding toolbox</article-title>. <source hwp:id="source-86">Frontiers in neuroinformatics</source> <volume>7</volume>, <fpage>8</fpage> (<year>2013</year>).</citation></ref><ref id="sc18" hwp:id="ref-87" hwp:rev-id="xref-ref-87-1"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.87" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-87"><string-name name-style="western" hwp:sortable="Nili H."><surname>Nili</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-77">A toolbox for representational similarity analysis</article-title>. <source hwp:id="source-87">PLoS Computational Biology</source> <volume>10</volume>, <fpage>e1003553</fpage> (<year>2014</year>).</citation></ref><ref id="sc19" hwp:id="ref-88" hwp:rev-id="xref-ref-88-1"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.88" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-88"><string-name name-style="western" hwp:sortable="Diedrichsen J."><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-78">Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</article-title>. <source hwp:id="source-88">PLoS Computational Biology</source> <volume>13</volume>, <fpage>e1005508</fpage> (<year>2017</year>).</citation></ref><ref id="sc20" hwp:id="ref-89" hwp:rev-id="xref-ref-89-1"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.89" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-89"><string-name name-style="western" hwp:sortable="Ejaz N."><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hamada M."><surname>Hamada</surname>, <given-names>M.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Diedrichsen J."><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-79">Hand use predicts the structure of representations in sensorimotor cortex</article-title>. <source hwp:id="source-89">Nature Neuroscience</source> <volume>18</volume>, <fpage>1034</fpage>–<lpage>1040</lpage> (<year>2015</year>).</citation></ref><ref id="sc21" hwp:id="ref-90" hwp:rev-id="xref-ref-90-1"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.90" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-90"><string-name name-style="western" hwp:sortable="Hartung B."><surname>Hartung</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schrater P. R."><surname>Schrater</surname>, <given-names>P. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blthoff H. H."><surname>Blthoff</surname>, <given-names>H. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kersten D."><surname>Kersten</surname>, <given-names>D.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Franz V. H."><surname>Franz</surname>, <given-names>V. H.</given-names></string-name> <article-title hwp:id="article-title-80">Is prior knowledge of object geometry used in visually guided reaching?</article-title> <source hwp:id="source-90">Journal of Vision</source> <volume>5</volume>, <fpage>2</fpage> (<year>2005</year>). URL +<ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1167/5.6.2./data/journals/jov/933512/jov-5-6-2.pdf" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/5.6.2./data/journals/jov/933512/jov-5-6-2.pdf" hwp:id="ext-link-8">http://dx.doi.org/10.1167/5.6.2./data/journals/jov/933512/jov-5-6-2.pdf</ext-link>.</citation></ref></ref-list><ack hwp:id="ack-1"><title hwp:id="title-31">Acknowledgements</title><p hwp:id="p-98">This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216; the National Eye Institute of NIH (R01 EY021594 to W.A.F.); The New York Stem Cell Foundation (to W.A.F.); ONR MURI N00014-13-1-0333 (to J.B.T.); a grant from Toyota Research Institute (to J.B.T.); and a grant from Mitsubishi MELCO (to J.B.T.). W.A.F. is a New York Stem Cell FoundationRobertson Investigator. A high performance clustering environment for computations (Openmind) was provided by the McGovern Institute for Brain Research. The content is solely the responsibility of the authors and does not necessarily represent the official views of NIH.</p></ack><sec id="s4" hwp:id="sec-29"><title hwp:id="title-32">Competing Interests</title><p hwp:id="p-99">The authors declare that they have no competing financial interests.</p></sec><sec id="s5" sec-type="supplementary-material" hwp:id="sec-30"><title hwp:id="title-33">Supplementary Information</title><p hwp:id="p-100">Supplementary Information can be found at the end of this document.</p><table-wrap id="tblE1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/TBLE1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tblE1</object-id><label>Extended Data Table 1:</label><caption hwp:id="caption-6"><p hwp:id="p-101">Pose distributions for the FIV-S image set (in radians).</p></caption><graphic xlink:href="282798v2_tblE1" position="float" orientation="portrait" hwp:id="graphic-9"/></table-wrap><table-wrap id="tblE2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/TBLE2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tblE2</object-id><label>Extended Data Table 2:</label><caption hwp:id="caption-7"><p hwp:id="p-102">Inference model architecture</p></caption><graphic xlink:href="282798v2_tblE2" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap><table-wrap id="tblE3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1 xref-table-wrap-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/TBLE3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tblE3</object-id><label>Extended Data Table 3:</label><caption hwp:id="caption-8"><p hwp:id="p-103">ID network architecture</p></caption><graphic xlink:href="282798v2_tblE3" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap><table-wrap id="tblE4" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/TBLE4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tblE4</object-id><label>Extended Data Table 4:</label><caption hwp:id="caption-9"><p hwp:id="p-104">VAE decoder architecture</p></caption><graphic xlink:href="282798v2_tblE4" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap><table-wrap id="tblE5" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/TBLE5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tblE5</object-id><label>Extended Data Table 5:</label><caption hwp:id="caption-10"><p hwp:id="p-105">VAE-QN Pose architecture</p></caption><graphic xlink:href="282798v2_tblE5" position="float" orientation="portrait" hwp:id="graphic-13"/></table-wrap><fig id="figE1" position="float" orientation="portrait" fig-type="figure" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">figE1</object-id><label>Extended Data Fig. 1:</label><caption hwp:id="caption-11"><p hwp:id="p-106">A more detailed diagram of the modeling framework. <bold>a</bold>, Schematic of the probabilistic generative that consists of a distribution over familiar person identities, detailed view of a graphics program with its key stages exposed, and an image-level transformations module. <bold>b</bold>, Schematic of the efficient inverse graphics model for efficient inference in the generative model based on DCNNs. <italic toggle="yes">f</italic><sub>1</sub> is for face segmentation and normalization, <italic toggle="yes">f</italic><sub>2</sub> to <italic toggle="yes">f</italic><sub>5</sub> for 3D scene parameter inference, and finally <italic toggle="yes">f</italic><sub>6</sub> for person identity recognition. <bold>c</bold>, Schematic of the ventral stream hierarchy with the three face patches indicated. Colored boxes in (<bold>a</bold>) to (<bold>c</bold>) show the hypothesized explanations of the neural sites based on the generative and inference models. Rectangles indicate representations, trapezoids indicate transformations or algorithms using representations.</p></caption><graphic xlink:href="282798v2_figE1" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><fig id="figE2" position="float" orientation="portrait" fig-type="figure" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4 xref-fig-7-5 xref-fig-7-6 xref-fig-7-7 xref-fig-7-8 xref-fig-7-9 xref-fig-7-10 xref-fig-7-11 xref-fig-7-12 xref-fig-7-13"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">figE2</object-id><label>Extended Data Fig. 2:</label><caption hwp:id="caption-12"><p hwp:id="p-107">Evaluation of VGG-Raw, VGG<sup>+</sup>, and EIG<sup>−</sup> networks based on the FIV image set (Extending <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-31" hwp:rel-id="F2">Fig. 2</xref> from main text). <bold>a</bold>, A schematic of the VGG-Raw network architecture. <bold>g</bold>-<bold>j</bold>, Analysis of the VGG-Raw network. <bold>f</bold>, A schematic of the VGG<sup>+</sup> network architecture. Unlike (a), this network has an initial segmentation and normalization step. <bold>g</bold>-<bold>j</bold>, Analysis of the VGG<sup>+</sup> network. <bold>k</bold>, The EIG<sup>−</sup> network architecture. <bold>l</bold>-<bold>o</bold>, Analysis of the EIG<sup>−</sup> network. Each row of the figure follow the same convention consisting of five parts (from left to right): (i) network architecture, (ii) similarity matrices of the key model layers, (iii) linear coefficients based on idealized similarity template analysis, (iv) comparison of data and model coefficients, and (v) comparison of data and model similarity matrices. Error bars show 95% bootstrap confidence intervals (CIs).</p></caption><graphic xlink:href="282798v2_figE2" position="float" orientation="portrait" hwp:id="graphic-15"/></fig><fig id="figE3" position="float" orientation="portrait" fig-type="figure" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figE3</object-id><label>Extended Data Fig. 3:</label><caption hwp:id="caption-13"><p hwp:id="p-108">Scatter plots of data and model similarity matrices and analysis of earlier network layers (Extending <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-32" hwp:rel-id="F2">Fig. 2</xref> from main text). <bold>a</bold>, Scatter plots for every pair of EIG model layer and neural site. <bold>b</bold>, Scatter plots for every pair of VGG network layer and neural site. In (<bold>a</bold>) and (<bold>b</bold>), individual dots are colored according to whether they represent pairs of images showing the same view of different faces (yellow), the same face in different views (dark blue), or mirrorsymmetric views of different faces (light blue). We can see that the quantitatively presented results in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-33" hwp:rel-id="F2">Fig. 2i</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-34" hwp:rel-id="F2">2m</xref> are qualitatively visible in these scatter plots. For example, EIG’s layer <italic toggle="yes">f</italic><sub>3</sub> shows the strongest linear relationship with ML/MF, <italic toggle="yes">f</italic><sub>4</sub> shows the strongest linear relationship with AL, and <italic toggle="yes">f</italic><sub>5</sub> shows the strongest linear relationship with AM. Moreover, color-coding shows how the network layers which best capture the qualitative coding preferences in each face patch also best capture fine-grained structure within those qualitative categories: EIG <italic toggle="yes">f</italic><sub>3</sub> best captures the fine-grained structure of view-dependent coding in ML/MF, EIG <italic toggle="yes">f</italic><sub>4</sub> best captures the finegrained structure among mirror-symmetric views in AL, and EIG <italic toggle="yes">f</italic><sub>5</sub> best captures the fine-grained structure of view-invariant identity encoding in AM. <bold>c</bold>, Pearson’s r between the similarity matrices arising from the earlier layers in the second stage of the EIG network and neural sites. <bold>d</bold>, Pearson’s r between the similarity matrices arising from the earlier layers in the VGG network and neural sites. In (<bold>c</bold>) and (<bold>d</bold>), we see that the basic differences observed between EIG and VGG in their top convolutional layers are also seen at earlier convolutional layers, and none of the earlier layers in either network provide better accounts of ML/MF than the top convolutional layers. Error bars show 95% bootstrap confidence intervals (CIs).</p></caption><graphic xlink:href="282798v2_figE3" position="float" orientation="portrait" hwp:id="graphic-16"/></fig><fig id="figE4" position="float" orientation="portrait" fig-type="figure" hwp:id="F9" hwp:rev-id="xref-fig-9-1 xref-fig-9-2 xref-fig-9-3 xref-fig-9-4 xref-fig-9-5 xref-fig-9-6 xref-fig-9-7 xref-fig-9-8 xref-fig-9-9 xref-fig-9-10 xref-fig-9-11 xref-fig-9-12 xref-fig-9-13 xref-fig-9-14 xref-fig-9-15 xref-fig-9-16"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figE4</object-id><label>Extended Data Fig. 4:</label><caption hwp:id="caption-14"><p hwp:id="p-109">Evaluation of alternative models using the FIV-S image set. <bold>a</bold>, Sample images from the FIV-S image set. <bold>b</bold>-<bold>f</bold>, Results of the full EIG network. <bold>g</bold>-<bold>k</bold>, Results of the EIG<sup>−</sup> network. <bold>l</bold>-<bold>p</bold>, Results of the VGG network. <bold>q</bold>-<bold>u</bold>, Results of the VGG-Raw network. <bold>v</bold>-<bold>z</bold>, Results of the ID network. <bold>aa-ae</bold>, Results of the Regress-ID network. Each row of the figure follows the same convention as in the <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-10" hwp:rel-id="F7">Extended Data Fig. 2</xref>. Error bars show 95% bootstrap confidence intervals (CIs).</p></caption><graphic xlink:href="282798v2_figE4" position="float" orientation="portrait" hwp:id="graphic-17"/></fig><fig id="figE5" position="float" orientation="portrait" fig-type="figure" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">figE5</object-id><label>Extended Data Fig. 5:</label><caption hwp:id="caption-15"><p hwp:id="p-110">Evaluation of the VAE models using the FIV-S image set. <bold>a</bold>-<bold>e</bold>, Results of the betaVAE model. <bold>f</bold>-<bold>j</bold> Results of the VAE-QN network. Each row of the figure follows the same convention as in the <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-11" hwp:rel-id="F7">Extended Data Fig. 2</xref>. Error bars show 95% bootstrap confidence intervals (CIs).</p></caption><graphic xlink:href="282798v2_figE5" position="float" orientation="portrait" hwp:id="graphic-18"/></fig><fig id="figE6" position="float" orientation="portrait" fig-type="figure" hwp:id="F11" hwp:rev-id="xref-fig-11-1 xref-fig-11-2 xref-fig-11-3 xref-fig-11-4 xref-fig-11-5 xref-fig-11-6 xref-fig-11-7 xref-fig-11-8 xref-fig-11-9 xref-fig-11-10 xref-fig-11-11 xref-fig-11-12 xref-fig-11-13"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figE6</object-id><label>Extended Data Fig. 6:</label><caption hwp:id="caption-16"><p hwp:id="p-111">Trade-off arising from training targets and using pretrained weights. <bold>a</bold>, Mirror-symmetry coefficients in <italic toggle="yes">f</italic><sub>4</sub> of the EIG network, FFCL of the three variants of the ID network (ID-1, ID-2, ID-3), and FFCL of the VGG network. <bold>b</bold>, View-invariance coefficients in <italic toggle="yes">f</italic><sub>5</sub> of the EIG network, SFCL of the three variants of the ID network, and SFCL of the VGG network. <bold>c</bold>, Similarity to behavior for each behavioral experiment and model pair (extending <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig. 4b</xref>, main text). <bold>d</bold> Evolution of mirror-symmetry and view-invariance coefficients throughout training in the EIG network’s <italic toggle="yes">f</italic><sub>4</sub> and <italic toggle="yes">f</italic><sub>5</sub> layers (left column) and the ID-3 network’s FFCL and SFCL layers (right column). Error bars show 95% bootstrap confidence intervals (CIs).</p></caption><graphic xlink:href="282798v2_figE6" position="float" orientation="portrait" hwp:id="graphic-19"/></fig><fig id="figE7" position="float" orientation="portrait" fig-type="figure" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figE7</object-id><label>Extended Data Fig. 7:</label><caption hwp:id="caption-17"><p hwp:id="p-112">Variants of the EIG network architecture each trained from scratch without pretraining. <bold>a</bold>-<bold>d</bold>, The regular EIG architecture with a single hidden full-connectivity layer tested using the FIV-S image set. Notice that mirror-symmetry in the intermediate stage is significantly reduced. <bold>e</bold>-<bold>h</bold>, The EIG architecture extended with an extra hidden full-connectivity layer (<italic toggle="yes">f</italic><sub>4<italic toggle="yes">e</italic></sub>) and tested using the FIV-S image set. With this additional stage (which equates the total number of hidden layers in this network to that found in the ID network), this network gives rise to mirror-symmetry even though it is trained from scratch. Its second hidden full-connectivity layer (<italic toggle="yes">f</italic><sub>4<italic toggle="yes">e</italic></sub>; <bold>f</bold>, <bold>g</bold>) shows strong mirror-symmetry. Each row of the figure follows the same convention as in the <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-12" hwp:rel-id="F7">Extended Data Fig. 2</xref>. Error bars show 95% bootstrap confidence intervals (CIs).</p></caption><graphic xlink:href="282798v2_figE7" position="float" orientation="portrait" hwp:id="graphic-20"/></fig><fig id="figE8" position="float" orientation="portrait" fig-type="figure" hwp:id="F13" hwp:rev-id="xref-fig-13-1 xref-fig-13-2 xref-fig-13-3 xref-fig-13-4 xref-fig-13-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figE8</object-id><label>Extended Data Fig. 8:</label><caption hwp:id="caption-18"><p hwp:id="p-113">Comparison of intermediate stages of the generative model to <italic toggle="yes">f</italic><sub>3</sub>. <bold>a</bold>, FIV image set based comparisons of <italic toggle="yes">f</italic><sub>3</sub> similarity patterns to that of raw images, attended images, and the 2.5D components. <bold>b</bold>, FIV-S-2 image set based comparisons of <italic toggle="yes">f</italic><sub>3</sub> similarity patterns to that of raw images, attended images, and the 2.5D components.</p></caption><graphic xlink:href="282798v2_figE8" position="float" orientation="portrait" hwp:id="graphic-21"/></fig><fig id="figE9" position="float" orientation="portrait" fig-type="figure" hwp:id="F14" hwp:rev-id="xref-fig-14-1 xref-fig-14-2 xref-fig-14-3 xref-fig-14-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figE9</object-id><label>Extended Data Fig. 9:</label><caption hwp:id="caption-19"><p hwp:id="p-114">Decoding analysis. <bold>a</bold>, Average accuracy of a 25-way linear classifier decoding FIV identities from the VGG-Raw network and the EIG network. Dashed line shows chance performance (4%). <bold>b</bold>, Average goodness-of-fit <italic toggle="yes">R</italic><sup>2</sup> values resulting from linearly decoding approximate shape and texture properties of the FIV images from the VGG-Raw network and the EIG network. Error bars indicate standard deviation. All results are based on held-out test sets (see text for further details).</p></caption><graphic xlink:href="282798v2_figE9" position="float" orientation="portrait" hwp:id="graphic-22"/></fig><fig id="figE10" position="float" orientation="portrait" fig-type="figure" hwp:id="F15" hwp:rev-id="xref-fig-15-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figE10</object-id><label>Extended Data Fig. 10:</label><caption hwp:id="caption-20"><p hwp:id="p-115">Learning curve analysis. The moving-window average performance of the participants in each experiment. We don’t observe any pronounced effects of learning, especially in Exps. 2 and 3. Error bars indicate one standard deviation.</p></caption><graphic xlink:href="282798v2_figE10" position="float" orientation="portrait" hwp:id="graphic-23"/></fig><fig id="figE11" position="float" orientation="portrait" fig-type="figure" hwp:id="F16" hwp:rev-id="xref-fig-16-1 xref-fig-16-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE11</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F16</object-id><object-id pub-id-type="publisher-id">figE11</object-id><label>Extended Data Fig. 11:</label><caption hwp:id="caption-21"><p hwp:id="p-116">Lighting direction judgment experiment. <bold>a</bold>, The lighting source could be located at one of the 9 locations frontal to the center of the face, as illustrated they covered the full range from above the face (1.31 rads) to below the face (−1.31 rads). <bold>b</bold>, An example trial.</p></caption><graphic xlink:href="282798v2_figE11" position="float" orientation="portrait" hwp:id="graphic-24"/></fig><fig id="figE12" position="float" orientation="portrait" fig-type="figure" hwp:id="F17" hwp:rev-id="xref-fig-17-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE12</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F17</object-id><object-id pub-id-type="publisher-id">figE12</object-id><label>Extended Data Fig. 12:</label><caption hwp:id="caption-22"><p hwp:id="p-117">Snapshot of a trial from the depth judgment experiment.</p></caption><graphic xlink:href="282798v2_figE12" position="float" orientation="portrait" hwp:id="graphic-25"/></fig><fig id="figE13" position="float" orientation="portrait" fig-type="figure" hwp:id="F18" hwp:rev-id="xref-fig-18-1 xref-fig-18-2 xref-fig-18-3 xref-fig-18-4 xref-fig-18-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;282798v2/FIGE13</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F18</object-id><object-id pub-id-type="publisher-id">figE13</object-id><label>Extended Data Fig. 13:</label><caption hwp:id="caption-23"><p hwp:id="p-118">Decoding lighting elevation and profile depth from the VGG network. <bold>a</bold>, Normalized average light source elevation judgments of the depth-suppression group (left), the control group (right), lighting elevation values decoded from VGG, and the ground truth light source location. <bold>b</bold>, Average human judgments vs. lighting elevation decoded from VGG across all 90 trials without pooling to 9 bins. Pearson’s r values are shown for all trials (grey), control trials (red), and depth suppression trials (blue). <bold>c</bold>, Normalized average profile depth judgments of the depth-suppression group (left), control group (right) and profile depth values decoded from VGG. <bold>d</bold>, Average human judgments vs. profile depth values decoded from VGG across all 108 trials without pooling to 9 bins. Pearson’s r values are shown as in (<bold>b</bold>).</p></caption><graphic xlink:href="282798v2_figE13" position="float" orientation="portrait" hwp:id="graphic-26"/></fig></sec><sec id="s6" hwp:id="sec-31"><title hwp:id="title-34">Supplementary Information</title><sec id="s6a" hwp:id="sec-32"><label>1</label><title hwp:id="title-35">Evaluation of alternative architectures and loss functions using the FIV-S image set</title><p hwp:id="p-119">In this section, we summarize evaluations of various alternative network architecture and loss functions using the FIV-S image set (see <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-5" hwp:rel-id="F9">Extended Data Fig. 4a</xref>). On the FIV-S images, EIG<sup>−</sup> (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-6" hwp:rel-id="F9">Extended Data Figs. 4g-k</xref>) fit just as well as the full EIG model (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-7" hwp:rel-id="F9">Extended Data Figs. 4b-f</xref>), in both qualitative and quantitative terms. This confirms our expectation that the face segmentation stage is needed only to handle background clutter in the image, or hair or clothing that might occlude or distract from face shape and appearance, but that could also provide spurious cues to a familiar person’s identity.</p><p hwp:id="p-120">When tested on the FIV-S, VGG-Raw showed less view-invariance overall (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-8" hwp:rel-id="F9">Extended Data Figs. 4r</xref>, s; <italic toggle="yes">p &lt;</italic> 0.05 in comparison to VGG, <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-9" hwp:rel-id="F9">Extended Data Figs. 4m</xref>, n). This relative lack of generalization across viewpoints suggests a form of dataset bias. On the other hand, VGG gave rise to patterns of results similar to its performance on the more natural FIV image set, with strong view-invariance in the top fully connected layers (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-10" hwp:rel-id="F9">Extended Data Figs. 4l-p</xref>). VGG’s top two layers remained similar to each other (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-11" hwp:rel-id="F9">Extended Data Fig. 4m</xref>), unlike both the neural responses (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-35" hwp:rel-id="F2">Fig. 2C</xref>, main text) and the EIG network (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-12" hwp:rel-id="F9">Extended Data Figs. 4c</xref>, h). Crucially, the training data used to finetune VGG matches EIG’s training data, which suggests that the superior fit of EIG to the neural data on the natural FIV faces, relative to VGG, is more likely a consequence of their respective targets as opposed to differences in training or test set distributions.</p><p hwp:id="p-121">Two other VGG variants that further interpolated towards the EIG<sup>−</sup> model were also tested to rule out possible alternative explanations for its lower fit, due to differences in architecture (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-13" hwp:rel-id="F9">Extended Data Figs. 4v-z</xref>; ID network, see below for its details), training loss functions (<xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-14" hwp:rel-id="F9">Extended Data Figs. 4aa-ae</xref>; Regress-ID,see below for its details), or alternative efficient analysis-by-synthesis methods with a learned image decoder based on a reconstruction loss (<italic toggle="yes">β</italic>-VAE and VAE-QN, see below for their details; <xref rid="figE5" ref-type="fig" hwp:id="xref-fig-10-2" hwp:rel-id="F10">Extended Data Fig. 5</xref>). Taken together, these results rule out the training data, the architectural differences between VGG and Alexnet, and the loss function as potential confounders. They positively support the inverse-graphics hypothesis for how the multistage inference network of primate face perception is organized: classic neural selectivity patterns across all three levels of ML/MF, AL and AM arise uniquely when a inference network is trained with targets that are 3D scene properties – that is, when the network is trained to infer the inputs to a causal generative model of observed face images.</p><sec id="s6a1" hwp:id="sec-33"><title hwp:id="title-36">ID network</title><p hwp:id="p-122">This network allows us to rule out differences between the Alexnet and VGG architectures as a possible confound. Specifically, the architecture of the ID network was based on AlexNet similar to EIG except for its top layer (i.e., its TFCL), which was a 500-dimensional classification layer. We provide the details of this network’s architecture in <xref rid="tblE3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Extended Data Table 3</xref>. This network was trained using the same generative model based dataset as VGG, minimizing a cross-entropy loss, but starting from randomly assigned initial weights.</p></sec><sec id="s6a2" hwp:id="sec-34"><title hwp:id="title-37">Regress-ID network</title><p hwp:id="p-123">Although the ID network matches EIG in training data and architecture, the loss function it optimizes is different from EIG. Moreover, ID network starts with random initial weights, unlike the EIG network. We built Regress-ID to equate the training loss function (MSE loss), the use of pretrained weights, and architecture to that of EIG. Moreover Regress-ID’s training set was the identical set of 200, 000 images we used for training EIG.</p><p hwp:id="p-124">The Regress-ID network’s architecture is identical to the ID network except for it doesn’t have a classification layer (i.e., removing the TFCL layer in <xref rid="tblE3" ref-type="table" hwp:id="xref-table-wrap-3-2" hwp:rel-id="T3">Extended Data Table 3</xref> from the ID network gives the Regress-ID). We paired each image in the training set with an identity embedding vector representation as its target. The identity embeddings were obtained using the VGG network trained on identities from the generative model: For each image in the dataset, we recorded the SFCL activations of the VGG network. We trained the Regress-ID network to map images to these embeddings using stochastic gradient descent and a MSE loss.</p></sec><sec id="s6a3" hwp:id="sec-35"><title hwp:id="title-38">VAE variants: <italic toggle="yes">β</italic>-VAE and VAE-QN</title><p hwp:id="p-125">Finally, we built two VAE variants in order to test whether the exact form of the generative model mattered. EIG inverts a structured causal generative model based on a graphics program, however with VAEs generic function approximators (in this case convolutional neural networks) can be used to assimilate aspects of the generative model through learning. These models consist of an encoder (akin to an inference network) and a decoder (akin to a generative model). We equated these encoders to EIG’s in terms of their architectures and use of pretrained weights, similar to what we did for the Regress-ID network. (We also experimented with a pretrained VGG-Raw as encoder as well as with randomly initialized Alexnet architecture, neither of which led to any significantly different results to report here.) We tested <italic toggle="yes">β</italic>-VAEs<sup><xref rid="cv1" ref-type="bibr" hwp:id="xref-ref-91-1" hwp:rel-id="ref-91">1</xref></sup> and a modified Generative Query Network<sup><xref rid="cv2" ref-type="bibr" hwp:id="xref-ref-92-1" hwp:rel-id="ref-92">2</xref></sup>, which are the specific variants of VAEs engineered to encourage learning disentangled representations from their inputs. In our case, this means disentangling scene extrinsic (e.g., pose) and scene intrinsic (shape and texture) elements of the scene.</p><p hwp:id="p-126">To emphasize again, differently from EIG, these models don’t use a probabilistic graphics program as their generative model but instead learn one as part of training in the form of a convolutional neural network by minimizing a reconstruction loss, a loss function that is different from that of both EIG and VGG. (We provide its details below.) Overall, we found that these two models converge onto very different solutions from each other but neither captures the patterns in the neural data. One possible interpretation is that these models are not disentangling enough meaning that they are not capturing the overall causal structure underlying the images sufficiently. We now give the details of each the VAE variants.</p><p hwp:id="p-127"><italic toggle="yes">β-VAE.</italic> The loss function that VAE optimizes, also referred to as the evidence lower bound (ELBO), is as the following<sup><xref rid="cv3" ref-type="bibr" hwp:id="xref-ref-93-1" hwp:rel-id="ref-93">3</xref></sup>:
<disp-formula id="eqnS2" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="282798v2_eqnS2.gif" position="float" orientation="portrait" hwp:id="graphic-27"/></alternatives>
</disp-formula>
where <italic toggle="yes">I</italic><sub><italic toggle="yes">target</italic></sub> is the target image, <italic toggle="yes">I</italic><sub><italic toggle="yes">output</italic></sub> is the reconstructed image, <italic toggle="yes">|I</italic><sub><italic toggle="yes">target</italic></sub> <italic toggle="yes">- I</italic><sub><italic toggle="yes">output</italic></sub><italic toggle="yes">|</italic> is the reconstruction loss used in this study (i.e., l1 norm), and <italic toggle="yes">KLD</italic>(<italic toggle="yes">μ, σ</italic>) denotes a the Kullback-Leibler (KL) divergence with respect to the standard multivariate Gaussian distribution.</p><p hwp:id="p-128">The reconstruction, <italic toggle="yes">I</italic><sub><italic toggle="yes">output</italic></sub> is obtained by inputting <italic toggle="yes">z N</italic> (<italic toggle="yes">μ, σ</italic>), a single sample from a multi-variate Gaussian distribution, to a decoder, where <italic toggle="yes">μ</italic> and <italic toggle="yes">σ</italic> are the outputs of an encoder that takes as input an image, <italic toggle="yes">I</italic><sub><italic toggle="yes">input</italic></sub>.</p><p hwp:id="p-129">As encoder, we used the Regress-ID architecture (including the pretrained AlexNet weights) except we replaced the SFCL layer with two fully connected layers each with 200 units. Following the re-parametrization trick in <sup><xref rid="cv3" ref-type="bibr" hwp:id="xref-ref-93-2" hwp:rel-id="ref-93">3</xref></sup>, these two layers represent parameters of a multivariate Gaussian distribution, <italic toggle="yes">mu</italic> and a transformation of <italic toggle="yes">σ, log var</italic>. <italic toggle="yes">z</italic> denotes a single sample from this distribution. The decoder (shared across <italic toggle="yes">β</italic>-VAE and VAE-QN) consisted of a series of 2D deconvolution and batch-normalization layers (<xref rid="tblE4" ref-type="table" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Extended Data Table 4</xref>).</p><p hwp:id="p-130">The <italic toggle="yes">β</italic>-VAE <sup><xref rid="cv1" ref-type="bibr" hwp:id="xref-ref-91-2" hwp:rel-id="ref-91">1</xref></sup>, the version of VAE we use, modifies the last term in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-4" hwp:rel-id="disp-formula-1">Equation 1</xref>, <italic toggle="yes">KLD</italic>(<italic toggle="yes">μ, σ</italic>) to be <italic toggle="yes">γ ∗</italic> (<italic toggle="yes">c - KLD</italic>(<italic toggle="yes">μ, σ</italic>)), where <italic toggle="yes">γ</italic> and <italic toggle="yes">c</italic> are constants. The resulting equation is as the following.
<disp-formula id="ueqn2" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="282798v2_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-28"/></alternatives>
</disp-formula>
The hyperparameter <italic toggle="yes">γ</italic> controls the influence of the KLD associated loss. The hyperparameter <italic toggle="yes">c</italic>, referred to as controlled capacity, serves as an annealing parameter turning down the influence of KLD <italic toggle="yes">e ∗ c</italic> amount where <italic toggle="yes">e</italic> is the training epoch number.</p><p hwp:id="p-131">In <italic toggle="yes">β</italic>-VAE training, <italic toggle="yes">I</italic><sub><italic toggle="yes">target</italic></sub> and <italic toggle="yes">I</italic><sub><italic toggle="yes">input</italic></sub> are identical images except <italic toggle="yes">I</italic><sub><italic toggle="yes">target</italic></sub> is downscaled to be 64×64 pixels, whereas <italic toggle="yes">I</italic><sub><italic toggle="yes">input</italic></sub> is 227×227 pixels. We trained <italic toggle="yes">β</italic>-VAE using the Pose dataset, which consisted of 2500 sets of images where each set contains 20 images of the same face identity with randomly sampled positions (total of 50000 images). Identities varied across sets. We use <italic toggle="yes">γ</italic> = 0.1, <italic toggle="yes">C</italic> = 100 and L1 distance as the reconstruction criteria between <italic toggle="yes">I</italic><sub><italic toggle="yes">target</italic></sub> and <italic toggle="yes">I</italic><sub><italic toggle="yes">output</italic></sub>. We used Adam optimizer with a learning rate of 1 <italic toggle="yes">∗</italic> 10<sup>−5</sup>.</p><p hwp:id="p-132">Despite our extended efforts – including experimenting with different encoders and decoders, trying a range of settings for <italic toggle="yes">γ</italic> and <italic toggle="yes">C</italic>, and using other datasets – this version of the VAE did not seem to learn to perform the task as indicated by the low view-invariance coefficients at layer SFCL, and more generally, this model did not give rise to patterns similar to those found in the neural data or in EIG.</p><sec id="s6a3a" hwp:id="sec-36"><title hwp:id="title-39">VAE-QN</title><p hwp:id="p-133">We also considered a second VAE variant, based on the recent Generative Query Network (GQN) that aimed to learn view-invariant scene representations as well as a neural graphics engine using a form of meta-learning <sup><xref rid="cv2" ref-type="bibr" hwp:id="xref-ref-92-2" hwp:rel-id="ref-92">2</xref></sup>. We formulated GQN for our purposes in the following way. Given an input image and a query pose, the network, like the <italic toggle="yes">β</italic>-VAE network, encodes the image and samples a <italic toggle="yes">z</italic> vector, but differently from <italic toggle="yes">β</italic>-VAE it also infers the pose of the face in the image. It then renders the scene captured in the <italic toggle="yes">z</italic> vector at the query pose. Following GQN, we reasoned that the network would need to learn to infer the intrinsic properties of an input face in order to be able to render it at random query angles. We refer to this model as VAE query network or VAE-QN in short.</p><p hwp:id="p-134">The VAE-QN model used the same encoder architecture (and pretrained weights) as the <italic toggle="yes">β</italic>-VAE network for estimating <italic toggle="yes">mu</italic> and <italic toggle="yes">sigma</italic> and sampling <italic toggle="yes">z</italic>. It also used a parallel encoder for pose, which forks the output of the pre-TCL layer in the encoder (the convolutional layer just before TCL) and outputs a two dimensional pose vector, <italic toggle="yes">P</italic> = {<italic toggle="yes">P</italic><sub><italic toggle="yes">x</italic></sub>, <italic toggle="yes">P</italic><sub><italic toggle="yes">z</italic></sub>}, using a small convolutional network (architecture shown in <xref rid="tblE5" ref-type="table" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Extended Data Table 5</xref>).</p><p hwp:id="p-135">The basic decoder architecture followed that of The basic decoder architecture followed that of the <italic toggle="yes">β</italic>-VAE but required the following modifications to allow factoring in of the pose query. It included an initial fully connected layer that took as input a concatenated vector of <italic toggle="yes">z</italic> and <italic toggle="yes">P</italic> (vector of length 202) and outputed a 200 dimensional vector. This fully connected layer than was fed into the decoder architecture shown in <xref rid="tblE4" ref-type="table" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">Extended Data Table 4</xref>.</p><p hwp:id="p-136">VAE-QN was trained following the same parameters as <italic toggle="yes">β</italic>-VAE, including <italic toggle="yes">γ</italic> = 0.1, <italic toggle="yes">C</italic> = 100 and L1 distance as a reconstruction criterion, and Adam optimizer with a learning rate of 1 <italic toggle="yes">∗</italic> 10<sup>−5</sup>.</p><p hwp:id="p-137">We found that VAE-QN achieved a high-level of view-invariance by its SFCL, indicating that it learned the task. It also achieved a lower reconstruction loss on the validation set when compared to <italic toggle="yes">β</italic>-VAE. Indeed, we found reconstructions by this model to be less blurrier and more accurate than <italic toggle="yes">β</italic>-VAE. However, unlike EIG, this network gave rise to patterns that were similar to those found in the VGG network with high view-invariance and weaker mirror-symmetry in its FFCL.</p></sec></sec></sec><sec id="s6b" hwp:id="sec-37"><label>2</label><title hwp:id="title-40">Impact of pretrained weights and dropout rate</title><p hwp:id="p-138">In this section, we report the impact that two key aspects of the training procedure have on the emergence of fully brain-like transformations: whether to start training from pretrained weights and the rate of dropout used during training. In each case, we evaluate how these aspects of training impact the EIG network in comparison to variants of the ID network by looking at the speed or efficiency of training and the resulting network similarity matrices.</p><sec id="s6b1" hwp:id="sec-38"><title hwp:id="title-41">Trade-off arising from the use of pretrained weights and training targets</title><p hwp:id="p-139">The pretrained Alexnet weights provide the EIG network not only with a generic way to extract useful features from images <sup><xref rid="cv4" ref-type="bibr" hwp:id="xref-ref-94-1" hwp:rel-id="ref-94">4</xref></sup> but it also gives a good initial representational state, in particular those of its TCL and FFCL, which contain the right qualitative structure (view-specificity in TCL and mirror-symmetry in FFCL) with respect to the neural data. Here we analyze the impact of starting training from this initial state, particularly how it interacts with the training targets implied by the classification vs. inference hypotheses.</p><p hwp:id="p-140">On the classification hypothesis side, we include three variants of the ID network (ID-1, ID-2, and ID-3 networks) in addition to the VGG network. The ID network mentioned above (see also SI Section 1) shares the same architecture as EIG, but its weights are initialized randomly from scratch. Here, instead, the three variants of the ID network all start training from the pretrained weights, like the EIG network. All three networks were trained using the same generative model based dataset as the VGG network until an early stopping criteria on a shared validation set was triggered.</p><p hwp:id="p-141">In the ID-1 network, we finetuned the layer SFCL starting from its pretrained weights and trained the layer TFCL starting from random initial weights. All other weights in the network were assigned and fixed to their pretrained weights. The ID-2 network, like the EIG network, updated the top four layers, including finetuning layers TCL, FFCL, and SFCL starting from their pretrained weights and training TFCL starting from random initial weights. The ID-3 network also updated all four layers like the ID-2 network but also included a high dropout rate (i.e., with probability 0.5) between each of the fully connected layers.</p><p hwp:id="p-142">The ID-1 network’s FFCL – note that it is identical to that of the pretrained Alexnet by design – show a high level of mirror-symmetry (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Extended Data Fig. 6a</xref>), consistent with the selectivity patterns seen in the neural data. However, this representational result comes at the cost of performing the task much worse. Its view-invariance coefficient is very low (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-2" hwp:rel-id="F11">Extended Data Fig. 6b</xref>) and its final average validation loss is very high in comparison all other models we considered. These indicate that the network cannot learn to perform the task of person identification much beyond the chance level. Consistent with this, its behavioral fits were poor (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-3" hwp:rel-id="F11">Extended Data Fig. 6c</xref>) including in Exp. 1 which does not require any particular qualitative generalization.</p><p hwp:id="p-143">Overall, a strong pattern of trade-off emerged between performance and behavioral fits on one side and neural fits on the other: Across ID-1, ID-2, ID-3, and VGG networks, the better a network performed the classification task (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-4" hwp:rel-id="F11">Extended Data Fig. 6b</xref>) and the better it fitted the behavioral data (in particular Exp. 1; <xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-5" hwp:rel-id="F11">Extended Data Fig. 6c</xref>), the less it was mirror symmetric (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-6" hwp:rel-id="F11">Extended Data Fig. 6a</xref>). That is, through training, only by over-writing the existing mirror symmetry in FFCL of the pretrained weights the classification networks come to perform their corresponding task well. We illustrate this for ID-3 network in <xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-7" hwp:rel-id="F11">Extended Data Fig. 6d</xref> (right column) by plotting the mirror symmetry and view-invariance scores for its FFCL and SFCL layers throughout training.</p><p hwp:id="p-144">This trade-off does not apply to the EIG network. Mirror-symmetry in EIG’s <italic toggle="yes">f</italic><sub>4</sub> layer increases with further training (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-8" hwp:rel-id="F11">Extended Data Fig. 6d</xref>, left column), and significantly exceeds the mirror symmetry found in the pretrained Alexnet weights (<italic toggle="yes">p &lt;</italic> 0.001; EIG vs. ID-1 as shown in <xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-9" hwp:rel-id="F11">Extended Data Fig. 6a</xref>). Moreover, view-invariance converges to a high value in EIG’s <italic toggle="yes">f</italic><sub>5</sub> layer with further training (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-10" hwp:rel-id="F11">Extended Data Fig. 6b</xref>, and <xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-11" hwp:rel-id="F11">6d</xref> left column). Finally, the EIG network gives overall better fits to the behavioral data (<xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-12" hwp:rel-id="F11">Extended Data Fig. 6c</xref>). In short, these results indicate that mirror-symmetry gets in the way of performance in the classification networks whereas it facilitates and is facilitated by training on the latent variables as targets in the EIG network.</p></sec><sec id="s6b2" hwp:id="sec-39"><title hwp:id="title-42">Pretraining is not necessary, but it improves efficiency</title><p hwp:id="p-145">Is pretraining necessary for a full set of brain-like transformations, including an intermediate mirror-symmetry stage, to emerge in the EIG network? To answer this question, we trained the EIG network starting from random initial weights as opposed to transferring weights from the pretrained Alexnet. We found that this training substantially reduced mirror-symmetry in the hidden full connectivity layer (<italic toggle="yes">f</italic><sub>4</sub>, Extended Data Figs. 7a-d). However, when we trained a larger network with an additional hidden layer of full connectivity (<italic toggle="yes">f</italic><sub>4</sub><italic toggle="yes">e</italic> <xref rid="figE7" ref-type="fig" hwp:id="xref-fig-12-2" hwp:rel-id="F12">Extended Data Fig. 7e</xref>), we found that it recovered a significant level of mirror-symmetry at that extra hidden layer (Extended Data Figs. 7f, g). We also found that it took approximately five times longer to train these networks: three to five hundreds more training epochs were required to attain a loss value comparable to that of 75 epochs-long training with the original EIG network. Training of the network with two hidden full connectivity layers took about hundred more training epochs to reach the same levels of validation loss as the network with one hidden full connectivity layer. Even though longer training times are expected for networks starting from random initial weights, these results indicate that the original EIG training is efficient in two ways: (1) it reduces the number of hidden layers required for recapitulating the full set of of brain-like transformations, and (2) it trains much faster.</p></sec><sec id="s6b3" hwp:id="sec-40"><title hwp:id="title-43">Number of hidden full-connectivity layers in EIG</title><p hwp:id="p-146">We evaluated different architectures by varying the number of hidden full-connectivity layers before the scene properties layer to be zero, one (the standard EIG architecture; <xref rid="tblE2" ref-type="table" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Extended Data Table 2</xref>), or two, meaning that the total number of full-connectivity layers could be one, two, or three. We found that with no hidden full-connectivity layers, training was not faster despite the smaller total number of parameters needed to be trained and the network did not give rise to an intermediate stage mirror-symmetry layer. Instead, it transformed directly from view-based representations in TCL to view-invariant representations in the FCL.</p><p hwp:id="p-147">We found that with one hidden full-connectivity layer, a full set of brain-like transformations emerged provided that training started with the pretrained weights in Alexnet (e.g., <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-15" hwp:rel-id="F9">Extended Data Fig. 4c</xref>; and see the subsection above). Finally, we found that with two hidden full-connectivity layers, an intermediate mirror-symmetry stage was always recovered independently of whether training started from scratch (<xref rid="figE7" ref-type="fig" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Extended Data Fig. 7f</xref>) or from pretrained weights in Alexnet. In sum, for a full set of brain-like transformations to emerge, these results suggest that an Alexnet-like architecture is needed with multiple convolutional layers followed by at least one hidden full-connectivity layer before the scene properties layer.</p></sec><sec id="s6b4" hwp:id="sec-41"><title hwp:id="title-44">Impact of dropout rate on the EIG network</title><p hwp:id="p-148">The use of dropout in the ID networks (e.g., ID-3 vs ID-2 networks in <xref rid="figE6" ref-type="fig" hwp:id="xref-fig-11-13" hwp:rel-id="F11">Extended Data Fig. 6</xref>) improves performance while reducing mirror symmetry. Here we explore how the rate of dropout impacts the learned representations in the EIG network. Similar to the ID networks, we found that high dropout rates (e.g., dropout with probability 0.5, but not with probability 0.1 or 0.01) during any face-specific training (either finetuning or training from scratch) reduced mirror symmetry in the EIG network. However, unlike the ID networks, it did not lead to better behavioral fits nor any performance benefits in terms of the validation loss. Instead, using a high dropout rate resulted in a five-fold slow-down in the learning speed. (The number of epochs required to reach a certain level of validation loss were 373 epochs and 75 epochs for training with and without dropout.)</p><p hwp:id="p-149">Note that during training, dropout randomizes the set of weights across forward calls breaking any potential symmetry of weights between pairs of mirror-symmetric poses. In contrast to EIG’s tolerance of low dropout rates, a perhaps more stringent requirement is observed in the theory developed by Leibo et al.<sup><xref rid="cv5" ref-type="bibr" hwp:id="xref-ref-95-1" hwp:rel-id="ref-95">5</xref></sup> where obtaining mirror-symmetry required squaring of the odd (but not even) PCA components, akin to using the same exact weights twice.</p></sec><sec id="s6b5" hwp:id="sec-42"><title hwp:id="title-45">Training set with mirror-symmetric views</title><p hwp:id="p-150">Finally, another requirement of the theory developed by Leibo et al.<sup><xref rid="cv5" ref-type="bibr" hwp:id="xref-ref-95-2" hwp:rel-id="ref-95">5</xref></sup> is a dataset containing mirror-symmetric views of the same identity. We found that this requirement was largely optional in EIG, with not including reflections of images over the vertical midline during training still resulted in mirror symmetry albeit at a somewhat reduced level.</p></sec></sec><sec id="s6c" hwp:id="sec-43"><label>3</label><title hwp:id="title-46">Functionally interpreting ML/MF and <italic toggle="yes">f</italic><sub>3</sub> using the generative model</title><p hwp:id="p-151">Albedos and normals for each of the 25 person identities in the FIV image set are approximated using EIG and the generative model. The 3D shape and texture properties for each frontal-pose FIV image are inferred using EIG (outputs at <italic toggle="yes">f</italic><sub>5</sub>). Given the resulting 3D meshes, we obtained the face proper regions by masking out the neck, ears, and hair from the resulting 3D meshes for each identity. Using the generative model, we rendered the 2.5D components of each of the masked meshes at the 7 mean pose values underlying the extrinsic scene parameter distribution for the FIV-S image set (<xref rid="tblE1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Extended Data Table 1</xref>). Finally, we adjusted the size and location of faces in the images using the same normalization procedure as the attended images.</p><p hwp:id="p-152">We hypothesized that the random variables in the generative model (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-20" hwp:rel-id="F1">Fig. 1b</xref> main text and <xref rid="figE1" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Extended Data Fig. 1a</xref>) that are hierarchically below the 3D scene properties each provide a conditional independence stage that could be exploited by ML/MF or AL. We tested this hypothesis using the similarities arising from each of the potential conditional independence stages (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Fig. 3a</xref>, main text): raw input images, attended images, and 2.5D components including albedos and normals. The attended images and 2.5D components are both better accounts of ML/MF than the raw images (<italic toggle="yes">p &lt;</italic> 0.001; raw images <italic toggle="yes">r</italic> = 0.29[0.24, 0.35], attended images <italic toggle="yes">r</italic> = 0.52[0.47, 0.57], albedos 0.60[0.56, 0.64], and normals 0.63[0.59, 0.67]) with the 2.5D components providing a significantly better account than the attended images (<italic toggle="yes">p &lt;</italic> 0.001 for each 2.5D component). However, <italic toggle="yes">f</italic><sub>3</sub> continued to provide a better account of ML/MF than the 2.5D components (<italic toggle="yes">p &lt;</italic> 0.001).</p><p hwp:id="p-153">Similar to our ML/MF results (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-5" hwp:rel-id="F3">Fig. 3b</xref>, main text), we found that <italic toggle="yes">f</italic><sub>3</sub> similarity matrix itself was highly correlated with the 2.5D components to a much better degree than the raw input images (albedos 0.83[0.81, 0.85], normals 0.84[0.82, 0.86], raw images 0.36[0.28, 0.44]; <italic toggle="yes">p &lt;</italic> 0.001 for each comparison), with the exception that attended images correlated with <italic toggle="yes">f</italic><sub>3</sub> as highly as the 2.5D components (0.86[0.85, 0.87]; <xref rid="figE8" ref-type="fig" hwp:id="xref-fig-13-4" hwp:rel-id="F13">Extended Data Fig. 8a</xref>). To better understand <italic toggle="yes">f</italic><sub>3</sub> and its relationship to attended images, we used the FIV-S-2 image set consisting of higher image-level variability which allowed us to tell apart attended images from the 2.5D components. Unlike the attended images, albedos and normals continued to correlate consistently well with <italic toggle="yes">f</italic><sub>3</sub> (raw images 0.25[0.23, 0.29], attended images 0.46[0.44, 0.50], albedos 0.85[0.82, 0.86], normals 0.87[0.85, 0.88]; <xref rid="figE8" ref-type="fig" hwp:id="xref-fig-13-5" hwp:rel-id="F13">Extended Data Fig. 8b</xref>). These results collectively suggest that both ML/MF and <italic toggle="yes">f</italic><sub>3</sub> can be understood as 2.5D-like surface representations and also suggests use of image sets with broader image-level variability in future experiments for better understanding ML/MF computations.</p><sec id="s6c1" hwp:id="sec-44"><title hwp:id="title-47">Linear decodability of the 2.5D-like representations</title><p hwp:id="p-154">We consistently find that VGG and its variants trained to estimate face identity (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-36" hwp:rel-id="F2">Fig. 2j-m</xref>, <xref rid="figE2" ref-type="fig" hwp:id="xref-fig-7-13" hwp:rel-id="F7">Extended Data Fig. 2a-j</xref>, <xref rid="figE4" ref-type="fig" hwp:id="xref-fig-9-16" hwp:rel-id="F9">Extended Data Fig 4l-ae</xref>) do not produce an AL-like mirror symmetric representation distinct from both the 2.5D-like representation in ML/MF and EIG-<italic toggle="yes">f</italic><sub>3</sub> and the 3D scene property representation in AM and EIG-<italic toggle="yes">f</italic><sub>5</sub>; instead, all fully connected layers of these networks have similar responses with strong viewpoint-invariant identity coding from the first fully connected layer (FFCL) upwards. To explain this, we hypothesized that these discriminatively trained networks are performing a fundamentally different computation in their hidden layers than EIG and the face-patch circuitry: While EIG and the ventral stream appear to need a distinct hidden-layer transformation to solve the nonlinear mapping from 2.5D surface components to 3D object properties – our interpretation for the function of AL and EIG-<italic toggle="yes">f</italic><sub>4</sub> – the identity classification task that VGG and its variants are trained for might be linearly solvable from the high-level image features computed in these networks’ top convolutional layer (TCL), with no need for further nonlinear transformations.</p><p hwp:id="p-155">To test this hypothesis, we attempted to linearly decode identity on the FIV faces from each of the models: layers TCL, FFCL, and SFCL in VGG and layers <italic toggle="yes">f</italic><sub>3</sub>, <italic toggle="yes">f</italic><sub>4</sub>, <italic toggle="yes">f</italic><sub>5</sub> in EIG. Specifically, we trained a one-layer linear-softmax classification network on ML/MF, and on the max-pooling outputs of EIG-<italic toggle="yes">f</italic><sub>3</sub> and VGG-TCL, to decode all 25 FIV identities. We split the 175 FIV images to 6 poses (6 <italic toggle="yes">×</italic> 25 = 150 images) for training and 1 pose (25 images) for testing with averaging results across all 7 possible splits. <xref rid="figE9" ref-type="fig" hwp:id="xref-fig-14-2" hwp:rel-id="F14">Extended Data Fig. 9a</xref> shows the held-out test performance of the linear classifier. All layers in both networks gave rise to above chance (4%) decoding performance, but we found far better decodability of identity in the VGG network, with its TCL representation already achieving near FFCL and SFCL performance. In contrast, in the EIG network identity wasn’t nearly as linearly decodable initially at its <italic toggle="yes">f</italic><sub>3</sub> layer but increased to a comparable level of performance as the VGG network by layer <italic toggle="yes">f</italic><sub>5</sub> (<xref rid="figE9" ref-type="fig" hwp:id="xref-fig-14-3" hwp:rel-id="F14">Extended Data Fig. 9a</xref> left panel). These results support our conjecture that the face identity might be linearly solvable from the TCL representations of the VGG network and its variants, without a need for further nonlinear transformations.</p><p hwp:id="p-156">We also tested whether a nonlinear transformation on top of the 2.5D-like representations –e.g., the layer <italic toggle="yes">f</italic><sub>4</sub> in the EIG network– are required for mapping these representations to 3D object properties. We attempted to linearly decode the shape and texture properties of the FIV images –approximated using the EIG network as its layer <italic toggle="yes">f</italic><sub>5</sub> outputs given the 175 FIV images– based on both models, layers TCL, FFCL, and SFCL in VGG and layers <italic toggle="yes">f</italic><sub>3</sub>, <italic toggle="yes">f</italic><sub>4</sub>, and <italic toggle="yes">f</italic><sub>5</sub> in EIG. We performed linear regression using the partial least squares (PLS) method with 33 retained components <sup><xref rid="cv6" ref-type="bibr" hwp:id="xref-ref-96-1" hwp:rel-id="ref-96">6</xref>, <xref rid="cv7" ref-type="bibr" hwp:id="xref-ref-97-1" hwp:rel-id="ref-97">7</xref></sup>. We split the 175 FIV images to 6 poses (6 <italic toggle="yes">×</italic> 25 = 150 images) for training and 1 pose (25 images) for testing with averaging results across all 7 possible splits. <xref rid="figE9" ref-type="fig" hwp:id="xref-fig-14-4" hwp:rel-id="F14">Extended Data Fig. 9b</xref> shows the goodness-of-fit <italic toggle="yes">R</italic><sup>2</sup> values on the held-out test sets. We found that these shape and texture vectors were not linearly decodable from any of the VGG layers, whereas it became increasingly more decodable in the EIG network from layer <italic toggle="yes">f</italic><sub>3</sub> to <italic toggle="yes">f</italic><sub>4</sub>. Notably, the intrinsic scene properties (i.e., the shape and texture properties) were much less linearly decodable at layer <italic toggle="yes">f</italic><sub>3</sub> when compared to layer <italic toggle="yes">f</italic><sub>5</sub> indicating that indeed the transformation from 2.5D-like representations to 3D scenes requires some nonlinear transformation.</p></sec></sec><sec id="s6d" hwp:id="sec-45"><label>4</label><title hwp:id="title-48">Neural data</title><p hwp:id="p-157">The neural experiments and the data presented in the main text were originally reported in <sup><xref rid="cv8" ref-type="bibr" hwp:id="xref-ref-98-1" hwp:rel-id="ref-98">8</xref></sup>.</p><sec id="s6d1" hwp:id="sec-46"><title hwp:id="title-49">Stimulus and experimental procedure</title><p hwp:id="p-158">The neural experiments used the FIV image set. FIV included images of 25 person identities with each identity viewed at 7 different head orientations: left-profile, left-half-profile, straight, right-half-profile, right-profile, upwards, downwards. (The original recordings also used an 8th viewing condition,the back of the head, but we didn’t analyze the corresponding data in this study).</p><p hwp:id="p-159">Images were shown in a rapid serial presentation mode with 200 msec on-time followed by 200 msec blank screen with gray background. Images were presented centrally and subtended an angle of 7°. Monkeys were given a juice reward for maintaining fixation at the center of the screen for 3 seconds.</p></sec><sec id="s6d2" hwp:id="sec-47"><title hwp:id="title-50">Neural recordings</title><p hwp:id="p-160">Single-unit recordings were made from three male rhesus macaque monkeys (<italic toggle="yes">Macaca mulatta</italic>). Before the recordings, face-selective regions in each subject were localized using functional magnetic resonance imaging (fMRI). The face-selective regions were determined as the regions that were activated more to faces in comparison to bodies, objects, fruits, hands, and scrambled patterns. Single-unit recordings were performed at four of the fMRI-identified face-selective patches, all in the inferior temporal cortex: middle lateral and middle fundus areas ML/MF, anterior lateral area, AL, and anteriormedial area, AM. Following the original study, we combined the responses from the regions ML and MF in our analysis due to their general similarity (referred to as ML/MF).</p><p hwp:id="p-161">A single neuron was targeted at each recording session, in which each image was presented 1 to 10 times in a random order. Following <sup><xref rid="cv8" ref-type="bibr" hwp:id="xref-ref-98-2" hwp:rel-id="ref-98">8</xref></sup>, we only analyze responses of the well isolated units.</p></sec></sec><sec id="s6e" hwp:id="sec-48"><label>5</label><title hwp:id="title-51">Supplementary psychophysics methods and model-free analysis</title><sec id="s6e1" hwp:id="sec-49"><title hwp:id="title-52">Experiment 1</title><p hwp:id="p-162">A total of 48 participants were recruited over Amazon’s crowdsourcing platform, Mechanical Turk (one additional participant was eliminated due to performing at or worse than the chance performance, 50%). The task took about 10 minutes to complete. Each participant was paid $1.50 ($9.00<italic toggle="yes">/</italic>hour). All participants provided their informed consent and were at the age of 18 or older according to their self-report.</p><p hwp:id="p-163">The average performance of participants was 66% with a standard deviation of 7%, a min value of 53%, and a max value of 78%. Two tailed t-tests revealed that the performance of the Experiment 1 participants were not statistically distinguishable from that of the Experiment 2 participants (<italic toggle="yes">p</italic> = 0.23) but both Experiment 1 and 2 participants performed more accurately than the Experiment 3 participants (<italic toggle="yes">p &lt;</italic> 0.001 and <italic toggle="yes">p &lt;</italic> 0.05). See the corresponding subsections below for further details of the accuracy distributions of Experiment 2 and 3 participants.</p><p hwp:id="p-164">The average reaction time of participants was 1479 msecs, with a standard deviation of 626 msecs, a min value of 426 msecs, and a max value of 3754 msecs. Two-tailed t-tests revealed that the reaction time distributions were not statistically distinguishable from each other across all pairs of the three experiments (<italic toggle="yes">p &gt;</italic> 0.2 in all pair-wise comparisons). See the corresponding subsections for the reaction time statistics of Experiments 2 and 3.</p><p hwp:id="p-165">To examine effects of learning in each experiment, we considered a moving-window performance of the participants as the trials progressed from the 1st trial to the 96th trial (<xref rid="figE10" ref-type="fig" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Extended Data Fig. 10</xref>). We used a window size of 10 trials and stride of 1. For each participant and moving-window index, we found that participant’s average performance in the next 10 trials including the current trial. Even though the data suggested some learning in the early trials in Experiment 1 (e.g., significant difference between the 1st and 10th windows’ performance, <italic toggle="yes">p &lt;</italic> 0.05), there was no indication of learning in Experiments 2 and 3 (e.g., no significant difference between the 1st and 10th windows, <italic toggle="yes">p &gt;</italic> 0.35 in both experiments).</p></sec><sec id="s6e2" hwp:id="sec-50"><title hwp:id="title-53">Experiment 2</title><p hwp:id="p-166">A total of 48 participants were recruited over Amazon’s crowdsourcing platform, Mechanical Turk (seven additional participants were eliminated due to performing at or worse than the chance performance, 50% and two other participants were eliminated because their average reaction times were very short, 19 msecs and 30 msecs, much less than even the perception-to-action cycle of the expert video game players, 100 msecs). The task took about 10 minutes to complete. Each participant was paid $1.50 ($9.00<italic toggle="yes">/</italic>hour). All participants provided their informed consent and were at the age of 18 or older according to their self-report.</p><p hwp:id="p-167">Participants’ average accuracy was 64% with a standard deviation of 7%, a min value of 52%, and a max value of 80%. Their average reaction time was 1542 msecs, with a standard deviation of 564 msecs, a min value of 151 msecs, and a max value of 3716 msecs.</p></sec><sec id="s6e3" hwp:id="sec-51"><title hwp:id="title-54">Experiment 3</title><p hwp:id="p-168">A total of 44 participants were recruited over Amazon’s crowdsourcing platform, Mechanical Turk (12 additional participants were eliminated due to performing at or worse than the chance performance, 50%, and one other participant was eliminated because their average reaction time was very short, 16 msecs, much less than even the perception-to-action cycle of the expert video game players, 100 msecs). The task took about 10 minutes to complete. Each participant was paid $1.50 ($9.00<italic toggle="yes">/</italic>hour). All participants provided their informed consent and were at the age of 18 or older according to their self-report.</p><p hwp:id="p-169">Participants’ average accuracy was 61% with a standard deviation of 6%, a min value of 51%, and a max value of 73%. Their average reaction time was 1403 msecs, with a standard deviation of 472 msecs, a min value of 508 msecs, and a max value of 2792 msecs.</p></sec><sec id="s6e4" hwp:id="sec-52"><title hwp:id="title-55">Bootstrap procedure</title><p hwp:id="p-170">In order to quantify the correlations between the models’ predictions and the data, we sampled whole subject responses with replacement. We generated 10, 000 such boot-strap samples. All p-values were estimated using direct bootstrap hypothesis testing <sup><xref rid="cv9" ref-type="bibr" hwp:id="xref-ref-99-1" hwp:rel-id="ref-99">9</xref></sup>.</p></sec><sec id="s6e5" hwp:id="sec-53"><title hwp:id="title-56">Lighting elevation judgment task</title><p hwp:id="p-171">A total of 60 participants were recruited over Amazons crowd-sourcing platform, Mechanical Turk. The task took about 10 minutes to complete. Each participant was paid $1.50 ($9.00<italic toggle="yes">/</italic>hour). Half of these participants participated in the light source elevation condition, and the other half participated in the depth-suppression condition. The experimental procedure was identical between the two groups.</p><p hwp:id="p-172">Before the beginning of the experimental trials, both groups of participants were instructed that they would see images of faces that could be lit anywhere from the top of the face to the bottom of the face using an illustration of the range of possible scene lighting conditions (<xref rid="figE11" ref-type="fig" hwp:id="xref-fig-16-1" hwp:rel-id="F16">Extended Data Fig. 11a</xref>). An example trial from the lighting source elevation condition is shown in <xref rid="figE11" ref-type="fig" hwp:id="xref-fig-16-2" hwp:rel-id="F16">Extended Data Fig. 11b</xref>.</p></sec><sec id="s6e6" hwp:id="sec-54"><title hwp:id="title-57">Face depth judgment task</title><p hwp:id="p-173">A total of 40 participants were recruited over Amazons crowdsourcing platform, Mechanical Turk. The task took about 15 minutes to complete. Each participant was paid $2.00 ($8.00<italic toggle="yes">/</italic>hour).</p></sec><sec id="s6e7" hwp:id="sec-55"><title hwp:id="title-58">Linearly decoding lighting elevation and face depth from the VGG network layers</title><p hwp:id="p-174">The VGG network doesn’t explicitly infer the lighting elevation or the profile depth of a face given an image. But it is possible that the representations it learns in its intermediate stages can support linearly decoding these latent variables. To test this possibility and therefore to evaluate the classification hypothesis on the hollow-face effect, we attempted to linearly decode lighting elevation and profile depth of a face from each of the VGG network layers.</p><p hwp:id="p-175">We attempted to learn a linear mapping (using a fully-connected layer) from each of the three layers of the VGG network (TCL, FFCL, and SFCL) to lighting elevation based on an image set of 10000 face image-lighting elevation (<italic toggle="yes">L</italic><sub><italic toggle="yes">z</italic></sub>) pairs drawn from the generative model. For both target variables (lighting elevation and profile depth which is described below), we trained the newly added linear layer using stochastic gradient descent while freezing weights in the rest of the network. We report results based on the layer where lighting elevation was most accurately decodeable (FFCL; decodings based on this layer also correlated best with the data). We found that the lighting elevation was linearly decodeable (<xref rid="figE13" ref-type="fig" hwp:id="xref-fig-18-2" hwp:rel-id="F18">Extended Data Fig. 13a</xref> control stimuli) and overall the resulting decoder matched human judgments to a good extent (<xref rid="figE13" ref-type="fig" hwp:id="xref-fig-18-3" hwp:rel-id="F18">Extended Data Fig. 13a, b</xref>), but it correlated (<italic toggle="yes">r</italic> = 0.89) with the data significantly worse when compared to EIG (<italic toggle="yes">r</italic> = 0.95, <italic toggle="yes">p &lt;</italic> 0.01 two-tailed test for difference of two correlation coefficients). More important than this small difference in correlations with human judgments is the cause of the difference. In the illusory condition, both EIG and humans show a sharp nonlinear change in judged lighting direction between flat faces and the first step of concave faces (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-10" hwp:rel-id="F5">Fig. 5a</xref> left panel, main text). VGG, in contrast, shows a much more gradual, almost linear trend at the same point, and has much greater variance (larger error bars) in its predictions at this step and adjacent steps (<xref rid="figE13" ref-type="fig" hwp:id="xref-fig-18-4" hwp:rel-id="F18">Extended Data Fig. 13a</xref> left panel). This suggests that VGG has not captured the interaction between lighting and shape perception that we see in humans and EIG, and which appears to be characteristic of the approximate inverse graphics solution for faces, but perhaps is picking up on some weaker, less specific statistical correlate of lighting direction that can be decoded in images more generally.</p><p hwp:id="p-176">We also attempted to learn a linear mapping from each of the same three layers in the VGG network to profile depths of faces using a segmented version of the same 10000 images as inputs and profile depths as outputs. For each face, we calculated its profile depth the same we did it for the EIG network (and described above): use the generative model to first obtain its 3D shape and assign a depth value as the average displacement of the three key points in the face – nose, left cheek, and right cheek– with respect to the mean face. We report results based on the layer where profile depth was most accurately decodeable (TCL). We found that the decoded profile depths did not capture the trends in the data (<xref rid="figE13" ref-type="fig" hwp:id="xref-fig-18-5" hwp:rel-id="F18">Extended Data Fig. 13b</xref>; overall correlation <italic toggle="yes">r</italic> = 0.28) as well as EIG did (<italic toggle="yes">r</italic> = 0.60, <italic toggle="yes">p &lt;</italic> 0.001 two-tailed test for difference of two correlation coefficients). We also found that overall correlation with judged profile depths was slightly (but not significantly) higher with profile depths decoded from another layer of the network (SFCL; <italic toggle="yes">r</italic> = 0.35, <italic toggle="yes">p</italic> = 0.63 two-tailed test for difference of two correlation coefficients).</p></sec></sec></sec><ref-list hwp:id="ref-list-3"><title hwp:id="title-59">References</title><ref id="cv1" hwp:id="ref-91" hwp:rev-id="xref-ref-91-1 xref-ref-91-2"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.91" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-91"><string-name name-style="western" hwp:sortable="Burgess C. P."><surname>Burgess</surname>, <given-names>C. P.</given-names></string-name> <etal>et al.</etal> <source hwp:id="source-91">Understanding disentangling in <italic toggle="yes">beta</italic>-vae</source>. arXiv preprint arXiv:1804.03599 (<year>2018</year>).</citation></ref><ref id="cv2" hwp:id="ref-92" hwp:rev-id="xref-ref-92-1 xref-ref-92-2"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.92" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-92"><string-name name-style="western" hwp:sortable="Eslami S. A."><surname>Eslami</surname>, <given-names>S. A.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-81">Neural scene representation and rendering</article-title>. <source hwp:id="source-92">Science</source> <volume>360</volume>, <fpage>1204</fpage>–<lpage>1210</lpage> (<year>2018</year>).</citation></ref><ref id="cv3" hwp:id="ref-93" hwp:rev-id="xref-ref-93-1 xref-ref-93-2"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.93" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-93"><string-name name-style="western" hwp:sortable="Kingma D. P."><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Welling M."><surname>Welling</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-82">Auto-encoding variational bayes</article-title>. In <source hwp:id="source-93">Advances in Neural Information Processing Systems</source> (<year>2015</year>).</citation></ref><ref id="cv4" hwp:id="ref-94" hwp:rev-id="xref-ref-94-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.94" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-94"><string-name name-style="western" hwp:sortable="Razavian A. S."><surname>Razavian</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Azizpour H."><surname>Azizpour</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sullivan J."><surname>Sullivan</surname>, <given-names>J.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Carlsson S."><surname>Carlsson</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-83">CNN features off-the-shelf: an astounding baseline for recognition</article-title>. In <source hwp:id="source-94">IEEE Conference on Computer Vision and Pattern Recognition Workshops</source>, <fpage>512</fpage>–<lpage>519</lpage> (IEEE, <year>2014</year>).</citation></ref><ref id="cv5" hwp:id="ref-95" hwp:rev-id="xref-ref-95-1 xref-ref-95-2"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.95" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-95"><string-name name-style="western" hwp:sortable="Leibo J. Z."><surname>Leibo</surname>, <given-names>J. Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liao Q."><surname>Liao</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anselmi F."><surname>Anselmi</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Poggio T."><surname>Poggio</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-84">View-tolerant face recognition and hebbian learning imply mirror-symmetric neural tuning to head orientation</article-title>. <source hwp:id="source-95">Current Biology</source> <volume>27</volume>, <fpage>62</fpage>–<lpage>67</lpage> (<year>2017</year>).</citation></ref><ref id="cv6" hwp:id="ref-96" hwp:rev-id="xref-ref-96-1"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.96" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-96"><string-name name-style="western" hwp:sortable="Helland I."><surname>Helland</surname>, <given-names>I.</given-names></string-name> <article-title hwp:id="article-title-85">Partial least squares regression</article-title>. <source hwp:id="source-96">Encyclopedia of statistical sciences</source> (<year>2006</year>).</citation></ref><ref id="cv7" hwp:id="ref-97" hwp:rev-id="xref-ref-97-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.97" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-97"><string-name name-style="western" hwp:sortable="Pedregosa F."><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-86">Scikit-learn: Machine learning in python</article-title>. <source hwp:id="source-97">Journal of machine learning research</source> <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage> (<year>2011</year>).</citation></ref><ref id="cv8" hwp:id="ref-98" hwp:rev-id="xref-ref-98-1 xref-ref-98-2"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="282798v2.98" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-98"><string-name name-style="western" hwp:sortable="Freiwald W. A."><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tsao D. Y."><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> <article-title hwp:id="article-title-87">Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title>. <source hwp:id="source-98">Science</source> <volume>330</volume>, <fpage>845</fpage>–<lpage>851</lpage> (<year>2010</year>).</citation></ref><ref id="cv9" hwp:id="ref-99" hwp:rev-id="xref-ref-99-1"><label>9.</label><citation publication-type="book" citation-type="book" ref:id="282798v2.99" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-99"><string-name name-style="western" hwp:sortable="Efron B."><surname>Efron</surname>, <given-names>B.</given-names></string-name> &amp; <string-name name-style="western" hwp:sortable="Tibshirani R. J."><surname>Tibshirani</surname>, <given-names>R. J.</given-names></string-name> <source hwp:id="source-99">An Introduction to the Bootstrap</source> (<publisher-name>Chapman &amp; Hall/CRC press</publisher-name>, <publisher-loc>New York and London</publisher-loc>, <year>1994</year>).</citation></ref></ref-list></back></article>
