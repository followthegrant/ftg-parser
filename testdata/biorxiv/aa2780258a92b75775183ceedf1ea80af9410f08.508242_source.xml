<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/508242</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;508242</article-id><article-id pub-id-type="other" hwp:sub-type="slug">508242</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">508242</article-id><article-id pub-id-type="other" hwp:sub-type="tag">508242</article-id><article-version>1.5</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Identifying complex motifs in massive omics data with a variable-convolutional layer in deep neural network</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1 xref-corresp-1-2"><label>*</label>To whom correspondence should be addressed. Email: <email hwp:id="email-1">gaog@mail.cbi.pku.edu.cn</email>, and mail could also be sent to <email hwp:id="email-2">dingy@mail.cbi.pku.edu.cn</email></corresp><fn id="n1" fn-type="equal" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>#</label><p hwp:id="p-1">These authors contributed equally to this work.</p></fn><fn fn-type="others" hwp:id="fn-2"><p hwp:id="p-2"><bold>Biographical Note</bold>: Ge Gao is a Principal Investigator at the Biomedical Pioneering Innovation Center (BIOPIC) &amp; Beijing Advanced Innovation Center for Genomics (ICG), Peking University. His research group focuses on developing novel computational tools for deciphering the function and evolution of gene regulation circuits.</p></fn><fn fn-type="others" hwp:id="fn-3"><p hwp:id="p-3">Yang Ding is a PostDoc Research Fellow at Beijing Institute of Radiation Medicine. He is interested in high-throughput computing-based data mining and methodology development in genomics and boinformatics.</p></fn><fn fn-type="others" hwp:id="fn-4"><p hwp:id="p-4">jing-Yi Li is a Ph.D. candidate at Peking University. He is interested in developing deep learning methods for omics data.</p></fn><fn fn-type="others" hwp:id="fn-5"><p hwp:id="p-5">Xin-Ming Tu is an undergraduate student at Peking university. He is interested in using machine learning to understand genomics.</p></fn><fn fn-type="others" hwp:id="fn-6"><p hwp:id="p-6">Shen Jin is a PhD candidate student at Technische Universität München. He is interested in using computational methods to understand host-microbe interactions.</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Li Jing-Yi"><surname>Li</surname><given-names>Jing-Yi</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">#</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Jin Shen"><surname>Jin</surname><given-names>Shen</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">#</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Tu Xin-Ming"><surname>Tu</surname><given-names>Xin-Ming</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Ding Yang"><surname>Ding</surname><given-names>Yang</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Gao Ge"><surname>Gao</surname><given-names>Ge</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-2" hwp:rel-id="corresp-1">*</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4"><label>1</label><institution hwp:id="institution-1">Biomedical Pioneering Innovation Center (BIOPIC) &amp; Beijing Advanced Innovation Center for Genomics (ICG), Center for Bioinformatics (CBI), and State Key Laboratory of Protein and Plant Gene Research at School of Life Sciences, Peking University</institution>, Beijing, 100871, <country>China</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Computational Biology Department, Carnegie Mellon University</institution>, Pittsburgh, Pennsylvania, 15213</aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Beijing Institute of Radiation Medicine</institution>, Beijing, 100850, <country>China</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-12-31T11:06:10-08:00">
    <day>31</day><month>12</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-05-30T05:03:33-07:00">
    <day>30</day><month>5</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-12-31T11:11:58-08:00">
    <day>31</day><month>12</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-05-30T05:08:05-07:00">
    <day>30</day><month>5</month><year>2021</year>
  </pub-date><elocation-id>508242</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-12-29"><day>29</day><month>12</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2021-05-30"><day>30</day><month>5</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-05-30"><day>30</day><month>5</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="license-1"><p hwp:id="p-7">This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></p></license></permissions><self-uri xlink:href="508242.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/508242v5.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="508242.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/508242v5/508242v5.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/508242v5/508242v5.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">ABSTRACT</title><p hwp:id="p-8">Motif identification is among the most common and essential computational tasks for bioinformatics and genomics. Here we proposed a novel convolutional layer for deep neural network, named Variable Convolutional (vConv) layer, for effective motif identification in high-throughput omics data by learning kernel length from data adaptively. Empirical evaluations on DNA-protein binding and DNase footprinting cases well demonstrated that vConv-based networks have superior performance to their convolutional counterparts regardless of model complexity. Meanwhile, vConv could be readily integrated into multi-layer neural networks as an “in-place replacement” of canonical convolutional layer. All source codes are freely available on GitHub for academic usage.</p></abstract><counts><page-count count="20"/></counts><custom-meta-wrap><custom-meta hwp:id="custom-meta-1"><meta-name>special-property</meta-name><meta-value>contains-inline-supplementary-material</meta-value></custom-meta></custom-meta-wrap><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-2">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-9">The authors have declared no competing interest.</p></notes><fn-group content-type="external-links" hwp:id="fn-group-1"><fn fn-type="dataset" hwp:id="fn-7"><p hwp:id="p-10">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/gao-lab/vConv" ext-link-type="uri" xlink:href="https://github.com/gao-lab/vConv" hwp:id="ext-link-2">https://github.com/gao-lab/vConv</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">INTRODUCTION</title><p hwp:id="p-11">Recurring sequence motifs [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>, <xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>] have been well demonstrated to exert or regulate important biological functions, such as protein binding [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>], transcription initiation [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>], alternative splicing [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>], subcellular localization [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>], translation control [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>], and microRNA targeting [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]. Effectively and efficiently identifying these motifs in massive omics data is a critical step for follow-up investigations.</p><p hwp:id="p-12">Various computational tools have been developed to identify sequence motifs via word-based and profile-based models [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>-<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]. Word-based tools start with a fixed-length and conservative segment and then perform a global scanning; such tools include DREME [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>], Fmotif [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>], RSAT peak-motifs [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>], SIOMICS [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>, <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>], and Discover [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>]. While these tools can theoretically obtain the globally optimal solution, they suffer from high computational complexity when applied to data with complex motifs or large-scale datasets [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>]. Profile-based tools attempt to find representative motifs by heuristically fine-tuning a series of possible motifs, either generated from a subset of input data or randomly chosen [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>-<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>], leading to a faster (but probably sub-optimal) motif calling [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>].</p><p hwp:id="p-13">Several convolutional neural network (CNN)-based tools have been proposed recently as a more scalable approach for identifying motifs. Alipanahi et al. developed DeepBind to identify protein binding motifs from large-scale ChIP-Seq datasets [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>] by treating each convolutional kernel as an individual motif scanner and discriminating motif-containing sequences from others based on the output of all kernels. Along with this line, several convolution-based networks have been proposed to effectively handle large amount of data in various settings [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>-<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>]. Meanwhile, however, the inherent fixed-kernel design of canonical convolutional networks could hinder effective identification [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>-<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>] of <italic toggle="yes">bona fide</italic> sequence patterns, which are usually of various lengths and unknown <italic toggle="yes">a priori</italic> and often function combinatorially [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>, <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>]. One possible workaround is to build these representations in a hierarchical manner [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>], yet the deeper structure itself would demand additional time/space cost for network training and hyper-parameter search (also see Supplementary Notes 1).</p><p hwp:id="p-14">Here, we proposed a novel convolutional layer for deep neural network called Variable Convolutional neural layer (vConv), which learns the kernel length directly from the data. Evaluations based on both simulations and real-world datasets showed that vConv-based networks outperformed canonical convolution-based networks, making it an ideal option for the <italic toggle="yes">ab initio</italic> discovery of motifs from high-throughput datasets. Further inspection also suggested the vConv layer being robust for various hyper-parameter setups. All source codes are publicly available on GitHub (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/gao-lab/vConv" ext-link-type="uri" xlink:href="https://github.com/gao-lab/vConv" hwp:id="ext-link-3">https://github.com/gao-lab/vConv</ext-link>).</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">MATERIAL AND METHODS</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Design and implementation of vConv</title><p hwp:id="p-15">Without loss of generality, we focus on nucleotide sequences where each nucleotide can take only one of four alphabets (A, C, G, and T (for DNA) or U (for RNA)). vConv (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1</xref>) is designed to be a convolutional layer equipped with a trainable “mask” to adaptively tune the effective length of the kernel during training.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10 xref-fig-1-11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Fig. 1.</label><caption hwp:id="caption-1"><p hwp:id="p-16">The design of vConv. To build up the mask matrix, vConv generates two sigmoid functions of opposite orientations (A and B), followed by overlaying them (C). Then vConv “masks” this mask matrix onto the kernel using the Hadamard product (D), and treats the masked kernel as an ordinary kernel to convolve the input sequence (E).</p></caption><graphic xlink:href="508242v5_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-17">In brief, vConv uses the trainable left and right boundary values (<inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="508242v5_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="508242v5_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>, respectively) to generate two sigmoid functions <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="508242v5_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> of opposite orientations (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1A-B</xref>), overlays them to define the final mask (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1C</xref> and <xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">Equation 5</xref>), uses this mask to mask a raw kernel (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1D</xref>), and finally uses the masked kernel to convolve sequences (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">Fig. 1E</xref>).
<disp-formula id="eqn5" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1 xref-disp-formula-1-2">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="508242v5_eqn5.gif" position="float" orientation="portrait" hwp:id="graphic-2"/></alternatives>
</disp-formula>
In practice, for a kernel with length <italic toggle="yes">L</italic>, the mask is generated as follows:</p><list list-type="order" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-18">We first generate an <italic toggle="yes">L</italic> × 4 matrix <italic toggle="yes">M</italic> where <italic toggle="yes">M</italic>[<italic toggle="yes">i, j</italic>] = <italic toggle="yes">i</italic> − 1 (the grey numbered matrix in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1A-B</xref>);</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-19">We generate the first sigmoid <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="508242v5_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> in Equation (5)) by subtracting <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="508242v5_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> from <italic toggle="yes">M</italic> followed by a sigmoid transformation, thus pushing all those <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="508242v5_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> towards 0 and all those <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="508242v5_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> towards 1 (white and black cells in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">Fig. 1A</xref>, respectively);</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-20">Similarly, we generate the second sigmoid <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="508242v5_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> in Equation (5)) by adding <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="508242v5_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula> to −1 × <italic toggle="yes">M</italic> followed by a sigmoid transformation, thus pushing all those <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="508242v5_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula> towards 1 and all those <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="508242v5_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> towards 0 (black and white cells in <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig. 1B</xref>, respectively);</p></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-21">Finally, we sum up the two sigmoid functions and shift all elements by -1 to restrict them within [0, 1] (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-9" hwp:rel-id="F1">Fig. 1C</xref>) to get the final mask.</p></list-item></list><p hwp:id="p-22">One can multiply this mask and the original kernel by the Hadamard product ∘ (i.e.,<inline-formula hwp:id="inline-formula-12"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="508242v5_inline12.gif" hwp:id="inline-graphic-12"/></alternatives></inline-formula>) to effectively mask the raw kernel with the corresponding close-to-0 elements and obtain the masked kernel (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Fig. 1D</xref>). This masked kernel can then be used as an ordinary kernel in a canonical convolutional layer (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-11" hwp:rel-id="F1">Fig. 1E</xref>). During training, the masking is essentially an operator with trainable weights (<inline-formula hwp:id="inline-formula-13"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="508242v5_inline13.gif" hwp:id="inline-graphic-13"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-14"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="508242v5_inline14.gif" hwp:id="inline-graphic-14"/></alternatives></inline-formula>) and will be automatically trained together with other weights.</p><p hwp:id="p-23">As <xref ref-type="disp-formula" rid="eqn5" hwp:id="xref-disp-formula-1-2" hwp:rel-id="disp-formula-1">Equation 5</xref> implies, all <italic toggle="yes">i</italic>’s falling outside the boundaries (i.e., <inline-formula hwp:id="inline-formula-15"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="508242v5_inline15.gif" hwp:id="inline-graphic-15"/></alternatives></inline-formula> or <inline-formula hwp:id="inline-formula-16"><alternatives hwp:id="alternatives-17"><inline-graphic xlink:href="508242v5_inline16.gif" hwp:id="inline-graphic-16"/></alternatives></inline-formula>) have their (masking) elements <inline-formula hwp:id="inline-formula-17"><alternatives hwp:id="alternatives-18"><inline-graphic xlink:href="508242v5_inline17.gif" hwp:id="inline-graphic-17"/></alternatives></inline-formula> close to zero, and all <italic toggle="yes">i</italic>’s within the boundaries have their elements close to 1; those <italic toggle="yes">i</italic>’s around the boundaries have their elements around 0.5, thus ‘soft’-masking boundary kernel elements.</p><p hwp:id="p-24">To make <inline-formula hwp:id="inline-formula-18"><alternatives hwp:id="alternatives-19"><inline-graphic xlink:href="508242v5_inline18.gif" hwp:id="inline-graphic-18"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-19"><alternatives hwp:id="alternatives-20"><inline-graphic xlink:href="508242v5_inline19.gif" hwp:id="inline-graphic-19"/></alternatives></inline-formula> converge faster during training, we combine the binary cross-entropy (BCE) loss with a sum of masked Shannon losses (MSLs) from each kernel mask. Mathematically, we have the following total loss <italic toggle="yes">L</italic>:
<disp-formula id="eqn6" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-21"><graphic xlink:href="508242v5_eqn6.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
where <inline-formula hwp:id="inline-formula-20"><alternatives hwp:id="alternatives-22"><inline-graphic xlink:href="508242v5_inline20.gif" hwp:id="inline-graphic-20"/></alternatives></inline-formula> is the sum of the Shannon entropy across all nucleotide positions of <italic toggle="yes">P</italic><sub><italic toggle="yes">z</italic></sub>, the position weight matrix (PWM) learned by the <italic toggle="yes">z</italic>-th kernel. For preciseness, we set <italic toggle="yes">P</italic> = <italic toggle="yes">P</italic>(<italic toggle="yes">w</italic><sup><italic toggle="yes">z</italic></sup>, <italic toggle="yes">b</italic> = 2) , where <inline-formula hwp:id="inline-formula-21"><alternatives hwp:id="alternatives-23"><inline-graphic xlink:href="508242v5_inline21.gif" hwp:id="inline-graphic-21"/></alternatives></inline-formula> following the exact kernel-to-PWM transformation specified by Ding et al. [<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>]. One can then immediately deduce the formula for updating the left boundary <inline-formula hwp:id="inline-formula-22"><alternatives hwp:id="alternatives-24"><inline-graphic xlink:href="508242v5_inline22.gif" hwp:id="inline-graphic-22"/></alternatives></inline-formula> at time step <italic toggle="yes">t</italic> via gradient descent with learning rate <italic toggle="yes">r</italic> (<italic toggle="yes">r</italic>&gt;0):
<disp-formula id="ueqn1" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-25"><graphic xlink:href="508242v5_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
Similarly, for updating the right boundary <inline-formula hwp:id="inline-formula-23"><alternatives hwp:id="alternatives-26"><inline-graphic xlink:href="508242v5_inline23.gif" hwp:id="inline-graphic-23"/></alternatives></inline-formula>, we have:
<disp-formula id="ueqn2" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-27"><graphic xlink:href="508242v5_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
The Shannon entropies of all kernel positions far from the mask boundaries <inline-formula hwp:id="inline-formula-24"><alternatives hwp:id="alternatives-28"><inline-graphic xlink:href="508242v5_inline24.gif" hwp:id="inline-graphic-24"/></alternatives></inline-formula> contribute little to the final derivatives. Therefore, if most boundary-flanking kernel positions have a low Shannon entropy (i.e., a high information content; defined by <italic toggle="yes">H</italic>(<italic toggle="yes">P</italic>(<italic toggle="yes">w</italic><sup><italic toggle="yes">z</italic></sup>, 2)) − <italic toggle="yes">threshold</italic> &lt; 0), then the masked Shannon loss (MSL) will help push the boundaries outwards (i.e.,<inline-formula hwp:id="inline-formula-25"><alternatives hwp:id="alternatives-29"><inline-graphic xlink:href="508242v5_inline25.gif" hwp:id="inline-graphic-25"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-26"><alternatives hwp:id="alternatives-30"><inline-graphic xlink:href="508242v5_inline26.gif" hwp:id="inline-graphic-26"/></alternatives></inline-formula> during gradient descent, just as if the current mask is too narrow to span all informative positions. Likewise, if most boundary-flanking kernel positions have a high Shannon entropy (i.e. a low information content; defined by <italic toggle="yes">H</italic>(<italic toggle="yes">P</italic>(<italic toggle="yes">w</italic><sup><italic toggle="yes">z</italic></sup>, 2))<sub><italic toggle="yes">i</italic></sub> − <italic toggle="yes">threshold</italic> &gt; 0), then the MSL will help push the boundaries inwards (i.e.,<inline-formula hwp:id="inline-formula-27"><alternatives hwp:id="alternatives-31"><inline-graphic xlink:href="508242v5_inline27.gif" hwp:id="inline-graphic-27"/></alternatives></inline-formula> and <inline-formula hwp:id="inline-formula-28"><alternatives hwp:id="alternatives-32"><inline-graphic xlink:href="508242v5_inline28.gif" hwp:id="inline-graphic-28"/></alternatives></inline-formula>), just as if the current mask is too broad to exclude some positions with low information content.</p><p hwp:id="p-25">To speed up computation in practice, we approximate the sum by ignoring most small, outside-mask kernel positions and retaining only those terms with <inline-formula hwp:id="inline-formula-29"><alternatives hwp:id="alternatives-33"><inline-graphic xlink:href="508242v5_inline29.gif" hwp:id="inline-graphic-29"/></alternatives></inline-formula>.</p><p hwp:id="p-26">Finally, when introducing a vConv layer into a model, values of <inline-formula hwp:id="inline-formula-30"><alternatives hwp:id="alternatives-34"><inline-graphic xlink:href="508242v5_inline30.gif" hwp:id="inline-graphic-30"/></alternatives></inline-formula>, and <italic toggle="yes">threshold</italic> in MSL are of user’s choice. In all subsequent benchmarking on vConv-based networks, unless otherwise specified, we set for each vConv layer the <inline-formula hwp:id="inline-formula-31"><alternatives hwp:id="alternatives-35"><inline-graphic xlink:href="508242v5_inline31.gif" hwp:id="inline-graphic-31"/></alternatives></inline-formula> as <inline-formula hwp:id="inline-formula-32"><alternatives hwp:id="alternatives-36"><inline-graphic xlink:href="508242v5_inline32.gif" hwp:id="inline-graphic-32"/></alternatives></inline-formula> for an unmasked kernel length of <italic toggle="yes">l</italic>, the <italic toggle="yes">λ</italic> as 0.0025, and the <italic toggle="yes">threshold</italic> in MSL as <inline-formula hwp:id="inline-formula-33"><alternatives hwp:id="alternatives-37"><inline-graphic xlink:href="508242v5_inline33.gif" hwp:id="inline-graphic-33"/></alternatives></inline-formula> for a kernel with <italic toggle="yes">c</italic> channels. This value of <italic toggle="yes">threshold</italic> is the Shannon entropy for the (discrete) distribution whose highest probability is 0.7, and all others <italic toggle="yes">c</italic>-1’s are equally divided. When processing the nucleic acid sequence, there are totally <italic toggle="yes">c=</italic>4 different probability values, so the probability of the remaining 3 items equally divided is (1-0.7)/3; on the other hand, in Basenji-based Basset networks, the <italic toggle="yes">c</italic> for deeper convolutional layers is not necessarily 4.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-6">Benchmark vConv-based networks on simulated sequence classification</title><p hwp:id="p-27">We simulated for each motif case 6,000 sequences (with 3,000 positive and 3,000 negative) of length 1,000, picked 600 as the test dataset randomly, and split the rest into 4,860 training and 540 validation sequences by setting “validation_split=0.1” in model.fit of Keras Model API (version 2.2.4) [<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>]; in this way, for each motif case, the same collection of training, validation, and test datasets was used for all hyper-parameter settings tested (see below). The ratio of counts of positive to negative sequences in the test dataset was within [0.8, 1.2] for all 7 motif cases. All sequences were generated independently from each other. Each negative sequence is a random sequence whose bases were independently sampled from the categorical distribution P(A)=P(C)=P(G)=P(T)=0.25. For each positive sequence, we first constructed a random sequence as described above, then overwrote/replaced a random-choice fragment with a sequence fragment generated from one of the motifs associated with the case in question (“signal fragment”, see <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref> for more details). For the cases of 2, 4, 6 and 8 motifs, new motifs were introduced incrementally (i.e., all motifs in “2 motifs” were also included in “4 motifs”, all motifs in “4 motifs” were also included in “6 motifs”, and all motifs in “6 motifs” were also included in “8 motifs”).</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-2"><p hwp:id="p-28">Motifs used to generate each dataset. All motifs were derived from JASPAR [<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>].</p></caption><graphic xlink:href="508242v5_tbl1" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap><p hwp:id="p-29">In each iteration of benchmarking a certain case, we (1) picked a hyper-parameter setting from <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>; (2) used the training and validation datasets to train first a vConv-based network (see Supplementary Fig. 1 for its structure) with this hyper-parameter setting and then a canonical convolution-based network with a model structure identical to that of the vConv-based network, except that the vConv layer was replaced with a canonical convolutional layer with the same kernel number and (unmasked) kernel length; and (3) obtained the AUROC values of these two networks on the test dataset. Iterating over all possible hyper-parameter settings yielded a series of AUROC values for both the vConv-based network and the convolution-based network for the case at hand.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-3"><p hwp:id="p-30">Hyper-parameter space for the comparison between vConv-based networks and convolution-based networks on simulation datasets.</p></caption><graphic xlink:href="508242v5_tbl2" position="float" orientation="portrait" hwp:id="graphic-7"/></table-wrap><p hwp:id="p-31">We used AdaDelta [<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>] with learning rate 1 as the optimizer. All parameters except for vConv-exclusive ones were initialized by the Glorot uniform initializer [<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>]; vConv-exclusive parameters were initialized as specified in the subsection “Design and implementation of vConv”. Both convolution-based and vConv-based networks were trained with early stopping on validation loss with patience set to 50. All vConv-based networks with <italic toggle="yes">n</italic> kernels except for vConv-based Basset networks were first trained without updating <inline-formula hwp:id="inline-formula-34"><alternatives hwp:id="alternatives-38"><inline-graphic xlink:href="508242v5_inline34.gif" hwp:id="inline-graphic-34"/></alternatives></inline-formula> for 10 epochs and then trained with updating <inline-formula hwp:id="inline-formula-35"><alternatives hwp:id="alternatives-39"><inline-graphic xlink:href="508242v5_inline35.gif" hwp:id="inline-graphic-35"/></alternatives></inline-formula> for vConv-based Basset networks, we updated <inline-formula hwp:id="inline-formula-36"><alternatives hwp:id="alternatives-40"><inline-graphic xlink:href="508242v5_inline36.gif" hwp:id="inline-graphic-36"/></alternatives></inline-formula> since the first epoch.</p></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-7">Benchmark vConv-based networks on real-world sequence classification</title><p hwp:id="p-32">To further demonstrate the performance of vConv-based networks in the real world, we examined whether replacing convolutional layers with vConv helps to improve the performance for motif identification in real-world ChIP-Seq data, against three published convolution-based networks: DeepBind [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">26</xref>], Zeng et al.’s convolution-based networks [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>], and Basset [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>].</p><p hwp:id="p-33">For the first comparison, we downloaded DeepBind’s original training (with positively sequences only) and test datasets from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/jisraeli/DeepBind/tree/master/data/encode" ext-link-type="uri" xlink:href="https://github.com/jisraeli/DeepBind/tree/master/data/encode" hwp:id="ext-link-4">https://github.com/jisraeli/DeepBind/tree/master/data/encode</ext-link>, and generated negative training dataset by shuffling positive sequences with matching dinucleotide composition as specified by DeepBind [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-3" hwp:rel-id="ref-26">26</xref>]. We then trained vConv-based networks (see below for the model structure and training details) and compared their AUROC on test datasets with those of three sets of DeepBind performances from Supplementary Table 5 of DeepBind’s original paper [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-4" hwp:rel-id="ref-26">26</xref>]: (1) the DeepBind* networks, which were trained by using all the training dataset; (2) the DeepBind networks, whose positive training sequences consist of those from the top 500 odd-numbered peaks only; and (3) the DeepBindbest networks, the one of DeepBind and DeepBind* with larger AUROC for each test dataset. During comparison, we found that there are two datasets with different AUROC values yet sharing the same model name in their original dataset (REST_HepG2_NRSF_HudsonAlpha and REST_SK-N-SH_NRSF_HudsonAlpha), which makes us unable to determine their true AUROC values, so we excluded them from the comparison.</p><p hwp:id="p-34">For the second comparison between vConv-based networks and Zeng et al.’s [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">45</xref>] convolution-based networks, we downloaded 690 ENCODE ChIP-Seq-based training and test datasets representing the DNA binding profile of various transcription factors and other DNA-binding proteins from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://cnn.csail.mit.edu/motif_discovery/" ext-link-type="uri" xlink:href="http://cnn.csail.mit.edu/motif_discovery/" hwp:id="ext-link-5">http://cnn.csail.mit.edu/motif_discovery/</ext-link>. We then trained vConv-based networks (see below for the model structure and training details) and compared its AUROC value on test datasets with the AUROC values of Zeng et al.’s networks from <ext-link l:rel="related" l:ref-type="uri" l:ref="http://cnn.csail.mit.edu/motif_discovery_pred/" ext-link-type="uri" xlink:href="http://cnn.csail.mit.edu/motif_discovery_pred/" hwp:id="ext-link-6">http://cnn.csail.mit.edu/motif_discovery_pred/</ext-link>.</p><p hwp:id="p-35">The first two comparisons used the same model structure (Supplementary Fig. 1) and hyper-parameter space (Supplementary Table 1). For direct comparison between vConv-based networks and convolution-based networks for DeepBind and Zeng et al.’s cases [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">45</xref>], the vConv-based network was implemented by replacing the convolutional layer of each network considered for comparison with a vConv layer (with exactly the same hyper-parameters for the number of kernels and the initial (unmasked) kernel length). The hyper-parameter space is listed in the Supplementary Table 1. The remaining details of parameter initialization followed those of the corresponding convolution-based networks [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-5" hwp:rel-id="ref-26">26</xref>, <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-4" hwp:rel-id="ref-45">45</xref>], and all these networks used the training strategy described in the simulation case.</p><p hwp:id="p-36">Finally, for the third comparison with the Basset network [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">46</xref>], we used the Basenji reimplementation of Basset [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-2" hwp:rel-id="ref-28">28</xref>] (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/calico/basenji/tree/master/manuscripts/basset" ext-link-type="uri" xlink:href="https://github.com/calico/basenji/tree/master/manuscripts/basset" hwp:id="ext-link-7">https://github.com/calico/basenji/tree/master/manuscripts/basset</ext-link>) made and recommended by the original author of Basset (Kelley D., personal communications); this reimplementation is a deep CNN with nine convolutional layers (see Supplementary Fig. 2 for the full details). We (1) generated the datasets by running “make_data.sh“ under “manuscripts/basset/” (of this repository), (2) re-trained the original Basset networks (which makes prediction for 164 cell types by a single model) by running “python basenji_train.py -k -o train_basset params_basset.json data_basset” under “manuscripts/basset/” (of this repository) to obtain their AUROC values, (3) replaced either the first (‘Single vConv-based’; left figure in Supplementary Fig. 3) or all nine (‘Completed vConv-based’; right figure in Supplementary Fig. 3) convolutional layers in the Basset network with vConv layer(s), re-initialized all weights, and retrained it to obtain the AUROC value of the vConv-based Basset network, and finally (4) compared the AUROC values between the two types of networks. The details of parameter initialization (except for the vConv-exclusive parameters) and stochastic gradient descent-based optimization (with learning rate 0.005) were exactly the same between the vConv-based and the original Basset networks (Supplementary Fig. 2 and 3).</p></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-8">Benchmark vConv-based networks on real-world motif discovery</title><p hwp:id="p-37">Finally, we compared vConv-based networks with the canonical methods on the motif discovery problem. We followed the protocol proposed by Zhang et al. (Zhang, et al., 2015) to assess the accuracy of the extracted representative motifs based on the ENCODE CTCF ChIP-Seq datasets [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>]. In brief, for each motif discovery tool and each ChIP-Seq dataset, we located the motif-containing sequence fragments for candidate motifs discovered by this tool, checked whether they overlapped with the ChIP-Seq peaks with CisFinder (retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://lgsun.grc.nia.nih.gov/CisFinder/download.html" ext-link-type="uri" xlink:href="https://lgsun.grc.nia.nih.gov/CisFinder/download.html" hwp:id="ext-link-8">https://lgsun.grc.nia.nih.gov/CisFinder/download.html</ext-link> at Feb 22, 2020), and reported the largest per-motif ratio of overlapping fragments to all fragments across all candidate motifs as the final accuracy of this tool on this dataset (see Supplementary Fig. 4 for more details). For the sake of demonstrating the advantage of vConv-based networks, here the model structure of the vConv-based network was chosen to be the one used in simulation (Supplementary Fig. 1).</p></sec></sec><sec id="s3" hwp:id="sec-7"><title hwp:id="title-9">RESULTS</title><sec id="s3a" hwp:id="sec-8"><title hwp:id="title-10">vConv-based networks identify motifs more effectively than convolution-based networks</title><p hwp:id="p-38">We first present direct comparisons between vConv-based and convolution-based networks based on multiple simulated datasets (see the Methods for more details). vConv-based networks performed statistically significantly better across most hyperparameter settings of all seven cases (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2</xref>; with all Wilcoxon rank sum test’s p-values &lt; 1e-18 for the null hypothesis that the AUROC of the vConv-based network is equal to or smaller than that of the convolution-based network). This improvement became larger when more motifs are introduced (2 motifs to 8 motifs in Supplementary Fig. 5) and with increased heterogeneity of the motif length (2 motifs v.s. TwoDiffMotif1/2/3 in Supplementary Fig. 5). In addition, vConv-based networks showed a smaller mean standard error of the AUROC than convolution-based networks across different hyper-parameter settings (Levene’s test, p-value &lt; 0.001 for all datasets; see Supplementary Table 2), suggesting that vConv-based networks also have a better robustness to hyper-parameters than convolution-based networks.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Fig. 2.</label><caption hwp:id="caption-4"><p hwp:id="p-39">Pairwise comparison of AUROC between vConv-based and convolution-based networks with exactly the same hyper-parameter. As expected, vConv-based networks performed better than convolution-based networks in most cases. The percentage in the title on the figure indicates the percentage of those cases where the vConv-based network was better than the convolution-based network.</p></caption><graphic xlink:href="508242v5_fig2" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-40">We further visualized the first layer of trained vConv-based and convolution-based networks on the most complex dataset (8 motifs), and found that vConv-based networks accurately recovered the underlying real motif from sequences and converge to the real length well (Supplementary Fig. 6). Of note, we found that the vConv-based networks successfully learned a motif (MA0234.1) which was missed by canonical convolution-based networks (Supplementary Fig. 6, second row).</p><p hwp:id="p-41">These findings further compelled us to suspect that vConv-based networks will perform better than convolution-based networks on real-world cases possibly with combinatorial regulation.</p><p hwp:id="p-42">To test this hypothesis, we examined whether replacing convolutional layer(s) with vConv would improve performance of the following convolution-based networks: (1) DeepBind [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-6" hwp:rel-id="ref-26">26</xref>], a series of DNA-protein-binding classifiers trained for each of 504 ENCODE ChIP-Seq datasets; (2) Zeng et al.’s convolution-based networks [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-5" hwp:rel-id="ref-45">45</xref>], a series of structurally optimized variants of DeepBind models for each of 690 ChIP-Seq datasets; and (3) Basset [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-3" hwp:rel-id="ref-46">46</xref>], a complex DNase footprint predictor with nine convolutional layers (see Methods for more details). As expected, vConv-based networks showed a statistically significantly improved performance compared to their convolution-based network counterparts for all these three cases (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3</xref> and Supplementary Fig. 7), with a few exceptions some of which can be attributed to small sample size of the input dataset (Supplementary Fig. 8A-B) and disappeared upon additional grid-search on kernel length (Supplementary Fig. 8C). Of note, stacking vConv layers for a complex convolution-based network like Basset can statistically significantly improve the performance with respect to both the original Basset network and the single vConv-based network (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3C</xref>), well demonstrating its combined power with the popular hierarchical representation learning strategy in CNN application.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Fig. 3.</label><caption hwp:id="caption-5"><p hwp:id="p-43">vConv-based networks outperformed convolution-based networks on real-world cases. (A), comparison of AUROC between vConv-based networks and DeepBindbest networks for DeepBind (see also Supplementary Fig. 7 for comparison with other DeepBind networks). (B), comparison of AUROC between vConv-based networks and networks from Zeng et al. . (C), comparison of AUROC between vConv-based networks with ‘Completed vConv-based’, the vConv-based network with all convolutional layers of Basset network replaced by vConv (right figure in Supplementary Fig. 3; see Methods for more details) and networks from Basset. All p-values shown are from Wilcoxon rank sum test, single-tailed, with the null hypothesis that the AUROC of the vConv-based network is equal to or smaller than that of the convolution-based network. See Methods for more details of each specific network.</p></caption><graphic xlink:href="508242v5_fig3" position="float" orientation="portrait" hwp:id="graphic-9"/></fig></sec><sec id="s3b" hwp:id="sec-9"><title hwp:id="title-11">vConv-based networks discover motifs from real-world sequences more accurately and faster than canonical tools</title><p hwp:id="p-44">We further evaluated vConv-based networks’ performance for <italic toggle="yes">ab initio</italic> motif discovery. In brief, for a particular trained vConv-based network, we first selected kernels with corresponding dense layer weights higher than a predetermined baseline (defined as mean (all dense layer weights) - standard deviation (all dense layer weights)) and then extracted and aligned these kernels’ corresponding segments to compute the representative PWM (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4A</xref>). We then compared the accuracy of recovering ChIP-Seq peaks by the vConv-based motif discovery and other motif discovery tools across all these ChIP-Seq datasets [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">47</xref>] (see the Methods for details).</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;508242v5/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Fig. 4.</label><caption hwp:id="caption-6"><p hwp:id="p-45">vConv-based networks discover motifs more accurately and faster. (A) shows the process of calling a representative motif from a trained vConv-based network. (B) shows the difference in accuracy, defined as the accuracy of vConv-based motif discovery minus the accuracy of the motif discovery algorithm shown on the x-axis on the same dataset. MEME [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>] failed to complete the discovery within a reasonable time (∼50% datasets remained unfinished even after running for 1.5 weeks with 2,000 cores, amounting to 504,000 CPU hours) for these datasets, and its results were thus not listed here. See also Supplementary Fig. 9 for other metrics. (C) shows the time cost of each motif discovery algorithm as a function of millions of base pairs in the dataset tested.</p></caption><graphic xlink:href="508242v5_fig4" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-46">Out of all 100 CTCF datasets, the vConv-based network outperformed DREME [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>] on 95 datasets, CisFinder [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>] on 91 datasets and MEME-ChIP [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>] on 87 datasets with respect to AUROC (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4B</xref>; see also Supplementary Fig. 9 for other metrics). In addition, the vConv-based network ran much faster than DREME and MEME-ChIP on large datasets (more than 17 million base pairs finished in less than 30 minutes compared to hours by DREME and MEME-ChIP’s; see <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4C</xref>).</p></sec></sec><sec id="s4" hwp:id="sec-10"><title hwp:id="title-12">DISCUSSION</title><p hwp:id="p-47">Inspired by the theoretical model (Supplementary Notes 1) for analyzing the influence of the kernel size in the convolutional layer, we designed and implemented a novel convolutional layer, vConv, which adaptively tunes the kernel lengths at run time. Evaluations based on both simulations and real-world datasets showed that such design enables vConv-based networks to outperform canonical convolution-based network (see <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3</xref> and <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">4</xref>, as well as Supplementary Fig. 5-8 for more details), especially on datasets containing multiple combinatorially regulatory <italic toggle="yes">cis</italic>-elements of various lengths [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-2" hwp:rel-id="ref-38">38</xref>, <xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>]. Meanwhile, vConv could be readily integrated into multi-layer neural networks, as an “in-place replacement” of canonical convolutional layer with potential applications in more scenarios such as <italic toggle="yes">cis</italic>-regulatory motif prediction [<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>], predicting non-coding functions <italic toggle="yes">de novo</italic> [<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>], predicting the transcription factors binding intensities [<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>], and quantitative detection of histone modifications [<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]. To facilitate its application in various fields, we have implemented vConv as a new type of convolutional layer in Keras (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/gao-lab/vConv" ext-link-type="uri" xlink:href="https://github.com/gao-lab/vConv" hwp:id="ext-link-9">https://github.com/gao-lab/vConv</ext-link>).</p><p hwp:id="p-48">In theory, a regular fixed-length convolution kernel with size larger than that of the <italic toggle="yes">bona fide</italic> motif, may be able to converge to a solution where the kernel captures the motif faithfully with zero padding everywhere else, just as what the vConv kernel does. Nevertheless, preliminary empirical evaluations suggest that such “perfect” solution could be very rare in reality with a general training strategy: none of the convolution-based network kernels similar to the ground truth motifs in the 8 motifs case (as determined by Tomtom; see Supplementary Fig. 6) contains more than 1 position whose sum of absolute elements are one order of magnitude more closer to 0 than other positions.</p><p hwp:id="p-49">vConv is scalable considering the fact that its current design only adds a <italic toggle="yes">O(ncl)</italic> per <italic toggle="yes">n</italic>-kernel-convolutional layer (with kernel length <italic toggle="yes">l</italic> and number of channels <italic toggle="yes">c</italic>) replaced to the overall time complexity compared with that of the canonical fixed-kernel convolutional layer, and that preliminary empirical evaluations suggest no evidence of convergence slowdown (as benchmarked in Supplementary Fig. 10 for the simulation case). While we notice that the current variable-length kernel strategy implemented by vConv could, in theory, be approximated by combing multiple fixed-length-kernel networks, we’d argue that such approximation would lead to serious inflation for the number of trainable parameters, and higher time cost for training and optimization. For example, to approximate a simple network with one vConv layer of <italic toggle="yes">n</italic> unmasked kernels with average length <italic toggle="yes">l</italic> (which needs <italic toggle="yes">n</italic>*(4*<italic toggle="yes">l</italic>+2)+<italic toggle="yes">n</italic> = <italic toggle="yes">n</italic>*(4*<italic toggle="yes">l</italic>+3) parameters), the ensemble of <italic toggle="yes">m</italic> convolution-based networks would take <italic toggle="yes">m</italic>*(<italic toggle="yes">n</italic>*(4*<italic toggle="yes">l</italic>)+<italic toggle="yes">n</italic>) = <italic toggle="yes">m</italic>*(<italic toggle="yes">n</italic>*(4*<italic toggle="yes">l</italic>+1)) parameters, far larger than <italic toggle="yes">n</italic>*(4*<italic toggle="yes">l</italic>+3) when <italic toggle="yes">m</italic> is of tens or hundreds - a popular choice in ensemble learning.</p><p hwp:id="p-50">vConv-based networks show robustness with various hyper-parameters and initialization setups (Supplementary Table 2). We believe that the major vConv-exclusive components, Shannon loss (MSL) and boundary parameters, contribute to the increased robustness. Intuitively, such components could regularize the parameters to learn, making the overall objective function simpler (i.e., having fewer suboptima) and thus more robust. Consistently, we found out that the vConv-based network -- when equipped with MSL -- had a smaller variance of AUROC that is statistically significantly different from that of the vConv-based network without MSL in 3 out of all 7 simulation cases (Levene’s test, p&lt;0.001; Supplementary Table 3), suggesting that MSL does contribute to this increased robustness in certain conditions. Of interest, equipping MSL also improved the AUROC of the vConv-based network (Supplementary Fig. 11).</p></sec><sec id="s5" hwp:id="sec-11"><title hwp:id="title-13">FURTHER WORK</title><p hwp:id="p-51">We note that the current theoretical analysis (see details in Supplementary Notes 1) reported above relies on prior knowledge of the real motif ℳ, which may not be feasible to obtain for real-world datasets. A possible workaround is to estimate the empirical null distribution of P<sub><italic toggle="yes">real</italic></sub> over a set of randomly initialized PWMs and kernels and then derive the expectation of P<sub><italic toggle="yes">real</italic></sub> over the given dataset. Such theoretical distribution, once established, could serve as a general background distribution for further development of motif scanners.</p><p hwp:id="p-52">Moreover, although current motifs in convolution-based networks are automatically represented by PWMs, this might be an oversimplified representation of the genuine motifs, which may allow insertions and deletions within motifs (e.g., the HMM motifs from Pfam [<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>] and Rfam [<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">54</xref>]). While recurrent neural networks have been expected to be able to learn such motifs [<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref>-<xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>], the interpretation of such models is still challenging. A promising alternative would be to use the CNN framework to learn complex motifs directly, as demonstrated by a recently developed CNN model HOCNNLB [<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref>] which can learn first-order Markov models by re-encoding input sequences. Combining the variable-length nature of vConv and such complex-motif-capturing models might be much more useful for mining biological motifs.</p></sec><sec sec-type="supplementary-material" hwp:id="sec-12"><title hwp:id="title-14">Supporting information</title><supplementary-material position="float" orientation="portrait" hwp:id="DC1"><object-id pub-id-type="other" hwp:sub-type="slug">DC1</object-id><label>supplementary files</label><media xlink:href="supplements/508242_file03.zip" position="float" orientation="portrait" hwp:id="media-1"/></supplementary-material></sec></body><back><sec id="s6" hwp:id="sec-13"><title hwp:id="title-15">DATA AVAILABILITY</title><p hwp:id="p-53">A Keras-based implementation is released at GitHub (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/gao-lab/vConv" ext-link-type="uri" xlink:href="https://github.com/gao-lab/vConv" hwp:id="ext-link-10">https://github.com/gao-lab/vConv</ext-link>) for academic usage, with all scripts/data for reproducing results available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/gao-lab/vConv-Figures_and_Tables" ext-link-type="uri" xlink:href="https://github.com/gao-lab/vConv-Figures_and_Tables" hwp:id="ext-link-11">https://github.com/gao-lab/vConv-Figures_and_Tables</ext-link>.</p></sec><ack hwp:id="ack-1"><title hwp:id="title-16">ACKNOWLEDGEMENT</title><p hwp:id="p-54">The authors would like to thank Drs. Cheng Li, Letian Tao, Minghua Deng, Zemin Zhang, Jian Lu and Liping Wei at Peking University for their helpful comments and suggestions during the study. The analysis was supported by the High-performance Computing Platform of Peking University, and we thank Dr. Chun Fan and Yin-Ping Ma for their assistance during the analysis.</p></ack><sec id="s7" hwp:id="sec-14"><title hwp:id="title-17">KEY POINTS</title><list list-type="bullet" hwp:id="list-2"><list-item hwp:id="list-item-5"><p hwp:id="p-55">We proposed a novel convolutional layer, vConv, which learns kernel structure from data adaptively.</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-56">vConv could be readily integrated into multi-layer neural networks, as an “in-place replacement” of canonical convolutional layer.</p></list-item><list-item hwp:id="list-item-7"><p hwp:id="p-57">vConv’s high robustness and low time footprint further enable its usage for real-world massive omics data.</p></list-item></list></sec><sec id="s8" hwp:id="sec-15"><title hwp:id="title-18">FUNDING</title><p hwp:id="p-58">This work was supported by funds from the National Key Research and Development Program [grant number 2016YFC0901603]; the China 863 Program [grant number 2015AA020108]; the State Key Laboratory of Protein and Plant Gene Research; the Beijing Advanced Innovation Center for Genomics (ICG) at Peking University; and in part by the National Program for the Support of Top-notch Young Professionals [to G.G.].</p></sec><sec id="s9" hwp:id="sec-16"><title hwp:id="title-19">Conflict of interest statement</title><p hwp:id="p-59">None declared.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-20">REFERENCES</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Achar A"><surname>Achar</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sætrom P."><surname>Sætrom</surname> <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-2">RNA motif discovery: a computational overview</article-title>, <source hwp:id="source-1">Biology direct</source> <year>2015</year>;<volume>10</volume>:<fpage>61</fpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>2.</label><citation publication-type="book" citation-type="book" ref:id="508242v5.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Kulakovskiy IV"><surname>Kulakovskiy</surname> <given-names>IV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Makeev VJ"><surname>Makeev</surname> <given-names>VJ</given-names></string-name>. <chapter-title>DNA sequence motif: a jack of all trades for ChIP-Seq data</chapter-title>. <source hwp:id="source-2">Advances in protein chemistry and structural biology</source>. <publisher-name>Elsevier</publisher-name>, <year>2013</year>, <fpage>135</fpage>–<lpage>171</lpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Stormo GD"><surname>Stormo</surname> <given-names>GD</given-names></string-name>. <article-title hwp:id="article-title-3">DNA motif databases and their uses</article-title>, <source hwp:id="source-3">Current protocols in bioinformatics</source> <year>2015</year>;<volume>51</volume>:2.15. 11-12.15. 16.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Kadonaga JT"><surname>Kadonaga</surname> <given-names>JT</given-names></string-name>. <article-title hwp:id="article-title-4">Perspectives on the RNA polymerase II core promoter</article-title>, <source hwp:id="source-4">Wiley Interdisciplinary Reviews: Developmental Biology</source> <year>2012</year>;<volume>1</volume>:<fpage>40</fpage>–<lpage>51</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Blencowe BJ"><surname>Blencowe</surname> <given-names>BJ</given-names></string-name>. <article-title hwp:id="article-title-5">Exonic splicing enhancers: mechanism of action, diversity and role in human genetic diseases</article-title>, <source hwp:id="source-5">Trends in biochemical sciences</source> <year>2000</year>;<volume>25</volume>:<fpage>106</fpage>–<lpage>110</lpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Zhang B"><surname>Zhang</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gunawardane L"><surname>Gunawardane</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Niazi F"><surname>Niazi</surname> <given-names>F</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-6">A novel RNA motif mediates the strict nuclear localization of a long noncoding RNA</article-title>, <source hwp:id="source-6">Molecular and cellular biology</source> <year>2014</year>;<volume>34</volume>:<fpage>2318</fpage>–<lpage>2329</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Zucchelli S"><surname>Zucchelli</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cotella D"><surname>Cotella</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Takahashi H"><surname>Takahashi</surname> <given-names>H</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-7">SINEUPs: A new class of natural and synthetic antisense long non-coding RNAs that activate translation</article-title>, <source hwp:id="source-7">RNA biology</source> <year>2015</year>;<volume>12</volume>:<fpage>771</fpage>–<lpage>779</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Thomson DW"><surname>Thomson</surname> <given-names>DW</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dinger ME"><surname>Dinger</surname> <given-names>ME</given-names></string-name>. <article-title hwp:id="article-title-8">Endogenous microRNA sponges: evidence and controversy</article-title>, <source hwp:id="source-8">Nature Reviews Genetics</source> <year>2016</year>;<volume>17</volume>:<fpage>272</fpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Liu B"><surname>Liu</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yang J"><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Y"><surname>Li</surname> <given-names>Y</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-9">An algorithmic perspective of de novo cis-regulatory motif finding based on ChIP-seq data</article-title>, <source hwp:id="source-9">Brief Bioinform</source> <year>2018</year>;<volume>19</volume>:<fpage>1069</fpage>–<lpage>1081</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><label>10.</label><citation publication-type="book" citation-type="book" ref:id="508242v5.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Das MK"><surname>Das</surname> <given-names>MK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dai H-K."><surname>Dai</surname> <given-names>H-K.</given-names></string-name> <chapter-title>A survey of DNA motif finding algorithms</chapter-title>. <source hwp:id="source-10">In: BMC bioinformatics</source>. <year>2007</year>, p. <fpage>S21</fpage>. <publisher-name>Springer</publisher-name>.</citation></ref><ref id="c11" hwp:id="ref-11"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Tran NT"><surname>Tran</surname> <given-names>NT</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang CH"><surname>Huang</surname> <given-names>CH</given-names></string-name>. <article-title hwp:id="article-title-10">A survey of motif finding Web tools for detecting binding site motifs in ChIP-Seq data</article-title>, <source hwp:id="source-11">Biol Direct</source> <year>2014</year>;<volume>9</volume>:<fpage>4</fpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Lihu A"><surname>Lihu</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holban S."><surname>Holban</surname> <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-11">A review of ensemble methods for de novo motif discovery in ChIP-Seq data</article-title>, <source hwp:id="source-12">Brief Bioinform</source> <year>2015</year>;<volume>16</volume>:<fpage>964</fpage>–<lpage>973</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Zambelli F"><surname>Zambelli</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pesole G"><surname>Pesole</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pavesi G."><surname>Pavesi</surname> <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-12">Motif discovery and transcription factor binding sites before and after the next-generation sequencing era</article-title>, <source hwp:id="source-13">Briefings in bioinformatics</source> <year>2013</year>;<volume>14</volume>:<fpage>225</fpage>–<lpage>237</lpage>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>14.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Caldonazzo Garbelini JM"><surname>Caldonazzo Garbelini</surname> <given-names>JM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kashiwabara AY"><surname>Kashiwabara</surname> <given-names>AY</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sanches DS"><surname>Sanches</surname> <given-names>DS</given-names></string-name>. <article-title hwp:id="article-title-13">Sequence motif finder using memetic algorithm</article-title>, <source hwp:id="source-14">BMC bioinformatics</source> <year>2018</year>;<volume>19</volume>:<fpage>4</fpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Bailey TL"><surname>Bailey</surname> <given-names>TL</given-names></string-name>. <article-title hwp:id="article-title-14">DREME: motif discovery in transcription factor ChIP-seq data</article-title>, <source hwp:id="source-15">Bioinformatics</source> <year>2011</year>;<volume>27</volume>:<fpage>1653</fpage>–<lpage>1659</lpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>16.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Jia C"><surname>Jia</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carson MB"><surname>Carson</surname> <given-names>MB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang Y"><surname>Wang</surname> <given-names>Y</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-15">A new exhaustive method and strategy for finding motifs in ChIP-enriched regions</article-title>, <source hwp:id="source-16">PLoS one</source> <year>2014</year>;<volume>9</volume>:<fpage>e86044</fpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>17.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Thomas-Chollier M"><surname>Thomas-Chollier</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Herrmann C"><surname>Herrmann</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Defrance M"><surname>Defrance</surname> <given-names>M</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-16">RSAT peak-motifs: motif analysis in full-size ChIP-seq datasets</article-title>, <source hwp:id="source-17">Nucleic Acids Res</source> <year>2012</year>;<volume>40</volume>:<fpage>e31</fpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>18.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Ding J"><surname>Ding</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hu H"><surname>Hu</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li X."><surname>Li</surname> <given-names>X.</given-names></string-name> <article-title hwp:id="article-title-17">SIOMICS: a novel approach for systematic identification of motifs in ChIP-seq data</article-title>, <source hwp:id="source-18">Nucleic Acids Res</source> <year>2014</year>;<volume>42</volume>:<fpage>e35</fpage>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><label>19.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Ding J"><surname>Ding</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dhillon V"><surname>Dhillon</surname> <given-names>V</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li X"><surname>Li</surname> <given-names>X</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-18">Systematic discovery of cofactor motifs from ChIP-seq data by SIOMICS</article-title>, <source hwp:id="source-19">Methods</source> <year>2015</year>;<issue>79-80</issue>:<fpage>47</fpage>–<lpage>51</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Maaskola J"><surname>Maaskola</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rajewsky N."><surname>Rajewsky</surname> <given-names>N.</given-names></string-name> <article-title hwp:id="article-title-19">Binding site discovery from nucleic acid sequences by discriminative learning of hidden Markov models</article-title>, <source hwp:id="source-20">Nucleic Acids Res</source> <year>2014</year>;<volume>42</volume>:<fpage>12995</fpage>–<lpage>13011</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Kulakovskiy IV"><surname>Kulakovskiy</surname> <given-names>IV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boeva VA"><surname>Boeva</surname> <given-names>VA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Favorov AV"><surname>Favorov</surname> <given-names>AV</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-20">Deep and wide digging for binding motifs in ChIP-Seq data</article-title>, <source hwp:id="source-21">Bioinformatics</source> <year>2010</year>;<volume>26</volume>:<fpage>2622</fpage>–<lpage>2623</lpage>.</citation></ref><ref id="c22" hwp:id="ref-22"><label>22.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Ikebata H"><surname>Ikebata</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yoshida R."><surname>Yoshida</surname> <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-21">Repulsive parallel MCMC algorithm for discovering diverse motifs from large sequence sets</article-title>, <source hwp:id="source-22">Bioinformatics</source> <year>2015</year>;<volume>31</volume>:<fpage>1561</fpage>–<lpage>1568</lpage>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Sharov AA"><surname>Sharov</surname> <given-names>AA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ko MS"><surname>Ko</surname> <given-names>MS</given-names></string-name>. <article-title hwp:id="article-title-22">Exhaustive search for over-represented DNA sequence motifs with CisFinder</article-title>, <source hwp:id="source-23">DNA Res</source> <year>2009</year>;<volume>16</volume>:<fpage>261</fpage>–<lpage>273</lpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Bailey TL"><surname>Bailey</surname> <given-names>TL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Williams N"><surname>Williams</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Misleh C"><surname>Misleh</surname> <given-names>C</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-23">MEME: discovering and analyzing DNA and protein sequence motifs</article-title>, <source hwp:id="source-24">Nucleic Acids Res</source> <year>2006</year>;<volume>34</volume>:<fpage>W369</fpage>–<lpage>373</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><label>25.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Machanick P"><surname>Machanick</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bailey TL"><surname>Bailey</surname> <given-names>TL</given-names></string-name>. <article-title hwp:id="article-title-24">MEME-ChIP: motif analysis of large DNA datasets</article-title>, <source hwp:id="source-25">Bioinformatics</source> <year>2011</year>;<volume>27</volume>:<fpage>1696</fpage>–<lpage>1697</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2 xref-ref-26-3 xref-ref-26-4 xref-ref-26-5 xref-ref-26-6"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Alipanahi B"><surname>Alipanahi</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Delong A"><surname>Delong</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weirauch MT"><surname>Weirauch</surname> <given-names>MT</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-25">Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning</article-title>, <source hwp:id="source-26">Nature biotechnology</source> <year>2015</year>;<volume>33</volume>:<fpage>831</fpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>27.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Angermueller C"><surname>Angermueller</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee HJ"><surname>Lee</surname> <given-names>HJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reik W"><surname>Reik</surname> <given-names>W</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-26">DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning</article-title>, <source hwp:id="source-27">Genome biology</source> <year>2017</year>;<volume>18</volume>:<fpage>67</fpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1 xref-ref-28-2"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Kelley DR"><surname>Kelley</surname> <given-names>DR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reshef YA"><surname>Reshef</surname> <given-names>YA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bileschi M"><surname>Bileschi</surname> <given-names>M</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-27">Sequential regulatory activity prediction across chromosomes with convolutional neural networks</article-title>, <source hwp:id="source-28">Genome research</source> <year>2018</year>;<volume>28</volume>:<fpage>739</fpage>–<lpage>750</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Wang M"><surname>Wang</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tai C E W"><surname>Tai</surname> <given-names>C E W</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-28">DeFine: deep convolutional neural networks accurately quantify intensities of transcription factor-DNA binding and facilitate evaluation of functional non-coding variants</article-title>, <source hwp:id="source-29">Nucleic acids research</source> <year>2018</year>;<volume>46</volume>:<fpage>e69</fpage>–<lpage>e69</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Zhang J"><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peng W"><surname>Peng</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang L."><surname>Wang</surname> <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-29">LeNup: learning nucleosome positioning from DNA sequences with improved convolutional neural networks</article-title>, <source hwp:id="source-30">Bioinformatics</source> <year>2018</year>;<volume>34</volume>:<fpage>1705</fpage>–<lpage>1712</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Zhou J"><surname>Zhou</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Theesfeld CL"><surname>Theesfeld</surname> <given-names>CL</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yao K"><surname>Yao</surname> <given-names>K</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-30">Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk</article-title>, <source hwp:id="source-31">Nature genetics</source> <year>2018</year>;<volume>50</volume>:<fpage>1171</fpage>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Koo PK"><surname>Koo</surname> <given-names>PK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eddy SR"><surname>Eddy</surname> <given-names>SR</given-names></string-name>. <article-title hwp:id="article-title-31">Representation learning of genomic sequence motifs with convolutional neural networks</article-title>, <source hwp:id="source-32">PLoS computational biology</source> <year>2019</year>;<volume>15</volume>:<fpage>e1007560</fpage>.</citation></ref><ref id="c33" hwp:id="ref-33"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Lan G"><surname>Lan</surname> <given-names>G</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhou J"><surname>Zhou</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xu R"><surname>Xu</surname> <given-names>R</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-32">Cross-Cell-Type Prediction of TF-Binding Site by Integrating Convolutional Neural Network and Adversarial Network</article-title>, <source hwp:id="source-33">Int J Mol Sci</source> <year>2019</year>;<volume>20</volume>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Zhang Q"><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shen Z"><surname>Shen</surname> <given-names>Z</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang DS"><surname>Huang</surname> <given-names>DS</given-names></string-name>. <article-title hwp:id="article-title-33">Modeling in-vivo protein-DNA binding by combining multiple-instance learning with a hybrid deep neural network</article-title>, <source hwp:id="source-34">Sci Rep</source> <year>2019</year>;<volume>9</volume>:<fpage>8484</fpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>35.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Yin W"><surname>Yin</surname> <given-names>W</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schütze H."><surname>Schütze</surname> <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-34">Multichannel variable-size convolution for sentence classification</article-title>, <source hwp:id="source-35">arXiv</source> preprint <pub-id pub-id-type="arxiv">1603.04513</pub-id> <year>2016</year>.</citation></ref><ref id="c36" hwp:id="ref-36"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Zhang Q"><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhu L"><surname>Zhu</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang D-S."><surname>Huang</surname> <given-names>D-S.</given-names></string-name> <article-title hwp:id="article-title-35">High-order convolutional neural network architecture for predicting DNA-protein binding sites</article-title>, <source hwp:id="source-36">IEEE/ACM transactions on computational biology and bioinformatics</source> <year>2018</year>;<volume>16</volume>:<fpage>1184</fpage>–<lpage>1192</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>37.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Han S"><surname>Han</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meng Z"><surname>Meng</surname> <given-names>Z</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Z"><surname>Li</surname> <given-names>Z</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-36">Optimizing filter size in convolutional neural networks for facial action unit recognition</article-title>. <source hwp:id="source-37">In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2018</year>, p. <fpage>5070</fpage>–<lpage>5078</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1 xref-ref-38-2"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Reiter F"><surname>Reiter</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wienerroither S"><surname>Wienerroither</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stark A."><surname>Stark</surname> <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-37">Combinatorial function of transcription factors and cofactors</article-title>, <source hwp:id="source-38">Curr Opin Genet Dev</source> <year>2017</year>;<volume>43</volume>:<fpage>73</fpage>–<lpage>81</lpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Lambert SA"><surname>Lambert</surname> <given-names>SA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jolma A"><surname>Jolma</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Campitelli LF"><surname>Campitelli</surname> <given-names>LF</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-38">The Human Transcription Factors</article-title>, <source hwp:id="source-39">Cell</source> <year>2018</year>;<volume>175</volume>:<fpage>598</fpage>–<lpage>599</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>40.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Ding Y"><surname>Ding</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li J-Y"><surname>Li</surname> <given-names>J-Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang M"><surname>Wang</surname> <given-names>M</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-39">An exact transformation of convolutional kernels enables accurate identification of sequence motifs, bioRxiv 2018:163220</article-title>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>41.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Chollet F"><surname>Chollet</surname> <given-names>F</given-names></string-name>, others. <source hwp:id="source-40">Keras</source> <year>2015</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>42.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Khan A"><surname>Khan</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fornes O"><surname>Fornes</surname> <given-names>O</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stigliani A"><surname>Stigliani</surname> <given-names>A</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-40">JASPAR 2018: update of the open-access database of transcription factor binding profiles and its web framework</article-title>, <source hwp:id="source-41">Nucleic acids research</source> <year>2017</year>;<volume>46</volume>:<fpage>D260</fpage>–<lpage>D266</lpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>43.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Zeiler MD"><surname>Zeiler</surname> <given-names>MD</given-names></string-name>. <article-title hwp:id="article-title-41">Adadelta: an adaptive learning rate method</article-title>, <source hwp:id="source-42">arXiv</source> preprint <pub-id pub-id-type="arxiv">1212.5701</pub-id> <year>2012</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><label>44.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Glorot X"><surname>Glorot</surname> <given-names>X</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname> <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-42">Understanding the difficulty of training deep feedforward neural networks</article-title>. In: <person-group person-group-type="editor" hwp:id="person-group-1">Yee <string-name name-style="western" hwp:sortable="Whye T."><surname>Whye</surname> <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="eds Mike T."><given-names>Mike T.</given-names> <surname>eds</surname></string-name></person-group>). <source hwp:id="source-43">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research: PMLR</source>, <year>2010</year>, <fpage>249</fpage>--256.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3 xref-ref-45-4 xref-ref-45-5"><label>45.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Zeng H"><surname>Zeng</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Edwards MD"><surname>Edwards</surname> <given-names>MD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu G"><surname>Liu</surname> <given-names>G</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-43">Convolutional neural network architectures for predicting DNA–protein binding</article-title>, <source hwp:id="source-44">Bioinformatics</source> <year>2016</year>;<volume>32</volume>:<fpage>i121</fpage>–<lpage>i127</lpage>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2 xref-ref-46-3"><label>46.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Kelley DR"><surname>Kelley</surname> <given-names>DR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Snoek J"><surname>Snoek</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rinn JL"><surname>Rinn</surname> <given-names>JL</given-names></string-name>. <article-title hwp:id="article-title-44">Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title>, <source hwp:id="source-45">Genome research</source> <year>2016</year>;<volume>26</volume>:<fpage>990</fpage>–<lpage>999</lpage>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2"><label>47.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><collab hwp:id="collab-1">ENCODE Project Consortium</collab>. <article-title hwp:id="article-title-45">An integrated encyclopedia of DNA elements in the human genome</article-title>, <source hwp:id="source-46">nature</source> <year>2012</year>;<volume>489</volume>:<fpage>57</fpage>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>48.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Nair S"><surname>Nair</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kim DS"><surname>Kim</surname> <given-names>DS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perricone J"><surname>Perricone</surname> <given-names>J</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-46">Integrating regulatory DNA sequence and gene expression to predict genome-wide chromatin accessibility across cellular contexts</article-title>, <source hwp:id="source-47">Bioinformatics</source> <year>2019</year>;<volume>35</volume>:<fpage>i108</fpage>–<lpage>i116</lpage>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>49.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Yang J"><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ma A"><surname>Ma</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoppe AD"><surname>Hoppe</surname> <given-names>AD</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-47">Prediction of regulatory motifs from human Chip-sequencing data using a deep learning framework</article-title>, <source hwp:id="source-48">Nucleic Acids Res</source> <year>2019</year>;<volume>47</volume>:<fpage>7809</fpage>–<lpage>7824</lpage>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>50.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Quang D"><surname>Quang</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xie X."><surname>Xie</surname> <given-names>X.</given-names></string-name> <article-title hwp:id="article-title-48">DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences</article-title>, <source hwp:id="source-49">Nucleic Acids Res</source> <year>2016</year>;<volume>44</volume>:<fpage>e107</fpage>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><label>51.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Wang M"><surname>Wang</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tai C E W"><surname>Tai</surname> <given-names>C E W</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-49">DeFine: deep convolutional neural networks accurately quantify intensities of transcription factor-DNA binding and facilitate evaluation of functional non-coding variants</article-title>, <source hwp:id="source-50">Nucleic Acids Res</source> <year>2018</year>;<volume>46</volume>:<fpage>e69</fpage>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>52.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Yin Q"><surname>Yin</surname> <given-names>Q</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu M"><surname>Wu</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu Q"><surname>Liu</surname> <given-names>Q</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-50">DeepHistone: a deep learning approach to predicting histone modifications</article-title>, <source hwp:id="source-51">BMC genomics</source> <year>2019</year>;<volume>20</volume>:<fpage>193</fpage>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>53.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="El-Gebali S"><surname>El-Gebali</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mistry J"><surname>Mistry</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bateman A"><surname>Bateman</surname> <given-names>A</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-51">The Pfam protein families database in 2019</article-title>, <source hwp:id="source-52">Nucleic Acids Res</source> <year>2019</year>;<volume>47</volume>:<fpage>D427</fpage>–<lpage>D432</lpage>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1"><label>54.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Kalvari I"><surname>Kalvari</surname> <given-names>I</given-names></string-name>, <string-name name-style="western" hwp:sortable="Argasinska J"><surname>Argasinska</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Quinones-Olvera N"><surname>Quinones-Olvera</surname> <given-names>N</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-52">Rfam 13.0: shifting to a genome-centric resource for non-coding RNA families</article-title>, <source hwp:id="source-53">Nucleic Acids Res</source> <year>2018</year>;<volume>46</volume>:<fpage>D335</fpage>–<lpage>D342</lpage>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>55.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Min S"><surname>Min</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park S"><surname>Park</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kim S"><surname>Kim</surname> <given-names>S</given-names></string-name> <etal>et al.</etal> <article-title hwp:id="article-title-53">Pre-Training of Deep Bidirectional Protein Sequence Representations with Structural Information</article-title>, <source hwp:id="source-54">arXiv</source> preprint <pub-id pub-id-type="arxiv">1912.05625</pub-id> <year>2019</year>.</citation></ref><ref id="c56" hwp:id="ref-56"><label>56.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Vazhayil A"><surname>Vazhayil</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kp S."><surname>Kp</surname> <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-54">DeepProteomics: Protein family classification using Shallow and Deep Networks</article-title>, <source hwp:id="source-55">arXiv</source> preprint <pub-id pub-id-type="arxiv">1809.04461</pub-id> <year>2018</year>.</citation></ref><ref id="c57" hwp:id="ref-57"><label>57.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Liu X."><surname>Liu</surname> <given-names>X.</given-names></string-name> <article-title hwp:id="article-title-55">Deep recurrent neural network for protein function prediction from sequence</article-title>, <source hwp:id="source-56">arXiv</source> preprint <pub-id pub-id-type="arxiv">1701.08318</pub-id> <year>2017</year>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>58.</label><citation publication-type="other" citation-type="journal" ref:id="508242v5.58" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Liza FF"><surname>Liza</surname> <given-names>FF</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grzes M."><surname>Grzes</surname> <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-56">Relating RNN layers with the spectral WFA ranks in sequence modelling</article-title>, <source hwp:id="source-57">Association for Computational Linguistics</source> <year>2019</year>:<fpage>24</fpage>–<lpage>33</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>59.</label><citation publication-type="journal" citation-type="journal" ref:id="508242v5.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Zhang Q"><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhu L"><surname>Zhu</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang DS"><surname>Huang</surname> <given-names>DS</given-names></string-name>. <article-title hwp:id="article-title-57">High-Order Convolutional Neural Network Architecture for Predicting DNA-Protein Binding Sites</article-title>, <source hwp:id="source-58">IEEE/ACM Trans Comput Biol Bioinform</source> <year>2019</year>;<volume>16</volume>:<fpage>1184</fpage>–<lpage>1192</lpage>.</citation></ref></ref-list></back></article>
