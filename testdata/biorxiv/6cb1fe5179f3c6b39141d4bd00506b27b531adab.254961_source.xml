<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/254961</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;254961</article-id><article-id pub-id-type="other" hwp:sub-type="slug">254961</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">254961</article-id><article-id pub-id-type="other" hwp:sub-type="tag">254961</article-id><article-version>1.4</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Visual and auditory brain areas share a representational structure that supports emotion perception</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>#</label>Correspondence to: Thalia Wheatley, 6207 Moore Hall, Dartmouth College, Hanover, NH 03755, USA. <email hwp:id="email-1">thalia.p.wheatley@dartmouth.edu</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-8762-7479</contrib-id><name name-style="western" hwp:sortable="Sievers Beau"><surname>Sievers</surname><given-names>Beau</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-1" hwp:rel-id="aff-6">6</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-8762-7479"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Parkinson Carolyn"><surname>Parkinson</surname><given-names>Carolyn</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Kohler Peter J."><surname>Kohler</surname><given-names>Peter J.</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Hughes James M."><surname>Hughes</surname><given-names>James M.</given-names></name></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Fogelson Sergey V."><surname>Fogelson</surname><given-names>Sergey V.</given-names></name></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Wheatley Thalia"><surname>Wheatley</surname><given-names>Thalia</given-names></name><xref ref-type="aff" rid="a6" hwp:id="xref-aff-6-2" hwp:rel-id="aff-6">6</xref><xref ref-type="aff" rid="a7" hwp:id="xref-aff-7-1" hwp:rel-id="aff-7">7</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">#</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">Department of Psychology, Harvard University</institution>, Cambridge, MA 02138, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Department of Psychology, University of California Los Angeles</institution>, Los Angeles, CA 90095, <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Brain Research Institute, University of California</institution>, Los Angeles, Los Angeles, CA 90095, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Department of Psychology, York University</institution>, Toronto, Ontario, <country>Canada</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Centre for Vision Research, York University</institution>, Toronto, Ontario, <country>Canada</country></aff><aff id="a6" hwp:id="aff-6" hwp:rev-id="xref-aff-6-1 xref-aff-6-2"><label>6</label><institution hwp:id="institution-6">Department of Psychological and Brain Sciences, Dartmouth College</institution>, Hanover, NH 03755, <country>USA</country></aff><aff id="a7" hwp:id="aff-7" hwp:rev-id="xref-aff-7-1"><label>7</label><institution hwp:id="institution-7">Santa Fe Institute</institution>, Santa Fe, NM 87501, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-01-29T16:33:26-08:00">
    <day>29</day><month>1</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-09-13T17:51:11-07:00">
    <day>13</day><month>9</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-01-29T16:38:55-08:00">
    <day>29</day><month>1</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-09-13T17:56:08-07:00">
    <day>13</day><month>9</month><year>2021</year>
  </pub-date><elocation-id>254961</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-01-26"><day>26</day><month>1</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2021-09-13"><day>13</day><month>9</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-09-13"><day>13</day><month>9</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by/4.0/</ext-link></p></license></permissions><self-uri xlink:href="254961.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/254961v4.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="254961.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/254961v4/254961v4.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/254961v4/254961v4.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Summary</title><p hwp:id="p-2">Emotionally expressive music and dance occur together across the world. This may be because features shared across the senses are represented the same way even in different sensory brain areas, putting music and movement in directly comparable terms. These shared representations may arise from a general need to identify environmentally relevant combinations of sensory features, particularly those that communicate emotion. To test the hypothesis that visual and auditory brain areas share a representational structure, we created music and animation stimuli with crossmodally matched features expressing a range of emotions. Participants confirmed that each emotion corresponded to a set of features shared across music and movement. A subset of participants viewed both music and animation during brain scanning, revealing that representations in auditory and visual brain areas were similar to one another. This shared representation captured not only simple stimulus features, but also combinations of features associated with emotion judgments. The posterior superior temporal cortex represented both music and movement using this same structure, suggesting supramodal abstraction of sensory content. Further exploratory analysis revealed that early visual cortex used this shared representational structure even when stimuli were presented auditorily. We propose that crossmodally shared representations support mutually reinforcing dynamics across auditory and visual brain areas, facilitating crossmodal comparison. These shared representations may help explain why emotions are so readily perceived and why some dynamic emotional expressions can generalize across cultural contexts.</p></abstract><counts><page-count count="29"/></counts><custom-meta-wrap><custom-meta hwp:id="custom-meta-1"><meta-name>special-property</meta-name><meta-value>contains-inline-supplementary-material</meta-value></custom-meta></custom-meta-wrap><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-2">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-3">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-1"><p hwp:id="p-4">Improved analyses, expanded literature review and discussion.</p></fn></fn-group><fn-group content-type="external-links" hwp:id="fn-group-2"><fn fn-type="dataset" hwp:id="fn-2"><p hwp:id="p-5">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/kvbqm/" ext-link-type="uri" xlink:href="https://osf.io/kvbqm/" hwp:id="ext-link-2">https://osf.io/kvbqm/</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-4">Introduction</title><p hwp:id="p-6">Wherever there is music, there is movement (<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Kaeppler, 1978</xref>; <xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Mehr et al., 2019</xref>; <xref ref-type="bibr" rid="c78" hwp:id="xref-ref-78-1" hwp:rel-id="ref-78">Savage, Brown, Sakai, &amp; Currie, 2015</xref>). Not only are music and dance pervasive across the anthropological and ethnographic record, some languages use a single word for both (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Baily, 1985</xref>; <xref ref-type="bibr" rid="c88" hwp:id="xref-ref-88-1" hwp:rel-id="ref-88">Trehub, Becker, Morley, &amp; Trehub, 2015</xref>). The link between music and movement is present from early in development, with infants as young as 7 months using movement to resolve ambiguities in musical rhythm (<xref ref-type="bibr" rid="c69" hwp:id="xref-ref-69-1" hwp:rel-id="ref-69">Phillips-silver &amp; Trainor, 2005</xref>). Further, communication of emotion through music and movement occurs across a range of dissimilar cultures (<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Fritz et al., 2009</xref>; <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-1" hwp:rel-id="ref-84">Sievers, Polansky, Casey, &amp; Wheatley, 2013</xref>; <xref ref-type="bibr" rid="c88" hwp:id="xref-ref-88-2" hwp:rel-id="ref-88">Trehub et al., 2015</xref>), although there are also many important cross-cultural differences in emotion expression, perception, and conceptualization (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Gendron, Roberson, Vyver, &amp; Barrett, 2014</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Jack, Caldara, &amp; Schyns, 2012</xref>; <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Jack, Sun, Delis, Garrod, &amp; Schyns, 2016</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Jackson et al., 2019</xref>; <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Margulis, Wong, Simchy-Gross, &amp; McAuley, 2019</xref>; <xref ref-type="bibr" rid="c97" hwp:id="xref-ref-97-1" hwp:rel-id="ref-97">Yuki, Maddux, &amp; Masuda, 2007</xref>). Here, we suggest that the link between music and movement may result from fundamental similarities in how music and movement are structured, perceived, and represented in the brain.</p><p hwp:id="p-7">Supporting this account, preliminary research suggests that emotional music and movement can share structural features across cultures. In both the United States and a small-scale society in rural Cambodia, angry music and movement are both fast and move downward, peaceful music and movement are both slow and move upward, and so on (<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-1" hwp:rel-id="ref-82">Sievers, Lee, Haslett, Wheatley, &amp; Wheatley, 2019</xref>; <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-2" hwp:rel-id="ref-84">Sievers et al., 2013</xref>). Though suggestive, shared features do not fully explain the pervasive, experiential link between music and movement. Here, we examine a possible explanation: Different sensory areas of the brain may share a representational geometry (<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Kriegeskorte &amp; Kievit, 2013</xref>), such that differences between sensory features and perceived emotions are represented by matched differences in patterns of neural activity, putting music and movement in comparable, task-relevant terms.</p><p hwp:id="p-8">We tested two related main hypotheses concerning both <italic toggle="yes">where</italic> and <italic toggle="yes">how</italic> music and movement are represented in the brain. (H1) The <italic toggle="yes">separate regions, shared representations</italic> hypothesis: that separate, modality-specific, auditory and visual areas use a shared representational geometry. (H2) The <italic toggle="yes">supramodal region</italic> hypothesis: that a supramodal area (or areas) uses a single representational geometry for both auditory and visual stimuli. Note that (H1) does not require patterns of activity in auditory and visual brain regions to be identical in every respect, as each sensory region likely represents modality-specific features. Evidence that the representation of music in auditory regions is very similar to the representation of movement in visual regions would support the separate regions, shared representations hypothesis (H1). By contrast, evidence of a single region that represents both music and movement using the same representational geometry would support the supramodal region hypothesis (H2). Importantly, (H1) and (H2) are not mutually exclusive, and while previous research has provided support for (H2) (2010), the status of (H1) remains unknown.</p><p hwp:id="p-9">Further, we asked how representations of perceived emotion in music and movement were organized, testing two auxiliary hypotheses. (A1) The <italic toggle="yes">simple features</italic> hypothesis: that sensory brain regions represent emotional stimuli in terms of differences in simple stimulus features, without respect to how those features may later be inferentially processed to yield emotion judgments (by e.g., simulation theory, <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Gordon, 1986</xref>; or theory theory, <xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Gopnik &amp; Wellman, 1994</xref>). (A2) The <italic toggle="yes">environmental conjunctions</italic> hypothesis: that sensory representations of emotional stimuli closely track emotion judgments, suggesting that the human perceptual system may directly represent latent configurations of stimulus features associated with emotion content. These task-relevant representations may act as a shortcut, reducing the need for downstream inferential processing (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Gallagher, 2008</xref>). Evidence that sensory representations fit a model based on stimulus features would support the simple features hypothesis (A1), while evidence that sensory representations fit a model based on emotion judgments would support the environmental conjunctions hypothesis (A2). (A1) and (A2) are not mutually exclusive, as sensory regions may represent both stimulus features and environmentally relevant feature conjunctions.</p><p hwp:id="p-10">By “perceived emotions” we refer only to participants’ perceptions of the stimuli and their judgments of what emotions the stimuli expressed. We do not refer to emotional states evoked in the participants by the stimuli, or to any other kind of emotion. Importantly, though we discuss the relevance of the findings to cross-cultural generalization, we did not test any hypotheses across cultures.</p><p hwp:id="p-11">Testing both sets of hypotheses required comparing representations between brain areas. To accomplish this, we used model-based representational similarity analysis (RSA) (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Kriegeskorte, Goebel, &amp; Bandettini, 2006</xref>; <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>), comparing representations evoked by separately presented auditory and visual stimuli to test (H1) and (H2). For detailed discussion of the limits and merits of this approach, see <xref ref-type="bibr" rid="c73" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Roskies (2021)</xref>. The model included predictors corresponding to both simple stimulus features and to participants’ judgments of emotion content, supporting tests of (A1) and (A2). We performed an additional supporting test of (H1) using a model-free approach that directly compared representational geometries across sensory areas without making any assumptions about representational content.</p><sec id="s1a" hwp:id="sec-2"><title hwp:id="title-5">Previous research on neural representation of emotion</title><p hwp:id="p-12">Emotion-related neural processes are distributed across a wide range of brain areas, with each area implicated in the production and/or perception of many emotions (<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Lindquist, Wager, Kober, Bliss-Moreau, &amp; Barrett, 2012</xref>; <xref ref-type="bibr" rid="c90" hwp:id="xref-ref-90-1" hwp:rel-id="ref-90">Wager et al., 2015</xref>). However, certain aspects of emotion processing are localized. Lesion and neuroimaging studies have demonstrated that some brain areas play an outsized role in the processing of specific emotions; for example, the amygdala for the conscious recognition of fearful stimuli (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Adolphs, Tranel, Damasio, &amp; Damasio, 1994</xref>; <xref ref-type="bibr" rid="c89" hwp:id="xref-ref-89-1" hwp:rel-id="ref-89">Tsuchiya, Moradi, Felsen, Yamazaki, &amp; Adolphs, 2009</xref>), and the insula for recognizing disgust (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Calder, Lawrence, &amp; Young, 2001</xref>; <xref ref-type="bibr" rid="c68" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Phillips et al., 1997</xref>). Because our hypotheses concern representations capable of distinguishing many different emotion expressions, we focus here on distributed representations of emotion, and not on areas implicated in processing individual emotions.</p><p hwp:id="p-13">Our hypotheses ask not only <italic toggle="yes">where</italic> in the brain emotions are represented, but <italic toggle="yes">how</italic> those representations are structured. For example, a single brain area may distinguish stimulus classes using different spatial patterns of activity that all have the same mean. To characterize the representational properties of these areas, it is necessary to use techniques that are sensitive to such spatially distributed patterns; e.g., multivariate pattern classification (<xref ref-type="bibr" rid="c61" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Norman, Polyn, Detre, &amp; Haxby, 2006</xref>) or representational similarity analysis (RSA; <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">Kriegeskorte &amp; Kievit, 2013</xref>). For example, <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">Peelen et al. (2010)</xref> showed that medial prefrontal cortex (mPFC) and posterior superior temporal sulcus (pSTS) supramodally represent emotion identity by demonstrating that patterns of activity in these areas had greater within-emotion similarity than between-emotion similarity. <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Chikazoe et al. (2014)</xref> used pattern analysis to locate supramodal valence (positive vs. neutral vs. negative) representations in medial and lateral orbitofrontal cortex and modality-specific valence representations in sensory cortices. Also investigating valence, <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Kim et al. (2017)</xref> presented emotional movie clips and orchestral music, and found a range of supramodal representations: valence direction in the precuneus, valence magnitude in mPFC, STS, and middle frontal gyrus (MFG), and both valence direction and magnitude in the STS, MFG, and thalamus. <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-1" hwp:rel-id="ref-86">Skerry &amp; Saxe (2015)</xref> found that a model describing participants’ appraisals of emotional narratives (e.g., “Did someone cause this situation intentionally, or did it occur by accident?”) fit activity in dorsal and middle medial prefrontal cortex, the temporoparietal junction, and a network of regions identified by a theory of mind localization task.</p><p hwp:id="p-14">Importantly, where previous studies have focused on emotions evoked by narrative content while controlling for stimulus features (e.g., <xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Chikazoe et al., 2014</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-2" hwp:rel-id="ref-86">Skerry &amp; Saxe, 2015</xref>), the present study takes a different approach, focusing on emotion perceived solely from stimulus features without any contextualizing narrative. Emotions perceived from stimulus features make up a large and understudied part of human experience. For example, people often communicate emotion using only body language and tone of voice, and actively seek out instrumental music and abstract visual art that communicates emotion only through variation in pitch, volume, shape, brightness, and so on. And although there are many cross-cultural differences in emotion experience, expression, and perception (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Gendron et al., 2014</xref>; <xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Jack et al., 2012</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">2016</xref>; <xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Jackson et al., 2019</xref>; <xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-2" hwp:rel-id="ref-54">Margulis et al., 2019</xref>; <xref ref-type="bibr" rid="c97" hwp:id="xref-ref-97-2" hwp:rel-id="ref-97">Yuki et al., 2007</xref>), preliminary evidence suggests the use of shared stimulus features to express emotion can generalize across dissimilar cultures (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-3" hwp:rel-id="ref-84">Sievers et al., 2013</xref>; <xref ref-type="bibr" rid="c88" hwp:id="xref-ref-88-3" hwp:rel-id="ref-88">Trehub et al., 2015</xref>). Despite its ubiquity and importance, the neural mechanisms supporting emotion perception from stimulus features remain poorly understood.</p><p hwp:id="p-15">The present approach allows us to test the <italic toggle="yes">shared features</italic> (A1) and <italic toggle="yes">environmental conjunctions</italic> (A2) hypotheses, assessing whether sensory brain areas represent conjunctions of features associated with environmentally relevant stimuli such as emotion expressions, or whether these areas represent simple features that may be used by separate, downstream areas to infer emotion content.</p></sec><sec id="s1b" hwp:id="sec-3"><title hwp:id="title-6">Stimuli and experimental paradigm</title><p hwp:id="p-16">The stimuli consisted of short piano melodies and animations of a bouncing ball generated by participants in a previous study. This study showed that emotions were expressed the same way in music and in movement in both the US and a small-scale society in rural Cambodia (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-4" hwp:rel-id="ref-84">Sievers et al., 2013</xref>). The participants used a computer program to create examples of five emotions (Angry, Happy, Peaceful, Sad, Scared) by manipulating five stimulus features (speed, irregularity, consonance/spikiness, ratio of big-to-small movements, ratio of upward-to-downward movements). Participants were split into separate music and movement groups, each of which had no knowledge of the other. This approach did not presuppose what combinations of features would be used for each emotion, and participants were not instructed to use any specific features or feature combinations. Instead, they were encouraged to explore the entire possibility space. Critically, this method allowed us to vary what emotions were communicated while holding the depicted objects constant (i.e., each emotion was communicated using only the piano or the bouncing ball). This guaranteed that emotion content could only be communicated by variation in stimulus features, and that processing requirements were consistent across the stimulus set.</p><p hwp:id="p-17">Note that this approach differs from previous research where emotion was communicated using narrative stories or emotionally charged images; e.g., the International Affective Picture System (<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Lang, Bradley, &amp; Cuthbert, 2008</xref>). Such studies often control for stimulus features, guaranteeing that emotion judgments are based solely on the content depicted in the stimuli. For example, a study of perceived emotion in spoken narrative might control for the speaker’s tone of voice, focusing on what the speaker said, rather than how they said it. The present study takes the opposite approach, controlling for the content depicted in the stimuli, guaranteeing that participants’ emotion judgments are based solely on variation in stimulus features. This is analogous to holding a speaker’s words constant, so that emotion can only be communicated by tone of voice.</p><p hwp:id="p-18">Because many emotions are perceived as mixes of other emotions (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Cowen &amp; Keltner, 2017</xref>), the stimulus set was augmented by linearly mixing the features of each emotion pair, creating mixed emotions (e.g. Happy–Sad). Emotions were mixed at 25%, 50%, and 75%. Three additional, “neutral” emotions were identified by searching for points in the stimulus feature possibility space that were distant from all other emotional feature combinations. For each set of stimulus features, or stimulus class, many individual stimuli were probabilistically generated (see <italic toggle="yes">Detailed methods</italic>). This ensured the results were not dependent on the idiosyncracies of single stimuli, but were instead generalizable to all stimuli that shared the same features. Further, this prevented participants from memorizing arbitrary associations between individual stimuli and emotion labels. Music and animation were matched, such that for each musical stimulus class there was an animation stimulus class with the same features. This process yielded 76 total emotional stimulus classes, including both music and animation. All stimuli are available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/kvbqm/" ext-link-type="uri" xlink:href="https://osf.io/kvbqm/" hwp:id="ext-link-3">https://osf.io/kvbqm/</ext-link>.</p><p hwp:id="p-19">A separate set of participants judged how well each stimulus fit all five emotion labels, and a subset of these participants viewed many music and animation stimuli while undergoing functional magnetic resonance imaging (fMRI) (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>).</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><title hwp:id="title-7">Experimental paradigm.</title><p hwp:id="p-20"><italic toggle="yes">A</italic>. Participants in <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-5" hwp:rel-id="ref-84">Sievers et al. (2013)</xref> manipulated stimulus features to generate music and animation communicating five prototypical emotions: Angry, Happy, Peaceful, Sad, and Scared. <italic toggle="yes">B</italic>. Mixed emotions were generated by linear interpolation between the stimulus features of prototypical emotions. <italic toggle="yes">C</italic>. Participants judged the emotion content of many prototypical and mixed emotions in music and animation. <italic toggle="yes">D</italic>. A subset of participants viewed many prototypical and mixed emotions in music and animation while undergoing jittered event-related fMRI scanning. <italic toggle="yes">E</italic>. Results were analyzed using searchlight representational similarity analysis (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">2008</xref>; <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-3" hwp:rel-id="ref-46">Kriegeskorte &amp; Kievit, 2013</xref>). For each searchlight sphere, the structure of the neural representational dissimilarity matrix (RDM) was predicted using a linear combination of stimulus feature and emotion judgment RDMs.</p></caption><graphic xlink:href="254961v4_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec></sec><sec id="s2" hwp:id="sec-4"><title hwp:id="title-8">Results</title><sec id="s2a" hwp:id="sec-5"><title hwp:id="title-9">Emotion judgments</title><p hwp:id="p-21">Participants broadly agreed about the emotion content of each stimulus class (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>). Agreement was assessed by measuring the distance of participants’ individual emotion judgments from the class mean, scaled by the maximum possible distance, and significance was assessed using permutation testing (see <italic toggle="yes">Detailed methods</italic>). For all 76 stimulus classes except one “neutral” emotion, participants’ judgments were closer to the class mean than would be expected by chance (mean <italic toggle="yes">t</italic>=−4.37; mean difference from null=.07; mean p&lt;.001). Importantly, this agreement rules out the possibility that participants invented and then memorized arbitrary associations between combinations of stimulus features and combinations of emotion labels.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><title hwp:id="title-10">Emotion judgment agreement.</title><p hwp:id="p-22">Agreement between participants was assessed by measuring the distance of participants’ individual emotion judgments from the class mean. Significance was assessed using permutation testing (see <italic toggle="yes">Detailed methods</italic>). Values above 0 indicate more agreement than expected by chance.</p></caption><graphic xlink:href="254961v4_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig></sec><sec id="s2b" hwp:id="sec-6"><title hwp:id="title-11">Shared representational geometry</title><p hwp:id="p-23">Auditory and visual brain regions shared a representational geometry. A single model of representational similarity (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-3" hwp:rel-id="ref-45">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-3" hwp:rel-id="ref-47">2008</xref>; <xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-4" hwp:rel-id="ref-46">Kriegeskorte &amp; Kievit, 2013</xref>) explained patterns of activity in visual brain regions during animation trials and auditory brain regions during music trials, providing strong support for the <italic toggle="yes">separate regions, shared representations</italic> hypothesis (H1) (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>; <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>). The model used 10 representational dissimilarity matrices (RDMs) as predictors: five based on the mean parameter settings used to create the stimuli (speed, irregularity/jitter, consonance/spikiness, ratio of big-to-small movements, ratio of upward-to-downward movements), and five based on the mean emotion judgments of the behavioral participants (Angry, Happy, Peaceful, Sad, and Scared) (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>). The model included no information specific to either vision or audition.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><title hwp:id="title-12">Main result.</title><p hwp:id="p-24">Highlighted brain areas were identified using a model including both stimulus features and emotion judgments as predictors, which was separately fit to animation trials (blue) and music trials (green). A significant proportion of participants’ model fits overlapped for both trial types (yellow). Neural dissimilarity matrices show pairwise distances between activity patterns evoked by each stimulus at the locations of best model fit (circled)—medial lingual gyrus (animation) and lateral superior temporal gyrus (music). Labels as in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>. Multidimensional scaling flattens these matrices to two dimensions, so the distance between dots reflects the similarity of patterns of neural activity. Dots are colored by mixing the legend colors based on participants’ judgments of the emotion content of each stimulus.</p></caption><graphic xlink:href="254961v4_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-4"><title hwp:id="title-13">Peak model fits.</title><p hwp:id="p-25">Anim.: Model fit to animation trials. Music: Model fit to music trials. Overlap: Percentage of participants with overlapping music and animation model fits. Inter.: Intermodal regions which fit the model even when the stimulus was presented in the non-preferred modality. For results per model predictor, see Figures S4 and S5. Labels determined programmatically using the atlas of <xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Destrieux, Fischl, Dale, and Halgren (2010)</xref>.</p></caption><graphic xlink:href="254961v4_tbl1a" position="float" orientation="portrait" hwp:id="graphic-4"/><graphic xlink:href="254961v4_tbl1b" position="float" orientation="portrait" hwp:id="graphic-5"/></table-wrap><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-5"><title hwp:id="title-14">Representational dissimilarity matrices.</title><p hwp:id="p-26">Columns and rows share labels. “Biggest Gap,” “Search One”, and “Search Four” are “neutral” emotions.</p></caption><graphic xlink:href="254961v4_fig4" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-27">The peak of the average model fit across participants was in the left medial lingual gyrus for animation trials (mean <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="254961v4_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>; 95% CI: .08–.21; t(19)=4.68; p=.005 corrected) and in right anterior superior temporal gyrus for music trials (mean <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="254961v4_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula>; 95% CI: .1–.2; t(19)=6.08; p=.01 corrected), (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3</xref>). Critically, a direct, model-free test of similarity between these areas showed that they were more similar to each other than would be expected by chance (ρ=.68, p&lt;.001), further supporting the <italic toggle="yes">separate regions, shared representations</italic> hypothesis (H1), and making it unlikely that the results reported above are an artifact of model misspecification (see <italic toggle="yes">Detailed methods</italic>).</p><p hwp:id="p-28">Model fit was driven by both stimulus feature <italic toggle="yes">and</italic> emotion judgment predictors, and was not dominated by a small number of predictors, providing support for both the <italic toggle="yes">simple features</italic> and <italic toggle="yes">environmental conjunctions</italic> hypotheses (A1 and A2). Individual predictors were assessed by mapping Spearman’s ρ across the brain. Spearman’s ρ was significant for all 10 predictors at the location of peak model fit (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>), and was distributed similarly across the brain (Figure S4). β weight maps for each predictor were also calculated (Figure S5), reflecting only the unique contribution of each predictor, whereas Spearman’s ρ reflects both unique and shared contributions. See Figure S6 for an assessment of model multicollinearity, including variance inflation factors for each predictor.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-6"><title hwp:id="title-15">Model fits.</title><p hwp:id="p-29">Maps of the mean coefficient of determination <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="254961v4_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> across participants. Maps thresholded at FWER=.05. Box plots show the median, quartiles, and range of the per-participant <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="254961v4_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula> values at the location of best model fit at the group level. The dotted line indicates the lower bound of the noise ceiling, and the solid line the upper bound. For per-parameter Spearman’s ρ and β weight maps, see Figures S4 and S5.</p></caption><graphic xlink:href="254961v4_fig5" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-30">The model accounted for 51% of the variance for animation trials, and 31% of the variance for music trials, relative to the lower bound of the noise ceiling (see <italic toggle="yes">Detailed methods</italic>). Note that because of small differences in functional anatomy across participants, the peak of the average model fit underestimates individual model fit. The mean of the individual peak model fits was in bilateral anterior superior temporal gyrus for music trials (mean individual <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="254961v4_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula>; 95% CI: .21–.31; t(19)=10.95; p&lt;.001 uncorrected) and in the lingual gyrus for animation trials (mean individual <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="254961v4_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula>; 95% CI: .24–.38; t(19)=9.2; p&lt;.001 uncorrected) (Figures S1 and S2).</p></sec><sec id="s2c" hwp:id="sec-7"><title hwp:id="title-16">Overlapping auditory and visual model fit</title><p hwp:id="p-31">Brain regions where music and animation were both represented were found in bilateral posterior superior temporal gyrus (pSTG) in 60% of participants (95% CI 36%–84%, p&lt;.001 corrected), supporting the <italic toggle="yes">supramodal region</italic> hypothesis (H2) (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6A</xref>; see Figure S3 for per-participant maps). To locate such supramodal representations we created binary overlap masks, selecting voxels where both music and animation model fits were significant at the individual level (permutation p&lt;.05 uncorrected). Multiple comparisons correction of these overlap maps was performed at the group level, testing the proportion of individuals with overlap in a region against the null hypothesis that no participants had overlap in that region. Critically, this analysis is insensitive to the magnitude of <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="254961v4_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> at the individual level, allowing detection of overlapping signals that have low magnitude but reappear across a significant proportion of the participants. The model fit for music trials was also significant at this location, though the model fit for animation trials was not (music mean <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="254961v4_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula>, 95% CI: .02–.05, t(19)=5.78, p=.01 corrected; animation mean <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="254961v4_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula>, 95% CI: .01–.05, t(19)=2.94, p=.13 corrected). The model accounted for 26% of the variance for animation trials, and 31% of the variance for music trials, relative to the lower bound of the noise ceiling. Due to individual differences in functional anatomy, this procedure underestimates the proportion of participants with supramodal representations.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;254961v4/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6:</label><caption hwp:id="caption-7"><title hwp:id="title-17">Results across modalities.</title><p hwp:id="p-32">A. Supramodal emotion in pSTG. Maps show the proportion of participants representing emotion in music and animation in the same brain areas, thresholded at voxelwise FWER=.05. Box plots show the median, quartiles, and range of <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="254961v4_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula> for music and animation trials at the marked peak. B. Intermodal RSA model fit. Maps show areas that represented emotional stimuli even when presented in the area’s non-preferred modality (see <italic toggle="yes">Detailed methods</italic>), thresholded at voxelwise FWER=.05. <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="254961v4_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula> values below .1 hidden for visual clarity. Box plot shows the median, quartiles, and range of per-participant <inline-formula hwp:id="inline-formula-12"><alternatives hwp:id="alternatives-12"><inline-graphic xlink:href="254961v4_inline12.gif" hwp:id="inline-graphic-12"/></alternatives></inline-formula> values at the marked peak. Dotted lines indicate the lower bound of the noise ceiling, while solid lines indicate the upper bound.</p></caption><graphic xlink:href="254961v4_fig6" position="float" orientation="portrait" hwp:id="graphic-8"/></fig></sec><sec id="s2d" hwp:id="sec-8"><title hwp:id="title-18">Exploratory intermodal RSA</title><p hwp:id="p-33">To find brain areas that represented stimuli presented in that area’s non-preferred modality, we performed an exploratory intermodal RSA (see <italic toggle="yes">Detailed methods</italic>). Intermodal RSA revealed a bilateral set of areas across occipital, superior parietal, temporal, cingulate, and frontal cortex that represented stimuli presented in their non-preferred modality (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6B</xref>; <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>). Note that some of these areas did not show significant unimodal model fit. Peak intermodal model fit was in left lingual gyrus (mean <inline-formula hwp:id="inline-formula-13"><alternatives hwp:id="alternatives-13"><inline-graphic xlink:href="254961v4_inline13.gif" hwp:id="inline-graphic-13"/></alternatives></inline-formula>; 95% CI: .20–.37; t(19)=6.9; p&lt;.001 corrected). Notably, the peak intermodal model fit exceeded the peak within-modality model fit for both music and animation, and also exceeded the lower bound of the noise ceiling, explaining 40% of the variance relative to the upper bound. This suggests intermodal activity in left lingual gyrus was dominated by representations of model features. However, the lower bound of the intermodal noise ceiling was relatively low (.07), suggesting that most reliable neural activity in this region was modality-specific.</p></sec></sec><sec id="s3" hwp:id="sec-9"><title hwp:id="title-19">Discussion</title><p hwp:id="p-34">Music and movement are subjectively linked, and both use similar features to communicate emotion content. We examined a possible explanation for this link: that the brain represents music and movement using a shared representational geometry. To investigate this, we tested two primary hypotheses. (H1) The <italic toggle="yes">separate regions, shared representations</italic> hypothesis, where separate auditory and visual regions use the same representational geometry; and (H2) the <italic toggle="yes">supramodal region</italic> hypothesis, where some region(s) represent both auditory and visual stimuli. We also tested two auxiliary hypotheses. (A1) The <italic toggle="yes">simple features</italic> hypothesis, where sensory areas represent individual stimulus features that are not directly associated with emotion content; and (A2) the <italic toggle="yes">environmental conjunctions</italic> hypothesis, where sensory areas represent conjunctions of features that directly track differences in emotion judgment.</p><p hwp:id="p-35">We found that brain activity in separate auditory and visual areas shared a representational geometry, supporting the <italic toggle="yes">separate regions, shared representations</italic> hypothesis (H1). Providing additional support for (H1), representations in auditory and visual brain areas were more similar to each other than to any randomly chosen pair of brain areas. Further, music and animation were represented in pSTG, suggesting the pSTG uses a supramodal representation, supporting the <italic toggle="yes">supramodal region</italic> hypothesis (H2).</p><p hwp:id="p-36">Stimulus feature predictors (speed, jitter, consonance/spikiness, ratio of upward-to-downward movements, and ratio of big-to-small movements) were significant in both auditory and visual regions, supporting the <italic toggle="yes">simple features</italic> hypothesis (A1). A shared, crossmodal representation of simple stimulus features would support downstream comparison of auditory and visual stimuli, including inferential assessment of emotion content by, e.g., simulation theory (<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Gordon, 1986</xref>) or theory theory (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">Gopnik &amp; Wellman, 1994</xref>) systems. On such an account there may be nothing emotional <italic toggle="yes">per se</italic> about representations in sensory brain regions. However, predictors based on participants’ emotion judgments were also significant in both auditory and visual regions, even when controlling for the stimulus feature predictors, supporting the <italic toggle="yes">environmental conjunctions</italic> hypothesis (A2). On this account, sensory regions represent conjunctions of task-relevant environmental features, such as those associated with emotion expressions, supporting direct perception of social information (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Chemero, 2006</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Gallagher, 2008</xref>).</p><p hwp:id="p-37">Other possible parameters such as valence and arousal (<xref ref-type="bibr" rid="c74" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">Russell, 1980</xref>), value (<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Levy &amp; Glimcher, 2012</xref>; <xref ref-type="bibr" rid="c80" hwp:id="xref-ref-80-1" hwp:rel-id="ref-80">Shuster &amp; Levy, 2018</xref>), Fourier features (<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-2" hwp:rel-id="ref-82">Sievers et al., 2019</xref>), HMAX (<xref ref-type="bibr" rid="c70" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Riesenhuber &amp; Poggio, 1999</xref>), and motion energy (<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Nishimoto et al., 2011</xref>) certainly covary with the stimulus feature and emotion judgment predictors. Because these and similar measures would be fully dependent on the model parameters, including them as controls would introduce collinearity and create post-treatment bias. Identifying exactly how the features used in the reported model map to the true dimensions on which emotion, audition, and vision are organized will require future research.</p><p hwp:id="p-38">An exploratory intermodal representational similarity analysis found that visual areas represented both stimulus feature and emotion judgment predictors when musical stimuli were presented. However, most reliable neural activity in these areas was modality-specific, as indicated by a low intermodal noise ceiling. Previous studies have shown multimodal processing in unimodal areas (for reviews, see <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Bulkin &amp; Groh, 2006</xref>; <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Ghazanfar &amp; Schroeder, 2006</xref>; <xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Kayser &amp; Logothetis, 2007</xref>), which may depend on projections between unimodal areas (<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Cappe &amp; Barone, 2005</xref>; <xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Falchier, Clavagnier, Barone, &amp; Kennedy, 2002</xref>; <xref ref-type="bibr" rid="c72" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">Rockland &amp; Ojima, 2003</xref>). The reported results extend this account by showing that crossmodal perception is the product not only of operations in association cortices or activity dependent on inter-areal projections, but of the use of a representational geometry that is shared across modalities.</p><p hwp:id="p-39">The reported findings in pSTG are near previously reported pSTS activation during action understanding (<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Beauchamp, Lee, Argall, &amp; Martin, 2004</xref>; <xref ref-type="bibr" rid="c96" hwp:id="xref-ref-96-1" hwp:rel-id="ref-96">Wyk, Hudac, Carter, Sobel, &amp; Pelphrey, 2009</xref>), emotion perception (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Kreifelts, Ethofer, Grodd, Erb, &amp; Wildgruber, 2007</xref>; <xref ref-type="bibr" rid="c71" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">Robins, Hunyadi, &amp; Schultz, 2009</xref>; <xref ref-type="bibr" rid="c92" hwp:id="xref-ref-92-1" hwp:rel-id="ref-92">Watson et al., 2014</xref>), affective and linguistic prosody recognition (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Belyk &amp; Brown, 2014</xref>), and crossmodal perception and recognition tasks (<xref ref-type="bibr" rid="c93" hwp:id="xref-ref-93-1" hwp:rel-id="ref-93">Werner &amp; Noppeney, 2010</xref>; <xref ref-type="bibr" rid="c95" hwp:id="xref-ref-95-1" hwp:rel-id="ref-95">Wright, Pelphrey, Allison, McKeown, &amp; McCarthy, 2003</xref>). Interestingly, the reported results were right lateralized, similar to previous findings on prosody recognition (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Belyk &amp; Brown, 2014</xref>). Damage to the pSTS does not impair voice recognition (<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Jiahui et al., 2017</xref>), suggesting its representations are downstream from feature detectors. Alongside these results, the reported findings are consistent with the hypothesis that the pSTG/pSTS acts as a hub for transforming unimodal inputs into a common supramodal representation (<xref ref-type="bibr" rid="c79" hwp:id="xref-ref-79-1" hwp:rel-id="ref-79">Schirmer &amp; Adolphs, 2017</xref>).</p><sec id="s3a" hwp:id="sec-10"><title hwp:id="title-20">Evoked emotion</title><p hwp:id="p-40">Although our participants perceived emotions in our stimuli, it is unlikely that our stimuli evoked emotions in our participants. This disjunction highlights the complex and sometimes paradoxical relationship between perceived and evoked emotion. For example, perceiving sadness in music can evoke feelings of romance or pleasure (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Kawakami, Furukawa, Katahira, &amp; Okanoya, 2013</xref>). The gap between perception and feeling has been theorized in terms of direct versus vicarious emotions (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-2" hwp:rel-id="ref-40">Kawakami et al., 2013</xref>), and in terms of emotion modules serving complementary functions (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Gelstein et al., 2011</xref>; <xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Mehr, Krasnow, Bryant, &amp; Hagen, 2020</xref>). Another possibility is that perceptual representations of stimulus features and emotion content interact with regions that produce context-sensitive appraisals and emotion experiences, such as those identified by <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-3" hwp:rel-id="ref-86">Skerry &amp; Saxe (2015)</xref>, and subcortical regions sensitive to emotion content, including the amygdala (<xref ref-type="bibr" rid="c91" hwp:id="xref-ref-91-1" hwp:rel-id="ref-91">Wang et al., 2014</xref>). Activation of these appraisal and experience-related regions may not be necessary for making simple judgments of emotion content from stimulus features, possibly accounting for their absence in our results.</p><p hwp:id="p-41">However, perceptual representations of emotion may also be linked to evoked emotions. <xref ref-type="bibr" rid="c76" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">Saarimäki et al. (2018)</xref> showed that emotions evoked by listening to short stories produced activity in visual cortex, suggesting that evoked emotions can activate associated sensory representations. This may be a special case of the more general principle that mental imagery and episodic memory depend in part on activity in sensory regions associated with similar experiences (<xref ref-type="bibr" rid="c94" hwp:id="xref-ref-94-1" hwp:rel-id="ref-94">Wheeler, Petersen, &amp; Buckner, 2000</xref>). Accordingly, perceptual representations of emotion content may form over development in a process similar to memory consolidation. This developmental process may be guided by language, supporting culture-specific particularity (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Barrett, Lindquist, &amp; Gendron, 2007</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Hoemann, Xu, &amp; Barrett, 2019</xref>). Activation of perceptual representations of emotion by imagined emotion experience could play an important role in art and music by allowing artists and composers to iteratively check whether their artistic products correspond with their perceptual representations.</p></sec><sec id="s3b" hwp:id="sec-11"><title hwp:id="title-21">Systematicity, iconicity, and conceptual scope</title><p hwp:id="p-42">The neural representational system identified here is likely involved in phenomena beyond emotion perception, raising an interesting question: What concepts can and cannot be communicated via combinations of crossmodal stimulus features? If feature combinations in music and movement are symbolic, like words in natural language, then we would expect stimulus feature combinations that refer to abstract, non-emotional concepts. Just as arbitrary sequences of phonemes can point to “the housing market” or “editorial policy,” arbitrary combinations of stimulus features should be able to do the same.</p><p hwp:id="p-43">But, strikingly, music and movement do not operate wholly on an arbitrary, symbolic basis. Music and movement systematically use variation in the magnitudes of stimulus features to communicate variation in the magnitudes of concepts which the stimulus iconically resembles (<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-3" hwp:rel-id="ref-82">Sievers et al., 2019</xref>, <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-6" hwp:rel-id="ref-84">2013</xref>; <xref ref-type="bibr" rid="c87" hwp:id="xref-ref-87-1" hwp:rel-id="ref-87">Spector &amp; Maurer, 2009</xref>). For example, participants in the present study perceived mixes of the features for “happy” and the features for “sad” as expressing emotions on a continuum between happiness and sadness, with this pattern generalizing across emotion pairs. The present results suggest this systematic mixing is made possible in part by a crossmodally shared neural representational geometry.</p><p hwp:id="p-44">The systematic and iconic properties of musical communication may partially account for its use in expressing emotion, even across different cultural contexts. For example, the stimulus generator for the present study was previously used to show that the same combinations of music and movement features express the same emotions in both the United States and a small-scale society in rural Cambodia (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-7" hwp:rel-id="ref-84">Sievers et al., 2013</xref>). It may be that the forms of emotional music and movement are fixed by iconic, functional relationships that are shared across cultures (<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">Mehr et al., 2020</xref>, p. @<xref ref-type="bibr" rid="c85" hwp:id="xref-ref-85-1" hwp:rel-id="ref-85">Sievers2021</xref>). This may be why lullabies are slow and consonant across a global sample of ethnographic reports and recordings (<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-2" hwp:rel-id="ref-56">Mehr et al., 2019</xref>), and why high emotional arousal is expressed using harsh sounding, high spectral centroid sounds even across species of terrestrial vertebrates (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Filippi et al., 2017</xref>).</p><p hwp:id="p-45">Music, movement, and crossmodal neural representations may be well-suited to communicate iconic concepts that vary in magnitude, whereas language may be well-suited to communicate symbolic concepts that vary in kind. Similarly, it may be that iconic communication tends to generalize across cultures while symbolic communication tends to be more culture-specific. For example, previous research has shown cross-cultural generalization of valence perception but not categorical emotion (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-3" hwp:rel-id="ref-23">Gendron et al., 2014</xref>) (though categorical emotions can also be shared across cultures (<xref ref-type="bibr" rid="c63" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">Parkinson, Walker, Memmi, &amp; Wheatley, 2017</xref>)). Further, musical narrative built from contrasting sets of stimulus features (<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Margulis, 2017</xref>) shows large cross-cultural variability in interpretation (<xref ref-type="bibr" rid="c54" hwp:id="xref-ref-54-3" hwp:rel-id="ref-54">Margulis et al., 2019</xref>), unlike the present stimuli (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-8" hwp:rel-id="ref-84">Sievers et al., 2013</xref>) which contained no such contrasts.</p><p hwp:id="p-46">Importantly, the present study did not test any hypotheses across cultures. Future research will need to explore broad areas of concept space across many cultures, collect free responses from participants, characterize culture-specific emotion concepts, and contrast concepts that vary in magnitude with concepts that vary in kind.</p></sec><sec id="s3c" hwp:id="sec-12"><title hwp:id="title-22">Direct perception</title><p hwp:id="p-47">The results support the <italic toggle="yes">environmental conjunctions</italic> hypothesis (A2), that sensory brain regions represent task-relevant combinations of stimulus features, reducing the need for downstream inferential processing and acting as a shortcut for making important judgments. These representations may provide a neural basis for the direct perception of social information (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Chemero, 2006</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">Gallagher, 2008</xref>)— exemplified here by emotion judgment, and potentially covering a range of other phenomena. Importantly, the <italic toggle="yes">simple features</italic> hypothesis (A1) was also supported, suggesting that direct perception and inferential processing systems coexist and may interact.</p><p hwp:id="p-48">Without contextualizing narrative, judgments of emotion content in music and movement depend on configurations of stimulus features (<xref ref-type="bibr" rid="c82" hwp:id="xref-ref-82-4" hwp:rel-id="ref-82">Sievers et al., 2019</xref>, <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-9" hwp:rel-id="ref-84">2013</xref>) in much the same way that the solution to a puzzle depends on the configuration of the pieces. In other words, stimulus features and emotion judgments are naturally confounded. The crux of the <italic toggle="yes">environmental conjunctions</italic> hypothesis (A2) is that any combination of features that is sufficiently confounded with a target is useful for identifying that target. We argue that the brain uses such natural confounds as a shortcut to make task-relevant judgments: if sensory regions represent feature combinations that are perfectly confounded with a target’s identity, downstream inferential processing may not be necessary to identify the target.</p><p hwp:id="p-49">In demonstrating support for the <italic toggle="yes">environmental conjunctions</italic> hypothesis (A2), we do not mean to suggest that sensory brain areas alone support purely conceptual, symbolic, or cognitive labeling of emotions. Previous studies using context-dependent and narrative stimuli have demonstrated the importance of inferential processing for emotion perception (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Barrett, Mesquita, &amp; Gendron, 2011</xref>; <xref ref-type="bibr" rid="c86" hwp:id="xref-ref-86-4" hwp:rel-id="ref-86">Skerry &amp; Saxe, 2015</xref>). Further, inferential processing may play a role in the gradual tuning of perceptual systems for direct perception across development. Simple adaptations for perceiving cross-sensory magnitude or position information (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Deneux et al., 2019</xref>; <xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Murphy et al., 2020</xref>) or for adaptive signalling (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Hebets et al., 2016</xref>; <xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Huron, 2012</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Johnstone, 1996</xref>, <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">1997</xref>) may work in concert with learning (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Clark, 2013</xref>; <xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Kok, Brouwer, Gerven, &amp; Lange, 2013</xref>; <xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Lange, Heilbron, &amp; Kok, 2018</xref>; <xref ref-type="bibr" rid="c77" hwp:id="xref-ref-77-1" hwp:rel-id="ref-77">Saffran, Aslin, &amp; Newport, 1996</xref>), language (<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">Hoemann et al., 2019</xref>), and cultural evolution (<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Laland, Odling-Smee, &amp; Feldman, 2000</xref>) processes to support the development of task-relevant representations. This arrangement could flexibly accommodate culture-specific emotion concepts and display rules (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-3" hwp:rel-id="ref-31">Jack et al., 2012</xref>, <xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-3" hwp:rel-id="ref-32">2016</xref>; <xref ref-type="bibr" rid="c97" hwp:id="xref-ref-97-3" hwp:rel-id="ref-97">Yuki et al., 2007</xref>).</p><p hwp:id="p-50">Such tuning of sensory representations to the features used to communicate and categorize emotions shows that the need to identify such signals has exerted a profound shaping force on perceptual processes. We do not see or hear the actions of others as raw sense impressions first, later decode their conceptual content, and finally make an abstract emotion judgment. Rather, we begin accumulating evidence for emotion judgments from the lowest levels of sensory processing.</p></sec></sec><sec sec-type="supplementary-material" hwp:id="sec-13"><title hwp:id="title-23">Supporting information</title><supplementary-material position="float" orientation="portrait" hwp:id="DC1"><object-id pub-id-type="other" hwp:sub-type="slug">DC1</object-id><label>Supplementary material</label><media xlink:href="supplements/254961_file03.pdf" position="float" orientation="portrait" hwp:id="media-1"/></supplementary-material></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-24">Acknowledgements</title><p hwp:id="p-51">We thank Sam Nasatase, Matteo Visconti di Oleggio Castello, J. Swaroop Guntupalli, and Joshua Greene for helpful comments during the writing process, and Paulina Calcaterra, Rebecca Drapkin, Caitlyn Lee, Elizabeth Reynolds, Tshibambe Nathanael Tshimbombu, and Kelsey Wheeler for assistance collecting fMRI data. This research was supported in part by the Nelson A. Rockefeller Center for Public Policy at Dartmouth, the John Templeton Foundation, the Neukom Institute for Computational Science, the Vision Science to Applications (VISTA) program funded by the Canada First Research Excellence Fund (CFREF, 2016–2023) and by the Natural Sciences and Engineering Research Council of Canada.</p></ack><sec hwp:id="sec-14"><title hwp:id="title-25">Author contributions</title><p hwp:id="p-52">B. Sievers: Conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, project administration, software, vizualization, writing - original draft, review &amp; editing. C. Parkinson: Investigation, methodology, writing - review &amp; editing. P.J. Kohler: Methodology, writing - review &amp; editing. J.M. Hughes: Methodology, writing - review &amp; editing. S.V. Fogelson: Methodology, writing - review &amp; editing. T. Wheatley: Conceptualization, funding acquisition, resources, supervision, writing - review &amp; editing. Roles defined by the Contributor Roles Taxonomy, available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://casrai.org/credit/" ext-link-type="uri" xlink:href="https://casrai.org/credit/" hwp:id="ext-link-4">https://casrai.org/credit/</ext-link></p></sec><sec id="s4" hwp:id="sec-15"><title hwp:id="title-26">Detailed methods</title><sec id="s4a" hwp:id="sec-16"><title hwp:id="title-27">Lead contact</title><p hwp:id="p-53">Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Thalia Wheatley (<email hwp:id="email-2">thalia.p.wheatley@dartmouth.edu</email>).</p></sec><sec id="s4b" hwp:id="sec-17"><title hwp:id="title-28">Materials availability</title><p hwp:id="p-54">Stimuli have been deposited to osf.io (<xref ref-type="bibr" rid="c81" hwp:id="xref-ref-81-1" hwp:rel-id="ref-81">Sievers, 2021</xref>).</p></sec><sec id="s4c" hwp:id="sec-18"><title hwp:id="title-29">Data and code availability</title><p hwp:id="p-55">De-identified fMRI data have been deposited to OpenNeuro (<xref ref-type="bibr" rid="c83" hwp:id="xref-ref-83-1" hwp:rel-id="ref-83">Sievers et al., 2021</xref>). All original code has been deposited to osf.io (<xref ref-type="bibr" rid="c81" hwp:id="xref-ref-81-2" hwp:rel-id="ref-81">Sievers, 2021</xref>). Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.</p></sec><sec id="s4d" hwp:id="sec-19"><title hwp:id="title-30">Experimental model and subject details</title><p hwp:id="p-56">79 participants (47 female) were recruited from the Dartmouth College student community to participate in the emotion evaluation task (experiment 1). 20 of these participants (11 female) also participated in the fMRI of emotion viewing task (experiment 2). All fMRI participants were right-handed and had normal or corrected-to-normal vision. All participants provided written informed consent, and the study was approved by the Dartmouth College Committee for the Protection of Human Subjects.</p></sec><sec id="s4e" hwp:id="sec-20"><title hwp:id="title-31">Method details</title><sec id="s4e1" hwp:id="sec-21"><title hwp:id="title-32">Stimuli</title><p hwp:id="p-57">Emotion stimuli were generated using a model developed for a prior study (<xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-10" hwp:rel-id="ref-84">Sievers et al., 2013</xref>) that used movement across a number line to create both music (simple piano melodies) and animated movement (a bouncing ball). The model had five stimulus feature parameters: speed, irregularity/jitter, consonance/spikiness, ratio of big-to-small movements, and ratio of upward-to-downward movements. Each time the model was run, it probabilistically generated a new stimulus based on its current parameter settings. Participants in <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-11" hwp:rel-id="ref-84">Sievers et al. (2013)</xref> (music <italic toggle="yes">N</italic>=25; movement <italic toggle="yes">N</italic>=25; total <italic toggle="yes">N</italic>=50) used this model to communicate five prototype emotions: Angry, Happy, Peaceful, Sad, and Scared. Critically, participants were split into separate music and movement groups, each of which had no knowledge of the other. Participants chose similar music and movement parameter settings for each emotion across modalities, showing that music and movement share an underlying structure. The median parameter settings across music and movement from the United States participants in <xref ref-type="bibr" rid="c84" hwp:id="xref-ref-84-12" hwp:rel-id="ref-84">Sievers et al. (2013)</xref> were used to generate the stimuli used in the present studies. All stimuli are available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/kvbqm/" ext-link-type="uri" xlink:href="https://osf.io/kvbqm/" hwp:id="ext-link-5">https://osf.io/kvbqm/</ext-link>.</p><p hwp:id="p-58">In addition to the prototype emotions, mixed emotion stimuli were created by interpolating linearly between the parameter settings for each prototype emotion pair; 25%, 50%, and 75% mixes were used. We also added three putatively “neutral” or “non-emotional” parameter settings that were selected to be distant from all other stimuli. “Search One” and “Search Four” were selected by a Monte Carlo search algorithm, and consisted of extreme values for all five parameters. “Biggest Gap” was created by selecting the midpoint of the largest gap between the five prototype emotions and the stimulus feature parameter endpoints.</p><p hwp:id="p-59">For each prototype, mixed, and “non-emotional” parameter setting in each modality, we probabilistically generated 20 exemplars, for a total of 1,520 stimuli (38 emotions x 2 modalities x 20 exemplars). To eliminate the possibility of generating unusual outlier stimuli, each candidate exemplar was compared to a larger, separate sample of 5000 same-emotion exemplars, and was re-generated if found to be further than one standard deviation from the emotion mean along any parameter.</p></sec><sec id="s4e2" hwp:id="sec-22"><title hwp:id="title-33">Experiment 1 (emotion evaluation)</title><p hwp:id="p-60">Participants (<italic toggle="yes">N</italic>=79, 47 female) evaluated the emotion content of the stimuli. Stimuli were presented using a computer program developed using Max/MSP version 5 (<xref ref-type="bibr" rid="c98" hwp:id="xref-ref-98-1" hwp:rel-id="ref-98">Zicarelli, 1998</xref>) that displayed five slider bars, one for each emotion prototype (Angry, Happy, Peaceful, Sad, and Scared). The on-screen order of slider bars and emotion stimuli were randomized across participants. Participants viewed or listened to each stimulus at least three times, and were asked “to evaluate the amount or intensity of emotion expressed by the music or animation by positioning the slider bars.”</p></sec><sec id="s4e3" hwp:id="sec-23"><title hwp:id="title-34">Experiment 2 (fMRI of emotion viewing)</title><p hwp:id="p-61">During each fMRI run, participants (<italic toggle="yes">N</italic>=20, 11 female) viewed 18 randomly selected exemplars from each of the 76 stimulus classes described above. Each stimulus class was shown once per run, and participants completed 18 runs across 3 separate scanning sessions (~3 hours of scan time, 1,368 stimulus impressions). Each scan session was scheduled for approximately the same time of day, and no more than one week elapsed between scan sessions.</p><p hwp:id="p-62">Stimuli were truncated to 3s in duration and followed by fixation periods of randomly varying duration (range: 0.5s–20s). The ratio of stimulus presentation to fixation was 1:1. A Monte Carlo procedure was used to select separate, optimized stimulus presentation orderings and timings for each participant. This procedure used AFNI <monospace>make_random_timing.py</monospace> to generate thousands of possible stimulus timings, and AFNI <monospace>3dDeconvolve</monospace> to select the timings that best supported deconvolving unique patterns of brain activity for each stimulus. Stimuli were presented using PsychoPy version 1.84.2 (<xref ref-type="bibr" rid="c66" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Peirce, 2007</xref>). Participants were instructed to attend to the emotion content of the stimuli. During randomly interspersed catch trials (10 per run), participants used a button box to rate on a four-point scale whether the most recently presented stimulus had emotion content that was “more mixed” or “more pure.” To ensure familiarity with the stimuli, all fMRI participants had previously completed the emotion evaluation task.</p></sec><sec id="s4e4" hwp:id="sec-24"><title hwp:id="title-35">fMRI acquisition</title><p hwp:id="p-63">Participants were scanned at the Dartmouth Brain Imaging Center using a 3T Phillips Achieva Intera scanner with a 32-channel head coil. Functional images were acquired using an echo-planar sequence (35ms TE, 3000ms TR; 90° flip angle; 3×3×3mm resolution) with 192 dynamic scans per run. A high resolution T1-weighted anatomical scan (3.7 ms TE; 8200ms TR; .938x.938×1mm resolution) was acquired at the end of each scanning session. Sound was delivered using an over-ear headphone system. Foam padding was placed around participants’ heads to minimize motion.</p></sec><sec id="s4e5" hwp:id="sec-25"><title hwp:id="title-36">fMRI preprocessing</title><p hwp:id="p-64">Anatomical images were skull-stripped and aligned to the last TR of the last EPI image using AFNI <monospace>align_epi_anat.py</monospace>. EPI images were motion corrected and aligned to the last TR of the last EPI image using AFNI <monospace>3dvolreg</monospace>. Rigid body transformations for aligning participants’ anatomical and EPI images to the AFNI version of the MNI 152 ICBM template were calculated using AFNI <monospace>@auto_tlrc</monospace>. Alignment transformations were concatenated and applied in a single step using AFNI <monospace>3dAllineate</monospace>. EPI images were scaled to show percent signal change and concatenated. EPI images were not smoothed. TRs where inter-TR motion exceeded a euclidean norm threshold of .3 were censored, along with the immediately preceding TR. The general linear model was used to estimate BOLD-responses evoked by each of the 76 emotional stimulus classes using AFNI <monospace>3dREML-fit</monospace>. All six demeaned motion parameters as well as polynomial trends were included as regressors of no interest.</p></sec></sec><sec id="s4f" hwp:id="sec-26"><title hwp:id="title-37">Quantification and statistical analysis</title><sec id="s4f1" hwp:id="sec-27"><title hwp:id="title-38">Post hoc power analysis</title><p hwp:id="p-65">Because the present study is the first to use the reported paradigm, we did not conduct an <italic toggle="yes">a priori</italic>/prospective power analysis. Because accurate assessment of effect size is impossible without stable patterns, we prioritized having a large number of fMRI trials per participant. The number of trials per stimulus class per participant was determined by consulting studies that used similar analysis methods (MVPA/RSA). E.g., <xref ref-type="bibr" rid="c65" hwp:id="xref-ref-65-2" hwp:rel-id="ref-65">Peelen et al. (2010)</xref> used 12 trials per class per participant for 18 participants, <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-3" hwp:rel-id="ref-42">Kim et al. (2017)</xref> used 10 trials per class per participant for 20 participants, and the present study used 18 trials per class per participant for 20 participants. A <italic toggle="yes">post hoc</italic>/retrospective power analysis using G*Power 3.1 (<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Faul, Erdfelder, Lang, &amp; Buchner, 2007</xref>) showed that this provided 85% power for music trials and 99% power for animation trials for the effects reported in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3</xref>.</p></sec><sec id="s4f2" hwp:id="sec-28"><title hwp:id="title-39">Representational similarity analysis</title><p hwp:id="p-66">Representational similarity analysis (RSA) (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-4" hwp:rel-id="ref-45">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-4" hwp:rel-id="ref-47">2008</xref>) was conducted using PyMVPA (<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Hanke et al., 2009</xref>), Scikit-Learn (<xref ref-type="bibr" rid="c64" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">Pedregosa et al., 2012</xref>), NumPy (<xref ref-type="bibr" rid="c62" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">Oliphant, 2006</xref>), and SciPy (<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Jones, Oliphant, &amp; Peterson, 2001</xref>). Stimulus feature representational distance matrices (RDMs) for each parameter (speed, irregularity/jitter, consonance/spikiness, ratio of big-to-small movements, ratio of upward-to-downward movements) were created by aggregating the Euclidean distances between the mean slider bar settings for each pair of emotions, including mixed emotions. Both music and animation stimuli were created using the same slider bar settings for each emotion, making it unnecessary to create modality-specific feature RDMs. Emotion RDMs were created by aggregating the Euclidean distances between the mean of each emotion judgment parameter in experiment 1 (Angry, Happy, Peaceful, Sad, and Scared) for each pair of emotions, including mixed emotions. Emotion judgments were averaged across music and animation, making it unnecessary to create modality-specific emotion judgment RDMs. Intermodal RDMs were built by calculating the full multi-modality RDM including both music and animation stimuli and selecting its lower-left square region (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6B</xref>).</p><p hwp:id="p-67">Representational similarity analysis was conducted separately for music trials, animation trials, and (for the intermodal analysis) music and animation trials together. Each analysis used a spherical searchlight with a 3-voxel (9mm) radius. In each searchlight sphere, music and animation neural RDMs were created by aggregating the ranked correlation distances (1-Spearman’s ρ) between the estimated stimulus-evoked pattern of BOLD activation for each emotion. The use of correlation distance ensured that the analysis would not mistake differences in the mean level of BOLD activity across music and animation trials for differences in representational similarity. Intermodal neural RDMs were created as described above, using neural data instead of stimulus features or emotion judgments (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Figure 6B</xref>). The fit of the model to stimulus-evoked patterns of BOLD activation was assessed using multiple regression, with the ranked model RDMs as predictors and the neural RDM as the target. This produced coefficient of determination (<italic toggle="yes">R</italic><sup>2</sup>) and β weight maps for each participant and each analysis. <italic toggle="yes">R</italic><sup>2</sup> values were adjusted using a permutation approach (similar to that of <xref ref-type="bibr" rid="c67" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Peres-Neto, Legendre, Dray, &amp; Borcard, 2006</xref>): Multiple regression was performed an additional 1000 times with randomly selected permutations of each predictor, and the mean <italic toggle="yes">R</italic><sup>2</sup> from this null distribution was subtracted from the reported <italic toggle="yes">R</italic><sup>2</sup> values <inline-formula hwp:id="inline-formula-14"><alternatives hwp:id="alternatives-14"><inline-graphic xlink:href="254961v4_inline14.gif" hwp:id="inline-graphic-14"/></alternatives></inline-formula>. Multiple regression β weights reflect only the unique contribution of each predictor, resulting in β weight maps that do not reflect the shared contributions of correlated predictors. To assess the contribution of individual predictors we calculated the ranked correlation (Spearman’s ρ) of each predictor to the neural RDM.</p><p hwp:id="p-68">All group-level statistics (including <inline-formula hwp:id="inline-formula-15"><alternatives hwp:id="alternatives-15"><inline-graphic xlink:href="254961v4_inline15.gif" hwp:id="inline-graphic-15"/></alternatives></inline-formula>, β weights, Spearman’s ρ, p-values, and any other values reported unless otherwise noted) were corrected for multiple comparisons using a maximum cluster mass sign-flipping permutation test performed with FSL randomise (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Jenkinson, Beckmann, Behrens, Woolrich, &amp; Smith, 2012</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Nichols &amp; Holmes, 2001</xref>), with a cluster-determining threshold of p=.01 and a family-wise error rate of .05.</p></sec><sec id="s4f3" hwp:id="sec-29"><title hwp:id="title-40">Intermodal RSA</title><p hwp:id="p-69">Intermodal RSA differed from the RSA analysis described above in that both the neural target RDM and the predictor RDMs used only between-modality distances, corresponding to the lower-left square region of the larger triangular RDM created using stimuli from both modalities (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Figure 6B</xref>). If activity in a brain area was unrelated to stimuli presented in its non-preferred modality, then the intermodal neural RDM should be uncorrelated with the intermodal model RDMs. However, if a brain area was even weakly representing emotion content across modalities, then the intermodal neural RDM should be correlated with the intermodal model RDMs. Note that because this analysis only considered between-modality distances, it could not in principle have identified any modality-specific activity.</p></sec><sec id="s4f4" hwp:id="sec-30"><title hwp:id="title-41">Model-free similarity analysis</title><p hwp:id="p-70">To rule out the possibility that the identified brain regions were a good fit for the stimulus features and emotion judgments in the reported model, but did not truly share a representational geometry (i.e., were not directly similar to each other), we performed a permutation test of inter-region representational similarity. This test assessed whether the representations at the locations of peak model fit were more similar than representations at randomly selected locations. Analogously, the claim “San Francisco and Oakland are close to each other,” is weaker than the claim “San Francisco and Oakland are closer to each other than 95% of all pairs of American cities.” To build a null distribution, we randomly selected 2000 pairs of coordinates in the right hemisphere of the brain. For each coordinate pair, we measured the ranked correlation (Spearman’s ρ) of the mean neural RDM for music trials at the first coordinate and the mean neural RDM for animation trials at the second coordinate. The mean inter-region similarity in the null distribution was ρ=−.007, whereas the inter-region similarity at the locations of peak model fit was ρ=.68 (p&lt;.001), more similar than any pair of coordinates in the null distribution.</p></sec><sec id="s4f5" hwp:id="sec-31"><title hwp:id="title-42">Noise ceiling</title><p hwp:id="p-71">The upper and lower bounds of the noise ceiling were calculated using an approach based on <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Nili et al. (2014)</xref>, but adapted for use with multiple regression. The approach described by <xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-2" hwp:rel-id="ref-59">Nili et al. (2014)</xref> depends on a simple principle: for any dataset, the model that accounts for the most variance in the data will always be derived from the data itself. For correlation, this best-fitting model is the mean of the data. Given measurement error and individual differences across the dataset, no model could possibly outperform the mean, and so the model fit of the mean establishes a ceiling against which other models can be usefully compared. Analogously, for a multiple regression model with n predictors, the best-fitting model is the mean of the data along with the top n predictors identified using principal component analysis (PCA). No multiple regression model with the same number of predictors could possibly outperform this mean-and-PCA model. In the present study, the upper bound of the noise ceiling was calculated at each searchlight center by performing a multiple regression analysis that used the mean neural RDM and the top 10 principal components of the neural RDM as predictors. The lower bound of the noise ceiling was calculated using a leave-one-subject-out cross-validation approach: For each subject, the same multiple regression procedure was applied, but the mean neural RDM and the top 10 principal components were calculated with that subject left out.</p></sec><sec id="s4f6" hwp:id="sec-32"><title hwp:id="title-43">Overlap maps</title><p hwp:id="p-72">Overlap maps were created for each participant by identifying voxels where both music and animation model fits were significant at the individual level (permutation p&lt;.05, uncorrected). Overlap maps were set to 1 if both model fits were significant, and 0 otherwise. Multiple comparisons correction of the overlap maps was performed at the group level (CDT=.01; FWER p=.05; see below), testing the proportion of individuals that showed overlap in a region against the null hypothesis that no participants showed overlap in that region.</p></sec><sec id="s4f7" hwp:id="sec-33"><title hwp:id="title-44">Multiple comparisons correction</title><p hwp:id="p-73">Group level maps were calculated and corrected for multiple comparisons using a maximum cluster mass sign-flipping permutation test FSL randomise (<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-2" hwp:rel-id="ref-34">Jenkinson et al., 2012</xref>; <xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-2" hwp:rel-id="ref-58">Nichols &amp; Holmes, 2001</xref>) (cluster-determining threshold p=.01; family-wise error rate p=.05). Tests for <inline-formula hwp:id="inline-formula-16"><alternatives hwp:id="alternatives-16"><inline-graphic xlink:href="254961v4_inline16.gif" hwp:id="inline-graphic-16"/></alternatives></inline-formula> were 1-sided. Tests for β weights and Spearman’s ρ were two-sided. Maps were visualized using Nilearn (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Abraham et al., 2014</xref>) and AFNI SUMA (<xref ref-type="bibr" rid="c75" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">Saad, Reynolds, Argall, Japee, &amp; Cox, 2004</xref>).</p></sec><sec id="s4f8" hwp:id="sec-34"><title hwp:id="title-45">Emotion judgments permutation procedure</title><p hwp:id="p-74">For each emotion, we averaged participants’ emotion judgment ratings, yielding a class mean. We then calculated the Euclidean distance of each individual judgment to this mean, scaled by the maximum possible distance (determined by the limits of each slider), yielding a distribution of scaled distances to the mean for each stimulus class. A null distribution of scaled distances to the class means was created by applying this procedure 2000 times, each with a different permutation of the emotion labels over the whole dataset. Welch’s independent samples <italic toggle="yes">t</italic>-test was applied to test whether the observed distributions of scaled distances to class means differed from the null. This approach was chosen because it accounts for the simultaneous use of five rating scales and conservatively respects the dependency structure of the data.</p></sec></sec></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-46">Bibliography</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="other" citation-type="journal" ref:id="254961v4.1" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Abraham A."><surname>Abraham</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pedregosa F."><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eickenberg M."><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gervais P."><surname>Gervais</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Muller A."><surname>Muller</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kossaifi J."><surname>Kossaifi</surname>, <given-names>J.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Varoquaux G."><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-2">Machine Learning for Neuroimaging with Scikit-Learn</article-title>, <volume>8</volume>(<month>February</month>), <fpage>1</fpage>–<lpage>10</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3389/fninf.2014.00014" ext-link-type="uri" xlink:href="https://doi.org/10.3389/fninf.2014.00014" hwp:id="ext-link-6">https://doi.org/10.3389/fninf.2014.00014</ext-link></citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Adolphs R."><surname>Adolphs</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tranel D."><surname>Tranel</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Damasio H."><surname>Damasio</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Damasio A."><surname>Damasio</surname>, <given-names>A.</given-names></string-name> (<year>1994</year>). <article-title hwp:id="article-title-3">Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala</article-title>. <source hwp:id="source-1">Nature</source>, <volume>372</volume>(<issue>6507</issue>), <fpage>669</fpage>–<lpage>672</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/372669a0" ext-link-type="uri" xlink:href="https://doi.org/10.1038/372669a0" hwp:id="ext-link-7">https://doi.org/10.1038/372669a0</ext-link></citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="book" citation-type="book" ref:id="254961v4.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Baily J."><surname>Baily</surname>, <given-names>J.</given-names></string-name> (<year>1985</year>). <chapter-title>Music Structure and Human Movement</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Howell P."><given-names>P.</given-names> <surname>Howell</surname></string-name>, <string-name name-style="western" hwp:sortable="Cross I."><given-names>I.</given-names> <surname>Cross</surname></string-name>, &amp; <string-name name-style="western" hwp:sortable="West R."><given-names>R.</given-names> <surname>West</surname></string-name></person-group> (Eds.), <source hwp:id="source-2">Musical structure and cognition</source> (pp. <fpage>237</fpage>–<lpage>258</lpage>). <publisher-loc>London</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Barrett L. F."><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lindquist K. A."><surname>Lindquist</surname>, <given-names>K. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gendron M."><surname>Gendron</surname>, <given-names>M.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-4">Language as context for the perception of emotion</article-title>. <source hwp:id="source-3">Trends in Cognitive Sciences</source>, <volume>11</volume>(<issue>8</issue>), <fpage>327</fpage>–<lpage>332</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.tics.2007.06.003" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2007.06.003" hwp:id="ext-link-8">https://doi.org/10.1016/j.tics.2007.06.003</ext-link></citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Barrett L. F."><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mesquita B."><surname>Mesquita</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gendron M."><surname>Gendron</surname>, <given-names>M.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-5">Context in Emotion Perception</article-title>. <source hwp:id="source-4">Current Directions in Psychological Science</source>, <volume>20</volume>(<issue>5</issue>), <fpage>286</fpage>–<lpage>290</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1177/0963721411422522" ext-link-type="uri" xlink:href="https://doi.org/10.1177/0963721411422522" hwp:id="ext-link-9">https://doi.org/10.1177/0963721411422522</ext-link></citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Beauchamp M. S."><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee K."><surname>Lee</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Argall B."><surname>Argall</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Martin A."><surname>Martin</surname>, <given-names>A.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-6">Integration of auditory and visual information about objects in superior temporal sulcus</article-title>. <source hwp:id="source-5">Neuron</source>, <volume>41</volume>, <fpage>809</fpage>–<lpage>823</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/S0896-6273(04)00070-4" ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(04)00070-4" hwp:id="ext-link-10">https://doi.org/10.1016/S0896-6273(04)00070-4</ext-link></citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Belyk M."><surname>Belyk</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Brown S."><surname>Brown</surname>, <given-names>S.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-7">Perception of affective and linguistic prosody: An ALE meta-analysis of neuroimaging studies</article-title>. <source hwp:id="source-6">Social Cognitive and Affective Neuroscience</source>, <volume>9</volume>, <fpage>1395</fpage>–<lpage>1403</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/scan/nst124" ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nst124" hwp:id="ext-link-11">https://doi.org/10.1093/scan/nst124</ext-link></citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Bulkin D. A."><surname>Bulkin</surname>, <given-names>D. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Groh J. M."><surname>Groh</surname>, <given-names>J. M.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-8">Seeing sounds: visual and auditory interactions in the brain</article-title>. <source hwp:id="source-7">Current Opinion in Neurobiology</source>, <volume>16</volume>(<issue>4</issue>), <fpage>415</fpage>–<lpage>419</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.conb.2006.06.008" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2006.06.008" hwp:id="ext-link-12">https://doi.org/10.1016/j.conb.2006.06.008</ext-link></citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Calder A. J."><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lawrence A. D."><surname>Lawrence</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Young A. W."><surname>Young</surname>, <given-names>A. W.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-9">Neuropsychology of Fear and Loathing</article-title>. <source hwp:id="source-8">Nature Reviews Neuroscience</source>, <volume>2</volume>(<issue>5</issue>), <fpage>352</fpage>–<lpage>363</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/35072584" ext-link-type="uri" xlink:href="https://doi.org/10.1038/35072584" hwp:id="ext-link-13">https://doi.org/10.1038/35072584</ext-link></citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Cappe C."><surname>Cappe</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barone P."><surname>Barone</surname>, <given-names>P.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-10">Heteromodal connections supporting multisensory integration at low levels of cortical processing in the monkey</article-title>. <source hwp:id="source-9">European Journal of Neuroscience</source>, <volume>22</volume>(<issue>11</issue>), <fpage>2886</fpage>–<lpage>2902</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1111/j.1460-9568.2005.04462.x" ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2005.04462.x" hwp:id="ext-link-14">https://doi.org/10.1111/j.1460-9568.2005.04462.x</ext-link></citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><citation publication-type="book" citation-type="book" ref:id="254961v4.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Chemero A."><surname>Chemero</surname>, <given-names>A.</given-names></string-name> (<year>2006</year>). <chapter-title>Information and direct perception: a new approach</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Farias P."><given-names>P.</given-names> <surname>Farias</surname></string-name> &amp; <string-name name-style="western" hwp:sortable="Queiroz J."><given-names>J.</given-names> <surname>Queiroz</surname></string-name></person-group> (Eds.), <source hwp:id="source-10">Advanced issues in cognitive science and semiotics</source> (pp. <fpage>59</fpage>–<lpage>72</lpage>). <publisher-loc>Aachen</publisher-loc>: <publisher-name>Shaker Verlag</publisher-name>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Chikazoe J."><surname>Chikazoe</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee D. H."><surname>Lee</surname>, <given-names>D. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Anderson A. K."><surname>Anderson</surname>, <given-names>A. K.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-11">Population coding of affect across stimuli, modalities and individuals</article-title>. <source hwp:id="source-11">Nature Neuroscience</source>, <volume>17</volume>(<issue>8</issue>), <fpage>1114</fpage>–<lpage>1122</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/nn.3749" ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3749" hwp:id="ext-link-15">https://doi.org/10.1038/nn.3749</ext-link></citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Clark A."><surname>Clark</surname>, <given-names>A.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-12">Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title>. <source hwp:id="source-12">The Behavioral and Brain Sciences</source>, <volume>36</volume>(<issue>3</issue>), <fpage>181</fpage>–<lpage>204</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1017/S0140525X12000477" ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X12000477" hwp:id="ext-link-16">https://doi.org/10.1017/S0140525X12000477</ext-link></citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Cowen A. S."><surname>Cowen</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Keltner D."><surname>Keltner</surname>, <given-names>D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-13">Self-report captures 27 distinct categories of emotion bridged by continuous gradients</article-title>. <source hwp:id="source-13">Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>38</issue>), <fpage>E7900</fpage>–<lpage>E7909</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.1702247114" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1702247114" hwp:id="ext-link-17">https://doi.org/10.1073/pnas.1702247114</ext-link></citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Deneux T."><surname>Deneux</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harrell E. R."><surname>Harrell</surname>, <given-names>E. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kempf A."><surname>Kempf</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ceballo S."><surname>Ceballo</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Filipchuk A."><surname>Filipchuk</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bathellier B."><surname>Bathellier</surname>, <given-names>B.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-14">Context-dependent signaling of coincident auditory and visual events in primary visual cortex</article-title>. <source hwp:id="source-14">eLife</source>, <volume>8</volume>, <fpage>1</fpage>–<lpage>23</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.7554/eLife.44006" ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.44006" hwp:id="ext-link-18">https://doi.org/10.7554/eLife.44006</ext-link></citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Destrieux C."><surname>Destrieux</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fischl B."><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dale A."><surname>Dale</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Halgren E."><surname>Halgren</surname>, <given-names>E.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-15">Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title>. <source hwp:id="source-15">NeuroImage</source>, <volume>53</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>15</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuroimage.2010.06.010" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2010.06.010" hwp:id="ext-link-19">https://doi.org/10.1016/j.neuroimage.2010.06.010</ext-link></citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Falchier A."><surname>Falchier</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Clavagnier S."><surname>Clavagnier</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barone P."><surname>Barone</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kennedy H."><surname>Kennedy</surname>, <given-names>H.</given-names></string-name> (<year>2002</year>). <article-title hwp:id="article-title-16">Anatomical evidence of multimodal integration in primate striate cortex</article-title>. <source hwp:id="source-16">The Journal of Neuroscience</source>, <volume>22</volume>(<issue>13</issue>), <fpage>5749</fpage>–<lpage>5759</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/20026562" ext-link-type="uri" xlink:href="https://doi.org/20026562" hwp:id="ext-link-20">https://doi.org/20026562</ext-link></citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Faul F."><surname>Faul</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erdfelder E."><surname>Erdfelder</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lang A.-G."><surname>Lang</surname>, <given-names>A.-G.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Buchner A."><surname>Buchner</surname>, <given-names>A.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-17">G*Power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>. <source hwp:id="source-17">Behavior Research Methods</source>, <volume>39</volume>(<issue>2</issue>), <fpage>175</fpage>–<lpage>91</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3758/bf03193146" ext-link-type="uri" xlink:href="https://doi.org/10.3758/bf03193146" hwp:id="ext-link-21">https://doi.org/10.3758/bf03193146</ext-link></citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Filippi P."><surname>Filippi</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Congdon J. V."><surname>Congdon</surname>, <given-names>J. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoang J."><surname>Hoang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bowling D. L."><surname>Bowling</surname>, <given-names>D. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reber S. A."><surname>Reber</surname>, <given-names>S. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pašukonis A."><surname>Pašukonis</surname>, <given-names>A.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Güntürkün O."><surname>Güntürkün</surname>, <given-names>O.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-18">Humans recognize emotional arousal in vocalizations across all classes of terrestrial vertebrates: evidence for acoustic universals</article-title>. <source hwp:id="source-18">Proceedings of the Royal Society of London B: Biological Sciences</source>, <volume>284</volume>(<issue>1859</issue>), <fpage>1</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Fritz T."><surname>Fritz</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jentschke S."><surname>Jentschke</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gosselin N."><surname>Gosselin</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sammler D."><surname>Sammler</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peretz I."><surname>Peretz</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Turner R."><surname>Turner</surname>, <given-names>R.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Koelsch S."><surname>Koelsch</surname>, <given-names>S.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-19">Report Universal Recognition of Three Basic Emotions in Music</article-title>. <source hwp:id="source-19">Current Biology</source>, <volume>19</volume>(<issue>7</issue>), <fpage>573</fpage>–<lpage>576</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.cub.2009.02.058" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2009.02.058" hwp:id="ext-link-22">https://doi.org/10.1016/j.cub.2009.02.058</ext-link></citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Gallagher S."><surname>Gallagher</surname>, <given-names>S.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-20">Direct perception in the intersubjective context</article-title>. <source hwp:id="source-20">Consciousness and Cognition</source>, <volume>17</volume>(<issue>2</issue>), <fpage>535</fpage>–<lpage>543</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.concog.2008.03.003" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.concog.2008.03.003" hwp:id="ext-link-23">https://doi.org/10.1016/j.concog.2008.03.003</ext-link></citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Gelstein S."><surname>Gelstein</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yeshurun Y."><surname>Yeshurun</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rozenkrantz L."><surname>Rozenkrantz</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shushan S."><surname>Shushan</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Frumin I."><surname>Frumin</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Roth Y."><surname>Roth</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Sobel N."><surname>Sobel</surname>, <given-names>N.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-21">Human tears contain a chemosignal</article-title>. <source hwp:id="source-21">Science</source>, <volume>331</volume>(<issue>6014</issue>), <fpage>226</fpage>–<lpage>230</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1126/science.1198331" ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1198331" hwp:id="ext-link-24">https://doi.org/10.1126/science.1198331</ext-link></citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2 xref-ref-23-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Gendron M."><surname>Gendron</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Roberson D."><surname>Roberson</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vyver J. M. van der"><surname>Vyver</surname>, <given-names>J. M. van der</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barrett L. F."><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-22">Perceptions of emotion from facial expressions are not culturally universal: Evidence from a remote culture</article-title>. <source hwp:id="source-22">Emotion</source>, <volume>14</volume>(<issue>2</issue>), <fpage>251</fpage>–<lpage>262</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/a0036052" ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0036052" hwp:id="ext-link-25">https://doi.org/10.1037/a0036052</ext-link></citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Ghazanfar A. A."><surname>Ghazanfar</surname>, <given-names>A. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schroeder C. E."><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-23">Is neocortex essentially multisensory?</article-title> <source hwp:id="source-23">Trends in Cognitive Sciences</source>, <volume>10</volume>(<issue>6</issue>), <fpage>278</fpage>–<lpage>285</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.tics.2006.04.008" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2006.04.008" hwp:id="ext-link-26">https://doi.org/10.1016/j.tics.2006.04.008</ext-link></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><citation publication-type="book" citation-type="book" ref:id="254961v4.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Gopnik A."><surname>Gopnik</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wellman H. M."><surname>Wellman</surname>, <given-names>H. M.</given-names></string-name> (<year>1994</year>). <chapter-title>The theory theory</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-3"><string-name name-style="western" hwp:sortable="Hirschfeld L. A."><given-names>L. A.</given-names> <surname>Hirschfeld</surname></string-name> &amp; <string-name name-style="western" hwp:sortable="Gelman S. A."><given-names>S. A.</given-names> <surname>Gelman</surname></string-name></person-group> (Eds.), <source hwp:id="source-24">Mapping the mind: Domain specificity in cognition and culture</source> (pp. <fpage>257</fpage>–<lpage>293</lpage>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Gordon R. M."><surname>Gordon</surname>, <given-names>R. M.</given-names></string-name> (<year>1986</year>). <article-title hwp:id="article-title-24">Folk Psychology as Simulation</article-title>. <source hwp:id="source-25">Mind &amp; Language</source>, <volume>1</volume>(<issue>2</issue>), <fpage>158</fpage>–<lpage>171</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1111/j.1468-0017.1986.tb00324.x" ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1468-0017.1986.tb00324.x" hwp:id="ext-link-27">https://doi.org/10.1111/j.1468-0017.1986.tb00324.x</ext-link></citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Hanke M."><surname>Hanke</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Halchenko Y. O."><surname>Halchenko</surname>, <given-names>Y. O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sederberg P. B."><surname>Sederberg</surname>, <given-names>P. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hanson S. J."><surname>Hanson</surname>, <given-names>S. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haxby J. V."><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Pollmann S."><surname>Pollmann</surname>, <given-names>S.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-25">PyMVPA: A python toolbox for multivariate pattern analysis of fMRI data</article-title>. <source hwp:id="source-26">Neuroinformatics</source>, <volume>7</volume>(<issue>1</issue>), <fpage>37</fpage>–<lpage>53</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1007/s12021-008-9041-y" ext-link-type="uri" xlink:href="https://doi.org/10.1007/s12021-008-9041-y" hwp:id="ext-link-28">https://doi.org/10.1007/s12021-008-9041-y</ext-link></citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Hebets E. A."><surname>Hebets</surname>, <given-names>E. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barron A. B."><surname>Barron</surname>, <given-names>A. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balakrishnan C. N."><surname>Balakrishnan</surname>, <given-names>C. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hauber M. E."><surname>Hauber</surname>, <given-names>M. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mason P. H."><surname>Mason</surname>, <given-names>P. H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Hoke K. L."><surname>Hoke</surname>, <given-names>K. L.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-26">A systems approach to animal communication</article-title>. <source hwp:id="source-27">Proceedings of the Royal Society B: Biological Sciences</source>, <volume>283</volume>(<issue>1826</issue>), <fpage>20152889</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1098/rspb.2015.2889" ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.2015.2889" hwp:id="ext-link-29">https://doi.org/10.1098/rspb.2015.2889</ext-link></citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Hoemann K."><surname>Hoemann</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xu F."><surname>Xu</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barrett L. F."><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-27">Emotion words, emotion concepts, and emotional development in children: A constructionist hypothesis</article-title>. <source hwp:id="source-28">Developmental Psychology</source>, <volume>55</volume>(<issue>9</issue>), <fpage>1830</fpage>–<lpage>1849</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/dev0000686" ext-link-type="uri" xlink:href="https://doi.org/10.1037/dev0000686" hwp:id="ext-link-30">https://doi.org/10.1037/dev0000686</ext-link></citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.30" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Huron D."><surname>Huron</surname>, <given-names>D.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-28">Understanding Music-Related Emotion: Lessons from Ethology</article-title>. <source hwp:id="source-29">Proceedings of the 12th International Conference on Music Perception and Cognition</source>, <fpage>473</fpage>–<lpage>481</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2 xref-ref-31-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Jack R. E."><surname>Jack</surname>, <given-names>R. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Caldara R."><surname>Caldara</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schyns P. G."><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-29">Internal representations reveal cultural diversity in expectations of facial expressions of emotion</article-title>. <source hwp:id="source-30">Journal of Experimental Psychology: General</source>, <volume>141</volume>(<issue>1</issue>), <fpage>19</fpage>–<lpage>25</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/a0023463" ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0023463" hwp:id="ext-link-31">https://doi.org/10.1037/a0023463</ext-link></citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2 xref-ref-32-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Jack R. E."><surname>Jack</surname>, <given-names>R. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sun W."><surname>Sun</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Delis I."><surname>Delis</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garrod O. G. B."><surname>Garrod</surname>, <given-names>O. G. B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schyns P. G."><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-30">Four not six: Revealing culturally common facial expressions of emotion</article-title>. <source hwp:id="source-31">Journal of Experimental Psychology: General</source>, <volume>145</volume>(<issue>6</issue>), <fpage>708</fpage>–<lpage>730</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/xge0000162" ext-link-type="uri" xlink:href="https://doi.org/10.1037/xge0000162" hwp:id="ext-link-32">https://doi.org/10.1037/xge0000162</ext-link></citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Jackson J. C."><surname>Jackson</surname>, <given-names>J. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Watts J."><surname>Watts</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henry T. R."><surname>Henry</surname>, <given-names>T. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="List J.-m."><surname>List</surname>, <given-names>J.-m.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Forkel R."><surname>Forkel</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mucha P. J."><surname>Mucha</surname>, <given-names>P. J.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Lindquist K. A."><surname>Lindquist</surname>, <given-names>K. A.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-31">Emotion semantics show both cultural variation and universal structure</article-title>. <source hwp:id="source-32">Science</source>, <volume>366</volume>(<month>December</month>), <fpage>1517</fpage>–<lpage>1522</lpage>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1 xref-ref-34-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Jenkinson M."><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Beckmann C. F."><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Behrens T. E. J."><surname>Behrens</surname>, <given-names>T. E. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Woolrich M. W."><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Smith S. M."><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-32">FSL</article-title>. <source hwp:id="source-33">NeuroImage</source>, <volume>62</volume>, <fpage>782</fpage>–<lpage>790</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuroimage.2011.09.015" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.09.015" hwp:id="ext-link-33">https://doi.org/10.1016/j.neuroimage.2011.09.015</ext-link></citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Jiahui G."><surname>Jiahui</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garrido L."><surname>Garrido</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu R. R."><surname>Liu</surname>, <given-names>R. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Susilo T."><surname>Susilo</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barton J. J. S."><surname>Barton</surname>, <given-names>J. J. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Duchaine B."><surname>Duchaine</surname>, <given-names>B.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-33">Normal voice processing after posterior superior temporal sulcus lesion</article-title>. <source hwp:id="source-34">Neuropsychologia</source>, <volume>105</volume>(<month>September</month> 2016), <fpage>215</fpage>–<lpage>222</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuropsychologia.2017.03.008" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2017.03.008" hwp:id="ext-link-34">https://doi.org/10.1016/j.neuropsychologia.2017.03.008</ext-link></citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Johnstone R. A."><surname>Johnstone</surname>, <given-names>R. A.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-34">Multiple displays in animal communication: ‘backup signals’ and ‘multiple messages’</article-title>. <source hwp:id="source-35">Proceedings of the Royal Society B: Biological Sciences</source>, <volume>351</volume>, <fpage>329</fpage>–<lpage>338</lpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="book" citation-type="book" ref:id="254961v4.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Johnstone R. A."><surname>Johnstone</surname>, <given-names>R. A.</given-names></string-name> (<year>1997</year>). <chapter-title>The evolution of animal signals</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-4"><string-name name-style="western" hwp:sortable="Krebs J. R."><given-names>J. R.</given-names> <surname>Krebs</surname></string-name> &amp; <string-name name-style="western" hwp:sortable="Davies N. B."><given-names>N. B.</given-names> <surname>Davies</surname></string-name></person-group> (Eds.), <source hwp:id="source-36">Behavioral ecology</source> (pp. <fpage>155</fpage>–<lpage>178</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="website" citation-type="web" ref:id="254961v4.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Jones E."><surname>Jones</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Oliphant E."><surname>Oliphant</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Peterson P."><surname>Peterson</surname>, <given-names>P.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-35">SciPy: Open Source Scientific Tools for Python</article-title>. Retrieved from <ext-link l:rel="related" l:ref-type="uri" l:ref="https://scipy.org/" ext-link-type="uri" xlink:href="https://scipy.org/" hwp:id="ext-link-35">https://scipy.org/</ext-link></citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Kaeppler A. L."><surname>Kaeppler</surname>, <given-names>A. L.</given-names></string-name> (<year>1978</year>). <article-title hwp:id="article-title-36">Dance in anthropological perspective</article-title>. <source hwp:id="source-37">Annual Review of Anthropology</source>, <volume>7</volume>, <fpage>31</fpage>–<lpage>49</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1 xref-ref-40-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Kawakami A."><surname>Kawakami</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Furukawa K."><surname>Furukawa</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Katahira K."><surname>Katahira</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Okanoya K."><surname>Okanoya</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-37">Sad music induces pleasant emotion</article-title>. <source hwp:id="source-38">Frontiers in Psychology</source>, <volume>4</volume>(<issue>311</issue>), <fpage>1</fpage>–<lpage>15</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3389/fpsyg.2013.00311" ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00311" hwp:id="ext-link-36">https://doi.org/10.3389/fpsyg.2013.00311</ext-link></citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Kayser C."><surname>Kayser</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Logothetis N. K."><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-38">Do early sensory cortices integrate cross-modal information?</article-title> <source hwp:id="source-39">Brain Structure and Function</source>, <volume>212</volume>(<issue>2</issue>), <fpage>121</fpage>–<lpage>132</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1007/s00429-007-0154-0" ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00429-007-0154-0" hwp:id="ext-link-37">https://doi.org/10.1007/s00429-007-0154-0</ext-link></citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2 xref-ref-42-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Kim J."><surname>Kim</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shinkareva S. V."><surname>Shinkareva</surname>, <given-names>S. V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wedell D. H."><surname>Wedell</surname>, <given-names>D. H.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-39">Representations of modality-general valence for videos and music derived from fMRI data</article-title>. <source hwp:id="source-40">NeuroImage</source>, <volume>148</volume>(<month>January</month>), <fpage>42</fpage>–<lpage>54</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuroimage.2017.01.002" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.01.002" hwp:id="ext-link-38">https://doi.org/10.1016/j.neuroimage.2017.01.002</ext-link></citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Kok P."><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brouwer G. J."><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gerven M. A. J. van"><surname>Gerven</surname>, <given-names>M. A. J. van</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Lange F. P. de."><surname>Lange</surname>, <given-names>F. P. de.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-40">Prior expectations bias sensory representations in visual cortex</article-title>. <source hwp:id="source-41">Journal of Neuroscience</source>, <volume>33</volume>(<issue>41</issue>), <fpage>16275</fpage>–<lpage>16284</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.0742-13.2013" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0742-13.2013" hwp:id="ext-link-39">https://doi.org/10.1523/JNEUROSCI.0742-13.2013</ext-link></citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.44" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Kreifelts B."><surname>Kreifelts</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ethofer T."><surname>Ethofer</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grodd W."><surname>Grodd</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erb M."><surname>Erb</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wildgruber D."><surname>Wildgruber</surname>, <given-names>D.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-41">Audiovisual integration of emotional signals in voice and face: An event-related fMRI study</article-title>. <source hwp:id="source-42">NeuroImage</source>, <volume>37</volume>(<issue>4</issue>), <fpage>1445</fpage>–<lpage>1456</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.neuroimage.2007.06.020" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2007.06.020" hwp:id="ext-link-40">https://doi.org/10.1016/j.neuroimage.2007.06.020</ext-link></citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2 xref-ref-45-3 xref-ref-45-4"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goebel R."><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bandettini P."><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-42">Information-based functional brain mapping</article-title>. <source hwp:id="source-43">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>103</volume>(<issue>10</issue>), <fpage>3863</fpage>–<lpage>8</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.0600244103" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0600244103" hwp:id="ext-link-41">https://doi.org/10.1073/pnas.0600244103</ext-link></citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2 xref-ref-46-3 xref-ref-46-4"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kievit R. a."><surname>Kievit</surname>, <given-names>R. a.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-43">Representational geometry: integrating cognition, computation, and the brain</article-title>. <source hwp:id="source-44">Trends in Cognitive Sciences</source>, <volume>17</volume>(<issue>8</issue>), <fpage>401</fpage>–<lpage>12</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.tics.2013.06.007" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.06.007" hwp:id="ext-link-42">https://doi.org/10.1016/j.tics.2013.06.007</ext-link></citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2 xref-ref-47-3 xref-ref-47-4"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mur M."><surname>Mur</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Bandettini P."><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> (<year>2008</year>). <article-title hwp:id="article-title-44">Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source hwp:id="source-45">Frontiers in Systems Neuroscience</source>, <volume>2</volume>(<month>November</month>), <fpage>4</fpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.3389/neuro.06.004.2008" ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.06.004.2008" hwp:id="ext-link-43">https://doi.org/10.3389/neuro.06.004.2008</ext-link></citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Laland K. N."><surname>Laland</surname>, <given-names>K. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Odling-Smee J."><surname>Odling-Smee</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Feldman M. W."><surname>Feldman</surname>, <given-names>M. W.</given-names></string-name> (<year>2000</year>). <article-title hwp:id="article-title-45">Niche construction, biological evolution, and cultural change</article-title>. <source hwp:id="source-46">Behavioral and Brain Sciences</source>, <volume>23</volume>(<issue>1</issue>), <fpage>131</fpage>–<lpage>175</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1017/S0140525X00002417" ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X00002417" hwp:id="ext-link-44">https://doi.org/10.1017/S0140525X00002417</ext-link></citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="book" citation-type="book" ref:id="254961v4.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Lang P. J."><surname>Lang</surname>, <given-names>P. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bradley M. M."><surname>Bradley</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cuthbert B. N."><surname>Cuthbert</surname>, <given-names>B. N.</given-names></string-name> (<year>2008</year>). <source hwp:id="source-47">International affective picture system (IAPS): Affective ratings of pictures and instruction manual. Technical Report A-8</source>. <publisher-loc>Gainesville, FL</publisher-loc>: <publisher-name>University of Florida</publisher-name>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Lange F. P. de"><surname>Lange</surname>, <given-names>F. P. de</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heilbron M."><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kok P."><surname>Kok</surname>, <given-names>P.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-46">How Do Expectations Shape Perception?</article-title> <source hwp:id="source-48">Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>9</issue>), <fpage>764</fpage>–<lpage>779</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.tics.2018.06.002" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2018.06.002" hwp:id="ext-link-45">https://doi.org/10.1016/j.tics.2018.06.002</ext-link></citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.51" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Levy D. J."><surname>Levy</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Glimcher P. W."><surname>Glimcher</surname>, <given-names>P. W.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-47">The root of all value: a neural common currency for choice</article-title>. <source hwp:id="source-49">Current Opinion in Neurobiology</source>, <volume>22</volume>(<issue>6</issue>), <fpage>1027</fpage>–<lpage>1038</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.conb.2012.06.001" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.06.001" hwp:id="ext-link-46">https://doi.org/10.1016/j.conb.2012.06.001</ext-link></citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.52" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Lindquist K. A."><surname>Lindquist</surname>, <given-names>K. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wager T. D."><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kober H."><surname>Kober</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bliss-Moreau E."><surname>Bliss-Moreau</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barrett L. F."><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-48">The brain basis of emotion: A meta-analytic review</article-title>. <source hwp:id="source-50">Behavioral and Brain Sciences</source>, <volume>35</volume>(<issue>03</issue>), <fpage>121</fpage>–<lpage>143</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1017/S0140525X11000446" ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X11000446" hwp:id="ext-link-47">https://doi.org/10.1017/S0140525X11000446</ext-link></citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.53" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Margulis E. H."><surname>Margulis</surname>, <given-names>E. H.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-49">An exploratory study of narrative experiences of music</article-title>. <source hwp:id="source-51">Music Perception</source>, <volume>35</volume>(<issue>2</issue>), <fpage>235</fpage>–<lpage>248</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1525/MP.2017.35.2.235" ext-link-type="uri" xlink:href="https://doi.org/10.1525/MP.2017.35.2.235" hwp:id="ext-link-48">https://doi.org/10.1525/MP.2017.35.2.235</ext-link></citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1 xref-ref-54-2 xref-ref-54-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Margulis E. H."><surname>Margulis</surname>, <given-names>E. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wong P. C. M."><surname>Wong</surname>, <given-names>P. C. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simchy-Gross R."><surname>Simchy-Gross</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="McAuley J. D."><surname>McAuley</surname>, <given-names>J. D.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-50">What the music said: narrative listening across cultures</article-title>. <source hwp:id="source-52">Palgrave Communications</source>, <volume>5</volume>(<issue>146</issue>), <fpage>1</fpage>–<lpage>8</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1057/s41599-019-0363-1" ext-link-type="uri" xlink:href="https://doi.org/10.1057/s41599-019-0363-1" hwp:id="ext-link-49">https://doi.org/10.1057/s41599-019-0363-1</ext-link></citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.55" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Mehr S. A."><surname>Mehr</surname>, <given-names>S. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krasnow M. M."><surname>Krasnow</surname>, <given-names>M. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bryant G. A."><surname>Bryant</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Hagen E. H."><surname>Hagen</surname>, <given-names>E. H.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-51">Origins of music in credible signaling</article-title>. <source hwp:id="source-53">Behavioral and Brain Sciences</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1017/S0140525X20000345" ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X20000345" hwp:id="ext-link-50">https://doi.org/10.1017/S0140525X20000345</ext-link></citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1 xref-ref-56-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.56" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Mehr S. A."><surname>Mehr</surname>, <given-names>S. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Singh M."><surname>Singh</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Knox D."><surname>Knox</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ketter D. M."><surname>Ketter</surname>, <given-names>D. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pickens-Jones D."><surname>Pickens-Jones</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Atwood S."><surname>Atwood</surname>, <given-names>S.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Glowacki L."><surname>Glowacki</surname>, <given-names>L.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-52">Universality and diversity in human song</article-title>. <source hwp:id="source-54">Science (New York, N.Y.)</source>, <volume>366</volume>(<issue>6468</issue>). <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1126/science.aax0868" ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aax0868" hwp:id="ext-link-51">https://doi.org/10.1126/science.aax0868</ext-link></citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><citation publication-type="website" citation-type="web" ref:id="254961v4.57" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Murphy D. L. K."><surname>Murphy</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="King C. D."><surname>King</surname>, <given-names>C. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schlebusch S. N."><surname>Schlebusch</surname>, <given-names>S. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shera C. A."><surname>Shera</surname>, <given-names>C. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Groh J. M."><surname>Groh</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cooper M."><surname>Cooper</surname>, <given-names>M.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Mohl J."><surname>Mohl</surname>, <given-names>J.</given-names></string-name> (<year>2020</year>). <article-title hwp:id="article-title-53">Evidence for a system in the auditory periphery that may contribute to linking sounds and images in space</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1101/2020.07.19.210864" ext-link-type="uri" xlink:href="https://doi.org/10.1101/2020.07.19.210864" hwp:id="ext-link-52">https://doi.org/10.1101/2020.07.19.210864</ext-link></citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1 xref-ref-58-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Nichols T. E."><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Holmes A. P."><surname>Holmes</surname>, <given-names>A. P.</given-names></string-name> (<year>2001</year>). <article-title hwp:id="article-title-54">Nonparametric Permutation Tests For Functional Neuroimaging: A Primer with Examples</article-title>. <source hwp:id="source-55">Human Brain Mapping</source>, <volume>25</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>25</lpage>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1 xref-ref-59-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.59" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Nili H."><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wingfield C."><surname>Wingfield</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walther A."><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Su L."><surname>Su</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Marslen-Wilson W."><surname>Marslen-Wilson</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-55">A Toolbox for Representational Similarity Analysis</article-title>. <source hwp:id="source-56">PLoS Computational Biology</source>, <volume>10</volume>(<issue>4</issue>). <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1371/journal.pcbi.1003553" ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003553" hwp:id="ext-link-53">https://doi.org/10.1371/journal.pcbi.1003553</ext-link></citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Nishimoto S."><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vu A. T."><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naselaris T."><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Benjamini Y."><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yu B."><surname>Yu</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Gallant J. L."><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-56">Reconstructing visual experiences from brain activity evoked by natural movies</article-title>. <source hwp:id="source-57">Current Biology</source>, <volume>21</volume>(<issue>19</issue>), <fpage>1641</fpage>–<lpage>6</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.cub.2011.08.031" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2011.08.031" hwp:id="ext-link-54">https://doi.org/10.1016/j.cub.2011.08.031</ext-link></citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Norman K."><surname>Norman</surname>, <given-names>K.</given-names></string-name> a, <string-name name-style="western" hwp:sortable="Polyn S. M."><surname>Polyn</surname>, <given-names>S. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Detre G. J."><surname>Detre</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Haxby J. V."><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-57">Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>. <source hwp:id="source-58">Trends in Cognitive Sciences</source>, <volume>10</volume>(<issue>9</issue>), <fpage>424</fpage>–<lpage>30</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.tics.2006.07.005" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2006.07.005" hwp:id="ext-link-55">https://doi.org/10.1016/j.tics.2006.07.005</ext-link></citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><citation publication-type="book" citation-type="book" ref:id="254961v4.62" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Oliphant T. E."><surname>Oliphant</surname>, <given-names>T. E.</given-names></string-name> (<year>2006</year>). <source hwp:id="source-59">A guide to NumPy</source>. <publisher-loc>USA</publisher-loc>: <publisher-name>Trelgol Publishing</publisher-name>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Parkinson C."><surname>Parkinson</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walker T. T."><surname>Walker</surname>, <given-names>T. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Memmi S."><surname>Memmi</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wheatley T."><surname>Wheatley</surname>, <given-names>T.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-58">Emotions are understood from biological motion across remote cultures</article-title>. <source hwp:id="source-60">Emotion</source>, <volume>17</volume>(<issue>3</issue>), <fpage>459</fpage>–<lpage>477</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/emo0000194" ext-link-type="uri" xlink:href="https://doi.org/10.1037/emo0000194" hwp:id="ext-link-56">https://doi.org/10.1037/emo0000194</ext-link></citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><citation publication-type="website" citation-type="web" ref:id="254961v4.64" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Pedregosa F."><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Varoquaux G."><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gramfort A."><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Michel V."><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thirion B."><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grisel O."><surname>Grisel</surname>, <given-names>O.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Duchesnay É."><surname>Duchesnay</surname>, <given-names>É.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-59">Scikit-learn: Machine Learning in Python</article-title>, <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1007/s13398-014-0173-7.2" ext-link-type="uri" xlink:href="https://doi.org/10.1007/s13398-014-0173-7.2" hwp:id="ext-link-57">https://doi.org/10.1007/s13398-014-0173-7.2</ext-link></citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1 xref-ref-65-2"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.65" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Peelen M. V."><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Atkinson A. P."><surname>Atkinson</surname>, <given-names>A. P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Vuilleumier P."><surname>Vuilleumier</surname>, <given-names>P.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-60">Supramodal Representations of Perceived Emotions in the Human Brain</article-title>. <source hwp:id="source-61">Journal of Neuroscience</source>, <volume>30</volume>(<issue>30</issue>), <fpage>10127</fpage>–<lpage>10134</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.2161-10.2010" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2161-10.2010" hwp:id="ext-link-58">https://doi.org/10.1523/JNEUROSCI.2161-10.2010</ext-link></citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Peirce J. W."><surname>Peirce</surname>, <given-names>J. W.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-61">PsychoPy-Psychophysics software in Python</article-title>. <source hwp:id="source-62">Journal of Neuroscience Methods</source>, <volume>162</volume>(<issue>1-2</issue>), <fpage>8</fpage>–<lpage>13</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.jneumeth.2006.11.017" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2006.11.017" hwp:id="ext-link-59">https://doi.org/10.1016/j.jneumeth.2006.11.017</ext-link></citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Peres-Neto P. R."><surname>Peres-Neto</surname>, <given-names>P. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Legendre P."><surname>Legendre</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dray S."><surname>Dray</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Borcard D."><surname>Borcard</surname>, <given-names>D.</given-names></string-name> (<year>2006</year>). <article-title hwp:id="article-title-62">Variation partitioning of species data matrices: Estimation and comparison of fractions</article-title>. <source hwp:id="source-63">Ecology</source>, <volume>87</volume>(<issue>10</issue>), <fpage>2614</fpage>–<lpage>2625</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1890/0012-9658(2006)87%5B2614:VPOSDM%5D2.0.CO;2" ext-link-type="uri" xlink:href="https://doi.org/10.1890/0012-9658(2006)87%5B2614:VPOSDM%5D2.0.CO;2" hwp:id="ext-link-60">https://doi.org/10.1890/0012-9658(2006)87%5B2614:VPOSDM%5D2.0.CO;2</ext-link></citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.68" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Phillips M. L."><surname>Phillips</surname>, <given-names>M. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Young A. W."><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Senior C."><surname>Senior</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brammer M."><surname>Brammer</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Andrew C."><surname>Andrew</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Calder A. J."><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="David A. S."><surname>David</surname>, <given-names>A. S.</given-names></string-name> (<year>1997</year>). <article-title hwp:id="article-title-63">A specific neural substrate for percieving facial expressions of disgust</article-title>. <source hwp:id="source-64">Nature</source>, <volume>389</volume>(<day>2</day> <month>October</month>), <fpage>495</fpage>–<lpage>498</lpage>.</citation></ref><ref id="c69" hwp:id="ref-69" hwp:rev-id="xref-ref-69-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Phillips-silver J."><surname>Phillips-silver</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Trainor L. J."><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name> (<year>2005</year>). <article-title hwp:id="article-title-64">Feeling the Beat : Movement Influences Infant Rhythm Perception</article-title>. <source hwp:id="source-65">Science</source>, <volume>308</volume>, <fpage>1430</fpage>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Riesenhuber M."><surname>Riesenhuber</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Poggio T."><surname>Poggio</surname>, <given-names>T.</given-names></string-name> (<year>1999</year>). <article-title hwp:id="article-title-65">Hierarchical models of object recognition in cortex</article-title>. <source hwp:id="source-66">Nature Neuroscience</source>, <volume>2</volume>(<issue>11</issue>), <fpage>1019</fpage>–<lpage>1025</lpage>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.71" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Robins D. L."><surname>Robins</surname>, <given-names>D. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hunyadi E."><surname>Hunyadi</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schultz R. T."><surname>Schultz</surname>, <given-names>R. T.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-66">Superior temporal activation in response to dynamic audio-visual emotional cues</article-title>. <source hwp:id="source-67">Brain and Cognition</source>, <volume>69</volume>(<issue>2</issue>), <fpage>269</fpage>–<lpage>278</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.bandc.2008.08.007" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandc.2008.08.007" hwp:id="ext-link-61">https://doi.org/10.1016/j.bandc.2008.08.007</ext-link></citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Rockland K. S."><surname>Rockland</surname>, <given-names>K. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Ojima H."><surname>Ojima</surname>, <given-names>H.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-67">Multisensory convergence in calcarine visual areas in macaque monkey</article-title>. <source hwp:id="source-68">International Journal of Psychophysiology</source>, <volume>50</volume>(<issue>1-2</issue>), <fpage>19</fpage>–<lpage>26</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/S0167-8760(03)00121-1" ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0167-8760(03)00121-1" hwp:id="ext-link-62">https://doi.org/10.1016/S0167-8760(03)00121-1</ext-link></citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.73" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Roskies A. L."><surname>Roskies</surname>, <given-names>A. L.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-68">Representational similarity analysis in neuroimaging: proxy vehicles and provisional representations</article-title>. <source hwp:id="source-69">Synthese</source>, (<issue>0123456789</issue>). <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1007/s11229-021-03052-4" ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11229-021-03052-4" hwp:id="ext-link-63">https://doi.org/10.1007/s11229-021-03052-4</ext-link></citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Russell J. a."><surname>Russell</surname>, <given-names>J. a.</given-names></string-name> (<year>1980</year>). <article-title hwp:id="article-title-69">A circumplex model of affect</article-title>. <source hwp:id="source-70">Journal of Personality and Social Psychology</source>, <volume>39</volume>(<issue>6</issue>), <fpage>1161</fpage>–<lpage>1178</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/h0077714" ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0077714" hwp:id="ext-link-64">https://doi.org/10.1037/h0077714</ext-link></citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Saad Z. S."><surname>Saad</surname>, <given-names>Z. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reynolds R. C."><surname>Reynolds</surname>, <given-names>R. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Argall B."><surname>Argall</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Japee S."><surname>Japee</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cox R. W."><surname>Cox</surname>, <given-names>R. W.</given-names></string-name> (<year>2004</year>). <article-title hwp:id="article-title-70">SUMA: an interface for surface-based intra- and inter-subject analysis with AFNI</article-title>. <source hwp:id="source-71">Biomedical Imaging: Nano to Macro, 2004. IEEE International Symposium on</source>, (<month>October</month> 2015), <fpage>1510</fpage>–<lpage>1513</lpage> Vol. <volume>2</volume>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1109/ISBI.2004.1398837" ext-link-type="uri" xlink:href="https://doi.org/10.1109/ISBI.2004.1398837" hwp:id="ext-link-65">https://doi.org/10.1109/ISBI.2004.1398837</ext-link></citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Saarimäki H."><surname>Saarimäki</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ejtehadian L. F."><surname>Ejtehadian</surname>, <given-names>L. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glerean E."><surname>Glerean</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jääskeläinen I. P."><surname>Jääskeläinen</surname>, <given-names>I. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vuilleumier P."><surname>Vuilleumier</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sams M."><surname>Sams</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Nummenmaa L."><surname>Nummenmaa</surname>, <given-names>L.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-71">Distributed affective space represents multiple emotion categories across the human brain</article-title>. <source hwp:id="source-72">Social Cognitive and Affective Neuroscience</source>, <volume>13</volume>(<issue>5</issue>), <fpage>471</fpage>–<lpage>482</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/scan/nsy018" ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nsy018" hwp:id="ext-link-66">https://doi.org/10.1093/scan/nsy018</ext-link></citation></ref><ref id="c77" hwp:id="ref-77" hwp:rev-id="xref-ref-77-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.77" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-77"><string-name name-style="western" hwp:sortable="Saffran J. R."><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Aslin R. N."><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Newport E. L."><surname>Newport</surname>, <given-names>E. L.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-72">Statistical learning by 8-month-old infants</article-title>. <source hwp:id="source-73">Science (New York, N.Y.)</source>, <volume>274</volume>(<issue>5294</issue>), <fpage>1926</fpage>–<lpage>1928</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1126/science.274.5294.1926" ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.274.5294.1926" hwp:id="ext-link-67">https://doi.org/10.1126/science.274.5294.1926</ext-link></citation></ref><ref id="c78" hwp:id="ref-78" hwp:rev-id="xref-ref-78-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.78" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-78"><string-name name-style="western" hwp:sortable="Savage P. E."><surname>Savage</surname>, <given-names>P. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brown S."><surname>Brown</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sakai E."><surname>Sakai</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Currie T. E."><surname>Currie</surname>, <given-names>T. E.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-73">Statistical universals reveal the structures and functions of human music</article-title>. <source hwp:id="source-74">Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>29</issue>), <fpage>8987</fpage>–<lpage>8992</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.1414495112" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1414495112" hwp:id="ext-link-68">https://doi.org/10.1073/pnas.1414495112</ext-link></citation></ref><ref id="c79" hwp:id="ref-79" hwp:rev-id="xref-ref-79-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.79" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-79"><string-name name-style="western" hwp:sortable="Schirmer A."><surname>Schirmer</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Adolphs R."><surname>Adolphs</surname>, <given-names>R.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-74">Emotion Perception from Face, Voice, and Touch: Comparisons and Convergence</article-title>. <source hwp:id="source-75">Trends in Cognitive Sciences</source>, <volume>21</volume>(<issue>3</issue>), <fpage>216</fpage>–<lpage>228</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.tics.2017.01.001" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2017.01.001" hwp:id="ext-link-69">https://doi.org/10.1016/j.tics.2017.01.001</ext-link></citation></ref><ref id="c80" hwp:id="ref-80" hwp:rev-id="xref-ref-80-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.80" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-80"><string-name name-style="western" hwp:sortable="Shuster A."><surname>Shuster</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Levy D. J."><surname>Levy</surname>, <given-names>D. J.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-75">Common Sense in Choice: The Effect of Sensory Modality on Neural Value Representations</article-title>. <source hwp:id="source-76">eNeuro</source>, <volume>5</volume>(<issue>2</issue>), <fpage>1</fpage>–<lpage>14</lpage>.</citation></ref><ref id="c81" hwp:id="ref-81" hwp:rev-id="xref-ref-81-1 xref-ref-81-2"><citation publication-type="website" citation-type="web" ref:id="254961v4.81" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-81"><string-name name-style="western" hwp:sortable="Sievers B."><surname>Sievers</surname>, <given-names>B.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-76">Visual and auditory brain areas share a representational structure that supports emotion perception: code and materials</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.17605/OSF.IO/KVBQM" ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/KVBQM" hwp:id="ext-link-70">https://doi.org/10.17605/OSF.IO/KVBQM</ext-link></citation></ref><ref id="c82" hwp:id="ref-82" hwp:rev-id="xref-ref-82-1 xref-ref-82-2 xref-ref-82-3 xref-ref-82-4"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.82" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-82"><string-name name-style="western" hwp:sortable="Sievers B."><surname>Sievers</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee C."><surname>Lee</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haslett W."><surname>Haslett</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wheatley T."><surname>Wheatley</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wheatley T."><surname>Wheatley</surname>, <given-names>T.</given-names></string-name> (<year>2019</year>). <article-title hwp:id="article-title-77">A multi-sensory code for emotional arousal</article-title>. <source hwp:id="source-77">Proceedings of the Royal Society B: Biological Sciences</source>, <volume>286</volume>(<issue>20190513</issue>), <fpage>1</fpage>–<lpage>10</lpage>.</citation></ref><ref id="c83" hwp:id="ref-83" hwp:rev-id="xref-ref-83-1"><citation publication-type="website" citation-type="web" ref:id="254961v4.83" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-83"><string-name name-style="western" hwp:sortable="Sievers B."><surname>Sievers</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Parkinson C."><surname>Parkinson</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kohler P. J."><surname>Kohler</surname>, <given-names>P. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hughes J."><surname>Hughes</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fogelson S. V."><surname>Fogelson</surname>, <given-names>S. V.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wheatley T."><surname>Wheatley</surname>, <given-names>T.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-78">Visual and auditory brain areas share a representational structure that supports emotion perception: fMRI data</article-title>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.18112/openneuro.ds003715.v1.0.0" ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds003715.v1.0.0" hwp:id="ext-link-71">https://doi.org/10.18112/openneuro.ds003715.v1.0.0</ext-link></citation></ref><ref id="c84" hwp:id="ref-84" hwp:rev-id="xref-ref-84-1 xref-ref-84-2 xref-ref-84-3 xref-ref-84-4 xref-ref-84-5 xref-ref-84-6 xref-ref-84-7 xref-ref-84-8 xref-ref-84-9 xref-ref-84-10 xref-ref-84-11 xref-ref-84-12"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.84" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-84"><string-name name-style="western" hwp:sortable="Sievers B."><surname>Sievers</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Polansky L."><surname>Polansky</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Casey M."><surname>Casey</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wheatley T."><surname>Wheatley</surname>, <given-names>T.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-79">Music and movement share a dynamic structure that supports universal expressions of emotion</article-title>. <source hwp:id="source-78">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>110</volume>(<issue>1</issue>), <fpage>70</fpage>–<lpage>5</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.1209023110" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1209023110" hwp:id="ext-link-72">https://doi.org/10.1073/pnas.1209023110</ext-link></citation></ref><ref id="c85" hwp:id="ref-85" hwp:rev-id="xref-ref-85-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.85" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-85"><string-name name-style="western" hwp:sortable="Sievers B."><surname>Sievers</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Wheatley T."><surname>Wheatley</surname>, <given-names>T.</given-names></string-name> (<year>2021</year>). <article-title hwp:id="article-title-80">Rapid dissonant grunting, or, But why does music sound the way it does?</article-title> <source hwp:id="source-79">Behavioral and Brain Sciences</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.31234/osf.io/89d2h" ext-link-type="uri" xlink:href="https://doi.org/10.31234/osf.io/89d2h" hwp:id="ext-link-73">https://doi.org/10.31234/osf.io/89d2h</ext-link></citation></ref><ref id="c86" hwp:id="ref-86" hwp:rev-id="xref-ref-86-1 xref-ref-86-2 xref-ref-86-3 xref-ref-86-4"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.86" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-86"><string-name name-style="western" hwp:sortable="Skerry A. E."><surname>Skerry</surname>, <given-names>A. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Saxe R."><surname>Saxe</surname>, <given-names>R.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-81">Neural Representations of Emotion Are Organized around Abstract Event Features</article-title>. <source hwp:id="source-80">Current Biology</source>, <volume>25</volume>(<issue>15</issue>), <fpage>1945</fpage>–<lpage>54</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.cub.2015.06.009" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.06.009" hwp:id="ext-link-74">https://doi.org/10.1016/j.cub.2015.06.009</ext-link></citation></ref><ref id="c87" hwp:id="ref-87" hwp:rev-id="xref-ref-87-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.87" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-87"><string-name name-style="western" hwp:sortable="Spector F."><surname>Spector</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Maurer D."><surname>Maurer</surname>, <given-names>D.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-82">Synesthesia: a new approach to understanding the development of perception</article-title>. <source hwp:id="source-81">Developmental Psychology</source>, <volume>45</volume>(<issue>1</issue>), <fpage>175</fpage>–<lpage>89</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1037/a0014171" ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0014171" hwp:id="ext-link-75">https://doi.org/10.1037/a0014171</ext-link></citation></ref><ref id="c88" hwp:id="ref-88" hwp:rev-id="xref-ref-88-1 xref-ref-88-2 xref-ref-88-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.88" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-88"><string-name name-style="western" hwp:sortable="Trehub S. E."><surname>Trehub</surname>, <given-names>S. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Becker J."><surname>Becker</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morley I."><surname>Morley</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Trehub S. E."><surname>Trehub</surname>, <given-names>S. E.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-83">Cross-cultural perspectives on music and musicality</article-title>. <source hwp:id="source-82">Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>370</volume>(<issue>20140096</issue>), <fpage>1</fpage>–<lpage>9</lpage>.</citation></ref><ref id="c89" hwp:id="ref-89" hwp:rev-id="xref-ref-89-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.89" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-89"><string-name name-style="western" hwp:sortable="Tsuchiya N."><surname>Tsuchiya</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moradi F."><surname>Moradi</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Felsen C."><surname>Felsen</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamazaki M."><surname>Yamazaki</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Adolphs R."><surname>Adolphs</surname>, <given-names>R.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-84">Intact rapid detection of fearful faces in the absence of the amygdala</article-title>. <source hwp:id="source-83">Nature Neuroscience</source>, <volume>12</volume>(<issue>10</issue>), <fpage>1224</fpage>–<lpage>1225</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1038/nn.2380" ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2380" hwp:id="ext-link-76">https://doi.org/10.1038/nn.2380</ext-link></citation></ref><ref id="c90" hwp:id="ref-90" hwp:rev-id="xref-ref-90-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.90" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-90"><string-name name-style="western" hwp:sortable="Wager T. D."><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kang J."><surname>Kang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnson T. D."><surname>Johnson</surname>, <given-names>T. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nichols T. E."><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Satpute A. B."><surname>Satpute</surname>, <given-names>A. B.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barrett L. F."><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-85">A Bayesian Model of Category-Specific Emotional Brain Responses</article-title>. <source hwp:id="source-84">PLoS Computational Biology</source>, <volume>11</volume>(<issue>4</issue>), <fpage>1</fpage>–<lpage>27</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1371/journal.pcbi.1004066" ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004066" hwp:id="ext-link-77">https://doi.org/10.1371/journal.pcbi.1004066</ext-link></citation></ref><ref id="c91" hwp:id="ref-91" hwp:rev-id="xref-ref-91-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.91" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-91"><string-name name-style="western" hwp:sortable="Wang S."><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tudusciuc O."><surname>Tudusciuc</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mamelak A. N."><surname>Mamelak</surname>, <given-names>A. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ross I. B."><surname>Ross</surname>, <given-names>I. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adolphs R."><surname>Adolphs</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Rutishauser U."><surname>Rutishauser</surname>, <given-names>U.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-86">Neurons in the human amygdala selective for perceived emotion</article-title>. <source hwp:id="source-85">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>111</volume>(<issue>30</issue>). <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1073/pnas.1323342111" ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1323342111" hwp:id="ext-link-78">https://doi.org/10.1073/pnas.1323342111</ext-link></citation></ref><ref id="c92" hwp:id="ref-92" hwp:rev-id="xref-ref-92-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.92" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-92"><string-name name-style="western" hwp:sortable="Watson R."><surname>Watson</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Latinus M."><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Noguchi T."><surname>Noguchi</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garrod O."><surname>Garrod</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crabbe F."><surname>Crabbe</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Belin P."><surname>Belin</surname>, <given-names>P.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-87">Crossmodal Adaptation in Right Posterior Superior Temporal Sulcus during Face-Voice Emotional Integration</article-title>. <source hwp:id="source-86">Journal of Neuroscience</source>, <volume>34</volume>(<issue>20</issue>), <fpage>6813</fpage>–<lpage>6821</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1523/JNEUROSCI.4478-13.2014" ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4478-13.2014" hwp:id="ext-link-79">https://doi.org/10.1523/JNEUROSCI.4478-13.2014</ext-link></citation></ref><ref id="c93" hwp:id="ref-93" hwp:rev-id="xref-ref-93-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.93" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-93"><string-name name-style="western" hwp:sortable="Werner S."><surname>Werner</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Noppeney U."><surname>Noppeney</surname>, <given-names>U.</given-names></string-name> (<year>2010</year>). <article-title hwp:id="article-title-88">Superadditive responses in superior temporal sulcus predict audiovisual benefits in object categorization</article-title>. <source hwp:id="source-87">Cerebral Cortex</source>, <volume>20</volume>(<issue>8</issue>), <fpage>1829</fpage>–<lpage>1842</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/cercor/bhp248" ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhp248" hwp:id="ext-link-80">https://doi.org/10.1093/cercor/bhp248</ext-link></citation></ref><ref id="c94" hwp:id="ref-94" hwp:rev-id="xref-ref-94-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.94" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-94"><string-name name-style="western" hwp:sortable="Wheeler M. E."><surname>Wheeler</surname>, <given-names>M. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Petersen S. E."><surname>Petersen</surname>, <given-names>S. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Buckner R. L."><surname>Buckner</surname>, <given-names>R. L.</given-names></string-name> (<year>2000</year>). <article-title hwp:id="article-title-89">Memory’s echo: Vivid remembering reactivates sensory-specific cortex</article-title>. <source hwp:id="source-88">Proceedings of the National Academy of Sciences</source>, <volume>97</volume>(<issue>20</issue>), <fpage>11125</fpage>–<lpage>11129</lpage>.</citation></ref><ref id="c95" hwp:id="ref-95" hwp:rev-id="xref-ref-95-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.95" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-95"><string-name name-style="western" hwp:sortable="Wright T. M."><surname>Wright</surname>, <given-names>T. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pelphrey K. A."><surname>Pelphrey</surname>, <given-names>K. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Allison T."><surname>Allison</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McKeown M. J."><surname>McKeown</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="McCarthy G."><surname>McCarthy</surname>, <given-names>G.</given-names></string-name> (<year>2003</year>). <article-title hwp:id="article-title-90">Polysensory interactions along lateral temporal regions evoked by audiovisual speech</article-title>. <source hwp:id="source-89">Cerebral Cortex</source>, <volume>13</volume>(<issue>10</issue>), <fpage>1034</fpage>–<lpage>1043</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1093/cercor/13.10.1034" ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/13.10.1034" hwp:id="ext-link-81">https://doi.org/10.1093/cercor/13.10.1034</ext-link></citation></ref><ref id="c96" hwp:id="ref-96" hwp:rev-id="xref-ref-96-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.96" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-96"><string-name name-style="western" hwp:sortable="Wyk B. C. V."><surname>Wyk</surname>, <given-names>B. C. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hudac C. M."><surname>Hudac</surname>, <given-names>C. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carter E. J."><surname>Carter</surname>, <given-names>E. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sobel D. M."><surname>Sobel</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Pelphrey K. a."><surname>Pelphrey</surname>, <given-names>K. a.</given-names></string-name> (<year>2009</year>). <article-title hwp:id="article-title-91">Action understanding in the superior temporal sulcus region</article-title>. <source hwp:id="source-90">Psychological Science</source>, <volume>20</volume>(<issue>6</issue>), <fpage>771</fpage>–<lpage>7</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1111/j.1467-9280.2009.02359.x" ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2009.02359.x" hwp:id="ext-link-82">https://doi.org/10.1111/j.1467-9280.2009.02359.x</ext-link></citation></ref><ref id="c97" hwp:id="ref-97" hwp:rev-id="xref-ref-97-1 xref-ref-97-2 xref-ref-97-3"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.97" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-97"><string-name name-style="western" hwp:sortable="Yuki M."><surname>Yuki</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Maddux W. W."><surname>Maddux</surname>, <given-names>W. W.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Masuda T."><surname>Masuda</surname>, <given-names>T.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-92">Are the windows to the soul the same in the East and West? Cultural differences in using the eyes and mouth as cues to recognize emotions in Japan and the United States</article-title>. <source hwp:id="source-91">Journal of Experimental Social Psychology</source>, <volume>43</volume>(<issue>2</issue>), <fpage>303</fpage>–<lpage>311</lpage>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.1016/j.jesp.2006.02.004" ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jesp.2006.02.004" hwp:id="ext-link-83">https://doi.org/10.1016/j.jesp.2006.02.004</ext-link></citation></ref><ref id="c98" hwp:id="ref-98" hwp:rev-id="xref-ref-98-1"><citation publication-type="journal" citation-type="journal" ref:id="254961v4.98" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-98"><string-name name-style="western" hwp:sortable="Zicarelli D. D."><surname>Zicarelli</surname>, <given-names>D. D.</given-names></string-name> (<year>1998</year>). <article-title hwp:id="article-title-93">An extensible real-time signal processing environment for Max</article-title>. <source hwp:id="source-92">Proceedings of the 1998 International Computer Music Conference</source>, <fpage>463</fpage>–<lpage>466</lpage>.</citation></ref></ref-list></back></article>
