<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/360537</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;360537v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;360537</article-id><article-id pub-id-type="other" hwp:sub-type="slug">360537</article-id><article-id pub-id-type="other" hwp:sub-type="tag">360537</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Neuroscience" hwp:journal="biorxiv"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Episodic Control as Meta-Reinforcement Learning</article-title></title-group><author-notes hwp:id="author-notes-1"><fn fn-type="equal" id="n1" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>*</label><p hwp:id="p-1">These authors contributed equally to this work.</p></fn><fn id="n2" hwp:id="fn-2"><p hwp:id="p-2"><email hwp:id="email-1">ritters@google.com</email>, <email hwp:id="email-2">wangjane@google.com</email>, <email hwp:id="email-3">zebk@google.com</email>, <email hwp:id="email-4">botvinick@google.com</email></p></fn><fn fn-type="presented-at" hwp:id="fn-3"><p hwp:id="p-3">Presented at the <italic toggle="yes">40th Annual Cognitive Science Society Meeting</italic>, Madison, Wisconsin, 2018.</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Ritter S"><surname>Ritter</surname><given-names>S</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Wang JX"><surname>Wang</surname><given-names>JX</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Kurth-Nelson Z"><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Botvinick M"><surname>Botvinick</surname><given-names>M</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4"><label>1</label><institution hwp:id="institution-1">DeepMind</institution>, London, <country>UK</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Princeton Neuroscience Institute</institution>, Princeton, NJ</aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">MPS-UCL Centre for Computational Psychiatry</institution>, London, <country>UK</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Gatsby Computational Neuroscience Unit</institution>, UCL, London, <country>UK</country></aff></contrib-group><pub-date pub-type="epub-original" hwp:start="2018"><year>2018</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2018-07-03T05:54:29-07:00">
    <day>3</day><month>7</month><year>2018</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2018-07-03T05:54:29-07:00">
    <day>3</day><month>7</month><year>2018</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2018-07-03T06:01:22-07:00">
    <day>3</day><month>7</month><year>2018</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2018-07-03T06:01:22-07:00">
    <day>3</day><month>7</month><year>2018</year>
  </pub-date><elocation-id>360537</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2018-07-02"><day>02</day><month>7</month><year>2018</year></date>
<date date-type="rev-recd" hwp:start="2018-07-02"><day>02</day><month>7</month><year>2018</year></date>
<date date-type="accepted" hwp:start="2018-07-03"><day>03</day><month>7</month><year>2018</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-4">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="360537.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2019/pdf/360537v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="360537.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2019/abstracts/360537v1/360537v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2019/fulltext/360537v1/360537v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-5">Recent research has placed episodic reinforcement learning (RL) alongside model-free and model-based RL on the list of processes centrally involved in human reward-based learning. In the present work, we extend the unified account of model-free and model-based RL developed by <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Wang et al. (2018)</xref> to further integrate episodic learning. In this account, a generic model-free “meta-learner” learns to deploy and coordinate among all of these learning algorithms. The meta-learner learns through brief encounters with many novel tasks, so that it <italic toggle="yes">learns to learn</italic> about new tasks. We show that when equipped with an episodic memory system inspired by theories of reinstatement and gating, the meta-learner learns to use the episodic and model-based learning algorithms observed in humans in a task designed to dissociate among the influences of various learning strategies. We discuss implications and predictions of the model.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">Reinforcement learning</kwd><kwd hwp:id="kwd-2">model-based</kwd><kwd hwp:id="kwd-3">deep learning</kwd><kwd hwp:id="kwd-4">meta-learning</kwd><kwd hwp:id="kwd-5">episodic memory</kwd></kwd-group><counts><page-count count="7"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">Introduction</title><p hwp:id="p-6">Nearly every decision an intelligent organism makes is informed by its memory of the results of its past decisions. To be successful, agents must distill the results of past decisions into memories, then make use of those memories to make better decisions in the future. Accordingly, much effort has been directed toward understanding (1) what humans and animals store after a sequence of actions and rewards, and (2) how they use that stored information to appraise the value of future actions.</p><p hwp:id="p-7">Model-free and model-based reinforcement learning (RL; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Daw, Gershman, Seymour, Dayan, &amp; Dolan, 2011</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Sutton &amp; Barto, 1998</xref>) offer distinct solutions to these two problems. Model-free RL stores statistics about the relationship between states, actions and rewards, and appraises actions by calculating how frequently they led to reward. Meanwhile, model-based RL stores estimated state-state transition probabilities, and appraises actions by using this model to simulate sequences of states to predict future reward. Signatures of both model-free and model-based learning appear in behavior and in the brain (e.g. <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">Daw et al., 2011</xref>), and a venerable tradition holds that they are implemented by dissociable neural systems (for review see <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Dolan &amp; Dayan, 2013</xref>).</p><p hwp:id="p-8">However, the recent theory of meta-reinforcement learning (meta-RL) proposed that model-free learning, model-based learning, and their sometimes complex interaction could all be explained by a simple unified mechanism (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Wang et al., 2018</xref>). In meta-RL, a recurrent neural network (RNN) receives a reward signal as part of its input and is trained by model-free learning on a series of interrelated tasks to rapidly learn from this signal. Through this training, the RNN learns to distill the history of observations, actions, and rewards into its hidden state (a form of <italic toggle="yes">working memory</italic>) and to use this summary to select rewarding actions. In essence, this end result constitutes a learned reinforcement learning algorithm that operates in the RNN’s activation dynamics. Critically, <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Wang et al. (2017)</xref> showed that this meta-learned RL algorithm can be model-based, even though it was acquired through model-free learning.</p><p hwp:id="p-9">While meta-RL provides a full account of incremental learning as it is carried out in working memory, it does not account for the episodic learning processes to which attention has recently been called (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Gershman &amp; Daw, 2017</xref>). In addition to learning by incrementally storing recent sequences of behavior in working memory, humans appear to learn by storing summaries of individual episodes for long periods of time, then retrieving them when similar contexts are encountered. For example, cues triggering episodic memory retrieval impact reward-based learning, both for good and for ill (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Bornstein, Khaw, Shohamy, &amp; Daw, 2017</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Vikbladh, Shohamy, &amp; Daw, 2017</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Bornstein &amp; Norman, 2017</xref>), and distinctive aspects of episodic memory function contribute to decision-making behavior (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Wimmer, Braun, Daw, &amp; Shohamy, 2014</xref>; <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Wimmer &amp; Buechel, 2016</xref>; <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Duncan &amp; Shohamy, 2016</xref>). Such observations, along with some fundamental computational insights (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Lengyel &amp; Dayan, 2007</xref>), have recently landed episodic learning a spot beside incremental model-free and model-based reinforcement learning on the list of processes centrally involved in decision making (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Gershman &amp; Daw, 2017</xref>).</p><p hwp:id="p-10">In the present work, we develop a natural extension to meta-RL that enables it to integrate episodic learning. The resulting theory, based on an algorithm introduced to the machine learning literature by <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Ritter et al. (2018)</xref>, explains how incremental and episodic learning, as well as the coordination between them can be meta-learned through purely model-free RL. The episodic meta-RL theory proposes the following:
<list list-type="order" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-11">Meta-RL’s working memory is supplemented by an episodic memory which stores working memory states.</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-12">Each state is paired with a perceptual context embedding that is later used to retrieve the working memory state when similar perceptual contexts are encountered.</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-13">The retrieved states are then gated into the working memory using a parameterized function, whose parameters are optimized toward the same model-free objective that trains the working memory dynamics.</p></list-item></list></p><p hwp:id="p-14">This proposal is inspired in part by evidence that episodic memory retrieval in humans operates through reinstatement, triggering patterns of neural activity related to those that were induced by the original encoding of the relevant episode (e.g., <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Xiao et al., 2017</xref>), and evidence that reinstatement occurs not only in perceptual systems, but also recreates patterns of activity in neural circuits supporting working memory (<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Hoskin, Bornstein, Norman, &amp; Cohen, 2017</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Cohen &amp; O’Reilly, 1996</xref>). Our implementation of this proposal draws additional inspiration from recent work on differentiable memory systems (e.g., <xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Graves et al., 2016</xref>), especially that of <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Pritzel et al. (2017)</xref>, which makes use of context-based retrieval for RL.</p><p hwp:id="p-15">To empirically test this model, in this work we compared its behavior to that of humans observed by <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">Vikbladh et al. (2017)</xref> in a task designed to dissociate the effects of multiple types of incremental and episodic learning. Vikbladh and colleagues found evidence of the use of a model-based form of episodic memory, whereby traces of specific episodes are retrieved from long-term memory based on visual similarity, then used along with knowledge of the transition structure of the environment to select actions. This episodic model-based learning was present in conjunction with incremental model-free and incremental model-based learning. In the following sections, we describe the task in detail and demonstrate that meta-RL with episodic memory reproduces this model-based episodic learning and its coordination with incremental model-based learning. To conclude, we consider directions for future work, including testable predictions of the theory.</p></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-4">Task</title><p hwp:id="p-16">The task we study is a version of the two-step task (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">Daw et al., 2011</xref>) augmented with episodic cues to previous trials. The task structure, which was inspired by <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-3" hwp:rel-id="ref-22">Vikbladh et al. (2017)</xref>, is diagrammed in <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>. Each trial consisted of two stages. On the first stage, state <italic toggle="yes">s</italic><sub>0</sub>, the agent was either presented with the “no-cue” stimulus (a vector of all −1’s) on uncued trials, or with a binary vector associated with a previously seen second-stage context on cued trials (see <xref ref-type="fig" rid="fig1" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Figure 1</xref>). In response, the agent chose either <italic toggle="yes">a</italic><sub>1</sub> or <italic toggle="yes">a</italic><sub>2</sub> and transitioned into one of two second-stage states <italic toggle="yes">s</italic><sub>1</sub> or <italic toggle="yes">s</italic><sub>2</sub> with probabilities <italic toggle="yes">p</italic>(<italic toggle="yes">s</italic><sub>1</sub>|<italic toggle="yes">a</italic><sub>1</sub>) = <italic toggle="yes">p</italic>(<italic toggle="yes">s</italic><sub>2</sub>|<italic toggle="yes">a</italic><sub>2</sub>) = 0.9 (common transition) and <italic toggle="yes">p</italic>(<italic toggle="yes">s</italic><sub>1</sub>|<italic toggle="yes">a</italic><sub>2</sub>) = <italic toggle="yes">p</italic>(<italic toggle="yes">s</italic><sub>2</sub>|<italic toggle="yes">a</italic><sub>1</sub>) = 0.1 (uncommon transition). These transition probabilities <italic toggle="yes">p</italic> were fixed across episodes. On the second stage, the agent was presented with a stimulus representing the context of that second-stage state, followed by a final step in which it was shown the reward outcome. On uncued trials, the states <italic toggle="yes">s</italic><sub>1</sub> and <italic toggle="yes">s</italic><sub>2</sub> yielded Bernoulli probabilistic rewards of 0 or 1 according to [<italic toggle="yes">r<sub>a</sub>, r<sub>b</sub></italic>] = [0.9,0.1] or [0.1,0.9], with the reward contingencies having a 10% chance of switching at the beginning of each trial.</p><fig id="fig1" position="float" orientation="portrait" fig-type="figure" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;360537v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1:</label><caption hwp:id="caption-1"><p hwp:id="p-17">Contextual two-step task modeled after <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">Daw et al. (2011)</xref> and <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-4" hwp:rel-id="ref-22">Vikbladh et al. (2017)</xref>. (Top) Two trial types are shown: uncued and cued. All trials start in state <italic toggle="yes">s</italic><sub>0</sub> at the first stage, at which point agents are presented with either a “no-cue” stimulus or are cued with a second-stage stimulus seen on a previous trial. Transition probabilities after taking actions <italic toggle="yes">a</italic><sub>1</sub> or <italic toggle="yes">a</italic><sub>2</sub> are depicted in the graph. On uncued trials, <italic toggle="yes">s</italic><sub>1</sub> and <italic toggle="yes">s</italic><sub>2</sub> result in Bernoulli rewards with probabilities <italic toggle="yes">r<sub>a</sub></italic> and <italic toggle="yes">r<sub>b</sub></italic>. On cued trials, transitioning into the same state as in the trial k during which the cue was first presented results in receiving the same reward received on trial k, <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="360537_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula>. (Bottom) Trials within an episode are split into 4 blocks, with block 3 consisting of cued trials which are cued with stimuli from block 1, and block 4 cued from block 2.</p></caption><graphic xlink:href="360537_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-18">On cued trials, if the agent transitioned into the same state as on the trial k during which the cue was first presented, the agent was given the exact same reward as on trial k. If the agent transitioned into the other state, the reward was determined as on uncued trials. This is the critical feature of the task for assessing episodic memory use; if the agent could remember the state entered and reward achieved on the cue-associated trial, it could act to access or avoid that state, depending on whether the past trial was rewarded.</p><p hwp:id="p-19">The first half of every episode (50 trials) consisted entirely of uncued trials, and the second half consisted entirely of cued trials, with trials 51-75 being cued with stimuli from trials 1-25 and trials 76-100 cued with stimuli from trials 26-50, randomly sampled without replacement. This was done to reduce autocorrelation in the reward probabilities by enforcing a minimum of 25 trials between seeing the stimulus on the second stage and being cued with it on the first stage. The agent was trained for 10,000 episodes of 100 trials each, and evaluated with weights fixed on 500 further episodes.</p><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-5">Learning Algorithms</title><p hwp:id="p-20">The two-step task with episodic cueing is designed to dissociate among the influences of four different learning strategies on choice. First, the incremental model-free strategy prescribes taking the same action that was taken on the last trial if it was rewarded, and taking the opposite action if it was not rewarded, regardless of whether the transition on the previous trial was common or uncommon. In contrast, the incremental model-based strategy prescribes taking the same action only if the previous trial was rewarded <italic toggle="yes">and</italic> the previous transition was common. If the previous transition was uncommon and the trial was rewarded, the agent will take the opposite action. Episodic model-free and model-based strategies operate like their incremental counterparts, but with respect to the trial associated with the cue rather than the immediately previous trial.</p></sec></sec><sec id="s3" hwp:id="sec-4"><title hwp:id="title-6">Model</title><p hwp:id="p-21">In episodic meta-RL (EMRL; <xref ref-type="fig" rid="fig2" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">Ritter et al., 2018</xref>), vectors represent working memory states, and a recurrent neural network, specifically a long short-term memory network (LSTM; <xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Hochreiter &amp; Schmidhuber, 1997</xref>), updates the working memory state at each time step and uses it to select actions. To implement context-based reinstatement of the LSTM’s activations, EMRL augments this working memory with an episodic memory containing working memory states. Based on <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">Pritzel et al.’s (2017)</xref> differentiable neural dictionary (see also <xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Blundell et al., 2016</xref>), this episodic memory stores a visual representation of the context along with each item, which is used at retrieval time to find working memory states stored in contexts similar to the retrieval context. In our experiments, the EMRL agent writes to the memory at the end of the each trial. The agent reads from the memory on every time step by searching for the perceptual representation in the array with the smallest cosine distances from the representation of the current state, then retrieving the associated working memory state.</p><fig id="fig2" position="float" orientation="portrait" fig-type="figure" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;360537v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2:</label><caption hwp:id="caption-2"><p hwp:id="p-22">(Left) A high-level schematic of the recurrent network (LSTM) that comprises the episodic meta-RL (EMRL; <xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-3" hwp:rel-id="ref-19">Ritter et al., 2018</xref>) agent’s working memory. On each time step the LSTM receives an environment state, the action taken on the previous trial, and the reward received on the previous trial. The LSTM encodes this information incrementally into its cell state <italic toggle="yes">c</italic>, and then outputs a policy and value estimate (not shown). (Middle) The storage operation to long-term memory at a single LSTM time step. Storage is triggered when reward is received at the end of each two-step trial, at which point the agent appends a perceptual representation of the current context (a “cue”) along with its cell state to a non-parametric store of such items. (Right) The long-term memory retrieval operation which occurs on every time step. A search is carried out over the cues stored in long-term memory for the closest match to the current cue. The working memory activations associated with the closest match are retrieved and reinstated to the working memory state.</p></caption><graphic xlink:href="360537_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-23">The retrieved activations are reinstated through a learned gating function that arbitrates among the influences on the current working memory state of (1) current perceptual inputs, (2) the previous working memory state, and (3) the working memory state retrieved from long-term memory. This gating mechanism is a natural extension to the standard LSTM working memory, which uses gates to arbitrate between new inputs and the previous working memory state:
<disp-formula id="ueqn1" hwp:id="disp-formula-1">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="360537_ueqn1.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
where <italic toggle="yes">c<sub>t</sub></italic> is the current working memory state, <italic toggle="yes">c<sub>in</sub></italic> is the agent’s representation of its current input, <italic toggle="yes">c<sub>prev</sub></italic> is the working memory activation from the previous timestep, and o signifies elementwise multiplication.</p><p hwp:id="p-24">The gates <italic toggle="yes">i</italic> and <italic toggle="yes">f</italic> are values between zero and one that allow (or disallow) inputs and and past working memory activations into the current state. These gates are computed accordingly:
<disp-formula id="ueqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-3"><graphic xlink:href="360537_ueqn2.gif" position="float" orientation="portrait" hwp:id="graphic-4"/></alternatives>
</disp-formula>
where <italic toggle="yes">x</italic> is the perceptual input, <italic toggle="yes">h</italic> is a function of the previous working memory state, and the weight matrices <italic toggle="yes">W</italic> and bias vectors <italic toggle="yes">b</italic> contain learned parameters.</p><p hwp:id="p-25">To reinstate working memory activations retrieved from episodic memory without losing the current contents of working memory, our architecture adds the retrieved activations to the current working memory state, after passing them through a gate that is computed in a manner exactly analogous to the standard LSTM gates:
<disp-formula id="ueqn3" hwp:id="disp-formula-3">
<alternatives hwp:id="alternatives-4"><graphic xlink:href="360537_ueqn3.gif" position="float" orientation="portrait" hwp:id="graphic-5"/></alternatives>
</disp-formula>
<italic toggle="yes">c<sub>ltm</sub></italic> contains the retrieved activations from long-term memory. This reinstatement gate <italic toggle="yes">r</italic> is intended to learn to allow activations from episodic memory into working memory when they are useful, but not when they will interfere with the maintenance of important information in working memory.</p><p hwp:id="p-26">To illustrate how this architecture works in practice, consider the episodic two-step task, wherein the working memory keeps track of the reward probabilities at each outcome state. In order to infer these quantities, it must maintain information about recent actions and states. When the agent receives reward in the final step of a two-step trial, it will save the activations of its current working memory - which encode the agent’s outcome state and reward - to its long-term memory. These are saved along with a representation of the context stimulus, which models the participant’s visual representation of the object images or fractals in the human experiments (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-5" hwp:rel-id="ref-9">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-5" hwp:rel-id="ref-22">Vikbladh et al., 2017</xref>).</p><p hwp:id="p-27">At the beginning of a future two-step trial, the agent will encounter this same stimulus. It will then search its long-term memory for matches for that stimulus, and will retrieve the hidden state from the past trial. Crucially, this hidden state will encode the state the agent encountered at the end of the last exposure to that stimulus as well as the reward received and the action taken. Possessing this critical episodic information, the agent is able to exploit the structure of the episodic two-step task. Specifically, it can learn to implement model-based or model-free valuation with respect to trial information retrieved from long-term memory.</p></sec><sec id="s4" hwp:id="sec-5"><title hwp:id="title-7">Methods and Results</title><p hwp:id="p-28">After training, we assessed EMRL’s performance on a set of evaluation episodes which followed the same structure as the training episodes. To isolate the behavior of the <italic toggle="yes">learned</italic> learning algorithm operating in the activations, all data shown and described in this section were obtained with the weights frozen. To verify that the observed effects are consistent across trained agents, we performed the following analyses on 10 instances of each agent type, where each instance was trained with a different random seed. The reported statistical tests measure the variability across these agent instances. Agent optimization was performed by an implementation of asynchronous actor-critic (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Mnih et al., 2016</xref>).</p><p hwp:id="p-29">First, to determine whether EMRL’s episodic memory was performing effectively, we compared the reward EMRL acquired with that acquired by a control meta-RL agent (MRL; <xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3a</xref>) that was trained and tested in exactly the same way, but did not have access to episodic memory (i.e., its r-gate was always fixed to zero). EMRL achieved more reward overall than MRL (t(9)=4.28, p=1.21e-4). This increase in reward came entirely from the cued trials: the difference between reward obtained by EMRL and that obtained by MRL in the cued trials was highly significant (t(9)=7.205, p=1e-6), while the difference in uncued trials was not significant (t(9)=-1.09, p=0.289). These results provide evidence that EMRL was able to use its episodic memory to acquire the additional reward available during cued trials.</p><fig id="fig3" position="float" orientation="portrait" fig-type="figure" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3 xref-fig-3-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;360537v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3:</label><caption hwp:id="caption-3"><p hwp:id="p-30">EMRL achieves more reward than MRL and exhibits both incremental and episodic model-based behavior. Bar heights indicate means over 10 trained agents with different random seeds. Error bars indicate the standard error of the mean over the 10 trained agents. (a) Average reward obtained by MRL and EMRL on cued and uncued trials. EMRL earns more reward than MRL on cued trials, suggesting that EMRL can use its episodic memory to exploit the task’s episodic structure. For comparison, a random policy achieves 0.5 reward on this task. (b) Proportion of uncued trials in which EMRL repeated the action it took on the previous trial (<italic toggle="yes">t</italic> − 1), split by whether it received reward on <italic toggle="yes">t</italic> − 1 and whether the transition on <italic toggle="yes">t</italic> − 1 was common. The interaction between those two factors is a sign of model-based learning<sup>1</sup>. (c) Same as b, but for cued trials. (d) Same as b, but split by whether EMRL received reward on the past trial <italic toggle="yes">k</italic> when the cue was first encountered and whether the transition on trial <italic toggle="yes">k</italic> was common.</p></caption><graphic xlink:href="360537_fig3" position="float" orientation="portrait" hwp:id="graphic-6"/></fig><p hwp:id="p-31">Next, we asked whether EMRL exhibited the canonical patterns of incremental model-based and model-free behavior first described in the two-step task by (<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-6" hwp:rel-id="ref-9">Daw et al., 2011</xref>). We formally tested for these patterns of behavior by performing ANOVAs on the probabilities of repeating the previous action, with two binary factors: whether the previous trial was rewarded, and whether the previous trial had a common transition. A main effect of previous trial being rewarded would indicate a model-free strategy, while an interaction between previous trial being rewarded and previous trial being common would indicate a model-based strategy<sup><xref ref-type="fn" rid="fn1" hwp:id="xref-fn-4-1" hwp:rel-id="fn-4">1</xref></sup>. On uncued trials (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Figure 3b</xref>), we found a strong effect of the interaction term (F(1,9)=842, p=3.34e-10), indicating that the learned algorithm correctly exploited the transition structure of the task when no episodic information was available. This behavior reproduces the main two-step task result from <xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">Wang et al. (2017)</xref>. The main effect of reward was not significant, indicating the absence of model-free behavior (F(1,9)=3.462, p=0.096). The main effect of transition type was also not significant F(1,9)=2.944, p=0.120). On cued trials (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3c</xref>), we also found an effect of the interaction term (F(1,9)=53.78, p=4.4e-5), indicating that EMRL continued to use the incremental strategy during the cued trials. The main effect of reward was not significant, indicating the absence of model-free behavior (F(1,9)=0.170, p=0.690). The main effect of transition type was also not significant (F(1,9)=5.017, p=0.052).</p><p hwp:id="p-32">Next, most centrally, we asked whether EMRL could apply model-based reasoning to information retrieved from episodic memory (<xref ref-type="fig" rid="fig3" hwp:id="xref-fig-3-4" hwp:rel-id="F3">Figure 3d</xref>). We performed the same ANOVA described above, but using as factors: whether the past cue-associated trial was rewarded and whether the past cue-associated trial had a common transition. Since our task guaranteed receiving the same reward if the agent reached the same state as the past trial, the agent should prefer to take the opposite action as on the past trial if it experienced an uncommon transition and received reward on that trial. We indeed found a strong effect of the interaction term in this analysis (F(1,9)=36.1, p=2e-4), and non-significant main effects of reward and transition type (F(1,9)=4.354, p=0.665; F(1,9)=4.132, p=0.073). Note that we only performed this analysis on cued trials because the factors would be undefined on uncued trials.</p><p hwp:id="p-33">To supplement the ANOVA analysis, we fit a probabilistic choice model to EMRL’s behavior, similar to the model that <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-6" hwp:rel-id="ref-22">Vikbladh et al. (2017)</xref> fit to their human data. This model casts participants’ choices as a softmax of the weighted sum of valuations from the four valuation strategies. Fitting this model yields estimates of the weight on each strategy (see Supplement for full details). In accordance with the ANOVA results, binomial sign tests on the estimated weights (<xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>) revealed a significant contribution of incremental model-based learning (cued trials, p=0.002; uncued trials, p=0.002) and episodic model-based learning (cued trials, p=0.002), and no significant effect of incremental model-free learning (cued trials, p=0.754; uncued trials, p=0.109) or episodic model-free learning (cued trials, p=0.344).</p><fig id="fig4" position="float" orientation="portrait" fig-type="figure" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;360537v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4:</label><caption hwp:id="caption-4"><p hwp:id="p-34">Parameter estimates in a model with weighted contributions of four decision systems. The estimates provide evidence that incremental and episodic model-based valuations contribute to EMRL’s behavior. The model was fit to EMRL’s actions separately on cued and uncued trials. Each gray dot denotes the model fit parameter for one of the 10 trained agents. ** denotes significant difference from 0 at p&lt;0.005 by binomial sign test.</p></caption><graphic xlink:href="360537_fig4" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-35">Overall, the ANOVA and model fit results confirm that EMRL exhibits both incremental and episodic model-based learning, but not episodic model-free learning, in accord with the human behavior observed by <xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-7" hwp:rel-id="ref-22">Vikbladh et al. (2017)</xref>. The results show further that, like MRL (<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-3" hwp:rel-id="ref-23">Wang et al., 2018</xref>), EMRL does not use incremental model-free learning, opting instead for the model-based incremental learning that achieves greater reward in the two-step task.</p><sec id="s4a" hwp:id="sec-6"><title hwp:id="title-8">Analysis of Reinstatement Gate Activations</title><p hwp:id="p-36">To see what kind of gating function the r-gates learned, we carried out preliminary analyses of the r-gate activations. <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5a</xref> shows timecourses (averaged over 500 episodes) of the r-gate mean for a single trained agent. Each timecourse represents one two-step trial step; that is, whether it was a step where the agent took an action, saw an outcome, or received a reward. The timecourses show that the r-gate was more open during cued trials compared to uncued trials, consistent with the presence of useful episodic information on cued trials. Further, the r-gate was most open during the first stage of each trial. This is sensible, because this is the step on which reinstated information can be used to select an action. <xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5b</xref> compares the r-gate mean for a single trained agent on cued trials when the agent selected the optimal or the opposite action. The r-gate was significantly more open on correct trials, consistent with the idea that retrieved information is necessary to select the correct action on these trials.</p><fig id="fig5" position="float" orientation="portrait" fig-type="figure" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;360537v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5:</label><caption hwp:id="caption-5"><p hwp:id="p-37">(a) Timecourse of the mean values of the reinstatement gate averaged over 500 episodes, split by stage of the trial. (b) Mean values of the reinstatement gate on cued trials on which the agent selected the optimal action (correct) and on cued trials on which it selected the opposite action (incorrect), averaged over all units.</p></caption><graphic xlink:href="360537_fig5" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-38">While these effects are very clear, and the latter is highly significant, their magnitude is very small. Indeed, the r-gates allow a substantial proportion of the retrieved vector into working memory even during uncued trials when the information is not useful, and this proportion only increases slightly during the cued trials, where the information is critical. This suggests that mechanisms other than complete gating in/out are at play. For instance, the r-gates may subtly modify retrieved vectors to help the recurrent dynamics and policy layer determine whether or not to use the retrieved information. Further work will be needed to understand the solution the r-gates have found, and the insights gained may extend to biological gating systems. Alternatively, applying regularization, such as dropout, may lead to more easily interpretable neural mechanisms, as in <xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Banino et al. (2018)</xref>.</p></sec></sec><sec id="s5" hwp:id="sec-7"><title hwp:id="title-9">Discussion</title><p hwp:id="p-39">The experiments in this work establish that when trained via model-free learning on a task distribution with both incremental and episodic reward structure, EMRL learns to simultaneously execute incremental and episodic model-based learning algorithms. This ability to deploy and coordinate both learning algorithms mirrors that of the participants in the study by Vikbladh and colleagues, providing initial support for EMRL as a model of human decision making. EMRL thus provides an empirically grounded unified account of incremental and episodic learning processes, whereby a single model-free learning mechanism learns to execute and deploy the variety of learning algorithms observed in humans. In addition to support from behavioral data, the model accords in principle with a large neuroscientific literature supporting the notion that episodic memory retrieval recreates patterns of activity in neural circuits supporting working memory (<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">Cohen &amp; O’Reilly, 1996</xref>; <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Staresina, Henson, Kriegeskorte, &amp; Alink, 2012</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Hoskin et al., 2017</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Xiao et al., 2017</xref>). Further, EMRL’s system for controlling the influence of reinstated activations on working memory is formally equivalent to LSTM gates, which functionally resemble gating mechanisms hypothesized to operate in prefrontal cortex (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Chatham &amp; Badre, 2015</xref>)</p><p hwp:id="p-40">It is worth pointing out <italic toggle="yes">why</italic> EMRL learns to use the valuation strategies that it does, and does not learn to use the others (summarized in <xref ref-type="fig" rid="fig4" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>). In essence, optimized neural systems like MRL and EMRL will approach optimal behavior on their training distribution, subject to the limitations of the architecture design and efficacy of stochastic gradient learning. In this work, the training distribution was the episodic two-step task itself, which yields greater reward to model-based than model-free strategies. EMRL’s architecture, with essential features such as associative retrieval and differentiable working memory, enables a broad spectrum of strategies – including model-based and model-free – to be learned via gradient descent. Accordingly, EMRL learns to use model-based instead of model-free strategies because (1) it can, and (2) model-based strategies earn more reward on the training distribution. Future work may investigate ecologically inspired training distributions (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Anderson, 1990</xref>) that lead generic learning algorithms like MRL/EMRL to reproduce human-like deviations from optimality in laboratory tasks.</p><p hwp:id="p-41">The key takeaway from the success so far of EMRL is a proof of the sufficiency of a small set of well motivated architectural components, when trained to optimize a specific objective function, to produce a variety of episodic and incremental learning processes observed in humans. The architecture components are: 1) a recurrent working memory with 2) a non-parametric store of working memory activations that can be retrieved by context and reinstated through 3) a learned gating system. The objective function is total reward achieved on a distribution of learning tasks which contain both incremental and episodic structure.</p><p hwp:id="p-42">This model makes a number of predictions which may be tested through further empirical work:
<list list-type="bullet" hwp:id="list-2"><list-item hwp:id="list-item-4"><p hwp:id="p-43">Pattern reinstatement has been observed during the retrieval of static stimuli (e.g., <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">Bornstein &amp; Norman, 2017</xref>); EMRL predicts that such pattern reinstatement should also be measurable during the retrieval of memories that encode sequences of events or the results of computation carried out in WM.</p></list-item><list-item hwp:id="list-item-5"><p hwp:id="p-44">There is considerable evidence that gating mechanisms regulate the flow of information into and out of working memory (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Chatham &amp; Badre, 2015</xref>). Analogous experiments should find evidence for a similar mechanism that gates reinstated activations from long-term memory into working memory.</p></list-item><list-item hwp:id="list-item-6"><p hwp:id="p-45">It should be possible to modulate the degree of WM pattern reinstatement on a given task by training participants on variants of that task where memory retrieval is either useful or distracting. EMRL predicts specifically that pattern reinstatement in PFC should be affected by such manipulations, while reinstatement in hippocampus should be unaffected.</p></list-item></list></p><p hwp:id="p-46">In summary, this work presents a new theory that explains the collage of learning processes observed in humans during decision making as an interplay between working and episodic memory that is itself learned through training to maximize reward on a distribution of learning tasks. Future work may test the predictions made by this model and test the model’s ability to replicate additional sources of empirical data.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-10">Acknowledgements</title><p hwp:id="p-47">We would like to thank Siddhant Jayakumar, Charles Blundell, and Razvan Pascanu for helpful discussion and advice; the CogSci reviewers for insightful comments and feedback; and those at Google and DeepMind behind the codebases and infrastructure we used to build the agents.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-11">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Akam T."><surname>Akam</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Costa R."><surname>Costa</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-2">Simple plans or sophisticated habits? state, transition and learning interactions in the two-step task</article-title>. <source hwp:id="source-1">PLoS Comput Biol</source>, <volume>11</volume>(<issue>12</issue>), <fpage>e1004648</fpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="book" citation-type="book" ref:id="360537v1.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Anderson J. R."><surname>Anderson</surname>, <given-names>J. R.</given-names></string-name> (<year>1990</year>). <source hwp:id="source-2">The adaptive character of thought</source>. <publisher-name>Psychology Press</publisher-name>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Banino A."><surname>Banino</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barry C."><surname>Barry</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uria B."><surname>Uria</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blundell C."><surname>Blundell</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lillicrap T."><surname>Lillicrap</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mirowski P."><surname>Mirowski</surname>, <given-names>P.</given-names></string-name>, … others (<year>2018</year>). <article-title hwp:id="article-title-3">Vector-based navigation using grid-like representations in artificial agents</article-title>. <source hwp:id="source-3">Nature</source>, <volume>557</volume>(<issue>7705</issue>), <fpage>429</fpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="other" citation-type="journal" ref:id="360537v1.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Blundell C."><surname>Blundell</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uria B."><surname>Uria</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pritzel A."><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Y."><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ruderman A."><surname>Ruderman</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leibo J. Z."><surname>Leibo</surname>, <given-names>J. Z.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Hassabis D."><surname>Hassabis</surname>, <given-names>D.</given-names></string-name> (<year>2016</year>). <source hwp:id="source-4">Model-free episodic control</source>. <italic toggle="yes">arXiv preprint arXiv:1606.04460</italic>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Bornstein A. M."><surname>Bornstein</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khaw M. W."><surname>Khaw</surname>, <given-names>M. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-4">Reminders of past choices bias decisions for reward in humans</article-title>. <source hwp:id="source-5">Nature Communications</source>, <volume>8</volume>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Bornstein A. M."><surname>Bornstein</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Norman K. A."><surname>Norman</surname>, <given-names>K. A.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-5">Reinstated episodic context guides sampling-based decisions for reward</article-title>. <source hwp:id="source-6">Nature Neuroscience</source>, <volume>20</volume>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.7" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Chatham C. H."><surname>Chatham</surname>, <given-names>C. H.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Badre D."><surname>Badre</surname>, <given-names>D.</given-names></string-name> (<year>2015</year>). <article-title hwp:id="article-title-6">Multiple gates on working memory</article-title>. <source hwp:id="source-7">Current Opinion in Behavioral Sciences</source>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Cohen J. D."><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="O’Reilly R. C."><surname>O’Reilly</surname>, <given-names>R. C.</given-names></string-name> (<year>1996</year>). <article-title hwp:id="article-title-7">A preliminary theory of the interactions between prefrontal cortex and hippocampus that contribute to planning and prospective memory</article-title>. In <source hwp:id="source-8">Prospective memory: Theory and applications</source>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4 xref-ref-9-5 xref-ref-9-6"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gershman S. J."><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Seymour B."><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dolan R. J."><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name> (<year>2011</year>). <article-title hwp:id="article-title-8">Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source hwp:id="source-9">Neuron</source>, <volume>69</volume>(<issue>6</issue>), <fpage>1204</fpage>–<lpage>1215</lpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Dolan R. J."><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> (<year>2013</year>). <article-title hwp:id="article-title-9">Goals and habits in the brain</article-title>. <source hwp:id="source-10">Neuron</source>, <volume>80</volume>(<issue>2</issue>), <fpage>312</fpage>–<lpage>325</lpage>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Duncan K. D."><surname>Duncan</surname>, <given-names>K. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-10">Memory states influence value-based decisions</article-title>. <source hwp:id="source-11">Journal of Experimental Psychology: General</source>, <volume>145</volume>(<issue>11</issue>), <fpage>1420</fpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.12" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Gershman S. J."><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-11">Reinforcement learning and episodic memory in humans and animals: An integrative framework</article-title>. <source hwp:id="source-12">Annual review of psychology</source>, <volume>68</volume>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Graves A."><surname>Graves</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wayne G."><surname>Wayne</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Reynolds M."><surname>Reynolds</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harley T."><surname>Harley</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Danihelka I."><surname>Danihelka</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grabska-Barwińska A."><surname>Grabska-Barwińska</surname>, <given-names>A.</given-names></string-name>, … others (<year>2016</year>). <article-title hwp:id="article-title-12">Hybrid computing using a neural network with dynamic external memory</article-title>. <source hwp:id="source-13">Nature</source>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Hochreiter S."><surname>Hochreiter</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Schmidhuber J."><surname>Schmidhuber</surname>, <given-names>J.</given-names></string-name> (<year>1997</year>). <article-title hwp:id="article-title-13">Long short-term memory</article-title>. <source hwp:id="source-14">Neural computation</source>, <volume>9</volume>(<issue>8</issue>), <fpage>1735</fpage>–<lpage>1780</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Hoskin A. N."><surname>Hoskin</surname>, <given-names>A. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bornstein A. M."><surname>Bornstein</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norman K. A."><surname>Norman</surname>, <given-names>K. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Cohen J. D."><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-14">Refresh my memory: Episodic memory reinstatements intrude on working memory maintenance</article-title>. <source hwp:id="source-15">bioRxiv</source>, <volume>170720</volume>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="confproc" citation-type="confproc" ref:id="360537v1.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Lengyel M."><surname>Lengyel</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Dayan P."><surname>Dayan</surname>, <given-names>P.</given-names></string-name> (<year>2007</year>). <article-title hwp:id="article-title-15">Hippocampal contributions to control: The third way</article-title>. In <conf-name>Proc. of neural information processing systems, NIPS</conf-name>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><citation publication-type="confproc" citation-type="confproc" ref:id="360537v1.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Mnih V."><surname>Mnih</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Badia A. P."><surname>Badia</surname>, <given-names>A. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mirza M."><surname>Mirza</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Graves A."><surname>Graves</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lillicrap T. P."><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Harley T."><surname>Harley</surname>, <given-names>T.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Kavukcuoglu K."><surname>Kavukcuoglu</surname>, <given-names>K.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-16">Asynchronous methods for deep reinforcement learning</article-title>. In <conf-name>Proc. of int’l conf. on machine learning, ICML</conf-name>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><citation publication-type="other" citation-type="journal" ref:id="360537v1.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Pritzel A."><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uria B."><surname>Uria</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Srinivasan S."><surname>Srinivasan</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Puigdomènech A."><surname>Puigdomènech</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vinyals O."><surname>Vinyals</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hassabis D."><surname>Hassabis</surname>, <given-names>D.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Blundell C."><surname>Blundell</surname>, <given-names>C.</given-names></string-name> (<year>2017</year>). <source hwp:id="source-16">Neural episodic control</source>. <italic toggle="yes">arXiv preprint arXiv:1703.01988</italic>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2 xref-ref-19-3"><citation publication-type="confproc" citation-type="confproc" ref:id="360537v1.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Ritter S."><surname>Ritter</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang J. X."><surname>Wang</surname>, <given-names>J. X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kurth-Nelson Z."><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jayakumar S. M."><surname>Jayakumar</surname>, <given-names>S. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Blundell C."><surname>Blundell</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pascanu R."><surname>Pascanu</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Botvinick M."><surname>Botvinick</surname>, <given-names>M.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-17">Been there, done that: Meta-learning with episodic recall</article-title>. In <conf-name>Proceedings of the 35th international conference on machine learning</conf-name>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Staresina B. P."><surname>Staresina</surname>, <given-names>B. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Henson R. N."><surname>Henson</surname>, <given-names>R. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kriegeskorte N."><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Alink A."><surname>Alink</surname>, <given-names>A.</given-names></string-name> (<year>2012</year>). <article-title hwp:id="article-title-18">Episodic reinstatement in the medial temporal lobe</article-title>. <source hwp:id="source-17">Journal of Neuroscience</source>, <volume>32</volume>(<issue>50</issue>), <fpage>18150</fpage>–<lpage>18156</lpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="book" citation-type="book" ref:id="360537v1.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Sutton R. S."><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Barto A. G."><surname>Barto</surname>, <given-names>A. G.</given-names></string-name> (<year>1998</year>). <source hwp:id="source-18">Reinforcement learning: An introduction</source> (Vol. <volume>1</volume>). <publisher-name>MIT press</publisher-name> <publisher-loc>Cambridge</publisher-loc>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2 xref-ref-22-3 xref-ref-22-4 xref-ref-22-5 xref-ref-22-6 xref-ref-22-7"><citation publication-type="confproc" citation-type="confproc" ref:id="360537v1.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Vikbladh O."><surname>Vikbladh</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Daw N."><surname>Daw</surname>, <given-names>N.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-19">Episodic contributions to model-based reinforcement learning</article-title>. In <conf-name>Annual conference on cognitive computational neuroscience, CCN</conf-name>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2 xref-ref-23-3 xref-ref-23-4"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Wang J. X."><surname>Wang</surname>, <given-names>J. X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kurth-Nelson Z."><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kumaran D."><surname>Kumaran</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tirumala D."><surname>Tirumala</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Soyer H."><surname>Soyer</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leibo J. Z."><surname>Leibo</surname>, <given-names>J. Z.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Botvinick M."><surname>Botvinick</surname>, <given-names>M.</given-names></string-name> (<year>2018</year>). <article-title hwp:id="article-title-20">Prefrontal cortex as a meta-reinforcement learning system</article-title>. <source hwp:id="source-19">Nature neuroscience</source>, <volume>21</volume>(<issue>6</issue>), <fpage>860</fpage>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2"><citation publication-type="other" citation-type="journal" ref:id="360537v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Wang J. X."><surname>Wang</surname>, <given-names>J. X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kurth-Nelson Z."><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tirumala D."><surname>Tirumala</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Soyer H."><surname>Soyer</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leibo J. Z."><surname>Leibo</surname>, <given-names>J. Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Munos R."><surname>Munos</surname>, <given-names>R.</given-names></string-name>, … <string-name name-style="western" hwp:sortable="Botvinick M."><surname>Botvinick</surname>, <given-names>M.</given-names></string-name> (<year>2017</year>). <source hwp:id="source-20">Learning to reinforcement learn</source>. Retrieved from arXivpreprintarXiv: <pub-id pub-id-type="arxiv">1611.05763</pub-id></citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Wimmer G. E."><surname>Wimmer</surname>, <given-names>G. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Braun E. K."><surname>Braun</surname>, <given-names>E. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Daw N. D."><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Shohamy D."><surname>Shohamy</surname>, <given-names>D.</given-names></string-name> (<year>2014</year>). <article-title hwp:id="article-title-21">Episodic memory encoding interferes with reward learning and decreases striatal prediction errors</article-title>. <source hwp:id="source-21">J Neurosci</source>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Wimmer G. E."><surname>Wimmer</surname>, <given-names>G. E.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Buechel C."><surname>Buechel</surname>, <given-names>C.</given-names></string-name> (<year>2016</year>). <article-title hwp:id="article-title-22">Reactivation of reward-related patterns from single past episodes supports memory-based decision making</article-title>. <source hwp:id="source-22">Journal of Neuroscience</source>, <volume>36</volume>(<issue>10</issue>), <fpage>2868</fpage>–<lpage>2880</lpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><citation publication-type="journal" citation-type="journal" ref:id="360537v1.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Xiao X."><surname>Xiao</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dong Q."><surname>Dong</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gao J."><surname>Gao</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Men W."><surname>Men</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poldrack R. A."><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name name-style="western" hwp:sortable="Xue G."><surname>Xue</surname>, <given-names>G.</given-names></string-name> (<year>2017</year>). <article-title hwp:id="article-title-23">Transformed neural pattern reinstatement during episodic memory retrieval</article-title>. <source hwp:id="source-23">Journal of Neuroscience</source>, <volume>37</volume>.</citation></ref></ref-list><sec id="s6" sec-type="supplementary-material" hwp:id="sec-8"><title hwp:id="title-12">Supplement: Model Fit Details</title><p hwp:id="p-48">The probability of the agent taking action <italic toggle="yes">a</italic><sub>0</sub> in the starting state <italic toggle="yes">s</italic><sub>0</sub> at trial <italic toggle="yes">t</italic> was modeled as the softmax of the weighted sum of the differences between the value estimates for the two actions for each of the four valuation strategies:
<disp-formula id="ueqn4" hwp:id="disp-formula-4">
<alternatives hwp:id="alternatives-5"><graphic xlink:href="360537_ueqn4.gif" position="float" orientation="portrait" hwp:id="graphic-9"/></alternatives>
</disp-formula>
where <italic toggle="yes">Q</italic><sup>diff</sup> = <italic toggle="yes">Q</italic>(<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic><sub>0</sub>) – <italic toggle="yes">Q</italic>(<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic><sub>1</sub>). At each trial <italic toggle="yes">t</italic>, the incremental model-free value estimate <italic toggle="yes">Q<sub>if</sub></italic> (<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic>) for the action <italic toggle="yes">a</italic> taken on trial <italic toggle="yes">t</italic> was updated as
<disp-formula id="ueqn5" hwp:id="disp-formula-5">
<alternatives hwp:id="alternatives-6"><graphic xlink:href="360537_ueqn5.gif" position="float" orientation="portrait" hwp:id="graphic-10"/></alternatives>
</disp-formula></p><p hwp:id="p-49">The incremental model-based value estimate <italic toggle="yes">Q<sub>ib</sub></italic>(<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic>) was computed as
<disp-formula id="ueqn6" hwp:id="disp-formula-6">
<alternatives hwp:id="alternatives-7"><graphic xlink:href="360537_ueqn6.gif" position="float" orientation="portrait" hwp:id="graphic-11"/></alternatives>
</disp-formula>
where <italic toggle="yes">s</italic><sub>1</sub> and <italic toggle="yes">s</italic><sub>2</sub> are the two second-stage states, <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>) is the agent’s estimate of the reward received in state <italic toggle="yes">s</italic> and <italic toggle="yes">p</italic>(<italic toggle="yes">s,a</italic>) is the probability of transitioning from state <italic toggle="yes">s</italic><sub>0</sub> into state <italic toggle="yes">s</italic> after taking action <italic toggle="yes">a. R</italic>(<italic toggle="yes">s</italic>) was updated as
<disp-formula id="ueqn7" hwp:id="disp-formula-7">
<alternatives hwp:id="alternatives-8"><graphic xlink:href="360537_ueqn7.gif" position="float" orientation="portrait" hwp:id="graphic-12"/></alternatives>
</disp-formula></p><p hwp:id="p-50">The episodic model-free value estimate <italic toggle="yes">Q<sub>ef</sub></italic> (<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic>) was set equal to 1 if on the past (cue-associated) trial, the agent took action <italic toggle="yes">a</italic> and was rewarded, or took the other action and was not rewarded; <italic toggle="yes">Q<sub>ef</sub></italic> (<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic>) was 0 otherwise. <italic toggle="yes">Q<sub>eb</sub></italic>(<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic>) was 1 if on the past (cue-associated) trial, the agent took action <italic toggle="yes">a</italic> and this resulted in a common transition with reward or an uncommon transition without reward, or if the agent took the other action and this resulted in a common transition without reward or an uncommon transition with reward. <italic toggle="yes">Q<sub>eb</sub></italic>(<italic toggle="yes">s</italic><sub>0</sub>,<italic toggle="yes">a</italic>) was 0 otherwise.</p><p hwp:id="p-51">All incrementally learned values were updated with the same learning rate α, for a total of five parameters. These were estimated by maximum likelihood on the concatenated data of all 500 episodes, with incrementally learned values reset to 0.5 at the beginning of each episode. Cued and uncued trials were fit separately. Note that episodic valuation is undefined during uncued trials.</p></sec><fn-group hwp:id="fn-group-1"><fn id="fn1" hwp:id="fn-4" hwp:rev-id="xref-fn-4-1"><label>1</label><p hwp:id="p-52"><xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Akam, Costa, and Dayan (2015)</xref> demonstrated that there exist sophisticated model-free strategies which can, under the right conditions, produce this reward-by-transition interaction. See <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-4" hwp:rel-id="ref-23">Wang et al. (2018)</xref>, Supp. Figures 6 and 7 for an in depth analysis of this issue with respect to MRL.</p></fn></fn-group></back></article>
