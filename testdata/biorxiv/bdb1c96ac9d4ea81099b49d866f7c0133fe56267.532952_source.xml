<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/532952</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;532952</article-id><article-id pub-id-type="other" hwp:sub-type="slug">532952</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">532952</article-id><article-id pub-id-type="other" hwp:sub-type="tag">532952</article-id><article-version>1.5</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Ecology" hwp:journal="biorxiv"><subject>Ecology</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Correspondence: <email hwp:id="email-1">ben.weinstein@weecology.org</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2176-7935</contrib-id><name name-style="western" hwp:sortable="Weinstein Ben G."><surname>Weinstein</surname><given-names>Ben G.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-2176-7935"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Marconi Sergio"><surname>Marconi</surname><given-names>Sergio</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Bohlman Stephanie"><surname>Bohlman</surname><given-names>Stephanie</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Zare Alina"><surname>Zare</surname><given-names>Alina</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="White Ethan"><surname>White</surname><given-names>Ethan</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3"><label>1</label><institution hwp:id="institution-1">Department of Wildlife Ecology and Conservation, University of Florida</institution>, Gainesville, FL 32603, <country>USA</country>; <email hwp:id="email-2">sergio.marconi@weecology.org</email> (S.M.); <email hwp:id="email-3">ethanwhite@ufl.edu</email> (E.W.)</aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">School of Forest Resources and Conservation, University of Florida</institution>, Gainesville, FL 32603, <country>USA</country>; <email hwp:id="email-4">sbohlman@ufl.edu</email></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Department of Electrical and Computer Engineering, University of Florida</institution>, Gainesville, FL 32601, <country>USA</country>; <email hwp:id="email-5">azare@ufl.edu</email></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2019-01-28T15:36:11-08:00">
    <day>28</day><month>1</month><year>2019</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-02-12T11:15:29-08:00">
    <day>12</day><month>2</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2019-01-28T15:44:34-08:00">
    <day>28</day><month>1</month><year>2019</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-02-12T11:18:46-08:00">
    <day>12</day><month>2</month><year>2021</year>
  </pub-date><elocation-id>532952</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2019-01-28"><day>28</day><month>1</month><year>2019</year></date>
<date date-type="rev-recd" hwp:start="2021-02-12"><day>12</day><month>2</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-02-12"><day>12</day><month>2</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">Â© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="532952.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/532952v5.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="532952.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/532952v5/532952v5.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/532952v5/532952v5.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-2">Remote sensing can transform the speed, scale, and cost of biodiversity and forestry surveys. Data acquisition currently outpaces the ability to identify individual organisms in high resolution imagery. We outline an approach for identifying tree-crowns in RGB imagery while using a semi-supervised deep learning detection network. Individual crown delineation has been a long-standing challenge in remote sensing and available algorithms produce mixed results. We show that deep learning models can leverage existing Light Detection and Ranging (LIDAR)-based unsupervised delineation to generate trees that are used for training an initial RGB crown detection model. Despite limitations in the original unsupervised detection approach, this noisy training data may contain information from which the neural network can learn initial tree features. We then refine the initial model using a small number of higher-quality hand-annotated RGB images. We validate our proposed approach while using an open-canopy site in the National Ecological Observation Network. Our results show that a model using 434,551 self-generated trees with the addition of 2848 hand-annotated trees yields accurate predictions in natural landscapes. Using an intersection-over-union threshold of 0.5, the full model had an average tree crown recall of 0.69, with a precision of 0.61 for the visually-annotated data. The model had an average tree detection rate of 0.82 for the field collected stems. The addition of a small number of hand-annotated trees improved the performance over the initial self-supervised model. This semi-supervised deep learning approach demonstrates that remote sensing can overcome a lack of labeled training data by generating noisy data for initial training using unsupervised methods and retraining the resulting models with high quality labeled data.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">Deep Learning</kwd><kwd hwp:id="kwd-2">Trees</kwd><kwd hwp:id="kwd-3">Detection</kwd><kwd hwp:id="kwd-4">Remote Sensing</kwd><kwd hwp:id="kwd-5">LIDAR</kwd><kwd hwp:id="kwd-6">RGB</kwd><kwd hwp:id="kwd-7">NEON</kwd></kwd-group><counts><page-count count="13"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes><fn-group content-type="external-links" hwp:id="fn-group-1"><fn fn-type="dataset" hwp:id="fn-1"><p hwp:id="p-4">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/weecology/DeepLidar/" ext-link-type="uri" xlink:href="https://github.com/weecology/DeepLidar/" hwp:id="ext-link-2">https://github.com/weecology/DeepLidar/</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1.</label><title hwp:id="title-4">Introduction</title><p hwp:id="p-5">Image-based artificial intelligence can advance our understanding of individual organisms, species, and ecosystems by greatly increasing the scale and efficiency of data collection [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>]. The growing availability of sub-meter airborne imagery brings opportunities for the remote sensing of biological landscapes that scales from individual organisms to global systems. However, the laborious, non-reproducible, and costly annotation of these datasets limits the use of this imagery [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>].</p><p hwp:id="p-6">Tree detection is a central task in forestry and ecosystem research and both commercial and scientific applications rely on delineating individual tree crowns from imagery [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>,<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>]. While there has been considerable research in unsupervised tree detection while using airborne LIDAR (Light Detection and Ranging; a sensor that uses laser pulses to map three-dimensional structure) [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">3</xref>,<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">6</xref>], less is known regarding tree detection in RGB (red, green, blue) orthophotos. When compared to LIDAR, two dimensional RGB orthophotos are less expensive to acquire and easier to process, but they lack direct three-dimensional information on crown shape. Effective RGB-based tree detection would unlock data at much larger scales due to increasing satellite-based RGB resolution and the growing use of uncrewed aerial vehicles. Initial studies of tree detection in RGB imagery focused on pixel-based methods and watershed algorithms to find local maxima among the pixels to create potential tree crowns [<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>]. When combined with hand-crafted rules on tree geometries, these approaches separately performed tree-detection and crown delineation [<xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">6</xref>,<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]. The need to hand-craft tree geometry rules makes it a challenge to create a single approach that encompass a range of tree types [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>].</p><p hwp:id="p-7">Deep learning is a well-established method for detecting and identifying objects in RGB images, but it has only recently been applied to vegetation detection [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>,<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>]. When compared to previous rule-based approaches, deep learning has three features that make it ideal for tree detection. First, convolutional neural networks (CNNs) directly delineate objects of interest from training data rather than using hand-crafted pixel features. This reduces the expertise that is required for each use-case and improves the transferability among projects [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>]. Second, CNNs learn hierarchical combinations of image features that focus on the object-level, rather than pixel-level, representations of objects. Finally, neural networks are re-trainable to incorporate the idiosyncrasies of individual datasets. This allows for models to be refined with data from new local areas without discarding information from previous training sets.</p><p hwp:id="p-8">The challenge for applying deep learning to natural systems is the need for large training datasets. A lack of training data is a pervasive problem in remote sensing due to the cost of data collection and annotation [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>]. In addition, the spatial extent of training data often prohibits the field-based verification of annotated objects. For tree detection, the high variation in tree crown appearance, due to taxonomy, health status, and human management, increases the risk of overfitting when using small amounts of training data [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref>]. One approach to addressing the data limitation in deep learning is âself-supervised learningâ (<italic toggle="yes">sensus</italic> [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]), which uses unsupervised methods to generate training data that is used to train supervised models [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>]. This approach has recently been applied to remote sensing for hyperspectral image classification [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>]. Self-supervision, which only relies on unlabeled data, can be combined with labeled data in a semi-supervised framework (<italic toggle="yes">sensu</italic> [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-2" hwp:rel-id="ref-14">14</xref>]), which may improve deep learning on limited training data by providing neural networks the opportunity to learn generalized features on a wider array of training examples, followed by retraining on a smaller number of high quality annotations [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>]. It is unknown whether moderate to low quality annotations can be used to generate trees for initial model training, given the imperfect nature of existing unsupervised tree delimitation approaches.</p></sec><sec id="s2" hwp:id="sec-2"><label>2.</label><title hwp:id="title-5">Materials and Methods</title><p hwp:id="p-9">We propose a semi-supervised pipeline for detecting tree crowns based on RGB data. <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref> outlines this pipeline. In the proposed workflow, a LIDAR unsupervised algorithm generates initial tree predictions. The bounding box for each tree is extracted and the corresponding RGB crop is used to train an initial deep learning model. Subsequently, using this self-supervised model as a starting point, we retrain the model using a small number of hand-annotations to correct errors from the unsupervised detection. The LIDAR data is only used to initialize the training of the network. It is not used for the final prediction step. The result is a deep learning neural network that combines unsupervised and supervised approaches to perform tree delineation in new RGB imagery without the need for co-registered LIDAR data. This provides the potential for expanding the use of deep learning in remote sensing applications with limited labeled data by exploring whether generating hundreds of thousands of noisy labels will yield improved performance, even though these labeled data are imperfect due to the limitations of the generative algorithm [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>].</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-10">A conceptual figure of the proposed semi-supervised pipeline. A Light Detection and Ranging (LIDAR)-based unsupervised detection generates initial training data for a self-supervised (red, green, blue (RGB) deep learning model. The model is then retrained based on a small number of hand-annotated trees to create the full model.</p></caption><graphic xlink:href="532952v5_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><sec id="s2a" hwp:id="sec-3"><label>2.1.</label><title hwp:id="title-6">Study Site and Field Data</title><p hwp:id="p-11">We used data from the National Ecological Observatory Network (NEON) site at the San Joaquin Experimental Range in California to assess our proposed approach (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>). The site contains an open woodland of live oak (<italic toggle="yes">Quercus agrifolia</italic>), blue oak (<italic toggle="yes">Quercus douglasii</italic>), and foothill pine (<italic toggle="yes">Pinus sabiniana</italic>) forest. The majority of the site is a single-story canopy with mixed understory of herbaceous vegetation.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-12">Map showing the location of the study site in the western United States.</p></caption><graphic xlink:href="532952v5_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-13">The aerial remote sensing data products were provided by the NEON Airborne Observation Platform. We used the NEON 2018 âclassified LiDAR point cloudâ data product (NEON ID: DP1.30003.001) and the âorthorectified camera mosaicâ (NEON ID: DP1.30010.001). The LiDAR data consist of three-dimensional (3D) spatial point coordinates (4-6 points/m<sup>2</sup>). which provides high resolution information regarding crown shape and height. The RGB data are a 1km Ã 1km mosaic of individual images with a cell size of 0.1 meters (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref>). Both data products are georeferenced in the UTM projection Zone 11. In addition to airborne data, NEON field teams semi-annually catalog âWoody Plant Vegetation Structureâ (NEON ID: DP1.10098.001), which lists the tag and species identity of trees with diameter at breast height&gt; 10cm in 40m Ã 40m plots at the site. For each tagged tree, the trunk location was obtained while using the azimuth and distance to the nearest georeferenced point within the plot. All the data are publicly available on the NEON Data Portal (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://data.neonscience.org/" ext-link-type="uri" xlink:href="http://data.neonscience.org/" hwp:id="ext-link-3">http://data.neonscience.org/</ext-link>). All code for this project is available on GitHub (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/weecology/DeepLidar" ext-link-type="uri" xlink:href="https://github.com/weecology/DeepLidar" hwp:id="ext-link-4">https://github.com/weecology/DeepLidar</ext-link>) and archived on Zenodo [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>].</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-14">The San Joaquin, CA (SJER) site (<bold>C</bold>) in National Ecological Observation Network contains 148 1 km<sup>2</sup> tiles (<bold>B</bold>), each with a spatial resolution of 0.1 m. For our analysis, we further divided each tile in 40 Ã 40 m windows (<bold>A</bold>) for individual tree prediction (n = 729 per 1 km<sup>2</sup> tile).</p></caption><graphic xlink:href="532952v5_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><p hwp:id="p-15">For hand annotations, we selected two 1 km Ã 1 km RGB tiles and used the program RectLabel (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://rectlabel.com/" ext-link-type="uri" xlink:href="https://rectlabel.com/" hwp:id="ext-link-5">https://rectlabel.com/</ext-link>) to draw bounding boxes around each visible tree. We chose not to include snags, or low bushes that appeared to be non-woody. In total, we hand-annotated 2848 trees for the San Joaquin site. In addition to the two 1 km tiles, we hand-annotated canopy bounding boxes on the cropped RGB images for each NEON field plot (n = 34), which were withheld from training and were used as a validation dataset.</p></sec><sec id="s2b" hwp:id="sec-4"><label>2.2.</label><title hwp:id="title-7">Unsupervised LIDAR Detection</title><p hwp:id="p-16">We tested three existing unsupervised algorithms for use in generating trees for the self-supervised portion of the workflow [19â21]. Existing unsupervised algorithms yield imperfect crown delineations, in part, because: 1) the algorithms are not designed to learn the specifics of different regions and datasets; 2) it is difficult to design hand-crafted features that are flexible enough to encompass the high variability in tree appearance; 3) distinguishing between trees and vertical objects, such as boulders and artificial structures, can be difficult with only three-dimensional LIDAR data. We evaluated three available unsupervised LIDAR detection algorithms while using the recall and precision statistics for hand-annotated images (see Model evaluation) in order to choose the best performing algorithm to generate training labels [19â21]. We then used the best performing method ([<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>]) to create initial self-supervised tree predictions in the LIDAR point cloud. This algorithm uses a canopy height model and a threshold of tree height to crown width to cluster the LIDAR cloud into individual trees (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>). We used a canopy height model of 0.5m resolution to generate local tree tops and a maximum crown diameter of 60% of tree height. A bounding box was automatically drawn over the entire set of points assigned to each tree to create the training data. In total, we generated 434,551 unsupervised tree labels to use during model training. By pretraining the RGB network on these unsupervised labels, the model learns a wider variety of tree shapes and appearances than would be possible when solely using hand annotated training data.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-17">Example results from the unsupervised lidar algorithm [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>], as implemented in the R liDR package [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]. Two plots from the San Joaquin National Ecological Observatory Network (NEON) site are shown (SJER_009, SJER_010).</p></caption><graphic xlink:href="532952v5_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s2c" hwp:id="sec-5"><label>2.3.</label><title hwp:id="title-8">Deep Learning RGB Detection</title><p hwp:id="p-18">Convolutional neural networks are often used for object detection, due to their ability to represent semantic information as combinations of image features. Early applications passed a sliding window over the entire image and treated each window as a separate classification problem. This was improved by considering the potential detection boxes that are generated by image segmentation techniques [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">23</xref>] or by combining the bounding box proposal and classification into a single deep learning framework [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>]. We chose the retinanet one-stage detector [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>,<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>], which allows for pixel information to be shared at multiple scales, from individual pixels to groups of connected objects for learning both bounding boxes and image classes. Retinanet differs from other object detection frameworks, such as RCNN, by combining object detection and classification into a single network. This allows for faster training and it decreases the sensitivity to the number of box proposals among the images [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>]. We used a resnet-50 classification backbone that was pretrained on the ImageNet dataset [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]. We experimented with deeper architectures (resnet-101 and resnet-152), but found no improvement where offset increased the training time.</p><p hwp:id="p-19">We first cut the tile into smaller windows for model training, since the entire 1 km RGB tile cannot fit into GPU memory. We experimented with a number of different window sizes and found the optimal performance at 400 Ã 400 pixels due to a balance between memory constraints and providing the model sufficient spatial context for tree detection. We allowed each window to overlap by 5%, to ensure that we captured all trees that were potentially divided among images during cropping. This resulted in 729 windows per 1 km tile. The order of tiles and windows were randomized before training to minimize overfitting among epochs. Using the pool of unsupervised tree predictions, we trained the network with a batch size of 6 on a Tesla K80 GPU for eight epochs. For each predicted tree, the model returns a bounding box and a confidence score (0â1). After prediction, we passed each image through a non-max suppression filter to remove the predicted boxes that overlapped by more than 15%, only maintaining the box with the superior predicted score. Finally, we removed boxes within confidence scores less than 0.2.</p></sec><sec id="s2d" hwp:id="sec-6"><label>2.4.</label><title hwp:id="title-9">Model Evaluation</title><p hwp:id="p-20">We used the NEON woody vegetation data to evaluate tree recall while using field-collected points corresponding to individual tree stems (n = 111 trees). A field-collected tree point was considered to be correctly predicted if the point fell within a predicted bounding box. This is a more conservative approach than most other studies, where the field-collected tree point is considered to be correctly predicted if an edge of the bounding box falls within a horizontal search radius (e.g., 3 m in [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>] to 8 m in [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>]). Due to these variations in accuracy measurement, it is difficult to establish state-of-art performance, but 70â80% detection rate between the predicted trees and field located trees is typical [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">5</xref>,<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>]. There are too few previous attempts at individual tree crown prediction to provide an expectation for accuracy, given the variation in tree appearance and segmentation difficulty.</p><p hwp:id="p-21">We computed recall and precision based on an intersection-over-union score of greater than 0.5 for each predicted crown to evaluate the hand-annotated crown areas. The intersection-over-union evaluation metric measures the area of overlap divided by the area of union of the ground truth bounding box and the predicted bounding box. Direct comparisons of predicted and observed crown overlap are rarely performed due to the difficulty of collecting data for a sufficient number of validation examples. The most common approach is to compare the predicted crown area to a matched tree, such as in [<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>] or use per pixel overlap in visually annotated data [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-4" hwp:rel-id="ref-9">9</xref>,<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>]. When compared to previous works, our use of a minimum 0.5 intersection-over-union score is more stringent. We chose this value, because it more closely resembles the required accuracy for forestry and ecological investigations [<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. Finally, to provide a baseline of comparison, we reran the evaluation data with a model trained solely using the hand-annotated data. This allows for a direct comparison of the contribution of high-quality annotations when compared to the self-supervised model or the full model combining both self-supervision and hand-annotation data.</p></sec></sec><sec id="s3" hwp:id="sec-7"><label>3.</label><title hwp:id="title-10">Results</title><p hwp:id="p-22">Initial exploration of existing LIDAR-based tree detection tools showed that the best performing algorithm [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>] was able to correctly recall the crown area of 14% of trees at intersection-over-union score of 0.5 (<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>). Challenges included the over-segmentation of large individual trees, erroneous predicted tree objects based on imperfections in the ground model, and the inclusion of non-tree vertical objects (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>).</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2 xref-fig-5-3 xref-fig-5-4 xref-fig-5-5 xref-fig-5-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-5"><p hwp:id="p-23">Predicted individual tree crowns for the unsupervised lidar (<bold>A</bold>,<bold>B</bold>), self-supervised RGB (<bold>C</bold>,<bold>D</bold>) and full (semi-supervised) model (<bold>E</bold>,<bold>F</bold>) for two NEON tower plots, SJER_015 (<bold>A</bold>,<bold>C</bold>,<bold>E</bold>), and SJER_053 (<bold>B</bold>,<bold>D</bold>,<bold>F</bold>) at the San Joaquin, CA site. For each tree prediction, the detection probability is shown in white.</p></caption><graphic xlink:href="532952v5_fig5" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-6"><p hwp:id="p-24">Exploratory analysis of lidar-based unsupervised algorithms. Recall and precision statistics are shown for intersection-over-union with a threshold of 0.5 overlap for the hand annotated trees on the NEON field plots (n = 271 trees).</p></caption><graphic xlink:href="532952v5_tbl1" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap><p hwp:id="p-25">We extracted RGB crops and pretrained the RGB neural network using the bounding boxes from the <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Silva et al. (2016)</xref> predictions. This self-supervised network had a field collected stem recall of 0.83, and a hand-annotated crown area recall of 0.53 with a precision of 0.32. Retraining the self-supervised model with hand-annotated trees increased the recall of the hand annotated tree crowns to 0.69 with a precision of 0.61 (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>, <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5</xref>). The field collected stem recall did not meaningfully change among the models. We anticipate the remaining stems that were not captured are either not visible in the airborne image or belong to trees judged to be too small based on the training data.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-7"><p hwp:id="p-26">Evaluation metrics for each of the models. All evaluation was conducted on the 34 NEON field plots. Stem recall was calculated using the field-collected tree stem locations (n = 111 trees). Precision and recall for crown overlap was calculated on hand-annotated bounding boxes around each tree crown (n = 271 trees) with a minimum predicted probability threshold of 0.5.</p></caption><graphic xlink:href="532952v5_tbl2" position="float" orientation="portrait" hwp:id="graphic-7"/></table-wrap><p hwp:id="p-27">By comparing the images of the predictions from the unsupervised LIDAR detection, the self-supervised RGB deep learning model, and the combined full model, we can learn about the contributions of each stage of the pipeline. The LIDAR unsupervised detection does a good job of identifying trees versus background based on height. Most of the small trees are well segmented, but there is consistent over-segmentation of the large trees, with multiple crown predictions abutting together. Visual inspection shows that these predictions represent multiple major branches of a single large tree, rather than multiple small trees (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-3" hwp:rel-id="F5">Figure 5A</xref>). These large trees are more accurately segmented in the self-supervised RGB model, but there is a proliferation of bounding boxes, and overall lower confidence scores, even for well-resolved trees (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-4" hwp:rel-id="F5">Figure 5D</xref>). This is shown in the precision-recall curves for the hand-annotated validation data, which has a higher level of precision for the same level of bounding box recall (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6</xref>).</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-8"><p hwp:id="p-28">Precision-recall curves for the hand-annotated NEON plots. For each model, we calculated the proportion of correctly predicted boxes for score thresholds [0,0.1, â¦,0.7]. An annotation was considered to be correctly predicted if the intersection-over-union (IoU) score was greater than 0.5. The recall and precision scores for the initial lidar-based unsupervised algorithm is shown in black X.</p></caption><graphic xlink:href="532952v5_fig6" position="float" orientation="portrait" hwp:id="graphic-8"/></fig><p hwp:id="p-29">The full model reduces the extraneous boxes and improves the segmentation of large trees by combining the self-supervised and the hand annotated datasets (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7</xref>). The full model has optimal performance in areas of well-spaced large trees (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7B</xref>), but it tends to under-segment small clusters of trees (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Figure 7C</xref>).</p><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-9"><p hwp:id="p-30">Predictions from the full model on data withheld from model training. Canopy complexity increases from (<bold>A</bold>) well-defined large trees to (<bold>B</bold>) mixed-species canopies to (<bold>C</bold>) tightly packed clusters of trees. As canopy complexity increases, the full model tends to under-segment small tree clusters.</p></caption><graphic xlink:href="532952v5_fig7" position="float" orientation="portrait" hwp:id="graphic-9"/></fig></sec><sec id="s4" hwp:id="sec-8"><label>4.</label><title hwp:id="title-11">Discussion</title><p hwp:id="p-31">We built a neural network-based pipeline for identifying individual trees in RGB imagery using recent developments in deep learning. Commercial high resolution RGB data is increasingly available at near global scales, which means that accurate RGB based crown delineation methods could be used to detect overstory trees at unprecedented extents. We used an unsupervised LIDAR tree detection algorithm to generate labels for initial training to address the long-standing challenge of a lack of labeled training data. This self-supervised approach allows for the network to learn the general features of trees, even if the LIDAR-based unsupervised detection is imperfect. The addition of only 2848 hand-annotated trees generated a final model that performed well when applied to a large geographic area. This approach opens the door for the use of deep learning in airborne biodiversity surveys, despite the persistent lack of annotated data in the forestry and ecology datasets.</p><p hwp:id="p-32">Many of the false positives in our evaluation dataset were due to disagreements between the hand annotations, unsupervised LIDAR pretraining, and RGB prediction in what defines a tree. For example, small trees were often considered too low for inclusion in the LIDAR algorithm (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-5" hwp:rel-id="F5">Figure 5A</xref>), whereas they were included in the full model based on the hand-annotations (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-6" hwp:rel-id="F5">Figure 5B</xref>). Similarly, large bushes were sometimes included in hand annotations due to the difficulty of determining the overall woody structure. When deploying these models to the applied problems, it will be important to have strict quantitative guidelines that define tree definitions. Where LIDAR data is available, draping the two-dimensional (2D) boxes over the 3D point cloud to filter out the points based on vertical height should be useful for improving precision. It should be noted that the quantitative results are likely biased toward the RGB model, since looking at the RGB, and not the LIDAR data, made the hand-annotations. However, the good recall rate for the field-collected stems suggests that hand annotations were useful in capturing field conditions. An unexpected benefit of the RGB model was the ability to discriminate trees from other vertical objects, such as houses or poles, despite a lack of distinction in the unsupervised LIDAR training data (<xref rid="fig8" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure 8</xref>). This may be useful in urban tree detection and other non-forested sites.</p><fig id="fig8" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;532952v5/FIG8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">fig8</object-id><label>Figure 8.</label><caption hwp:id="caption-10"><p hwp:id="p-33">Improvement in prediction quality during the training pipeline. (<bold>A</bold>) Bounding boxes from the lidar-based unsupervised detection erroneously identified artificial structures as trees. (<bold>B</bold>) Predictions from the self-supervised RGB model showed that the addition of RGB data diminished the effect of incorrectly labeled training data, with only edges of the artificial structures maintained as tree predictions. (<bold>C</bold>) In the full semi-supervised model, combining the self-supervised RGB data with hand-annotations eliminated the influence of the original misclassification in the training data, while still capturing the majority of trees in the image.</p></caption><graphic xlink:href="532952v5_fig8" position="float" orientation="portrait" hwp:id="graphic-10"/></fig><p hwp:id="p-34">It is likely that accurate tree detection will be region specific, and that the best model will vary among environments. This will require training a new model for each geographic area while using both RGB and LIDAR training data. The proposed approach could save resources by allowing a smaller scale LIDAR flight to generate training data, and then cover a much larger area with less expensive RGB orthophotos. Uncrewed aerial vehicles (UAVs) can be used for capturing LIDAR at high resolution, but at a limited spatial extent. In combination with our method, these UAVs may allow for the cost effective development of custom regional tree detection models. In addition, the National Ecological Observatory Network, which provided the data for this analysis, has 45 forested NEON sites that were selected to cover the major ecoclimatic domains in the United States. These sites could serve as pools of LIDAR and RGB data at 10,000 ha scales for regional model training. Combining these two detectors together could produce accurate individual level tree maps at broad scales, with potential applications in forest inventory, ecosystem health, post-natural disaster recovery, and carbon dynamics.</p><p hwp:id="p-35">While the semi-supervised deep learning method performed well at the open-canopy test site, geographic areas with complex canopy conditions will be more challenging. As the scale and tree diversity increase, the model will need additional hand-annotated training data to capture the variation within the tree class. We found that approximately 2000 trees produced reasonable performance in the oak woodland landscape while there is no hard rule for the minimum hand-annotations needed. It is difficult to diagnose the exact nature of the features used for tree detections, given the complex nature of convolutional neural networks. One potential is that the model detects shadows of vertical objects and uses them to help locate crown areas. Whether these features generalize to more complex canopy structures and new ecosystems remains an open question.</p><p hwp:id="p-36">The current model only uses LIDAR in the pretraining step. Where available, directly incorporating a LIDAR canopy height model into the deep learning approach should allow the model to simultaneously learn the vertical features of individual trees, in addition to the two-dimensional color features in the RGB data. Recent applications of three-dimensional CNNs [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>], as well as point-based semantic segmentation [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>], provide new avenues for joint multi-sensor modeling. These developments will be crucial in segmenting the complex canopies that overlap in the two-dimensional RGB imagery. In addition, recent extensions of region-proposal networks refine the bounding boxes to identify the individual pixels that belong to a class [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>]. This will provide a better estimate of the tree crown area, as trees typically have a non-rectangular shape.</p></sec><sec id="s5" hwp:id="sec-9"><label>5.</label><title hwp:id="title-12">Conclusions</title><p hwp:id="p-37">Applying deep learning models to natural landscapes opens new opportunities in ecology, forestry, and land management. Despite a lack of high-quality training data, deep learning algorithms can be deployed for tree prediction while using unsupervised detection to produce generated trees for pretraining the neural network. Although the lidar-based algorithm that was used to generate the pretraining data achieved less than 20% recall of hand-annotated tree crowns, the deeply learned RGB features from those data achieved greater than 50% recall. When combined with a small number of hand-annotated images, the recall increased to 69% with 60% precision. As shown by the comparison with field-collected stems, the majority of the remaining predictions represent valid trees (&gt;80%), but the overlap with the hand-estimated crown area was less than the desired 50%. Many previous papers have used a lower overlap threshold (e.g., 20% overlap in [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>]), and we expect this value to improve with a combination of better validation data and more hand-annotated training samples.</p><p hwp:id="p-38">There is the potential for this method to provide additional important information regarding natural systems, in addition to scaling tree detection at much lower costs. The current model could be expanded from a single class, âTreeâ, to one that provides more detailed classifications that are based on taxonomy and health status. For example, splitting the âTreeâ class into living and dead trees would provide management insight when surveying for outbreaks of tree pests and pathogens [<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>], as well as post-fire timber operations [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>]. The growing use of drones in environmental remote sensing also opens up additional possibilities of combining high density local information with broad scale information captured from traditional fixed wing and satellite mounted sensors. Unsupervised tree detection algorithms have been shown to be more effective at very high point densities (e.g., [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-2" hwp:rel-id="ref-36">36</xref>]). We anticipate that, as the quality of the unsupervised algorithm data increases, fewer hand annotated samples will be needed to customize the model to a local geographic area. In addition, the availability of hyperspectral data could assist in dividing the âtreeâ class into multiple species labels. This would yield additional insights into the economic value, ecological habitat, and carbon storage capacity for large geographic areas [<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>]. While further work is needed to understand the best way to combine data among scales and sensors, we show that deep learning-based approaches hold the potential for large scale actionable information on natural systems to be derived from remote sensing data.</p></sec></body><back><sec hwp:id="sec-10"><title hwp:id="title-13">Author Contributions</title><p hwp:id="p-39">B.G.W., E.W., S.B. and A.Z. conceived of project design. E.W. and S.M. collected the preliminary data. B.G.W. performed the analysis and wrote the text. All authors contributed to the text.</p></sec><sec hwp:id="sec-11"><title hwp:id="title-14">Funding</title><p hwp:id="p-40">This research was supported by the Gordon and Betty Moore Foundationâs Data-Driven Discovery Initiative through grant GBMF4563 to E.P. White. The authors declare no conflict of interest.</p></sec><ack hwp:id="ack-1"><title hwp:id="title-15">Acknowledgments</title><p hwp:id="p-41">Financial support for S.A.B was provided by the USDA/NIFA McIntire-Stennis program (FLA-FOR-005470).and a sabbatical fellowship award from sDiv (the Synthesis Centre of iDiv; DFG FZT 118).</p></ack><sec sec-type="COI-statement" hwp:id="sec-12"><title hwp:id="title-16">Conflicts of Interest</title><p hwp:id="p-42">The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-17">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>1.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Anderson C.B."><surname>Anderson</surname>, <given-names>C.B.</given-names></string-name> <article-title hwp:id="article-title-2">Biodiversity monitoring, earth observations and the ecology of scale</article-title>. <source hwp:id="source-1">Ecol. Lett.</source> <year>2018</year>, <volume>21</volume>, <fpage>1572</fpage>â<lpage>1585</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1111/ele.13106" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/ele.13106" hwp:id="ext-link-6">CrossRef</ext-link>] [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://pubmed.ncbi.nlm.nih.gov/30004184/" ext-link-type="uri" xlink:href="https://pubmed.ncbi.nlm.nih.gov/30004184/" hwp:id="ext-link-7">PubMed</ext-link>]</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><label>2.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Weinstein B.G."><surname>Weinstein</surname>, <given-names>B.G.</given-names></string-name> <article-title hwp:id="article-title-3">A computer vision for animal ecology</article-title>. <source hwp:id="source-2">J. Animal Ecol.</source> <year>2018</year>, <volume>87</volume>, <fpage>533</fpage>â<lpage>545</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="http://dx.doi.org/10.1111/1365-2656.12780" ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1365-2656.12780" hwp:id="ext-link-8">CrossRef</ext-link>]</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><label>3.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Wu B."><surname>Wu</surname>, <given-names>B.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Yu B."><surname>Yu</surname>, <given-names>B.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Wu Q."><surname>Wu</surname>, <given-names>Q.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Huang Y."><surname>Huang</surname>, <given-names>Y.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Chen Z."><surname>Chen</surname>, <given-names>Z.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Wu J."><surname>Wu</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-4">Individual tree crown delineation using localized contour tree method and airborne LiDAR data in coniferous forests</article-title>. <source hwp:id="source-3">Int. J. Appl. Earth Obs. Geoinf.</source> <year>2016</year>, <volume>52</volume>, <fpage>82</fpage>â<lpage>94</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>4.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Caughlin T.T."><surname>Caughlin</surname>, <given-names>T.T.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Graves S.J."><surname>Graves</surname>, <given-names>S.J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Asner G.P."><surname>Asner</surname>, <given-names>G.P.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Hall J.S."><surname>Hall</surname>, <given-names>J.S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Martin R.E."><surname>Martin</surname>, <given-names>R.E.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Ashton M.S."><surname>Ashton</surname>, <given-names>M.S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Bohlman S.A."><surname>Bohlman</surname>, <given-names>S.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Van Breugel M."><surname>Van Breugel</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-5">A hyperspectral image can predict tropical tree growth rates in single-species stands</article-title>. <source hwp:id="source-4">Ecol. Appl.</source> <year>2016</year>, <volume>26</volume>, <fpage>2369</fpage>â<lpage>2375</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://http://dx.doi.org/10.1002/eap.1436" ext-link-type="uri" xlink:href="https://http://dx.doi.org/10.1002/eap.1436" hwp:id="ext-link-9">CrossRef</ext-link>] [<ext-link l:rel="related" l:ref-type="uri" l:ref="http://www.ncbi.nlm.nih.gov/pubmed/27907255" ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/27907255" hwp:id="ext-link-10">PubMed</ext-link>]</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2"><label>5.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Ayrey E."><surname>Ayrey</surname>, <given-names>E.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Fraver S."><surname>Fraver</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kershaw J.A."><surname>Kershaw</surname>, <given-names>J.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kenefic L.S."><surname>Kenefic</surname>, <given-names>L.S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Hayes D."><surname>Hayes</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Weiskittel A.R."><surname>Weiskittel</surname>, <given-names>A.R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Roth B.E."><surname>Roth</surname>, <given-names>B.E.</given-names></string-name> <article-title hwp:id="article-title-6">Layer Stacking: A Novel Algorithm for Individual Forest Tree Segmentation from LiDAR Point Clouds</article-title>. <source hwp:id="source-5">Can. J. Remote Sens.</source> <year>2017</year>, <volume>43</volume>, <fpage>16</fpage>â<lpage>27</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1080/27038992.2017.1252907" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1080/27038992.2017.1252907" hwp:id="ext-link-11">CrossRef</ext-link>]</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><label>6.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Liu T."><surname>Liu</surname>, <given-names>T.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Im J."><surname>Im</surname>, <given-names>J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Quackenbush L.J."><surname>Quackenbush</surname>, <given-names>L.J.</given-names></string-name> <article-title hwp:id="article-title-7">A novel transferable individual tree crown delineation model based on Fishing Net Dragging and boundary classification</article-title>. <source hwp:id="source-6">ISPRS J. Photogramm. Sens.</source> <year>2015</year>, <volume>110</volume>, <fpage>34</fpage>â<lpage>47</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/isprsjprs.2015.10.002" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/isprsjprs.2015.10.002" hwp:id="ext-link-12">CrossRef</ext-link>]</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>7.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Gougeon F.A."><surname>Gougeon</surname>, <given-names>F.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Leckie D.G."><surname>Leckie</surname>, <given-names>D.G.</given-names></string-name> <article-title hwp:id="article-title-8">The Individual Tree Crown Approach Applied to Ikonos Images of a Coniferous Plantation Area</article-title>. <source hwp:id="source-7">Photogramm. Eng. Sens.</source> <year>2006</year>, <volume>72</volume>, <fpage>1287</fpage>â<lpage>1297</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.14358/PERS.72.11.1287" ext-link-type="uri" xlink:href="https://dx.doi.org/10.14358/PERS.72.11.1287" hwp:id="ext-link-13">CrossRef</ext-link>]</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>8.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Weinmann M."><surname>Weinmann</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Weinmann M."><surname>Weinmann</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Mallet C."><surname>Mallet</surname>, <given-names>C.</given-names></string-name>; <string-name name-style="western" hwp:sortable="BrÃ©dif M."><surname>BrÃ©dif</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-9">A Classification-Segmentation Framework for the Detection of Individual Trees in Dense MMS Point Cloud Data Acquired in Urban Areas</article-title>. <source hwp:id="source-8">Remote. Sens.</source> <year>2017</year>, <volume>9</volume>, <fpage>277</fpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.3390/rs9030277" ext-link-type="uri" xlink:href="https://dx.doi.org/10.3390/rs9030277" hwp:id="ext-link-14">CrossRef</ext-link>]</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3 xref-ref-9-4"><label>9.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Gomes M.F."><surname>Gomes</surname>, <given-names>M.F.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Maillard P."><surname>Maillard</surname>, <given-names>P.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Deng H."><surname>Deng</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-10">Individual tree crown detection in sub-meter satellite imagery using Marked Point Processes and a geometrical-optical model</article-title>. <source hwp:id="source-9">Remote. Sens. Environ.</source> <year>2018</year>, <volume>211</volume>, <fpage>184</fpage>â<lpage>195</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.rse.2018.04.002" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.rse.2018.04.002" hwp:id="ext-link-15">CrossRef</ext-link>]</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2"><label>10.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Li W."><surname>Li</surname>, <given-names>W.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Fu H."><surname>Fu</surname>, <given-names>H.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Yu L."><surname>Yu</surname>, <given-names>L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Cracknell A."><surname>Cracknell</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-11">Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images</article-title>. <source hwp:id="source-10">Remote. Sens.</source> <year>2016</year>, <volume>9</volume>, <fpage>22</fpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.3390/rs9010022" ext-link-type="uri" xlink:href="https://dx.doi.org/10.3390/rs9010022" hwp:id="ext-link-16">CrossRef</ext-link>]</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>11.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Guirado E."><surname>Guirado</surname>, <given-names>E.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Tabik S."><surname>Tabik</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Alcaraz-Segura D."><surname>Alcaraz-Segura</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Cabello J."><surname>Cabello</surname>, <given-names>J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Herrera F."><surname>Herrera</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-12">Deep-learning Versus OBIA for Scattered Shrub Detection with Google Earth Imagery: Ziziphus lotus as Case Study</article-title>. <source hwp:id="source-11">Remote. Sens.</source> <year>2017</year>, <volume>9</volume>, <fpage>1220</fpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.3390/rs9121220" ext-link-type="uri" xlink:href="https://dx.doi.org/10.3390/rs9121220" hwp:id="ext-link-17">CrossRef</ext-link>]</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>12.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Ayrey E."><surname>Ayrey</surname>, <given-names>E.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Hayes D.J."><surname>Hayes</surname>, <given-names>D.J.</given-names></string-name> <article-title hwp:id="article-title-13">The Use of Three-Dimensional Convolutional Neural Networks to Interpret LiDAR for Forest Inventory</article-title>. <source hwp:id="source-12">Remote. Sens.</source> <year>2018</year>, <volume>10</volume>, <fpage>649</fpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.3390/rs10040649" ext-link-type="uri" xlink:href="https://dx.doi.org/10.3390/rs10040649" hwp:id="ext-link-18">CrossRef</ext-link>]</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>13.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Zhu X.X."><surname>Zhu</surname>, <given-names>X.X.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Tuia D."><surname>Tuia</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Mou L."><surname>Mou</surname>, <given-names>L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Xia G.-S."><surname>Xia</surname>, <given-names>G.-S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Zhang L."><surname>Zhang</surname>, <given-names>L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Xu F."><surname>Xu</surname>, <given-names>F.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Fraundorfer F."><surname>Fraundorfer</surname>, <given-names>F.</given-names></string-name> <article-title hwp:id="article-title-14">Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources</article-title>. <source hwp:id="source-13">IEEE Geosci. Remote Sens. Mag.</source> <year>2017</year>, <volume>5</volume>, <fpage>8</fpage>â<lpage>36</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1109/MGRS.2017.2762307" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1109/MGRS.2017.2762307" hwp:id="ext-link-19">CrossRef</ext-link>]</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1 xref-ref-14-2"><label>14.</label><citation publication-type="other" citation-type="journal" ref:id="532952v5.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Dahlkamp H."><surname>Dahlkamp</surname>, <given-names>H.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kaehler A."><surname>Kaehler</surname>, <given-names>A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Stavens D."><surname>Stavens</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Thrun S."><surname>Thrun</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Bradski G."><surname>Bradski</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-15">Self-supervised Monocular Road Detection in Desert Terrain</article-title>. In <source hwp:id="source-14">Robotics: Science and Systems II</source>; Available online: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://tinyurl.com/y6xtjqfa" ext-link-type="uri" xlink:href="https://tinyurl.com/y6xtjqfa" hwp:id="ext-link-20">https://tinyurl.com/y6xtjqfa</ext-link> (<date-in-citation content-type="access-date">accessed on 1 June 2019</date-in-citation>).</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><label>15.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Wu H."><surname>Wu</surname>, <given-names>H.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Prasad S."><surname>Prasad</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-16">Semi-Supervised Deep Learning Using Pseudo Labels for Hyperspectral Image Classification</article-title>. <source hwp:id="source-15">IEEE Trans. Image Process.</source> <year>2018</year>, <volume>27</volume>, <fpage>1259</fpage>â<lpage>1270</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1109/TIP.2017.2772836" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1109/TIP.2017.2772836" hwp:id="ext-link-21">CrossRef</ext-link>]</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>16.</label><citation publication-type="other" citation-type="journal" ref:id="532952v5.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Romero A."><surname>Romero</surname>, <given-names>A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Ballas N."><surname>Ballas</surname>, <given-names>N.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kahou S.E."><surname>Kahou</surname>, <given-names>S.E.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Chassang A."><surname>Chassang</surname>, <given-names>A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Gatta C."><surname>Gatta</surname>, <given-names>C.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-17">FitNets: Hints for Thin Deep Nets</article-title>. <source hwp:id="source-16">arXiv preprint</source> <year>2014</year>, arXiv:<pub-id pub-id-type="arxiv">1412.6550</pub-id>, <fpage>1</fpage>â<lpage>13</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1"><label>17.</label><citation publication-type="book" citation-type="book" ref:id="532952v5.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Erhan D."><surname>Erhan</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Manzagol P.-A."><surname>Manzagol</surname>, <given-names>P.-A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Bengio Y."><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Bengio S."><surname>Bengio</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Vincent P."><surname>Vincent</surname>, <given-names>P.</given-names></string-name> <chapter-title>The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training</chapter-title>. In <source hwp:id="source-17">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics (AISTATS)</source>, <publisher-name>Clearwater Beach</publisher-name>, <publisher-loc>FL, USA</publisher-loc>, <day>16â18</day> <month>April</month> <year>2009</year>; Volume <volume>5</volume>, pp. <fpage>153</fpage>â<lpage>160</lpage>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>18.</label><citation publication-type="other" citation-type="journal" ref:id="532952v5.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Weinstein B."><surname>Weinstein</surname>, <given-names>B.</given-names></string-name>; <string-name name-style="western" hwp:sortable="White E."><surname>White</surname>, <given-names>E.</given-names></string-name> <source hwp:id="source-18">Weecology/DeepLidar: Resubmission II, Version 3.0</source>. Available online: <ext-link l:rel="related" l:ref-type="uri" l:ref="http://doi.org/10.5281/zenodo.3066235" ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.3066235" hwp:id="ext-link-22">http://doi.org/10.5281/zenodo.3066235</ext-link> (<date-in-citation content-type="access-date">accessed on 1 June 2019</date-in-citation>).</citation></ref><ref id="c19" hwp:id="ref-19"><label>19.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Dalponte M."><surname>Dalponte</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Coomes D.A."><surname>Coomes</surname>, <given-names>D.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Murrell D."><surname>Murrell</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-18">Tree-centric mapping of forest carbon density from airborne laser scanning and hyperspectral data</article-title>. <source hwp:id="source-19">Methods Ecol. Evol.</source> <year>2016</year>, <volume>7</volume>, <fpage>1236</fpage>â<lpage>1245</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1111/2041-210X.12575" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1111/2041-210X.12575" hwp:id="ext-link-23">CrossRef</ext-link>]</citation></ref><ref id="c20" hwp:id="ref-20"><label>20.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Li W."><surname>Li</surname>, <given-names>W.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Guo Q."><surname>Guo</surname>, <given-names>Q.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Jakubowski M.K."><surname>Jakubowski</surname>, <given-names>M.K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kelly M."><surname>Kelly</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-19">A New Method for Segmenting Individual Trees from the Lidar Point Cloud</article-title>. <source hwp:id="source-20">Photogramm. Eng. Sens.</source> <year>2012</year>, <volume>78</volume>, <fpage>75</fpage>â<lpage>84</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.14358/PERS.78.1.75" ext-link-type="uri" xlink:href="https://dx.doi.org/10.14358/PERS.78.1.75" hwp:id="ext-link-24">CrossRef</ext-link>]</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><label>21.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Silva C.A."><surname>Silva</surname>, <given-names>C.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Hudak A.T."><surname>Hudak</surname>, <given-names>A.T.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Vierling L.A."><surname>Vierling</surname>, <given-names>L.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Loudermilk E.L."><surname>Loudermilk</surname>, <given-names>E.L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="OâBrien J.J."><surname>OâBrien</surname>, <given-names>J.J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Hiers J.K."><surname>Hiers</surname>, <given-names>J.K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Jack S.B."><surname>Jack</surname>, <given-names>S.B.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Gonzalez-Benecke C."><surname>Gonzalez-Benecke</surname>, <given-names>C.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Lee H."><surname>Lee</surname>, <given-names>H.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Falkowski M.J."><surname>Falkowski</surname>, <given-names>M.J.</given-names></string-name>; <etal>et al.</etal> <article-title hwp:id="article-title-20">Imputation of Individual Longleaf Pine (Pinus palustris Mill.) Tree Attributes from Field and LiDAR Data</article-title>. <source hwp:id="source-21">Can. J. Remote Sens.</source> <year>2016</year>, <volume>42</volume>, <fpage>554</fpage>â<lpage>573</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1080/07038992.2016.1196582" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1080/07038992.2016.1196582" hwp:id="ext-link-25">CrossRef</ext-link>]</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>22.</label><citation publication-type="other" citation-type="journal" ref:id="532952v5.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Roussel J.-R."><surname>Roussel</surname>, <given-names>J.-R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Auty D."><surname>Auty</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="De Boissieu F."><surname>De Boissieu</surname>, <given-names>F.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Meador A.S."><surname>Meador</surname>, <given-names>A.S.</given-names></string-name> <source hwp:id="source-22">lidR: Airborne LiDAR Data Manipulation and Visualization for Forestry Applications</source>. Available online: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://rdrr.io/cran/lidR/" ext-link-type="uri" xlink:href="https://rdrr.io/cran/lidR/" hwp:id="ext-link-26">https://rdrr.io/cran/lidR/</ext-link> (<date-in-citation content-type="access-date">accessed on 1 June 2019</date-in-citation>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><label>23.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Uijlings J.R.R."><surname>Uijlings</surname>, <given-names>J.R.R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Van De Sande K.E.A."><surname>Van De Sande</surname>, <given-names>K.E.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Gevers T."><surname>Gevers</surname>, <given-names>T.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Smeulders A.W.M."><surname>Smeulders</surname>, <given-names>A.W.M.</given-names></string-name> <article-title hwp:id="article-title-21">Selective Search for Object Recognition</article-title>. <source hwp:id="source-23">Int. J. Comput. Vis.</source> <year>2013</year>, <volume>104</volume>, <fpage>154</fpage>â<lpage>171</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1007/s11263-013-0620-5" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1007/s11263-013-0620-5" hwp:id="ext-link-27">CrossRef</ext-link>]</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>24.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Ren S."><surname>Ren</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Girshick R."><surname>Girshick</surname>, <given-names>R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Sun J."><surname>Sun</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-22">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</article-title>. <source hwp:id="source-24">Nips</source> <year>2015</year>, <volume>1</volume>, <fpage>91</fpage>â<lpage>99</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1109/TPAMI.2016.2577031" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1109/TPAMI.2016.2577031" hwp:id="ext-link-28">CrossRef</ext-link>]</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><label>25.</label><citation publication-type="book" citation-type="book" ref:id="532952v5.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Lin T.Y."><surname>Lin</surname>, <given-names>T.Y.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Goyal P."><surname>Goyal</surname>, <given-names>P.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Girshick R."><surname>Girshick</surname>, <given-names>R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Dollar P."><surname>Dollar</surname>, <given-names>P.</given-names></string-name> <chapter-title>Focal Loss for Dense Object Detection</chapter-title>. In <source hwp:id="source-25">Proceedings of the IEEE International Conference Computer Vision</source>, <publisher-loc>Venice, Italy</publisher-loc>, <volume>22â29</volume> <month>October</month> <year>2017</year>; pp. <fpage>2999</fpage>â<lpage>3007</lpage>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><label>26.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Gaiser H."><surname>Gaiser</surname>, <given-names>H.</given-names></string-name>; <string-name name-style="western" hwp:sortable="de Vries M."><surname>de Vries</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Lacatusu V."><surname>Lacatusu</surname>, <given-names>V.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Williamson A."><surname>Williamson</surname>, <given-names>A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Liscio E.D.D."><surname>Liscio</surname>, <given-names>E.D.D.</given-names></string-name> <source hwp:id="source-26">fizy-r/Keras-retinanet</source> <year>2018</year>. Available online: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/fizyr/keras-retinanet" ext-link-type="uri" xlink:href="https://github.com/fizyr/keras-retinanet" hwp:id="ext-link-29">https://github.com/fizyr/keras-retinanet</ext-link> (<date-in-citation content-type="access-date">accessed on 1 June 2019</date-in-citation>).</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>27.</label><citation publication-type="book" citation-type="book" ref:id="532952v5.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Zhang X."><surname>Zhang</surname>, <given-names>X.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Ren S."><surname>Ren</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Sun J."><surname>Sun</surname>, <given-names>J.</given-names></string-name> <chapter-title>Deep Residual Learning for Image Recognition</chapter-title>. In <source hwp:id="source-27">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, <publisher-loc>Las Vegas, NV, USA</publisher-loc>, <day>27â30</day> <month>June</month> <year>2016</year>; pp. <fpage>770</fpage>â<lpage>778</lpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>28.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Vastaranta M."><surname>Vastaranta</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kankare V."><surname>Kankare</surname>, <given-names>V.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Holopainen M."><surname>Holopainen</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Yu X."><surname>Yu</surname>, <given-names>X.</given-names></string-name>; <string-name name-style="western" hwp:sortable="HyyppÃ¤ J."><surname>HyyppÃ¤</surname>, <given-names>J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="HyyppÃ¤ H."><surname>HyyppÃ¤</surname>, <given-names>H.</given-names></string-name> <article-title hwp:id="article-title-23">Combination of individual tree detection and area-based approach in imputation of forest variables using airborne laser data</article-title>. <source hwp:id="source-28">ISPRS J. Photogramm. Sens.</source> <year>2012</year>, <volume>67</volume>, <fpage>73</fpage>â<lpage>79</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.isprsjprs.2011.10.006" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.isprsjprs.2011.10.006" hwp:id="ext-link-30">CrossRef</ext-link>]</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>29.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Duncanson L."><surname>Duncanson</surname>, <given-names>L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Cook B."><surname>Cook</surname>, <given-names>B.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Hurtt G."><surname>Hurtt</surname>, <given-names>G.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Dubayah R."><surname>Dubayah</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-24">An efficient, multi-layered crown delineation algorithm for mapping individual tree structure across multiple ecosystems</article-title>. <source hwp:id="source-29">Remote. Sens. Environ.</source> <year>2014</year>, <volume>154</volume>, <fpage>378</fpage>â<lpage>386</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.rse.2013.07.044" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.rse.2013.07.044" hwp:id="ext-link-31">CrossRef</ext-link>]</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>30.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Coomes D.A."><surname>Coomes</surname>, <given-names>D.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Dalponte M."><surname>Dalponte</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Jucker T."><surname>Jucker</surname>, <given-names>T.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Asner G.P."><surname>Asner</surname>, <given-names>G.P.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Banin L.F."><surname>Banin</surname>, <given-names>L.F.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Burslem D.F."><surname>Burslem</surname>, <given-names>D.F.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Lewis S.L."><surname>Lewis</surname>, <given-names>S.L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Nilus R."><surname>Nilus</surname>, <given-names>R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Phillips O.L."><surname>Phillips</surname>, <given-names>O.L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Phua M.-H."><surname>Phua</surname>, <given-names>M.-H.</given-names></string-name>; <etal>et al.</etal> <article-title hwp:id="article-title-25">Area-based vs tree-centric approaches to mapping forest carbon in Southeast Asian forests from airborne laser scanning data</article-title>. <source hwp:id="source-30">Remote. Sens. Environ.</source> <year>2017</year>, <volume>194</volume>, <fpage>77</fpage>â<lpage>88</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.rse.2017.03.017" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.rse.2017.03.017" hwp:id="ext-link-32">CrossRef</ext-link>]</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>31.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Yin D."><surname>Yin</surname>, <given-names>D.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Wang L."><surname>Wang</surname>, <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-26">Individual mangrove tree measurement using UAV-based LiDAR data: Possibilities and challenges</article-title>. <source hwp:id="source-31">Remote. Sens. Environ.</source> <year>2019</year>, <volume>223</volume>, <fpage>34</fpage>â<lpage>49</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.rse.2018.12.034" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.rse.2018.12.034" hwp:id="ext-link-33">CrossRef</ext-link>]</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>32.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="A Jeronimo S.M."><surname>A Jeronimo</surname>, <given-names>S.M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Kane V.R."><surname>Kane</surname>, <given-names>V.R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Churchill D.J."><surname>Churchill</surname>, <given-names>D.J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="McGaughey R.J."><surname>McGaughey</surname>, <given-names>R.J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Franklin J.F."><surname>Franklin</surname>, <given-names>J.F.</given-names></string-name> <article-title hwp:id="article-title-27">Applying LiDAR Individual Tree Detection to Management of Structurally Diverse Forest Landscapes</article-title>. <source hwp:id="source-32">J. For.</source> <year>2018</year>, <volume>116</volume>, <fpage>336</fpage>â<lpage>346</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1093/jofore/fvy023" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1093/jofore/fvy023" hwp:id="ext-link-34">CrossRef</ext-link>]</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>33.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Zhou Y."><surname>Zhou</surname>, <given-names>Y.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Tuzel O."><surname>Tuzel</surname>, <given-names>O.</given-names></string-name> <source hwp:id="source-33">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</source>. <year>2017</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>34.</label><citation publication-type="book" citation-type="book" ref:id="532952v5.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Qi C.R."><surname>Qi</surname>, <given-names>C.R.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Su H."><surname>Su</surname>, <given-names>H.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Mo K."><surname>Mo</surname>, <given-names>K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Guibas L.J."><surname>Guibas</surname>, <given-names>L.J.</given-names></string-name> <chapter-title>PointNet: Deep learning on point sets for 3D classification and segmentation</chapter-title>. In <source hwp:id="source-34">Proceedings of the 30th IEEE Conference Computer Vision Pattern Recognition, CVPR 2017</source>, <publisher-loc>Honolulu, HI, USA</publisher-loc>, <day>21â26</day> <month>July</month> <year>2017</year>; pp. <fpage>77</fpage>â<lpage>85</lpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>35.</label><citation publication-type="book" citation-type="book" ref:id="532952v5.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="He K."><surname>He</surname>, <given-names>K.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Gkioxari G."><surname>Gkioxari</surname>, <given-names>G.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Dollar P."><surname>Dollar</surname>, <given-names>P.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Girshick R."><surname>Girshick</surname>, <given-names>R.</given-names></string-name> <string-name name-style="western" hwp:sortable="Mask R-CNN"><surname>Mask</surname> <given-names>R-CNN</given-names></string-name>. In <source hwp:id="source-35">Proceedings of the IEEE International Conference Computer Vision 2017</source>, <publisher-loc>Venice, Italy</publisher-loc>, <day>22â29</day> <month>October</month> <year>2017</year>; pp. <fpage>2980</fpage>â<lpage>2988</lpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1 xref-ref-36-2"><label>36.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Wallace L."><surname>Wallace</surname>, <given-names>L.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Lucieer A."><surname>Lucieer</surname>, <given-names>A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Watson C.S."><surname>Watson</surname>, <given-names>C.S.</given-names></string-name> <article-title hwp:id="article-title-28">Evaluating tree detection and segmentation routines on very high resolution UAV LiDAR ata</article-title>. <source hwp:id="source-36">IEEE Trans. Geosci. Remote Sens.</source> <year>2014</year>, <volume>52</volume>, <fpage>7619</fpage>â<lpage>7628</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1109/TGRS.2014.23115649" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1109/TGRS.2014.23115649" hwp:id="ext-link-35">CrossRef</ext-link>]</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>37.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Wulder M.A."><surname>Wulder</surname>, <given-names>M.A.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Dymond C.C."><surname>Dymond</surname>, <given-names>C.C.</given-names></string-name>; <string-name name-style="western" hwp:sortable="White J.C."><surname>White</surname>, <given-names>J.C.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Leckie D.G."><surname>Leckie</surname>, <given-names>D.G.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Carroll A.L."><surname>Carroll</surname>, <given-names>A.L.</given-names></string-name> <article-title hwp:id="article-title-29">Surveying mountain pine beetle damage of forests: A review of remote sensing opportunities</article-title>. <source hwp:id="source-37">Ecol. Manag.</source> <year>2006</year>, <volume>221</volume>, <fpage>27</fpage>â<lpage>41</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.foreco.2005.09.021" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.foreco.2005.09.021" hwp:id="ext-link-36">CrossRef</ext-link>]</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>38.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Vogeler J.C."><surname>Vogeler</surname>, <given-names>J.C.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Yang Z."><surname>Yang</surname>, <given-names>Z.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Cohen W.B."><surname>Cohen</surname>, <given-names>W.B.</given-names></string-name> <article-title hwp:id="article-title-30">Mapping post-fire habitat characteristics through the fusion of remote sensing tools</article-title>. <source hwp:id="source-38">Remote. Sens. Environ.</source> <year>2016</year>, <volume>173</volume>, <fpage>294</fpage>â<lpage>303</lpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.1016/j.rse2015.08.011" ext-link-type="uri" xlink:href="https://dx.doi.org/10.1016/j.rse2015.08.011" hwp:id="ext-link-37">CrossRef</ext-link>]</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>39.</label><citation publication-type="journal" citation-type="journal" ref:id="532952v5.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Deng S."><surname>Deng</surname>, <given-names>S.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Katoh M."><surname>Katoh</surname>, <given-names>M.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Yu X."><surname>Yu</surname>, <given-names>X.</given-names></string-name>; <string-name name-style="western" hwp:sortable="HyyppÃ¤ J."><surname>HyyppÃ¤</surname>, <given-names>J.</given-names></string-name>; <string-name name-style="western" hwp:sortable="Gao T."><surname>Gao</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-31">Comparison of Tree Species Classifications at the Individual Tree Level by Combining ALS Data and RGB Images Using Different Algorithms</article-title>. <source hwp:id="source-39">Remote. Sens.</source> <year>2016</year>, <volume>8</volume>, <fpage>1034</fpage>. [<ext-link l:rel="related" l:ref-type="uri" l:ref="https://dx.doi.org/10.3390/rs8121034" ext-link-type="uri" xlink:href="https://dx.doi.org/10.3390/rs8121034" hwp:id="ext-link-38">CrossRef</ext-link>]</citation></ref></ref-list></back></article>
