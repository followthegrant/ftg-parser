<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2021.05.17.444523</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2021.05.17.444523</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2021.05.17.444523</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2021.05.17.444523</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2021.05.17.444523</article-id><article-version>1.4</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Zoology" hwp:journal="biorxiv"><subject>Zoology</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">DiversityScanner: Robotic discovery of small invertebrates with machine learning methods</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1 xref-corresp-1-2"><label>†</label>Correspondence: <email hwp:id="email-1">pylatiuk@kit.edu</email> &amp; <email hwp:id="email-2">Rudolf.Meier@mfn.berlin</email></corresp></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Wührl Lorenz"><surname>Wührl</surname><given-names>Lorenz</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-2"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3507-7134</contrib-id><name name-style="western" hwp:sortable="Pylatiuk Christian"><surname>Pylatiuk</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">†</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3507-7134"/></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Giersch Matthias"><surname>Giersch</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Lapp Florian"><surname>Lapp</surname><given-names>Florian</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6253-3078</contrib-id><name name-style="western" hwp:sortable="von Rintelen Thomas"><surname>von Rintelen</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-6253-3078"/></contrib><contrib contrib-type="author" hwp:id="contrib-6"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3773-6586</contrib-id><name name-style="western" hwp:sortable="Balke Michael"><surname>Balke</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-3773-6586"/></contrib><contrib contrib-type="author" hwp:id="contrib-7"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5751-8706</contrib-id><name name-style="western" hwp:sortable="Schmidt Stefan"><surname>Schmidt</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-2" hwp:rel-id="aff-4">4</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-5751-8706"/></contrib><contrib contrib-type="author" hwp:id="contrib-8"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9204-3352</contrib-id><name name-style="western" hwp:sortable="Cerretti Pierfilippo"><surname>Cerretti</surname><given-names>Pierfilippo</given-names></name><xref ref-type="aff" rid="a5" hwp:id="xref-aff-5-1" hwp:rel-id="aff-5">5</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-9204-3352"/></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-9"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4452-2885</contrib-id><name name-style="western" hwp:sortable="Meier Rudolf"><surname>Meier</surname><given-names>Rudolf</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-2" hwp:rel-id="corresp-1">†</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-4452-2885"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4"><label>1</label><institution hwp:id="institution-1">Institute for Automation and Applied Informatics (IAI), Karlsruhe Institute of Technology (KIT)</institution>, Karlsruhe, <country>Germany</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Department of Biological Science, National University of Singapore (NUS)</institution>, <country>Singapore</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Museum für Naturkunde, Leibniz-Institut für Evolutions- und Biodiversitätsforschung</institution>, Berlin, <country>Germany</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1 xref-aff-4-2"><label>4</label><institution hwp:id="institution-4">SNSB – Zoologische Staatssammlung München</institution>, Munich, <country>Germany</country></aff><aff id="a5" hwp:id="aff-5" hwp:rev-id="xref-aff-5-1"><label>5</label><institution hwp:id="institution-5">Department of Biology and Biotechnology ‘Charles Darwin’, Sapienza University of Rome</institution>, Rome, <country>Italy</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2021"><year>2021</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2021-05-18T08:51:19-07:00">
    <day>18</day><month>5</month><year>2021</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2021-10-27T00:39:20-07:00">
    <day>27</day><month>10</month><year>2021</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2021-05-18T08:55:14-07:00">
    <day>18</day><month>5</month><year>2021</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2021-10-27T00:46:37-07:00">
    <day>27</day><month>10</month><year>2021</year>
  </pub-date><elocation-id>2021.05.17.444523</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2021-05-17"><day>17</day><month>5</month><year>2021</year></date>
<date date-type="rev-recd" hwp:start="2021-10-26"><day>26</day><month>10</month><year>2021</year></date>
<date date-type="accepted" hwp:start="2021-10-27"><day>27</day><month>10</month><year>2021</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2021, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2021</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="license-1"><p hwp:id="p-1">This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by/4.0/</ext-link></p></license></permissions><self-uri xlink:href="444523.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/change-list" xlink:role="change-list" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2021/pdf/2021.05.17.444523v4.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="444523.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2021/abstracts/2021.05.17.444523v4/2021.05.17.444523v4.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2021/fulltext/2021.05.17.444523v4/2021.05.17.444523v4.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">ABSTRACT</title><p hwp:id="p-2">Invertebrate biodiversity remains poorly explored although it comprises much of the terrestrial animal biomass, more than 90% of the species-level diversity and supplies many ecosystem services. The main obstacle is specimen- and species-rich samples. Traditional sorting techniques require manual handling and are slow while molecular techniques based on metabarcoding struggle with obtaining reliable abundance information. Here we present a fully automated sorting robot, which detects each specimen, images and measures it before moving it from a mixed invertebrate sample to the well of a 96-well microplate in preparation for DNA barcoding. The images are then used by a newly trained convolutional neural network (CNN) to assign the specimens to 14 particularly common, usually family-level “classes” of insects in Malaise trap samples and an “other-class” (N=15). The average assignment precision for the classes is 91.4% (75-100%). In order to obtain biomass information, the specimen images are also used to measure specimen length and estimate body volume. We outline how the DiversityScanner robot can be a key component for tackling and monitoring invertebrate diversity. The robot generates large numbers of images that become training sets for CNNs once the images are labelled with identifications based on DNA barcodes. In addition, the robot allows for taxon-specific subsampling of large invertebrate samples by only removing the specimens that belong to one of the 14 classes. We conclude that a combination of automation, machine learning, and DNA barcoding has the potential to tackle invertebrate diversity at an unprecedented scale.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">automation</kwd><kwd hwp:id="kwd-2">biodiversity</kwd><kwd hwp:id="kwd-3">biomass</kwd><kwd hwp:id="kwd-4">convolutional neural network</kwd><kwd hwp:id="kwd-5">DNA barcoding</kwd><kwd hwp:id="kwd-6">dark taxa</kwd></kwd-group><counts><page-count count="26"/></counts><custom-meta-wrap>
    <custom-meta hwp:id="custom-meta-1">
      <meta-name>has-earlier-version</meta-name>
      <meta-value>yes</meta-value>
    </custom-meta>
  </custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-3">The authors have declared no competing interest.</p></notes><fn-group content-type="summary-of-updates" hwp:id="fn-group-1"><title hwp:id="title-4">Summary of Updates:</title><fn fn-type="update" hwp:id="fn-1"><p hwp:id="p-4">extended text with additional information, new figures and tables.</p></fn></fn-group><fn-group content-type="external-links" hwp:id="fn-group-2"><fn fn-type="dataset" hwp:id="fn-2"><p hwp:id="p-5">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.youtube.com/watch?v=ElJ5VSHa4OI" ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=ElJ5VSHa4OI" hwp:id="ext-link-2">https://www.youtube.com/watch?v=ElJ5VSHa4OI</ext-link>.
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-5">INTRODUCTION</title><p hwp:id="p-6">Biodiversity science is currently at an inflection point. For decades, biodiversity declines had been mostly an academic concern although many biologists already predicted that these declines would eventually threaten whole ecosystems. Unfortunately, we are now at this stage, which explains why the World Economic Forum considers biodiversity decline one of the top three global risks based on likelihood and impact for the next 10 years (<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">World Economic Forum’s Global Risk Initiative 2020</xref>). This new urgency is also leading to a reassessment of research priorities. Biologists traditionally focused on charismatic taxa (e.g., vertebrates, vascular plants, butterflies) with a preference for endangered species. However, with regard to quantitative arguments, many of these taxon biases were unfortunate. For example, if one were to adopt a biomass point of view to terrestrial animal diversity, invertebrates would receive most of the attention because they contribute &gt;45 times the biomass of wild vertebrates (Table S23 in (<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Bar-On <italic toggle="yes">et al</italic>. 2018</xref>)), contain &gt;90% of the species diversity (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Groombridge 1992</xref>), and much of the functional and evolutionary diversity. This means that efficient tools for assessing and monitoring invertebrate biodiversity are urgently needed in order to address the knowledge gaps in biodiversity science, which <xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Robert May (2011)</xref> characterized as follows: “We are astonishingly ignorant about how many species are alive on earth today, and even more ignorant about how many we can lose (and) yet still maintain ecosystem services that humanity ultimately depends upon.” Much of the undiscovered and undescribed animal diversity belongs to clades that are nowadays called “dark taxa” which <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Hartop et al. (2021)</xref> recently defined as taxa “for which the undescribed fauna is estimated to exceed the described fauna by at least one order of magnitude and the total diversity exceeds 1,000 species.” Species discovery in these taxa is particularly difficult because it requires species-level sorting of thousands of small specimens that frequently need dissection for identification using morphological traits.</p><p hwp:id="p-7">Fortunately, there are three technical developments that promise relief. The first is cost-effective methods for obtaining barcode amplicons (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Wang <italic toggle="yes">et al</italic>. 2018</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Srivathsan <italic toggle="yes">et al</italic>. 2021</xref>) in conjunction with 2nd and 3rd generation sequencing technologies (<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Hebert <italic toggle="yes">et al</italic>. 2018</xref>; <xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Srivathsan <italic toggle="yes">et al</italic>. 2019a</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">Srivathsan <italic toggle="yes">et al</italic>. 2021</xref>). In particular, portable nanopore sequencers by Oxford Nanopore Technologies are in the process of democratizing access to DNA sequence data (<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Pomerantz <italic toggle="yes">et al</italic>. 2018</xref>; <xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Watsa <italic toggle="yes">et al</italic>. 2020</xref>; <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Buchner <italic toggle="yes">et al</italic>. 2021</xref>; <xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-3" hwp:rel-id="ref-35">Srivathsan <italic toggle="yes">et al</italic>. 2021</xref>). The two remaining developments remain underutilized. They are automation and data processing with neural networks. Currently, automation mostly exists in the form of pipetting robots in molecular laboratories, while data processing with neural networks is only widely used for monitoring charismatic taxa. Bulk invertebrate samples have benefited very little (but see (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Ärje <italic toggle="yes">et al</italic>. 2020b</xref>)) although thousands of samples are collected every day. They include plankton samples in marine biology, macroinvertebrate samples used for assessing freshwater quality, and mass insect samples (<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Brown 2005</xref>; <xref ref-type="bibr" rid="c6" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Borkent &amp; Brown 2015</xref>; <xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Brown <italic toggle="yes">et al</italic>. 2018</xref>; <xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Karlsson <italic toggle="yes">et al</italic>. 2020b</xref>). The desirable end goal should be convolutional neural networks that use images (1) to identify the specimens to species, (2) provide specimen and species counts, (3) measure biomass, and (4) compare the results to samples previously obtained from the same sites.</p><p hwp:id="p-8">Currently, the most popular way to process bulk invertebrate samples is with metabarcoding but the technique is affected by taxonomic bias and struggles with providing abundance information (<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Creedy <italic toggle="yes">et al</italic>. 2019</xref>). However, computer-based identification systems for invertebrates that could be used for specimen-based approaches are starting to yield promising results (<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">Feng <italic toggle="yes">et al</italic>. 2016</xref>; <xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Perre <italic toggle="yes">et al</italic>. 2016</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Knyshov <italic toggle="yes">et al</italic>. 2021</xref>). Particularly attractive are deep convolutional neural networks with transfer learning (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">Ärje <italic toggle="yes">et al</italic>. 2020b</xref>), but they require large sets of training images, which are hard to obtain for invertebrates given that most species are difficult to identify. It is here that robotics can have an impact when imaging is combined with DNA barcoding. The robot provides the images while the DNA barcodes can be used to sort the specimens to putative species (“MOTUs”). Comparing the barcodes with public databases will then reveal for which specimens the preliminary MOTU ID can be replaced with a scientific name. Imaging combined with labelling at species-level resolution will then provide the images for training convolutional neural networks for image-based identification.</p><p hwp:id="p-9">The first robots that can be used for insect sorting are becoming available. For example, one recently developed system can size and identify stoneflies (Plecoptera) that are routinely used for freshwater quality assessment (<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Sarpola <italic toggle="yes">et al</italic>. 2008</xref>). Another system is designed for processing samples consisting of soil mesofauna (<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Chamblin <italic toggle="yes">et al</italic>. 2011</xref>). However, Chamblin et al. use a robotic arm, which makes the system comparatively expensive. Other robots have been designed for specific, commercial insect sorting purposes. This includes one that can separate intact mealworm larvae (<italic toggle="yes">Tenebrio molitor</italic>) from skins, feces, and dead worms (<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Kim 2014</xref>) and another one that sorts mosquitoes (<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Lepek <italic toggle="yes">et al</italic>. 2020</xref>) and is capable of distinguishing males from females. However, all these machines lack the ability to recognize a wide variety of insect specimens in bulk invertebrate samples. The machine closest to this capability is the BIODISCOVER by <xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Ärje et al. (2020a)</xref>, which can identify ethanol-preserved specimens which, however, have to be fed into the machine manually one by one. After identification, all specimens are returned into the same container.</p><p hwp:id="p-10">We here describe a new system that overcomes some of these shortcomings. It recognizes insect specimens based on an overview image of a sample. Specimens below 3 mm body length are then imaged and moved into the wells of a 96-well microplate. We here demonstrate that the images are of sufficient quality for using convolutional neural networks for classifying the specimens into 14 common groups of insects (usually family-level). Furthermore, the images yield length measurements and an estimation of biomass based on specimen volume.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-6">CONCEPT AND METHODS</title><p hwp:id="p-11">The aim of the project is to develop an insect classification and sorting robot that is compact and that works reliably (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig 1</xref>). Note that we here use to the term “classification” in the machine learning context as assigning objects to different “classes”; i.e., the term “class” is here not used as a rank in a Linnean classification. Indeed, most of our “classes” are family-ranked taxa (N=10), two contain two families, and two are of higher rank (Calyptratae and the paraphyletic acalyptrate Diptera).</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-12">The DiversityScanner with 1: x-axis; 2: y-axis; 3: z-axis; 4: Petri dish; 5: Microwell plate; 6: Overview camera (C1), 7: Detail camera (C2). The electronics box with Raspberry Pi, motor control unit, and the syringe pump are in the lower part of the sorting robot and therefore not visible in this view. The status of both, insect position determination and status of the sorting process are displayed on a touch screen, where the sorting process can also be started and stopped</p></caption><graphic xlink:href="444523v4_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-13">Our robot relies mostly on standard components that all connected via parts that can be produced by a normal 3D printer. The basic design uses a cube-shaped frame (50 × 50 × 50cm) and three linear drives with accurately positioning stepper motors. It is based on a zebrafish embryo handling robot (<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Pfriem <italic toggle="yes">et al</italic>. 2012</xref>). The robot is equipped with two high-resolution cameras with customized lenses, LED lighting and image recognition software. Furthermore, a transport system based on a suction pump is integrated to transfer insects into the wells of a standard 96-well microplate. Thus, the robot system can be divided into: (1) the Transport System, (2) the Image Acquisition System, (3) the Image Processing System, and (4) a touch screen with graphical user interface (GUI).</p><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-7">Transport System</title><p hwp:id="p-14">The x- and y-axes of the robot are realised by LEZ1 linear drives (Isel AG, Eichenzell, Germany) and connected to the outer frame of the robot at half height. Both linear drives are driven by high-precision stepper motors with little tolerance to ensure good positioning accuracy. The y-axis is connected orthogonally to the shaft slide of the x-axis and is transported by it. The shaft slide of the y-axis transports the camera (C2) and the z-axis with the suction hose. In order to move the suction hose in the z-direction (=up and down) the z-axis is driven by an AR42H50 spindle drive with stepper motor (Nanotec Electronic GmbH &amp; Co. KG, Feldkirchen, Germany). All three axes are controlled by a single TMCM-3110 motor controller (Trinamic, Hamburg, Germany) that allows for precise, fast and smooth movements. The motor controller is protected from water and ethanol droplets by being housed in a box at the bottom of the robot. The transport system is controlled by a Raspberry Pi single-board computer that was programmed in Python for the sorting robot. In order to pick up insects from a petri dish and discharge them in a well of a 96-well microplate, a suction hose with a pipette tip is positioned by the transportation system. The hose is connected to a LA100 syringe pump (Landgraf Laborsysteme HLL GmbH, Langenhagen, Germany), that is also controlled by the Raspberry Pi. The sorting process is illustrated in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2</xref>.</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-15">Process flowchart for classification and sorting</p></caption><graphic xlink:href="444523v4_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-16">The sorting system includes two cameras with different lenses: the overview camera (C1) and the detailed view camera (C2). The first camera (C1) is a Ximea MQ042CG-CM camera with a CK12M1628S11 lens (Lensation GmbH, Karlsruhe, Germany) with a focal length of 16mm and an aperture of 2.8 is positioned directly above the petri dish to take a detailed overview image of all insects inside. This image is used for detecting insects and their position within the Petri dish for the sorting process (see <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> left). The second camera (C2) is a Ximea MQ013CG-E2 with a telecentric Lensation TCST-10-40 lens with a magnification of 1x. This camera is moved by the × and y axes of the robot to a position above the insect to take a detailed image for classification and size measuring (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figs 4</xref>, <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">6</xref>).</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-17">Sample image obtained with the detail camera (C2) before (left) and after processing (right). <bold>Left:</bold> The square Petri dish has a size of 120 × 120mm. <bold>Right:</bold> Blue line defines the area in which the object positions are determined (10 mm from edge); circles represent detected objects (green = meet size and distance conditions for imaging and movement; red = size too large and/or distance to small)</p></caption><graphic xlink:href="444523v4_fig3" position="float" orientation="portrait" hwp:id="graphic-3"/></fig><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-18">Specimen image obtained with the detail camera (C2) before <bold>(a)</bold> and after processing <bold>(b)</bold>. As well as the individual image-processing steps for determining the individual regions <bold>(i - vi). (i) &amp; (ii)</bold> Contour determination; <bold>(iii)</bold> Connecting surfaces; <bold>(iv)</bold> Placing random points; <bold>(v)</bold> Regression; <bold>(vi)</bold> Defining dividing lines</p></caption><graphic xlink:href="444523v4_fig4" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><sec id="s2a1" hwp:id="sec-4"><title hwp:id="title-8">Image Processing Software</title><p hwp:id="p-19">Three different software algorithms are used: The first algorithm determines the position of each object within the square petri dish. The second measures the length and volume of each insect. The third is an artificial neural network to classify insects into different classes.</p></sec><sec id="s2a2" hwp:id="sec-5"><title hwp:id="title-9">Determination of Object Position</title><p hwp:id="p-20">Most objects in a sample are insects, but there are also insect parts and debris. After the overview image is taken, several image processing operations are used to detect insects that are suitable for processing: (1) A median filter removes noise from the image, (2) the RGB-image is converted to grey scale, (3) an adaptive threshold filter segregates the objects, and (4) a contour finder identifies the boundaries of all objects. Three conditions must be met for an object to be considered for imaging and transfer: (1) the size must be within a specified interval, (2) the object has to be &gt;10mm away from the petri dish edge (blue line in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3</xref>), and (3) its distance to other objects must exceed a minimum threshold value. Therefore, it is desirable that insects are evenly distributed in the Petri dish to avoid clusters because they reduce the number of insects that can be sorted by the robot.</p><p hwp:id="p-21">The coordinates of the detected objects are stored in a list, which is then used to control the position of the pipetting tip and the detailed camera. After an object is removed, a new overview image is taken to determine the new coordinates of the objects, as they might have moved due to the pipetting of an object. This position identifying process continues until no more suitable objects are detected or all wells of the 96-well microplate are filled with one insect each. Due to the limited size of the Petri dish (120 × 120 mm), the number of specimens available for one sorting process is currently limited. Experiments have shown that 150 (+/-10) specimens is the optimum for sorting when approximately equally distributed. With this number, hardly any preparatory work, i.e. additional separation of the insects in the Petri dish, is necessary. Since the work of the robot is automatically interrupted after the 96-well-microplate is filled, new insects can also be added to the Petri dish during this work step.</p><p hwp:id="p-22">The robot excludes specimens from sorting that are too large for automatic processing. For efficient operation, it is thus preferable to only place small specimens (body length &lt; 3 mm) into the Petri dish. Size pre-sorting of whole samples can either be done manually or by employing the methods described by <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Buffington and Gates (2013)</xref>. The use of sieves enables fast and uncomplicated pre-sorting of insects into different size classes. With regard to cleaning the DiversityScanner, only the central petri-dish, the microplate, and the suction tube have contact to specimens. The dishes and microplates can be autoclaved while the suction tube can either be flushed with bleach or a new tube can be used for each new sample.</p></sec><sec id="s2a3" hwp:id="sec-6"><title hwp:id="title-10">Object Dimensions</title><p hwp:id="p-23">Lengths and volume of insect bodies are useful for estimating biomass. Several image processing operations are used to make such measurements. First, the contour is determined using morphological operators. Only those surfaces are selected which have a minimum value. If more than one surface is found (e.g. two body parts of the same specimen separated by a light area), they are connected so that there is only one contour. Within this contour, points are placed randomly, which are used to create a regression. The more points are used, the more accurate the regression and thus the estimate of the insect length will be. To find the dividing lines of the head, thorax and abdomen, straight lines are placed at right angles to and along the regression line. Only those points of a line are considered that lie within the contour in the process. Subsequently, the dividing line between the head and thorax or between the thorax and abdomen is determined by examining the changes in length. As not all species have a clear dividing line between the body regions, some dividing lines are set incorrectly or need to be adjusted manually before the total volume can be determined (see Results for details). To estimate the volume, a straight line is drawn through each body part and then additional perpendicular straight lines which must be within the body contour. Now the distance and length of the straight lines can be used to determine the volume slice by slice. The lengths and estimated volumes of the individual body parts as well as the total length can be displayed on the screen of the sorting robot and the measurements are stored. <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Figure 3</xref> shows an example of a detailed picture before (a) and after (b) the volume estimation, as well as the necessary steps (i - vi). All operations use the free OpenCV program library (version 4.5.1) and Python scripts (version 3.8.6). Currently, volume estimates are mostly satisfying for body parts that are rotationally symmetrical and the method works better for insects with rotationally symmetrical morphology such as Hymenoptera.</p></sec></sec><sec id="s2b" hwp:id="sec-7"><label>2.2</label><title hwp:id="title-11">Insect Classification</title><p hwp:id="p-24">In order to recognize different classes of insects and assign specimens to classes, machine learning algorithms based on convolutional neural networks (CNN) are applied.</p><sec id="s2b1" hwp:id="sec-8"><title hwp:id="title-12">Data Set</title><p hwp:id="p-25">We here use 5,083 colour images in 15 classes split into 3,182 for training (∼62.5 %), 777 for validation (∼15 %) and 1,124 images for testing (22.5 %; <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>).</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-5"><p hwp:id="p-26">Classes and number of images available for training, validation and testing</p></caption><graphic xlink:href="444523v4_tbl1" position="float" orientation="portrait" hwp:id="graphic-5"/></table-wrap><p hwp:id="p-27">The images were obtained with the detailed camera for insects from five Malaise trap samples: three from Germany near Rastatt, Kitzing and Framersbach and two from Italy (Province of L’Aquila: Valle di Teve and Foresta Demaniale Chiarano-Sparvera). The images reflect the abundances of each taxon that are typical for Malaise trap samples (<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">Karlsson <italic toggle="yes">et al</italic>. 2020b</xref>). Only the common classes are covered by the trained CNN. Insects that do not belong to these are assigned to residual class (N=758), which also includes images of body parts (mainly legs and wings).</p></sec><sec id="s2b2" hwp:id="sec-9"><title hwp:id="title-13">Data Augmentation</title><p hwp:id="p-28">Data augmentation was performed to increase the number of images and the invariance within a class. The following processing operations are applied randomly to the images: rotation, width shift, height shift, shear, zoom, horizontal flip and fill mode nearest.</p></sec><sec id="s2b3" hwp:id="sec-10"><title hwp:id="title-14">Network Architecture</title><p hwp:id="p-29">The VGG19 architecture is used as base model for classification, (<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Simonyan &amp; Zisserman 2014</xref>). The model is initialized with pre-trained ImageNet weights and the last layer is removed. For the new classification layer, a global average pooling, a dense layer with 1024 units and a reLU-activation, and a linear layer with a dropout rate during training of 0.4 are added. For the final classification, a softmax and a L2-regularization with a value of 0.02 are applied. In total the model has about 20.5 million parameters and the input size of an image is 224×224 pixels. The number of nodes in the last layer corresponds to the number of classes in the experiment. For training, the parameters of the original model are frozen and only the classification layer is trained. Afterwards, the whole model is optimized, whereby training is applied to all layers. Class activation maps are obtained by a global average pooling layer to illustrate the decisive features used by the neural network (<xref ref-type="fig" rid="fig5" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Fig, 5 a-d</xref>).</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-6"><p hwp:id="p-30">Class activation maps for specimens belonging to four different insect classes. The warmer the colour, the more important the region is for classifying the insects (red = very important – blue: less important). <bold>(a)</bold> Hymenoptera Diapriidae: The focus is on the antennae, head, mesosoma and the wing venation; <bold>(b)</bold> Diptera Calyptratae: Here, the focus is on the head and the eye; <bold>(c)</bold> Diptera Keroplatidae and Mycetophilidae: The focus is on the thorax and the legs; <bold>(d)</bold> Diptera Psychodidae: in this class, the focus is only on the wings</p></caption><graphic xlink:href="444523v4_fig5" position="float" orientation="portrait" hwp:id="graphic-6"/></fig></sec><sec id="s2b4" hwp:id="sec-11"><title hwp:id="title-15">Setup</title><p hwp:id="p-31">The model is implemented in Keras (version 2.4.3) based on Tensorflow (version 2.2.1) and all experiments are conducted in the Python programming language (version 3.8.6). The networks are trained on a single board computer (Nvidia, Santa Clara, California, USA) as well as on more powerful GPUs using the online tool Colabatory. The working principles of the robot are illustrated in the following video clip: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.youtube.com/watch?v=ElJ5VSHa4OI" ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=ElJ5VSHa4OI" hwp:id="ext-link-3">https://www.youtube.com/watch?v=ElJ5VSHa4OI</ext-link>.</p></sec></sec></sec><sec id="s3" hwp:id="sec-12"><label>3</label><title hwp:id="title-16">RESULTS</title><p hwp:id="p-32">Currently, the sorting robot images and pipettes insects up to 3 mm length (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Fig. 6 a-o</xref>), because larger insects do not fit through the pipetting tip. Detected insects are classified by the algorithm into 14 different classes of insects. All other insect classes and non-insect objects are combined in the class “other” (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref>). A lower-bound size limit does not exist in terms of handling but is defined by the visibility of the specimens on the overview image. However, if the smallest insects (&lt;1mm) are to be identified and sorted, the detailed specimen images should be used to avoid the processing of small body parts of insects. <xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Karlsson et al. (2020a)</xref> examined the distribution of species in Malaise traps in the Swedish Malaise trap program. The results show that 75% of the specimens belong to Diptera families with small specimens (e.g. Chironomidae, Sciaridae, Phoridae, Cecidomyiidae, Mycetophilidae). <xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Brown (2005)</xref> documented the same bias towards small Diptera families (64-84%) for several Neotropical samples. In addition, ca. 50% of Hymenoptera are small, which means that in many samples &gt;60% of the specimens are suitable for the DiversityScanner.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-7"><p hwp:id="p-33">Sample images for the 15 classes (a-p). <bold>(a)</bold> acalyptrate Diptera; <bold>(b)</bold> Diptera Calyptratae; <bold>(c)</bold> Diptera Cecidomyiidae; <bold>(d)</bold> Diptera Chironomidae; <bold>(e)</bold> Diptera Dolichopodidae; <bold>(f)</bold> Diptera Empididae &amp; Hybotidae; <bold>(g)</bold> Diptera Keroplatidae &amp; Mycetophilidae; <bold>(h)</bold> Diptera Phoridae; <bold>(i)</bold> Diptera Psychodidae; <bold>(j)</bold> Diptera Sciaridae; (<bold>k)</bold> Hemiptera Cicadellidae; <bold>(l)</bold> Hymenoptera Braconidae; <bold>(m)</bold> Hymenoptera Diapriidae; <bold>(n)</bold> Hymenoptera Ichneumonidae; <bold>(o)</bold> Other Insects (e.g. Hemiptera Aphididae; <bold>(p)</bold> Other objects</p></caption><graphic xlink:href="444523v4_fig6" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-8"><p hwp:id="p-34">Classification accuracy (predicted label = true label) for each class in percent</p></caption><graphic xlink:href="444523v4_tbl2" position="float" orientation="portrait" hwp:id="graphic-8"/></table-wrap><p hwp:id="p-35">The best classification result is for “Hymenoptera Diapriidae” and “Hemiptera Cicadellidae”, where all insects were correctly classified (100 %), whereas insects of the class Hymenoptera Ichneumonidae had the lowest correct classification rate (75 %). <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref> shows the confusion matrix for the testing images by comparing the “predicted” (CNN) with the “true” labels (taxonomists). The diagonal (grey) shows the percentage of images in a class that were correctly assigned. The table furthermore specifies the proportion of images incorrectly assigned. For the class Diptera Chironomidae, for example, 97% are correctly predicted, but 3% are assigned to the class Diptera Cecidomyiidae.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3.</label><caption hwp:id="caption-9"><p hwp:id="p-36">Confusion matrix for 15 classes including the “other” class. The true label is shown on the y-axis, the predicted label on the x-axis</p></caption><graphic xlink:href="444523v4_tbl3" position="float" orientation="portrait" hwp:id="graphic-9"/><graphic xlink:href="444523v4_tbl3a" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap><p hwp:id="p-37">In terms of taxon-specific processing, the DiversityScanner currently supports two processes: Either all insects are classified and sorted until the last well of the 96-well microplate is filled or only insects belonging to a predefined class are pipetted into the plates.</p><p hwp:id="p-38">To determine the times required for sorting and classifying, the times required for each specimen were determined (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Fig. 7a</xref>). The average time is 38 seconds (well plate #1) and 37 seconds for the second well plate with some specimens (e.g., #1, #8, #35) requiring significantly more time. For this test, the biomass determination was deactivated; i.e., the total time required per object consisted of the time for the calculation of the GUI, the write operations on the SD card of the Raspberry Pi, the movement time of the axes, the runtimes of the algorithms for object detection and classification as well as the times for moving the syringe pump. Faster sorting is feasible, but reduces quality because specimens are not allowed to settle for imaging and expulsion into the well. Note that classification and object recognition only account for a small proportion of the total time required per specimen (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Fig 7 b, c</xref>). The average time for classification is 4.28 seconds for the first well plate and 4.24 seconds for the second (total average: 4.26 seconds). The average time for object detection is &lt;1.30 seconds. Thus, most of the time shown in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-3" hwp:rel-id="F7">Fig 7</xref> (a) is needed for the actual sorting as well as the other operations described above.</p><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2 xref-fig-7-3 xref-fig-7-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2021.05.17.444523v4/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7:</label><caption hwp:id="caption-10"><p hwp:id="p-39">Times of the process steps of the Diversity Scanner. <bold>(a)</bold> Time per specimen for sorting <bold>(b)</bold> Time per specimen for classification <bold>(c)</bold> Time per overview image for object detection <bold>(d)</bold> Time per specimen for volume estimation</p></caption><graphic xlink:href="444523v4_fig7" position="float" orientation="portrait" hwp:id="graphic-11"/></fig><p hwp:id="p-40">The image processing for volume estimation takes significantly longer and shows the greatest differences in the time required per specimen. On the Raspberry Pi in particular 108.06 seconds on average for 144 specimens. The times are shown in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-4" hwp:rel-id="F7">Fig 7</xref> (d). As the biomass determination would slow down the otherwise fast sorting and classification of individual specimens, it is recommended not to run it during the sorting process. If it is necessary to estimate the volume, it is advisable to apply the algorithm to the images afterwards and to transfer this process to a faster computer. For example, on a notebook with Intel Core i7-4510U with 2.0 GHz, the average processing time for the same data set is only 12.24 seconds per specimen. To ensure that the individual volume of the head, thorax and abdomen was always determined for the runtime tests and not the total volume, only images of the class Hymenoptera Diapriidae were used for the test as these often show a clear dividing line. Note, however, that for those cases where an automatic determination of the body tagmata is not possible, the images are labelled so that the user can manually set areas.</p></sec><sec id="s4" hwp:id="sec-13"><label>4</label><title hwp:id="title-17">DISCUSSION</title><p hwp:id="p-41">The use of CNNs for the identification of charismatic species is becoming a routine procedure (<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Fairbrass <italic toggle="yes">et al</italic>. 2019</xref>; <xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Stowell <italic toggle="yes">et al</italic>. 2019</xref>; <xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Tabak <italic toggle="yes">et al</italic>. 2019</xref>; <xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Milošević <italic toggle="yes">et al</italic>. 2020</xref>). However, these methods have been largely unavailable for small invertebrates although they comprise much of the multicellular animal species diversity (<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">Groombridge 1992</xref>; <xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Stork <italic toggle="yes">et al</italic>. 2015</xref>) and contribute many ecosystem services (<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Wagner 2020</xref>). The main problem is the lack of trained CNNs, which cannot be obtained without first producing sets of training images for thousands of species. We believe that the best strategy for obtaining these training images is combining automated specimen imaging with DNA barcoding. A DiversityScanner can image 1,000 specimens per day so that a laboratory equipped with a few DiversityScanner will be able to process several full invertebrate samples per day. Each contains thousands of specimens that can be imaged with minimal manual labour. After imaging, the specimens are moved to microplates for DNA barcoding. Once barcoded, the images can be re-labeled with approximately species-level resolution given that most animal species have species-specific barcodes, even when congruence with morphology is rigorously assessed by barcoding thousands of specimens (<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-2" hwp:rel-id="ref-41">Wang <italic toggle="yes">et al</italic>. 2018</xref>; <xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">Yeo <italic toggle="yes">et al</italic>. 2018</xref>). Common species, genera, and families rapidly acquire sufficiently large numbers of training images. Indeed, for the most common 14 “classes” of insects in Malaise traps, we already had enough images for creating such networks after partially imaging only five Malaise trap samples (5083 images, see 2.3 Insect Classification - Data Set).</p><p hwp:id="p-42">A further useful feature of the DiversityScanner is that for particularly hyperabundant taxa, it can be instructed to only transfer a limited number of specimens for a particular class. For example, the robot can be told to move only 1-2 microplates’ worth of non-biting midges (Chironomidae), if this taxon is too abundant. This ability to only find and move some taxa also helps with implementing clade-specific molecular recipes (e.g., different DNA extraction or PCR recipes for taxa that are difficult to barcode: e.g. Hymenoptera) and restricting barcoding to either males or females given that often only one sex has species-specific morphological differences (<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Eberhard 2010</xref>).</p><p hwp:id="p-43">With regard to classification accuracy rates, we observe only a very weak correlation between the number of training images, morphological heterogeneity and classification accuracy. There are classes with large numbers of training images that perform better than classes with lower numbers (e.g., Diptera Calyptratae 57 training images: 83% vs. Diptera Phoridae 64 training images: 97%), but the better performance of Phoridae could also be due to higher morphological uniformity. However, this is not in line with the observation of a comparatively high classification accuracy obtained for the class “others” that has the highest heterogeneity. Indeed, this class performed better than Hymenoptera Ichneumonidae (6% better: 81%) whereby it is conceivable that Ichneumonidae performed poorly because it may have suffered from incomplete imaging given that the specimens are often at the upper limit of the manageable size for the DiversityScanner. Overall, we need more data to understand fully which factors improve classification accuracy.</p><p hwp:id="p-44">Some biologists doubt that CNNs will be sufficiently powerful to yield species-level identifications for closely related species and we agree that it remains unclear to what extent species-level identifications can be achieved (<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-3" hwp:rel-id="ref-2">Ärje <italic toggle="yes">et al</italic>. 2020b</xref>; <xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">Knyshov <italic toggle="yes">et al</italic>. 2021</xref>). However, we predict that the main limitation will be the number, quality, and orientation of training images. <xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6</xref> illustrates the latter problem.</p><p hwp:id="p-45">The insects are imaged in very different positions so that an even larger number of training images would be needed in order to have a realistic chance for achieving high accuracy at high taxonomic resolution. One solution for this problem is imaging identified specimens in many orientations. Fortunately, this is now feasible because high-quality cameras are available for obtaining large numbers of images at different magnifications and orientations. This is particularly straightforward once specimens have been pre-sorted to putative species based on DNA barcodes. As illustrated by the BIODISCOVER robot, inserting these specimens into a cuvette allows for imaging from many angles. We predict that once large numbers of species have been extensively imaged and included in CNNs, the DiversityScanner will be able to identify many specimens based on images only. DNA barcoding would be restricted to those specimens that are not identifiable based on visual information; i.e., the DiversityScanner would learn how to sort specimen to species, but also learn how to identify those specimens that still require barcoding. This will make the robot a powerful tool for discovering rare new species in large samples. This ability would be particularly important in the 21<sup>st</sup> century because new species continue to arrive at well-characterized sampling sites (<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Parmesan 2006</xref>). Some of these species recently shifted their distribution in response to climate change (<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Wilson <italic toggle="yes">et al</italic>. 2007</xref>; <xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Fartmann <italic toggle="yes">et al</italic>. 2021</xref>) while others may be new anthropogenic introductions (<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Bertelsmeier 2021</xref>). For both it would be desirable to have an early-warning system based on automated workflows. Note also, that we here implicitly assumed that the biologists want species-level identifications for entire samples, but for many purposes it would be sufficient to have CNNs that provide family-genus- or species-group level identifications for some taxa (e.g., those where external morphology is insufficient for species-level identification).</p><p hwp:id="p-46">This first version of the DiversityScanner still struggles with several aspects of complex Malaise trap samples. Specimens often clump so that they may be erroneously recognised as one insect on the overview image. This may lead to the pick-up of several specimens, but improved object detection algorithms and additional scrutiny of the specimen images is likely to resolve such cases in future versions of the scanner. Such algorithms would also avoid instances where no insect is picked up although the overview image had identified an object. Here, further optimizati Classes and number of images available for training, validation and testing on of the volume and the volume flow by the syringe pump may be needed. An additional modification of the DiverstiyScanner that is currently under consideration is the handling of larger specimens. The suction tip diameter of the tube can be increased or one can install a gripper with a sensor-based feedback system. These changes could be accommodated within the current design because they only require changes with regard to the maximum size and minimum distance parameters. Larger specimens could also be imaged completely by installing an additional lens.</p><p hwp:id="p-47">One major goal is to keep the design of the DiversityScanner simple and comparatively low-cost (&lt;5,000 €), so that eventually many robots can sort a large number of insects simultaneously in many laboratories. Robotic handling is desirable, because parataxonomists fatigue and make unpredictable errors (<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Krell 2004</xref>). Furthermore, parataxonomists “only” sort but do not image. Compare this to having several DiversityScanners running in parallel. One operator would be able to feed them with trap samples and handle the DNA barcoding of filled plates. Thousands of imaged and barcoded specimens could be obtained every week. This makes robotic specimen handling an attractive alternative to manual sorting.</p><p hwp:id="p-48">Currently, the robot only handles and images small invertebrates, because we wanted the robot to cover the most abundant specimen size class. In comparison, the BIODISCOVER robot (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-2" hwp:rel-id="ref-1">Ärje <italic toggle="yes">et al</italic>. 2020a</xref>) allows for the imaging of larger specimens. Other systems such as the light trap for photographing alive moths presented by <xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Bjerge et al. (2021)</xref> are also capable of photographing much larger insects. However, what these systems have in common is that large images are taken from which the areas containing a specimen are cropped. As a result, the resolution of the images is comparatively low. (BIODISCOVER (<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-3" hwp:rel-id="ref-1">Ärje <italic toggle="yes">et al</italic>. 2020a</xref>): 496×496 pixels; Moth light trap (<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-2" hwp:rel-id="ref-5">Bjerge <italic toggle="yes">et al</italic>. 2021</xref>) average size: 368 × 353). The DiversityScanner moves the camera over the insect which can then be photographed at high resolution (here: 1280×1024 pixels). For classification using neural networks, a lower resolution is sufficient. However, higher resolution is important for determining the volume as well as for taxonomic work on the specimens. In addition, the BIODISCOVER robot requires that all insects are fed manually one after the other and that they are returned to a common tray. In this regard, the setup presented by <xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-2" hwp:rel-id="ref-11">Chamblin et al. (2011)</xref> for photographing and sorting soil mesofauna is more promising because it is able to remove individual specimens from a Petri dish and place them into a 96-well microplate. It has similar size restrictions as the DiversityScanner but is comparatively expensive because it uses a microscope and a 6-axis robot arm.</p><p hwp:id="p-49">Overall, we believe that robots like the DiversityScanner have the potential to solve some of the problems that Robert May mentioned when he bemoaned our lack of biodiversity knowledge. Automation can expedite biodiversity discovery and monitoring of neglected “dark taxa”. Of course, the DiversityScanner can only address some of the challenges. For example, newly discovered species will still need description and described species identification. Moreover, even when all species have been described, we will still know very little about the ecological roles that these species play in the ecosystems. Fortunately, molecular approaches to diet analysis and life history stage matching can help fill these gaps (<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-2" hwp:rel-id="ref-45">Yeo <italic toggle="yes">et al</italic>. 2018</xref>; <xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Srivathsan <italic toggle="yes">et al</italic>. 2019b</xref>). However, given that ecosystems routinely consist of thousands of species, automation and data analysis will also be needed for high-throughput species interaction research.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-18">Acknowledgments</title><p hwp:id="p-50">We would like to specially thank Daniel Moser and Stefan Vollmannshauser for their support with manufacturing the mechanical parts and helping us with connecting the electronic circuits. Mr Leshon Lee prepared the video documenting the working principles of the DiversityScanner. Funding was provided by the Center for Integrative Biodiversity Discovery at the Museum für Naturkunde Berlin.</p></ack><sec id="s5" hwp:id="sec-14"><title hwp:id="title-19">Author Contributions</title><p hwp:id="p-51">Conceptualization: R.M., T.v.R., L.W. and C.P.; writing original draft preparation: L.W., R.M. and M.G.; writing review and editing: C.P., R.M., S.S., P.C., M.B. and T.v.R.; visualization: L.W. and M.G. and S.S.; supervision: C.P., R.M. and T.v.R.; funding acquisition: C.P., T.v.R. and R.M.; L.W. and C.P. contributed equally. All authors have read and agreed to the published version of the manuscript.</p></sec><sec id="s6" hwp:id="sec-15"><title hwp:id="title-20">Data Availability Statement</title><p hwp:id="p-52">All image data that were used for training and testing are accessible at the media repository of the Museum für Naturkunde Berlin: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://doi.org/10.7479/4tbx-qm72" ext-link-type="uri" xlink:href="https://doi.org/10.7479/4tbx-qm72" hwp:id="ext-link-4">https://doi.org/10.7479/4tbx-qm72</ext-link> All files for printing the robot parts and the software code are accessible at the repository of the Open Science Framework: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://osf.io/en594/" ext-link-type="uri" xlink:href="https://osf.io/en594/" hwp:id="ext-link-5">https://osf.io/en594/</ext-link></p></sec><ref-list hwp:id="ref-list-1"><label>5</label><title hwp:id="title-21">REFERENCES</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1 xref-ref-1-2 xref-ref-1-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Ärje J"><surname>Ärje</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Melvad C"><surname>Melvad</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jeppesen MR"><surname>Jeppesen</surname> <given-names>MR</given-names></string-name> <etal>et al.</etal> (<year>2020a</year>) <article-title hwp:id="article-title-2">Automatic image based identification and biomass estimation of invertebrates</article-title>. <source hwp:id="source-1">Methods in Ecology and Evolution</source>, <volume>11</volume>, <fpage>922</fpage>–<lpage>931</lpage>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2 xref-ref-2-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Ärje J"><surname>Ärje</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Raitoharju J"><surname>Raitoharju</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Iosifidis A"><surname>Iosifidis</surname> <given-names>A</given-names></string-name> <etal>et al.</etal> (<year>2020b</year>) <article-title hwp:id="article-title-3">Human experts vs. machines in taxa recognition</article-title>. <source hwp:id="source-2">Signal Processing: Image Communication</source>, <volume>87</volume>, <fpage>115917</fpage>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Bar-On YM"><surname>Bar-On</surname> <given-names>YM</given-names></string-name>, <string-name name-style="western" hwp:sortable="Phillips R"><surname>Phillips</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Milo R"><surname>Milo</surname> <given-names>R</given-names></string-name> (<year>2018</year>) <article-title hwp:id="article-title-4">The biomass distribution on Earth</article-title>. <source hwp:id="source-3">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>, <fpage>6506</fpage>–<lpage>6511</lpage>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Bertelsmeier C"><surname>Bertelsmeier</surname> <given-names>C</given-names></string-name> (<year>2021</year>) <article-title hwp:id="article-title-5">Globalization and the anthropogenic spread of invasive social insects</article-title>. <source hwp:id="source-4">Current opinion in insect science</source>, <volume>46</volume>, <fpage>16</fpage>–<lpage>23</lpage>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1 xref-ref-5-2"><citation publication-type="book" citation-type="book" ref:id="2021.05.17.444523v4.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Bjerge K"><surname>Bjerge</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nielsen JB"><surname>Nielsen</surname> <given-names>JB</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sepstrup MV"><surname>Sepstrup</surname> <given-names>MV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Helsing-Nielsen F"><surname>Helsing-Nielsen</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Høye TT"><surname>Høye</surname> <given-names>TT</given-names></string-name> (<year>2021</year>) <chapter-title>An Automated Light Trap to Monitor Moths (Lepidoptera) Using Computer Vision-Based Tracking and Deep Learning</chapter-title>. <source hwp:id="source-5">Sensors</source> (<publisher-loc>Basel, Switzerland</publisher-loc>), <volume>21</volume>, <fpage>343</fpage>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Borkent A"><surname>Borkent</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brown BV"><surname>Brown</surname> <given-names>BV</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-6">How to inventory tropical flies (Diptera)--One of the megadiverse orders of insects</article-title>. <source hwp:id="source-6">Zootaxa</source>, <volume>3949</volume>, <fpage>301</fpage>–<lpage>322</lpage>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Brown BV"><surname>Brown</surname> <given-names>BV</given-names></string-name> (<year>2005</year>) <article-title hwp:id="article-title-7">Malaise Trap Catches and the Crisis in Neotropical Dipterology</article-title>. <source hwp:id="source-7">American Entomologist</source>, <volume>51</volume>, <fpage>180</fpage>–<lpage>183</lpage>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Brown BV"><surname>Brown</surname> <given-names>BV</given-names></string-name>, <string-name name-style="western" hwp:sortable="Borkent A"><surname>Borkent</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adler PH"><surname>Adler</surname> <given-names>PH</given-names></string-name> <etal>et al.</etal> (<year>2018</year>) <article-title hwp:id="article-title-8">Comprehensive inventory of true flies (Diptera) at a tropical site</article-title>. <source hwp:id="source-8">Communications biology</source>, <volume>1</volume>, <fpage>21</fpage>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Buchner D"><surname>Buchner</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Macher T-H"><surname>Macher</surname> <given-names>T-H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Beermann AJ"><surname>Beermann</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Werner M-T"><surname>Werner</surname> <given-names>M-T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leese F"><surname>Leese</surname> <given-names>F</given-names></string-name> (<year>2021</year>) <article-title hwp:id="article-title-9">Standardized high-throughput biomonitoring using DNA metabarcoding: Strategies for the adoption of automated liquid handlers</article-title>. <source hwp:id="source-9">Environmental Science and Ecotechnology</source>, <volume>8</volume>, <fpage>100122</fpage>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="other" citation-type="journal" ref:id="2021.05.17.444523v4.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Buffington M"><surname>Buffington</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gates M"><surname>Gates</surname> <given-names>M</given-names></string-name> (<year>2013</year>) <article-title hwp:id="article-title-10">The Fractionator: a simple tool for mining ‘Black Gold’</article-title>. <source hwp:id="source-10">Skaphion</source>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1 xref-ref-11-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Chamblin MA"><surname>Chamblin</surname> <given-names>MA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paasch RK"><surname>Paasch</surname> <given-names>RK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lytle DA"><surname>Lytle</surname> <given-names>DA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Moldenke AR"><surname>Moldenke</surname> <given-names>AR</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shapiro LG"><surname>Shapiro</surname> <given-names>LG</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dietterich TG"><surname>Dietterich</surname> <given-names>TG</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-11">Design of an Automated System for Imaging and Sorting Soil Mesofauna</article-title>. <source hwp:id="source-11">Biological Engineering Transactions</source>, <volume>4</volume>, <fpage>17</fpage>–<lpage>41</lpage>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Creedy TJ"><surname>Creedy</surname> <given-names>TJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ng WS"><surname>Ng</surname> <given-names>WS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vogler AP"><surname>Vogler</surname> <given-names>AP</given-names></string-name> (<year>2019</year>) <article-title hwp:id="article-title-12">Toward accurate species-level metabarcoding of arthropod communities from the tropical forest canopy</article-title>. <source hwp:id="source-12">Ecology and evolution</source>, <volume>9</volume>, <fpage>3105</fpage>–<lpage>3116</lpage>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="book" citation-type="book" ref:id="2021.05.17.444523v4.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Eberhard WG"><surname>Eberhard</surname> <given-names>WG</given-names></string-name> (<year>2010</year>) <chapter-title>Rapid divergent evolution of genitalia</chapter-title>. <source hwp:id="source-13">In : The evolution of primary sexual characters in animals</source>, pp. <fpage>40</fpage>–<lpage>78</lpage>. <publisher-name>Oxford University Press, Oxford</publisher-name>, <publisher-loc>New York</publisher-loc>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Fairbrass AJ"><surname>Fairbrass</surname> <given-names>AJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Firman M"><surname>Firman</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Williams C"><surname>Williams</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Brostow GJ"><surname>Brostow</surname> <given-names>GJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Titheridge H"><surname>Titheridge</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones KE"><surname>Jones</surname> <given-names>KE</given-names></string-name> (<year>2019</year>) <article-title hwp:id="article-title-13">CityNet—Deep learning tools for urban ecoacoustic assessment</article-title>. <source hwp:id="source-14">Methods in Ecology and Evolution</source>, <volume>10</volume>, <fpage>186</fpage>–<lpage>197</lpage>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Fartmann T"><surname>Fartmann</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Poniatowski D"><surname>Poniatowski</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holtmann L"><surname>Holtmann</surname> <given-names>L</given-names></string-name> (<year>2021</year>) <article-title hwp:id="article-title-14">Habitat availability and climate warming drive changes in the distribution of grassland grasshoppers</article-title>. <source hwp:id="source-15">Agriculture, Ecosystems &amp; Environment</source>, <volume>320</volume>, <fpage>107565</fpage>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Feng L"><surname>Feng</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhanu B"><surname>Bhanu</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heraty J"><surname>Heraty</surname> <given-names>J</given-names></string-name> (<year>2016</year>) <article-title hwp:id="article-title-15">A software system for automated identification and retrieval of moth images based on wing attributes</article-title>. <source hwp:id="source-16">Pattern Recognition</source>, <volume>51</volume>, <fpage>225</fpage>–<lpage>241</lpage>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2"><citation publication-type="other" citation-type="journal" ref:id="2021.05.17.444523v4.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Groombridge B"><surname>Groombridge</surname> <given-names>B</given-names></string-name> (<year>1992</year>) <article-title hwp:id="article-title-16">Global biodiversity status of the Earth’s living resources, No. 333.95 G562gl</article-title>. <source hwp:id="source-17">World Conservation Monitoring Centre, Cambridge (RU)</source>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="other" citation-type="journal" ref:id="2021.05.17.444523v4.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Hartop E"><surname>Hartop</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Srivathsan A"><surname>Srivathsan</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ronquist F"><surname>Ronquist</surname> <given-names>F</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier R"><surname>Meier</surname> <given-names>R</given-names></string-name> (<year>2021</year>) <source hwp:id="source-18">Large-scale Integrative Taxonomy (LIT): resolving the data conundrum for dark taxa</source>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Hebert PDN"><surname>Hebert</surname> <given-names>PDN</given-names></string-name>, <string-name name-style="western" hwp:sortable="Braukmann TWA"><surname>Braukmann</surname> <given-names>TWA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prosser SWJ"><surname>Prosser</surname> <given-names>SWJ</given-names></string-name> <etal>et al.</etal> (<year>2018</year>) <article-title hwp:id="article-title-17">A Sequel to Sanger: amplicon sequencing that scales</article-title>. <source hwp:id="source-19">BMC genomics</source>, <volume>19</volume>, <fpage>219</fpage>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Karlsson D"><surname>Karlsson</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Forshage M"><surname>Forshage</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holston K"><surname>Holston</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ronquist F"><surname>Ronquist</surname> <given-names>F</given-names></string-name> (<year>2020a</year>) <article-title hwp:id="article-title-18">The data of the Swedish Malaise Trap Project, a countrywide inventory of Sweden’s insect fauna</article-title>. <source hwp:id="source-20">Biodiversity Data Journal</source>, <volume>8</volume>, <fpage>e56286</fpage>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Karlsson D"><surname>Karlsson</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hartop E"><surname>Hartop</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Forshage M"><surname>Forshage</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jaschhof M"><surname>Jaschhof</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ronquist F"><surname>Ronquist</surname> <given-names>F</given-names></string-name> (<year>2020b</year>) <article-title hwp:id="article-title-19">The Swedish Malaise Trap Project: A 15 Year Retrospective on a Countrywide Insect Inventory</article-title>. <source hwp:id="source-21">Biodiversity data journal</source>, <volume>8</volume>, <fpage>e47255</fpage>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="website" citation-type="web" ref:id="2021.05.17.444523v4.22" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Kim M"><surname>Kim</surname> <given-names>M</given-names></string-name> (<year>2014</year>) <source hwp:id="source-22">Mealworm sorting unit and sorting apparatus</source>. Available from : (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://patents.google.com/patent/KR101464734B1/en" ext-link-type="uri" xlink:href="https://patents.google.com/patent/KR101464734B1/en" hwp:id="ext-link-6">https://patents.google.com/patent/KR101464734B1/en</ext-link>).</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Knyshov A"><surname>Knyshov</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hoang S"><surname>Hoang</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weirauch C"><surname>Weirauch</surname> <given-names>C</given-names></string-name> (<year>2021</year>) <article-title hwp:id="article-title-20">Pretrained Convolutional Neural Networks Perform Well in a Challenging Test Case: Identification of Plant Bugs (Hemiptera: Miridae) Using a Small Number of Training Images</article-title>. <source hwp:id="source-23">Insect Systematics and Diversity</source>, <volume>5</volume>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Krell F-T"><surname>Krell</surname> <given-names>F-T</given-names></string-name> (<year>2004</year>) <article-title hwp:id="article-title-21">Parataxonomy vs. taxonomy in biodiversity studies – pitfalls and applicability of ‘morphospecies’ sorting</article-title>. <source hwp:id="source-24">Biodiversity and Conservation</source>, <volume>13</volume>, <fpage>795</fpage>–<lpage>812</lpage>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="other" citation-type="journal" ref:id="2021.05.17.444523v4.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Lepek H"><surname>Lepek</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nave T"><surname>Nave</surname> <given-names>T</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fleischmann Y"><surname>Fleischmann</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eisenberg R"><surname>Eisenberg</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Karlin BE"><surname>Karlin</surname> <given-names>BE</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tirosh I"><surname>Tirosh</surname> <given-names>I</given-names></string-name> (<year>2020</year>) <source hwp:id="source-25">Method for sex sorting of mosquitoes and apparatus therefore: US Patent(16/479,648)</source>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="May RM"><surname>May</surname> <given-names>RM</given-names></string-name> (<year>2011</year>) <article-title hwp:id="article-title-22">Why worry about how many species and their loss?</article-title> <source hwp:id="source-26">PLoS biology</source>, <volume>9</volume>, <fpage>e1001130</fpage>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Milošević D"><surname>Milošević</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Milosavljević A"><surname>Milosavljević</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Predić B"><surname>Predić</surname> <given-names>B</given-names></string-name> <etal>et al.</etal> (<year>2020</year>) <article-title hwp:id="article-title-23">Application of deep learning in aquatic bioassessment: Towards automated identification of non-biting midges</article-title>. <source hwp:id="source-27">The Science of the total environment</source>, <volume>711</volume>, <fpage>135160</fpage>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Parmesan C"><surname>Parmesan</surname> <given-names>C</given-names></string-name> (<year>2006</year>) <article-title hwp:id="article-title-24">Ecological and Evolutionary Responses to Recent Climate Change</article-title>. <source hwp:id="source-28">Annual Review of Ecology, Evolution, and Systematics</source>, <volume>37</volume>, <fpage>637</fpage>–<lpage>669</lpage>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Perre P"><surname>Perre</surname> <given-names>P</given-names></string-name>, <string-name name-style="western" hwp:sortable="Faria FA"><surname>Faria</surname> <given-names>FA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jorge LR"><surname>Jorge</surname> <given-names>LR</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title hwp:id="article-title-25">Toward an Automated Identification of Anastrepha Fruit Flies in the fraterculus group (Diptera, Tephritidae)</article-title>. <source hwp:id="source-29">Neotropical entomology</source>, <volume>45</volume>, <fpage>554</fpage>–<lpage>558</lpage>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Pfriem A"><surname>Pfriem</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pylatiuk C"><surname>Pylatiuk</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Alshut R"><surname>Alshut</surname> <given-names>R</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ziegener B"><surname>Ziegener</surname> <given-names>B</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schulz S"><surname>Schulz</surname> <given-names>S</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bretthauer G"><surname>Bretthauer</surname> <given-names>G</given-names></string-name> (<year>2012</year>) <article-title hwp:id="article-title-26">A modular, low-cost robot for zebrafish handling. Annual International Conference of the IEEE Engineering in Medicine and Biology Society</article-title>. <source hwp:id="source-30">IEEE Engineering in Medicine and Biology Society. Annual International Conference</source>, <volume>2012</volume>, <fpage>980</fpage>–<lpage>983</lpage>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Pomerantz A"><surname>Pomerantz</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peñafiel N"><surname>Peñafiel</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arteaga A"><surname>Arteaga</surname> <given-names>A</given-names></string-name> <etal>et al.</etal> (<year>2018</year>) <article-title hwp:id="article-title-27">Real-time DNA barcoding in a rainforest using nanopore sequencing: opportunities for rapid biodiversity assessments and local capacity building</article-title>. <source hwp:id="source-31">GigaScience</source>, <volume>7</volume>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Sarpola MJ"><surname>Sarpola</surname> <given-names>MJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paasch RK"><surname>Paasch</surname> <given-names>RK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mortensen EN"><surname>Mortensen</surname> <given-names>EN</given-names></string-name> <etal>et al.</etal> (<year>2008</year>) <article-title hwp:id="article-title-28">An Aquatic Insect Imaging System to Automate Insect Classification</article-title>. <source hwp:id="source-32">Transactions of the ASABE</source>, <volume>51</volume>, <fpage>2217</fpage>–<lpage>2225</lpage>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><citation publication-type="website" citation-type="web" ref:id="2021.05.17.444523v4.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Simonyan K"><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zisserman A"><surname>Zisserman</surname> <given-names>A</given-names></string-name> (<year>2014</year>) <source hwp:id="source-33">Very Deep Convolutional Networks for Large-Scale Image Recognition</source>. Available from : (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://arxiv.org/pdf/1409.1556v6" ext-link-type="uri" xlink:href="http://arxiv.org/pdf/1409.1556v6" hwp:id="ext-link-7">http://arxiv.org/pdf/1409.1556v6</ext-link>).</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Srivathsan A"><surname>Srivathsan</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hartop E"><surname>Hartop</surname> <given-names>E</given-names></string-name>, <string-name name-style="western" hwp:sortable="Puniamoorthy J"><surname>Puniamoorthy</surname> <given-names>J</given-names></string-name> <etal>et al.</etal> (<year>2019a</year>) <article-title hwp:id="article-title-29">Rapid, large-scale species discovery in hyperdiverse taxa using 1D MinION sequencing</article-title>. <source hwp:id="source-34">BMC biology</source>, <volume>17</volume>, <fpage>96</fpage>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2 xref-ref-35-3"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Srivathsan A"><surname>Srivathsan</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee L"><surname>Lee</surname> <given-names>L</given-names></string-name>, <string-name name-style="western" hwp:sortable="Katoh K"><surname>Katoh</surname> <given-names>K</given-names></string-name> <etal>et al.</etal> (<year>2021</year>) <article-title hwp:id="article-title-30">ONTbarcoder and MinION barcodes aid biodiversity discovery and identification by everyone, for everyone</article-title>. <source hwp:id="source-35">BMC Biology</source>, <volume>19</volume>, <fpage>217</fpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Srivathsan A"><surname>Srivathsan</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nagarajan N"><surname>Nagarajan</surname> <given-names>N</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier R"><surname>Meier</surname> <given-names>R</given-names></string-name> (<year>2019b</year>) <article-title hwp:id="article-title-31">Boosting natural history research via metagenomic clean-up of crowdsourced feces</article-title>. <source hwp:id="source-36">PLoS biology</source>, <volume>17</volume>, <fpage>e3000517</fpage>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Stork NE"><surname>Stork</surname> <given-names>NE</given-names></string-name>, <string-name name-style="western" hwp:sortable="McBroom J"><surname>McBroom</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gely C"><surname>Gely</surname> <given-names>C</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hamilton AJ"><surname>Hamilton</surname> <given-names>AJ</given-names></string-name> (<year>2015</year>) <article-title hwp:id="article-title-32">New approaches narrow global species estimates for beetles, insects, and terrestrial arthropods</article-title>. <source hwp:id="source-37">Proceedings of the National Academy of Sciences</source>, <volume>112</volume>, <fpage>7519</fpage>–<lpage>7523</lpage>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Stowell D"><surname>Stowell</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wood MD"><surname>Wood</surname> <given-names>MD</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pamuła H"><surname>Pamuła</surname> <given-names>H</given-names></string-name>, <string-name name-style="western" hwp:sortable="Stylianou Y"><surname>Stylianou</surname> <given-names>Y</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glotin H"><surname>Glotin</surname> <given-names>H</given-names></string-name> (<year>2019</year>) <article-title hwp:id="article-title-33">Automatic acoustic detection of birds through deep learning: The first Bird Audio Detection challenge</article-title>. <source hwp:id="source-38">Methods in Ecology and Evolution</source>, <volume>10</volume>, <fpage>368</fpage>– <lpage>380</lpage>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Tabak MA"><surname>Tabak</surname> <given-names>MA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norouzzadeh MS"><surname>Norouzzadeh</surname> <given-names>MS</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wolfson DW"><surname>Wolfson</surname> <given-names>DW</given-names></string-name> <etal>et al.</etal> (<year>2019</year>) <article-title hwp:id="article-title-34">Machine learning to classify animal species in camera trap images: Applications in ecology</article-title>. <source hwp:id="source-39">Methods in Ecology and Evolution</source>, <volume>10</volume>, <fpage>585</fpage>–<lpage>590</lpage>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Wagner DL"><surname>Wagner</surname> <given-names>DL</given-names></string-name> (<year>2020</year>) <article-title hwp:id="article-title-35">Insect Declines in the Anthropocene</article-title>. <source hwp:id="source-40">Annual review of entomology</source>, <volume>65</volume>, <fpage>457</fpage>–<lpage>480</lpage>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1 xref-ref-41-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.41" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Wang WY"><surname>Wang</surname> <given-names>WY</given-names></string-name>, <string-name name-style="western" hwp:sortable="Srivathsan A"><surname>Srivathsan</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Foo M"><surname>Foo</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yamane SK"><surname>Yamane</surname> <given-names>SK</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier R"><surname>Meier</surname> <given-names>R</given-names></string-name> (<year>2018</year>) <article-title hwp:id="article-title-36">Sorting specimen-rich invertebrate samples with cost-effective NGS barcodes: Validating a reverse workflow for specimen processing</article-title>. <source hwp:id="source-41">Molecular ecology resources</source>, <volume>18</volume>, <fpage>490</fpage>–<lpage>501</lpage>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.42" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Watsa M"><surname>Watsa</surname> <given-names>M</given-names></string-name>, <string-name name-style="western" hwp:sortable="Erkenswick GA"><surname>Erkenswick</surname> <given-names>GA</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pomerantz A"><surname>Pomerantz</surname> <given-names>A</given-names></string-name>, <string-name name-style="western" hwp:sortable="Prost S"><surname>Prost</surname> <given-names>S</given-names></string-name> (<year>2020</year>) <article-title hwp:id="article-title-37">Portable sequencing as a teaching tool in conservation and biodiversity research</article-title>. <source hwp:id="source-42">PLoS biology</source>, <volume>18</volume>, <fpage>e3000667</fpage>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Wilson RJ"><surname>Wilson</surname> <given-names>RJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gutiérrez D"><surname>Gutiérrez</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gutiérrez J"><surname>Gutiérrez</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Monserrat VJ"><surname>Monserrat</surname> <given-names>VJ</given-names></string-name> (<year>2007</year>) <article-title hwp:id="article-title-38">An elevational shift in butterfly species richness and composition accompanying recent climate change</article-title>. <source hwp:id="source-43">Global Change Biology</source>, <volume>13</volume>, <fpage>1873</fpage>–<lpage>1887</lpage>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="website" citation-type="web" ref:id="2021.05.17.444523v4.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><collab hwp:id="collab-1">World Economic Forum’s Global Risk Initiative</collab> (<year>2020</year>) <source hwp:id="source-44">The global risks report</source> 2020. Available from : (<ext-link l:rel="related" l:ref-type="uri" l:ref="http://www3.weforum.org/docs/WEF_Global_Risk_Report_2020.pdf" ext-link-type="uri" xlink:href="http://www3.weforum.org/docs/WEF_Global_Risk_Report_2020.pdf" hwp:id="ext-link-8">http://www3.weforum.org/docs/WEF_Global_Risk_Report_2020.pdf</ext-link>), (Accessed <date-in-citation content-type="access-date">27/ 09/ 2021</date-in-citation>).</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1 xref-ref-45-2"><citation publication-type="journal" citation-type="journal" ref:id="2021.05.17.444523v4.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Yeo D"><surname>Yeo</surname> <given-names>D</given-names></string-name>, <string-name name-style="western" hwp:sortable="Puniamoorthy J"><surname>Puniamoorthy</surname> <given-names>J</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ngiam RWJ"><surname>Ngiam</surname> <given-names>RWJ</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier R"><surname>Meier</surname> <given-names>R</given-names></string-name> (<year>2018</year>) <article-title hwp:id="article-title-39">Towards holomorphology in entomology: rapid and cost-effective adult-larva matching using NGS barcodes</article-title>. <source hwp:id="source-45">Systematic Entomology</source>, <volume>43</volume>, <fpage>678</fpage>–<lpage>691</lpage>.</citation></ref></ref-list></back></article>
